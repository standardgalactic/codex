




Contents

Cover
About This eBook
Title Page
Copyright Page
Dedication Page
Preface
A Note on Notation
Contents
1. Recurrent Problems

1.1 The Tower of Hanoi
1.2 Lines in the Plane
1.3 The Josephus Problem
Exercises

Warmups
Homework exercises
Exam problems
Bonus problems
Research problems


2. Sums

2.1 Notation
2.2 Sums and Recurrences
2.3 Manipulation of Sums
2.4 Multiple Sums
2.5 General Methods

Method 0: You could look it up.
Method 1: Guess the answer, prove it by induction.
Method 2: Perturb the sum.
Method 3: Build a repertoire.
Method 4: Replace sums by integrals.
Method 5: Expand and contract.
Method 6: Use finite calculus.
Method 7: Use generating functions.

2.6 Finite and Infinite Calculus
2.7 Infinite Sums
Exercises

Warmups
Basics
Homework exercises
Exam problems
Bonus problems
Research problem


3. Integer Functions

3.1 Floors and Ceilings
3.2 Floor/Ceiling Applications
3.3 Floor/Ceiling Recurrences
3.4 'MOD': The Binary Operation
3.5 Floor/Ceiling Sums
Exercises

Warmups
Basics
Homework exercises
Exam problems
Bonus problems
Research problems


4. Number Theory

4.1 Divisibility
4.2 Primes
4.3 Prime Examples
4.4 Factorial Factors
4.5 Relative Primality
4.6 'MOD': The Congruence Relation
4.7 Independent Residues
4.8 Additional Applications
4.9 PHI and MU
Exercises

Warmups
Basics
Homework exercises
Exam problems
Bonus problems
Research problems


5. Binomial Coefficients

5.1 Basic Identities
5.2 Basic Practice

Problem 1: A sum of ratios.
Problem 2: From the literature of sorting.
Problem 3: From an old exam.
Problem 4: A sum involving two binomial coefficients.
Problem 5: A sum with three factors.
Problem 6: A sum with menacing characteristics.
Problem 7: A new obstacle.
Problem 8: A different obstacle.

5.3 Tricks of the Trade

Trick 1: Going halves.
Trick 2: High-order differences.
Trick 3: Inversion.

5.4 Generating Functions
5.5 Hypergeometric Functions
5.6 Hypergeometric Transformations
5.7 Partial Hypergeometric Sums
5.8 Mechanical Summation
Exercises

Warmups
Basics
Homework exercises
Exam problems
Bonus problems
Research problems


6. Special Numbers

6.1 Stirling Numbers
6.2 Eulerian Numbers
6.3 Harmonic Numbers
6.4 Harmonic Summation
6.5 Bernoulli Numbers
6.6 Fibonacci Numbers
6.7 Continuants
Exercises

Warmups
Basics
Homework exercises
Exam problems
Bonus problems
Research problems


7. Generating Functions

7.1 Domino Theory and Change
7.2 Basic Maneuvers
7.3 Solving Recurrences

Example 1: Fibonacci numbers revisited.
Rational Expansion Theorem for Distinct Roots.
General Expansion Theorem for Rational Generating Functions.
Example 2: A more-or-less random recurrence.
Example 3: Mutually recursive sequences.
Example 4: A closed form for change.
Example 5: A divergent series.
Example 6: A recurrence that goes all the way back.

7.4 Special Generating Functions
7.5 Convolutions

Example 1: A Fibonacci convolution.
Example 2: Harmonic convolutions.
Example 3: Convolutions of convolutions.
Example 4: A convoluted recurrence.
Example 5: A recurrence with m-fold convolution.

7.6 Exponential gf's
7.7 Dirichlet Generating Functions
Exercises

Warmups
Basics
Homework exercises
Exam problems
Bonus problems
Research problems


8. Discrete Probability

8.1 Definitions
8.2 Mean and Variance
8.3 Probability Generating Functions
8.4 Flipping Coins
8.5 Hashing

Analysis of Hashing: Introduction.
Case 1: The key is not present.
Case 2: The key is present.
Case 2, continued: Variants of the variance.
Case 1, again: Unsuccessful search revisited.

Exercises

Warmups
Basics
Homework exercises
Exam problems
Bonus problems
Research problem


9. Asymptotics

9.1 A Hierarchy
9.2 O Notation
9.3 O Manipulation

Problem 1: Return to the Wheel of Fortune.
Problem 2: Perturbation of Stirling's formula.
Problem 3: The nth prime number.
Problem 4: A sum from an old final exam.
Problem 5: An infinite sum.
Problem 6: Big Phi.

9.4 Two Asymptotic Tricks

Trick 1: Bootstrapping.
Trick 2: Trading tails.

9.5 Euler's Summation Formula
9.6 Final Summations

Summation 1: This one is too easy.
Summation 1, again: Recapitulation and generalization.
Summation 2: Harmonic numbers harmonized.
Summation 3: Stirling's approximation.
Summation 4: A bell-shaped summand.
Summation 5: The clincher.

Exercises

Warmups
Basics
Homework exercises
Exam problems
Bonus problems
Research problems


A. Answers to Exercises
B. Bibliography
C. Credits for Exercises
Index
List of Tables

Major topics include
About the authors





i
ii
iii
iv
v
vi
vii
viii
ix
x
xi
xii
xiii
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658






















About This eBook
ePUB is an open, industry-standard format for eBooks. However, support of ePUB and its many features varies across reading devices and applications. Use your device or app settings to customize the presentation to your liking. Settings that you can customize often include font, font size, single or double column, landscape or portrait mode, and figures that you can click or tap to enlarge. For additional information about the settings and features on your reading device or app, visit the device manufacturer's Web site.
Many titles include programming code or configuration examples. To optimize the presentation of these elements, view the eBook in single-column, landscape mode and adjust the font size to the smallest setting. In addition to presenting code and configurations in the reflowable text format, we have included images of the code that mimic the presentation found in the print book; therefore, where the reflowable format may compromise the presentation of the code listing, you will see a "Click here to view code image" link. Click the link to view the print-fidelity code image. To return to the previous page viewed, click the Back button on your device or app.









Concrete Mathematics
Second Edition
A Foundation for Computer Science
Ronald L. Graham
AT&T Bell Laboratories
Donald E. Knuth
Stanford University
Oren Patashnik
Center for Communications Research

Upper Saddle River, NJ • Boston • Indianapolis • San FranciscoNew York • Toronto • Montréal • London • Munich • Paris • MadridCapetown • Sydney • Tokyo • Singapore • Mexico City









Library of Congress Cataloging-in-Publication Data
Graham, Ronald Lewis, 1935-
    Concrete mathematics : a foundation for computer science / Ronald
  L. Graham, Donald E. Knuth, Oren Patashnik. -- 2nd ed.
  xiii,657 p. 24 cm.
  Bibliography: p. 604
  Includes index.
  ISBN 978-0-201-55802-9
  1. Mathematics.  2. Computer science-Mathematics.  I. Knuth,
Donald Ervin, 1938-    .  II. Patashnik, Oren, 1954-     . III. Title.
QA39.2.G733 1994
510-dc20                                                      93-40325
                                                                  CIP
Internet page http://www-cs-faculty.stanford.edu/~knuth/gkp.html contains current information about this book and related books.
Electronic version by Mathematical Sciences Publishers (MSP), http://msp.org
Copyright © 1994, 1989 by Addison-Wesley Publishing Company, Inc.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of the publisher. Printed in the United States of America.
ISBN-13 978-0-201-55802-9ISBN-10         0-201-55802-5
First digital release, August 2015









Dedicated to Leonhard Euler (1707-1783)









Preface
"Audience, level, and treatment—a description of such matters is what prefaces are supposed to be about."
—P. R. Halmos [173]
This book is based on a course of the same name that has been taught annually at Stanford University since 1970. About fifty students have taken it each year—juniors and seniors, but mostly graduate students—and alumni of these classes have begun to spawn similar courses elsewhere. Thus the time seems ripe to present the material to a wider audience (including sophomores).
It was a dark and stormy decade when Concrete Mathematics was born. Long-held values were constantly being questioned during those turbulent years; college campuses were hotbeds of controversy. The college curriculum itself was challenged, and mathematics did not escape scrutiny. John Hammersley had just written a thought-provoking article "On the enfeeblement of mathematical skills by 'Modern Mathematics' and by similar soft intellectual trash in schools and universities" [176]; other worried mathematicians [332] even asked, "Can mathematics be saved?" One of the present authors had embarked on a series of books called The Art of Computer Programming, and in writing the first volume he (DEK) had found that there were mathematical tools missing from his repertoire; the mathematics he needed for a thorough, well-grounded understanding of computer programs was quite different from what he'd learned as a mathematics major in college. So he introduced a new course, teaching what he wished somebody had taught him.
"People do acquire a little brief authority by equipping themselves with jargon: they can pontificate and air a superficial expertise. But what we should ask of educated mathematicians is not what they can speechify about, nor even what they know about the existing corpus of mathematical knowledge, but rather what can they now do with their learning and whether they can actually solve mathematical problems arising in practice. In short, we look for deeds not words."
—J. Hammersley [176]
The course title "Concrete Mathematics" was originally intended as an antidote to "Abstract Mathematics," since concrete classical results were rapidly being swept out of the modern mathematical curriculum by a new wave of abstract ideas popularly called the "New Math." Abstract mathematics is a wonderful subject, and there's nothing wrong with it: It's beautiful, general, and useful. But its adherents had become deluded that the rest of mathematics was inferior and no longer worthy of attention. The goal of generalization had become so fashionable that a generation of mathematicians had become unable to relish beauty in the particular, to enjoy the challenge of solving quantitative problems, or to appreciate the value of technique. Abstract mathematics was becoming inbred and losing touch with reality; mathematical education needed a concrete counterweight in order to restore a healthy balance.
When DEK taught Concrete Mathematics at Stanford for the first time, he explained the somewhat strange title by saying that it was his attempt to teach a math course that was hard instead of soft. He announced that, contrary to the expectations of some of his colleagues, he was not going to teach the Theory of Aggregates, nor Stone's Embedding Theorem, nor even the Stone-Čech compactification. (Several students from the civil engineering department got up and quietly left the room.)
"The heart of mathematics consists of concrete examples and concrete problems."
—P. R. Halmos [172]
Although Concrete Mathematics began as a reaction against other trends, the main reasons for its existence were positive instead of negative. And as the course continued its popular place in the curriculum, its subject matter "solidified" and proved to be valuable in a variety of new applications. Meanwhile, independent confirmation for the appropriateness of the name came from another direction, when Z. A. Melzak published two volumes entitled Companion to Concrete Mathematics [267].
"It is downright sinful to teach the abstract before the concrete."
—Z. A. Melzak [267]
The material of concrete mathematics may seem at first to be a disparate bag of tricks, but practice makes it into a disciplined set of tools. Indeed, the techniques have an underlying unity and a strong appeal for many people. When another one of the authors (RLG) first taught the course in 1979, the students had such fun that they decided to hold a class reunion a year later.
But what exactly is Concrete Mathematics? It is a blend of CONtinuous and disCRETE mathematics. More concretely, it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems. Once you, the reader, have learned the material in this book, all you will need is a cool head, a large sheet of paper, and fairly decent handwriting in order to evaluate horrendous-looking sums, to solve complex recurrence relations, and to discover subtle patterns in data. You will be so fluent in algebraic techniques that you will often find it easier to obtain exact results than to settle for approximate answers that are valid only in a limiting sense.
Concrete Mathematics is a bridge to abstract mathematics.
The major topics treated in this book include sums, recurrences, elementary number theory, binomial coefficients, generating functions, discrete probability, and asymptotic methods. The emphasis is on manipulative technique rather than on existence theorems or combinatorial reasoning; the goal is for each reader to become as familiar with discrete operations (like the greatest-integer function and finite summation) as a student of calculus is familiar with continuous operations (like the absolute-value function and indefinite integration).
"The advanced reader who skips parts that appear too elementary may miss more than the less advanced reader who skips parts that appear too complex."
— G. Pólya [297]
Notice that this list of topics is quite different from what is usually taught nowadays in undergraduate courses entitled "Discrete Mathematics." Therefore the subject needs a distinctive name, and "Concrete Mathematics" has proved to be as suitable as any other.
(We're not bold enough to try Distinuous Mathematics.)
The original textbook for Stanford's course on concrete mathematics was the "Mathematical Preliminaries" section in The Art of Computer Programming [207]. But the presentation in those 110 pages is quite terse, so another author (OP) was inspired to draft a lengthy set of supplementary notes. The present book is an outgrowth of those notes; it is an expansion of, and a more leisurely introduction to, the material of Mathematical Preliminaries. Some of the more advanced parts have been omitted; on the other hand, several topics not found there have been included here so that the story will be complete.
". . . a concrete life preserver thrown to students sinking in a sea of abstraction."
— W. Gottschalk
The authors have enjoyed putting this book together because the subject began to jell and to take on a life of its own before our eyes; this book almost seemed to write itself. Moreover, the somewhat unconventional approaches we have adopted in several places have seemed to fit together so well, after these years of experience, that we can't help feeling that this book is a kind of manifesto about our favorite way to do mathematics. So we think the book has turned out to be a tale of mathematical beauty and surprise, and we hope that our readers will share at least ε of the pleasure we had while writing it.
Since this book was born in a university setting, we have tried to capture the spirit of a contemporary classroom by adopting an informal style. Some people think that mathematics is a serious business that must always be cold and dry; but we think mathematics is fun, and we aren't ashamed to admit the fact. Why should a strict boundary line be drawn between work and play? Concrete mathematics is full of appealing patterns; the manipulations are not always easy, but the answers can be astonishingly attractive. The joys and sorrows of mathematical work are reflected explicitly in this book because they are part of our lives.
Students always know better than their teachers, so we have asked the first students of this material to contribute their frank opinions, as "graffiti" in the margins. Some of these marginal markings are merely corny, some are profound; some of them warn about ambiguities or obscurities, others are typical comments made by wise guys in the back row; some are positive, some are negative, some are zero. But they all are real indications of feelings that should make the text material easier to assimilate. (The inspiration for such marginal notes comes from a student handbook entitled Approaching Stanford, where the official university line is counterbalanced by the remarks of outgoing students. For example, Stanford says, "There are a few things you cannot miss in this amorphous shape which is Stanford"; the margin says, "Amorphous . . . what the h*** does that mean? Typical of the pseudo-intellectualism around here." Stanford: "There is no end to the potential of a group of students living together." Graffito: "Stanford dorms are like zoos without a keeper.")
Math graffiti:Kilroy wasn't Haar. Free the group. Nuke the kernel. Power to the n. N=1 ⇒ P=NP.
I have only a marginal interest in this subject.
This was the most enjoyable course I've ever had. But it might be nice to summarize the material as you go along.
The margins also include direct quotations from famous mathematicians of past generations, giving the actual words in which they announced some of their fundamental discoveries. Somehow it seems appropriate to mix the words of Leibniz, Euler, Gauss, and others with those of the people who will be continuing the work. Mathematics is an ongoing endeavor for people everywhere; many strands are being woven into one rich fabric.
This book contains more than 500 exercises, divided into six categories:
I see:Concrete mathematics means drilling.
• Warmups are exercises that EVERY READER should try to do when first reading the material.
• Basics are exercises to develop facts that are best learned by trying one's own derivation rather than by reading somebody else's.
The homework was tough but I learned a lot. It was worth every hour.
• Homework exercises are problems intended to deepen an understanding of material in the current chapter.
• Exam problems typically involve ideas from two or more chapters simultaneously; they are generally intended for use in take-home exams (not for in-class exams under time pressure).
Take-home exams are vital—keep them.
• Bonus problems go beyond what an average student of concrete mathematics is expected to handle while taking a course based on this book; they extend the text in interesting ways.
Exams were harder than the homework led me to expect.
• Research problems may or may not be humanly solvable, but the ones presented here seem to be worth a try (without time pressure).
Answers to all the exercises appear in Appendix A, often with additional information about related results. (Of course, the "answers" to research problems are incomplete; but even in these cases, partial results or hints are given that might prove to be helpful.) Readers are encouraged to look at the answers, especially the answers to the warmup problems, but only AFTER making a serious attempt to solve the problem without peeking.
Cheaters may pass this course by just copying the answers, but they're only cheating themselves.
We have tried in Appendix C to give proper credit to the sources of each exercise, since a great deal of creativity and/or luck often goes into the design of an instructive problem. Mathematicians have unfortunately developed a tradition of borrowing exercises without any acknowledgment; we believe that the opposite tradition, practiced for example by books and magazines about chess (where names, dates, and locations of original chess problems are routinely specified) is far superior. However, we have not been able to pin down the sources of many problems that have become part of the folklore. If any reader knows the origin of an exercise for which our citation is missing or inaccurate, we would be glad to learn the details so that we can correct the omission in subsequent editions of this book.
Difficult exams don't take into account students who have other classes to prepare for.
The typeface used for mathematics throughout this book is a new design by Hermann Zapf [227], commissioned by the American Mathematical Society and developed with the help of a committee that included B. Beeton, R. P. Boas, L. K. Durst, D. E. Knuth, P. Murdock, R. S. Palais, P. Renz, E. Swanson, S. B. Whidden, and W. B. Woolf. The underlying philosophy of Zapf's design is to capture the flavor of mathematics as it might be written by a mathematician with excellent handwriting. A handwritten rather than mechanical style is appropriate because people generally create mathematics with pen, pencil, or chalk. (For example, one of the trademarks of the new design is the symbol for zero, '0', which is slightly pointed at the top because a handwritten zero rarely closes together smoothly when the curve returns to its starting point.) The letters are upright, not italic, so that subscripts, superscripts, and accents are more easily fitted with ordinary symbols. This new type family has been named AMS Euler, after the great Swiss mathematician Leonhard Euler (1707-1783) who discovered so much of mathematics as we know it today. The alphabets include Euler Text ( through ), Euler Fraktur ( through ), and Euler Script Capitals ( through ), as well as Euler Greek ( through ) and special symbols such as  and . We are especially pleased to be able to inaugurate the Euler family of typefaces in this book, because Leonhard Euler's spirit truly lives on every page: Concrete mathematics is Eulerian mathematics.
I'm unaccustomed to this face.
Dear prof: Thanks for (1) the puns, (2) the subject matter.
The authors are extremely grateful to Andrei Broder, Ernst Mayr, Andrew Yao, and Frances Yao, who contributed greatly to this book during the years that they taught Concrete Mathematics at Stanford. Furthermore we offer 1024 thanks to the teaching assistants who creatively transcribed what took place in class each year and who helped to design the examination questions; their names are listed in Appendix C. This book, which is essentially a compendium of sixteen years' worth of lecture notes, would have been impossible without their first-rate work.
I don't see how what I've learned will ever help me.
Many other people have helped to make this book a reality. For example, we wish to commend the students at Brown, Columbia, CUNY, Princeton, Rice, and Stanford who contributed the choice graffiti and helped to debug our first drafts. Our contacts at Addison-Wesley were especially efficient and helpful; in particular, we wish to thank our publisher (Peter Gordon), production supervisor (Bette Aaronson), designer (Roy Brown), and copy editor (Lyn Dupré). The National Science Foundation and the Office of Naval Research have given invaluable support. Cheryl Graham was tremendously helpful as we prepared the index. And above all, we wish to thank our wives (Fan, Jill, and Amy) for their patience, support, encouragement, and ideas.
I had a lot of trouble in this class, but I know it sharpened my math skills and my thinking skills.
This second edition features a new Section 5.8, which describes some important ideas that Doron Zeilberger discovered shortly after the first edition went to press. Additional improvements to the first printing can also be found on almost every page.
We have tried to produce a perfect book, but we are imperfect authors. Therefore we solicit help in correcting any mistakes that we've made. A reward of $2.56 will gratefully be conveyed to anyone who is the first to report any error, whether it is mathematical, historical, or typographical.
I would advise the casual student to stay away from this course.
Murray Hill, New Jerseyand Stanford, CaliforniaMay 1988 and October 1993
— RLGDEKOP









A Note on Notation
Some of the symbolism in this book has not (yet?) become standard. Here is a list of notations that might be unfamiliar to readers who have learned similar material from other books, together with the page numbers where these notations are explained. (See the general index, at the end of the book, for references to more standard notations.)


Notation
Name
Page


ln x
natural logarithm: loge x
276


lg x
binary logarithm: log2 x
70


log x
common logarithm: log10 x
449


x
floor: max{ n | n ≤ x, integer n }
67


x
ceiling: min{ n | n ≥ x, integer n }
67


x mod y
remainder: x - yx/y
82


{x}
fractional part: x mod 1
70



indefinite summation
48



definite summation
49


xn
falling factorial power: x!/(x - n)!
47, 211



rising factorial power: Γ(x + n)/Γ(x)
48, 211


n¡
subfactorial: n!/0! - n!/1! + · · · + (-1)nn!/n!
194


ℜz
real part: x, if z = x + iy
64


ℑz
imaginary part: y, if z = x + iy
64


Hn
harmonic number: 1/1 + · · · + 1/n
29



generalized harmonic number: 1/1x + · · · + 1/nx
277


f(m)(z)
mth derivative of f at z
470



Stirling cycle number (the "first kind")
259



Stirling subset number (the "second kind")
258



Eulerian number
267



Second-order Eulerian number
270


(am . . . a0)b
radix notation for 
11


K(a1, . . . , an)
continuant polynomial
302



hypergeometric function
205


#A
cardinality: number of elements in the set A
39


[zn] f(z)
coefficient of zn in f(z)
197


[α . . β]
closed interval: the set {x | α ≤ x ≤ β}
73


[m = n]
1 if m = n, otherwise 0*
24


[m\n]
1 if m divides n, otherwise 0*
102


[m\\n]
1 if m exactly divides n, otherwise 0*
146


[m ⊥ n]
1 if m is relatively prime to n, otherwise 0*
115


If you don't understand what the x denotes at the bottom of this page, try asking your Latin professor instead of your math professor.
Prestressed concrete mathematics is concrete mathematics that's preceded by a bewildering list of notations.
Also 'nonstring' is a string.
*In general, if S is any statement that can be true or false, the bracketed notation [S] stands for 1 if S is true, 0 otherwise.
Throughout this text, we use single-quote marks ('. . . ') to delimit text as it is written, double-quote marks (". . . ") for a phrase as it is spoken. Thus, the string of letters 'string' is sometimes called a "string."
An expression of the form 'a/bc' means the same as 'a/(bc)'. Moreover, log x/log y = (log x)/(log y) and 2n! = 2(n!).









Contents
1 Recurrent Problems
1.1 The Tower of Hanoi
1.2 Lines in the Plane
1.3 The Josephus Problem
Exercises
2 Sums
2.1 Notation
2.2 Sums and Recurrences
2.3 Manipulation of Sums
2.4 Multiple Sums
2.5 General Methods
2.6 Finite and Infinite Calculus
2.7 Infinite Sums
Exercises
3 Integer Functions
3.1 Floors and Ceilings
3.2 Floor/Ceiling Applications
3.3 Floor/Ceiling Recurrences
3.4 'mod': The Binary Operation
3.5 Floor/Ceiling Sums
Exercises
4 Number Theory
4.1 Divisibility
4.2 Primes
4.3 Prime Examples
4.4 Factorial Factors
4.5 Relative Primality
4.6 'mod': The Congruence Relation
4.7 Independent Residues
4.8 Additional Applications
4.9 Phi and Mu
Exercises
5 Binomial Coefficients
5.1 Basic Identities
5.2 Basic Practice
5.3 Tricks of the Trade
5.4 Generating Functions
5.5 Hypergeometric Functions
5.6 Hypergeometric Transformations
5.7 Partial Hypergeometric Sums
5.8 Mechanical Summation
Exercises
6 Special Numbers
6.1 Stirling Numbers
6.2 Eulerian Numbers
6.3 Harmonic Numbers
6.4 Harmonic Summation
6.5 Bernoulli Numbers
6.6 Fibonacci Numbers
6.7 Continuants
Exercises
7 Generating Functions
7.1 Domino Theory and Change
7.2 Basic Maneuvers
7.3 Solving Recurrences
7.4 Special Generating Functions
7.5 Convolutions
7.6 Exponential Generating Functions
7.7 Dirichlet Generating Functions
Exercises
8 Discrete Probability
8.1 Definitions
8.2 Mean and Variance
8.3 Probability Generating Functions
8.4 Flipping Coins
8.5 Hashing
Exercises
9 Asymptotics
9.1 A Hierarchy
9.2 O Notation
9.3 O Manipulation
9.4 Two Asymptotic Tricks
9.5 Euler's Summation Formula
9.6 Final Summations
Exercises
A Answers to Exercises
B Bibliography
C Credits for Exercises
Index
List of Tables









1. Recurrent Problems
This chapter explores three sample problems that give a feel for what's to come. They have two traits in common: They've all been investigated repeatedly by mathematicians; and their solutions all use the idea of recurrence, in which the solution to each problem depends on the solutions to smaller instances of the same problem.

1.1 The Tower of Hanoi
Let's look first at a neat little puzzle called the Tower of Hanoi, invented by the French mathematician Edouard Lucas in 1883. We are given a tower of eight disks, initially stacked in decreasing size on one of three pegs:



Raise your hand if you've never seen this. OK, the rest of you can cut to equation (1.1).
The objective is to transfer the entire tower to one of the other pegs, moving only one disk at a time and never moving a larger one onto a smaller.
Gold—wow. Are our disks made of concrete?
Lucas [260] furnished his toy with a romantic legend about a much larger Tower of Brahma, which supposedly has 64 disks of pure gold resting on three diamond needles. At the beginning of time, he said, God placed these golden disks on the first needle and ordained that a group of priests should transfer them to the third, according to the rules above. The priests reportedly work day and night at their task. When they finish, the Tower will crumble and the world will end.
It's not immediately obvious that the puzzle has a solution, but a little thought (or having seen the problem before) convinces us that it does. Now the question arises: What's the best we can do? That is, how many moves are necessary and sufficient to perform the task?
The best way to tackle a question like this is to generalize it a bit. The Tower of Brahma has 64 disks and the Tower of Hanoi has 8; let's consider what happens if there are n disks.
One advantage of this generalization is that we can scale the problem down even more. In fact, we'll see repeatedly in this book that it's advantageous to LOOK AT SMALL CASES first. It's easy to see how to transfer a tower that contains only one or two disks. And a small amount of experimentation shows how to transfer a tower of three.
The next step in solving the problem is to introduce appropriate notation: NAME AND CONQUER. Let's say that Tn is the minimum number of moves that will transfer n disks from one peg to another under Lucas's rules. Then T1 is obviously 1, and T2 = 3.
We can also get another piece of data for free, by considering the smallest case of all: Clearly T0 = 0, because no moves at all are needed to transfer a tower of n = 0 disks! Smart mathematicians are not ashamed to think small, because general patterns are easier to perceive when the extreme cases are well understood (even when they are trivial).
But now let's change our perspective and try to think big; how can we transfer a large tower? Experiments with three disks show that the winning idea is to transfer the top two disks to the middle peg, then move the third, then bring the other two onto it. This gives us a clue for transferring n disks in general: We first transfer the n - 1 smallest to a different peg (requiring Tn-1 moves), then move the largest (requiring one move), and finally transfer the n-1 smallest back onto the largest (requiring another Tn-1 moves). Thus we can transfer n disks (for n > 0) in at most 2Tn-1 + 1 moves:
Tn ≤ 2Tn-1 + 1,          for n > 0.This formula uses '≤' instead of ' = ' because our construction proves only that 2Tn-1 + 1 moves suffice; we haven't shown that 2Tn-1 + 1 moves are necessary. A clever person might be able to think of a shortcut.
Most of the published "solutions" to Lucas's problem, like the early one of Allardice and Fraser [7], fail to explain why Tn must be ≥ 2Tn-1 + 1.
But is there a better way? Actually no. At some point we must move the largest disk. When we do, the n - 1 smallest must be on a single peg, and it has taken at least Tn-1 moves to put them there. We might move the largest disk more than once, if we're not too alert. But after moving the largest disk for the last time, we must transfer the n - 1 smallest disks (which must again be on a single peg) back onto the largest; this too requires Tn-1 moves. Hence
Tn ≥ 2Tn-1 + 1,          for n > 0.These two inequalities, together with the trivial solution for n = 0, yield



(Notice that these formulas are consistent with the known values T1 = 1 and T2 = 3. Our experience with small cases has not only helped us to discover a general formula, it has also provided a convenient way to check that we haven't made a foolish error. Such checks will be especially valuable when we get into more complicated maneuvers in later chapters.)
Yeah, yeah . . . I seen that word before.
A set of equalities like (1.1) is called a recurrence (a.k.a. recurrence relation or recursion relation). It gives a boundary value and an equation for the general value in terms of earlier ones. Sometimes we refer to the general equation alone as a recurrence, although technically it needs a boundary value to be complete.
The recurrence allows us to compute Tn for any n we like. But nobody really likes to compute from a recurrence, when n is large; it takes too long. The recurrence only gives indirect, local information. A solution to the recurrence would make us much happier. That is, we'd like a nice, neat, "closed form" for Tn that lets us compute it quickly, even for large n. With a closed form, we can understand what Tn really is.
So how do we solve a recurrence? One way is to guess the correct solution, then to prove that our guess is correct. And our best hope for guessing the solution is to look (again) at small cases. So we compute, successively, T3 = 2·3 + 1 = 7; T4 = 2·7 + 1 = 15; T5 = 2·15 + 1 = 31; T6 = 2·31 + 1 = 63. Aha! It certainly looks as if



At least this works for n ≤ 6.
Mathematical induction is a general way to prove that some statement about the integer n is true for all n ≥ n0. First we prove the statement when n has its smallest value, n0; this is called the basis. Then we prove the statement for n > n0, assuming that it has already been proved for all values between n0 and n - 1, inclusive; this is called the induction. Such a proof gives infinitely many results with only a finite amount of work.
Mathematical induction proves that we can climb as high as we like on a ladder, by proving that we can climb onto the bottom rung (the basis) and that from each rung we can climb up to the next one (the induction).
Recurrences are ideally set up for mathematical induction. In our case, for example, (1.2) follows easily from (1.1): The basis is trivial, since T0 = 20 − 1 = 0. And the induction follows for n > 0 if we assume that (1.2) holds when n is replaced by n − 1:
Tn = 2Tn-1 + 1 = 2(2n-1 - 1) + 1 = 2n - 1.
Hence (1.2) holds for n as well. Good! Our quest for Tn has ended successfully.
Of course the priests' task hasn't ended; they're still dutifully moving disks, and will be for a while, because for n = 64 there are 264-1 moves (about 18 quintillion). Even at the impossible rate of one move per microsecond, they will need more than 5000 centuries to transfer the Tower of Brahma. Lucas's original puzzle is a bit more practical. It requires 28 - 1 = 255 moves, which takes about four minutes for the quick of hand.
The Tower of Hanoi recurrence is typical of many that arise in applications of all kinds. In finding a closed-form expression for some quantity of interest like Tn we go through three stages:
1 Look at small cases. This gives us insight into the problem and helps us in stages 2 and 3.
2 Find and prove a mathematical expression for the quantity of interest. For the Tower of Hanoi, this is the recurrence (1.1) that allows us, given the inclination, to compute Tn for any n.
3 Find and prove a closed form for our mathematical expression. For the Tower of Hanoi, this is the recurrence solution (1.2).
What is a proof? "One half of one percent pure alcohol."
The third stage is the one we will concentrate on throughout this book. In fact, we'll frequently skip stages 1 and 2 entirely, because a mathematical expression will be given to us as a starting point. But even then, we'll be getting into subproblems whose solutions will take us through all three stages.
Our analysis of the Tower of Hanoi led to the correct answer, but it required an "inductive leap"; we relied on a lucky guess about the answer. One of the main objectives of this book is to explain how a person can solve recurrences without being clairvoyant. For example, we'll see that recurrence (1.1) can be simplified by adding 1 to both sides of the equations:
T0 + 1 = 1;
Tn + 1 = 2Tn-1 + 2,          for n > 0.
Now if we let Un = Tn + 1, we have



Interesting: We get rid of the +1 in (1.1) by adding, not by subtracting.
It doesn't take genius to discover that the solution to this recurrence is just Un = 2n; hence Tn = 2n - 1. Even a computer could discover this.


1.2 LINES IN THE PLANE
Our second sample problem has a more geometric flavor: How many slices of pizza can a person obtain by making n straight cuts with a pizza knife? Or, more academically: What is the maximum number Ln of regions defined by n lines in the plane? This problem was first solved in 1826, by the Swiss mathematician Jacob Steiner [338].
(A pizza with Swiss cheese?)
Again we start by looking at small cases, remembering to begin with the smallest of all. The plane with no lines has one region; with one line it has two regions; and with two lines it has four regions:



(Each line extends infinitely in both directions.)
Sure, we think, Ln = 2n; of course! Adding a new line simply doubles the number of regions. Unfortunately this is wrong. We could achieve the doubling if the nth line would split each old region in two; certainly it can split an old region in at most two pieces, since each old region is convex. (A straight line can split a convex region into at most two new regions, which will also be convex.) But when we add the third line—the thick one in the diagram below—we soon find that it can split at most three of the old regions, no matter how we've placed the first two lines:



Thus L3 = 4 + 3 = 7 is the best we can do.
A region is convex if it includes all line segments between any two of its points. (That's not what my dictionary says, but it's what mathematicians believe.)
And after some thought we realize the appropriate generalization. The nth line (for n > 0) increases the number of regions by k if and only if it splits k of the old regions, and it splits k old regions if and only if it hits the previous lines in k - 1 different places. Two lines can intersect in at most one point. Therefore the new line can intersect the n - 1 old lines in at most n - 1 different points, and we must have k ≤ n. We have established the upper bound
Ln ≤ Ln-1 + n,          for n > 0.
Furthermore it's easy to show by induction that we can achieve equality in this formula. We simply place the nth line in such a way that it's not parallel to any of the others (hence it intersects them all), and such that it doesn't go through any of the existing intersection points (hence it intersects them all in different places). The recurrence is therefore



The known values of L1, L2, and L3 check perfectly here, so we'll buy this.
Now we need a closed-form solution. We could play the guessing game again, but 1, 2, 4, 7, 11, 16, . . . doesn't look familiar; so let's try another tack. We can often understand a recurrence by "unfolding" or "unwinding" it all the way to the end, as follows:
Ln = Ln-1 + n
= Ln-2 + (n - 1) + n
= Ln-3 + (n - 2) + (n - 1) + n
⋮
= L0 + 1 + 2 + · · · + (n - 2) + (n - 1) + n
= 1   +   Sn ,          where Sn = 1 + 2 + 3 + · · · + (n - 1) + n.
Unfolding? I'd call this "plugging in."
In other words, Ln is one more than the sum Sn of the first n positive integers.
The quantity Sn pops up now and again, so it's worth making a table of small values. Then we might recognize such numbers more easily when we see them the next time:

These values are also called the triangular numbers, because Sn is the number of bowling pins in an n-row triangular array. For example, the usual four-row array  has S4 = 10 pins.
To evaluate Sn we can use a trick that Gauss reportedly came up with in 1786, when he was nine years old [88] (and which had been used already by Archimedes in propositions 10 and 11 of his classic treatise on spirals):

It seems a lot of stuff is attributed to Gauss—either he was really smart or he had a great press agent.
We merely add Sn to its reversal, so that each of the n columns on the right sums to n + 1. Simplifying,



Maybe he just had a magnetic personality.
OK, we have our solution:



Actually Gauss is often called the greatest mathematician of all time. So it's nice to be able to understand at least one of his discoveries.
As experts, we might be satisfied with this derivation and consider it a proof, even though we waved our hands a bit when doing the unfolding and reflecting. But students of mathematics should be able to meet stricter standards; so it's a good idea to construct a rigorous proof by induction. The key induction step is
Ln = Ln-1 + n = ((n - 1)n + 1) + n = n(n + 1) + 1.
Now there can be no doubt about the closed form (1.6).
When in doubt, look at the words. Why is it "closed," as opposed to "open"? What image does it bring to mind?Answer: The equation is "closed," not defined in terms of itself—not leading to recurrence. The case is "closed"—it won't happen again. Metaphors are the key.
Incidentally we've been talking about "closed forms" without explicitly saying what we mean. Usually it's pretty clear. Recurrences like (1.1) and (1.4) are not in closed form—they express a quantity in terms of itself; but solutions like (1.2) and (1.6) are. Sums like 1 + 2 + · · · + n are not in closed form—they cheat by using '· · ·'; but expressions like n(n + 1)/2 are. We could give a rough definition like this: An expression for a quantity f(n) is in closed form if we can compute it using at most a fixed number of "well known" standard operations, independent of n. For example, 2n - 1 and n(n + 1)/2 are closed forms because they involve only addition, subtraction, multiplication, division, and exponentiation, in explicit ways.
The total number of simple closed forms is limited, and there are recurrences that don't have simple closed forms. When such recurrences turn out to be important, because they arise repeatedly, we add new operations to our repertoire; this can greatly extend the range of problems solvable in "simple" closed form. For example, the product of the first n integers, n!, has proved to be so important that we now consider it a basic operation. The formula 'n!' is therefore in closed form, although its equivalent '1·2·. . .·n' is not.
Is "zig" a technical term?
And now, briefly, a variation of the lines-in-the-plane problem: Suppose that instead of straight lines we use bent lines, each containing one "zig." What is the maximum number Zn of regions determined by n such bent lines in the plane? We might expect Zn to be about twice as big as Ln, or maybe three times as big. Let's see:



From these small cases, and after a little thought, we realize that a bent line is like two straight lines except that regions merge when the "two" lines don't extend past their intersection point.
. . . and a little afterthought. . .



Regions 2, 3, and 4, which would be distinct with two lines, become a single region when there's a bent line; we lose two regions. However, if we arrange things properly—the zig point must lie "beyond" the intersections with the other lines—that's all we lose; that is, we lose only two regions per bent line. Thus
Exercise 18 has the details.



Comparing the closed forms (1.6) and (1.7), we find that for large n,
Ln ∼ n2,
Zn ∼ 2n2;
so we get about four times as many regions with bent lines as with straight lines. (In later chapters we'll be discussing how to analyze the approximate behavior of integer functions when n is large. The '∼' symbol is defined in Section 9.1.)


1.3 The Josephus Problem
Our final introductory example is a variant of an ancient problem named for Flavius Josephus, a famous historian of the first century. Legend has it that Josephus wouldn't have lived to become famous without his mathematical talents. During the Jewish-Roman war, he was among a band of 41 Jewish rebels trapped in a cave by the Romans. Preferring suicide to capture, the rebels decided to form a circle and, proceeding around it, to kill every third remaining person until no one was left. But Josephus, along with an unindicted co-conspirator, wanted none of this suicide nonsense; so he quickly calculated where he and his friend should stand in the vicious circle.
(Ahrens [5, vol. 2] and Herstein and Kaplansky [187] discuss the interesting history of this problem. Josephus himself [197] is a bit vague.)
. . . thereby saving his tale for us to hear.
In our variation, we start with n people numbered 1 to n around a circle, and we eliminate every second remaining person until only one survives. For example, here's the starting configuration for n = 10:



The elimination order is 2, 4, 6, 8, 10, 3, 7, 1, 9, so 5 survives. The problem: Determine the survivor's number, J(n).
Here's a case where n = 0 makes no sense.
We just saw that J(10) = 5. We might conjecture that J(n) = n/2 when n is even; and the case n = 2 supports the conjecture: J(2) = 1. But a few other small cases dissuade us—the conjecture fails for n = 4 and n = 6.



Even so, a bad guess isn't a waste of time, because it gets us involved in the problem.
It's back to the drawing board; let's try to make a better guess. Hmmm . . . J(n) always seems to be odd. And in fact, there's a good reason for this: The first trip around the circle eliminates all the even numbers. Furthermore, if n itself is an even number, we arrive at a situation similar to what we began with, except that there are only half as many people, and their numbers have changed.
So let's suppose that we have 2n people originally. After the first go-round, we're left with



and 3 will be the next to go. This is just like starting out with n people, except that each person's number has been doubled and decreased by 1. That is,
J(2n) = 2J(n) - 1,          for n ≥ 1.
This is the tricky part: We have J(2n) = newnumber (J(n)), where newnumber (k) = 2k - 1.
We can now go quickly to large n. For example, we know that J(10) = 5, so
J(20) = 2J(10) - 1 = 2·5 - 1 = 9.
Similarly J(40) = 17, and we can deduce that J(5·2m) = 2m+1 + 1.
But what about the odd case? With 2n + 1 people, it turns out that person number 1 is wiped out just after person number 2n, and we're left with



Again we almost have the original situation with n people, but this time their numbers are doubled and increased by 1. Thus
J(2n + 1) = 2J(n) + 1,          for n ≥ 1.
Odd case? Hey, leave my brother out of it.
Combining these equations with J(1) = 1 gives us a recurrence that defines J in all cases:



Instead of getting J(n) from J(n - 1), this recurrence is much more "efficient," because it reduces n by a factor of 2 or more each time it's applied. We could compute J(1000000), say, with only 19 applications of (1.8). But still, we seek a closed form, because that will be even quicker and more informative. After all, this is a matter of life or death.
Our recurrence makes it possible to build a table of small values very quickly. Perhaps we'll be able to spot a pattern and guess the answer.



Voilà! It seems we can group by powers of 2 (marked by vertical lines in the table); J(n) is always 1 at the beginning of a group and it increases by 2 within a group. So if we write n in the form n = 2m + l, where 2m is the largest power of 2 not exceeding n and where l is what's left, the solution to our recurrence seems to be



(Notice that if 2m ≤ n < 2m+1, the remainder l = n - 2m satisfies 0 ≤ l < 2m+1 - 2m = 2m.)
We must now prove (1.9). As in the past we use induction, but this time the induction is on m. When m = 0 we must have l = 0; thus the basis of (1.9) reduces to J(1) = 1, which is true. The induction step has two parts, depending on whether l is even or odd. If m > 0 and 2m + l is equal to 2k, for some integer k, then l is even and
J(2m + l) = 2J(2m-1 + l/2) - 1 = 2(2l/2 + 1) - 1 = 2l + 1,
by (1.8) and the induction hypothesis; this is exactly what we want. A similar proof works in the odd case, when 2m + l = 2k + 1. We might also note that (1.8) implies the relation
J(2n + 1) - J(2n) = 2.
Either way, the induction is complete and (1.9) is established.
But there's a simpler way! The key fact is that J(2m) = 1 for all m, and this follows immediately from our first equation, J(2n) = 2J(n)-1. Hence we know that the first person will survive whenever n is a power of 2. And in the general case, when n = 2m + l, the number of people is reduced to a power of 2 after there have been l executions. The first remaining person at this point, the survivor, is number 2l + 1.
To illustrate solution (1.9), let's compute J(100). In this case we have 100 = 26 + 36, so J(100) = 2·36 + 1 = 73.
Now that we've done the hard stuff (solved the problem) we seek the soft: Every solution to a problem can be generalized so that it applies to a wider class of problems. Once we've learned a technique, it's instructive to look at it closely and see how far we can go with it. Hence, for the rest of this section, we will examine the solution (1.9) and explore some generalizations of the recurrence (1.8). These explorations will uncover the structure that underlies all such problems.
Powers of 2 played an important role in our finding the solution, so it's natural to look at the radix 2 representations of n and J(n). Suppose n's binary expansion is
n = (bm bm-1 . . . b1 b0)2;
that is,
n = bm2m + bm-12m-1 + · · · + b12 + b0 ,
where each bi is either 0 or 1 and where the leading bit bm is 1. Recalling that n = 2m + l, we have, successively,


n =
(1 bm-1 bm-2 . . . b1 b0)2 ,


l =
(0 bm-1 bm-2 . . . b1 b0)2 ,


2l =
(bm-1 bm-2 . . . b1 b0 0)2 ,


2l + 1 =
(bm-1 bm-2 . . . b1 b0 1)2 ,


J(n) =
(bm-1 bm-2 . . . b1 b0 bm)2 .


(The last step follows because J(n) = 2l + 1 and because bm = 1.) We have proved that



that is, in the lingo of computer programming, we get J(n) from n by doing a one-bit cyclic shift left! Magic. For example, if n = 100 = (1100100)2 then J(n) = J((1100100)2)= (1001001)2, which is 64 + 8 + 1 = 73. If we had been working all along in binary notation, we probably would have spotted this pattern immediately.
If we start with n and iterate the J function m + 1 times, we're doing m + 1 one-bit cyclic shifts; so, since n is an (m+1)-bit number, we might expect to end up with n again. But this doesn't quite work. For instance if n = 13 we have J((1101)2) = (1011)2, but then J((1011)2) = (111)2 and the process breaks down; the 0 disappears when it becomes the leading bit. In fact, J(n) must always be ≤ n by definition, since J(n) is the survivor's number; hence if J(n) < n we can never get back up to n by continuing to iterate.
("Iteration" here means applying a function to itself.)
Repeated application of J produces a sequence of decreasing values that eventually reach a "fixed point," where J(n) = n. The cyclic shift property makes it easy to see what that fixed point will be: Iterating the function enough times will always produce a pattern of all 1's whose value is 2ν(n) - 1, where ν(n) is the number of 1 bits in the binary representation of n. Thus, since ν(13) = 3, we have



similarly



Curiously enough, if M is a compact C∞ n-manifold (n > 1), there exists a differentiable immersion of M into R2n-ν(n) but not necessarily into R2n-ν(n)-1. I wonder if Josephus was secretly a topologist?
Curious, but true.
Let's return briefly to our first guess, that J(n) = n/2 when n is even. This is obviously not true in general, but we can now determine exactly when it is true:


J(n) =
n/2,


2l + 1 =
(2m + l)/2,


l =
 (2m - 2) .


If this number  is an integer, then n = 2m + l will be a solution, because l will be less than 2m. It's not hard to verify that 2m - 2 is a multiple of 3 when m is odd, but not when m is even. (We will study such things in Chapter 4.) Therefore there are infinitely many solutions to the equation J(n) = n/2, beginning as follows:



Notice the pattern in the rightmost column. These are the binary numbers for which cyclic-shifting one place left produces the same result as ordinary-shifting one place right (halving).
OK, we understand the J function pretty well; the next step is to generalize it. What would have happened if our problem had produced a recurrence that was something like (1.8), but with different constants? Then we might not have been lucky enough to guess the solution, because the solution might have been really weird. Let's investigate this by introducing constants α, β, and γ and trying to find a closed form for the more general recurrence



Looks like Greek to me.
(Our original recurrence had α = 1, β = -1, and γ = 1.) Starting with f(1) = α and working our way up, we can construct the following general table for small values of n:



It seems that α's coefficient is n's largest power of 2. Furthermore, between powers of 2, β's coefficient decreases by 1 down to 0 and γ's increases by 1 up from 0. Therefore if we express f(n) in the form



by separating out its dependence on α, β, and γ, it seems that



Here, as usual, n = 2m + l and 0 ≤ l < 2m, for n ≥ 1.
It's not terribly hard to prove (1.13) and (1.14) by induction, but the calculations are messy and uninformative. Fortunately there's a better way to proceed, by choosing particular values and then combining them. Let's illustrate this by considering the special case α = 1, β = γ = 0, when f(n) is supposed to be equal to A(n): Recurrence (1.11) becomes


A(1) =
1;
 


A(2n) =
2A(n),
for n ≥ 1;


A(2n + 1) =
2A(n) ,
for n ≥ 1.


Hold onto your hats, this next part is new stuff.
Sure enough, it's true (by induction on m) that A(2m + l) = 2m.
Next, let's use recurrence (1.11) and solution (1.13) in reverse, by starting with a simple function f(n) and seeing if there are any constants (α, β, γ) that will define it. Plugging the constant function f(n) = 1 into (1.11) says that
1 = α;
1 = 2·1 + β;
1 = 2·1 + γ;
hence the values (α, β, γ) = (1, -1, -1) satisfying these equations will yield A(n) - B(n) - C(n) = f(n) = 1. Similarly, we can plug in f(n) = n:


1 =
α;


2n =
2·n + β;


2n + 1 =
2·n + γ.


A neat idea!
These equations hold for all n when α = 1, β = 0, and γ = 1, so we don't need to prove by induction that these parameters will yield f(n) = n. We already know that f(n) = n will be the solution in such a case, because the recurrence (1.11) uniquely defines f(n) for every value of n.
And now we're essentially done! We have shown that the functions A(n), B(n), and C(n) of (1.13), which solve (1.11) in general, satisfy the equations


A(n) =
2m,
where n = 2m + l and 0 ≤ l < 2m;


A(n) - B(n) - C(n) =
1;
 


A(n) + C(n) =
n.
 


Our conjectures in (1.14) follow immediately, since we can solve these equations to get C(n) = n - A(n) = l and B(n) = A(n) - 1 - C(n) = 2m - 1 - l.
Beware: The authors are expecting us to figure out the idea of the repertoire method from seat-of-the-pants examples, instead of giving us a top-down presentation. The method works best with recurrences that are "linear," in the sense that the solutions can be expressed as a sum of arbitrary parameters multiplied by functions of n, as in (1.13). Equation (1.13) is the key.
This approach illustrates a surprisingly useful repertoire method for solving recurrences. First we find settings of general parameters for which we know the solution; this gives us a repertoire of special cases that we can solve. Then we obtain the general case by combining the special cases. We need as many independent special solutions as there are independent parameters (in this case three, for α, β, and γ). Exercises 16 and 20 provide further examples of the repertoire approach.
We know that the original J-recurrence has a magical solution, in binary:
J((bm bm-1 . . . b1 b0)2) = (bm-1 . . . b1 b0 bm)2,          where bm = 1.
Does the generalized Josephus recurrence admit of such magic?
Sure, why not? We can rewrite the generalized recurrence (1.11) as



if we let β0 = β and β1 = γ. And this recurrence unfolds, binary-wise:


f (bm bm-1 . . . b1 b0)2
= 2f((bm bm-1 . . . b1)2) + βb0


 
= 4f (bm bm-1 . . . b2)2 + 2βb1 + βb0


 
          ⋮


 
= 2mf((bm)2) +2m -1βbm-1+ · · · +2βb1+βb0


 
= 2mα + 2m -1βbm-1 + · · · + 2βb1 + βb0 .


('relax' = 'destroy')
Suppose we now relax the radix 2 notation to allow arbitrary digits instead of just 0 and 1. The derivation above tells us that



Nice. We would have seen this pattern earlier if we had written (1.12) in another way:
I think I get it: The binary representations of A(n), B(n), and C(n) have 1's in different positions.

For example, when n = 100 = (1100100)2, our original Josephus values α = 1, β = -1, and γ = 1 yield

as before. The cyclic-shift property follows because each block of binary digits (1 0 . . . 0 0)2 in the representation of n is transformed into
(1 -1 . . . -1 -1)2 = (0 0 . . . 0 1)2.
So our change of notation has given us the compact solution (1.16) to the general recurrence (1.15). If we're really uninhibited we can now generalize even more. The recurrence



is the same as the previous one except that we start with numbers in radix d and produce values in radix c. That is, it has the radix-changing solution



"There are two kinds of generalizations. One is cheap and the other is valuable. It is easy to generalize by diluting a little idea with a big terminology. It is much more difficult to prepare a refined and condensed extract from several good ingredients."
—G. Pólya [297]
For example, suppose that by some stroke of luck we're given the recurrence


f(1) =
34;
 


f(2) =
5;
 


f(3n) =
10f(n) + 76,
for n ≥ 1;


f(3n + 1) =
10f(n) - 2,
for n ≥ 1;


f(3n + 2) =
10f(n) + 8,
for n ≥ 1;


and suppose we want to compute f(19). Here we have d = 3 and c = 10. Now 19 = (201)3, and the radix-changing solution tells us to perform a digit-by-digit replacement from radix 3 to radix 10. So the leading 2 becomes a 5, and the 0 and 1 become 76 and -2, giving
f(19) = f((201)3) = (5 76 -2)10 = 1258,
which is our answer.
Perhaps this was a stroke of bad luck.
Thus Josephus and the Jewish-Roman war have led us to some interesting general recurrences.
But in general I'm against recurrences of war.


Exercises

Warmups
1. All horses are the same color; we can prove this by induction on the number of horses in a given set. Here's how: "If there's just one horse then it's the same color as itself, so the basis is trivial. For the induction step, assume that there are n horses numbered 1 to n. By the induction hypothesis, horses 1 through n - 1 are the same color, and similarly horses 2 through n are the same color. But the middle horses, 2 through n - 1, can't change color when they're in different groups; these are horses, not chameleons. So horses 1 and n must be the same color as well, by transitivity. Thus all n horses are the same color; QED." What, if anything, is wrong with this reasoning?
Please do all the warmups in all the chapters!
—The Mgm't
2. Find the shortest sequence of moves that transfers a tower of n disks from the left peg A to the right peg B, if direct moves between A and B are disallowed. (Each move must be to or from the middle peg. As usual, a larger disk must never appear above a smaller one.)
3. Show that, in the process of transferring a tower under the restrictions of the preceding exercise, we will actually encounter every properly stacked arrangement of n disks on three pegs.
4. Are there any starting and ending configurations of n disks on three pegs that are more than 2n - 1 moves apart, under Lucas's original rules?
5. A "Venn diagram" with three overlapping circles is often used to illustrate the eight possible subsets associated with three given sets:



Can the sixteen possibilities that arise with four given sets be illustrated by four overlapping circles?
6. Some of the regions defined by n lines in the plane are infinite, while others are bounded. What's the maximum possible number of bounded regions?
7. Let H(n) = J(n + 1) - J(n). Equation (1.8) tells us that H(2n) = 2, and H(2n+1) = J(2n+2)-J(2n+1) = (2J(n+1)-1) - (2J(n)+1) = 2H(n)-2, for all n ≥ 1. Therefore it seems possible to prove that H(n) = 2 for all n, by induction on n. What's wrong here?


Homework exercises
8. Solve the recurrence
Q0 = α;          Q1 = β;
Qn = (1 + Qn-1)/Qn-2,          for n > 1.
Assume that Qn ≠ 0 for all n ≥ 0. Hint: Q4 = (1 + α)/β.
9. Sometimes it's possible to use induction backwards, proving things from n to n - 1 instead of vice versa! For example, consider the statement



. . . now that's a horse of a different color.
This is true when n = 2, since (x1 + x2)2 - 4x1x2 = (x1 - x2)2 ≥ 0.
a. By setting xn = (x1 + · · · + xn-1)/(n - 1), prove that P(n) implies P(n - 1) whenever n > 1.
b. Show that P(n) and P(2) imply P(2n).
c. Explain why this implies the truth of P(n) for all n.
10. Let Qn be the minimum number of moves needed to transfer a tower of n disks from A to B if all moves must be clockwise—that is, from A to B, or from B to the other peg, or from the other peg to A. Also let Rn be the minimum number of moves needed to go from B back to A under this restriction. Prove that



(You need not solve these recurrences; we'll see how to do that in Chapter 7.)
11. A Double Tower of Hanoi contains 2n disks of n different sizes, two of each size. As usual, we're required to move only one disk at a time, without putting a larger one over a smaller one.
a. How many moves does it take to transfer a double tower from one peg to another, if disks of equal size are indistinguishable from each other?
b. What if we are required to reproduce the original top-to-bottom order of all the equal-size disks in the final arrangement? [Hint: This is difficult—it's really a "bonus problem."]
12. Let's generalize exercise 11a even further, by assuming that there are n different sizes of disks and exactly mk disks of size k. Determine A(m1, . . . , mn), the minimum number of moves needed to transfer a tower when equal-size disks are considered to be indistinguishable.
13. What's the maximum number of regions definable by n zig-zag lines,



each of which consists of two parallel infinite half-lines joined by a straight segment?
14. How many pieces of cheese can you obtain from a single thick piece by making five straight slices? (The cheese must stay in its original position while you do all the cutting, and each slice must correspond to a plane in 3D.) Find a recurrence relation for Pn, the maximum number of three-dimensional regions that can be defined by n different planes.
Good luck keeping the cheese in position.
15. Josephus had a friend who was saved by getting into the next-to-last position. What is I(n), the number of the penultimate survivor when every second person is executed?
16. Use the repertoire method to solve the general four-parameter recurrence


g(1) =
α;
 


g(2n + j) =
3g(n) + γn + βj,
          for j = 0, 1     and     n ≥ 1.


Hint: Try the function g(n) = n.


Exam problems
17. If Wn is the minimum number of moves needed to transfer a tower of n disks from one peg to another when there are four pegs instead of three, show that
Wn(n+1)/2 ≤ 2Wn(n-1)/2 + Tn,          for n > 0.
(Here Tn = 2n - 1 is the ordinary three-peg number.) Use this to find a closed form f(n) such that Wn(n+1)/2 ≤ f(n) for all n ≥ 0.
18. Show that the following set of n bent lines defines Zn regions, where Zn is defined in (1.7): The jth bent line, for 1 ≤ j ≤ n, has its zig at (n2j, 0) and goes up through the points (n2j - nj, 1) and (n2j - nj - n-n, 1).
19. Is it possible to obtain Zn regions with n bent lines when the angle at each zig is 30°?
20. Use the repertoire method to solve the general five-parameter recurrence


h(1) =
α;
 


h(2n + j) =
4h(n) + γjn + βj,
for j = 0, 1     and     n ≥ 1.


Hint: Try the functions h(n) = n and h(n) = n2.
Is this like a five-star general recurrence?
21. Suppose there are 2n people in a circle; the first n are "good guys" and the last n are "bad guys." Show that there is always an integer q (depending on n) such that, if we go around the circle executing every qth person, all the bad guys are first to go. (For example, when n = 3 we can take q = 5; when n = 4 we can take q = 30.)


Bonus problems
22. Show that it's possible to construct a Venn diagram for all 2n possible subsets of n given sets, using n convex polygons that are congruent to each other and rotated about a common center.
23. Suppose that Josephus finds himself in a given position j, but he has a chance to name the elimination parameter q such that every qth person is executed. Can he always save himself?


Research problems
24. Find all recurrence relations of the form



whose solution is periodic regardless of the initial values X0, . . . , Xk-1.
25. Solve infinitely many cases of the four-peg Tower of Hanoi problem by proving that equality holds in the relation of exercise 17.
26. Generalizing exercise 23, let's say that a Josephus subset of {1, 2, . . . , n} is a set of k numbers such that, for some q, the people with the other n-k numbers will be eliminated first. (These are the k positions of the "good guys" Josephus wants to save.) It turns out that when n = 9, three of the 29 possible subsets are non-Josephus, namely {1, 2, 5, 8, 9}, {2, 3, 4, 5, 8}, and {2, 5, 6, 7, 8}. There are 13 non-Josephus sets when n = 12, none for any other values of n ≤ 12. Are non-Josephus subsets rare for large n?
Yes, and well done if you find them.











2. Sums
Sums are everywhere in mathematics, so we need basic tools to handle them. This chapter develops the notation and general techniques that make summation user-friendly.

2.1 Notation
In Chapter 1 we encountered the sum of the first n integers, which we wrote out as 1 + 2 + 3 + · · · + (n - 1) + n. The ' · · · ' in such formulas tells us to complete the pattern established by the surrounding terms. Of course we have to watch out for sums like 1 + 7 + · · · + 41.7, which are meaningless without a mitigating context. On the other hand, the inclusion of terms like 3 and (n - 1) was a bit of overkill; the pattern would presumably have been clear if we had written simply 1 + 2 + · · · + n. Sometimes we might even be so bold as to write just 1 + · · · + n.
We'll be working with sums of the general form



where each ak is a number that has been defined somehow. This notation has the advantage that we can "see" the whole sum, almost as if it were written out in full, if we have a good enough imagination.
A term is how long this course lasts.
Each element ak of a sum is called a term. The terms are often specified implicitly as formulas that follow a readily perceived pattern, and in such cases we must sometimes write them in an expanded form so that the meaning is clear. For example, if
1 + 2 + · · · + 2n-1
is supposed to denote a sum of n terms, not of 2n-1, we should write it more explicitly as
20 + 21 + · · · + 2n-1.
The three-dots notation has many uses, but it can be ambiguous and a bit long-winded. Other alternatives are available, notably the delimited form



"Le signe  indique que l'on doit donner au nombre entier i toutes ses valeurs 1, 2, 3, . . . , et prendre la somme des termes."—J. Fourier [127]
which is called Sigma-notation because it uses the Greek letter Σ (upper-case sigma). This notation tells us to include in the sum precisely those terms ak whose index k is an integer that lies between the lower and upper limits 1 and n, inclusive. In words, we "sum over k, from 1 to n." Joseph Fourier introduced this delimited ∑-notation in 1820, and it soon took the mathematical world by storm.
Incidentally, the quantity after ∑ (here ak) is called the summand.
The index variable k is said to be bound to the ∑ sign in (2.2), because the k in ak is unrelated to appearances of k outside the Sigma-notation. Any other letter could be substituted for k here without changing the meaning of (2.2). The letter i is often used (perhaps because it stands for "index"), but we'll generally sum on k since it's wise to keep i for .
Well, I wouldn't want to use a or n as the index variable instead of k in (2.2); those letters are "free variables" that do have meaning outside the ∑ here.
It turns out that a generalized Sigma-notation is even more useful than the delimited form: We simply write one or more conditions under the ∑, to specify the set of indices over which summation should take place. For example, the sums in (2.1) and (2.2) can also be written as



In this particular example there isn't much difference between the new form and (2.2), but the general form allows us to take sums over index sets that aren't restricted to consecutive integers. For example, we can express the sum of the squares of all odd positive integers below 100 as follows:



The delimited equivalent of this sum,



is more cumbersome and less clear. Similarly, the sum of reciprocals of all prime numbers between 1 and N is



the delimited form would require us to write



where pk denotes the kth prime and π(N) is the number of primes ≤ N. (Incidentally, this sum gives the approximate average number of distinct prime factors of a random integer near N, since about 1/p of those integers are divisible by p. Its value for large N is approximately ln ln N + M, where M ≈ 0.2614972128476427837554268386086958590515666 is Mertens's constant [271]; ln x stands for the natural logarithm of x, and ln ln x stands for ln(ln x).)
The biggest advantage of general Sigma-notation is that we can manipulate it more easily than the delimited form. For example, suppose we want to change the index variable k to k + 1. With the general form, we have



The summation symbol looks like a distorted pacman.
it's easy to see what's going on, and we can do the substitution almost without thinking. But with the delimited form, we have



it's harder to see what's happened, and we're more likely to make a mistake.
On the other hand, the delimited form isn't completely useless. It's nice and tidy, and we can write it quickly because (2.2) has seven symbols compared with (2.3)'s eight. Therefore we'll often use ∑ with upper and lower delimiters when we state a problem or present a result, but we'll prefer to work with relations-under-∑ when we're manipulating a sum whose index variables need to be transformed.
A tidy sum.
The ∑ sign occurs more than 1000 times in this book, so we should be sure that we know exactly what it means. Formally, we write



That's nothing. You should see how many times Σ appears in The Iliad.
as an abbreviation for the sum of all terms ak such that k is an integer satisfying a given property P(k). (A "property P(k)" is any statement about k that can be either true or false.) For the time being, we'll assume that only finitely many integers k satisfying P(k) have ak ≠ 0; otherwise infinitely many nonzero numbers are being added together, and things can get a bit tricky. At the other extreme, if P(k) is false for all integers k, we have an "empty" sum; the value of an empty sum is defined to be zero.
A slightly modified form of (2.4) is used when a sum appears within the text of a paragraph rather than in a displayed equation: We write '∑P(k) ak', attaching property P(k) as a subscript of ∑, so that the formula won't stick out too much. Similarly, '' is a convenient alternative to (2.2) when we want to confine the notation to a single line.
People are often tempted to write



because the terms for k = 0, 1, and n in this sum are zero. Somehow it seems more efficient to add up n - 2 terms instead of n + 1 terms. But such temptations should be resisted; efficiency of computation is not the same as efficiency of understanding! We will find it advantageous to keep upper and lower bounds on an index of summation as simple as possible, because sums can be manipulated much more easily when the bounds are simple. Indeed, the form  can even be dangerously ambiguous, because its meaning is not at all clear when n = 0 or n = 1 (see exercise 1). Zero-valued terms cause no harm, and they often save a lot of trouble.
So far the notations we've been discussing are quite standard, but now we are about to make a radical departure from tradition. Kenneth E. Iverson introduced a wonderful idea in his programming language APL [191, page 11; see also 220], and we'll see that it greatly simplifies many of the things we want to do in this book. The idea is simply to enclose a true-or-false statement in brackets, and to say that the result is 1 if the statement is true, 0 if the statement is false. For example,



Iverson's convention allows us to express sums with no constraints whatever on the index of summation, because we can rewrite (2.4) in the form



Hey: The "Kronecker delta" that I've seen in other books (I mean δkn, which is 1 if k = n, 0 otherwise) is just a special case of Iverson's convention: We can write [ k = n ] instead.
If P(k) is false, the term ak[P(k)] is zero, so we can safely include it among the terms being summed. This makes it easy to manipulate the index of summation, because we don't have to fuss with boundary conditions.
"I am often surprised by new, important applications [of this notation]."—B. de Finetti [123]
A slight technicality needs to be mentioned: Sometimes ak isn't defined for all integers k. We get around this difficulty by assuming that [P(k)] is "very strongly zero" when P(k) is false; it's so much zero, it makes ak[P(k)] equal to zero even when ak is undefined. For example, if we use Iverson's convention to write the sum of reciprocal primes ≤ N as



there's no problem of division by zero when p = 0, because our convention tells us that [0 prime][0 ≤ N]/0 = 0.
Let's sum up what we've discussed so far about sums. There are two good ways to express a sum of terms: One way uses ' · · · ', the other uses '∑'. The three-dots form often suggests useful manipulations, particularly the combination of adjacent terms, since we might be able to spot a simplifying pattern if we let the whole sum hang out before our eyes. But too much detail can also be overwhelming. Sigma-notation is compact, impressive to family and friends, and often suggestive of manipulations that are not obvious in three-dots form. When we work with Sigma-notation, zero terms are not generally harmful; in fact, zeros often make ∑-manipulation easier.
. . . and it's less likely to lose points on an exam for "lack of rigor."


2.2 Sums and Recurrences
OK, we understand now how to express sums with fancy notation. But how does a person actually go about finding the value of a sum? One way is to observe that there's an intimate relation between sums and recurrences. The sum



is equivalent to the recurrence



(Think of Sn as not just a single number, but as a sequence defined for all n ≥ 0.)
Therefore we can evaluate sums in closed form by using the methods we learned in Chapter 1 to solve recurrences in closed form.
For example, if an is equal to a constant plus a multiple of n, the sum-recurrence (2.6) takes the following general form:



Proceeding as in Chapter 1, we find R1 = α + β + γ, R2 = α + 2β + 3γ, and so on; in general the solution can be written in the form



where A(n), B(n), and C(n) are the coefficients of dependence on the general parameters α, β, and γ.
The repertoire method tells us to try plugging in simple functions of n for Rn, hoping to find constant parameters α, β, and γ where the solution is especially simple. Setting Rn = 1 implies α = 1, β = 0, γ = 0; hence
A(n) = 1.
Setting Rn = n implies α = 0, β = 1, γ = 0; hence
B(n) = n.
Setting Rn = n2 implies α = 0, β = -1, γ = 2; hence
2C(n) - B(n) = n2
and we have C(n) = (n2 + n)/2. Easy as pie.
Actually easier; π = .
Therefore if we wish to evaluate



the sum-recurrence (2.6) boils down to (2.7) with α = β = a, γ = b, and the answer is aA(n) + aB(n) + bC(n) = a(n + 1) + b(n + 1)n/2.
Conversely, many recurrences can be reduced to sums; therefore the special methods for evaluating sums that we'll be learning later in this chapter will help us solve recurrences that might otherwise be difficult. The Tower of Hanoi recurrence is a case in point:
T0 = 0;
Tn = 2Tn-1 + 1,          for n > 0.
It can be put into the special form (2.6) if we divide both sides by 2n:
T0/20 = 0;
Tn/2n = Tn-1/2n -1 + 1/2n,          for n > 0.
Now we can set Sn = Tn/2n, and we have
S0 = 0;
Sn = Sn-1 + 2-n,          for n > 0.
It follows that



(Notice that we've left the term for k = 0 out of this sum.) The sum of the geometric series  will be derived later in this chapter; it turns out to be . Hence Tn = 2nSn = 2n - 1.
We have converted Tn to Sn in this derivation by noticing that the recurrence could be divided by 2n. This trick is a special case of a general technique that can reduce virtually any recurrence of the form



to a sum. The idea is to multiply both sides by a summation factor, sn:
snanTn = snbnTn-1 + sncn.
This factor sn is cleverly chosen to make
snbn = sn-1an-1.
Then if we write Sn = snanTn we have a sum-recurrence,
Sn = Sn-1 + sncn.
Hence



and the solution to the original recurrence (2.9) is



For example, when n = 1 we get T1 = (s1b1T0+s1c1)/s1a1 = (b1T0+c1)/a1.
(The value of s1 cancels out, so it can be anything but zero.)
But how can we be clever enough to find the right sn? No problem: The relation sn = sn-1an-1/bn can be unfolded to tell us that the fraction



or any convenient constant multiple of this value, will be a suitable summation factor. For example, the Tower of Hanoi recurrence has an = 1 and bn = 2; the general method we've just derived says that sn = 2-n is a good thing to multiply by, if we want to reduce the recurrence to a sum. We don't need a brilliant flash of inspiration to discover this multiplier.
We must be careful, as always, not to divide by zero. The summation-factor method works whenever all the a's and all the b's are nonzero.
Let's apply these ideas to a recurrence that arises in the study of "quicksort," one of the most important methods for sorting data inside a computer. The average number of comparison steps made by a typical implementation of quicksort when it is applied to n items in random order satisfies the recurrence



(Quicksort was invented by Hoare in 1962 [189].)
Hmmm. This looks much scarier than the recurrences we've seen before; it includes a sum over all previous values, and a division by n. Trying small cases gives us some data (C2 = 3, C3 = 6, ) but doesn't do anything to quell our fears.
We can, however, reduce the complexity of (2.12) systematically, by first getting rid of the division and then getting rid of the ∑ sign. The idea is to multiply both sides by n, obtaining the relation



hence, if we replace n by n - 1,



We can now subtract this equation from the first, and the ∑ sign disappears:
nCn - (n - 1)Cn-1 = 2n + 2Cn-1,          for n > 2.
Therefore the original recurrence for Cn reduces to a much simpler one:


C0 =
C1 = 0;        C2 = 3;
 


nCn =
(n + 1)Cn-1 + 2n,          for n > 2.


Progress. We're now in a position to apply a summation factor, since this recurrence has the form of (2.9) with an = n, bn = n + 1, and
cn = 2n - 2[n = 1] + 2[n = 2].
The general method described on the preceding page tells us to multiply the recurrence through by some multiple of



The solution, according to (2.10), is therefore



We started with a ∑ in the recurrence, and worked hard to get rid of it. But then after applying a summation factor, we came up with another ∑. Are sums good, or bad, or what?
The sum that remains is very similar to a quantity that arises frequently in applications. It arises so often, in fact, that we give it a special name and a special notation:



The letter H stands for "harmonic"; Hn is a harmonic number, so called because the kth harmonic produced by a violin string is the fundamental tone produced by a string that is 1/k times as long.
We can complete our study of the quicksort recurrence (2.12) by putting Cn into closed form; this will be possible if we can express Cn in terms of Hn. The sum in our formula for Cn is



We can relate this to Hn without much difficulty by changing k to k - 1 and revising the boundary conditions:



Alright! We have found the sum needed to complete the solution to (2.12): The average number of comparisons made by quicksort when it is applied to n randomly ordered items of data is



But your spelling is alwrong.
As usual, we check that small cases are correct: C2 = 3, C3 = 6.


2.3 Manipulation of Sums
The key to success with sums is an ability to change one ∑ into another that is simpler or closer to some goal. And it's easy to do this by learning a few basic rules of transformation and by practicing their use.
Not to be confused with finance.
Let K be any finite set of integers. Sums over the elements of K can be transformed by using three simple rules:









My other math books have different definitions for these laws.
The distributive law allows us to move constants in and out of a ∑. The associative law allows us to break a ∑ into two parts, or to combine two ∑'s into one. The commutative law says that we can reorder the terms in any way we please; here p(k) is any permutation of the set of all integers. For example, if K = {-1, 0, +1} and if p(k) = -k, these three laws tell us respectively that


ca-1 + ca0 + ca1 = c(a-1 + a0 + a1);
(distributive law)


(a-1 + b-1) + (a0 + b0) + (a1 + b1)
 


     = (a-1 + a0 + a1) + (b-1 + b0 + b1);
(associative law)


a-1 + a0 + a1 = a1 + a0 + a-1 .
(commutative law)


Why not call it permutative instead of commutative?
Gauss's trick in Chapter 1 can be viewed as an application of these three basic laws. Suppose we want to compute the general sum of an arithmetic progression,



By the commutative law we can replace k by n - k, obtaining



This is something like changing variables inside an integral, but easier.
These two equations can be added by using the associative law:



And we can now apply the distributive law and evaluate a trivial sum:



Dividing by 2, we have proved that



"What's one and one and one and one and one and one and one and one and one and one?" "I don't know," said Alice. "I lost count." "She can't do Addition."—Lewis Carroll [50]
The right-hand side can be remembered as the average of the first and last terms, namely ), times the number of terms, namely (n + 1).
It's important to bear in mind that the function p(k) in the general commutative law (2.17) is supposed to be a permutation of all the integers. In other words, for every integer n there should be exactly one integer k such that p(k) = n. Otherwise the commutative law might fail; exercise 3 illustrates this with a vengeance. Transformations like p(k) = k + c or p(k) = c - k, where c is an integer constant, are always permutations, so they always work.
On the other hand, we can relax the permutation restriction a little bit: We need to require only that there be exactly one integer k with p(k) = n when n is an element of the index set K. If n ∉ K (that is, if n is not in K), it doesn't matter how often p(k) = n occurs, because such k don't take part in the sum. Thus, for example, we can argue that



since there's exactly one k such that 2k = n when n  K and n is even.
Iverson's convention, which allows us to obtain the values 0 or 1 from logical statements in the middle of a formula, can be used together with the distributive, associative, and commutative laws to deduce additional properties of sums. For example, here is an important rule for combining different sets of indices: If K and K′ are any sets of integers, then



Additional, eh?
This follows from the general formulas



and



Typically we use rule (2.20) either to combine two almost-disjoint index sets, as in



or to split off a single term from a sum, as in



(The two sides of (2.20) have been switched here.)
This operation of splitting off a term is the basis of a perturbation method that often allows us to evaluate a sum in closed form. The idea is to start with an unknown sum and call it Sn:



(Name and conquer.) Then we rewrite Sn+1 in two ways, by splitting off both its last term and its first term:



Now we can work on this last sum and try to express it in terms of Sn. If we succeed, we obtain an equation whose solution is the sum we seek.
For example, let's use this approach to find the sum of a general geometric progression,



If it's geometric, there should be a geometric proof.



The general perturbation scheme in (2.24) tells us that



and the sum on the right is x∑0≤k≤n axk = xSn by the distributive law. Therefore Sn + axn+1 = a + xSn, and we can solve for Sn to obtain



(When x = 1, the sum is of course simply (n + 1)a.) The right-hand side can be remembered as the first term included in the sum minus the first term excluded (the term after the last), divided by 1 minus the term ratio.
Ah yes, this formula was drilled into me in high school.
That was almost too easy. Let's try the perturbation technique on a slightly more difficult sum,



In this case we have S0 = 0, S1 = 2, S2 = 10, S3 = 34, S4 = 98; what is the general formula? According to (2.24) we have



so we want to express the right-hand sum in terms of Sn. Well, we can break it into two sums with the help of the associative law,



and the first of the remaining sums is 2Sn. The other sum is a geometric progression, which equals (2 - 2n+2)/(1 - 2) = 2n+2 - 2 by (2.25). Therefore we have Sn + (n + 1)2n+1 = 2Sn + 2n+2 - 2, and algebra yields



Now we understand why S3 = 34: It's 32 + 2, not 2·17.
A similar derivation with x in place of 2 would have given us the equation Sn + (n + 1)xn+1 = xSn + (x - xn+2)/(1 - x); hence we can deduce that



It's interesting to note that we could have derived this closed form in a completely different way, by using elementary techniques of differential calculus. If we start with the equation



and take the derivative of both sides with respect to x, we get



because the derivative of a sum is the sum of the derivatives of its terms. We will see many more connections between calculus and discrete mathematics in later chapters.


2.4 Multiple Sums
The terms of a sum might be specified by two or more indices, not just by one. For example, here's a double sum of nine terms, governed by two indices j and k:



Oh no, a nine-term governor.
Notice that this doesn't mean to sum over all j ≥ 1 and all k ≤ 3.
We use the same notations and methods for such sums as we do for sums with a single index. Thus, if P(j, k) is a property of j and k, the sum of all terms aj,k such that P(j, k) is true can be written in two ways, one of which uses Iverson's convention and sums over all pairs of integers j and k:



Only one ∑ sign is needed, although there is more than one index of summation; ∑ denotes a sum over all combinations of indices that apply.
We also have occasion to use two ∑'s, when we're talking about a sum of sums. For example,



is an abbreviation for



which is the sum, over all integers j, of ∑k aj,k [P(j, k)], the latter being the sum over all integers k of all terms aj,k for which P(j, k) is true. In such cases we say that the double sum is "summed first on k." A sum that depends on more than one index can be summed first on any one of its indices.
Multiple ∑'s are evaluated right to left (inside-out).
In this regard we have a basic law called interchanging the order of summation, which generalizes the associative law (2.16) we saw earlier:



The middle term of this law is a sum over two indices. On the left, ∑j ∑k stands for summing first on k, then on j. On the right, ∑k ∑j stands for summing first on j, then on k. In practice when we want to evaluate a double sum in closed form, it's usually easier to sum it first on one index rather than on the other; we get to choose whichever is more convenient.
Who's panicking? I think this rule is fairly obvious compared to some of the stuff in Chapter 1.
Sums of sums are no reason to panic, but they can appear confusing to a beginner, so let's do some more examples. The nine-term sum we began with provides a good illustration of the manipulation of double sums, because that sum can actually be simplified, and the simplification process is typical of what we can do with ∑ ∑'s:



The first line here denotes a sum of nine terms in no particular order. The second line groups them in threes, (a1b1 + a1b2 + a1b3) + (a2b1 + a2b2 + a2b3) + (a3b1 + a3b2 + a3b3). The third line uses the distributive law to factor out the a's, since aj and [1 ≤ j ≤ 3] do not depend on k; this gives a1(b1 + b2 + b3) + a2(b1 + b2 + b3) + a3(b1 + b2 + b3). The fourth line is the same as the third, but with a redundant pair of parentheses thrown in so that the fifth line won't look so mysterious. The fifth line factors out the (b1 + b2 + b3) that occurs for each value of j: (a1 + a2 + a3)(b1 + b2 + b3). The last line is just another way to write the previous line. This method of derivation can be used to prove a general distributive law,



valid for all sets of indices J and K.
The basic law (2.27) for interchanging the order of summation has many variations, which arise when we want to restrict the ranges of the indices instead of summing over all integers j and k. These variations come in two flavors, vanilla and rocky road. First, the vanilla version:



This is just another way to write (2.27), since the Iversonian [j  J, k  K] factors into [j  J][k  K]. The vanilla-flavored law applies whenever the ranges of j and k are independent of each other.
The rocky-road formula for interchange is a little trickier. It applies when the range of an inner sum depends on the index variable of the outer sum:



Here the sets J, K(j), K′, and J′(k) must be related in such a way that
[j  J][k  K(j)] = [k  K′][j  J′(k)].
A factorization like this is always possible in principle: We can let J = K′ be the set of all integers and K(j), J′(k) be sets corresponding to the property P(j, k) that governs a double sum. But there are important special cases where the sets J, K(j), K′, and J′(k) have a simple form. Such cases arise frequently in applications. For example, here's a particularly useful factorization:



This Iversonian equation allows us to write



One of these two sums of sums is usually easier to evaluate than the other; we can use (2.32) to switch from the hard one to the easy one.
Let's apply these ideas to a useful example. Consider the array



(Now is a good time to do warmup exercises 4 and 6.)
(Or to check out the Snickers bar languishing in the freezer.)
of n2 products ajak. Our goal will be to find a simple formula for



the sum of all elements on or above the main diagonal of this array. Because ajak = akaj, the array is symmetrical about its main diagonal; therefore  will be approximately half the sum of all the elements (except for a fudge factor that takes account of the main diagonal).
Does rocky road have fudge in it?
Such considerations motivate the following manipulations. We have



because we can rename (j, k) as (k, j). Furthermore, since
[1 ≤ j ≤ k ≤ n] + [1 ≤ k ≤ j ≤ n] = [1 ≤ j, k ≤ n] + [1 ≤ j = k ≤ n] ,
we have



The first sum is , by the general distributive law (2.28). The second sum is . Therefore we have



an expression for the upper triangular sum in terms of simpler single sums.
Encouraged by such success, let's look at another double sum:



Again we have symmetry when j and k are interchanged:



So we can add S to itself, making use of the identity
[1 ≤ j < k ≤ n] + [1 ≤ k < j ≤ n] = [1 ≤ j, k ≤ n] - [1 ≤ j = k ≤ n]
to conclude that



The second sum here is zero; what about the first? It expands into four separate sums, each of which is vanilla flavored:



In the last step both sums have been simplified according to the general distributive law (2.28). If the manipulation of the first sum seems mysterious, here it is again in slow motion:



An index variable that doesn't appear in the summand (here j) can simply be eliminated if we multiply what's left by the size of that variable's index set (here n).
Returning to where we left off, we can now divide everything by 2 and rearrange things to obtain an interesting formula:



This identity yields Chebyshev's monotonic inequalities as a special case:



(Chebyshev [58] actually proved the analogous result for integrals instead of sums, , if f(x) and g(x) are monotone nondecreasing functions.)
(In general, if a1 ≤ · · · ≤ an and if p is a permutation of {1, . . . , n}, it's not difficult to prove that the largest value of  occurs when bp(1) ≤ · · · ≤ bp(n), and the smallest value occurs when bp(1) ≥ · · · ≥ bp(n).)
Multiple summation has an interesting connection with the general operation of changing the index of summation in single sums. We know by the commutative law that



if p(k) is any permutation of the integers. But what happens when we replace k by f(j), where f is an arbitrary function
f : J → K
that takes an integer j  J into an integer f(j)  K? The general formula for index replacement is



where #f-(k) stands for the number of elements in the set
f-(k) = {j | f(j) = k},
that is, the number of values of j  J such that f(j) equals k.
It's easy to prove (2.35) by interchanging the order of summation,



since ∑jJ[f(j) = k] = #f-(k). In the special case that f is a one-to-one correspondence between J and K, we have #f-(k) = 1 for all k, and the general formula (2.35) reduces to



My other math teacher calls this a "bijection"; maybe I'll learn to love that word some day.
And then again . . .
This is the commutative law (2.17) we had before, slightly disguised.
Our examples of multiple sums so far have all involved general terms like ak or bk. But this book is supposed to be concrete, so let's take a look at a multiple sum that involves actual numbers:



For example, S1 = 0; S2 = 1; .
Watch out—the authors seem to think that j, k, and n are "actual numbers."
The normal way to evaluate a double sum is to sum first on j or first on k, so let's explore both options.



Alas! We don't know how to get a sum of harmonic numbers into closed form.
Get out the whip.
If we try summing first the other way, we get



We're back at the same impasse.
But there's another way to proceed, if we replace k by k + j before deciding to reduce Sn to a sum of sums:



It's smart to say k ≤ n instead of k ≤ n - 1 here. Simple bounds save energy.
Aha! We've found Sn. Combining this with the false starts we made gives us a further identity as a bonus:



We can understand the trick that worked here in two ways, one algebraic and one geometric. (1) Algebraically, if we have a double sum whose terms involve k+f(j), where f is an arbitrary function, this example indicates that it's a good idea to try replacing k by k-f(j) and summing on j. (2) Geometrically, we can look at this particular sum Sn as follows, in the case n = 4:


 
k = 1 k = 2 k = 3 k = 4


j = 1
 +  +    


j = 2
 +    


j = 3
   


j = 4
 


Our first attempts, summing first on j (by columns) or on k (by rows), gave us H1 + H2 + H3 = H3 + H2 + H1. The winning idea was essentially to sum by diagonals, getting .


2.5 General Methods
Now let's consolidate what we've learned, by looking at a single example from several different angles. On the next few pages we're going to try to find a closed form for the sum of the first n squares, which we'll call :



We'll see that there are at least eight different ways to solve this problem, and in the process we'll learn useful strategies for attacking sums in general.
First, as usual, we look at some small cases.



No closed form for  is immediately evident; but when we do find one, we can use these values as a check.

Method 0: You could look it up.
A problem like the sum of the first n squares has probably been solved before, so we can most likely find the solution in a handy reference book. Sure enough, page 36 of the CRC Standard Mathematical Tables [28] has the answer:



Just to make sure we haven't misread it, we check that this formula correctly gives . Incidentally, page 36 of the CRC Tables has further information about the sums of cubes, . . . , tenth powers.
The definitive reference for mathematical formulas is the Handbook of Mathematical Functions, edited by Abramowitz and Stegun [2]. Pages 813-814 of that book list the values of  for n ≤ 100; and pages 804 and 809 exhibit formulas equivalent to (2.38), together with the analogous formulas for sums of cubes, . . . , fifteenth powers, with or without alternating signs.
(Harder sums can be found in Hansen's comprehensive table [178].)
But the best source for answers to questions about sequences is an amazing little book called the Handbook of Integer Sequences, by Sloane [330], which lists thousands of sequences by their numerical values. If you come up with a recurrence that you suspect has already been studied, all you have to do is compute enough terms to distinguish your recurrence from other famous ones; then chances are you'll find a pointer to the relevant literature in Sloane's Handbook. For example, 1, 5, 14, 30, . . . turns out to be Sloane's sequence number 1574, and it's called the sequence of "square pyramidal numbers" (because there are  balls in a pyramid that has a square base of n2 balls). Sloane gives three references, one of which is to the handbook of Abramowitz and Stegun that we've already mentioned.
Still another way to probe the world's store of accumulated mathematical wisdom is to use a computer program (such as Axiom, MACSYMA, Maple, or Mathematica) that provides tools for symbolic manipulation. Such programs are indispensable, especially for people who need to deal with large formulas.
It's good to be familiar with standard sources of information, because they can be extremely helpful. But Method 0 isn't really consistent with the spirit of this book, because we want to know how to figure out the answers by ourselves. The look-up method is limited to problems that other people have decided are worth considering; a new problem won't be there.
Or, at least to problems having the same answers as problems that other people have decided to consider.


Method 1: Guess the answer, prove it by induction.
Perhaps a little bird has told us the answer to a problem, or we have arrived at a closed form by some other less-than-rigorous means. Then we merely have to prove that it is correct.
We might, for example, have noticed that the values of  have rather small prime factors, so we may have come up with formula (2.38) as something that works for all small values of n. We might also have conjectured the equivalent formula



which is nicer because it's easier to remember. The preponderance of the evidence supports (2.39), but we must prove our conjectures beyond all reasonable doubt. Mathematical induction was invented for this purpose.
"Well, Your Honor, we know that , so the basis is easy. For the inductive step, suppose that n > 0, and assume that (2.39) holds when n is replaced by n - 1. Since
n = n-1 + n2,
we have


3n
= (n - 1)(n -  )(n) + 3n2


 
= (n3 -  n2 +  n) + 3n2


 
= (n3 +  n2 +  n)


 
= n(n +  )(n + 1) .


Therefore (2.39) indeed holds, beyond a reasonable doubt, for all n ≥ 0." Judge Wapner, in his infinite wisdom, agrees.
Induction has its place, and it is somewhat more defensible than trying to look up the answer. But it's still not really what we're seeking. All of the other sums we have evaluated so far in this chapter have been conquered without induction; we should likewise be able to determine a sum like  from scratch. Flashes of inspiration should not be necessary. We should be able to do sums even on our less creative days.


Method 2: Perturb the sum.
So let's go back to the perturbation method that worked so well for the geometric progression (2.25). We extract the first and last terms of +1 in order to get an equation for :



Oops—the 's cancel each other. Occasionally, despite our best efforts, the perturbation method produces something like , so we lose.
Seems more like a draw.
On the other hand, this derivation is not a total loss; it does reveal a way to sum the first few nonnegative integers in closed form,



even though we'd hoped to discover the sum of first integers squared. Could it be that if we start with the sum of the integers cubed, which we might call , we will get an expression for the integers squared? Let's try it.



Sure enough, the 's cancel, and we have enough information to determine  without relying on induction:


3n
= (n + 1)3 - 3(n + 1)n/2 - (n + 1)


 
= (n + 1)(n2 + 2n + 1 -  n - 1) = (n + 1)(n + )n.


Method 2′: Perturb your TA.


Method 3: Build a repertoire.
A slight generalization of the recurrence (2.7) will also suffice for summands involving n2. The solution to



will be of the general form



and we have already determined A(n), B(n), and C(n), because (2.40) is the same as (2.7) when δ = 0. If we now plug in Rn = n3, we find that n3 is the solution when α = 0, β = 1, γ = -3, δ = 3. Hence
3D(n) - 3C(n) + B(n) = n3;
this determines D(n).
We're interested in the sum , which equals ; thus we get  if we set α = β = γ = 0 and δ = 1 in (2.40) and (2.41). Consequently . We needn't do the algebra to compute D(n) from B(n) and C(n), since we already know what the answer will be; but doubters among us should be reassured to find that
3D(n) = n3 + 3C(n) - B(n) = n3 + 3 - n = n(n+ )(n+1) .


Method 4: Replace sums by integrals.
People who have been raised on calculus instead of discrete mathematics tend to be more familiar with ∫ than with ∑, so they find it natural to try changing ∑ to ∫. One of our goals in this book is to become so comfortable with ∑ that we'll think ∫ is more difficult than ∑ (at least for exact results). But still, it's a good idea to explore the relation between ∑ and ∫, since summation and integration are based on very similar ideas.
In calculus, an integral can be regarded as the area under a curve, and we can approximate this area by adding up the areas of long, skinny rectangles that touch the curve. We can also go the other way if a collection of long, skinny rectangles is given: Since  is the sum of the areas of rectangles whose sizes are 1 × 1, 1 × 4, . . . , 1 × n2, it is approximately equal to the area under the curve f(x) = x2 between 0 and n.



The horizontal scale here is ten times the vertical scale.
The area under this curve is ; therefore we know that  is approximately .
One way to use this fact is to examine the error in the approximation, . Since  satisfies the recurrence , we find that En satisfies the simpler recurrence


En = n -  n3 = n-1 + n2 -  n3
= En-1 +  (n-1)3 + n2 -  n3


 
= En-1 + n -  .


Another way to pursue the integral approach is to find a formula for En by summing the areas of the wedge-shaped error terms. We have



This is for people addicted to calculus.
Either way, we could find En and then .


Method 5: Expand and contract.
Yet another way to discover a closed form for  is to replace the original sum by a seemingly more complicated double sum that can actually be simplified if we massage it properly:



(The last step here is something like the last step of the perturbation method, because we get an equation with the unknown quantity on both sides.)
Going from a single sum to a double sum may appear at first to be a backward step, but it's actually progress, because it produces sums that are easier to work with. We can't expect to solve every problem by continually simplifying, simplifying, and simplifying: You can't scale the highest mountain peaks by climbing only uphill.


Method 6: Use finite calculus.


Method 7: Use generating functions.
Stay tuned for still more exciting calculations of , as we learn further techniques in the next section and in later chapters.


2.6 Finite and Infinite Calculus
We've learned a variety of ways to deal with sums directly. Now it's time to acquire a broader perspective, by looking at the problem of summation from a higher level. Mathematicians have developed a "finite calculus," analogous to the more traditional infinite calculus, by which it's possible to approach summation in a nice, systematic fashion.
Infinite calculus is based on the properties of the derivative operator D, defined by



Finite calculus is based on the properties of the difference operator Δ, defined by



This is the finite analog of the derivative in which we restrict ourselves to positive integer values of h. Thus, h = 1 is the closest we can get to the "limit" as h → 0, and Δf(x) is the value of (f(x + h) - f(x))/h when h = 1.
The symbols D and Δ are called operators because they operate on functions to give new functions; they are functions of functions that produce functions. If f is a suitably smooth function of real numbers to real numbers, then Df is also a function from reals to reals. And if f is any real-to-real function, so is Δf. The values of the functions Df and Δf at a point x are given by the definitions above.
As opposed to a cassette function.
Early on in calculus we learn how D operates on the powers f(x) = xm. In such cases Df(x) = mxm-1. We can write this informally with f omitted,
D(xm) = mxm-1.
It would be nice if the Δ operator would produce an equally elegant result; unfortunately it doesn't. We have, for example,
Δ(x3) = (x + 1)3 - x3 = 3x2 + 3x + 1.
Math power.
But there is a type of "mth power" that does transform nicely under Δ, and this is what makes finite calculus interesting. Such newfangled mth powers are defined by the rule



Notice the little straight line under the m; this implies that the m factors are supposed to go down and down, stepwise. There's also a corresponding definition where the factors go up and up:



When m = 0, we have , because a product of no factors is conventionally taken to be 1 (just as a sum of no terms is conventionally 0).
The quantity xm is called "x to the m falling," if we have to read it aloud; similarly,  is "x to the m rising." These functions are also called falling factorial powers and rising factorial powers, since they are closely related to the factorial function n! = n(n - 1) . . . (1). In fact, .
Several other notations for factorial powers appear in the mathematical literature, notably "Pochhammer's symbol" (x)m for  or xm; notations like x(m) or x(m) are also seen for xm. But the underline/overline convention is catching on, because it's easy to write, easy to remember, and free of redundant parentheses.
Falling powers xm are especially nice with respect to Δ. We have


Δ(xm)
= (x + 1)m - xm


 
= (x + 1)x... (x - m + 2) - x... (x - m + 2)(x - m + 1)


 
= mx(x - 1)...(x - m + 2),


Mathematical terminology is sometimes crazy: Pochhammer [293] actually used the notation (x)m for the binomial coefficient (), not for factorial powers.
hence the finite calculus has a handy law to match D(xm) = mxm-1:



This is the basic factorial fact.
The operator D of infinite calculus has an inverse, the anti-derivative (or integration) operator ∫. The Fundamental Theorem of Calculus relates D to ∫:
g(x) = Df(x)          if and only if          ∫ g(x) dx = f(x) + C.
"Quemadmodum ad differentiam denotandam usi sumus signo Δ, ita summam indicabimus signo Σ. . . . ex quo æquatio z = Δy, si invertatur, dabit quoque y = Σz + C."
—L. Euler [110]
Here ∫ g(x) dx, the indefinite integral of g(x), is the class of functions whose derivative is g(x). Analogously, Δ has as an inverse, the anti-difference (or summation) operator ∑; and there's another Fundamental Theorem:



Here ∑g(x) δx, the indefinite sum of g(x), is the class of functions whose difference is g(x). (Notice that the lowercase δ relates to uppercase Δ as d relates to D.) The "C" for indefinite integrals is an arbitrary constant; the "C" for indefinite sums is any function p(x) such that p(x + 1) = p(x). For example, C might be the periodic function a + b sin 2πx; such functions get washed out when we take differences, just as constants get washed out when we take derivatives. At integer values of x, the function C is constant.
Now we're almost ready for the punch line. Infinite calculus also has definite integrals: If g(x) = Df(x), then



Therefore finite calculus—ever mimicking its more famous cousin—has definite sums: If g(x) = Δf(x), then



This formula gives a meaning to the notation , just as the previous formula defines .
But what does  really mean, intuitively? We've defined it by analogy, not by necessity. We want the analogy to hold, so that we can easily remember the rules of finite calculus; but the notation will be useless if we don't understand its significance. Let's try to deduce its meaning by looking first at some special cases, assuming that g(x) = Δf(x) = f(x + 1) - f(x). If b = a, we have



Next, if b = a + 1, the result is



More generally, if b increases by 1, we have



These observations, and mathematical induction, allow us to deduce exactly what  means in general, when a and b are integers with b ≥ a:



In other words, the definite sum is the same as an ordinary sum with limits, but excluding the value at the upper limit.
You call this a punch line?
Let's try to recap this in a slightly different way. Suppose we've been given an unknown sum that's supposed to be evaluated in closed form, and suppose we can write it in the form . The theory of finite calculus tells us that we can express the answer as f(b) - f(a), if we can only find an indefinite sum or anti-difference function f such that g(x) = f(x + 1) - f(x). One way to understand this principle is to write ∑a≤k<b g(k) out in full, using the three-dots notation:



Everything on the right-hand side cancels, except f(b) - f(a); so f(b) - f(a) is the value of the sum. (Sums of the form ∑a≤k<b(f(k + 1) - f(k)) are often called telescoping, by analogy with a collapsed telescope, because the thickness of a collapsed telescope is determined solely by the outer radius of the outermost tube and the inner radius of the innermost tube.)
And all this time I thought it was telescoping because it collapsed from a very long expression to a very short one.
But rule (2.48) applies only when b ≥ a; what happens if b < a? Well, (2.47) says that we must have



This is analogous to the corresponding equation for definite integration. A similar argument proves , the summation analog of the identity . In full garb,



for all integers a, b, and c.
At this point a few of us are probably starting to wonder what all these parallels and analogies buy us. Well for one, definite summation gives us a simple way to compute sums of falling powers: The basic laws (2.45), (2.47), and (2.48) imply the general law
Others have been wondering this for some time now.



This formula is easy to remember because it's so much like the familiar .
In particular, when m = 1 we have k1 = k, so the principles of finite calculus give us an easy way to remember the fact that



The definite-sum method also gives us an inkling that sums over the range 0 ≤ k < n often turn out to be simpler than sums over 1 ≤ k ≤ n; the former are just f(n) - f(0), while the latter must be evaluated as f(n + 1) - f(1).
Ordinary powers can also be summed in this new way, if we first express them in terms of falling powers. For example,
k2 = k2 + k1,
hence



Replacing n by n + 1 gives us yet another way to compute the value of our old friend  in closed form.
With friends like this . . .
Gee, that was pretty easy. In fact, it was easier than any of the umpteen other ways that beat this formula to death in the previous section. So let's try to go up a notch, from squares to cubes: A simple calculation shows that
k3 = k3 + 3k2 + k1.
(It's always possible to convert between ordinary powers and factorial powers by using Stirling numbers, which we will study in Chapter 6.) Thus



Falling powers are therefore very nice for sums. But do they have any other redeeming features? Must we convert our old friendly ordinary powers to falling powers before summing, but then convert back before we can do anything else? Well, no, it's often possible to work directly with factorial powers, because they have additional properties. For example, just as we have (x + y)2 = x2 + 2xy + y2, it turns out that (x + y)2 = x2 + 2x1y1 + y2, and the same analogy holds between (x + y)m and (x + y)m. (This "factorial binomial theorem" is proved in exercise 5.37.)
So far we've considered only falling powers that have nonnegative exponents. To extend the analogies with ordinary powers to negative exponents, we need an appropriate definition of xm for m < 0. Looking at the sequence
x3 = x(x - 1)(x - 2),
x2 = x(x - 1),
x1 = x,
x0 = 1,
we notice that to get from x3 to x2 to x1 to x0 we divide by x - 2, then by x - 1, then by x. It seems reasonable (if not imperative) that we should divide by x + 1 next, to get from x0 to x-1, thereby making x-1= 1/(x+1). Continuing, the first few negative-exponent falling powers are



and our general definition for negative falling powers is



(It's also possible to define falling powers for real or even complex m, but we will defer that until Chapter 5.)
How can a complex number be even?
With this definition, falling powers have additional nice properties. Perhaps the most important is a general law of exponents, analogous to the law
xm+n = xmxn
for ordinary powers. The falling-power version is



For example, x2+3 = x2(x - 2)3; and with a negative n we have



If we had chosen to define x-1 as 1/x instead of as 1/(x + 1), the law of exponents (2.52) would have failed in cases like m = -1 and n = 1. In fact, we could have used (2.52) to tell us exactly how falling powers ought to be defined in the case of negative exponents, by setting m = -n. When an existing notation is being extended to cover more cases, it's always best to formulate definitions in such a way that general laws continue to hold.
Laws have their exponents and their detractors.
Now let's make sure that the crucial difference property holds for our newly defined falling powers. Does Δxm = mxm-1 when m < 0? If m = -2, for example, the difference is



Yes—it works! A similar argument applies for all m < 0.
Therefore the summation property (2.50) holds for negative falling powers as well as positive ones, as long as no division by zero occurs:



But what about when m = -1? Recall that for integration we use



when m = -1. We'd like to have a finite analog of ln x; in other words, we seek a function f(x) such that
x-1 =  = Δf(x) = f(x + 1) - f(x).
It's not too hard to see that
f(x) =  +  + ··· + 
is such a function, when x is an integer, and this quantity is just the harmonic number Hx of (2.13). Thus Hx is the discrete analog of the continuous ln x. (We will define Hx for noninteger x in Chapter 6, but integer values are good enough for present purposes. We'll also see in Chapter 9 that, for large x, the value of Hx - ln x is approximately 0.577 + 1/(2x). Hence Hx and ln x are not only analogous, their values usually differ by less than 1.)
0.577 exactly? Maybe they mean . Then again, maybe not.
We can now give a complete description of the sums of falling powers:



This formula indicates why harmonic numbers tend to pop up in the solutions to discrete problems like the analysis of quicksort, just as so-called natural logarithms arise naturally in the solutions to continuous problems.
Now that we've found an analog for ln x, let's see if there's one for ex. What function f(x) has the property that Δf(x) = f(x), corresponding to the identity Dex = ex? Easy:
f(x + 1) - f(x) = f(x)                    f(x + 1) = 2f(x);
so we're dealing with a simple recurrence, and we can take f(x) = 2x as the discrete exponential function.
The difference of cx is also quite simple, for arbitrary c, namely
Δ(cx) = cx+1 - cx = (c - 1)cx.
Hence the anti-difference of cx is cx/(c - 1), if c ≠ 1. This fact, together with the fundamental laws (2.47) and (2.48), gives us a tidy way to understand the general formula for the sum of a geometric progression:



Every time we encounter a function f that might be useful as a closed form, we can compute its difference Δf = g; then we have a function g whose indefinite sum ∑g(x) δx is known. Table 55 is the beginning of a table of difference/anti-difference pairs useful for summation.
'Table 55' is on page 55. Get it?
Despite all the parallels between continuous and discrete math, some continuous notions have no discrete analog. For example, the chain rule of infinite calculus is a handy rule for the derivative of a function of a function; but there's no corresponding chain rule of finite calculus, because there's no nice form for Δf(g(x)). Discrete change-of-variables is hard, except in certain cases like the replacement of x by c ± x.
However, Δ(f(x) g(x)) does have a fairly nice form, and it provides us with a rule for summation by parts, the finite analog of what infinite calculus calls integration by parts. Let's recall that the formula
D(uv) = u Dv + v Du
of infinite calculus leads to the rule for integration by parts,
∫ u Dv = uv - ∫ v Du ,


Table 55 What's the difference?



after integration and rearranging terms; we can do a similar thing in finite calculus.
We start by applying the difference operator to the product of two functions u(x) and v(x):



This formula can be put into a convenient form using the shift operator E, defined by
Ef(x) = f(x + 1).
Substituting Ev(x) for v(x+1) yields a compact rule for the difference of a product:



(The E is a bit of a nuisance, but it makes the equation correct.) Taking the indefinite sum on both sides of this equation, and rearranging its terms, yields the advertised rule for summation by parts:



Infinite calculus avoids E here by letting 1 → 0.
As with infinite calculus, limits can be placed on all three terms, making the indefinite sums definite.
I guess ex = 2x, for small values of 1.
This rule is useful when the sum on the left is harder to evaluate than the one on the right. Let's look at an example. The function ∫ xex dx is typically integrated by parts; its discrete analog is ∑x2x δx, which we encountered earlier this chapter in the form . To sum this by parts, we let u(x) = x and Δv(x) = 2x; hence Δu(x) = 1, v(x) = 2x, and Ev(x) = 2x+1. Plugging into (2.56) gives
∑x2x δx = x2x - ∑2x+1 δx = x2x - 2x+1 + C.
And we can use this to evaluate the sum we did before, by attaching limits:



It's easier to find the sum this way than to use the perturbation method, because we don't have to think.
The ultimate goal of mathematics is to eliminate all need for intelligent thought.
We stumbled across a formula for ∑0≤k<n Hk earlier in this chapter, and counted ourselves lucky. But we could have found our formula (2.36) systematically, if we had known about summation by parts. Let's demonstrate this assertion by tackling a sum that looks even harder, ∑0≤k<n kHk. The solution is not difficult if we are guided by analogy with ∫ x ln x dx: We take u(x) = Hx and Δv(x) = x = x1, hence Δu(x) = x-1, v(x) = x2/2, Ev(x) = (x + 1)2/2, and we have



(In going from the first line to the second, we've combined two falling powers (x+1)2 x-1 by using the law of exponents (2.52) with m = -1 and n = 2.) Now we can attach limits and conclude that





2.7 Infinite Sums
When we defined ∑-notation at the beginning of this chapter, we finessed the question of infinite sums by saying, in essence, "Wait until later. For now, we can assume that all the sums we meet have only finitely many nonzero terms." But the time of reckoning has finally arrived; we must face the fact that sums can be infinite. And the truth is that infinite sums are bearers of both good news and bad news.
This is finesse?
First, the bad news: It turns out that the methods we've used for manipulating ∑'s are not always valid when infinite sums are involved. But next, the good news: There is a large, easily understood class of infinite sums for which all the operations we've been performing are perfectly legitimate. The reasons underlying both these news items will be clear after we have looked more closely at the underlying meaning of summation.
Everybody knows what a finite sum is: We add up a bunch of terms, one by one, until they've all been added. But an infinite sum needs to be defined more carefully, lest we get into paradoxical situations.
For example, it seems natural to define things so that the infinite sum
S = 1 +  +  +  +  +  + · · ·
is equal to 2, because if we double it we get
2S = 2 + 1 +  +  +  +  + · · · = 2 + S .
On the other hand, this same reasoning suggests that we ought to define
T = 1 + 2 + 4 + 8 + 16 + 32 + · · ·
to be -1, for if we double it we get
2T = 2 + 4 + 8 + 16 + 32 + 64 + · · · = T - 1.
Something funny is going on; how can we get a negative number by summing positive quantities? It seems better to leave T undefined; or perhaps we should say that T = ∞, since the terms being added in T become larger than any fixed, finite number. (Notice that ∞ is another "solution" to the equation 2T = T - 1; it also "solves" the equation 2S = 2 + S.)
Sure: 1 + 2 + 4 + 8 + · · · is the "infinite precision" representation of the number -1, in a binary computer with infinite word size.
Let's try to formulate a good definition for the value of a general sum ∑kK ak, where K might be infinite. For starters, let's assume that all the terms ak are nonnegative. Then a suitable definition is not hard to find: If there's a bounding constant A such that



for all finite subsets F ⊂ K, then we define ∑kK ak to be the least such A. (It follows from well-known properties of the real numbers that the set of all such A always contains a smallest element.) But if there's no bounding constant A, we say that ∑kK ak = ∞; this means that if A is any real number, there's a set of finitely many terms ak whose sum exceeds A.
The definition in the previous paragraph has been formulated carefully so that it doesn't depend on any order that might exist in the index set K. Therefore the arguments we are about to make will apply to multiple sums with many indices k1, k2, . . . , not just to sums over the set of integers.
In the special case that K is the set of nonnegative integers, our definition for nonnegative terms ak implies that



The set K might even be uncountable. But only a countable number of terms can be nonzero, if a bounding constant A exists, because at most nA terms are ≥ 1/n.
Here's why: Any nondecreasing sequence of real numbers has a limit (possibly ∞). If the limit is A, and if F is any finite set of nonnegative integers whose elements are all ≤ n, we have ; hence A = ∞ or A is a bounding constant. And if A′ is any number less than the stated limit A, then there's an n such that ; hence the finite set F = {0, 1, . . . , n} witnesses to the fact that A′ is not a bounding constant.
We can now easily compute the value of certain infinite sums, according to the definition just given. For example, if ak = xk, we have



In particular, the infinite sums S and T considered a minute ago have the respective values 2 and ∞, just as we suspected. Another interesting example is



Now let's consider the case that the sum might have negative terms as well as nonnegative ones. What, for example, should be the value of



If we group the terms in pairs, we get
(1 - 1) + (1 - 1) + (1 - 1) + · · · = 0 + 0 + 0 + · · · ,
so the sum comes out zero; but if we start the pairing one step later, we get
1 - (1 - 1) - (1 - 1) - (1 - 1) - · · · = 1 - 0 - 0 - 0 - · · ·;
the sum is 1.
"Aggregatum quantitatum a - a + a - a + a - a etc. nunc est = a, nunc = 0, adeoque continuata in infinitum serie ponendus = a/2, fateor acumen et veritatem animadversionis tuæ."
—G. Grandi [163]
We might also try setting x = -1 in the formula ∑k≥0 xk = 1/(1 - x), since we've proved that this formula holds when 0 ≤ x < 1; but then we are forced to conclude that the infinite sum is , although it's a sum of integers!
Another interesting example is the doubly infinite ∑kak where ak = 1/(k + 1) for k ≥ 0 and ak = 1/(k - 1) for k < 0. We can write this as



If we evaluate this sum by starting at the "center" element and working outward,



we get the value 1; and we obtain the same value 1 if we shift all the parentheses one step to the left,



because the sum of all numbers inside the innermost n parentheses is



A similar argument shows that the value is 1 if these parentheses are shifted any fixed amount to the left or right; this encourages us to believe that the sum is indeed 1. On the other hand, if we group terms in the following way,



the nth pair of parentheses from inside out contains the numbers



We'll prove in Chapter 9 that limn→∞(H2n - Hn+1) = ln 2; hence this grouping suggests that the doubly infinite sum should really be equal to 1 + ln 2.
There's something flaky about a sum that gives different values when its terms are added up in different ways. Advanced texts on analysis have a variety of definitions by which meaningful values can be assigned to such pathological sums; but if we adopt those definitions, we cannot operate with ∑-notation as freely as we have been doing. We don't need the delicate refinements of "conditional convergence" for the purposes of this book; therefore we'll stick to a definition of infinite sums that preserves the validity of all the operations we've been doing in this chapter.
Is this the first page with no graffiti?
In fact, our definition of infinite sums is quite simple. Let K be any set, and let ak be a real-valued term defined for each k  K. (Here 'k' might actually stand for several indices k1, k2, . . . , and K might therefore be multidimensional.) Any real number x can be written as the difference of its positive and negative parts,
x = x+ - x-,          where x+ = x·[x > 0] and x- = -x·[x < 0].
(Either x+ = 0 or x- = 0, or both.) We've already explained how to define values for the infinite sums  and , because  and  are nonnegative. Therefore our general definition is



unless the right-hand sums are both equal to ∞. In the latter case, we leave ∑kK ak undefined.
In other words, absolute convergence means that the sum of absolute values converges.
Let  and . If A+ and A- are both finite, the sum ∑kK ak is said to converge absolutely to the value A = A+ - A-. If A+ = ∞ but A- is finite, the sum ∑kK ak is said to diverge to + ∞. Similarly, if A- = ∞ but A+ is finite, ∑kK ak is said to diverge to -∞. If A+ = A- = ∞, all bets are off.
We started with a definition that worked for nonnegative terms, then we extended it to real-valued terms. If the terms ak are complex numbers, we can extend the definition once again, in the obvious way: The sum ∑kK ak is defined to be ∑kK ℜak + i ∑kK ℑak, where ℜak and ℑak are the real and imaginary parts of ak—provided that both of those sums are defined. Otherwise ∑kK ak is undefined. (See exercise 18.)
The bad news, as stated earlier, is that some infinite sums must be left undefined, because the manipulations we've been doing can produce inconsistencies in all such cases. (See exercise 34.) The good news is that all of the manipulations of this chapter are perfectly valid whenever we're dealing with sums that converge absolutely, as just defined.
We can verify the good news by showing that each of our transformation rules preserves the value of all absolutely convergent sums. This means, more explicitly, that we must prove the distributive, associative, and commutative laws, plus the rule for summing first on one index variable; everything else we've done has been derived from those four basic operations on sums.
The distributive law (2.15) can be formulated more precisely as follows: If ∑kK ak converges absolutely to A and if c is any complex number, then ∑kK cak converges absolutely to cA. We can prove this by breaking the sum into real and imaginary, positive and negative parts as above, and by proving the special case in which c > 0 and each term ak is nonnegative. The proof in this special case works because ∑kF cak = c ∑kF ak for all finite sets F; the latter fact follows by induction on the size of F.
The associative law (2.16) can be stated as follows: If ∑kK ak and ∑kK bk converge absolutely to A and B, respectively, then ∑kK (ak + bk) converges absolutely to A + B. This turns out to be a special case of a more general theorem that we will prove shortly.
The commutative law (2.17) doesn't really need to be proved, because we have shown in the discussion following (2.35) how to derive it as a special case of a general rule for interchanging the order of summation.
The main result we need to prove is the fundamental principle of multiple sums: Absolutely convergent sums over two or more indices can always be summed first with respect to any one of those indices. Formally, we shall prove that if J and the elements of {Kj | j  J} are any sets of indices such that



Best to skim this page the first time you get here.—Your friendly TA
then there exist complex numbers Aj for each j  J such that



It suffices to prove this assertion when all terms are nonnegative, because we can prove the general case by breaking everything into real and imaginary, positive and negative parts as before. Let's assume therefore that aj,k ≥ 0 for all pairs (j, k)  M, where M is the master index set {(j, k) | j  J, k  Kj}.
We are given that ∑(j,k)M aj,k is finite, namely that



for all finite subsets F ⊆ M, and that A is the least such upper bound. If j is any element of J, each sum of the form ∑kFj aj,k where Fj is a finite subset of Kj is bounded above by A. Hence these finite sums have a least upper bound Aj ≥ 0, and ∑kKj aj,k = Aj by definition.
We still need to prove that A is the least upper bound of ∑jG Aj, for all finite subsets G ⊆ J. Suppose that G is a finite subset of J with ∑jG Aj = A′ > A. We can find finite subsets Fj ⊆ Kj such that ∑kFj aj,k > (A/A′)Aj for each j  G with Aj > 0. There is at least one such j. But then ∑jG,kFj aj,k > (A/A′) ∑jG Aj = A, contradicting the fact that we have ∑(j,k)F aj,k ≤ A for all finite subsets F ⊆ M. Hence ∑jG Aj ≤ A, for all finite subsets G ⊆ J.
Finally, let A′ be any real number less than A. Our proof will be complete if we can find a finite set G ⊆ J such that ∑jG Aj > A′. We know that there's a finite set F ⊆ M such that ∑(j,k)F aj,k > A′; let G be the set of j's in this F, and let Fj = {k | (j, k)  F}. Then ∑jG Aj ≥ ∑jG ∑kFj aj,k = ∑(j,k)F aj,k > A′; QED.
OK, we're now legitimate! Everything we've been doing with infinite sums is justified, as long as there's a finite bound on all finite sums of the absolute values of the terms. Since the doubly infinite sum (2.58) gave us two different answers when we evaluated it in two different ways, its positive terms  must diverge to ∞; otherwise we would have gotten the same answer no matter how we grouped the terms.
So why have I been hearing a lot lately about "harmonic convergence"?



Exercises

Warmups
1. What does the notation



mean?
2. Simplify the expression x · ([x > 0] - [x < 0]).
3. Demonstrate your understanding of ∑-notation by writing out the sums



in full. (Watch out—the second sum is a bit tricky.)
4. Express the triple sum



as a three-fold summation (with three ∑'s),
a. summing first on k, then j, then i;
b. summing first on i, then j, then k.
Also write your triple sums out in full without the ∑-notation, using parentheses to show what is being added together first.
5. What's wrong with the following derivation?



6. What is the value of ∑k[1 ≤ j ≤ k ≤ n], as a function of j and n?
7. Let ∇f(x) = f(x) - f(x-1). What is ∇(x)?
Yield to the rising power.
8. What is the value of 0m, when m is a given integer?
9. What is the law of exponents for rising factorial powers, analogous to (2.52)? Use this to define .
10. The text derives the following formula for the difference of a product:
Δ(uv) = u Δv + Ev Δu.
How can this formula be correct, when the left-hand side is symmetric with respect to u and v but the right-hand side is not?


Basics
11. The general rule (2.56) for summation by parts is equivalent to



Prove this formula directly by using the distributive, associative, and commutative laws.
12. Show that the function p(k) = k + (-1)kc is a permutation of the set of all integers, whenever c is an integer.
13. Use the repertoire method to find a closed form for .
14. Evaluate  by rewriting it as the multiple sum ∑1≤j≤k≤n 2k.
15. Evaluate  by the text's Method 5 as follows: First write ; then apply (2.33).
16. Prove that xm/(x - n)m = xn/(x - m)n, unless one of the denominators is zero.
17. Show that the following formulas can be used to convert between rising and falling factorial powers, for all integers m:



(The answer to exercise 9 defines .)
18. Let ℜz and ℑz be the real and imaginary parts of the complex number z. The absolute value |z| is . A sum ∑kK ak of complex terms ak is said to converge absolutely when the real-valued sums ∑kK ℜak and ∑kK ℑak both converge absolutely. Prove that ∑kK ak converges absolutely if and only if there is a bounding constant B such that ∑kF |ak| ≤ B for all finite subsets F ⊆ K.


Homework exercises
19. Use a summation factor to solve the recurrence


T0 =
5;


2Tn =
nTn-1 + 3 · n!,          for n > 0.


20. Try to evaluate  by the perturbation method, but deduce the value of  instead.
21. Evaluate the sums ,  and  by the perturbation method, assuming that n ≥ 0.
22. Prove Lagrange's identity (without using induction):



It's hard to prove the identity of somebody who's been dead for 175 years.
Prove, in fact, an identity for the more general double sum



23. Evaluate the sum  in two ways:
a. Replace 1/k(k + 1) by the "partial fractions" 1/k - 1/(k + 1).
b. Sum by parts.
24. What is ∑0≤k<n Hk/(k + 1)(k + 2)? Hint: Generalize the derivation of (2.57).
25. The notation ΠkK ak means the product of the numbers ak for all k  K. Assume for simplicity that ak ≠ 1 for only finitely many k; hence infinite products need not be defined. What laws does this Π-notation satisfy, analogous to the distributive, associative, and commutative laws that hold for ∑?
This notation was introduced by Jacobi in 1829 [192].
26. Express the double product P = Π1≤j≤k≤n ajak in terms of the single product  by manipulating Π-notation. (This exercise gives us a product analog of the upper-triangle identity (2.33).)
27. Compute Δ(cx), and use it to deduce the value of .
28. At what point does the following derivation go astray?





Exam problems
29. Evaluate the sum .
30. Cribbage players have long been aware that 15 = 7 + 8 = 4 + 5 + 6 = 1 + 2 + 3 + 4 + 5. Find the number of ways to represent 1050 as a sum of consecutive positive integers. (The trivial representation '1050' by itself counts as one way; thus there are four, not three, ways to represent 15 as a sum of consecutive positive integers. Incidentally, a knowledge of cribbage rules is of no use in this problem.)
31. Riemann's zeta function ζ(k) is defined to be the infinite sum



Prove that ∑k≥2 (ζ(k) - 1) = 1. What is the value of ∑k≥1 (ζ(2k) - 1)?
32. Let . Prove that



for all real x ≥ 0, and evaluate the sums in closed form.


Bonus problems
33. Let ∧kK ak denote the minimum of the numbers ak (or their greatest lower bound, if K is infinite), assuming that each ak is either real or ±∞. What laws are valid for ∧-notation, analogous to those that work for ∑ and Π? (See exercise 25.)
The laws of the jungle.
34. Prove that if the sum ∑kK ak is undefined according to (2.59), then it is extremely flaky in the following sense: If A- and A+ are any given real numbers, it's possible to find a sequence of finite subsets F1 ⊂ F2 ⊂ F3 ⊂ · · · of K such that



35. Prove Goldbach's theorem



where P is the set of "perfect powers" defined recursively as follows:
P = { mn | m ≥ 2, n ≥ 2, m ∉ P }.
Perfect power corrupts perfectly.
36. Solomon Golomb's "self-describing sequence" f(1), f(2), f(3), . . .  is the only nondecreasing sequence of positive integers with the property that it contains exactly f(k) occurrences of k for each k. A few moments' thought reveals that the sequence must begin as follows:



Let g(n) be the largest integer m such that f(m) = n. Show that
a. 
b. 
c. 


Research problem
37. Will all the 1/k by 1/(k + 1) rectangles, for k ≥ 1, fit together inside a 1 by 1 square? (Recall that their areas sum to 1.)














3. Integer Functions
Whole numbers constitute the backbone of discrete mathematics, and we often need to convert from fractions or arbitrary real numbers to integers. Our goal in this chapter is to gain familiarity and fluency with such conversions and to learn some of their remarkable properties.

3.1 Floors and Ceilings
We start by covering the floor (greatest integer) and ceiling (least integer) functions, which are defined for all real x as follows:



Kenneth E. Iverson introduced this notation, as well as the names "floor" and "ceiling," early in the 1960s [191, page 12]. He found that typesetters could handle the symbols by shaving the tops and bottoms off of '[' and ']'. His notation has become sufficiently popular that floor and ceiling brackets can now be used in a technical paper without an explanation of what they mean. Until recently, people had most often been writing '[x]' for the greatest integer ≤ x, without a good equivalent for the least integer function. Some authors had even tried to use ']x['—with a predictable lack of success.
)Ouch.(
Besides variations in notation, there are variations in the functions themselves. For example, some pocket calculators have an INT function, defined as x when x is positive and x when x is negative. The designers of these calculators probably wanted their INT function to satisfy the identity INT(-x) = -INT(x). But we'll stick to our floor and ceiling functions, because they have even nicer properties than this.
One good way to become familiar with the floor and ceiling functions is to understand their graphs, which form staircase-like patterns above and below the line f(x) = x:



We see from the graph that, for example,
e = 2,       -e = -3,
e = 3,       -e = -2,
since e = 2.71828 . . . .
By staring at this illustration we can observe several facts about floors and ceilings. First, since the floor function lies on or below the diagonal line f(x) = x, we have x ≤ x; similarly x ≥ x. (This, of course, is quite obvious from the definition.) The two functions are equal precisely at the integer points:
x = x                    x is an integer                    x = x .
(We use the notation '⇔' to mean "if and only if.") Furthermore, when they differ the ceiling is exactly 1 higher than the floor:



Cute.By Iverson's bracket convention, this is a complete equation.
If we shift the diagonal line down one unit, it lies completely below the floor function, so x - 1 < x; similarly x + 1 > x. Combining these observations gives us



Finally, the functions are reflections of each other about both axes:



Thus each is easily expressible in terms of the other. This fact helps to explain why the ceiling function once had no notation of its own. But we see ceilings often enough to warrant giving them special symbols, just as we have adopted special notations for rising powers as well as falling powers. Mathematicians have long had both sine and cosine, tangent and cotangent, secant and cosecant, max and min; now we also have both floor and ceiling.
Next week we're getting walls.
To actually prove properties about the floor and ceiling functions, rather than just to observe such facts graphically, the following four rules are especially useful:



(We assume in all four cases that n is an integer and that x is real.) Rules (a) and (c) are immediate consequences of definition (3.1); rules (b) and (d) are the same but with the inequalities rearranged so that n is in the middle.
It's possible to move an integer term in or out of a floor (or ceiling):



(Because rule (3.5(a)) says that this assertion is equivalent to the inequalities x + n ≤ x + n < x + n + 1.) But similar operations, like moving out a constant factor, cannot be done in general. For example, we have nx ≠ nx when n = 2 and x = 1/2. This means that floor and ceiling brackets are comparatively inflexible. We are usually happy if we can get rid of them or if we can prove anything at all when they are present.
It turns out that there are many situations in which floor and ceiling brackets are redundant, so that we can insert or delete them at will. For example, any inequality between a real and an integer is equivalent to a floor or ceiling inequality between integers:



These rules are easily proved. For example, if x < n then surely x < n, since x ≤ x. Conversely, if x < n then we must have x < n, since x < x + 1 and x + 1 ≤ n.
It would be nice if the four rules in (3.7) were as easy to remember as they are to prove. Each inequality without floor or ceiling corresponds to the same inequality with floor or with ceiling; but we need to think twice before deciding which of the two is appropriate.
The difference between x and x is called the fractional part of x, and it arises often enough in applications to deserve its own notation:



Hmmm. We'd better not write {x} for the fractional part when it could be confused with the set containing x as its only element.
We sometimes call x the integer part of x, since x = x + {x}. If a real number x can be written in the form x = n + θ, where n is an integer and 0 ≤ θ < 1, we can conclude by (3.5(a)) that n = x and θ = {x}.
Identity (3.6) doesn't hold if n is an arbitrary real. But we can deduce that there are only two possibilities for x + y in general: If we write x = x + {x} and y = y + {y}, then we have x + y = x + y + {x} + {y}. And since 0 ≤ {x} + {y} < 2, we find that sometimes x + y is x + y, otherwise it's x + y + 1.
The second case occurs if and only if there's a "carry" at the position of the decimal point, when the fractional parts {x} and {y} are added together.


3.2 Floor/Ceiling Applications
We've now seen the basic tools for handling floors and ceilings. Let's put them to use, starting with an easy problem: What's lg 35? (Following a convention that many authors have proposed independently, we use 'lg' to denote the base-2 logarithm.) Well, since 25 < 35 ≤ 26, we can take logs to get 5 < lg 35 ≤ 6; so relation (3.5(c)) tells us that lg 35 = 6.
Note that the number 35 is six bits long when written in radix 2 notation: 35 = (100011)2. Is it always true that lg n is the length of n written in binary? Not quite. We also need six bits to write 32 = (100000)2. So lg n is the wrong answer to the problem. (It fails only when n is a power of 2, but that's infinitely many failures.) We can find a correct answer by realizing that it takes m bits to write each number n such that 2m-1 ≤ n < 2m; thus (3.5(a)) tells us that m - 1 = lg n, so m = lg n + 1. That is, we need lg n + 1 bits to express n in binary, for all n > 0. Alternatively, a similar derivation yields the answer lg(n + 1); this formula holds for n = 0 as well, if we're willing to say that it takes zero bits to write n = 0 in binary.
Let's look next at expressions with several floors or ceilings. What is x? Easy—since x is an integer, x is just x. So is any other expression with an innermost x surrounded by any number of floors or ceilings.
Here's a tougher problem: Prove or disprove the assertion



(Of course π, e, and ϕ are the obvious first real numbers to try, aren't they?)
Equality obviously holds when x is an integer, because x = x. And there's equality in the special cases π = 3.14159 . . . , e = 2.71828 . . . , and , because we get 1 = 1. Our failure to find a counterexample suggests that equality holds in general, so let's try to prove it.
Incidentally, when we're faced with a "prove or disprove," we're usually better off trying first to disprove with a counterexample, for two reasons: A disproof is potentially easier (we need just one counterexample); and nit-picking arouses our creative juices. Even if the given assertion is true, our search for a counterexample often leads us to a proof, as soon as we see why a counterexample is impossible. Besides, it's healthy to be skeptical.
Skepticism is healthy only to a limited extent. Being skeptical about proofs and programs (particularly your own) will probably keep your grades healthy and your job fairly secure. But applying that much skepticism will probably also keep you shut away working all the time, instead of letting you get out for exercise and relaxation.
Too much skepticism is an open invitation to the state of rigor mortis, where you become so worried about being correct and rigorous that you never get anything finished.
—A skeptic
If we try to prove that  with the help of calculus, we might start by decomposing x into its integer and fractional parts x + {x} = n + θ and then expanding the square root using the binomial theorem: (n+θ)1/2 = n1/2 + n-1/2θ/2 - n-3/2θ2/8 + · · · . But this approach gets pretty messy.
It's much easier to use the tools we've developed. Here's a possible strategy: Somehow strip off the outer floor and square root of , then remove the inner floor, then add back the outer stuff to get . OK. We let  and invoke (3.5(a)), giving . That removes the outer floor bracket without losing any information. Squaring, since all three expressions are nonnegative, we have m2 ≤ x < (m + 1)2. That gets rid of the square root. Next we remove the floor, using (3.7(d)) for the left inequality and (3.7(a)) for the right: m2 ≤ x < (m + 1)2. It's now a simple matter to retrace our steps, taking square roots to get  and invoking (3.5(a)) to get . Thus ; the assertion is true. Similarly, we can prove that



The proof we just found doesn't rely heavily on the properties of square roots. A closer look shows that we can generalize the ideas and prove much more: Let f(x) be any continuous, monotonically increasing function on an interval of the real numbers, with the property that
f(x) = integer                    x = integer.
(The symbol '⇒' means "implies.") Then we have



(This observation was made by R. J. McEliece when he was an undergrad.)
whenever f(x), f(x), and f(x) are defined. Let's prove this general property for ceilings, since we did floors earlier and since the proof for floors is almost the same. If x = x, there's nothing to prove. Otherwise x < x, and f(x) < f(x) since f is increasing. Hence f(x) ≤ f(x), since   is nondecreasing. If f(x) < f(x), there must be a number y such that x ≤ y < x and f(y) = f(x), since f is continuous. This y is an integer, because of f's special property. But there cannot be an integer strictly between x and x. This contradiction implies that we must have f(x) = f(x).
An important special case of this theorem is worth noting explicitly:



if m and n are integers and the denominator n is positive. For example, let m = 0; we have x/10/10/10 = x/1000. Dividing thrice by 10 and throwing off digits is the same as dividing by 1000 and tossing the remainder.
Let's try now to prove or disprove another statement:



This works when x = π and x = e, but it fails when x = ϕ; so we know that it isn't true in general.
Before going any further, let's digress a minute to discuss different levels of problems that might appear in books about mathematics:
Level 1. Given an explicit object x and an explicit property P(x), prove that P(x) is true. For example, "Prove that π = 3." Here the problem involves finding a proof of some purported fact.
Level 2. Given an explicit set X and an explicit property P(x), prove that P(x) is true for all x  X. For example, "Prove that x ≤ x for all real x." Again the problem involves finding a proof, but the proof this time must be general. We're doing algebra, not just arithmetic.
Level 3. Given an explicit set X and an explicit property P(x), prove or disprove that P(x) is true for all x  X. For example, "Prove or disprove that  for all real x ≥ 0." Here there's an additional level of uncertainty; the outcome might go either way. This is closer to the real situation a mathematician constantly faces: Assertions that get into books tend to be true, but new things have to be looked at with a jaundiced eye. If the statement is false, our job is to find a counterexample. If the statement is true, we must find a proof as in level 2.
In my other texts "prove or disprove" seems to mean the same as "prove," about 99.44% of the time; but not in this book.
Level 4. Given an explicit set X and an explicit property P(x), find a necessary and sufficient condition Q(x) that P(x) is true. For example, "Find a necessary and sufficient condition that x ≥ x." The problem is to find Q such that P(x) ⇔ Q(x). Of course, there's always a trivial answer; we can take Q(x) = P(x). But the implied requirement is to find a condition that's as simple as possible. Creativity is required to discover a simple condition that will work. (For example, in this case, "x ≥ x ⇔ x is an integer.") The extra element of discovery needed to find Q(x) makes this sort of problem more difficult, but it's more typical of what mathematicians must do in the "real world." Finally, of course, a proof must be given that P(x) is true if and only if Q(x) is true.
But no simpler.
—A. Einstein
Level 5. Given an explicit set X, find an interesting property P(x) of its elements. Now we're in the scary domain of pure research, where students might think that total chaos reigns. This is real mathematics. Authors of textbooks rarely dare to pose level 5 problems.
End of digression. But let's convert the last question we looked at from level 3 to level 4: What is a necessary and sufficient condition that  We have observed that equality holds when x = 3.142 but not when x = 1.618; further experimentation shows that it fails also when x is between 9 and 10. Oho. Yes. We see that bad cases occur whenever m2 < x < m2 + 1, since this gives m on the left and m + 1 on the right. In all other cases where  is defined, namely when x = 0 or m2 + 1 ≤ x ≤ (m + 1)2, we get equality. The following statement is therefore necessary and sufficient for equality: Either x is an integer or  isn't.
Home of the Toledo Mudhens.
For our next problem let's consider a handy new notation, suggested by C. A. R. Hoare and Lyle Ramshaw, for intervals of the real line: [α . . β] denotes the set of real numbers x such that α ≤ x ≤ β. This set is called a closed interval because it contains both endpoints α and β. The interval containing neither endpoint, denoted by (α . . β), consists of all x such that α < x < β; this is called an open interval. And the intervals [α . . β) and (α . . β], which contain just one endpoint, are defined similarly and called half-open.
(Or, by pessimists, half-closed.)
How many integers are contained in such intervals? The half-open intervals are easier, so we start with them. In fact half-open intervals are almost always nicer than open or closed intervals. For example, they're additive—we can combine the half-open intervals [α . . β) and [β . . γ) to form the half-open interval [α . . γ). This wouldn't work with open intervals because the point β would be excluded, and it could cause problems with closed intervals because β would be included twice.
Back to our problem. The answer is easy if α and β are integers: Then [α . . β) contains the β - α integers α, α + 1, . . . , β - 1, assuming that α ≤ β. Similarly (α . . β] contains β - α integers in such a case. But our problem is harder, because α and β are arbitrary reals. We can convert it to the easier problem, though, since
α ≤ n < β                    α ≤ n < β,
α < n ≤ β                    α < n ≤ β,
when n is an integer, according to (3.7). The intervals on the right have integer endpoints and contain the same number of integers as those on the left, which have real endpoints. So the interval [α . . β) contains exactly β - α integers, and (α . . β] contains β - α. This is a case where we actually want to introduce floor or ceiling brackets, instead of getting rid of them.
By the way, there's a mnemonic for remembering which case uses floors and which uses ceilings: Half-open intervals that include the left endpoint but not the right (such as 0 ≤ θ < 1) are slightly more common than those that include the right endpoint but not the left; and floors are slightly more common than ceilings. So by Murphy's Law, the correct rule is the opposite of what we'd expect—ceilings for [α . . β) and floors for (α . . β].
Just like we can remember the date of Columbus's departure by singing, "In fourteen hundred and ninety-three/Columbus sailed the deep blue sea."
Similar analyses show that the closed interval [α . . β] contains exactly β-α+1 integers and that the open interval (α . . β) contains β-α-1; but we place the additional restriction α ≠ β on the latter so that the formula won't ever embarrass us by claiming that an empty interval (α . . α) contains a total of -1 integers. To summarize, we've deduced the following facts:



Now here's a problem we can't refuse. The Concrete Math Club has a casino (open only to purchasers of this book) in which there's a roulette wheel with one thousand slots, numbered 1 to 1000. If the number n that comes up on a spin is divisible by the floor of its cube root, that is, if



then it's a winner and the house pays us $5; otherwise it's a loser and we must pay $1. (The notation a\b, read "a divides b," means that b is an exact multiple of a; Chapter 4 investigates this relation carefully.) Can we expect to make money if we play this game?
We can compute the average winnings—that is, the amount we'll win (or lose) per play—by first counting the number W of winners and the number L = 1000 - W of losers. If each number comes up once during 1000 plays, we win 5W dollars and lose L dollars, so the average winnings will be



(A poll of the class at this point showed that 28 students thought it was a bad idea to play, 13 wanted to gamble, and the rest were too confused to answer.)
(So we hit them with the Concrete Math Club.)
If there are 167 or more winners, we have the advantage; otherwise the advantage is with the house.
How can we count the number of winners among 1 through 1000? It's not hard to spot a pattern. The numbers from 1 through 23 - 1 = 7 are all winners because  for each. Among the numbers 23 = 8 through 33 - 1 = 26, only the even numbers are winners. And among 33 = 27 through 43 - 1 = 63, only those divisible by 3 are. And so on.
The whole setup can be analyzed systematically if we use the summation techniques of Chapter 2, taking advantage of Iverson's convention about logical statements evaluating to 0 or 1:



This derivation merits careful study. Notice that line 6 uses our formula (3.12) for the number of integers in a half-open interval. The only "difficult" maneuver is the decision made between lines 3 and 4 to treat n = 1000 as a special case. (The inequality k3 ≤ n < (k + 1)3 does not combine easily with 1 ≤ n ≤ 1000 when k = 10.) In general, boundary conditions tend to be the most critical part of ∑-manipulations.
True.
Where did you say this casino is?
The bottom line says that W = 172; hence our formula for average winnings per play reduces to (6·172 - 1000)/1000 dollars, which is 3.2 cents. We can expect to be about $3.20 richer after making 100 bets of $1 each. (Of course, the house may have made some numbers more equal than others.)
The casino problem we just solved is a dressed-up version of the more mundane question, "How many integers n, where 1 ≤ n ≤ 1000, satisfy the relation " Mathematically the two questions are the same. But sometimes it's a good idea to dress up a problem. We get to use more vocabulary (like "winners" and "losers"), which helps us to understand what's going on.
Let's get general. Suppose we change 1000 to 1000000, or to an even larger number, N. (We assume that the casino has connections and can get a bigger wheel.) Now how many winners are there?
The same argument applies, but we need to deal more carefully with the largest value of k, which we can call K for convenience:



(Previously K was 10.) The total number of winners for general N comes to



We know that the remaining sum is N/K - K2 + 1 = N/K - K2 + 1; hence the formula



gives the general answer for a wheel of size N.
The first two terms of this formula are approximately , and the other terms are much smaller in comparison, when N is large. In Chapter 9 we'll learn how to derive expressions like
W = N2/3 + O(N1/3),
where O(N1/3) stands for a quantity that is no more than a constant times N1/3. Whatever the constant is, we know that it's independent of N; so for large N the contribution of the O-term to W will be quite small compared with . For example, the following table shows how close  is to W:



It's a pretty good approximation.
Approximate formulas are useful because they're simpler than formulas with floors and ceilings. However, the exact truth is often important, too, especially for the smaller values of N that tend to occur in practice. For example, the casino owner may have falsely assumed that there are only  winners when N = 1000 (in which case there would be a 10¢ advantage for the house).
Our last application in this section looks at so-called spectra. We define the spectrum of a positive real number α to be an infinite multiset of integers,
Spec(α) = {α, 2α, 3α, . . . } .
(A multiset is like a set but it can have repeated elements.) For example, the spectrum of 1/2 starts out {0, 1, 1, 2, 2, 3, 3, . . . }.
It's easy to prove that no two spectra are equal—that α ≠ β implies Spec(α) ≠ Spec(β). For, assuming without loss of generality that α < β, there's a positive integer m such that m(β - α) ≥ 1. (In fact, any m ≥ 1/(β - α) will do; but we needn't show off our knowledge of floors and ceilings all the time.) Hence mβ - mα ≥ 1, and mβ > mα. Thus Spec(β) has fewer than m elements ≤ mα, while Spec(α) has at least m.
. . . without lots of generality . . .
Spectra have many beautiful properties. For example, consider the two multisets



"If x be an incommensurable number less than unity, one of the series of quantities m/x, m/(1 - x), where m is a whole number, can be found which shall lie between any given consecutive integers, and but one such quantity can be found."
—Rayleigh [304]
It's easy to calculate Spec() with a pocket calculator, and the nth element of Spec() is just 2n more than the nth element of Spec(), by (3.6). A closer look shows that these two spectra are also related in a much more surprising way: It seems that any number missing from one is in the other, but that no number is in both! And it's true: The positive integers are the disjoint union of Spec() and Spec(). We say that these spectra form a partition of the positive integers.
Right, because exactly one of the counts must increase when n increases by 1.
To prove this assertion, we will count how many of the elements of Spec() are ≤ n, and how many of the elements of Spec() are ≤ n. If the total is n, for each n, these two spectra do indeed form a partition.
Whenever α > 0, the number of elements in Spec(α) that are ≤ n is



This derivation has two special points of interest. First, it uses the law



to change '≤' to '<', so that the floor brackets can be removed by (3.7). Also—and this is more subtle—it sums over the range k > 0 instead of k ≥ 1, because (n + 1)/α might be less than 1 for certain n and α. If we had tried to apply (3.12) to determine the number of integers in [1 . . (n + 1)/α), rather than the number of integers in (0 . . (n+1)/α), we would have gotten the right answer; but our derivation would have been faulty because the conditions of applicability wouldn't have been met.
Good, we have a formula for N(α, n). Now we can test whether or not Spec() and Spec() partition the positive integers, by testing whether or not  for all integers n > 0, using (3.14):



Everything simplifies now because of the neat identity



our condition reduces to testing whether or not



for all n > 0. And we win, because these are the fractional parts of two noninteger numbers that add up to the integer n + 1. A partition it is.


3.3 Floor/Ceiling Recurrences
Floors and ceilings add an interesting new dimension to the study of recurrence relations. Let's look first at the recurrence



Thus, for example, K1 is 1 + min(2K0, 3K0) = 3; the sequence begins 1, 3, 3, 4, 7, 7, 7, 9, 9, 10, 13, . . . . One of the authors of this book has modestly decided to call these the Knuth numbers.
Exercise 25 asks for a proof or disproof that Kn ≥ n, for all n ≥ 0. The first few K's just listed do satisfy the inequality, so there's a good chance that it's true in general. Let's try an induction proof: The basis n = 0 comes directly from the defining recurrence. For the induction step, we assume that the inequality holds for all values up through some fixed nonnegative n, and we try to show that Kn+1 ≥ n + 1. From the recurrence we know that Kn+1 = 1 + min(2Kn/2, 3Kn/3). The induction hypothesis tells us that 2Kn/2 ≥ 2n/2 and 3Kn/3 ≥ 3n/3. However, 2n/2 can be as small as n - 1, and 3n/3 can be as small as n - 2. The most we can conclude from our induction hypothesis is that Kn+1 ≥ 1 + (n - 2); this falls far short of Kn+1 ≥ n + 1.
We now have reason to worry about the truth of Kn ≥ n, so let's try to disprove it. If we can find an n such that either 2Kn/2 < n or 3Kn/3 < n, or in other words such that
Kn/2 < n/2          or          Kn/3 < n/3,
we will have Kn+1 < n + 1. Can this be possible? We'd better not give the answer away here, because that will spoil exercise 25.
Recurrence relations involving floors and/or ceilings arise often in computer science, because algorithms based on the important technique of "divide and conquer" often reduce a problem of size n to the solution of similar problems of integer sizes that are fractions of n. For example, one way to sort n records, if n > 1, is to divide them into two approximately equal parts, one of size n/2 and the other of size n/2. (Notice, incidentally, that



this formula comes in handy rather often.) After each part has been sorted separately (by the same method, applied recursively), we can merge the records into their final order by doing at most n - 1 further comparisons. Therefore the total number of comparisons performed is at most f(n), where



A solution to this recurrence appears in exercise 34.
The Josephus problem of Chapter 1 has a similar recurrence, which can be cast in the form
J(1) = 1;
J(n) = 2J(n/2) - (-1)n,          for n > 1.
We've got more tools to work with than we had in Chapter 1, so let's consider the more authentic Josephus problem in which every third person is eliminated, instead of every second. If we apply the methods that worked in Chapter 1 to this more difficult problem, we wind up with a recurrence like



where 'mod' is a function that we will be studying shortly, and where we have an = -2, +1, or  according as n mod 3 = 0, 1, or 2. But this recurrence is too horrible to pursue.
There's another approach to the Josephus problem that gives a much better setup. Whenever a person is passed over, we can assign a new number. Thus, 1 and 2 become n + 1 and n + 2, then 3 is executed; 4 and 5 become n + 3 and n + 4, then 6 is executed; . . . ; 3k + 1 and 3k + 2 become n + 2k + 1 and n + 2k + 2, then 3k + 3 is executed; . . . then 3n is executed (or left to survive). For example, when n = 10 the numbers are



The kth person eliminated ends up with number 3k. So we can figure out who the survivor is if we can figure out the original number of person number 3n.
If N > n, person number N must have had a previous number, and we can find it as follows: We have N = n + 2k + 1 or N = n + 2k + 2, hence k = (N - n - 1)/2; the previous number was 3k + 1 or 3k + 2, respectively. That is, it was 3k + (N - n - 2k) = k + N - n. Hence we can calculate the survivor's number J3(n) as follows:



"Not too slow, not too fast."
—L. Armstrong
This is not a closed form for J3(n); it's not even a recurrence. But at least it tells us how to calculate the answer reasonably fast, if n is large.
Fortunately there's a way to simplify this algorithm if we use the variable D = 3n + 1 - N in place of N. (This change in notation corresponds to assigning numbers from 3n down to 1, instead of from 1 up to 3n; it's sort of like a countdown.) Then the complicated assignment to N becomes



and we can rewrite the algorithm as follows:
D := 1;
while D ≤ 2n do D := D;
J3(n) := 3n + 1 - D .
Aha! This looks much nicer, because n enters the calculation in a very simple way. In fact, we can show by the same reasoning that the survivor Jq(n) when every qth person is eliminated can be calculated as follows:



In the case q = 2 that we know so well, this makes D grow to 2m+1 when n = 2m + l; hence J2(n) = 2(2m + l) + 1 - 2m+1 = 2l + 1. Good.
The recipe in (3.19) computes a sequence of integers that can be defined by the following recurrence:



"Known" like, say, harmonic numbers. A. M. Odlyzko and H. S. Wilf have shown [283] that , where C ≈ 1.622270503.
These numbers don't seem to relate to any familiar functions in a simple way, except when q = 2; hence they probably don't have a nice closed form. But if we're willing to accept the sequence  as "known," then it's easy to describe the solution to the generalized Josephus problem: The survivor Jq(n) is , where k is as small as possible such that .


3.4 'MOD': The Binary Operation
The quotient of n divided by m is n/m, when m and n are positive integers. It's handy to have a simple notation also for the remainder of this division, and we call it 'n mod m'. The basic formula



tells us that we can express n mod m as n - mn/m. We can generalize this to negative integers, and in fact to arbitrary real numbers:



Why do they call it 'mod': The Binary Operation? Stay tuned to find out in the next, exciting, chapter!
This defines 'mod' as a binary operation, just as addition and subtraction are binary operations. Mathematicians have used mod this way informally for a long time, taking various quantities mod 10, mod 2π, and so on, but only in the last twenty years has it caught on formally. Old notion, new notation.
We can easily grasp the intuitive meaning of x mod y, when x and y are positive real numbers, if we imagine a circle of circumference y whose points have been assigned real numbers in the interval [0 . . y). If we travel a distance x around the circle, starting at 0, we end up at x mod y. (And the number of times we encounter 0 as we go is x/y.)
When x or y is negative, we need to look at the definition carefully in order to see exactly what it means. Here are some integer-valued examples:
Beware of computer languages that use another definition.


5 mod 3 =
5 - 35/3
= 2;


5 mod -3 =
5 - (-3)5/(-3)
= -1;


-5 mod 3 =
-5 - 3-5/3
= 1;


-5 mod -3 =
-5 - (-3)-5/(-3)
= -2 .


The number after 'mod' is called the modulus; nobody has yet decided what to call the number before 'mod'. In applications, the modulus is usually positive, but the definition makes perfect sense when the modulus is negative. In both cases the value of x mod y is between 0 and the modulus:
0 ≤ x mod y < y,          for y > 0;
0 ≥ x mod y > y,          for y < 0.
How about calling the other number the modumor?
What about y = 0? Definition (3.21) leaves this case undefined, in order to avoid division by zero, but to be complete we can define



This convention preserves the property that x mod y always differs from x by a multiple of y. (It might seem more natural to make the function continuous at 0, by defining x mod 0 = limy→0 x mod y = 0. But we'll see in Chapter 4 that this would be much less useful. Continuity is not an important aspect of the mod operation.)
We've already seen one special case of mod in disguise, when we wrote x in terms of its integer and fractional parts, x = x + {x}. The fractional part can also be written x mod 1, because we have
x = x + x mod 1.
Notice that parentheses aren't needed in this formula; we take mod to bind more tightly than addition or subtraction.
The floor function has been used to define mod, and the ceiling function hasn't gotten equal time. We could perhaps use the ceiling to define a mod analog like
x mumble y = yx/y - x;
in our circle analogy this represents the distance the traveler needs to continue, after going a distance x, to get back to the starting point 0. But of course we'd need a better name than 'mumble'. If sufficient applications come along, an appropriate name will probably suggest itself.
There was a time in the 70s when 'mod' was the fashion. Maybe the new mumble function should be called 'punk'?
No—I like 'mumble'.
Notice that x mumble y = (-x) mod y.
The distributive law is mod's most important algebraic property: We have



for all real c, x, and y. (Those who like mod to bind less tightly than multiplication may remove the parentheses from the right side here, too.) It's easy to prove this law from definition (3.21), since
c(x mod y) = c(x - yx/y) = cx - cycx/cy = cx mod cy,
if cy ≠ 0; and the zero-modulus cases are trivially true. Our four examples using ±5 and ±3 illustrate this law twice, with c = -1. An identity like (3.23) is reassuring, because it gives us reason to believe that 'mod' has not been defined improperly.
The remainder, eh?
In the remainder of this section, we'll consider an application in which 'mod' turns out to be helpful although it doesn't play a central role. The problem arises frequently in a variety of situations: We want to partition n things into m groups as equally as possible.
Suppose, for example, that we have n short lines of text that we'd like to arrange in m columns. For æsthetic reasons, we want the columns to be arranged in decreasing order of length (actually nonincreasing order); and the lengths should be approximately the same—no two columns should differ by more than one line's worth of text. If 37 lines of text are being divided into five columns, we would therefore prefer the arrangement on the right:



Furthermore we want to distribute the lines of text columnwise—first deciding how many lines go into the first column and then moving on to the second, the third, and so on—because that's the way people read. Distributing row by row would give us the correct number of lines in each column, but the ordering would be wrong. (We would get something like the arrangement on the right, but column 1 would contain lines 1, 6, 11, . . . , 36, instead of lines 1, 2, 3, . . . , 8 as desired.)
A row-by-row distribution strategy can't be used, but it does tell us how many lines to put in each column. If n is not a multiple of m, the rowby-row procedure makes it clear that the long columns should each contain n/m lines, and the short columns should each contain n/m. There will be exactly n mod m long columns (and, as it turns out, there will be exactly n mumble m short ones).
Let's generalize the terminology and talk about 'things' and 'groups' instead of 'lines' and 'columns'. We have just decided that the first group should contain n/m things; therefore the following sequential distribution scheme ought to work: To distribute n things into m groups, when m > 0, put n/m things into one group, then use the same procedure recursively to put the remaining n′ = n - n/m things into m′ = m - 1 additional groups.
For example, if n = 314 and m = 6, the distribution goes like this:


remaining things
remaining groups
things/groups


314
6
53


261
5
53


208
4
52


156
3
52


104
2
52


52
1
52


It works. We get groups of approximately the same size, even though the divisor keeps changing.
Why does it work? In general we can suppose that n = qm + r, where q = n/m and r = n mod m. The process is simple if r = 0: We put n/m = q things into the first group and replace n by n′ = n - q, leaving n′ = qm′ things to put into the remaining m′ = m - 1 groups. And if r > 0, we put n/m = q + 1 things into the first group and replace n by n′ = n - q - 1, leaving n′ = qm′ + r - 1 things for subsequent groups. The new remainder is r′ = r - 1, but q stays the same. It follows that there will be r groups with q + 1 things, followed by m - r groups with q things.
How many things are in the kth group? We'd like a formula that gives n/m when k ≤ n mod m, and n/m otherwise. It's not hard to verify that



has the desired properties, because this reduces to q + (r - k + 1)/m if we write n = qm + r as in the preceding paragraph; here q = n/m. We have (r - k + 1)/m = [k ≤ r], if 1 ≤ k ≤ m and 0 ≤ r < m. Therefore we can write an identity that expresses the partition of n into m as-equal-as-possible parts in nonincreasing order:



This identity is valid for all positive integers m, and for all integers n (whether positive, negative, or zero). We have already encountered the case m = 2 in (3.17), although we wrote it in a slightly different form, n = n/2 + n/2.
If we had wanted the parts to be in nondecreasing order, with the small groups coming before the larger ones, we could have proceeded in the same way but with n/m things in the first group. Then we would have derived the corresponding identity



It's possible to convert between (3.25) and (3.24) by using either (3.4) or the identity of exercise 12.
Some claim that it's too dangerous to replace anything by an mx.
Now if we replace n in (3.25) by mx, and apply rule (3.11) to remove floors inside of floors, we get an identity that holds for all real x:



This is rather amazing, because the floor function is an integer approximation of a real value, but the single approximation on the left equals the sum of a bunch of them on the right. If we assume that x is roughly  on the average, the left-hand side is roughly , while the right-hand side comes to roughly ; the sum of all these rough approximations turns out to be exact!


3.5 Floor/Ceiling Sums
Equation (3.26) demonstrates that it's possible to get a closed form for at least one kind of sum that involves  . Are there others? Yes. The trick that usually works in such cases is to get rid of the floor or ceiling by introducing a new variable.
For example, let's see if it's possible to do the sum



in closed form. One idea is to introduce the variable ; we can do this "mechanically" by proceeding as we did in the roulette problem:



Once again the boundary conditions are a bit delicate. Let's assume first that n = a2 is a perfect square. Then the second sum is zero, and the first can be evaluated by our usual routine:



Falling powers make the sum come tumbling down.
In the general case we can let ; then we merely need to add the terms for a2 ≤ k < n, which are all equal to a, so they sum to (n - a2)a. This gives the desired closed form,



Another approach to such sums is to replace an expression of the form x by ∑j[1 ≤ j ≤ x]; this is legal whenever x ≥ 0. Here's how that method works in the sum of square roots, if we assume for convenience that n = a2:



Now here's another example where a change of variable leads to a transformed sum. A remarkable theorem was discovered independently by three mathematicians—Bohl [34], Sierpiński [326], and Weyl [368]—at about the same time in 1909: If α is irrational then the fractional parts {nα} are very uniformly distributed between 0 and 1, as n → ∞. One way to state this is that



for all irrational α and all bounded functions f that are continuous almost everywhere. For example, the average value of {nα} can be found by setting f(x) = x; we get . (That's exactly what we might expect; but it's nice to know that it is really, provably true, no matter how irrational α is.)
Warning: This stuff is fairly advanced. Better skim the next two pages on first reading; they aren't crucial.
—Friendly TA



The theorem of Bohl, Sierpiński, and Weyl is proved by approximating f(x) above and below by "step functions," which are linear combinations of the simple functions
fv(x) = [0 ≤ x < v]
when 0 ≤ v ≤ 1. Our purpose here is not to prove the theorem; that's a job for calculus books. But let's try to figure out the basic reason why it holds, by seeing how well it works in the special case f(x) = fv(x). In other words, let's try to see how close the sum



gets to the "ideal" value nv, when n is large and α is irrational.
For this purpose we define the discrepancy D(α, n) to be the maximum absolute value, over all 0 ≤ v ≤ 1, of the sum



Our goal is to show that D(α, n) is "not too large" when compared with n, by showing that |s(α, n, v)| is always reasonably small when α is irrational. We can assume without loss of generality that 0 < α < 1.
First we can rewrite s(α, n, v) in simpler form, then introduce a new index variable j:



Right, name and conquer. The change of variable from k to j is the main point.
—Friendly TA
If we're lucky, we can do the sum on k. But we ought to introduce some new variables, so that the formula won't be such a mess. Let us write


a = α-1,
α-1 =
a + α′;


b = vα-1,
vα-1 =
b - v′ .


Thus α′ = {α-1} is the fractional part of α-1, and v′ is the mumble-fractional part of vα-1.
Once again the boundary conditions are our only source of grief. For now, let's forget the restriction 'k < n' and evaluate the sum on k without it:



OK, that's pretty simple; we plug it in and plug away:



where S is a correction for the cases with k ≥ n that we have failed to exclude. The quantity jα′ will be an integer only when j = 0, since α (hence α′) is irrational; and jα′ - v′ will be an integer for at most one value of j. So we can change the ceiling terms to floors:



(The formula { 0 or 1} stands for something that's either 0 or 1; we needn't commit ourselves, because the details don't really matter.)
Interesting. Instead of a closed form, we're getting a sum that looks rather like s(α, n, v) but with different parameters: α′ instead of α, nα instead of n, and v′ instead of v. So we'll have a recurrence for s(α, n, v), which (hopefully) will lead to a recurrence for the discrepancy D(α, n). This means we want to get



into the act:
s(α, n, v) = -nv + nαb - nαv′ - s(α′, nα, v′) - S + {0 or 1} .
Recalling that b - v′ = vα-1, we see that everything will simplify beautifully if we replace nα(b - v′) by nα(b - v′) = nv:
s(α, n, v) = -s(α′, nα, v′) - S +  + {0 or 1} .
Here  is a positive error less than vα-1. Exercise 18 proves that S is, similarly, between 0 and vα-1 . And we can remove the term for j = nα - 1 = nα from the sum, since it contributes either v′ or v′ - 1. Hence, if we take the maximum of absolute values over all v, we get



The methods we'll learn in succeeding chapters will allow us to conclude from this recurrence that D(α, n) is always much smaller than n, when n is sufficiently large. Hence theorem (3.28) is true; however, convergence to the limit is not always very fast. (See exercises 9.45 and 9.61.)



Whew; that was quite an exercise in manipulation of sums, floors, and ceilings. Readers who are not accustomed to "proving that errors are small" might find it hard to believe that anybody would have the courage to keep going, when faced with such weird-looking sums. But actually, a second look shows that there's a simple motivating thread running through the whole calculation. The main idea is that a certain sum s(α, n, v) of n terms can be reduced to a similar sum of at most αn terms. Everything else cancels out except for a small residual left over from terms near the boundaries.
Let's take a deep breath now and do one more sum, which is not trivial but has the great advantage (compared with what we've just been doing) that it comes out in closed form so that we can easily check the answer. Our goal now will be to generalize the sum in (3.26) by finding an expression for



Is this a harder sum of floors, or a sum of harder floors?
Finding a closed form for this sum is tougher than what we've done so far (except perhaps for the discrepancy problem we just looked at). But it's instructive, so we'll hack away at it for the rest of this chapter.
As usual, especially with tough problems, we start by looking at small cases. The special case n = 1 is (3.26), with x replaced by x/m:



Be forewarned: This is the beginning of a pattern, in that the last part of the chapter consists of the solution of some long, difficult problem, with little more motivation than curiosity.
—Students
And as in Chapter 1, we find it useful to get more data by generalizing downwards to the case n = 0:



Touché. But c'mon, gang, do you always need to be told about applications before you can get interested in something? This sum arises, for example, in the study of random number generation and testing. But mathematicians looked at it long before computers came along, because they found it natural to ask if there's a way to sum arithmetic progressions that have been "floored."
—Your instructor
Our problem has two parameters, m and n; let's look at some small cases for m. When m = 1 there's just a single term in the sum and its value is x. When m = 2 the sum is x/2 + (x + n)/2. We can remove the interaction between x and n by removing n from inside the floor function, but to do that we must consider even and odd n separately. If n is even, n/2 is an integer, so we can remove it from the floor:



If n is odd, (n - 1)/2 is an integer so we get



The last step follows from (3.26) with m = 2.
These formulas for even and odd n slightly resemble those for n = 0 and 1, but no clear pattern has emerged yet; so we had better continue exploring some more small cases. For m = 3 the sum is



and we consider three cases for n: Either it's a multiple of 3, or it's 1 more than a multiple, or it's 2 more. That is, n mod 3 = 0, 1, or 2. If n mod 3 = 0 then n/3 and 2n/3 are integers, so the sum is



If n mod 3 = 1 then (n - 1)/3 and (2n - 2)/3 are integers, so we have



Again this last step follows from (3.26), this time with m = 3. And finally, if n mod 3 = 2 then



The left hemispheres of our brains have finished the case m = 3, but the right hemispheres still can't recognize the pattern, so we proceed to m = 4:



"Inventive genius requires pleasurable mental activity as a condition for its vigorous exercise. 'Necessity is the mother of invention' is a silly proverb. 'Necessity is the mother of futile dodges' is much nearer to the truth. The basis of the growth of modern invention is science, and science is almost wholly the outgrowth of pleasurable intellectual curiosity."
—A. N. White-head [371]
At least we know enough by now to consider cases based on n mod m. If n mod 4 = 0 then



And if n mod 4 = 1,



The case n mod 4 = 3 turns out to give the same answer. Finally, in the case n mod 4 = 2 we get something a bit different, and this turns out to be an important clue to the behavior in general:



This last step simplifies something of the form y/2 + (y + 1)/2, which again is a special case of (3.26).
To summarize, here's the value of our sum for small m:



It looks as if we're getting something of the form



where a, b, and c somehow depend on m and n. Even the myopic among us can see that b is probably (m - 1)/2. It's harder to discern an expression for a; but the case n mod 4 = 2 gives us a hint that a is probably gcd(m, n), the greatest common divisor of m and n. This makes sense because gcd(m, n) is the factor we remove from m and n when reducing the fraction n/m to lowest terms, and our sum involves the fraction n/m. (We'll look carefully at gcd operations in Chapter 4.) The value of c seems more mysterious, but perhaps it will drop out of our proofs for a and b.
In computing the sum for small m, we've effectively rewritten each term of the sum as



because (kn - kn mod m)/m is an integer that can be removed from inside the floor brackets. Thus the original sum can be expanded into the following tableau:



When we experimented with small values of m, these three columns led respectively to ax/a, bn, and c.
In particular, we can see how b arises. The second column is an arithmetic progression, whose sum we know—it's the average of the first and last terms, times the number of terms:



So our guess that b = (m - 1)/2 has been verified.
The first and third columns seem tougher; to determine a and c we must take a closer look at the sequence of numbers
0 mod m, n mod m, 2n mod m, . . . , (m - 1)n mod m.
Suppose, for example, that m = 12 and n = 5. If we think of the sequence as times on a clock, the numbers are 0 o'clock (we take 12 o'clock to be 0 o'clock), then 5 o'clock, 10 o'clock, 3 o'clock (= 15 o'clock), 8 o'clock, and so on. It turns out that we hit every hour exactly once.
Now suppose m = 12 and n = 8. The numbers are 0 o'clock, 8 o'clock, 4 o'clock (= 16 o'clock), but then 0, 8, and 4 repeat. Since both 8 and 12 are multiples of 4, and since the numbers start at 0 (also a multiple of 4), there's no way to break out of this pattern—they must all be multiples of 4.
In these two cases we have gcd(12, 5) = 1 and gcd(12, 8) = 4. The general rule, which we will prove next chapter, states that if d = gcd(m, n) then we get the numbers 0, d, 2d, . . . , m - d in some order, followed by d - 1 more copies of the same sequence. For example, with m = 12 and n = 8 the pattern 0, 8, 4 occurs four times.
Lemma now, dilemma later.
The first column of our sum now makes complete sense. It contains d copies of the terms x/m, (x + d)/m, . . . , (x + m - d)/m, in some order, so its sum is



This last step is yet another application of (3.26). Our guess for a has been verified:
a = d = gcd(m, n) .
Also, as we guessed, we can now compute c, because the third column has become easy to fathom. It contains d copies of the arithmetic progression 0/m, d/m, 2d/m, . . . , (m - d)/m, so its sum is



the third column is actually subtracted, not added, so we have



End of mystery, end of quest. The desired closed form is



where d = gcd(m, n). As a check, we can make sure this works in the special cases n = 0 and n = 1 that we knew before: When n = 0 we get d = gcd(m, 0) = m; the last two terms of the formula are zero so the formula properly gives mx/m. And for n = 1 we get d = gcd(m, 1) = 1; the last two terms cancel nicely, and the sum is just x.
By manipulating the closed form a bit, we can actually make it symmetric in m and n:



This is astonishing, because there's no algebraic reason to suspect that such a sum should be symmetrical. We have proved a "reciprocity law,"
Yup, I'm floored.



For example, if m = 41 and n = 127, the left sum has 41 terms and the right has 127; but they still come out equal, for all real x.


Exercises

Warmups
1. When we analyzed the Josephus problem in Chapter 1, we represented an arbitrary positive integer n in the form n = 2m +l, where 0 ≤ l < 2m. Give explicit formulas for l and m as functions of n, using floor and/or ceiling brackets.
2. What is a formula for the nearest integer to a given real number x? In case of ties, when x is exactly halfway between two integers, give an expression that rounds (a) up—that is, to x; (b) down—that is, to x.
3. Evaluate mαn/α, when m and n are positive integers and α is an irrational number greater than n.
4. The text describes problems at levels 1 through 5. What is a level 0 problem? (This, by the way, is not a level 0 problem.)
5. Find a necessary and sufficient condition that nx = nx, when n is a positive integer. (Your condition should involve {x}.)
6. Can something interesting be said about f(x) when f(x) is a continuous, monotonically decreasing function that takes integer values only when x is an integer?
7. Solve the recurrence
Xn = n,                       for 0 ≤ n < m;
Xn = Xn-m + 1,          for n ≥ m.
8. Prove the Dirichlet box principle: If n objects are put into m boxes, some box must contain ≥ n/m objects, and some box must contain ≤ n/m.
You know you're in college when the book doesn't tell you how to pronounce 'Dirichlet'.
9. Egyptian mathematicians in 1800 B.C. represented rational numbers between 0 and 1 as sums of unit fractions 1/x1 + · · · + 1/xk, where the x's were distinct positive integers. For example, they wrote  instead of . Prove that it is always possible to do this in a systematic way: If 0 < m/n < 1, then



(This is Fibonacci's algorithm, due to Leonardo Fibonacci, A.D. 1202.)


Basics
10. Show that the expression



is always either x or x. In what circumstances does each case arise?
11. Give details of the proof alluded to in the text, that the open interval (α . . β) contains exactly β - α - 1 integers when α < β. Why does the case α = β have to be excluded in order to make the proof correct?
12. Prove that



for all integers n and all positive integers m. [This identity gives us another way to convert ceilings to floors and vice versa, instead of using the reflective law (3.4).]
13. Let α and β be positive real numbers. Prove that Spec(α) and Spec(β) partition the positive integers if and only if α and β are irrational and 1/α + 1/β = 1.
14. Prove or disprove:
(x mod ny) mod y = x mod y,    integer n.
15. Is there an identity analogous to (3.26) that uses ceilings instead of floors?
16. Prove that n mod 2 = (1-(-1)n)/2. Find and prove a similar expression for n mod 3 in the form a+bωn +cω2n, where ω is the complex number . Hint: ω3 = 1 and 1 + ω + ω2 = 0.
17. Evaluate the sum ∑0≤k<mx + k/m in the case x ≥ 0 by substituting ∑j[1 ≤ j ≤ x + k/m] for x + k/m and summing first on k. Does your answer agree with (3.26)?
18. Prove that the boundary-value error term S in (3.30) is at most α-1v. Hint: Show that small values of j are not involved.


Homework exercises
19. Find a necessary and sufficient condition on the real number b > 1 such that
logb x = logbx
for all real x ≥ 1.
20. Find the sum of all multiples of x in the closed interval [α . . β], when x > 0.
21. How many of the numbers 2m, for 0 ≤ m ≤ M, have leading digit 1 in decimal notation?
22. Evaluate the sums  and .
23. Show that the nth element of the sequence
1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, . . .
is . (The sequence contains exactly m occurrences of m.)
24. Exercise 13 establishes an interesting relation between the two multisets Spec(α) and Spec (α/(α - 1)), when α is any irrational number > 1, because 1/α + (α - 1)/α = 1. Find (and prove) an interesting relation between the two multisets Spec(α) and Spec (α/(α + 1)), when α is any positive real number.
25. Prove or disprove that the Knuth numbers, defined by (3.16), satisfy Kn ≥ n for all nonnegative n.
26. Show that the auxiliary Josephus numbers (3.20) satisfy



27. Prove that infinitely many of the numbers  defined by (3.20) are even, and that infinitely many are odd.
28. Solve the recurrence
a0 = 1;
an = an-1 + ,          for n > 0.
29. Show that, in addition to (3.31), we have
D(α, n) ≥ D(α′, αn) - α-1 - 2.
There's a discrepancy between this formula and (3.31).
30. Show that the recurrence
X0 = m,
Xn =  - 2,          for n > 0,
has the solution Xn = α2n, if m is an integer greater than 2, where α + α-1 = m and α > 1. For example, if m = 3 the solution is



31. Prove or disprove: x + y + x + y ≤ 2x + 2y.
32. Let ∥x∥ = min (x - x, x - x) denote the distance from x to the nearest integer. What is the value of



(Note that this sum can be doubly infinite. For example, when x = 1/3 the terms are nonzero as k → -∞ and also as k → +∞ .)


Exam problems
33. A circle, 2n - 1 units in diameter, has been drawn symmetrically on a 2n × 2n chessboard, illustrated here for n = 3:



a. How many cells of the board contain a segment of the circle?
b. Find a function f(n, k) such that exactly  cells of the board lie entirely within the circle.
34. Let .
a. Find a closed form for f(n), when n ≥ 1.
b. Prove that f(n) = n - 1 + f(n/2) + f (n/2) for all n ≥ 1.
35. Simplify the formula (n + 1)2n!e mod n.
Simplify it, but don't change the value.
36. Assuming that n is a nonnegative integer, find a closed form for the sum



37. Prove the identity



for all positive integers m and n.
38. Let x1, . . . , xn be real numbers such that the identity



holds for all positive integers m. Prove something interesting about x1, . . . , xn.
39. Prove that the double sum ∑0≤k≤logbx ∑0<j<b(x + jbk)/bk+1 equals (b - 1) (logb x + 1) + x - 1, for every real number x ≥ 1 and every integer b > 1.
40. The spiral function σ(n), indicated in the diagram below, maps a nonnegative integer n onto an ordered pair of integers (x(n), y(n)). For example, it maps n = 9 onto the ordered pair (1, 2).



People in the southern hemisphere use a different spiral.
a. Prove that if ,



and find a similar formula for y(n). Hint: Classify the spiral into segments Wk, Sk, Ek, Nk according as , 4k - 1, 4k, 4k + 1.
b. Prove that, conversely, we can determine n from σ(n) by a formula of the form
n = (2k)2 ± (2k + x(n) + y(n)) ,   k = max (|x(n)|, |y(n)|) .
Give a rule for when the sign is + and when the sign is -.


Bonus problems
41. Let f and g be increasing functions such that the sets {f(1), f(2), . . . } and {g(1), g(2), . . . } partition the positive integers. Suppose that f and g are related by the condition g(n) = f(f(n)) + 1 for all n > 0. Prove that f(n) = nϕ and g(n) = nϕ2, where .
42. Do there exist real numbers α, β, and γ such that Spec(α), Spec(β), and Spec(γ) together partition the set of positive integers?
43. Find an interesting interpretation of the Knuth numbers, by unfolding the recurrence (3.16).
44. Show that there are integers  and  such that



when  is the solution to (3.20). Use this fact to obtain another form of the solution to the generalized Josephus problem:



45. Extend the trick of exercise 30 to find a closed-form solution to
Y0 = m,
Yn =  - 1,          for n > 0,
if m is a positive integer.
46. Prove that if , where m and l are nonnegative integers, then . Use this remarkable property to find a closed form solution to the recurrence



Hint: .
47. The function f(x) is said to be replicative if it satisfies



for every positive integer m. Find necessary and sufficient conditions on the real number c for the following functions to be replicative:
a. f(x) = x + c.
b. f(x) = [x + c is an integer].
c. f(x) = max(x, c).
d. f(x) = x + cx -  [x is not an integer].
48. Prove the identity
x3 = 3x xx + 3{x} {xx} + {x}3 - 3x xx + x3 ,
and show how to obtain similar formulas for xn when n > 3.
49. Find a necessary and sufficient condition on the real numbers 0 ≤ α < 1 and β ≥ 0 such that we can determine α and β from the infinite multiset of values
{nα + nβ | n > 0}.


Research problems
50. Find a necessary and sufficient condition on the nonnegative real numbers α and β such that we can determine α and β from the infinite multiset of values
{nαβ | n > 0}.
51. Let x be a real number ≥ . The solution to the recurrence
Z0(x) = x,
Zn(x) = Zn-1(x)2 - 1,          for n > 0,
can be written Zn(x) = f(x)2n, if x is an integer, where



because Zn(x) - 1 < f(x)2n < Zn(x) in that case. What other interesting properties does this function f(x) have?
52. Given nonnegative real numbers α and β, let
Spec(α; β) = {α + β, 2α + β, 3α + β, . . . }
be a multiset that generalizes Spec(α) = Spec(α; 0). Prove or disprove: If the m ≥ 3 multisets Spec(α1; β1), Spec(α2; β2), . . . , Spec(αm; βm) partition the positive integers, and if the parameters α1 < α2 < · · · < αm are rational, then



Spec this be hard.
53. Fibonacci's algorithm (exercise 9) is "greedy" in the sense that it chooses the least conceivable q at every step. A more complicated algorithm is known by which every fraction m/n with n odd can be represented as a sum of distinct unit fractions 1/q1 + · · · + 1/qk with odd denominators. Does the greedy algorithm for such a representation always terminate?











4. Number Theory
Integers are central to the discrete mathematics we are emphasizing in this book. Therefore we want to explore the theory of numbers, an important branch of mathematics concerned with the properties of integers.
We tested the number theory waters in the previous chapter, by introducing binary operations called 'mod' and 'gcd'. Now let's plunge in and really immerse ourselves in the subject.
In other words, be prepared to drown.

4.1 Divisibility
We say that m divides n (or n is divisible by m) if m > 0 and the ratio n/m is an integer. This property underlies all of number theory, so it's convenient to have a special notation for it. We therefore write



(The notation 'm|n' is actually much more common than 'm\n' in current mathematics literature. But vertical lines are overused—for absolute values, set delimiters, conditional probabilities, etc.—and backward slashes are underused. Moreover, 'm\n' gives an impression that m is the denominator of an implied ratio. So we shall boldly let our divisibility symbol lean leftward.)
If m does not divide n we write ''.
There's a similar relation, "n is a multiple of m," which means almost the same thing except that m doesn't have to be positive. In this case we simply mean that n = mk for some integer k. Thus, for example, there's only one multiple of 0 (namely 0), but nothing is divisible by 0. Every integer is a multiple of -1, but no integer is divisible by -1 (strictly speaking). These definitions apply when m and n are any real numbers; for example, 2π is divisible by π. But we'll almost always be using them when m and n are integers. After all, this is number theory.
". . . no integer is divisible by -1 (strictly speaking)."
—Graham, Knuth, and Patashnik [161]
The greatest common divisor of two integers m and n is the largest integer that divides them both:



In Britain we call this 'hcf' (highest common factor).
For example, gcd(12, 18) = 6. This is a familiar notion, because it's the common factor that fourth graders learn to take out of a fraction m/n when reducing it to lowest terms: 12/18 = (12/6)/(18/6) = 2/3. Notice that if n > 0 we have gcd(0, n) = n, because any positive number divides 0, and because n is the largest divisor of itself. The value of gcd(0, 0) is undefined.
Another familiar notion is the least common multiple,
Not to be confused with the greatest common multiple.



this is undefined if m ≤ 0 or n ≤ 0. Students of arithmetic recognize this as the least common denominator, which is used when adding fractions with denominators m and n. For example, lcm(12, 18) = 36, and fourth graders know that . The lcm is somewhat analogous to the gcd, but we don't give it equal time because the gcd has nicer properties.
One of the nicest properties of the gcd is that it is easy to compute, using a 2300-year-old method called Euclid's algorithm. To calculate gcd(m, n), for given values 0 ≤ m < n, Euclid's algorithm uses the recurrence



Thus, for example, gcd(12, 18) = gcd(6, 12) = gcd(0, 6) = 6. The stated recurrence is valid, because any common divisor of m and n must also be a common divisor of both m and the number n mod m, which is n - n/mm. There doesn't seem to be any recurrence for lcm(m, n) that's anywhere near as simple as this. (See exercise 2.)
Euclid's algorithm also gives us more: We can extend it so that it will compute integers m′ and n′ satisfying



(Remember that m′ or n′ can be negative.)
Here's how. If m = 0, we simply take m′ = 0 and n′ = 1. Otherwise we let r = n mod m and apply the method recursively with r and m in place of m and n, computing  and  such that



Since r = n - n/mm and gcd(r, m) = gcd(m, n), this equation tells us that



The left side can be rewritten to show its dependency on m and n:



hence  and n′ =  are the integers we need in (4.5). For example, in our favorite case m = 12, n = 18, this method gives 6 = 0·0+1·6 = 1·6 + 0·12 = (-1)·12 + 1·18.
But why is (4.5) such a neat result? The main reason is that there's a sense in which the numbers m′ and n′ actually prove that Euclid's algorithm has produced the correct answer in any particular case. Let's suppose that our computer has told us after a lengthy calculation that gcd(m, n) = d and that m′m + n′n = d; but we're skeptical and think that there's really a greater common divisor, which the machine has somehow overlooked. This cannot be, however, because any common divisor of m and n has to divide m′m + n′n; so it has to divide d; so it has to be ≤ d. Furthermore we can easily check that d does divide both m and n. (Algorithms that output their own proofs of correctness are called self-certifying.)
We'll be using (4.5) a lot in the rest of this chapter. One of its important consequences is the following mini-theorem:



(Proof: If k divides both m and n, it divides m′m + n′n, so it divides gcd(m, n). Conversely, if k divides gcd(m, n), it divides a divisor of m and a divisor of n, so it divides both m and n.) We always knew that any common divisor of m and n must be less than or equal to their gcd; that's the definition of greatest common divisor. But now we know that any common divisor is, in fact, a divisor of their gcd.
Sometimes we need to do sums over all divisors of n. In this case it's often useful to use the handy rule



which holds since n/m runs through all divisors of n when m does. For example, when n = 12 this says that a1 + a2 + a3 + a4 + a6 + a12 = a12 + a6 + a4 + a3 + a2 + a1.
There's also a slightly more general identity,



which is an immediate consequence of the definition (4.1). If n is positive, the right-hand side of (4.8) is ∑k\n an/k; hence (4.8) implies (4.7). And equation (4.8) works also when n is negative. (In such cases, the nonzero terms on the right occur when k is the negative of a divisor of n.)
Moreover, a double sum over divisors can be "interchanged" by the law



For example, this law takes the following form when n = 12:
a1,1 + (a1,2 + a2,2) + (a1,3 + a3,3)
+ (a1,4 + a2,4 + a4,4) + (a1,6 + a2,6 + a3,6 + a6,6)
+ (a1,12 + a2,12 + a3,12 + a4,12 + a6,12 + a12,12)
     = (a1,1 + a1,2 + a1,3 + a1,4 + a1,6 + a1,12)
+ (a2,2 + a2,4 + a2,6 + a2,12) + (a3,3 + a3,6 + a3,12)
+ (a4,4 + a4,12) + (a6,6 + a6,12) + a12,12 .
We can prove (4.9) with Iversonian manipulation. The left-hand side is



the right-hand side is



which is the same except for renaming the indices. This example indicates that the techniques we've learned in Chapter 2 will come in handy as we study number theory.


4.2 Primes
A positive integer p is called prime if it has just two divisors, namely 1 and p. Throughout the rest of this chapter, the letter p will always stand for a prime number, even when we don't say so explicitly. By convention, 1 isn't prime, so the sequence of primes starts out like this:
2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, . . . .
How about the p in 'explicitly'?
Some numbers look prime but aren't, like 91 (= 7 · 13) and 161 (= 7 · 23). These numbers, and others that have nontrivial divisors, are called composite. Every integer greater than 1 is either prime or composite, but not both.
Primes are of great importance, because they're the fundamental building blocks of all the positive integers. Any positive integer n can be written as a product of primes,



For example, 12 = 2 · 2 · 3; 11011 = 7 · 11 · 11 · 13; 11111 = 41 · 271. (Products denoted by Π are analogous to sums denoted by ∑, as explained in exercise 2.25. If m = 0, we consider this to be an empty product, whose value is 1 by definition; that's the way n = 1 gets represented by (4.10).) Such a factorization is always possible because if n > 1 is not prime it has a divisor n1 such that 1 < n1 < n; thus we can write n = n1 · n2, and (by induction) we know that n1 and n2 can be written as products of primes.
Moreover, the expansion in (4.10) is unique: There's only one way to write n as a product of primes in nondecreasing order. This statement is called the Fundamental Theorem of Arithmetic, and it seems so obvious that we might wonder why it needs to be proved. How could there be two different sets of primes with the same product? Well, there can't, but the reason isn't simply "by definition of prime numbers." For example, if we consider the set of all real numbers of the form  when m and n are integers, the product of any two such numbers is again of the same form, and we can call such a number "prime" if it can't be factored in a nontrivial way. The number 6 has two representations, ; yet exercise 36 shows that 2, 3, , and  are all "prime" in this system.
Therefore we should prove rigorously that (4.10) is unique. There is certainly only one possibility when n = 1, since the product must be empty in that case; so let's suppose that n > 1 and that all smaller numbers factor uniquely. Suppose we have two factorizations
n = p1 . . . pm = q1 . . . qk ,          p1≤ · · · ≤pm     and     q1≤ · · · ≤qk,
where the p's and q's are all prime. We will prove that p1 = q1. If not, we can assume that p1 < q1, making p1 smaller than all the q's. Since p1 and q1 are prime, their gcd must be 1; hence Euclid's self-certifying algorithm gives us integers a and b such that ap1 + bq1 = 1. Therefore
ap1q2 . . . qk + bq1q2 . . . qk = q2 . . . qk .
Now p1 divides both terms on the left, since q1q2 . . . qk = n; hence p1 divides the right-hand side, q2 . . . qk. Thus q2 . . . qk/p1 is an integer, and q2 . . . qk has a prime factorization in which p1 appears. But q2 . . . qk < n, so it has a unique factorization (by induction). Hence p1 must be either q2 or · · · or qn; yet p1 is strictly smaller! This contradiction shows that p1 must be equal to q1 after all. Therefore we can divide both of n's factorizations by p1, obtaining p2 . . . pm = q2 . . . qk < n. The other factors must likewise be equal (by induction), so our proof of uniqueness is complete.
Sometimes it's more useful to state the Fundamental Theorem in another way: Every positive integer can be written uniquely in the form



It's the factorization, not the theorem, that's unique.
The right-hand side is a product over infinitely many primes; but for any particular n all but a few exponents are zero, so the corresponding factors are 1. Therefore it's really a finite product, just as many "infinite" sums are really finite because their terms are mostly zero.
Formula (4.11) represents n uniquely, so we can think of the sequence n2, n3, n5, . . .  as a number system for positive integers. For example, the prime-exponent representation of 12 is 2, 1, 0, 0, . . .  and the prime-exponent representation of 18 is 1, 2, 0, 0, . . . . To multiply two numbers, we simply add their representations. In other words,



This implies that



and it follows immediately that






For example, since 12 = 22 ·31 and 18 = 21 ·32, we can get their gcd and lcm by taking the min and max of common exponents:
gcd(12, 18) = 2min(2,1) · 3min(1,2) = 21 ·31 = 6;
lcm(12, 18) = 2max(2,1) · 3max(1,2) = 22 ·32 = 36 .
If the prime p divides a product mn then it divides either m or n, perhaps both, because of the unique factorization theorem. But composite numbers do not have this property. For example, the nonprime 4 divides 60 = 6 · 10, but it divides neither 6 nor 10. The reason is simple: In the factorization 60 = 6 · 10 = (2 · 3)(2 · 5), the two prime factors of 4 = 2 · 2 have been split into two parts, hence 4 divides neither part. But a prime is unsplittable, so it must divide one of the original factors.


4.3 Prime Examples
How many primes are there? A lot. In fact, infinitely many. Euclid proved this long ago in his Theorem 9 : 20, as follows. Consider any finite set of primes, say {P1, P2, . . . , Pk}. Then, said Euclid, we should think about the number



—Euclid [98]
M = P1 · P2 · . . . · Pk + 1.
[Translation: "There are more primes than in any given set of primes."]
None of the k primes can divide M, because each divides M - 1. Thus there must be some other prime that divides M; perhaps M itself is prime. Every finite set of prime numbers must therefore be incomplete.
Euclid's proof suggests that we define Euclid numbers by the recurrence



The sequence starts out
e1 = 1 + 1 = 2;
e2 = 2 + 1 = 3;
e3 = 2·3 + 1 = 7;
e4 = 2·3·7 + 1 = 43;
these are all prime. But the next case, e5, is 1807 = 13·139. It turns out that e6 = 3263443 is prime, while
e7 = 547·607·1033·31051;
e8 = 29881·67003·9119521·6212157481 .
It is known that e9, . . . , e17 are composite, and the remaining en are probably composite as well. However, the Euclid numbers are all relatively prime to each other; that is,
gcd(em, en) = 1,          when m ≠ n.
Euclid's algorithm (what else?) tells us this in three short steps, because en mod em = 1 when n > m:
gcd(em, en) = gcd(1, em) = gcd(0, 1) = 1.
Therefore, if we let qj be the smallest prime factor of ej for all j ≥ 1, we get a sequence q1, q2, q3, . . . of infinitely many distinct primes.
Let's pause to consider the Euclid numbers from the standpoint of Chapter 1. Can we express en in closed form? Recurrence (4.16) can be simplified by removing the three dots: If n > 1 we have
en = e1 . . . en-2en-1 + 1 = (en-1 - 1)en-1 + 1 =  - en-1 + 1 .
Thus en has about twice as many decimal digits as en-1. Exercise 37 proves that there's a constant E ≈ 1.264 such that



And exercise 60 provides a similar formula that gives nothing but primes:



for some constant P. But equations like (4.17) and (4.18) cannot really be considered to be in closed form, because the constants E and P are computed from the numbers en and pn in a sort of sneaky way. No independent relation is known (or likely) that would connect them with other constants of mathematical interest.
Indeed, nobody knows any useful formula that gives arbitrarily large primes but only primes. Computer scientists at Chevron Geosciences did, however, strike mathematical oil in 1984. Using a program developed by David Slowinski, they discovered the largest prime known at that time,
2216091 - 1,
while testing a new Cray X-MP supercomputer. It's easy to compute this number in a few milliseconds on a personal computer, because modern computers work in binary notation and this number is simply (11 . . . 1)2. All 216,091 of its bits are '1'. But it's much harder to prove that this number is prime. In fact, just about any computation with it takes a lot of time, because it's so large. For example, even a sophisticated algorithm requires several minutes just to convert 2216091 - 1 to radix 10 on a PC. When printed out, its 65,050 decimal digits require 78 cents U.S. postage to mail first class.
Or probably more, by the time you read this.
Incidentally, 2216091 - 1 is the number of moves necessary to solve the Tower of Hanoi problem when there are 216,091 disks. Numbers of the form
2p - 1
(where p is prime, as always in this chapter) are called Mersenne numbers, after Father Marin Mersenne who investigated some of their properties in the seventeenth century [269]. The Mersenne primes known prior to 1998 occur for p = 2, 3, 5, 7, 13, 17, 19, 31, 61, 89, 107, 127, 521, 607, 1279, 2203, 2281, 3217, 4253, 4423, 9689, 9941, 11213, 19937, 21701, 23209, 44497, 86243, 110503, 132049, 216091, 756839, 859433, 1257787, 1398269, and 2976221.
The number 2n - 1 can't possibly be prime if n is composite, because 2km - 1 has 2m - 1 as a factor:
2km - 1 = (2m - 1)(2m (k-1) + 2m(k-2) + · · · + 1) .
But 2p - 1 isn't always prime when p is prime; 211 - 1 = 2047 = 23·89 is the smallest such nonprime. (Mersenne knew this.)
Factoring and primality testing of large numbers are hot topics nowadays. A summary of what was known up to 1981 appears in Section 4.5.4 of [208], and many new results continue to be discovered. Pages 391-394 of that book explain a special way to test Mersenne numbers for primality.
For most of the last five hundred years, the largest known prime has been a Mersenne prime, although only a few dozen Mersenne primes are known. Many people are trying to find larger ones, but it's getting tough. So those really interested in fame (if not fortune) and a spot in The Guinness Book of World Records might instead try numbers of the form 2nk + 1, for small values of k like 3 or 5. These numbers can be tested for primality almost as quickly as Mersenne numbers can; exercise 4.5.4-27 of [208] gives the details.
We haven't fully answered our original question about how many primes there are. There are infinitely many, but some infinite sets are "denser" than others. For instance, among the positive integers there are infinitely many even numbers and infinitely many perfect squares, yet in several important senses there are more even numbers than perfect squares. One such sense looks at the size of the nth value. The nth even integer is 2n and the nth perfect square is n2; since 2n is much less than n2 for large n, the nth even integer occurs much sooner than the nth perfect square, so we can say there are many more even integers than perfect squares. A similar sense looks at the number of values not exceeding x. There are x/2 such even integers and  such perfect squares; since x/2 is much larger than  for large x, again we can say there are many more even integers.
Weird. I thought there were the same number of even integers as perfect squares, since there's a one-to-one correspondence between them.
What can we say about the primes in these two senses? It turns out that the nth prime, Pn, is about n times the natural log of n:
Pn ∼ n ln n.
(The symbol '∼' can be read "is asymptotic to"; it means that the limit of the ratio Pn/n ln n is 1 as n goes to infinity.) Similarly, for the number of primes π(x) not exceeding x we have what's known as the prime number theorem:



Proving these two facts is beyond the scope of this book, although we can show easily that each of them implies the other. In Chapter 9 we will discuss the rates at which functions approach infinity, and we'll see that the function n ln n, our approximation to Pn, lies between 2n and n2 asymptotically. Hence there are fewer primes than even integers, but there are more primes than perfect squares.
These formulas, which hold only in the limit as n or x → ∞, can be replaced by more exact estimates. For example, Rosser and Schoenfeld [312] have established the handy bounds






If we look at a "random" integer n, the chances of its being prime are about one in ln n. For example, if we look at numbers near 1016, we'll have to examine about 16 ln 10 ≈ 36.8 of them before finding a prime. (It turns out that there are exactly 10 primes between 1016 - 370 and 1016 - 1.) Yet the distribution of primes has many irregularities. For example, all the numbers between P1P2 . . . Pn + 2 and P1P2 . . . Pn + Pn+1 - 1 inclusive are composite. Many examples of "twin primes" p and p + 2 are known (5 and 7, 11 and 13, 17 and 19, 29 and 31, . . . , 9999999999999641 and 9999999999999643, . . . ), yet nobody knows whether or not there are infinitely many pairs of twin primes. (See Hardy and Wright [181, §1.4 and §2.8].)
One simple way to calculate all π(x) primes ≤ x is to form the so-called sieve of Eratosthenes: First write down all integers from 2 through x. Next circle 2, marking it prime, and cross out all other multiples of 2. Then repeatedly circle the smallest uncircled, uncrossed number and cross out its other multiples. When everything has been circled or crossed out, the circled numbers are the primes. For example when x = 10 we write down 2 through 10, circle 2, then cross out its multiples 4, 6, 8, and 10. Next 3 is the smallest uncircled, uncrossed number, so we circle it and cross out 6 and 9. Now 5 is smallest, so we circle it and cross out 10. Finally we circle 7. The circled numbers are 2, 3, 5, and 7; so these are the π(10) = 4 primes not exceeding 10.
"Je me sers de la notation très simple n! pour désigner le produit de nombres décroissans depuis n jusqu'à l'unité, savoir n(n - 1) (n - 2) . . . . 3. 2. 1. L'emploi continuel de l'analyse combinatoire que je fais dans la plupart de mes démonstrations, a rendu cette notation indispensable."
—Ch. Kramp [228]


4.4 Factorial Factors
Now let's take a look at the factorization of some interesting highly composite numbers, the factorials:



According to our convention for an empty product, this defines 0! to be 1. Thus n! = (n - 1)! n for every positive integer n. This is the number of permutations of n distinct objects. That is, it's the number of ways to arrange n things in a row: There are n choices for the first thing; for each choice of first thing, there are n - 1 choices for the second; for each of these n(n - 1) choices, there are n - 2 for the third; and so on, giving n(n - 1)(n - 2) . . . (1) arrangements in all. Here are the first few values of the factorial function.



It's useful to know a few factorial facts, like the first six or so values, and the fact that 10! is about  million plus change; another interesting fact is that the number of digits in n! exceeds n when n ≥ 25.
We can prove that n! is plenty big by using something like Gauss's trick of Chapter 1:



We have , since the quadratic polynomial  has its smallest value at k = 1 and at k = n, while it has its largest value at . Therefore



that is,



This relation tells us that the factorial function grows exponentially!!
To approximate n! more accurately for large n we can use Stirling's formula, which we will derive in Chapter 9:



And a still more precise approximation tells us the asymptotic relative error: Stirling's formula undershoots n! by a factor of about 1/(12n). Even for fairly small n this more precise estimate is pretty good. For example, Stirling's approximation (4.23) gives a value near 3598696 when n = 10, and this is about 0.83% ≈ 1/120 too small. Good stuff, asymptotics.
But let's get back to primes. We'd like to determine, for any given prime p, the largest power of p that divides n!; that is, we want the exponent of p in n!'s unique factorization. We denote this number by p(n!), and we start our investigations with the small case p = 2 and n = 10. Since 10! is the product of ten numbers, 2(10!) can be found by summing the powers-of-2 contributions of those ten numbers; this calculation corresponds to summing the columns of the following array:



A powerful ruler.
(The column sums form what's sometimes called the ruler function ρ(k), because of their similarity to '', the lengths of lines marking fractions of an inch.) The sum of these ten sums is 8; hence 28 divides 10! but 29 doesn't.
There's also another way: We can sum the contributions of the rows. The first row marks the numbers that contribute a power of 2 (and thus are divisible by 2); there are 10/2 = 5 of them. The second row marks those that contribute an additional power of 2; there are 10/4 = 2 of them. And the third row marks those that contribute yet another; there are 10/8 = 1 of them. These account for all contributions, so we have 2(10!) = 5 + 2 + 1 = 8.
For general n this method gives



This sum is actually finite, since the summand is zero when 2k > n. Therefore it has only lg n nonzero terms, and it's computationally quite easy. For instance, when n = 100 we have
2(100!) = 50 + 25 + 12 + 6 + 3 + 1 = 97.
Each term is just the floor of half the previous term. This is true for all n, because as a special case of (3.11) we have n/2k+1 = n/2k/2. It's especially easy to see what's going on here when we write the numbers in binary:


100 =
(1100100)2 =
100


100/2 =
(110010)2 =
50


100/4 =
(11001)2 =
25


100/8 =
(1100)2 =
12


100/16 =
(110)2 =
6


100/32 =
(11)2 =
3


100/64 =
(1)2 =
1


We merely drop the least significant bit from one term to get the next.
The binary representation also shows us how to derive another formula,



where ν2(n) is the number of 1's in the binary representation of n. This simplification works because each 1 that contributes 2m to the value of n contributes 2m-1 + 2m-2 + · · · + 20 = 2m - 1 to the value of 2(n!).
Generalizing our findings to an arbitrary prime p, we have



by the same reasoning as before.
About how large is p(n!)? We get an easy (but good) upper bound by simply removing the floor from the summand and then summing an infinite geometric progression:



For p = 2 and n = 100 this inequality says that 97 < 100. Thus the upper bound 100 is not only correct, it's also close to the true value 97. In fact, the true value n - ν2(n) is ∼ n in general, because ν2(n) ≤ lg n is asymptotically much smaller than n.
When p = 2 and 3 our formulas give 2(n!) ∼ n and 3(n!) ∼ n/2, so it seems reasonable that every once in a while 3(n!) should be exactly half as big as 2(n!). For example, this happens when n = 6 and n = 7, because 6! = 24 · 32 · 5 = 7!/7. But nobody has yet proved that such coincidences happen infinitely often.
The bound on p(n!) in turn gives us a bound on pp(n!), which is p's contribution to n!:
pp(n!) < pn/(p-1) .
And we can simplify this formula (at the risk of greatly loosening the upper bound) by noting that p ≤ 2p-1; hence pn/(p-1) ≤ (2p-1)n/(p-1) = 2n. In other words, the contribution that any prime makes to n! is less than 2n.
We can use this observation to get another proof that there are infinitely many primes. For if there were only the k primes 2, 3, . . . , Pk, then we'd have n! < (2n)k = 2nk for all n > 1, since each prime can contribute at most a factor of 2n - 1. But we can easily contradict the inequality n! < 2nk by choosing n large enough, say n = 22k. Then
n! < 2nk = 222kk = nn/2 ,
contradicting the inequality n! ≥ nn/2 that we derived in (4.22). There are infinitely many primes, still.
We can even beef up this argument to get a crude bound on π(n), the number of primes not exceeding n. Every such prime contributes a factor of less than 2n to n!; so, as before,
n! < 2nπ(n) .
If we replace n! here by Stirling's approximation (4.23), which is a lower bound, and take logarithms, we get
nπ(n) > n lg(n/e) +  lg(2πn);
hence
π(n) > lg(n/e) .
This lower bound is quite weak, compared with the actual value π(n) ∼ n/ln n, because lg(n/e) is much smaller than n/ln n when n is large. But we didn't have to work very hard to get it, and a bound is a bound.


4.5 Relative Primality
When gcd(m, n) = 1, the integers m and n have no prime factors in common and we say that they're relatively prime.
This concept is so important in practice, we ought to have a special notation for it; but alas, number theorists haven't agreed on a very good one yet. Therefore we cry: HEAR US, O MATHEMATICIANS OF THE WORLD! LET US NOT WAIT ANY LONGER! WE CAN MAKE MANY FORMULAS CLEARER BY ADOPTING A NEW NOTATION NOW! LET US AGREE TO WRITE 'm ⊥ n', AND TO SAY "m IS PRIME TO n," IF m AND n ARE RELATIVELY PRIME. In other words, let us declare that
Like perpendicular lines don't have a common direction, perpendicular numbers don't have common factors.



A fraction m/n is in lowest terms if and only if m ⊥ n. Since we reduce fractions to lowest terms by casting out the largest common factor of numerator and denominator, we suspect that, in general,



and indeed this is true. It follows from a more general law, gcd(km, kn) = k gcd(m, n), proved in exercise 14.
The ⊥ relation has a simple formulation when we work with the prime-exponent representations of numbers, because of the gcd rule (4.14):



Furthermore, since mp and np are nonnegative, we can rewrite this as
The dot product is zero, like orthogonal vectors.



And now we can prove an important law by which we can split and combine two ⊥ relations with the same left-hand side:



In view of (4.29), this law is another way of saying that kpmp = 0 and kpnp = 0 if and only if kp(mp + np) = 0, when mp and np are nonnegative.
There's a beautiful way to construct the set of all nonnegative fractions m/n with m ⊥ n, called the Stern-Brocot tree because it was discovered independently by Moritz Stern [339], a German mathematician, and Achille Brocot [40], a French clockmaker. The idea is to start with the two fractions  and then to repeat the following operation as many times as desired:
Interesting how mathematicians will say "discovered" when absolutely anyone else would have said "invented."
Insert  between two adjacent fractions  and .
The new fraction (m + m′)/(n + n′) is called the mediant of m/n and m′/n′. For example, the first step gives us one new entry between  and ,



and the next gives two more:



The next gives four more,



and then we'll get 8, 16, and so on. The entire array can be regarded as an infinite binary tree structure whose top levels look like this:
I guess 1/0 is infinity, "in lowest terms."



Each fraction is , where  is the nearest ancestor above and to the left, and  is the nearest ancestor above and to the right. (An "ancestor" is a fraction that's reachable by following the branches upward.) Many patterns can be observed in this tree.
Why does this construction work? Why, for example, does each mediant fraction (m + m′)/(n + n′) turn out to be in lowest terms when it appears in this tree? (If m, m′, n, and n′ were all odd, we'd get even/even; somehow the construction guarantees that fractions with odd numerators and denominators never appear next to each other.) And why do all possible fractions m/n occur exactly once? Why can't a particular fraction occur twice, or not at all?
Conserve parody.
All of these questions have amazingly simple answers, based on the following fundamental fact: If m/n and m′/n′ are consecutive fractions at any stage of the construction, we have



This relation is true initially (1 · 1 - 0 · 0 = 1); and when we insert a new mediant (m + m′)/(n + n′), the new cases that need to be checked are
(m + m′)n - m(n + n′) = 1;
m′(n + n′) - (m + m′)n′ = 1.
Both of these equations are equivalent to the original condition (4.31) that they replace. Therefore (4.31) is invariant at all stages of the construction.
Furthermore, if m/n < m′/n′ and if all values are nonnegative, it's easy to verify that
m/n < (m + m′)/(n + n′) < m′/n′ .
A mediant fraction isn't halfway between its progenitors, but it does lie somewhere in between. Therefore the construction preserves order, and we couldn't possibly get the same fraction in two different places.
True, but if you get a compound fracture you'd better go see a doctor.
One question still remains. Can any positive fraction a/b with a ⊥ b possibly be omitted? The answer is no, because we can confine the construction to the immediate neighborhood of a/b, and in this region the behavior is easy to analyze: Initially we have



where we put parentheses around  to indicate that it's not really present yet. Then if at some stage we have



the construction forms (m + m′)/(n + n′) and there are three cases. Either (m + m′)/(n + n′) = a/b and we win; or (m + m′)/(n + n′) < a/b and we can set m ← m + m′, n ← n + n′; or (m + m′)/(n + n′) > a/b and we can set m′ ← m + m′, n′ ← n + n′. This process cannot go on indefinitely, because the conditions



imply that
an - bm ≥ 1          and          bm′ - an′ ≥ 1;
hence
(m′ + n′)(an - bm) + (m + n)(bm′ - an′) ≥ m′ + n′ + m + n;
and this is the same as a + b ≥ m′ + n′ + m + n by (4.31). Either m or n or m′ or n′ increases at each step, so we must win after at most a + b steps.
The Farey series of order N, denoted by , is the set of all reduced fractions between 0 and 1 whose denominators are N or less, arranged in increasing order. For example, if N = 6 we have



We can obtain  in general by starting with  and then inserting mediants whenever it's possible to do so without getting a denominator that is too large. We don't miss any fractions in this way, because we know that the Stern-Brocot construction doesn't miss any, and because a mediant with denominator ≤ N is never formed from a fraction whose denominator is > N. (In other words,  defines a subtree of the Stern-Brocot tree, obtained by pruning off unwanted branches.) It follows that m′n - mn′ = 1 whenever m/n and m′/n′ are consecutive elements of a Farey series.
This method of construction reveals that  can be obtained in a simple way from : We simply insert the fraction (m + m′)/N between consecutive fractions m/n, m′/n′ of  whose denominators sum to N. For example, it's easy to obtain  from the elements of , by inserting  according to the stated rule:



When N is prime, N - 1 new fractions will appear; but otherwise we'll have fewer than N - 1, because this process generates only numerators that are relatively prime to N.
Long ago in (4.5) we proved—in different words—that whenever m ⊥ n and 0 < m ≤ n we can find integers a and b such that



(Actually we said m′m + n′n = gcd(m, n), but we can write 1 for gcd(m, n), a for m′, and b for -n′.) The Farey series gives us another proof of (4.32), because we can let b/a be the fraction that precedes m/n in n. Thus (4.5) is just (4.31) again. For example, one solution to 3a - 7b = 1 is a = 5, b = 2, since  precedes  in 7. This construction implies that we can always find a solution to (4.32) with 0 ≤ b < a ≤ n, if 0 < m ≤ n. Similarly, if 0 ≤ n < m and m ⊥ n, we can solve (4.32) with 0 < a ≤ b ≤ m by letting a/b be the fraction that follows n/m in .
Sequences of three consecutive terms in a Farey series have an amazing property that is proved in exercise 61. But we had better not discuss the Farey series any further, because the entire Stern-Brocot tree turns out to be even more interesting.
Farey 'nough.
We can, in fact, regard the Stern-Brocot tree as a number system for representing rational numbers, because each positive, reduced fraction occurs exactly once. Let's use the letters L and R to stand for going down to the left or right branch as we proceed from the root of the tree to a particular fraction; then a string of L's and R's uniquely identifies a place in the tree. For example, LRRL means that we go left from  down to , then right to , then right to , then left to . We can consider LRRL to be a representation of . Every positive fraction gets represented in this way as a unique string of L's and R's.
Well, actually there's a slight problem: The fraction  corresponds to the empty string, and we need a notation for that. Let's agree to call it I, because that looks something like 1 and it stands for "identity."
This representation raises two natural questions: (1) Given positive integers m and n with m ⊥ n, what is the string of L's and R's that corresponds to m/n? (2) Given a string of L's and R's, what fraction corresponds to it? Question 2 seems easier, so let's work on it first. We define
f(S) = fraction corresponding to S
when S is a string of L's and R's. For example, f(LRRL) = .
According to the construction, f(S) = (m + m′)/(n + n′) if m/n and m′/n′ are the closest fractions preceding and following f(S) in the upper levels of the tree. Initially m/n = 0/1 and m′/n′ = 1/0; then we successively replace either m/n or m′/n′ by the mediant (m + m′)/(n + n′) as we move right or left in the tree, respectively.
How can we capture this behavior in mathematical formulas that are easy to deal with? A bit of experimentation suggests that the best way is to maintain a 2 × 2 matrix



that holds the four quantities involved in the ancestral fractions m/n and m′/n′ enclosing f(S). We could put the m's on top and the n's on the bottom, fractionwise; but this upside-down arrangement works out more nicely because we have  when the process starts, and  is traditionally called the identity matrix I.
A step to the left replaces n′ by n + n′ and m′ by m + m′; hence



(This is a special case of the general rule



for multiplying 2 × 2 matrices.) Similarly it turns out that



If you're clueless about matrices, don't panic; this book uses them only here.
Therefore if we define L and R as 2 × 2 matrices,



we get the simple formula M(S) = S, by induction on the length of S. Isn't that nice? (The letters L and R serve dual roles, as matrices and as letters in the string representation.) For example,



the ancestral fractions that enclose LRRL =  are  and . And this construction gives us the answer to Question 2:



How about Question 1? That's easy, now that we understand the fundamental connection between tree nodes and 2 × 2 matrices. Given a pair of positive integers m and n, with m ⊥ n, we can find the position of m/n in the Stern-Brocot tree by "binary search" as follows:
S := I;
while m/n ≠ f(S) do
if m/n < f(S) then (output(L); S := SL)
                       else  (output(R); S := SR).
This outputs the desired string of L's and R's.
There's also another way to do the same job, by changing m and n instead of maintaining the state S. If S is any 2 × 2 matrix, we have
f(RS) = f(S) + 1
because RS is like S but with the top row added to the bottom row. (Let's look at it in slow motion:



hence f(S) = (m + m′)/(n + n′) and f(RS) = ((m + n) + (m′ + n′)) /(n + n′).) If we carry out the binary search algorithm on a fraction m/n with m > n, the first output will be R; hence the subsequent behavior of the algorithm will have f(S) exactly 1 greater than if we had begun with (m - n)/n instead of m/n. A similar property holds for L, and we have






This means that we can transform the binary search algorithm to the following matrix-free procedure:
while m ≠ n do
if m < n then (output(L); n := n - m)
                         else  (output(R); m := m - n) .
For example, given m/n = 5/7, we have successively
      m =  5    5    3    1    1
      n  =  7    2    2    2    1
output    L    R    R    L
in the simplified algorithm.
Irrational numbers don't appear in the Stern-Brocot tree, but all the rational numbers that are "close" to them do. For example, if we try the binary search algorithm with the number e = 2.71828 . . . , instead of with a fraction m/n, we'll get an infinite string of L's and R's that begins
RRLRRLRLLLLRLRRRRRRLRLLLLLLLLRLR . . . .
We can consider this infinite string to be the representation of e in the Stern-Brocot number system, just as we can represent e as an infinite decimal 2.718281828459. . . or as an infinite binary fraction (10.101101111110 . . . )2. Incidentally, it turns out that e's representation has a regular pattern in the Stern-Brocot system:
Hermann Minkowski illustrated this remarkable binary representation at the International Congress of Mathematicians in Heidelberg, 1904.
e = RL0RLR2LRL4RLR6LRL8RLR10LRL12RL . . . ;
this is equivalent to a special case of something that Euler [105] discovered when he was 24 years old.
From this representation we can deduce that the fractions



are the simplest rational upper and lower approximations to e. For if m/n does not appear in this list, then some fraction in this list whose numerator is ≤ m and whose denominator is ≤ n lies between m/n and e. For example,  is not as simple an approximation as  , which appears in the list and is closer to e. We can see this because the Stern-Brocot tree not only includes all rationals, it includes them in order, and because all fractions with small numerator and denominator appear above all less simple ones. Thus,  is less than , which is less than e = RRLRRLR . . . . Excellent approximations can be found in this way. For example, ; we obtained this fraction from the first 16 letters of e's Stern-Brocot representation, and the accuracy is about what we would get with 16 bits of e's binary representation.
We can find the infinite representation of an irrational number α by a simple modification of the matrix-free binary search procedure:
if α < 1 then (output(L);  α := α/(1 - α))
      else  (output(R);  α := α - 1).
(These steps are to be repeated infinitely many times, or until we get tired.) If α is rational, the infinite representation obtained in this way is the same as before but with RL∞ appended at the right of α's (finite) representation. For example, if α = 1, we get RLLL . . . , corresponding to the infinite sequence of fractions  , which approach 1 in the limit. This situation is exactly analogous to ordinary binary notation, if we think of L as 0 and R as 1: Just as every real number x in [0 . . 1) has an infinite binary representation (.b1b2b3 . . . )2 not ending with all 1's, every real number α in [0.. ∞) has an infinite Stern-Brocot representation B1B2B3 . . . not ending with all R's. Thus we have a one-to-one order-preserving correspondence between [0 . . 1) and [0 .. ∞) if we let 0 ↔ L and 1 ↔ R.
There's an intimate relationship between Euclid's algorithm and the Stern-Brocot representations of rationals. Given α = m/n, we get m/n R's, then n/(m mod n) L's, then (m mod n)/(n mod (m mod n)) R's, and so on. These numbers m mod n, n mod (m mod n), . . . are just the values examined in Euclid's algorithm. (A little fudging is needed at the end to make sure that there aren't infinitely many R's.) We will explore this relationship further in Chapter 6.


4.6 'MOD': The Congruence Relation
Modular arithmetic is one of the main tools provided by number theory. We got a glimpse of it in Chapter 3 when we used the binary operation 'mod', usually as one operation amidst others in an expression. In this chapter we will use 'mod' also with entire equations, for which a slightly different notation is more convenient:



"Numerorum congruentiam hoc signo,≡, in posterum denotabimus, modulum ubi opus erit in clausulis adiungentes, -16 ≡ 9 (mod. 5), -7 ≡ 15 (mod. 11)."
—C. F. Gauss [142]
For example, 9 ≡ -16 (mod 5), because 9 mod 5 = 4 = (-16) mod 5. The formula 'a ≡ b (mod m)' can be read "a is congruent to b modulo m." The definition makes sense when a, b, and m are arbitrary real numbers, but we almost always use it with integers only.
Since x mod m differs from x by a multiple of m, we can understand congruences in another way:



For if a mod m = b mod m, then the definition of 'mod' in (3.21) tells us that a - b = a mod m + km - (b mod m + lm) = (k - l)m for some integers k and l. Conversely if a - b = km, then a = b if m = 0; otherwise


a mod m = a - a/m m
= b + km - (b + km)/m m


 
= b - b/m m = b mod m.


The characterization of ≡ in (4.36) is often easier to apply than (4.35). For example, we have 8 ≡ 23 (mod 5) because 8 - 23 = -15 is a multiple of 5; we don't have to compute both 8 mod 5 and 23 mod 5.
The congruence sign ' ≡ ' looks conveniently like ' = ', because congruences are almost like equations. For example, congruence is an equivalence relation; that is, it satisfies the reflexive law 'a ≡ a', the symmetric law 'a ≡ b ⇒ b ≡ a', and the transitive law 'a ≡ b ≡ c ⇒ a ≡ c'. All these properties are easy to prove, because any relation '≡' that satisfies 'a ≡ b ⇔ f(a) = f(b)' for some function f is an equivalence relation. (In our case, f(x) = x mod m.) Moreover, we can add and subtract congruent elements without losing congruence:
"I feel fine today modulo a slight headache."
—The Hacker's Dictionary [337]
a ≡ b     and      c ≡ d                    a + c ≡ b + d           (mod m);
a ≡ b     and      c ≡ d                    a - c ≡ b - d           (mod m).
For if a - b and c - d are both multiples of m, so are (a + c) - (b + d) = (a - b) + (c - d) and (a - c) - (b - d) = (a - b) - (c - d). Incidentally, it isn't necessary to write '(mod m)' once for every appearance of ' ≡ '; if the modulus is constant, we need to name it only once in order to establish the context. This is one of the great conveniences of congruence notation.
Multiplication works too, provided that we are dealing with integers:
a ≡ b     and     c ≡ d                ac ≡ bd     (mod m),
                                                                       integers b, c.
Proof: ac - bd = (a - b)c + b(c - d). Repeated application of this multiplication property now allows us to take powers:
a ≡ b          an ≡ bn          (mod m),     integers a, b;
                                                                  integer n ≥ 0.
For example, since 2 ≡ -1 (mod 3), we have 2n ≡ (-1)n (mod 3); this means that 2n - 1 is a multiple of 3 if and only if n is even.
Thus, most of the algebraic operations that we customarily do with equations can also be done with congruences. Most, but not all. The operation of division is conspicuously absent. If ad ≡ bd (mod m), we can't always conclude that a ≡ b. For example, 3·2 ≡ 5·2 (mod 4), but 3 ≢ 5.
We can salvage the cancellation property for congruences, however, in the common case that d and m are relatively prime:



For example, it's legit to conclude from 15 ≡ 35 (mod m) that 3 ≡ 7 (mod m), unless the modulus m is a multiple of 5.
To prove this property, we use the extended gcd law (4.5) again, finding d′ and m′ such that d′d + m′m = 1. Then if ad ≡ bd we can multiply both sides of the congruence by d′, obtaining ad′d ≡ bd′d. Since d′d ≡ 1, we have ad′d ≡ a and bd′d ≡ b; hence a ≡ b. This proof shows that the number d′ acts almost like 1/d when congruences are considered (mod m); therefore we call it the "inverse of d modulo m."
Another way to apply division to congruences is to divide the modulus as well as the other numbers:



This law holds for all real a, b, d, and m, because it depends only on the distributive law (a mod m)d = ad mod md: We have a mod m = b mod m ⇔ (a mod m)d = (b mod m)d ⇔ ad mod md = bd mod md. Thus, for example, from 3 · 2 ≡ 5 · 2 (mod 4) we conclude that 3 ≡ 5 (mod 2).
We can combine (4.37) and (4.38) to get a general law that changes the modulus as little as possible:



For we can multiply ad ≡ bd by d′, where d′d + m′m = gcd(d, m); this gives the congruence a · gcd(d, m) ≡ b · gcd(d, m) (mod m), which can be divided by gcd(d, m).
Let's look a bit further into this idea of changing the modulus. If we know that a ≡ b (mod 100), then we also must have a ≡ b (mod 10), or modulo any divisor of 100. It's stronger to say that a - b is a multiple of 100 than to say that it's a multiple of 10. In general,



because any multiple of md is a multiple of m.
Conversely, if we know that a ≡ b with respect to two small moduli, can we conclude that a ≡ b with respect to a larger one? Yes; the rule is
Modulitos?



For example, if we know that a ≡ b modulo 12 and 18, we can safely conclude that a ≡ b (mod 36). The reason is that if a - b is a common multiple of m and n, it is a multiple of lcm(m, n). This follows from the principle of unique factorization.
The special case m ⊥ n of this law is extremely important, because lcm(m, n) = mn when m and n are relatively prime. Therefore we will state it explicitly:



For example, a ≡ b (mod 100) if and only if a ≡ b (mod 25) and a ≡ b (mod 4). Saying this another way, if we know x mod 25 and x mod 4, then we have enough facts to determine x mod 100. This is a special case of the Chinese Remainder Theorem (see exercise 30), so called because it was discovered by Sun Ts in China, about A.D. 350.
The moduli m and n in (4.42) can be further decomposed into relatively prime factors until every distinct prime has been isolated. Therefore
a ≡ b (mod m)                    a ≡ b (mod pmp)   for all p,
if the prime factorization (4.11) of m is Πppmp. Congruences modulo powers of primes are the building blocks for all congruences modulo integers.


4.7 Independent Residues
One of the important applications of congruences is a residue number system, in which an integer x is represented as a sequence of residues (or remainders) with respect to moduli that are prime to each other:
Res(x) = (x mod m1, . . . , x mod mr),      if mj ⊥ mk for 1 ≤ j < k ≤ r.
Knowing x mod m1, . . . , x mod mr doesn't tell us everything about x. But it does allow us to determine x mod m, where m is the product m1 . . . mr. In practical applications we'll often know that x lies in a certain range; then we'll know everything about x if we know x mod m and if m is large enough.
For example, let's look at a small case of a residue number system that has only two moduli, 3 and 5:



Each ordered pair (x mod 3, x mod 5) is different, because x mod 3 = y mod 3 and x mod 5 = y mod 5 if and only if x mod 15 = y mod 15.
We can perform addition, subtraction, and multiplication on the two components independently, because of the rules of congruences. For example, if we want to multiply 7 = (1, 2) by 13 = (1, 3) modulo 15, we calculate 1·1 mod 3 = 1 and 2·3 mod 5 = 1. The answer is (1, 1) = 1; hence 7·13 mod 15 must equal 1. Sure enough, it does.
This independence principle is useful in computer applications, because different components can be worked on separately (for example, by different computers). If each modulus mk is a distinct prime pk, chosen to be slightly less than 231, then a computer whose basic arithmetic operations handle integers in the range [-231 . . 231) can easily compute sums, differences, and products modulo pk. A set of r such primes makes it possible to add, subtract, and multiply "multiple-precision numbers" of up to almost 31r bits, and the residue system makes it possible to do this faster than if such large numbers were added, subtracted, or multiplied in other ways.
For example, the Mersenne prime 231 - 1 works well.
We can even do division, in appropriate circumstances. For example, suppose we want to compute the exact value of a large determinant of integers. The result will be an integer D, and bounds on |D| can be given based on the size of its entries. But the only fast ways known for calculating determinants require division, and this leads to fractions (and loss of accuracy, if we resort to binary approximations). The remedy is to evaluate D mod pk = Dk, for various large primes pk. We can safely divide modulo pk unless the divisor happens to be a multiple of pk. That's very unlikely, but if it does happen we can choose another prime. Finally, knowing Dk for sufficiently many primes, we'll have enough information to determine D.
But we haven't explained how to get from a given sequence of residues (x mod m1, . . . , x mod mr) back to x mod m. We've shown that this conversion can be done in principle, but the calculations might be so formidable that they might rule out the idea in practice. Fortunately, there is a reasonably simple way to do the job, and we can illustrate it in the situation (x mod 3, x mod 5) shown in our little table. The key idea is to solve the problem in the two cases (1, 0) and (0, 1); for if (1, 0) = a and (0, 1) = b, then (x, y) = (ax + by) mod 15, since congruences can be multiplied and added.
In our case a = 10 and b = 6, by inspection of the table; but how could we find a and b when the moduli are huge? In other words, if m ⊥ n, what is a good way to find numbers a and b such that the equations
a mod m = 1,     a mod n = 0,     b mod m = 0,     b mod n = 1
all hold? Once again, (4.5) comes to the rescue: With Euclid's algorithm, we can find m′ and n′ such that
m′m + n′n = 1.
Therefore we can take a = n′n and b = m′m, reducing them both mod mn if desired.
Further tricks are needed in order to minimize the calculations when the moduli are large; the details are beyond the scope of this book, but they can be found in [208, page 274]. Conversion from residues to the corresponding original numbers is feasible, but it is sufficiently slow that we save total time only if a sequence of operations can all be done in the residue number system before converting back.
Let's firm up these congruence ideas by trying to solve a little problem: How many solutions are there to the congruence



if we consider two solutions x and x′ to be the same when x ≡ x′?
According to the general principles explained earlier, we should consider first the case that m is a prime power, pk, where k > 0. Then the congruence x2 ≡ 1 can be written
(x - 1)(x + 1) ≡ 0 (mod pk) ,
so p must divide either x - 1 or x + 1, or both. But p can't divide both x - 1 and x + 1 unless p = 2; we'll leave that case for later. If p > 2, then pk\(x - 1)(x + 1) ⇔ pk\(x - 1) or pk\(x + 1); so there are exactly two solutions, x ≡ +1 and x ≡ -1.
The case p = 2 is a little different. If 2k\(x - 1)(x + 1) then either x - 1 or x + 1 is divisible by 2 but not by 4, so the other one must be divisible by 2k-1. This means that we have four solutions when k ≥ 3, namely x ≡ ±1 and x ≡ 2k-1 ± 1. (For example, when pk = 8 the four solutions are x ≡ 1, 3, 5, 7 (mod 8); it's often useful to know that the square of any odd integer has the form 8n + 1.)
Now x2 ≡ 1 (mod m) if and only if x2 ≡ 1 (mod pmp) for all primes p with mp > 0 in the complete factorization of m. Each prime is independent of the others, and there are exactly two possibilities for x mod pmp except when p = 2. Therefore if m has exactly r different prime divisors, the total number of solutions to x2 ≡ 1 is 2r, except for a correction when m is even. The exact number in general is
All primes are odd except 2, which is the oddest of all.



For example, there are four "square roots of unity modulo 12," namely 1, 5, 7, and 11. When m = 15 the four are those whose residues mod 3 and mod 5 are ±1, namely (1, 1), (1, 4), (2, 1), and (2, 4) in the residue number system. These solutions are 1, 4, 11, and 14 in the ordinary (decimal) number system.


4.8 Additional Applications
There's some unfinished business left over from Chapter 3: We wish to prove that the m numbers



consist of precisely d copies of the m/d numbers
0, d, 2d,   ...,   m - d
in some order, where d = gcd(m, n). For example, when m = 12 and n = 8 we have d = 4, and the numbers are 0, 8, 4, 0, 8, 4, 0, 8, 4, 0, 8, 4.
The first part of the proof—to show that we get d copies of the first m/d values—is now trivial. We have
Mathematicians love to say that things are trivial.
jn ≡ kn (mod m)                j(n/d) ≡ k(n/d) (mod m/d)
by (4.38); hence we get d copies of the values that occur when 0 ≤ k < m/d.
Now we must show that those m/d numbers are {0, d, 2d, . . . , m - d} in some order. Let's write m = m′d and n = n′d. Then kn mod m = d(kn′ mod m′), by the distributive law (3.23); so the values that occur when 0 ≤ k < m′ are d times the numbers
0 mod m′, n′ mod m′, 2n′ mod m′, . . . , (m′ - 1)n′ mod m′ .
But we know that m′ ⊥ n′ by (4.27); we've divided out their gcd. Therefore we need only consider the case d = 1, namely the case that m and n are relatively prime.
So let's assume that m ⊥ n. In this case it's easy to see that the numbers (4.45) are just {0, 1, . . . , m - 1} in some order, by using the "pigeonhole principle." This principle states that if m pigeons are put into m pigeonholes, there is an empty hole if and only if there's a hole with more than one pigeon. (Dirichlet's box principle, proved in exercise 3.8, is similar.) We know that the numbers (4.45) are distinct, because
jn ≡ kn (mod m)                j ≡ k (mod m)
when m ⊥ n; this is (4.37). Therefore the m different numbers must fill all the pigeonholes 0, 1, . . . , m - 1. Therefore the unfinished business of Chapter 3 is finished.
The proof is complete, but we can prove even more if we use a direct method instead of relying on the indirect pigeonhole argument. If m ⊥ n and if a value j ∈ [0 . . m) is given, we can explicitly compute k ∈ [0 . . m) such that kn mod m = j by solving the congruence
kn ≡ j     (mod m)
for k. We simply multiply both sides by n′, where m′m + n′n = 1, to get
k ≡ jn′     (mod m);
hence k = jn′ mod m.
We can use the facts just proved to establish an important result discovered by Pierre de Fermat in 1640. Fermat was a great mathematician who contributed to the discovery of calculus and many other parts of mathematics. He left notebooks containing dozens of theorems stated without proof, and each of those theorems has subsequently been verified—including one that became the most famous of all, because it baffled the world's best mathematicians for 350 years. The famous one, called "Fermat's Last Theorem," states that



for all positive integers a, b, c, and n, when n > 2. (Of course there are lots of solutions to the equations a + b = c and a2 + b2 = c2.) Andrew Wiles has apparently settled the question at long last; his intricate, epoch-making proof of (4.46) appears in Annals of Mathematics (2) 141 (1995), 443-551.
NEWS FLASH
Euler [115] conjectured that a4 + b4 + c4 ≠ d4, but Noam Elkies [92] found infinitely many solutions in August, 1987.
Now Roger Frye has done an exhaustive computer search, proving (after about 110 hours on a Connection Machine) that the only solution with d < 1000000 is: 958004 + 2175194 + 4145604 = 4224814.
Fermat's theorem of 1640 is much easier to verify. It's now called Fermat's Little Theorem (or just Fermat's theorem, for short), and it states that



Proof: As usual, we assume that p denotes a prime. We know that the p - 1 numbers n mod p, 2n mod p, . . . , (p - 1)n mod p are the numbers 1, 2, . . . , p - 1 in some order. Therefore if we multiply them together we get


n · (2n) · . . . · ((p - 1)n)


 
≡ (n mod p) · (2n mod p) · . . . · ((p - 1)n mod p)


 
≡ (p - 1)!,


where the congruence is modulo p. This means that
(p - 1)! np-1 ≡ (p - 1)!     (mod p),
and we can cancel the (p - 1)! since it's not divisible by p. QED.
An alternative form of Fermat's theorem is sometimes more convenient:



This congruence holds for all integers n. The proof is easy: If n ⊥ p we simply multiply (4.47) by n. If not, p\n, so np ≡ 0 ≡ n.
In the same year that he discovered (4.47), Fermat wrote a letter to Mersenne, saying he suspected that the number
fn = 22n + 1
would turn out to be prime for all n ≥ 0. He knew that the first five cases gave primes:
". . . laquelle proposition, si elle est vraie, est de très grand usage."
—P. de Fermat [121]
21+1 = 3; 22+1 = 5; 24+1 = 17; 28+1 = 257; 216+1 = 65537;
but he couldn't see how to prove that the next case, 232 + 1 = 4294967297, would be prime.
It's interesting to note that Fermat could have proved that 232 + 1 is not prime, using his own recently discovered theorem, if he had taken time to perform a few dozen multiplications: We can set n = 3 in (4.47), deducing that
3232   ≡ 1   (mod 232 + 1),        if 232 + 1 is prime.
And it's possible to test this relation by hand, beginning with 3 and squaring 32 times, keeping only the remainders mod 232 + 1. First we have 32 = 9, then 322 = 81, then 323 = 6561, and so on until we reach
If this is Fermat's Little Theorem, the other one was last but not least.
3232   ≡  3029026160          (mod 232 + 1) .
The result isn't 1, so 232 + 1 isn't prime. This method of disproof gives us no clue about what the factors might be, but it does prove that factors exist. (They are 641 and 6700417, first found by Euler in 1732 [102].)
If 3232 had turned out to be 1, modulo 232 + 1, the calculation wouldn't have proved that 232 + 1 is prime; it just wouldn't have disproved it. But exercise 47 discusses a converse to Fermat's theorem by which we can prove that large prime numbers are prime, without doing an enormous amount of laborious arithmetic.
We proved Fermat's theorem by cancelling (p - 1)! from both sides of a congruence. It turns out that (p - 1)! is always congruent to -1, modulo p; this is part of a classical result known as Wilson's theorem:



One half of this theorem is trivial: If n > 1 is not prime, it has a prime divisor p that appears as a factor of (n - 1)!, so (n - 1)! cannot be congruent to -1. (If (n-1)! were congruent to -1 modulo n, it would also be congruent to -1 modulo p, but it isn't.)
The other half of Wilson's theorem states that (p - 1)! ≡ -1 (mod p). We can prove this half by pairing up numbers with their inverses mod p. If n ⊥ p, we know that there exists n′ such that
n′n ≡ 1       (mod p);
here n′ is the inverse of n, and n is also the inverse of n′. Any two inverses of n must be congruent to each other, since nn′ ≡ nn″ implies n′ ≡ n″.
If p is prime, is p′ prime prime?
Now suppose we pair up each number between 1 and p-1 with its inverse. Since the product of a number and its inverse is congruent to 1, the product of all the numbers in all pairs of inverses is also congruent to 1; so it seems that (p - 1)! is congruent to 1. Let's check, say for p = 5. We get 4! = 24; but this is congruent to 4, not 1, modulo 5. Oops—what went wrong? Let's take a closer look at the inverses:
1′ = 1,          2′ = 3,          3′ = 2,          4′ = 4.
Ah so; 2 and 3 pair up but 1 and 4 don't—they're their own inverses.
To resurrect our analysis we must determine which numbers are their own inverses. If x is its own inverse, then x2 ≡ 1 (mod p); and we have already proved that this congruence has exactly two roots when p > 2. (If p = 2 it's obvious that (p - 1)! ≡ -1, so we needn't worry about that case.) The roots are 1 and p - 1, and the other numbers (between 1 and p - 1) pair up; hence
(p - 1)! ≡ 1 · (p - 1) ≡ -1,
as desired.
Unfortunately, we can't compute factorials efficiently, so Wilson's theorem is of no use as a practical test for primality. It's just a theorem.


4.9 PHI and MU
How many of the integers {0, 1, . . . , m - 1} are relatively prime to m? This is an important quantity called φ(m), the "totient" of m (so named by J. J. Sylvester [347], a British mathematician who liked to invent new words). We have φ(1) = 1, φ(p) = p - 1, and φ(m) < m - 1 for all composite numbers m.
The φ function is called Euler's totient function, because Euler was the first person to study it. Euler discovered, for example, that Fermat's theorem (4.47) can be generalized to nonprime moduli in the following way:



"Si fuerit N ad x numerus primus et n numerus partium ad N primarum, tum potestas xn unitate minuta semper per numerum N erit divisibilis."
—L. Euler [111]
(Exercise 32 asks for a proof of Euler's theorem.)
If m is a prime power pk, it's easy to compute φ(m), because we have . The multiples of p in {0, 1, . . . , pk - 1} are {0, p, 2p, . . . , pk - p}; hence there are pk-1 of them, and φ(pk) counts what is left:
φ(pk) = pk - pk -1 .
Notice that this formula properly gives φ(p) = p - 1 when k = 1.
If m > 1 is not a prime power, we can write m = m1m2 where m1 ⊥ m2. Then the numbers 0 ≤ n < m can be represented in a residue number system as (n mod m1, n mod m2). We have
n ⊥ m                    n mod m1 ⊥ m1     and     n mod m2 ⊥ m2
by (4.30) and (4.4). Hence, n mod m is "good" if and only if n mod m1 and n mod m2 are both "good," if we consider relative primality to be a virtue. The total number of good values modulo m can now be computed, recursively: It is φ(m1)φ(m2), because there are φ(m1) good ways to choose the first component n mod m1 and φ(m2) good ways to choose the second component n mod m2 in the residue representation.
For example, φ(12) = φ(4)φ(3) = 2·2 = 4, because n is prime to 12 if and only if n mod 4 = (1 or 3) and n mod 3 = (1 or 2). The four values prime to 12 are (1, 1), (1, 2), (3, 1), (3, 2) in the residue number system; they are 1, 5, 7, 11 in ordinary decimal notation. Euler's theorem states that n4 ≡ 1 (mod 12) whenever n ⊥ 12.
"Si sint A et B numeri inter se primi et numerus partium ad A primarum sit = a, numerus vero partium ad B primarum sit = b, tum numerus partium ad productum AB primarum erit = ab."
—L. Euler [111]
A function f(m) of positive integers is called multiplicative if f(1) = 1 and



We have just proved that φ(m) is multiplicative. We've also seen another instance of a multiplicative function earlier in this chapter: The number of incongruent solutions to x2 ≡ 1 (mod m) is multiplicative. Still another example is f(m) = mα for any power α.
A multiplicative function is defined completely by its values at prime powers, because we can decompose any positive integer m into its prime-power factors, which are relatively prime to each other. The general formula



holds if and only if f is multiplicative.
In particular, this formula gives us the value of Euler's totient function for general m:



For example, .
Now let's look at an application of the φ function to the study of rational numbers mod 1. We say that the fraction m/n is basic if 0 ≤ m < n. Therefore φ(n) is the number of reduced basic fractions with denominator n; and the Farey series  contains all the reduced basic fractions with denominator n or less, as well as the non-basic fraction .
The set of all basic fractions with denominator 12, before reduction to lowest terms, is



Reduction yields



and we can group these fractions by their denominators:



What can we make of this? Well, every divisor d of 12 occurs as a denominator, together with all φ(d) of its numerators. The only denominators that occur are divisors of 12. Thus
φ(1) + φ(2) + φ(3) + φ(4) + φ(6) + φ(12) = 12.
A similar thing will obviously happen if we begin with the unreduced fractions  for any m, hence



We said near the beginning of this chapter that problems in number theory often require sums over the divisors of a number. Well, (4.54) is one such sum, so our claim is vindicated. (We will see other examples.)
Now here's a curious fact: If f is any function such that the sum



is multiplicative, then f itself is multiplicative. (This result, together with (4.54) and the fact that g(m) = m is obviously multiplicative, gives another reason why φ(m) is multiplicative.) We can prove this curious fact by induction on m: The basis is easy because f(1) = g(1) = 1. Let m > 1, and assume that f(m1m2) = f(m1)f(m2) whenever m1 ⊥ m2 and m1m2 < m. If m = m1m2 and m1 ⊥ m2, we have



and d1 ⊥ d2 since all divisors of m1 are relatively prime to all divisors of m2. By the induction hypothesis, f(d1d2) = f(d1)f(d2) except possibly when d1 = m1 and d2 = m2; hence we obtain



But this equals g(m1m2) = g(m1)g(m2), so f(m1m2) = f(m1)f(m2).
Conversely, if f(m) is multiplicative, the corresponding sum-over-divisors function g(m) = ∑d\m f(d) is always multiplicative. In fact, exercise 33 shows that even more is true. Hence the curious fact and its converse are both facts.
The Möbius function μ(m), named after the nineteenth-century mathematician August Möbius who also had a famous band, can be defined for all integers m ≥ 1 by the equation



This equation is actually a recurrence, since the left-hand side is a sum consisting of μ(m) and certain values of μ(d) with d < m. For example, if we plug in m = 1, 2, . . . , 12 successively we can compute the first twelve values:



Richard Dedekind [77] and Joseph Liouville [251] noticed the following important "inversion principle" in 1857:



According to this principle, the μ function gives us a new way to understand any function f(m) for which we know ∑d\m f(d).
Now is a good time to try warmup exercise 11.
The proof of (4.56) uses two tricks (4.7) and (4.9) that we described near the beginning of this chapter: If g(m) = ∑d\m f(d) then



The other half of (4.56) is proved similarly (see exercise 12).
Relation (4.56) gives us a useful property of the Möbius function, and we have tabulated the first twelve values; but what is the value of μ(m) when m is large? How can we solve the recurrence (4.55)? Well, the function g(m) = [m = 1] is obviously multiplicative—after all, it's zero except when m = 1. So the Möbius function defined by (4.55) must be multiplicative, by the curious fact we proved a minute or two ago. Therefore we can figure out what μ(m) is if we compute μ(pk).
Depending on how fast you read.
When m = pk, (4.55) says that
μ(1) + μ(p) + μ(p2) + · · · + μ(pk) = 0
for all k ≥ 1, since the divisors of pk are 1, . . . , pk. It follows that
µ(p) = -1;          µ(pk) = 0      for k > 1.
Therefore by (4.52), we have the general formula



That's μ.
If we regard (4.54) as a recurrence for the function φ(m), we can solve that recurrence by applying the Dedekind-Liouville rule (4.56). We get



For example,


φ(12)
= µ(1)·12 + µ(2)·6 + µ(3)·4 + µ(4)·3 + µ(6)·2 + µ(12)·1


 
= 12 - 6 - 4 + 0 + 2 + 0 = 4.


If m is divisible by r different primes, say {p1, . . . , pr}, the sum (4.58) has only 2r nonzero terms, because the μ function is often zero. Thus we can see that (4.58) checks with formula (4.53), which reads



if we multiply out the r factors (1 - 1/pj), we get precisely the 2r nonzero terms of (4.58). The advantage of the Möbius function is that it applies in many situations besides this one.
For example, let's try to figure out how many fractions are in the Farey series . This is the number of reduced fractions in [0 . . 1] whose denominators do not exceed n, so it is 1 greater than Φ(n) where we define



(We must add 1 to Φ(n) because of the final fraction .) The sum in (4.59) looks difficult, but we can determine Φ(x) indirectly by observing that



for all real x ≥ 0. Why does this identity hold? Well, it's a bit awesome yet not really beyond our ken. There are  basic fractions m/n with 0 ≤ m < n ≤ x, counting both reduced and unreduced fractions; that gives us the right-hand side. The number of such fractions with gcd(m, n) = d is Φ(x/d), because such fractions are m′/n′ with 0 ≤ m′ < n′ ≤ x/d after replacing m by m′d and n by n′d. So the left-hand side counts the same fractions in a different way, and the identity must be true.
Let's look more closely at the situation, so that equations (4.59) and (4.60) become clearer. The definition of Φ(x) implies that Φ(x) = Φ (x); but it turns out to be convenient to define Φ(x) for arbitrary real values, not just for integers. At integer values we have the table
(This extension to real values is a useful trick for many recurrences that arise in the analysis of algorithms.)



and we can check (4.60) when x = 12:
Φ(12) + Φ(6) + Φ(4) + Φ(3) + Φ(2) + Φ(2) + 6·Φ(1)
   = 46 + 12 + 6 + 4 + 2 + 2 + 6 = 78 =  · 12 · 13.
Amazing.
Identity (4.60) can be regarded as an implicit recurrence for Φ(x); for example, we've just seen that we could have used it to calculate Φ(12) from certain values of Φ(m) with m < 12. And we can solve such recurrences by using another beautiful property of the Möbius function:
In fact, Möbius [273] invented his function because of (4.61), not (4.56).



This inversion law holds for all functions f such that ∑k,d≥1|f(x/kd)| < ∞; we can prove it as follows. Suppose g(x) = ∑d≥1 f(x/d). Then



The proof in the other direction is essentially the same.
So now we can solve the recurrence (4.60) for Φ(x):



This is always a finite sum. For example,


Φ(12)
=  (12·13 - 6·7 - 4·5 + 0 - 2·3 + 2·3   


 
- 1·2 + 0 + 0 + 1·2 - 1·2 + 0)  


 
= 78 - 21 - 10 - 3 + 3 - 1 + 1 - 1 = 46.


In Chapter 9 we'll see how to use (4.62) to get a good approximation to Φ(x); in fact, we'll prove a result due to Mertens in 1874 [270],
Φ(x) = x2 + O(x log x) .
Therefore the function Φ(x) grows "smoothly"; it averages out the erratic behavior of φ(k).
In keeping with the tradition established last chapter, let's conclude this chapter with a problem that illustrates much of what we've just seen and that also points ahead to the next chapter. Suppose we have beads of n different colors; our goal is to count how many different ways there are to string them into circular necklaces of length m. We can try to "name and conquer" this problem by calling the number of possible necklaces N(m, n).
For example, with two colors of beads R and B, we can make necklaces of length 4 in N(4, 2) = 6 different ways:



All other ways are equivalent to one of these, because rotations of a necklace do not change it. However, reflections are considered to be different; in the case m = 6, for example,



The problem of counting these configurations was first solved by P. A. MacMahon in 1892 [264].
There's no obvious recurrence for N(m, n), but we can count the necklaces by breaking them each into linear strings in m ways and considering the resulting fragments. For example, when m = 4 and n = 2 we get
RRRR   RRRR   RRRR   RRRRRRBR   RRRB   BRRR   RBRRRBBR   RRBB   BRRB   BBRR   .RBRB   BRBR   RBRB   BRBRRBBB   BRBB   BBRB   BBBRBBBB   BBBB   BBBB   BBBB
Each of the nm possible patterns appears at least once in this array of mN(m, n) strings, and some patterns appear more than once. How many times does a pattern a0 . . . am-1 appear? That's easy: It's the number of cyclic shifts ak . . . am-1 a0 . . . ak-1 that produce the same pattern as the original a0 . . . am-1. For example, BRBR occurs twice, because the four ways to cut the necklace formed from BRBR produce four cyclic shifts (BRBR, RBRB, BRBR, RBRB); two of these coincide with BRBR itself. This argument shows that



Here Sn is a set of n different colors.
Let's see how many patterns satisfy a0 . . . am-1 = ak . . . am-1 a0 . . . ak-1, when k is given. For example, if m = 12 and k = 8, we want to count the number of solutions to
a0a1a2a3a4a5a6a7a8a9a10a11 = a8a9a10a11a0a1a2a3a4a5a6a7 .
This means a0 = a8 = a4; a1 = a9 = a5; a2 = a10 = a6; and a3 = a11 = a7. So the values of a0, a1, a2, and a3 can be chosen in n4 ways, and the remaining a's depend on them. Does this look familiar? In general, the solution to
aj = a(j+k) mod m,          for 0 ≤ j < m
makes us equate aj with a(j+kl) mod m for l = 1, 2, . . . ; and we know that the multiples of k modulo m are {0, d, 2d, . . . , m - d}, where d = gcd(k, m). Therefore the general solution is to choose a0, . . . , ad-1 independently and then to set aj = aj-d for d ≤ j < m. There are nd solutions.
We have just proved that



This sum can be simplified, since it includes only terms nd where d\m. Substituting d = gcd(k, m) yields



(We are allowed to replace k/d by k because k must be a multiple of d.) Finally, we have ∑0≤k<m/d[k ⊥ m/d] = φ(m/d) by definition, so we obtain MacMahon's formula:



When m = 4 and n = 2, for example, the number of necklaces is , just as we suspected.
It's not immediately obvious that the value N(m, n) defined by MacMahon's sum is an integer! Let's try to prove directly that



without using the clue that this is related to necklaces. In the special case that m is prime, this congruence reduces to np + (p - 1)n ≡ 0 (mod p); that is, it reduces to np ≡ n. We've seen in (4.48) that this congruence is an alternative form of Fermat's theorem. Therefore (4.64) holds when m = p; we can regard it as a generalization of Fermat's theorem to the case when the modulus is not prime. (Euler's generalization (4.50) is different.)
We've proved (4.64) for all prime moduli, so let's look at the smallest case left, m = 4. We must prove that
n4 + n2 + 2n ≡ 0        (mod 4).
The proof is easy if we consider even and odd cases separately. If n is even, all three terms on the left are congruent to 0 modulo 4, so their sum is too. If n is odd, n4 and n2 are each congruent to 1, and 2n is congruent to 2; hence the left side is congruent to 1 + 1 + 2 and thus to 0 modulo 4, and we're done.
Next, let's be a bit daring and try m = 12. This value of m ought to be interesting because it has lots of factors, including the square of a prime, yet it is fairly small. (Also there's a good chance we'll be able to generalize a proof for 12 to a proof for general m.) The congruence we must prove is



Now what? By (4.42) this congruence holds if and only if it also holds modulo 3 and modulo 4. So let's prove that it holds modulo 3. Our congruence (4.64) holds for primes, so we have n3 + 2n ≡ 0 (mod 3). Careful scrutiny reveals that we can use this fact to group terms of the larger sum:
n12 + n6 + 2n4 + 2n3 + 2n2 + 4n
= (n12 + 2n4) + (n6 + 2n2) + 2(n3 + 2n)
≡ 0 + 0 + 2·0 ≡ 0          (mod 3) .
So it works modulo 3.
We're half done. To prove congruence modulo 4 we use the same trick. We've proved that n4 + n2 + 2n ≡ 0 (mod 4), so we use this pattern to group:
n12 + n6 + 2n4 + 2n3 + 2n2 + 4n
= (n12 + n6 + 2n3) + 2(n4 + n2 + 2n)
≡ 0 + 2·0 ≡ 0          (mod 4) .
QED: Quite Easily Done.
QED for the case m = 12.
So far we've proved our congruence for prime m, for m = 4, and for m = 12. Now let's try to prove it for prime powers. For concreteness we may suppose that m = p3 for some prime p. Then the left side of (4.64) is
np3 + φ(p)np2 + φ(p2)np + φ(p3)n
= np3 + (p - 1)np2 + (p2 - p)np + (p3 - p2)n
= (np3 - np2) + p(np2 - np) + p2(np - n) + p3n.
We can show that this is congruent to 0 modulo p3 if we can prove that np3- np2 is divisible by p3, that np2- np is divisible by p2, and that np - n is divisible by p, because the whole thing will then be divisible by p3. By the alternative form of Fermat's theorem we have np ≡ n (mod p), so p divides np - n; hence there is an integer q such that
np = n + pq.
Now we raise both sides to the pth power, expand the right side according to the binomial theorem (which we'll meet in Chapter 5), and regroup, giving
np2 = (n + pq)p = np + (pq)1np-1  + (pq)2np-2  + · · ·
= np + p2Q
for some other integer Q. We're able to pull out a factor of p2 here because  in the second term, and because a factor of (pq)2 appears in all the terms that follow. So we find that p2 divides np2 - np.
Again we raise both sides to the pth power, expand, and regroup, to get
np3 = (np + p2Q)p
= np2 + (p2Q)1np(p-1)  + (p2Q)2np(p-2)  + · · ·
= np2 + p3Q
for yet another integer Q. So p3 divides np3 - np2. This finishes the proof for m = p3, because we've shown that p3 divides the left-hand side of (4.64).
Moreover we can prove by induction that
npk = npk -1 + pk
for some final integer  (final because we're running out of fonts); hence



Thus the left side of (4.64), which is
(npk- npk-1) + p(npk-1 - npk-2) + · · · + pk-1(np - n) + pkn,
is divisible by pk and so is congruent to 0 modulo pk.
We're almost there. Now that we've proved (4.64) for prime powers, all that remains is to prove it when m = m1m2, where m1 ⊥ m2, assuming that the congruence is true for m1 and m2. Our examination of the case m = 12, which factored into instances of m = 3 and m = 4, encourages us to think that this approach will work.
We know that the φ function is multiplicative, so we can write



But the inner sum is congruent to 0 modulo m2, because we've assumed that (4.64) holds for m2; so the entire sum is congruent to 0 modulo m2. By a symmetric argument, we find that the entire sum is congruent to 0 modulo m1 as well. Thus by (4.42) it's congruent to 0 modulo m. QED.


Exercises

Warmups
1. What is the smallest positive integer that has exactly k divisors, for 1 ≤ k ≤ 6?
2. Prove that gcd(m, n)·lcm(m, n) = m·n, and use this identity to express lcm(m, n) in terms of lcm(n mod m, m), when n mod m ≠ 0. Hint: Use (4.12), (4.14), and (4.15).
3. Let π(x) be the number of primes not exceeding x. Prove or disprove:
π(x) - π(x - 1) = [x is prime] .
4. What would happen if the Stern-Brocot construction started with the five fractions  instead of with ?
5. Find simple formulas for Lk and Rk, when L and R are the 2 × 2 matrices of (4.33).
6. What does 'a ≡ b (mod 0)' mean?
7. Ten people numbered 1 to 10 are lined up in a circle as in the Josephus problem, and every mth person is executed. (The value of m may be much larger than 10.) Prove that the first three people to go cannot be 10, k, and k + 1 (in this order), for any k.
8. The residue number system (x mod 3, x mod 5) considered in the text has the curious property that 13 corresponds to (1, 3), which looks almost the same. Explain how to find all instances of such a coincidence, without calculating all fifteen pairs of residues. In other words, find all relevant solutions to the congruences



Hint: Use the facts that 10u + 6v ≡ u (mod 3) and 10u + 6v ≡ v (mod 5).
9. Show that (377 - 1)/2 is odd and composite. Hint: What is 377 mod 4?
10. Compute φ(999).
11. Find a function σ(n) with the property that



(This is analogous to the Möbius function; see (4.56).)
12. Simplify the formula ∑d\m ∑k\d μ(k) g(d/k).
13. A positive integer n is called squarefree if it is not divisible by m2 for any m > 1. Find a necessary and sufficient condition that n is squarefree,
a. in terms of the prime-exponent representation (4.11) of n;
b. in terms of μ(n).


Basics
14. Prove or disprove:
a. gcd(km, kn) = k gcd(m, n);
b. lcm(km, kn) = k lcm(m, n).
15. Does every prime occur as a factor of some Euclid number en?
16. What is the sum of the reciprocals of the first n Euclid numbers?
17. Let fn be the "Fermat number" 22n + 1. Prove that fm ⊥ fn if m < n.
18. Show that if 2n + 1 is prime then n is a power of 2.
19. Prove the following identities when n is a positive integer:



Hint: This is a trick question and the answer is pretty easy.
20. For every positive integer n there's a prime p such that n < p ≤ 2n. (This is essentially "Bertrand's postulate," which Joseph Bertrand verified for n < 3000000 in 1845 and Chebyshev proved for all n in 1850.) Use Bertrand's postulate to prove that there's a constant b ≈ 1.25 such that the numbers
2b, 22b, 222b , . . .
are all prime.
21. Let Pn be the nth prime number. Find a constant K such that
(10n2 K) mod 10n  = Pn .
22. The number 1111111111111111111 is prime. Prove that, in any radix b, (11 . . . 1)b can be prime only if the number of 1's is prime.
Is this a test for strabismus?
23. State a recurrence for ρ(k), the ruler function in the text's discussion of 2(n!). Show that there's a connection between ρ(k) and the disk that's moved at step k when an n-disk Tower of Hanoi is being transferred in 2n - 1 moves, for 1 ≤ k ≤ 2n - 1.
24. Express p(n!) in terms of νp(n), the sum of the digits in the radix p representation of n, thereby generalizing (4.24).
Look, ma, sideways addition.
25. We say that m exactly divides n, written m\\n, if m\n and m ⊥ n/m. For example, in the text's discussion of factorial factors, pp(n!)\\n!. Prove or disprove the following:
a. k\\n and m\\n ⇔ km\\n, if k ⊥ m.
b. For all m, n > 0, either gcd(m, n)\\m or gcd(m, n)\\n.
26. Consider the sequence  of all nonnegative reduced fractions m/n such that mn ≤ N. For example,



Is it true that m′n - mn′ = 1 whenever m/n immediately precedes m′/n′ in ?
27. Give a simple rule for comparing rational numbers based on their representations as L's and R's in the Stern-Brocot number system.
28. The Stern-Brocot representation of π is
π = R3L7R15LR292LRLR2LR3LR14L2R . . . ;
use it to find all the simplest rational approximations to π whose denominators are less than 50. Is  one of them?
29. The text describes a correspondence between binary real numbers x = (.b1b2b3 . . . )2 in [0 . . 1) and Stern-Brocot real numbers α = B1B2B3 . . . in [0 .. ∞). If x corresponds to α and x ≠ 0, what number corresponds to 1 - x?
30. Prove the following statement (the Chinese Remainder Theorem): Let m1, . . . , mr be positive integers with mj ⊥ mk for 1 ≤ j < k ≤ r; let m = m1 . . . mr; and let a1, . . . , ar, A be integers. Then there is exactly one integer a such that



31. A number in decimal notation is divisible by 3 if and only if the sum of its digits is divisible by 3. Prove this well-known rule, and generalize it.
32. Prove Euler's theorem (4.50) by generalizing the proof of (4.47).
Why is "Euler" pronounced "Oiler" when "Euclid" is "Yooklid"?
33. Show that if f(m) and g(m) are multiplicative functions, then so is h(m) = ∑d\m f(d) g(m/d).
34. Prove that (4.56) is a special case of (4.61).


Homework exercises
35. Let I(m, n) be a function that satisfies the relation
I(m, n)m + I(n, m)n = gcd(m, n),
when m and n are nonnegative integers with m ≠ n. Thus, I(m, n) = m′ and I(n, m) = n′ in (4.5); the value of I(m, n) is an inverse of m with respect to n. Find a recurrence that defines I(m, n).
36. Consider the set  | integer m, n}. The number  is called a unit if m2 - 10n2 = ±1, since it has an inverse (that is, since . For example,  is a unit, and so is . Pairs of cancelling units can be inserted into any factorization, so we ignore them. Nonunit numbers of  are called prime if they cannot be written as a product of two nonunits. Show that 2, 3, and  are primes of . Hint: If  then 4 = (k2 - 10l2)(m2 - 10n2). Furthermore, the square of any integer mod 10 is 0, 1, 4, 5, 6, or 9.
37. Prove (4.17). Hint: Show that , and consider .
38. Prove that if a ⊥ b and a > b > 0 then
gcd(am - bm, an - bn) = agcd(m,n) - bgcd(m,n),     0 ≤ m < n.
(All variables are integers.) Hint: Use Euclid's algorithm.
39. Let S(m) be the smallest positive integer n for which there exists an increasing sequence of integers
m = a1 < a2 < · · · < at = n
such that a1a2 . . . at is a perfect square. (If m is a perfect square, we can let t = 1 and n = m.) For example, S(2) = 6 because the best such sequence is a1 = 2, a2 = 3, a3 = 6. We have



Prove that S(m) ≠ S(m′) whenever 0 < m < m′.
40. If the radix p representation of n is (am . . . a1a0)p, prove that
n!/pεp(n!) ≡ (-1)εp(n!) am! . . . a1! a0!     (mod p) .
(The left side is simply n! with all p factors removed. When n = p this reduces to Wilson's theorem.)
Wilson's theorem: "Martha, that boy is a menace."
41.
a. Show that if p mod 4 = 3, there is no integer n such that p divides n2 + 1. Hint: Use Fermat's theorem.
b. But show that if p mod 4 = 1, there is such an integer. Hint: Write (p - 1)! as  and think about Wilson's theorem.
42. Consider two fractions m/n and m′/n′ in lowest terms. Prove that when the sum m/n + m′/n′ is reduced to lowest terms, the denominator will be nn′ if and only if n ⊥ n′. (In other words, (mn′ + m′n)/nn′ will already be in lowest terms if and only if n and n′ have no common factor.)
43. There are 2k nodes at level k of the Stern-Brocot tree, corresponding to the matrices Lk, Lk-1 R, . . . , Rk. Show that this sequence can be obtained by starting with Lk and then multiplying successively by



for 1 ≤ n < 2k, where ρ(n) is the ruler function.
44. Prove that a baseball player whose batting average is .316 must have batted at least 19 times. (If he has m hits in n times at bat, then m/n ∈ [0.3155 . . 0.3165).)
45. The number 9376 has the peculiar self-reproducing property that
93762 = 87909376.
Radio announcer:". . . pitcher Mark LeChiffre hits a two-run single! Mark, who was batting .080, gets his second hit of the year."Anything wrong?
How many 4-digit numbers x satisfy the equation x2 mod 10000 = x?
How many n-digit numbers x satisfy the equation x2 mod 10n = x?
46.
a. Prove that if nj ≡ 1 and nk ≡ 1 (mod m), then ngcd(j,k) ≡ 1.
b. Show that 2n ≢ 1 (mod n), if n > 1. Hint: Consider the least prime factor of n.
47. Show that if nm-1 ≡ 1 (mod m) and if n(m-1)/p ≢ 1 (mod m) for all primes such that p\(m - 1), then m is prime. Hint: Show that if this condition holds, the numbers nk mod m are distinct, for 1 ≤ k < m.
48. Generalize Wilson's theorem (4.49) by ascertaining the value of the expression (Π1≤n<m, n⊥m n) mod m, when m > 1.
49. Let R(N) be the number of pairs of integers (m, n) such that 1 ≤ m ≤ N, 1 ≤ n ≤ N, and m ⊥ n.
a. Express R(N) in terms of the Φ function.
b. Prove that R(N) = ∑d≥1 N/d2μ(d).
50. Let m be a positive integer and let
ω = e2πi/m = cos(2π/m) + i sin(2π/m) .
We say that ω is an mth root of unity, since ωm = e2πi = 1. In fact, each of the m complex numbers ω0, ω1, . . . , ωm-1 is an mth root of unity, because (ωk)m = e2πki = 1; therefore z - ωk is a factor of the polynomial zm - 1, for 0 ≤ k < m. Since these factors are distinct, the complete factorization of zm - 1 over the complex numbers must be



What are the roots of disunity?
a. Let . (This polynomial of degree φ(m) is called the cyclotomic polynomial of order m.) Prove that



b. Prove that .


Exam problems
51. Prove Fermat's theorem (4.48) by expanding (1 + 1 + · · · + 1)p via the multinomial theorem.
52. Let n and x be positive integers such that x has no divisors ≤ n (except 1), and let p be a prime number. Prove that at least n/p of the numbers {x - 1, x2 - 1, . . . , xn -1 - 1} are multiples of p.
53. Find all positive integers n such that n \ (n - 1)!/(n + 1).
54. Determine the value of 1000! mod 10250 by hand calculation.
55. Let Pn be the product of the first n factorials, . Prove that  is an integer, for all positive integers n.
56. Show that



is a power of 2.
57. Let S(m, n) be the set of all integers k such that
m mod k + n mod k ≥ k.
For example, S(7, 9) = {2, 4, 5, 8, 10, 11, 12, 13, 14, 15, 16}. Prove that



Hint: Prove first that . Then consider (m + n)/d - m/d - n/d.
58. Let f(m) = ∑d\m d. Find a necessary and sufficient condition that f(m) is a power of 2.


Bonus problems
59. Prove that if x1, . . . , xn are positive integers with 1/x1 + · · · + 1/xn = 1, then max(x1, . . . , xn) < en. Hint: Prove the following stronger result by induction: "If 1/x1 + · · · + 1/xn + 1/α = 1, where x1, . . . , xn are positive integers and α is a rational number ≥ max(x1, . . . , xn), then α+1 ≤ en+1 and x1 . . . xn(α + 1) ≤ e1 . . . en en+1." (The proof is nontrivial.)
60. Prove that there's a constant P such that (4.18) gives only primes. You may use the following (highly nontrivial) fact: There is a prime between p and p + pθ, for all sufficiently large p, if .
61. Prove that if m/n, m′/n′, and m″/n″ are consecutive elements of , then
m″ = (n + N)/n′ m′ - m,
n″ = (n + N)/n′ n′ - n.
(This recurrence allows us to compute the elements of  in order, starting with  and .)
62. What binary number corresponds to e, in the binary ↔ Stern-Brocot correspondence? (Express your answer as an infinite sum; you need not evaluate it in closed form.)
63. Using only the methods of this chapter, show that if Fermat's Last Theorem (4.46) were false, the least n for which it fails would have to be prime. (You may assume that (4.46) holds when n = 4.) Furthermore, if ap + bp = cp is the smallest counterexample, show that



for some integer m. Thus c ≥ mp/2 must be really huge. Hint: Let x = a + b, and note that gcd (x, (ap + (x - a)p)/x) = gcd(x, pap-1).
64. The Peirce sequence  of order N is an infinite string of fractions separated by '<' or '=' signs, containing all the nonnegative fractions m/n with m ≥ 0 and n ≤ N (including fractions that are not reduced). It is defined recursively by starting with



For N ≥ 1, we form N+1 by inserting two symbols just before the kNth symbol of N, for all k > 0. The two inserted symbols are



Here N, j denotes the jth symbol of N, which will be either '<' or '=' when j is even; it will be a fraction when j is odd. For example,



(Equal elements occur in a slightly peculiar order.) Prove that the '<' and '=' signs defined by the rules above correctly describe the relations between adjacent fractions in the Peirce sequence.


Research problems
65. Are the Euclid numbers en all squarefree?
66. Are the Mersenne numbers 2p - 1 all squarefree?
67. Prove or disprove that max1≤j<k≤n ak/gcd(aj, ak) ≥ n, for all sequences of integers 0 < a1 < · · · < an.
68. Is there a constant Q such that Q2n is prime for all n ≥ 0?
69. Let Pn denote the nth prime. Prove or disprove that Pn+1 - Pn = O(log Pn)2.
70. Does 3(n!) = 2(n!)/2 for infinitely many n?
71. Prove or disprove: If k ≠ 1 there exists n > 1 such that 2n ≡ k (mod n). Are there infinitely many such n?
72. Prove or disprove: For all integers a, there exist infinitely many n such that φ(n)\(n + a).
73. If the Φ(n) + 1 terms of the Farey series



were fairly evenly distributed, we would expect n(k) ≈ k/Φ(n). Therefore the sum  measures the "deviation of n from uniformity." Is it true that D(n) = O(n1/2+) for all  > 0?
74. Approximately how many distinct values are there in the set {0! mod p, 1! mod p, . . . , (p - 1)! mod p}, as p → ∞?











5. Binomial Coefficients
Let's take a breather. The previous chapters have seen some heavy going, with sums involving floor, ceiling, mod, phi, and mu functions. Now we're going to study binomial coefficients, which turn out to be (a) more important in applications, and (b) easier to manipulate, than all those other quantities.
Lucky us!

5.1 Basic Identities
The symbol  is a binomial coefficient, so called because of an important property we look at later this section, the binomial theorem. But we read the symbol "n choose k." This incantation arises from its combinatorial interpretation—it is the number of ways to choose a k-element subset from an n-element set. For example, from the set {1, 2, 3, 4} we can choose two elements in six ways,
{1, 2} , {1, 3} , {1, 4} , {2, 3} , {2, 4} , {3, 4};
Otherwise known as combinations of n things, k at a time.
so .
To express the number  in more familiar terms it's easiest to first determine the number of k-element sequences, rather than subsets, chosen from an n-element set; for sequences, the order of the elements counts. We use the same argument we used in Chapter 4 to show that n! is the number of permutations of n objects. There are n choices for the first element of the sequence; for each, there are n-1 choices for the second; and so on, until there are n-k+1 choices for the kth. This gives n(n-1) . . . (n-k+1) = nk choices in all. And since each k-element subset has exactly k! different orderings, this number of sequences counts each subset exactly k! times. To get our answer, we simply divide by k!:



For example,



this agrees with our previous enumeration.
We call n the upper index and k the lower index. The indices are restricted to be nonnegative integers by the combinatorial interpretation, because sets don't have negative or fractional numbers of elements. But the binomial coefficient has many uses besides its combinatorial interpretation, so we will remove some of the restrictions. It's most useful, it turns out, to allow an arbitrary real (or even complex) number to appear in the upper index, and to allow an arbitrary integer in the lower. Our formal definition therefore takes the following form:



This definition has several noteworthy features. First, the upper index is called r, not n; the letter r emphasizes the fact that binomial coefficients make sense when any real number appears in this position. For instance, we have . There's no combinatorial interpretation here, but r = -1 turns out to be an important special case. A noninteger index like r = -1/2 also turns out to be useful.
Second, we can view  as a kth-degree polynomial in r. We'll see that this viewpoint is often helpful.
Third, we haven't defined binomial coefficients for noninteger lower indices. A reasonable definition can be given, but actual applications are rare, so we will defer this generalization to later in the chapter.
Final note: We've listed the restrictions 'integer k ≥ 0' and 'integer k < 0' at the right of the definition. Such restrictions will be listed in all the identities we will study, so that the range of applicability will be clear. In general the fewer restrictions the better, because an unrestricted identity is most useful; still, any restrictions that apply are an important part of the identity. When we manipulate binomial coefficients, it's easier to ignore difficult-to-remember restrictions temporarily and to check later that nothing has been violated. But the check needs to be made.
For example, almost every time we encounter  it equals 1, so we can get lulled into thinking that it's always 1. But a careful look at definition (5.1) tells us that  is 1 only when n ≥ 0 (assuming that n is an integer); when n < 0 we have . Traps like this can (and will) make life adventuresome.
Before getting to the identities that we will use to tame binomial coefficients, let's take a peek at some small values. The numbers in Table 155 form the beginning of Pascal's triangle, named after Blaise Pascal (1623-1662)


Table 155 Pascal's triangle.



because he wrote an influential treatise about them [285]. The empty entries in this table are actually 0's, because of a zero in the numerator of (5.1); for example, . These entries have been left blank simply to help emphasize the rest of the table.
Binomial coefficients were well known in Asia, many centuries before Pascal was born [90], but he had no way to know that.
It's worthwhile to memorize formulas for the first three columns,



these hold for arbitrary reals. (Recall that  is the formula we derived for triangular numbers in Chapter 1; triangular numbers are conspicuously present in the  column of Table 155.) It's also a good idea to memorize the first five rows or so of Pascal's triangle, so that when the pattern 1, 4, 6, 4, 1 appears in some problem we will have a clue that binomial coefficients probably lurk nearby.
The numbers in Pascal's triangle satisfy, practically speaking, infinitely many identities, so it's not too surprising that we can find some surprising relationships by looking closely. For example, there's a curious "hexagon property," illustrated by the six numbers 56, 28, 36, 120, 210, 126 that surround 84 in the lower right portion of Table 155. Both ways of multiplying alternate numbers from this hexagon give the same product: 56·36·210 = 28·120·126 = 423360. The same thing holds if we extract such a hexagon from any other part of Pascal's triangle.
In Italy it's called Tartaglia's triangle.
And now the identities. Our goal in this section will be to learn a few simple rules by which we can solve the vast majority of practical problems involving binomial coefficients.
"C'est une chose estrange combien il est fertile en proprietez."
—B. Pascal [285]
Definition (5.1) can be recast in terms of factorials in the common case that the upper index r is an integer, n, that's greater than or equal to the lower index k:



To get this formula, we just multiply the numerator and denominator of (5.1) by (n - k)!. It's occasionally useful to expand a binomial coefficient into this factorial form (for example, when proving the hexagon property). And we often want to go the other way, changing factorials into binomials.
The factorial representation hints at a symmetry in Pascal's triangle: Each row reads the same left-to-right as right-to-left. The identity reflecting this—called the symmetry identity—is obtained by changing k to n - k:



This formula makes combinatorial sense, because by specifying the k chosen things out of n we're in effect specifying the n - k unchosen things.
The restriction that n and k be integers in identity (5.4) is obvious, since each lower index must be an integer. But why can't n be negative? Suppose, for example, that n = -1. Is



a valid equation? No. For instance, when k = 0 we get 1 on the left and 0 on the right. In fact, for any integer k ≥ 0 the left side is



which is either 1 or -1; but the right side is 0, because the lower index is negative. And for negative k the left side is 0 but the right side is



which is either 1 or -1. So the equation  is always false!
The symmetry identity fails for all other negative integers n, too. But unfortunately it's all too easy to forget this restriction, since the expression in the upper index is sometimes negative only for obscure (but legal) values of its variables. Everyone who's manipulated binomial coefficients much has fallen into this trap at least three times.
I just hope I don't fall into this trap during the midterm.
But the symmetry identity does have a big redeeming feature: It works for all values of k, even when k < 0 or k > n. (Because both sides are zero in such cases.) Otherwise 0 ≤ k ≤ n, and symmetry follows immediately from (5.3):



Our next important identity lets us move things in and out of binomial coefficients:



The restriction on k prevents us from dividing by 0 here. We call (5.5) an absorption identity, because we often use it to absorb a variable into a binomial coefficient when that variable is a nuisance outside. The equation follows from definition (5.1), because rk = r(r - 1)k-1 and k! = k(k - 1)! when k > 0; both sides are zero when k < 0.
If we multiply both sides of (5.5) by k, we get an absorption identity that works even when k = 0:



This one also has a companion that keeps the lower index intact:



We can derive (5.7) by sandwiching an application of (5.6) between two applications of symmetry:



But wait a minute. We've claimed that the identity holds for all real r, yet the derivation we just gave holds only when r is a positive integer. (The upper index r - 1 must be a nonnegative integer if we're to use the symmetry property (5.4) with impunity.) Have we been cheating? No. It's true that the derivation is valid only for positive integers r; but we can claim that the identity holds for all values of r, because both sides of (5.7) are polynomials in r of degree k + 1. A nonzero polynomial of degree d or less can have at most d distinct zeros; therefore the difference of two such polynomials, which also has degree d or less, cannot be zero at more than d points unless it is identically zero. In other words, if two polynomials of degree d or less agree at more than d points, they must agree everywhere. We have shown that  whenever r is a positive integer; so these two polynomials agree at infinitely many points, and they must be identically equal.
(Well, not here anyway.)
The proof technique in the previous paragraph, which we will call the polynomial argument, is useful for extending many identities from integers to reals; we'll see it again and again. Some equations, like the symmetry identity (5.4), are not identities between polynomials, so we can't always use this method. But many identities do have the necessary form.
For example, here's another polynomial identity, perhaps the most important binomial identity of all, known as the addition formula:



When r is a positive integer, the addition formula tells us that every number in Pascal's triangle is the sum of two numbers in the previous row, one directly above it and the other just to the left. And the formula applies also when r is negative, real, or complex; the only restriction is that k be an integer, so that the binomial coefficients are defined.
One way to prove the addition formula is to assume that r is a positive integer and to use the combinatorial interpretation. Recall that  is the number of possible k-element subsets chosen from an r-element set. If we have a set of r eggs that includes exactly one bad egg, there are  ways to select k of the eggs. Exactly  of these selections involve nothing but good eggs; and  of them contain the bad egg, because such selections have k - 1 of the r - 1 good eggs. Adding these two numbers together gives (5.8). This derivation assumes that r is a positive integer, and that k ≥ 0. But both sides of the identity are zero when k < 0, and the polynomial argument establishes (5.8) in all remaining cases.
We can also derive (5.8) by adding together the two absorption identities (5.7) and (5.6):



the left side is , and we can divide through by r. This derivation is valid for everything but r = 0, and it's easy to check that remaining case.
Those of us who tend not to discover such slick proofs, or who are otherwise into tedium, might prefer to derive (5.8) by a straightforward manipulation of the definition. If k > 0,



Again, the cases for k ≤ 0 are easy to handle.
We've just seen three rather different proofs of the addition formula. This is not surprising; binomial coefficients have many useful properties, several of which are bound to lead to proofs of an identity at hand.
The addition formula is essentially a recurrence for the numbers of Pascal's triangle, so we'll see that it is especially useful for proving other identities by induction. We can also get a new identity immediately by unfolding the recurrence. For example,



Since , that term disappears and we can stop. This method yields the general formula



Notice that we don't need the lower limit k ≥ 0 on the index of summation, because the terms with k < 0 are zero.
This formula expresses one binomial coefficient as the sum of others whose upper and lower indices stay the same distance apart. We found it by repeatedly expanding the binomial coefficient with the smallest lower index: first , then , then , then . What happens if we unfold the other way, repeatedly expanding the one with largest lower index? We get



Now  is zero (so are  and , but these make the identity nicer), and we can spot the general pattern:



This identity, which we call summation on the upper index, expresses a binomial coefficient as the sum of others whose lower indices are constant. In this case the sum needs the lower limit k ≥ 0, because the terms with k < 0 aren't zero. Also, m and n can't in general be negative.
Identity (5.10) has an interesting combinatorial interpretation. If we want to choose m + 1 tickets from a set of n + 1 tickets numbered 0 through n, there are  ways to do this when the largest ticket selected is number k.
We can prove both (5.9) and (5.10) by induction using the addition formula, but we can also prove them from each other. For example, let's prove (5.9) from (5.10); our proof will illustrate some common binomial coefficient manipulations. Our general plan will be to massage the left side  of (5.9) so that it looks like the left side  of (5.10); then we'll invoke that identity, replacing the sum by a single binomial coefficient; finally we'll transform that coefficient into the right side of (5.9).
We can assume for convenience that r and n are nonnegative integers; the general case of (5.9) follows from this special case, by the polynomial argument. Let's write m instead of r, so that this variable looks more like a nonnegative integer. The plan can now be carried out systematically as follows:



Let's look at this derivation blow by blow. The key step is in the second line, where we apply the symmetry law (5.4) to replace  by . We're allowed to do this only when m + k ≥ 0, so our first step restricts the range of k by discarding the terms with k < -m. (This is legal because those terms are zero.) Now we're almost ready to apply (5.10); the third line sets this up, replacing k by k - m and tidying up the range of summation. This step, like the first, merely plays around with ∑-notation. Now k appears by itself in the upper index and the limits of summation are in the proper form, so the fourth line applies (5.10). One more use of symmetry finishes the job.
Certain sums that we did in Chapters 1 and 2 were actually special cases of (5.10), or disguised versions of this identity. For example, the case m = 1 gives the sum of the nonnegative integers up through n:



And the general case is equivalent to Chapter 2's rule



if we divide both sides of this formula by m!. In fact, the addition formula (5.8) tells us that



if we replace r and k respectively by x + 1 and m. Hence the methods of Chapter 2 give us the handy indefinite summation formula



Binomial coefficients get their name from the binomial theorem, which deals with powers of the binomial expression x + y. Let's look at the smallest cases of this theorem:
"At the age of twenty-one he [Moriarty] wrote a treatise upon the Binomial Theorem, which has had a European vogue. On the strength of it, he won the Mathematical Chair at one of our smaller Universities."
—S. Holmes [84]
(x + y)0 = 1x0y0
(x + y)1 = 1x1y0 + 1x0y1
(x + y)2 = 1x2y0 + 2x1y1 + 1x0y2
(x + y)3 = 1x3y0 + 3x2y1 + 3x1y2 + 1x0y3
(x + y)4 = 1x4y0 + 4x3y1 + 6x2y2 + 4x1y3 + 1x0y4 .
It's not hard to see why these coefficients are the same as the numbers in Pascal's triangle: When we expand the product



every term is itself the product of n factors, each either an x or y. The number of such terms with k factors of x and n - k factors of y is the coefficient of xkyn-k after we combine like terms. And this is exactly the number of ways to choose k of the n binomials from which an x will be contributed; that is, it's .
Some textbooks leave the quantity 00 undefined, because the functions x0 and 0x have different limiting values when x decreases to 0. But this is a mistake. We must define
x0 = 1,      for all x,
if the binomial theorem is to be valid when x = 0, y = 0, and/or x = -y. The theorem is too important to be arbitrarily restricted! By contrast, the function 0x is quite unimportant. (See [220] for further discussion.)
But what exactly is the binomial theorem? In its full glory it is the following identity:



The sum is over all integers k; but it is really a finite sum when r is a nonnegative integer, because all terms are zero except those with 0 ≤ k ≤ r. On the other hand, the theorem is also valid when r is negative, or even when r is an arbitrary real or complex number. In such cases the sum really is infinite, unless r is a nonnegative integer, and we must have |x/y| < 1 to guarantee the sum's absolute convergence.
Two special cases of the binomial theorem are worth special attention, even though they are extremely simple. If x = y = 1 and r = n is nonnegative, we get



This equation tells us that row n of Pascal's triangle sums to 2n. And when x is -1 instead of +1, we get



For example, 1 - 4 + 6 - 4 + 1 = 0; the elements of row n sum to zero if we give them alternating signs, except in the top row (when n = 0 and 00 = 1).
When r is not a nonnegative integer, we most often use the binomial theorem in the special case y = 1. Let's state this special case explicitly, writing z instead of x to emphasize the fact that an arbitrary complex number can be involved here:



The general formula in (5.12) follows from this one if we set z = x/y and multiply both sides by yr.
We have proved the binomial theorem only when r is a nonnegative integer, by using a combinatorial interpretation. We can't deduce the general case from the nonnegative-integer case by using the polynomial argument, because the sum is infinite in the general case. But when r is arbitrary, we can use Taylor series and the theory of complex variables:



The derivatives of the function f(z) = (1 + z)r are easily evaluated; in fact, f(k)(z) = rk (1 + z)r-k. Setting z = 0 gives (5.13).
(Chapter 9 tells the meaning of O.)
We also need to prove that the infinite sum converges, when |z| < 1. It does, because  by equation (5.83) below.
Now let's look more closely at the values of  when n is a negative integer. One way to approach these values is to use the addition law (5.8) to fill in the entries that lie above the numbers in Table 155, thereby obtaining Table 164. For example, we must have , since  and ; then we must have , since ; and so on.


Table 164 Pascal's triangle, extended upward.



All these numbers are familiar. Indeed, the rows and columns of Table 164 appear as columns in Table 155 (but minus the minus signs). So there must be a connection between the values of  for negative n and the values for positive n. The general rule is



it is easily proved, since
rk = r(r - 1) . . . (r - k + 1)
= (-1)k(-r)(1 - r) . . . (k - 1 - r) = (-1)k(k - r - 1)k
when k ≥ 0, and both sides are zero when k < 0.
Identity (5.14) is particularly valuable because it holds without any restriction. (Of course, the lower index must be an integer so that the binomial coefficients are defined.) The transformation in (5.14) is called negating the upper index, or "upper negation."
But how can we remember this important formula? The other identities we've seen—symmetry, absorption, addition, etc.—are pretty simple, but this one looks rather messy. Still, there's a mnemonic that's not too bad: To negate the upper index, we begin by writing down (-1)k, where k is the lower index. (The lower index doesn't change.) Then we immediately write k again, twice, in both lower and upper index positions. Then we negate the original upper index by subtracting it from the new upper index. And we complete the job by subtracting 1 more (always subtracting, not adding, because this is a negation process).
You call this a mnemonic? I'd call it pneumatic—full of air. It does help me remember, though.
Let's negate the upper index twice in succession, for practice. We get
(Now is a good time to do warmup exercise 4.)



so we're right back where we started. This is probably not what the framers of the identity intended; but it's reassuring to know that we haven't gone astray.
It's also frustrating, if we're trying to get somewhere else.
Some applications of (5.14) are, of course, more useful than this. We can use upper negation, for example, to move quantities between upper and lower index positions. The identity has a symmetric formulation,



which holds because both sides are equal to .
Upper negation can also be used to derive the following interesting sum:



The idea is to negate the upper index, then apply (5.9), and negate again:



(Here double negation helps, because we've sandwiched another operation in between.)
This formula gives us a partial sum of the rth row of Pascal's triangle, provided that the entries of the row have been given alternating signs. For instance, if r = 5 and m = 2 the formula gives .
Notice that if m ≥ r, (5.16) gives the alternating sum of the entire row, and this sum is zero when r is a positive integer. We proved this before, when we expanded (1 - 1)r by the binomial theorem; it's interesting to know that the partial sums of this expression can also be evaluated in closed form.
How about the simpler partial sum,



surely if we can evaluate the corresponding sum with alternating signs, we ought to be able to do this one? But no; there is no closed form for the partial sum of a row of Pascal's triangle. We can do columns—that's (5.10)—but not rows. Curiously, however, there is a way to partially sum the row elements if they have been multiplied by their distance from the center:



(This formula is easily verified by induction on m.) The relation between these partial sums with and without the factor of (r/2 - k) in the summand is analogous to the relation between the integrals



The apparently more complicated integral on the left, with the factor of x, has a closed form, while the simpler-looking integral on the right, without the factor, has none. Appearances can be deceiving.
(Well, the right-hand integral is , a constant plus a multiple of the "error function" of α, if we're willing to accept that as a closed form.)
Near the end of this chapter, we'll study a method by which it's possible to determine whether or not there is a closed form for the partial sums of a given series involving binomial coefficients, in a fairly general setting. This method is capable of discovering identities (5.16) and (5.18), and it also will tell us that (5.17) is a dead end.
Partial sums of the binomial series lead to a curious relationship of another kind:



This identity isn't hard to prove by induction: Both sides are zero when m < 0 and 1 when m = 0. If we let Sm stand for the sum on the left, we can apply the addition formula (5.8) and show easily that



and



when m > 0. Hence



and this recurrence is satisfied also by the right-hand side of (5.19). By induction, both sides must be equal; QED.
But there's a neater proof. When r is an integer in the range 0 ≥ r ≥ -m, the binomial theorem tells us that both sides of (5.19) are (x + y)m+ry-r. And since both sides are polynomials in r of degree m or less, agreement at m+1 different values is enough (but just barely!) to prove equality in general.
It may seem foolish to have an identity where one sum equals another. Neither side is in closed form. But sometimes one side turns out to be easier to evaluate than the other. For example, if we set x = -1 and y = 1, we get



an alternative form of identity (5.16). And if we set x = y = 1 and r = m + 1, we get



The left-hand side sums just half of the binomial coefficients with upper index 2m + 1, and these are equal to their counterparts in the other half because Pascal's triangle has left-right symmetry. Hence the left-hand side is just . This yields a formula that is quite unexpected,



(There's a nice combinatorial proof of this formula [247].)
Let's check it when m = 2: . Astounding.
So far we've been looking either at binomial coefficients by themselves or at sums of terms in which there's only one binomial coefficient per term. But many of the challenging problems we face involve products of two or more binomial coefficients, so we'll spend the rest of this section considering how to deal with such cases.
Here's a handy rule that often helps to simplify the product of two binomial coefficients:



We've already seen the special case k = 1; it's the absorption identity (5.6). Although both sides of (5.21) are products of binomial coefficients, one side often is easier to sum because of interactions with the rest of a formula. For example, the left side uses m twice, the right side uses it only once. Therefore we usually want to replace  by  when summing on m.
Equation (5.21) holds primarily because of cancellation between m!'s in the factorial representations of  and . If all variables are integers and r ≥ m ≥ k ≥ 0, we have



That was easy. Furthermore, if m < k or k < 0, both sides of (5.21) are zero; so the identity holds for all integers m and k. Finally, the polynomial argument extends its validity to all real r.
Yeah, right.
A binomial coefficient  can be written in the form (a + b)!/a! b! after a suitable renaming of variables. Similarly, the quantity in the middle of the derivation above, r!/k! (m - k)! (r - m)!, can be written in the form (a + b + c)!/a! b! c!. This is a "trinomial coefficient," which arises in the "trinomial theorem":



So  is really a trinomial coefficient in disguise. Trinomial coefficients pop up occasionally in applications, and we can conveniently write them as



"Excogitavi autem olim mirabilem regulam pro numeris coefficientibus potestatum, non tantum a binomio x + y, sed et a trinomio x + y + z, imo a polynomio quocunque, ut data potentia gradus cujuscunque v. gr. decimi, et potentia in ejus valore comprehensa, ut x5 y3 z2, possim statim assignare numerum coefficientem, quem habere debet, sine ulla Tabula jam calculata."
—G. W. Leibniz [245]
in order to emphasize the symmetry present.
Binomial and trinomial coefficients generalize to multinomial coefficients, which are always expressible as products of binomial coefficients:



Therefore, when we run across such a beastie, our standard techniques apply.


Table 169 Sums of products of binomial coefficients.



Now we come to Table 169, which lists identities that are among the most important of our standard techniques. These are the ones we rely on when struggling with a sum involving a product of two binomial coefficients. Each of these identities is a sum over k, with one appearance of k in each binomial coefficient; there also are four nearly independent parameters, called m, n, r, etc., one in each index position. Different cases arise depending on whether k appears in the upper or lower index, and on whether it appears with a plus or minus sign. Sometimes there's an additional factor of (-1)k, which is needed to make the terms summable in closed form.
Table 169 is far too complicated to memorize in full; it is intended only for reference. But the first identity in this table is by far the most memorable, and it should be remembered. It states that the sum (over all integers k) of the product of two binomial coefficients, in which the upper indices are constant and the lower indices have a constant sum for all k, is the binomial coefficient obtained by summing both lower and upper indices. This identity is known as Vandermonde's convolution, because Alexandre Vandermonde wrote a significant paper about it in the late 1700s [357]; it was, however, known to Chu Shih-Chieh in China as early as 1303. All of the other identities in Table 169 can be obtained from Vandermonde's convolution by doing things like negating upper indices or applying the symmetry law, etc., with care; therefore Vandermonde's convolution is the most basic of all.
Fold down the corner on this page, so you can find the table quickly later. You'll need it!
We can prove Vandermonde's convolution by giving it a nice combinatorial interpretation. If we replace k by k - m and n by n - m, we can assume that m = 0; hence the identity to be proved is



Let r and s be nonnegative integers; the general case then follows by the polynomial argument. On the right side,  is the number of ways to choose n people from among r men and s women. On the left, each term of the sum is the number of ways to choose k of the men and n - k of the women. Summing over all k counts each possibility exactly once.
Sexist! You mentioned men first.
Much more often than not we use these identities left to right, since that's the direction of simplification. But every once in a while it pays to go the other direction, temporarily making an expression more complicated. When this works, we've usually created a double sum for which we can interchange the order of summation and then simplify.
Before moving on let's look at proofs for two more of the identities in Table 169. It's easy to prove (5.23); all we need to do is replace the first binomial coefficient by , then Vandermonde's (5.22) applies.
The next one, (5.24), is a bit more difficult. We can reduce it to Vandermonde's convolution by a sequence of transformations, but we can just as easily prove it by resorting to the old reliable technique of mathematical induction. Induction is often the first thing to try when nothing else obvious jumps out at us, and induction on l works just fine here.
For the basis l = 0, all terms are zero except when k = -m; so both sides of the equation are . Now suppose that the identity holds for all values less than some fixed l, where l > 0. We can use the addition formula to replace  by ; the original sum now breaks into two sums, each of which can be evaluated by the induction hypothesis:



And this simplifies to the right-hand side of (5.24), if we apply the addition formula once again.
Two things about this derivation are worthy of note. First, we see again the great convenience of summing over all integers k, not just over a certain range, because there's no need to fuss over boundary conditions. Second, the addition formula works nicely with mathematical induction, because it's a recurrence for binomial coefficients. A binomial coefficient whose upper index is l is expressed in terms of two whose upper indices are l - 1, and that's exactly what we need to apply the induction hypothesis.
So much for Table 169. What about sums with three or more binomial coefficients? If the index of summation is spread over all the coefficients, our chances of finding a closed form aren't great: Only a few closed forms are known for sums of this kind, hence the sum we need might not match the given specs. One of these rarities, proved in exercise 43, is



Here's another, more symmetric example:



This one has a two-coefficient counterpart,



which incidentally doesn't appear in Table 169. The analogous four-coefficient sum doesn't have a closed form, but a similar sum does:



This follows from a five-parameter identity discovered by John Dougall [82] early in the twentieth century.
Is Dougall's identity the hairiest sum of binomial coefficients known? No! The champion so far is



Here the sum is over  index variables kij for 1 ≤ i < j < n. Equation (5.29) is the special case n = 3; the case n = 4 can be written out as follows, if we use (a, b, c, d) for (a1, a2, a3, a4) and (i, j, k) for (k12, k13, k23):



The left side of (5.31) is the coefficient of  after the product of n(n - 1) fractions



has been fully expanded into positive and negative powers of the z's. The right side of (5.31) was conjectured by Freeman Dyson in 1962 and proved by several people shortly thereafter. Exercise 86 gives a "simple" proof of (5.31).
Another noteworthy identity involving lots of binomial coefficients is



This one, proved in exercise 83, even has a chance of arising in practical applications. But we're getting far afield from our theme of "basic identities," so we had better stop and take stock of what we've learned.
We've seen that binomial coefficients satisfy an almost bewildering variety of identities. Some of these, fortunately, are easily remembered, and we can use the memorable ones to derive most of the others in a few steps. Table 174 collects ten of the most useful formulas, all in one place; these are the best identities to know.


5.2 Basic Practice
In the previous section we derived a bunch of identities by manipulating sums and plugging in other identities. It wasn't too tough to find those derivations—we knew what we were trying to prove, so we could formulate a general plan and fill in the details without much trouble. Usually, however, out in the real world, we're not faced with an identity to prove; we're faced with a sum to simplify. And we don't know what a simplified form might look like (or even if one exists). By tackling many such sums in this section and the next, we will hone our binomial coefficient tools.
To start, let's try our hand at a few sums involving a single binomial coefficient.

Problem 1: A sum of ratios.
We'd like to have a closed form for



Algorithm self-teach:1 read problem2 attempt solution3 skim book solution4 if attempt failed goto 1 else goto next problem
At first glance this sum evokes panic, because we haven't seen any identities that deal with a quotient of binomial coefficients. (Furthermore the sum involves two binomial coefficients, which seems to contradict the sentence preceding this problem.) However, just as we can use the factorial representations to reexpress a product of binomial coefficients as another product—that's how we got identity (5.21)—we can do likewise with a quotient. In fact we can avoid the grubby factorial representations by letting r = n and dividing both sides of equation (5.21) by ; this yields



Unfortunately, that algorithm can put you in an infinite loop.
Suggested patches:
0 set c ← 03a set c ← c + 13b if c = Ngoto your TA
So we replace the quotient on the left, which appears in our sum, by the one on the right; the sum becomes



We still have a quotient, but the binomial coefficient in the denominator doesn't involve the index of summation k, so we can remove it from the sum. We'll restore it later.



—E. W. Dijkstra
We can also simplify the boundary conditions by summing over all k ≥ 0; the terms for k > m are zero. The sum that's left isn't so intimidating:



. . . But this sub-chapter is called BASIC practice.
It's similar to the one in identity (5.9), because the index k appears twice with the same sign. But here it's -k and in (5.9) it's not. The next step should therefore be obvious; there's only one reasonable thing to do:





Table 174 The top ten binomial coefficient identities.



And now we can apply the parallel summation identity, (5.9):



Finally we reinstate the  in the denominator that we removed from the sum earlier, and then apply (5.7) to get the desired closed form:



This derivation actually works for any real value of n, as long as no division by zero occurs; that is, as long as n isn't one of the integers 0, 1, . . . , m - 1.
The more complicated the derivation, the more important it is to check the answer. This one wasn't too complicated but we'll check anyway. In the small case m = 2 and n = 4 we have



yes, this agrees perfectly with our closed form (4 + 1)/(4 + 1 - 2).


Problem 2: From the literature of sorting.
Our next sum appeared way back in ancient times (the early 1970s) before people were fluent with binomial coefficients. A paper that introduced an improved merging technique [196] concludes with the following remarks: "It can be shown that the expected number of saved transfers . . . is given by the expression



Here m and n are as defined above, and m Cn is the symbol for the number of combinations of m objects taken n at a time. . . . The author is grateful to the referee for reducing a more complex equation for expected transfers saved to the form given here."
We'll see that this is definitely not a final answer to the author's problem. It's not even a midterm answer.
Please, don't remind me of the midterm.
First we should translate the sum into something we can work with; the ghastly notation m-r-1 Cm-n-1 is enough to stop anybody, save the enthusiastic referee (please). In our language we'd write



The binomial coefficient in the denominator doesn't involve the index of summation, so we can remove it and work with the new sum



What next? The index of summation appears in the upper index of the binomial coefficient but not in the lower index. So if the other k weren't there, we could massage the sum and apply summation on the upper index (5.10). With the extra k, though, we can't. If we could somehow absorb that k into the binomial coefficient, using one of our absorption identities, we could then sum on the upper index. Unfortunately those identities don't work here. But if the k were instead m - k, we could use absorption identity (5.6):



So here's the key: We'll rewrite k as m - (m - k) and split the sum S into two sums:



where



The sums A and B that remain are none other than our old friends in which the upper index varies while the lower index stays fixed. Let's do B first, because it looks simpler. A little bit of massaging is enough to make the summand match the left side of (5.10):



In the last step we've included the terms with 0 ≤ k < m - n in the sum; they're all zero, because the upper index is less than the lower. Now we sum on the upper index, using (5.10), and get



The other sum A is the same, but with m replaced by m - 1. Hence we have a closed form for the given sum S, which can be simplified further:



And this gives us a closed form for the original sum:



Even the referee can't simplify this.
Again we use a small case to check the answer. When m = 4 and n = 2, we have



which agrees with our formula 2/(4 - 2 + 1).


Problem 3: From an old exam.
Let's do one more sum that involves a single binomial coefficient. This one, unlike the last, originated in the halls of academia; it was a problem on a take-home test. We want the value of Q1000000, when
Do old exams ever die?



This one's harder than the others; we can't apply any of the identities we've seen so far. And we're faced with a sum of 21000000 + 1 terms, so we can't just add them up. The index of summation k appears in both indices, upper and lower, but with opposite signs. Negating the upper index doesn't help, either; it removes the factor of (-1)k, but it introduces a 2k in the upper index.
When nothing obvious works, we know that it's best to look at small cases. If we can't spot a pattern and prove it by induction, at least we'll have some data for checking our results. Here are the nonzero terms and their sums for the first four values of n.



We'd better not try the next case, n = 4; the chances of making an arithmetic error are too high. (Computing terms like  and  by hand, let alone combining them with the others, is worthwhile only if we're desperate.)
So the pattern starts out 1, 0, -1, 0. Even if we knew the next term or two, the closed form wouldn't be obvious. But if we could find and prove a recurrence for Qn we'd probably be able to guess and prove its closed form. To find a recurrence, we need to relate Qn to Qn- 1 (or to Qsmaller values); but to do this we need to relate a term like , which arises when n = 7 and k = 13, to terms like . This doesn't look promising; we don't know any neat relations between entries in Pascal's triangle that are 64 rows apart. The addition formula, our main tool for induction proofs, only relates entries that are one row apart.
But this leads us to a key observation: There's no need to deal with entries that are 2n-1 rows apart. The variable n never appears by itself, it's always in the context 2n. So the 2n is a red herring! If we replace 2n by m, all we need to do is find a closed form for the more general (but easier) sum



Oh, the sneakiness of the instructor who set that exam.
then we'll also have a closed form for Qn = R2n. And there's a good chance that the addition formula will give us a recurrence for the sequence Rm.
Values of Rm for small m can be read from Table 155, if we alternately add and subtract values that appear in a southwest-to-northeast diagonal. The results are:



There seems to be a lot of cancellation going on.
Let's look now at the formula for Rm and see if it defines a recurrence. Our strategy is to apply the addition formula (5.8) and to find sums that have the form Rk in the resulting expression, somewhat as we did in the perturbation method of Chapter 2:



(In the next-to-last step we've used the formula , which we know is true when m ≥ 0.) This derivation is valid for m ≥ 2.
Anyway those of us who've done warmup exercise 4 know it.
From this recurrence we can generate values of Rm quickly, and we soon perceive that the sequence is periodic. Indeed,



The proof by induction is by inspection. Or, if we must give a more academic proof, we can unfold the recurrence one step to obtain
Rm = (Rm-2 - Rm-3) - Rm-2 = -Rm-3 ,
whenever m ≥ 3. Hence Rm = Rm-6 whenever m ≥ 6.
Finally, since Qn = R2n, we can determine Qn by determining 2n mod 6 and using the closed form for Rm. When n = 0 we have 20 mod 6 = 1; after that we keep multiplying by 2 (mod 6), so the pattern 2, 4 repeats. Thus



This closed form for Qn agrees with the first four values we calculated when we started on the problem. We conclude that Q1000000 = R4 = -1.


Problem 4: A sum involving two binomial coefficients.
Our next task is to find a closed form for



Wait a minute. Where's the second binomial coefficient promised in the title of this problem? And why should we try to simplify a sum we've already simplified? (This is the sum S from Problem 2.)
Well, this is a sum that's easier to simplify if we view the summand as a product of two binomial coefficients, and then use one of the general identities found in Table 169. The second binomial coefficient materializes when we rewrite k as :



And identity (5.26) is the one to apply, since its index of summation appears in both upper indices and with opposite signs.
But our sum isn't quite in the correct form yet. The upper limit of summation should be m - 1, if we're to have a perfect match with (5.26). No problem; the terms for n < k ≤ m - 1 are zero. So we can plug in, with (l, m, n, q) ← (m - 1, m - n - 1, 1, 0); the answer is



This is cleaner than the formula we got before. We can convert it to the previous formula by using (5.6) and (5.7):



Similarly, we can get interesting results by plugging special values into the other general identities we've seen. Suppose, for example, that we set m = n = 1 and q = 0 in (5.26). Then the identity reads



The left side is l ((l + 1)l/2) - (12 + 22 + · · · + l2), so this gives us a brand new way to solve the sum-of-squares problem that we beat to death in Chapter 2.
The moral of this story is: Special cases of very general sums are sometimes best handled in the general form. When learning general forms, it's wise to learn their simple specializations.


Problem 5: A sum with three factors.
Here's another sum that isn't too bad. We wish to simplify



The index of summation k appears in both lower indices and with the same sign; therefore identity (5.23) in Table 169 looks close to what we need. With a bit of manipulation, we should be able to use it.
The biggest difference between (5.23) and what we have is the extra k in our sum. But we can absorb k into one of the binomial coefficients by using one of the absorption identities:



We don't care that the s appears when the k disappears, because it's constant. And now we're ready to apply the identity and get the closed form,



If we had chosen in the first step to absorb k into , not , we wouldn't have been allowed to apply (5.23) directly, because n - 1 might be negative; the identity requires a nonnegative value in at least one of the upper indices.


Problem 6: A sum with menacing characteristics.
The next sum is more challenging. We seek a closed form for



So we should deep six this sum, right?
One useful measure of a sum's difficulty is the number of times the index of summation appears. By this measure we're in deep trouble—k appears six times. Furthermore, the key step that worked in the previous problem—to absorb something outside the binomial coefficients into one of them—won't work here. If we absorb the k + 1 we just get another occurrence of k in its place. And not only that: Our index k is twice shackled with the coefficient 2 inside a binomial coefficient. Multiplicative constants are usually harder to remove than additive constants.
We're lucky this time, though. The 2k's are right where we need them for identity (5.21) to apply, so we get



The two 2's disappear, and so does one occurrence of k. So that's one down and five to go.
The k + 1 in the denominator is the most troublesome characteristic left, and now we can absorb it into  using identity (5.5):



(Recall that n ≥ 0.) Two down, four to go.
To eliminate another k we have two promising options. We could use symmetry on ; or we could negate the upper index n + k, thereby eliminating that k as well as the factor (-1)k. Let's explore both possibilities, starting with the symmetry option:



Third down, three to go, and we're in position to make a big gain by plugging into (5.24): Replacing (l, m, n, s) by (n + 1, 1, n, n), we get



For a minute I thought we'd have to punt.
Zero, eh? After all that work? Let's check it when n = 2: . It checks.
Just for the heck of it, let's explore our other option, negating the upper index of :



Now (5.23) applies, with (l, m, n, s) ← (n + 1, 1, 0, -n - 1), and



Hey wait. This is zero when n > 0, but it's 1 when n = 0. Our other path to the solution told us that the sum was zero in all cases! What gives? The sum actually does turn out to be 1 when n = 0, so the correct answer is '[n = 0]'. We must have made a mistake in the previous derivation.
Let's do an instant replay on that derivation when n = 0, in order to see where the discrepancy first arises. Ah yes; we fell into the old trap mentioned earlier: We tried to apply symmetry when the upper index could be negative! We were not justified in replacing  by  when k ranges over all integers, because this converts zero into a nonzero value when k < -n. (Sorry about that.)
Try binary search: Replay the middle formula first, to see if the mistake was early or late.
The other factor in the sum, , turns out to be zero when k < -n, except when n = 0 and k = -1. Hence our error didn't show up when we checked the case n = 2. Exercise 6 explains what we should have done.


Problem 7: A new obstacle.
This one's even tougher; we want a closed form for



If m were 0 we'd have the sum from the problem we just finished. But it's not, and we're left with a real mess—nothing we used in Problem 6 works here. (Especially not the crucial first step.)
However, if we could somehow get rid of the m, we could use the result just derived. So our strategy is: Replace  by a sum of terms like  for some nonnegative integer l; the summand will then look like the summand in Problem 6, and we can interchange the order of summation.
What should we substitute for ? A painstaking examination of the identities derived earlier in this chapter turns up only one suitable candidate, namely equation (5.26) in Table 169. And one way to use it is to replace the parameters (l, m, n, q, k) by (n + k - 1, 2k, m - 1, 0, j), respectively:



In the last step we've changed the order of summation, manipulating the conditions below the ∑'s according to the rules of Chapter 2.
We can't quite replace the inner sum using the result of Problem 6, because it has the extra condition k ≥ j - n + 1. But this extra condition is superfluous unless j - n + 1 > 0; that is, unless j ≥ n. And when j ≥ n, the first binomial coefficient of the inner sum is zero, because its upper index is between 0 and k - 1, thus strictly less than the lower index 2k. We may therefore place the additional restriction j < n on the outer sum, without affecting which nonzero terms are included. This makes the restriction k ≥ j - n + 1 superfluous, and we can use the result of Problem 6. The double sum now comes tumbling down:



The inner sums vanish except when j = n - 1, so we get a simple closed form as our answer.


Problem 8: A different obstacle.
Let's branch out from Problem 6 in another way by considering the sum



Again, when m = 0 we have the sum we did before; but now the m occurs in a different place. This problem is a bit harder yet than Problem 7, but (fortunately) we're getting better at finding solutions. We can begin as in Problem 6,



Now (as in Problem 7) we try to expand the part that depends on m into terms that we know how to deal with. When m was zero, we absorbed k + 1 into ; if m > 0, we can do the same thing if we expand 1/(k + 1 + m) into absorbable terms. And our luck still holds: We proved a suitable identity



in Problem 1. Replacing r by -k - 2 gives the desired expansion,



Now the (k + 1)-1 can be absorbed into , as planned. In fact, it could also be absorbed into . Double absorption suggests that even more cancellation might be possible behind the scenes. Yes—expanding everything in our new summand into factorials and going back to binomial coefficients gives a formula that we can sum on k:



They expect us to check this on a sheet of scratch paper.
The sum over all integers j is zero, by (5.24). Hence -Sm is the sum for j < 0.
To evaluate -Sm for j < 0, let's replace j by -k - 1 and sum for k ≥ 0:



Finally (5.25) applies, and we have our answer:



Whew; we'd better check it. When n = 2 we find



Our derivation requires m to be an integer, but the result holds for all real m, because the quantity  is a polynomial in m of degree ≤ n.


5.3 Tricks of the Trade
Let's look next at three techniques that significantly amplify the methods we have already learned.


Trick 1: Going halves.
Many of our identities involve an arbitrary real number r. When r has the special form "integer minus one half," the binomial coefficient  can be written as a quite different-looking product of binomial coefficients. This leads to a new family of identities that can be manipulated with surprising ease.
This should really be called Trick 1/2.
One way to see how this works is to begin with the duplication formula



This identity is obvious if we expand the falling powers and interleave the factors on the left side:



Now we can divide both sides by k!2, and we get



If we set k = r = n, where n is an integer, this yields



And negating the upper index gives yet another useful formula,



For example, when n = 4 we have



. . . we halve . . .
Notice how we've changed a product of odd numbers into a factorial.
Identity (5.35) has an amusing corollary. Let , and take the sum over all integers k. The result is



by (5.23), because either n/2 or (n - 1)/2 is n/2, a nonnegative integer!
We can also use Vandermonde's convolution (5.27) to deduce that



Plugging in the values from (5.37) gives



this is what sums to (-1)n. Hence we have a remarkable property of the "middle" elements of Pascal's triangle:



For example, .
These illustrations of our first trick indicate that it's wise to try changing binomial coefficients of the form  into binomial coefficients of the form , where n is some appropriate integer (usually 0, 1, or k); the resulting formula might be much simpler.


Trick 2: High-order differences.
We saw earlier that it's possible to evaluate partial sums of the series , but not of the series . It turns out that there are many important applications of binomial coefficients with alternating signs, . One of the reasons for this is that such coefficients are intimately associated with the difference operator Δ defined in Section 2.6.
The difference Δf of a function f at the point x is
Δf(x) = f(x + 1) - f(x);
if we apply Δ again, we get the second difference


Δ2 f(x) = Δf(x + 1) - Δf(x)
= (f(x + 2) - f(x + 1)) - (f(x+1) - f(x))


 
= f(x + 2) - 2f(x + 1) + f(x),


which is analogous to the second derivative. Similarly, we have
Δ3 f(x) = f(x + 3) - 3f(x + 2) + 3f(x + 1) - f(x);
Δ4 f(x) = f(x + 4) - 4f(x + 3) + 6f(x + 2) - 4f(x + 1) + f(x);
and so on. Binomial coefficients enter these formulas with alternating signs.
In general, the nth difference is



This formula is easily proved by induction, but there's also a nice way to prove it directly using the elementary theory of operators. Recall that Section 2.6 defines the shift operator E by the rule
Ef(x) = f(x + 1);
hence the operator Δ is E - 1, where 1 is the identity operator defined by the rule 1f(x) = f(x). By the binomial theorem,



This is an equation whose elements are operators; it is equivalent to (5.40), since Ek is the operator that takes f(x) into f(x + k).
An interesting and important case arises when we consider negative falling powers. Let . Then, by rule (2.45), we have , , and in general



Equation (5.40) now tells us that



For example,



The sum in (5.41) is the partial fraction expansion of n!/(x(x + 1) . . . (x + n)).
Significant results can be obtained from positive falling powers too. If f(x) is a polynomial of degree d, the difference Δf(x) is a polynomial of degree d-1; therefore Δd f(x) is a constant, and Δn f(x) = 0 if n > d. This extremely important fact simplifies many formulas.
A closer look gives further information: Let
f(x) = adxd + ad-1xd-1 + · · · + a1x1 + a0x0
be any polynomial of degree d. We will see in Chapter 6 that we can express ordinary powers as sums of falling powers (for example, x2 = x2 + x1); hence there are coefficients bd, bd-1, . . . , b1, b0 such that
f(x) = bdxd + bd-1xd-1 + · · · + b1x1 + b0x0 .
(It turns out that bd = ad and b0 = a0, but the intervening coefficients are related in a more complicated way.) Let ck = k! bk for 0 ≤ k ≤ d. Then



thus, any polynomial can be represented as a sum of multiples of binomial coefficients. Such an expansion is called the Newton series of f(x), because Isaac Newton used it extensively.
We observed earlier in this chapter that the addition formula implies



Therefore, by induction, the nth difference of a Newton series is very simple:



If we now set x = 0, all terms  on the right side are zero, except the term with k - n = 0; hence



The Newton series for f(x) is therefore



For example, suppose f(x) = x3. It's easy to calculate
f(0) = 0,        f(1) = 1,        f(2) = 8,        f(3) = 27;
Δf(0) = 1,        Δf(1) = 7,        Δf(2) = 19;
Δ2 f(0) = 6,        Δ2 f(1) = 12;
Δ3 f(0) = 6.
So the Newton series is .
Our formula Δn f(0) = cn can also be stated in the following way, using (5.40) with x = 0:



Here c0, c1, c2, . . .  is an arbitrary sequence of coefficients; the infinite sum  is actually finite for all k ≥ 0, so convergence is not an issue. In particular, we can prove the important identity



because the polynomial a0 + a1k + · · · + ankn can always be written as a Newton series  with cn = n! an.
Many sums that appear to be hopeless at first glance can actually be summed almost trivially by using the idea of nth differences. For example, let's consider the identity



This looks very impressive, because it's quite different from anything we've seen so far. But it really is easy to understand, once we notice the telltale factor  in the summand, because the function



is a polynomial in k of degree n, with leading coefficient (-1)nsn/n!. Therefore (5.43) is nothing more than an application of (5.42).
We have discussed Newton series under the assumption that f(x) is a polynomial. But we've also seen that infinite Newton series



make sense too, because such sums are always finite when x is a nonnegative integer. Our derivation of the formula Δn f(0) = cn works in the infinite case, just as in the polynomial case; so we have the general identity



This formula is valid for any function f(x) that is defined for nonnegative integers x. Moreover, if the right-hand side converges for other values of x, it defines a function that "interpolates" f(x) in a natural way. (There are infinitely many ways to interpolate function values, so we cannot assert that (5.44) is true for all x that make the infinite series converge. For example, if we let f(x) = sin(πx), we have f(x) = 0 at all integer points, so the right-hand side of (5.44) is identically zero; but the left-hand side is nonzero at all noninteger x.)
A Newton series is finite calculus's answer to infinite calculus's Taylor series. Just as a Taylor series can be written



the Newton series for f(x) = g(a + x) can be written
(Since E = 1 + Δ, ; and Exg(a) = g(a + x).)



(This is the same as (5.44), because Δn f(0) = Δn g(a) for all n ≥ 0 when f(x) = g(a + x).) Both the Taylor and Newton series are finite when g is a polynomial, or when x = 0; in addition, the Newton series is finite when x is a positive integer. Otherwise the sums may or may not converge for particular values of x. If the Newton series converges when x is not a nonnegative integer, it might actually converge to a value that's different from g(a + x), because the Newton series (5.45) depends only on the spaced-out function values g(a), g(a + 1), g(a + 2), . . . .
One example of a convergent Newton series is provided by the binomial theorem. Let g(x) = (1 + z)x, where z is a fixed complex number such that |z| < 1. Then Δg(x) = (1 + z)x+1 - (1 + z)x = z(1 + z)x, hence Δn g(x) = zn(1 + z)x. In this case the infinite Newton series



converges to the "correct" value (1 + z)a+x, for all x.
James Stirling tried to use Newton series to generalize the factorial function to noninteger values. First he found coefficients Sn such that



is an identity for x = 0, x = 1, x = 2, etc. But he discovered that the resulting series doesn't converge except when x is a nonnegative integer. So he tried again, this time writing



"Forasmuch as these terms increase very fast, their differences will make a diverging progression, which hinders the ordinate of the parabola from approaching to the truth; therefore in this and the like cases, I interpolate the logarithms of the terms, whose differences constitute a series swiftly converging."
—J. Stirling [343]
Now Δ(ln x!) = ln(x + 1)! - ln x! = ln(x + 1), hence



by (5.40). The coefficients are therefore s0 = s1 = 0; s2 = ln 2; ; ; etc. In this way Stirling obtained a series that does converge (although he didn't prove it); in fact, his series converges for all x > -1. He was thereby able to evaluate ! satisfactorily. Exercise 88 tells the rest of the story.
(Proofs of convergence were not invented until the nineteenth century.)


Trick 3: Inversion.
A special case of the rule (5.44) we've just derived for Newton's series can be rewritten in the following way using (5.40):



This dual relationship between f and g is called an inversion formula; it's rather like the Möbius inversion formulas (4.56) and (4.61) that we encountered in Chapter 4. Inversion formulas tell us how to solve "implicit recurrences," where an unknown sequence is embedded in a sum.
Invert this: 'zınb ppo'.
For example, g(n) might be a known function, and f(n) might be unknown; and we might have found a way to show that . Then (5.48) lets us express f(n) as a sum of known values.
We can prove (5.48) directly by using the basic methods at the beginning of this chapter. If  for all n ≥ 0, then



The proof in the other direction is, of course, the same, because the relation between f and g is symmetric.
Let's illustrate (5.48) by applying it to the "football victory problem": A group of n fans of the winning football team throw their hats high into the air. The hats come back randomly, one hat to each of the n fans. How many ways h(n, k) are there for exactly k fans to get their own hats back?
For example, if n = 4 and if the hats and fans are named A, B, C, D, the 4! = 24 possible ways for hats to land generate the following numbers of rightful owners:
ABCD   4        BACD   2        CABD   1        DABC   0ABDC   2        BADC   0        CADB   0        DACB   1ACBD   2        BCAD   1        CBAD   2        DBAC   1ACDB   1        BCDA   0        CBDA   1        DBCA   2ADBC   1        BDAC   0        CDAB   0        DCAB   0ADCB   2        BDCA   1        CDBA   0        DCBA   0
Therefore h(4, 4) = 1; h(4, 3) = 0; h(4, 2) = 6; h(4, 1) = 8; h(4, 0) = 9.
We can determine h(n, k) by noticing that it is the number of ways to choose k lucky hat owners, namely , times the number of ways to arrange the remaining n-k hats so that none of them goes to the right owner, namely h(n - k, 0). A permutation is called a derangement if it moves every item, and the number of derangements of n objects is sometimes denoted by the symbol 'n¡', read "n subfactorial." Therefore h(n - k, 0) = (n - k)¡, and we have the general formula



(Subfactorial notation isn't standard, and it's not clearly a great idea; but let's try it a while to see if we grow to like it. We can always resort to 'Dn' or something, if 'n¡' doesn't work out.)
Our problem would be solved if we had a closed form for n¡, so let's see what we can find. There's an easy way to get a recurrence, because the sum of h(n, k) for all k is the total number of permutations of n hats:



(We've changed k to n - k and  to  in the last step.) With this implicit recurrence we can compute all the h(n, k)'s we like:



For example, here's how the row for n = 4 can be computed: The two rightmost entries are obvious—there's just one way for all hats to land correctly, and there's no way for just three fans to get their own. (Whose hat would the fourth fan get?) When k = 2 and k = 1, we can use our equation for h(n, k), giving , and . We can't use this equation for h(4, 0); rather, we can, but it gives us , which is true but useless. Taking another tack, we can use the relation h(4, 0) + 8 + 6 + 0 + 1 = 4! to deduce that h(4, 0) = 9; this is the value of 4¡. Similarly n¡ depends on the values of k¡ for k < n.
The art of mathematics, as of life, is knowing which truths are useless.
How can we solve a recurrence like (5.49)? Easy; it has the form of (5.48), with g(n) = n! and f(k) = (-1)kk¡. Hence its solution is



Well, this isn't really a solution; it's a sum that should be put into closed form if possible. But it's better than a recurrence. The sum can be simplified, since k! cancels with a hidden k! in , so let's try that: We get



The remaining sum converges rapidly to the number . In fact, the terms that are excluded from the sum are



and the parenthesized quantity lies between 1 and . Therefore the difference between n¡ and n!/e is roughly 1/n in absolute value; more precisely, it lies between 1/(n + 1) and 1/(n + 2). But n¡ is an integer. Therefore it must be what we get when we round n!/e to the nearest integer, if n > 0. So we have the closed form we seek:



This is the number of ways that no fan gets the right hat back. When n is large, it's more meaningful to know the probability that this happens. If we assume that each of the n! arrangements is equally likely—because the hats were thrown extremely high—this probability is



Baseball fans: .367 is also Ty Cobb's lifetime batting average, the all-time record. Can this be a coincidence?
(Hey wait, you're fudging. Cobb's average was 4191/11429 ≈ .366699, while 1/e ≈ .367879. But maybe if Wade Boggs has a few really good seasons. . . )
So when n gets large the probability that all hats are misplaced is almost 37%.
Incidentally, recurrence (5.49) for subfactorials is exactly the same as (5.46), the first recurrence considered by Stirling when he was trying to generalize the factorial function. Hence Sk = k¡. These coefficients are so large, it's no wonder the infinite series (5.46) diverges for noninteger x.
Before leaving this problem, let's look briefly at two interesting patterns that leap out at us in the table of small h(n, k). First, it seems that the numbers 1, 3, 6, 10, 15, . . . below the all-0 diagonal are the triangular numbers. This observation is easy to prove, since those table entries are the h(n, n-2)'s, and we have



It also seems that the numbers in the first two columns differ by ±1. Is this always true? Yes,



In other words, n¡ = n(n - 1)¡ + (-1)n. This is a much simpler recurrence for the derangement numbers than we had before.
Now let's invert something else. If we apply inversion to the formula



But inversion is the source of smog.
that we derived in (5.41), we find



This is interesting, but not really new. If we negate the upper index in , we have merely discovered identity (5.33) again.


5.4 Generating Functions
We come now to the most important idea in this whole book, the notion of a generating function. An infinite sequence a0, a1, a2, . . .  that we wish to deal with in some way can conveniently be represented as a power series in an auxiliary variable z,



It's appropriate to use the letter z as the name of the auxiliary variable, because we'll often be thinking of z as a complex number. The theory of complex variables conventionally uses 'z' in its formulas; power series (a.k.a. analytic functions or holomorphic functions) are central to that theory.
We will be seeing lots of generating functions in subsequent chapters. Indeed, Chapter 7 is entirely devoted to them. Our present goal is simply to introduce the basic concepts, and to demonstrate the relevance of generating functions to the study of binomial coefficients.
A generating function is useful because it's a single quantity that represents an entire infinite sequence. We can often solve problems by first setting up one or more generating functions, then by fooling around with those functions until we know a lot about them, and finally by looking again at the coefficients. With a little bit of luck, we'll know enough about the function to understand what we need to know about its coefficients.
If A(z) is any power series Σk≥0 akzk, we will find it convenient to write



(See [223] for a discussion of the history and usefulness of this notation.)
in other words, [zn] A(z) denotes the coefficient of zn in A(z).
Let A(z) be the generating function for a0, a1, a2, . . .  as in (5.52), and let B(z) be the generating function for another sequence b0, b1, b2, . . . . Then the product A(z)B(z) is the power series



the coefficient of zn in this product is



Therefore if we wish to evaluate any sum that has the general form



and if we know the generating functions A(z) and B(z), we have
cn = [zn] A(z)B(z) .
The sequence cn defined by (5.54) is called the convolution of the sequences an and bn; two sequences are "convolved" by forming the sums of all products whose subscripts add up to a given amount. The gist of the previous paragraph is that convolution of sequences corresponds to multiplication of their generating functions.
Generating functions give us powerful ways to discover and/or prove identities. For example, the binomial theorem tells us that (1 + z)r is the generating function for the sequence :



Similarly,



If we multiply these together, we get another generating function:
(1 + z)r(1 + z)s = (1 + z)r+s .
And now comes the punch line: Equating coefficients of zn on both sides of this equation gives us



We've discovered Vandermonde's convolution, (5.27)!
(5.27)! = (5.27)(4.27)(3.27)(2.27)(1.27)(0.27)!.
That was nice and easy; let's try another. This time we use (1-z)r, which is the generating function for the sequence . Multiplying by (1 + z)r gives another generating function whose coefficients we know:
(1 - z)r(1 + z)r = (1 - z2)r .
Equating coefficients of zn now gives the equation



We should check this on a small case or two. When n = 3, for example, the result is



Each positive term is cancelled by a corresponding negative term. And the same thing happens whenever n is odd, in which case the sum isn't very interesting. But when n is even, say n = 2, we get a nontrivial sum that's different from Vandermonde's convolution:



So (5.55) checks out fine when n = 2. It turns out that (5.30) is a special case of our new identity (5.55).
Binomial coefficients also show up in some other generating functions, most notably the following important identities in which the lower index stays fixed and the upper index varies:






If you have a highlighter pen, these two equations have got to be marked.
The second identity here is just the first one multiplied by zn, that is, "shifted right" by n places. The first identity is just a special case of the binomial theorem in slight disguise: If we expand (1 - z)-n-1 by (5.13), the coefficient of zk is , which can be rewritten as  or  by negating the upper index. These special cases are worth noting explicitly, because they arise so frequently in applications.
When n = 0 we get a special case of a special case, the geometric series:



This is the generating function for the sequence 1, 1, 1, . . . , and it is especially useful because the convolution of any other sequence with this one is the sequence of sums: When bk = 1 for all k, (5.54) reduces to



Therefore if A(z) is the generating function for the summands a0, a1, a2, . . . , then A(z)/(1 - z) is the generating function for the sums c0, c1, c2, . . . .
The problem of derangements, which we solved by inversion in connection with hats and football fans, can be resolved with generating functions in an interesting way. The basic recurrence



can be put into the form of a convolution if we expand  in factorials and divide both sides by n!:



The generating function for the sequence  is ez; hence if we let



the convolution/recurrence tells us that



Solving for D(z) gives



Equating coefficients of zn now tells us that



this is the formula we derived earlier by inversion.
So far our explorations with generating functions have given us slick proofs of things that we already knew how to derive by more cumbersome methods. But we haven't used generating functions to obtain any new results, except for (5.55). Now we're ready for something new and more surprising. There are two families of power series that generate an especially rich class of binomial coefficient identities: Let us define the generalized binomial series t(z) and the generalized exponential series t(z) as follows:



We will prove in Section 7.5 that these functions satisfy the identities



In the special case t = 0, we have
0(z) = 1 + z ;                    0(z) = ez ;
this explains why the series with parameter t are called "generalized" binomials and exponentials.
The following pairs of identities are valid for all real r:






The generalized binomial series t(z) was discovered in the 1750s by J. H. Lambert [236, §38], who noticed a few years later [237] that its powers satisfy the first identity in (5.60). Exercise 84 explains how to derive (5.61) from (5.60).
(When tk + r = 0, we have to be a little careful about how the coefficient of zk is interpreted; each coefficient is a polynomial in r. For example, the constant term of t(z)r is r(0 + r)-1, and this is equal to 1 even when r = 0.)
Since equations (5.60) and (5.61) hold for all r, we get very general identities when we multiply together the series that correspond to different powers r and s. For example,



This power series must equal



News flash:ln  ;ln  .
hence we can equate coefficients of zn and get the identity



valid for all real r, s, and t. When t = 0 this identity reduces to Vandermonde's convolution. (If by chance tk + r happens to equal zero in this formula, the denominator factor tk + r should be considered to cancel with the tk + r in the numerator of the binomial coefficient. Both sides of the identity are polynomials in r, s, and t.) Similar identities hold when we multiply t(z)r by t(z)s, etc.; Table 202 presents the results.


Table 202 General convolution identities, valid for integer n ≥ 0.



We have learned that it's generally a good idea to look at special cases of general results. What happens, for example, if we set t = 1? The generalized binomial 1(z) is very simple—it's just



therefore 1(z) doesn't give us anything we didn't already know from Vandermonde's convolution. But 1(z) is an important function,



which we haven't seen before; it satisfies the basic identity



Aha! This is the iterated power function  that I've often wondered about.

This function, first studied by Euler [117] and Eisenstein [91], arises in a great many applications [193, 204].
The special cases t = 2 and t = -1 of the generalized binomial are of particular interest, because their coefficients occur again and again in problems that have a recursive structure. Therefore it's useful to display these series explicitly for future reference:


















The power series for  is noteworthy too.
The coefficients  of 2(z) are called the Catalan numbers Cn, because Eugène Catalan wrote an influential paper about them in the 1830s [52]. The sequence begins as follows:



The coefficients of -1(z) are essentially the same, but there's an extra 1 at the beginning and the other numbers alternate in sign: 1, 1, -1, 2, -5, 14, . . . . Thus -1(z) = 1 + z2(-z). We also have -1(z) = 2(-z)-1.
Let's close this section by deriving an important consequence of (5.72) and (5.73), a relation that shows further connections between the functions -1(z) and 2(-z):



This holds because the coefficient of zk in  is



when k > n. The terms nicely cancel each other out. We can now use (5.68) and (5.69) to obtain the closed form



(The special case z = -1 came up in Problem 3 of Section 5.2. Since the numbers  are sixth roots of unity, the sums  have the periodic behavior we observed in that problem.) Similarly we can combine (5.70) with (5.71) to cancel the large coefficients and get





5.5 Hypergeometric Functions
The methods we've been applying to binomial coefficients are very effective, when they work, but we must admit that they often appear to be ad hoc—more like tricks than techniques. When we're working on a problem, we often have many directions to pursue, and we might find ourselves going around in circles. Binomial coefficients are like chameleons, changing their appearance easily. Therefore it's natural to ask if there isn't some unifying principle that will systematically handle a great variety of binomial coefficient summations all at once. Fortunately, the answer is yes. The unifying principle is based on the theory of certain infinite sums called hypergeometric series.
They're even more versatile than chameleons; we can dissect them and put them back together in different ways.
The study of hypergeometric series was launched many years ago by Euler, Gauss, and Riemann; such series, in fact, are still the subject of considerable research. But hypergeometrics have a somewhat formidable notation, which takes a little time to get used to.
Anything that has survived for centuries with such awesome notation must be really useful.
The general hypergeometric series is a power series in z with m + n parameters, and it is defined as follows in terms of rising factorial powers:



To avoid division by zero, none of the b's may be zero or a negative integer. Other than that, the a's and b's may be anything we like. The notation 'F(a1, . . . , am; b1, . . . , bn; z)' is also used as an alternative to the two-line form (5.76), since a one-line form sometimes works better typographically. The a's are said to be upper parameters; they occur in the numerator of the terms of F. The b's are lower parameters, and they occur in the denominator. The final quantity z is called the argument.
Standard reference books often use 'mFn' instead of 'F' as the name of a hypergeometric with m upper parameters and n lower parameters. But the extra subscripts tend to clutter up the formulas and waste our time, if we're compelled to write them over and over. We can count how many parameters there are, so we usually don't need extra additional unnecessary redundancy.
Many important functions occur as special cases of the general hypergeometric; indeed, that's why hypergeometrics are so powerful. For example, the simplest case occurs when m = n = 0: There are no parameters at all, and we get the familiar series



Actually the notation looks a bit unsettling when m or n is zero. We can add an extra '1' above and below in order to avoid this:



In general we don't change the function if we cancel a parameter that occurs in both numerator and denominator, or if we insert two identical parameters.
The next simplest case has m = 1, a1 = 1, and n = 0; we change the parameters to m = 2, a1 = a2 = 1, n = 1, and b1 = 1, so that n > 0. This series also turns out to be familiar, because  :



It's our old friend, the geometric series; F(a1, . . . , am; b1, . . . , bn; z) is called hypergeometric because it includes the geometric series F(1, 1; 1; z) as a very special case.
The general case m = 1 and n = 0 is, in fact, easy to sum in closed form,



using (5.13) and (5.14). If we replace a by -a and z by -z, we get the binomial theorem,



A negative integer as upper parameter causes the infinite series to become finite, since  whenever k > a ≥ 0 and a is an integer.
The general case m = 0, n = 1 is another famous series, but it's not as well known in the literature of discrete mathematics:



This function Ib-1 is called a "modified Bessel function" of order b - 1. The special case b = 1 gives us , which is the interesting series ∑k≥0 zk/k!2.
The special case m = n = 1 is called a "confluent hypergeometric series" and often denoted by the letter M:



This function, which has important applications to engineering, was introduced by Ernst Kummer.
By now a few of us are wondering why we haven't discussed convergence of the infinite series (5.76). The answer is that we can ignore convergence if we are using z simply as a formal symbol. It is not difficult to verify that formal infinite sums of the form ∑k≥n αkzk with -∞ < n < ∞ form a field, if the coefficients αk lie in a field. We can add, subtract, multiply, divide, differentiate, and do functional composition on such formal sums without worrying about convergence; any identities we derive will still be formally true. For example, the hypergeometric  doesn't converge for any nonzero z; yet we'll see in Chapter 7 that we can still use it to solve problems. On the other hand, whenever we replace z by a particular numerical value, we do have to be sure that the infinite sum is well defined.
We didn't discuss convergence of (5.56), (5.57), (5.58), . . . either.
The next step up in complication is actually the most famous hypergeometric of all. In fact, it was the hypergeometric series until about 1870, when everything was generalized to arbitrary m and n. This one has two upper parameters and one lower parameter:



It is often called the Gaussian hypergeometric, because many of its subtle properties were first proved by Gauss in his doctoral dissertation of 1812 [143], although Euler [118] and Pfaff [292] had already discovered some remarkable things about it. One of its important special cases is



Notice that z-1 ln(1 + z) is a hypergeometric function, but ln(1 + z) itself cannot be hypergeometric, since a hypergeometric series always has the value 1 when z = 0.
"There must be many universities to-day where 95 per cent, if not 100 per cent, of the functions studied by physics, engineering, and even mathematics students, are covered by this single symbol F(a, b; c; x)."
—W. W. Sawyer [318]
So far hypergeometrics haven't actually done anything for us except provide an excuse for name-dropping. But we've seen that several very different functions can all be regarded as hypergeometric; this will be the main point of interest in what follows. We'll see that a large class of sums can be written as hypergeometric series in a "canonical" way, hence we will have a good filing system for facts about binomial coefficients.
What series are hypergeometric? It's easy to answer this question if we look at the ratio between consecutive terms:



The first term is t0 = 1, and the other terms have ratios given by



This is a rational function of k, that is, a quotient of polynomials in k. According to the Fundamental Theorem of Algebra, any rational function of k can be factored over the complex numbers and put into this form. The a's are the negatives of the roots of the polynomial in the numerator, and the b's are the negatives of the roots of the polynomial in the denominator. If the denominator doesn't already contain the special factor (k + 1), we can include (k + 1) in both numerator and denominator. A constant factor remains, and we can call it z. Therefore hypergeometric series are precisely those series whose first term is 1 and whose term ratio tk+1/tk is a rational function of k.
Suppose, for example, that we're given an infinite series with term ratio



a rational function of k. The numerator polynomial splits nicely into two factors, (k + 2)(k + 5), and the denominator is 4(k + i/2)(k - i/2). Since the denominator is missing the required factor (k + 1), we write the term ratio as



and we can read off the results: The given series is



Thus, we have a general method for finding the hypergeometric representation of a given quantity S, when such a representation is possible: First we write S as an infinite series whose first term is nonzero. We choose a notation so that the series is ∑k≥0 tk with t0 ≠ 0. Then we calculate tk+1/tk. If the term ratio is not a rational function of k, we're out of luck. Otherwise we express it in the form (5.81); this gives parameters a1, . . . , am, b1, . . . , bn, and an argument z, such that S = t0 F(a1, . . . , am; b1, . . . , bn; z).
(Now is a good time to do warmup exercise 11.)
Gauss's hypergeometric series can be written in the recursively factored form



if we wish to emphasize the importance of term ratios.
Let's try now to reformulate the binomial coefficient identities derived earlier in this chapter, expressing them as hypergeometrics. For example, let's figure out what the parallel summation law,



looks like in hypergeometric notation. We need to write the sum as an infinite series that starts at k = 0, so we replace k by n - k:



This series is formally infinite but actually finite, because the (n - k)! in the denominator will make tk = 0 when k > n. (We'll see later that 1/x! is defined for all x, and that 1/x! = 0 when x is a negative integer. But for now, let's blithely disregard such technicalities until we gain more hypergeometric experience.) The term ratio is



Furthermore t0 = . Hence the parallel summation law is equivalent to the hypergeometric identity



Dividing through by  gives a slightly simpler version,



Let's do another one. The term ratio of identity (5.16),



is (k - m)/(r - m + k + 1) = (k + 1)(k - m)(1)/(k - m + r + 1)(k + 1), after we replace k by m - k; hence (5.16) gives a closed form for



This is essentially the same as the hypergeometric function on the left of (5.82), but with m in place of n and r + 1 in place of -r. Therefore identity (5.16) could have been derived from (5.82), the hypergeometric version of (5.9). (No wonder we found it easy to prove (5.16) by using (5.9).)
First derangements, now degenerates.
Before we go further, we should think about degenerate cases, because hypergeometrics are not defined when a lower parameter is zero or a negative integer. We usually apply the parallel summation identity when r and n are positive integers; but then -n-r is a negative integer and the hypergeometric (5.76) is undefined. How then can we consider (5.82) to be legitimate? The answer is that we can take the limit of  as  → 0.
We will look at such things more closely later in this chapter, but for now let's just be aware that some denominators can be dynamite. It is interesting, however, that the very first sum we've tried to express hypergeometrically has turned out to be degenerate.
(We proved the identities originally for integer r, and used the polynomial argument to show that they hold in general. Now we're proving them first for irrational r, and using a limiting argument to show that they hold for integers!)
Another possibly sore point in our derivation of (5.82) is that we expanded  as (r + n - k)!/r! (n - k)!. This expansion fails when r is a negative integer, because (-m)! has to be ∞ if the law
0! = 0 · (-1) · (-2) · . . . · (-m + 1) · (-m)!
is going to hold. Again, we need to approach integer results by considering a limit of r +  as  → 0.
But we defined the factorial representation  only when r is an integer! If we want to work effectively with hypergeometrics, we need a factorial function that is defined for all complex numbers. Fortunately there is such a function, and it can be defined in many ways. Here's one of the most useful definitions of z!, actually a definition of 1/z!:



(See exercise 21. Euler [99, 100, 72] discovered this when he was 22 years old.) The limit can be shown to exist for all complex z, and it is zero only when z is a negative integer. Another significant definition is



This integral exists only when the real part of z exceeds -1, but we can use the formula



to extend the definition to all complex z (except negative integers). Still another definition comes from Stirling's interpolation of ln z! in (5.47). All of these approaches lead to the same generalized factorial function.
There's a very similar function called the Gamma function, which relates to ordinary factorials somewhat as rising powers relate to falling powers. Standard reference books often use factorials and Gamma functions simultaneously, and it's convenient to convert between them if necessary using the following formulas:






We can use these generalized factorials to define generalized factorial powers, when z and w are arbitrary complex numbers:






How do you write z to the  power, when  is the complex conjugate of w?

The only proviso is that we must use appropriate limiting values when these formulas give ∞/∞. (The formulas never give 0/0, because factorials and Gamma-function values are never zero.) A binomial coefficient can be written



when z and w are any complex numbers whatever.
I see, the lower index arrives at its limit first. That's why  is zero when w is a negative integer. The value is infinite when z is a negative integer and w is not an integer.
Armed with generalized factorial tools, we can return to our goal of reducing the identities derived earlier to their hypergeometric essences. The binomial theorem (5.13) turns out to be neither more nor less than (5.77), as we might expect. So the next most interesting identity to try is Vandermonde's convolution (5.27):



The kth term here is



and we are no longer too shy to use generalized factorials in these expressions. Whenever tk contains a factor like (α + k)!, with a plus sign before the k, we get (α + k + 1)!/(α + k)! = k + α + 1 in the term ratio tk+1/tk, by (5.85); this contributes the parameter 'α + 1' to the corresponding hypergeometric—as an upper parameter if (α + k)! was in the numerator of tk, but as a lower parameter otherwise. Similarly, a factor like (α - k)! leads to (α - k - 1)!/(α - k)! = (-1)/(k - α); this contributes '-α' to the opposite set of parameters (reversing the roles of upper and lower), and negates the hypergeometric argument. Factors like r!, which are independent of k, go into t0 but disappear from the term ratio. Using such tricks we can predict without further calculation that the term ratio of (5.27) is



times (-1)2 = 1, and Vandermonde's convolution becomes



We can use this equation to determine F(a, b; c; z) in general, when z = 1 and when b is a negative integer.
Let's rewrite (5.91) in a form so that table lookup is easy when a new sum needs to be evaluated. The result turns out to be



Vandermonde's convolution (5.27) covers only the case that one of the upper parameters, say b, is a nonpositive integer; but Gauss [143] proved that (5.92) is valid also when a, b, c are complex numbers whose real parts satisfy ℜc > ℜa + ℜb. In other cases, the infinite series  doesn't converge. When b = -n, the identity can be written more conveniently with factorial powers instead of Gamma functions:



A few weeks ago, we were studying what Gauss had done in kindergarten. Now we're studying stuff beyond his Ph.D. thesis. Is this intimidating or what?
It turns out that all five of the identities in Table 169 are special cases of Vandermonde's convolution; formula (5.93) covers them all, when proper attention is paid to degenerate situations.
Notice that (5.82) is just the special case a = 1 of (5.93). Therefore we don't really need to remember (5.82); and we don't really need the identity (5.9) that led us to (5.82), even though Table 174 said that it was memorable. A computer program for formula manipulation, faced with the problem of evaluating , could convert the sum to a hypergeometric and plug into the general identity for Vandermonde's convolution.
Problem 1 in Section 5.2 asked for the value of



This problem is a natural for hypergeometrics, and after a bit of practice any hypergeometer can read off the parameters immediately as F(1, -m; -n; 1). Hmmm; that problem was yet another special takeoff on Vandermonde!
The sum in Problem 2 and Problem 4 likewise yields F(2, 1 - n; 2 - m; 1). (We need to replace k by k + 1 first.) And the "menacing" sum in Problem 6 turns out to be just F(n + 1, -n; 2; 1). Is there nothing more to sum, besides disguised versions of Vandermonde's powerful convolution?
Well, yes, Problem 3 is a bit different. It deals with a special case of the general sum  considered in (5.74), and this leads to a closed-form expression for



We also proved something new in (5.55), when we looked at the coefficients of (1 - z)r(1 + z)r:



This is called Kummer's formula when it's generalized to complex numbers:



Kummer was a summer.
(Ernst Kummer [229] proved this in 1836.)
The summer of '36.
It's interesting to compare these two formulas. Replacing c by 1-2n-a, we find that the results are consistent if and only if



when n is a positive integer. Suppose, for example, that n = 3; then we should have -6!/3! = limx→-3 x!/(2x)!. We know that (-3)! and (-6)! are both infinite; but we might choose to ignore that difficulty and to imagine that (-3)! = (-3)(-4)(-5)(-6)!, so that the two occurrences of (-6)! will cancel. Such temptations must, however, be resisted, because they lead to the wrong answer! The limit of x!/(2x)! as x → -3 is not (-3)(-4)(-5) but rather -6!/3! = (-4)(-5)(-6), according to (5.95).
The right way to evaluate the limit in (5.95) is to use equation (5.87), which relates negative-argument factorials to positive-argument Gamma functions. If we replace x by -n -  and let  → 0, two applications of (5.87) give



Now sin(x + y) = sin x cos y + cos x sin y; so this ratio of sines is



by the methods of Chapter 9. Therefore, by (5.86), we have



as desired.
Let's complete our survey by restating the other identities we've seen so far in this chapter, clothing them in hypergeometric garb. The triple-binomial sum in (5.29) can be written



When this one is generalized to complex numbers, it is called Dixon's formula:



One of the most general formulas we've encountered is the triple-binomial sum (5.28), which yields Saalschütz's identity:



(Historical note: Saalschütz [315] independently discovered this formula almost 100 years after Pfaff [292] had first published it. Taking the limit as n → ∞ yields equation (5.92).)
This formula gives the value at z = 1 of the general hypergeometric series with three upper parameters and two lower parameters, provided that one of the upper parameters is a nonpositive integer and that b1 + b2 = a1 + a2 + a3 + 1. (If the sum of the lower parameters exceeds the sum of the upper parameters by 2 instead of by 1, the formula of exercise 25 can be used to express F(a1, a2, a3; b1, b2; 1) in terms of two hypergeometrics that satisfy Saalschütz's identity.)
Our hard-won identity in Problem 8 of Section 5.2 reduces to



Sigh. This is just the special case c = 1 of Saalschütz's identity (5.97), so we could have saved a lot of work by going to hypergeometrics directly!
What about Problem 7? That extra-menacing sum gives us the formula



which is the first case we've seen with three lower parameters. So it looks new. But it really isn't; the left-hand side can be replaced by a multiple of



using exercise 26, and Saalschütz's identity wins again.
Well, that's another deflating experience, but it's also another reason to appreciate the power of hypergeometric methods.
(Historical note: The great relevance of hypergeometric series to binomial coefficient identities was first pointed out by George Andrews in 1974 [9, section 5].)
The convolution identities in Table 202 do not have hypergeometric equivalents, because their term ratios are rational functions of k only when t is an integer. Equations (5.64) and (5.65) aren't hypergeometric even when t = 1. But we can take note of what (5.62) tells us when t has small integer values:



The first of these formulas gives the result of Problem 7 again, when the quantities (r, s, n) are replaced respectively by (1, m - 2n - 1, n - m).
Finally, the "unexpected" sum (5.20) gives us an unexpected hypergeometric identity that turns out to be quite instructive. Let's look at it in slow motion. First we convert to an infinite sum,



The term ratio from (2m - k)! 2k/m! (m - k)! is 2(k - m)/(k - 2m), so we have a hypergeometric identity with z = 2:



But look at the lower parameter '-2m'. Negative integers are verboten, so this identity is undefined!
It's high time to look at such limiting cases carefully, as promised earlier, because degenerate hypergeometrics can often be evaluated by approaching them from nearby nondegenerate points. We must be careful when we do this, because different results can be obtained if we take limits in different ways. For example, here are two limits that turn out to be quite different when one of the upper parameters is increased by :



Similarly, we have defined ; this is not the same as . The proper way to treat (5.98) as a limit is to realize that the upper parameter -m is being used to make all terms of the series  zero for k > m; this means that we want to make the following more precise statement:



Each term of this limit is well defined, because the denominator factor  does not become zero until k > 2m. Therefore this limit gives us exactly the sum (5.20) we began with.


5.6 Hypergeometric Transformations
It should be clear by now that a database of known hypergeometric closed forms is a useful tool for doing sums of binomial coefficients. We simply convert any given sum into its canonical hypergeometric form, then look it up in the table. If it's there, fine, we've got the answer. If not, we can add it to the database if the sum turns out to be expressible in closed form. We might also include entries in the table that say, "This sum does not have a simple closed form in general." For example, the sum  corresponds to the hypergeometric



this has a simple closed form only if m is near 0, , or n.
But there's more to the story, since hypergeometric functions also obey identities of their own. This means that every closed form for hypergeometrics leads to additional closed forms and to additional entries in the database. For example, the identities in exercises 25 and 26 tell us how to transform one hypergeometric into one or two others with similar but different parameters. These can in turn be transformed again.
The hypergeometric database should really be a "knowledge base."
In 1797, J. F. Pfaff [292] discovered a surprising reflection law,



which is a transformation of another type. This is a formal identity in power series, if the quantity (-z)k/(1 - z)k+a is replaced by the infinite series  when the left-hand side is expanded (see exercise 50). We can use this law to derive new formulas from the identities we already know, when z ≠ 1.
For example, Kummer's formula (5.94) can be combined with the reflection law (5.101) if we choose the parameters so that both identities apply:



We can now set a = -n and go back from this equation to a new identity in binomial coefficients that we might need some day:



For example, when n = 3 this identity says that



It's almost unbelievable, but true, for all b. (Except when a factor in the denominator vanishes.)
This is fun; let's try again. Maybe we'll find a formula that will really astonish our friends. What does Pfaff's reflection law tell us if we apply it to the strange form (5.99), where z = 2? In this case we set a = -m, b = 1, and c = -2m + , obtaining



because none of the limiting terms is close to zero. This leads to another miraculous formula,



(Hysterical note: See exercise 51 if you get a different result.)
When m = 3, for example, the sum is



and  is indeed equal to .
When we looked at our binomial coefficient identities and converted them to hypergeometric form, we overlooked (5.19) because it was a relation between two sums instead of a closed form. But now we can regard (5.19) as an identity between hypergeometric series. If we differentiate it n times with respect to y and then replace k by m - n - k, we get



This yields the following hypergeometric transformation:



Notice that when z = 1 this reduces to Vandermonde's convolution, (5.93).
Differentiation seems to be useful, if this example is any indication; we also found it helpful in Chapter 2, when summing x + 2x2 + · · · + nxn. Let's see what happens when a general hypergeometric series is differentiated with respect to z:



The parameters move out and shift up.
It's also possible to use differentiation to tweak just one of the parameters while holding the rest of them fixed. For this we use the operator



How do you pronounce ϑ ?
(Dunno, but  calls it 'vartheta'.)
which acts on a function by differentiating it and then multiplying by z. This operator gives



which by itself isn't too useful. But if we multiply F by one of its upper parameters, say a1, and add it to ϑF, we get



Only one parameter has been shifted.
A similar trick works with lower parameters, but in this case things shift down instead of up:



We can now combine all these operations and make a mathematical "pun" by expressing the same quantity in two different ways. Namely, we have



Ever hear the one about the brothers who named their cattle ranch Focus, because it's where the sons raise meat?
and



where F = F(a1, . . . , am; b1, . . . , bn; z). And (5.106) tells us that the top line is the derivative of the bottom line. Therefore the general hypergeometric function F satisfies the differential equation



where D is the operator .
This cries out for an example. Let's find the differential equation satisfied by the standard 2-over-1 hypergeometric series F(z) = F(a, b; c; z). According to (5.107), we have
D(ϑ + c - 1)F = (ϑ + a)(ϑ + b)F.
What does this mean in ordinary notation? Well, (ϑ + c - 1)F is zF′(z) + (c - 1)F(z), and the derivative of this gives the left-hand side,
F′(z) + zF″(z) + (c - 1)F′(z) .
On the right-hand side we have


(ϑ+a) zF′(z)+bF(z))
= (zF′(z)+bF(z)) + a(zF′(z)+bF(z))


 
= zF′(z)+z2F″(z)+bzF′(z)+azF′(z)+abF(z) .


Equating the two sides tells us that



This equation is equivalent to the factored form (5.107).
Conversely, we can go back from the differential equation to the power series. Let's assume that  is a power series satisfying (5.107). A straightforward calculation shows that we must have



hence F(z) must be t0 F(a1, . . . , am; b1, . . . , bn; z). We've proved that the hypergeometric series (5.76) is the only formal power series that satisfies the differential equation (5.107) and has the constant term 1.
It would be nice if hypergeometrics solved all the world's differential equations, but they don't quite. The right-hand side of (5.107) always expands into a sum of terms of the form αkzkF(k)(z), where F(k)(z) is the kth derivative DkF(z); the left-hand side always expands into a sum of terms of the form βkzk-1 F(k)(z) with k > 0. So the differential equation (5.107) always takes the special form
zn-1(βn - zαn)F(n)(z) + · · · + (β1 - zα1)F′(z) - α0F(z) = 0.
Equation (5.108) illustrates this in the case n = 2. Conversely, we will prove in exercise 6.13 that any differential equation of this form can be factored in terms of the ϑ operator, to give an equation like (5.107). So these are the differential equations whose solutions are power series with rational term ratios.
Multiplying both sides of (5.107) by z dispenses with the D operator and gives us an instructive all-ϑ form,



The first factor ϑ = (ϑ + 1 - 1) on the left corresponds to the (k + 1) in the term ratio (5.81), which corresponds to the k! in the denominator of the kth term in a general hypergeometric series. The other factors (ϑ + bj - 1) correspond to the denominator factor (k + bj), which corresponds to  in (5.76). On the right, the z corresponds to zk, and (ϑ + aj) corresponds to .
The function F(z) = (1 - z)r satisfies ϑF = z(ϑ - r)F. This gives another proof of the binomial theorem.
One use of this differential theory is to find and prove new transformations. For example, we can readily verify that both of the hypergeometrics



satisfy the differential equation



hence Gauss's identity [143, equation 102]



must be true. In particular,



(Caution: We can't use (5.110) safely when |z| > 1/2, unless both sides are polynomials; see exercise 53.)
whenever both infinite sums converge. And in fact both sums always do converge, except in the degenerate case when  is a nonpositive integer.
Every new identity for hypergeometrics has consequences for binomial coefficients, and this one is no exception. Let's consider the sum



The terms are nonzero for 0 ≤ k ≤ m - n, and with a little delicate limit-taking as before we can express this sum as the hypergeometric



The value of α doesn't affect the limit, since the nonpositive upper parameter n - m cuts the sum off early. We can set α = 2, so that (5.111) applies. The limit can now be evaluated because the right-hand side is a special case of (5.92). The result can be expressed in simplified form,



as shown in exercise 54. For example, when m = 5 and n = 2 we get ; when m = 4 and n = 2, both side give .
We can also find cases where (5.110) gives binomial sums when z = -1, but these are really weird. If we set  and b = -n, we get the monstrous formula



These hypergeometrics are nondegenerate polynomials when n ≢ 2 (mod 3); and the parameters have been cleverly chosen so that the left-hand side can be evaluated by (5.94). We are therefore led to a truly mind-boggling result,



This is the most startling identity in binomial coefficients that we've seen. Small cases of the identity aren't even easy to check by hand. (It turns out that both sides do give  when n = 3.) But the identity is completely useless, of course; surely it will never arise in a practical problem.
The only use of (5.113) is to demonstrate the existence of incredibly useless identities.
So that's our hype for hypergeometrics. We've seen that hypergeometric series provide a high-level way to understand what's going on in binomial coefficient sums. A great deal of additional information can be found in the classic book by Bailey [18] and its sequel by Gasper and Rahman [141].


5.7 Partial Hypergeometric Sums
Most of the sums we've evaluated in this chapter range over all indices k ≥ 0, but sometimes we've been able to find a closed form that works over a general range a ≤ k < b. For example, we know from (5.16) that



The theory in Chapter 2 gives us a nice way to understand formulas like this: If f(k) = Δg(k) = g(k + 1) - g(k), then we've agreed to write ∑ f(k) δk = g(k) + C, and



Furthermore, when a and b are integers with a ≤ b, we have



Therefore identity (5.114) corresponds to the indefinite summation formula



and to the difference formula



It's easy to start with a function g(k) and to compute Δg(k) = f(k), a function whose sum will be g(k) + C. But it's much harder to start with f(k) and to figure out its indefinite sum ∑ f(k) δk = g(k) + C; this function g might not have a simple form. For example, there is apparently no simple form for ; otherwise we could evaluate sums like , about which we're clueless. Yet maybe there is a simple form for  and we just haven't thought of it; how can we be sure?
In 1977, R. W. Gosper [154] discovered a beautiful way to find indefinite sums ∑ f(k) δk = g(k) + C whenever f and g belong to a general class of functions called hypergeometric terms. Let us write



for the kth term of the hypergeometric series F(a1, . . . , am; b1, . . . , bn; z). We will regard F(a1, . . . , am; b1, . . . , bn; z)k as a function of k, not of z. In many cases it turns out that there are parameters c, A1, . . . , AM, B1, . . . , BN, and Z such that



given a1, . . . , am, b1, . . . , bn, and z. We will say that a given function F(a1, . . . , am; b1, . . . , bn; z)k is summable in hypergeometric terms if such constants c, A1, . . . , AM, B1, . . . , BN, Z exist. Gosper's algorithm either finds the unknown constants or proves that no such constants exist.
In general, we say that t(k) is a hypergeometric term if t(k+1)/t(k) is a rational function of k, not identically zero. This means, in essence, that t(k) is a constant multiple of a term like (5.115). (A technicality arises, however, with respect to zeros, because we want t(k) to be meaningful when k is negative and when one or more of the b's in (5.115) is zero or a negative integer. Strictly speaking, we get the most general hypergeometric term by multiplying (5.115) by a nonzero constant times a power of 0, then cancelling zeros of the numerator with zeros of the denominator. The examples in exercise 12 help clarify this general rule.)
Suppose we want to find ∑ t(k) δk, when t(k) is a hypergeometric term. Gosper's algorithm proceeds in two steps, each of which is fairly straightforward. Step 1 is to express the term ratio in the special form



where p, q, and r are polynomials subject to the following condition:



(Divisibility of polynomials is analogous to divisibility of integers. For example, (k + α)\q(k) means that the quotient q(k)/(k + α) is a polynomial. It's easy to see that (k + α)\q(k) if and only if q(-α) = 0.)
This condition is easy to achieve: We start by provisionally setting p(k) = 1, and we set q(k) and r(k + 1) to the numerator and denominator of the term ratio, factoring them into linear factors. For example, if t(k) has the form (5.115), we start with the factorizations q(k) = (k + a1) . . . (k + am)z and r(k) = (k + b1 - 1) . . . (k + bn - 1)k. Then we check if (5.118) is violated. If q and r have factors (k + α) and (k + β) where α - β = N > 0, we divide them out of q and r and replace p(k) by



The new p, q, and r still satisfy (5.117), and we can repeat this process until (5.118) holds. We'll see in a moment why (5.118) is important.
Step 2 of Gosper's algorithm is to finish the job—to find a hypergeometric term T(k) such that



whenever possible. But it's not obvious how to do this; we need to develop some theory before we know how to proceed. Gosper noticed, after studying a lot of special cases, that it is wise to write the unknown function T(k) in the form



where s(k) is a secret function that must be discovered somehow. Plugging (5.121) into (5.120) and applying (5.117) gives
(Exercise 55 gives a clue about why we might want to make this magic substitution.)



so we need to have



If we can find s(k) satisfying this fundamental recurrence relation, we've found ∑ t(k) δk. If we can't, there's no T.
We're assuming that T(k) is a hypergeometric term, which means that T(k + 1)/T(k) is a rational function of k. Therefore, by (5.121) and (5.120), r(k)s(k)/p(k) = T(k)/(T(k + 1) - T(k)) is a rational function of k, and s(k) itself must be a quotient of polynomials:
s(k) = f(k)/g(k).
But in fact we can prove that s(k) is itself a polynomial. For if g(k) is not constant, and if f(k) and g(k) have no common factors, let N be the largest integer such that (k + β) and (k + β + N - 1) both occur as factors of g(k) for some complex number β. The value of N is positive, since N = 1 always satisfies this condition. Equation (5.122) can be rewritten
p(k)g(k+1)g(k) = q(k)f(k+1)g(k) - r(k)g(k+1)f(k),
and if we set k = -β and k = -β - N we get
r(-β)g(1-β)f(-β) = 0 = q(-β-N)f(1-β-N)g(-β-N).
Now f(-β) ≠ 0 and f(1 - β - N) ≠ 0, because f and g have no common roots. Also g(1 - β) ≠ 0 and g(-β - N) ≠ 0, because g(k) would otherwise contain the factor (k + β - 1) or (k + β + N), contrary to the maximality of N. Therefore
r(-β) = q(-β - N) = 0.
But this contradicts condition (5.118). Hence s(k) must be a polynomial.
I see: Gosper came up with condition (5.118) in order to make this proof go through.
Our task now boils down to finding a polynomial s(k) that satisfies (5.122), when p(k), q(k), and r(k) are given polynomials, or proving that no such polynomial exists. It's easy to do this when s(k) has any particular degree d, since we can write



for unknown coefficients (αd, . . . , α0) and plug this expression into the fundamental recurrence (5.122). The polynomial s(k) will satisfy the recurrence if and only if the α's satisfy the linear equations that result when we equate coefficients of each power of k in (5.122).
But how can we determine the degree of s? It turns out that there actually are at most two possibilities. We can rewrite (5.122) in the form



If s(k) has degree d, then the sum s(k + 1) + s(k) = 2αdkd + · · · also has degree d, while the difference s(k + 1) - s(k) = Δs(k) = dαdkd-1 + · · · has degree d - 1. (The zero polynomial can be assumed to have degree -1.) Let's write deg(P) for the degree of a polynomial P. If deg(Q) ≥ deg(R), then the degree of the right-hand side of (5.124) is deg(Q) + d, so we must have d = deg(p) - deg(Q). On the other hand if deg(Q) < deg(R) = d′, we can write Q(k) = λ′kd′-1 +· · · and R(k) = λkd′ +· · · where λ ≠ 0; the right-hand side of (5.124) has the form
(2λ′αd + λd αd)kd+d′-1 + · · · .
Ergo, two possibilities: Either 2λ′ + λd ≠ 0, and d = deg(p) - deg(R) + 1; or 2λ′ + λd = 0, and d > deg(p) - deg(R) + 1. The second case needs to be examined only if -2λ′/λ is an integer d greater than deg(p) - deg(R) + 1.
OK, we now have enough facts to perform Step 2 of Gosper's two-step algorithm: By trying at most two values of d, we can discover s(k), whenever equation (5.122) has a polynomial solution. If s(k) exists, we can plug it into (5.121) and we have our T. If it doesn't, we've proved that t(k) is not summable in hypergeometric terms.
Time for an example: Let's try the partial sum (5.114). Gosper's method should be able to deduce the value of



for any fixed n, so we seek the sum of



Step 1 is to put the term ratio into the required form (5.117); we have



so we simply take p(k) = 1, q(k) = k - n, and r(k) = k. This choice of p, q, and r satisfies (5.118), unless n is a negative integer; let's suppose it isn't.
Why isn't it r(k) = k + 1?Oh, I see.
Now we do Step 2. According to (5.124), we should consider the polynomials Q(k) = -n and R(k) = 2k - n. Since R has larger degree than Q, we need to look at two cases. Either d = deg(p) - deg(R) + 1, which is 0; or d = -2λ′/λ where λ′ = -n and λ = 2, hence d = n. The first case is nicer, because it doesn't require n to be a positive integer, so let's try it first; we'll need to try the other possibility for d only if the first case fails. Assuming that d = 0, the value of s(k) is simply α0, and equation (5.122) reduces to
1 = (k - n)α0 - kα0 .
Hence we choose α0 = -1/n. This satisfies the equation and gives



precisely the answer we were hoping to confirm.
If we apply the same method to find the indefinite sum , without the (-1)k, everything will be almost the same except that q(k) will be n - k; hence Q(k) = n - 2k will have greater degree than R(k) = n, and we will conclude that d has the impossible value deg(p)-deg(Q) = -1. (The polynomial s(k) cannot have negative degree, because it cannot be zero.) Therefore the function  is not summable in hypergeometric terms.
However, once we have eliminated the impossible, whatever remains—however improbable—must be the truth (according to S. Holmes [83]). When we defined p, q, and r in Step 1, we decided to ignore the possibility that n might be a negative integer. What if it is? Let's set n = -N, where N is positive. Then the term ratio for  is



and it should be represented by , q(k) = -1, r(k) = 1, according to (5.119). Step 2 of Gosper's algorithm now tells us to look for a polynomial s(k) of degree d = N - 1; maybe there's hope after all. For example, when N = 2 recurrence (5.122) says that we should solve
k + 1 = -((k + 1)α1 + α0) - (kα1 + α0) .
Equating coefficients of k and 1 tells us that
1 = -α1 - α1;          1 = - α1 - α0 - α0;
hence  is a solution, and



Can this be the desired sum? Yes, it checks out:



"Excellent, Holmes!" "Elementary, my dear Watson."
Incidentally, we can write this summation formula in another form, by attaching an upper limit:



This representation conceals the fact that  is summable in hypergeometric terms, because m/2 is not a hypergeometric term. (See exercise 12.)
A problem might arise in the denominator of (5.121) if p(k) = 0 for some integer k. Exercise 97 gives some insight into what can be done in such situations.
Notice that we need not bother to compile a catalog of indefinitely summable hypergeometric terms, analogous to the database of definite hypergeometric sums mentioned earlier in this chapter, because Gosper's algorithm provides a quick, uniform method that works in all summable cases.
Marko Petkovšek [291] has found a nice way to generalize Gosper's algorithm to more complicated inversion problems, by showing how to determine all hypergeometric terms T(k) that satisfy the lth-order recurrence



given any hypergeometric term t(k) and polynomials pl(k), . . . , p1(k), p0(k).


5.8 Mechanical Summation
Gosper's algorithm, beautiful as it is, finds a closed form for only a few of the binomial sums we meet in practice. But we need not stop there. Doron Zeilberger [384] showed how to extend Gosper's algorithm so that it becomes even more beautiful, making it succeed in vastly more cases. With Zeilberger's extension we can handle summation over all k, not just partial sums, so we have an alternative to the hypergeometric methods of Sections 5.5 and 5.6. Moreover, as with Gosper's original method, the calculations can be done by computer, almost blindly; we need not rely on cleverness and luck.
The basic idea is to regard the term we want to sum as a function t(n, k) of two variables n and k. (In Gosper's algorithm we wrote just t(k).) When t(n, k) does not turn out to be indefinitely summable in hypergeometric terms, with respect to k—and let's face it, relatively few functions are—Zeilberger noticed that we can often modify t(n, k) in order to obtain another term that is indefinitely summable, using ideas pioneered by Sister Celine Fasenmyer in the 1940s [382]. For example, it often turns out in practice that β0(n)t(n, k) + β1(n)t(n + 1, k) is indefinitely summable with respect to k, for appropriate polynomials β0(n) and β1(n). And when we carry out the sum with respect to k, we obtain a recurrence in n that solves our problem.
Let's start with a simple case in order to get familiar with this general approach. Suppose we have forgotten the binomial theorem, and we want to evaluate . How could we discover the answer, without clairvoyance or inspired guesswork? Earlier in this chapter, for example in Problem 3 of Section 5.2, we learned how to replace  by  and to fiddle around with the result. But there's a more systematic way to proceed.
Or without looking on page 174.
Let  be the quantity we want to sum. Gosper's algorithm tells us that we can't evaluate the partial sums ∑ k≤m t(n, k) for arbitrary n in hypergeometric terms, except in the case z = -1. So let's consider a more general term



instead. We'll look for values of β0(n) and β1(n) that make Gosper's algorithm succeed. First we want to simplify (5.126) by using the relation between t(n + 1, k) and t(n, k) to eliminate t(n + 1, k) from the expression. Since



we have



where
p(n, k) = (n + 1 - k)β0(n) + (n + 1)β1(n) .
We now apply Gosper's algorithm to , with n held fixed, first writing



as in (5.117). Gosper's method would find such a representation by starting with , but with Zeilberger's extension we are better off starting with . Notice that if we set  and , equation (5.127) is equivalent to



So we can find , q, and r satisfying (5.127) by finding , q, and r satisfying (5.128), starting with . This makes life easy, because  does not involve the unknown quantities β0(n) and β1(n) that appear in . In our case , so we have



we may take q(n, k) = (n + 1 - k)z and r(n, k) = k. These polynomials in k are supposed to satisfy condition (5.118). If they don't, we're supposed to remove factors from q and r and include corresponding factors (5.119) in ; but we should do this only when the quantity α - β in (5.118) is a positive integer constant, independent of n, because we want our calculations to be valid for arbitrary n. (The formulas we derive will, in fact, be valid even when n and k are not integers, using the generalized factorials (5.83).)
This time I remembered why r(n, k) isn't k + 1.
Our first choices of q and r do satisfy (5.118), in this sense, so we can move right on to Step 2 of Gosper's algorithm: We want to solve the analog of (5.122), using (5.127) in place of (5.117). So we want to solve



for the secret polynomial



(The coefficients of s are considered to be functions of n, not just constants.) In our case equation (5.129) is
(n + 1 - k)β0(n) + (n + 1)β1(n)
= (n + 1 - k)zs(n, k + 1) - ks(n, k) ,
and we regard this as a polynomial equation in k with coefficients that are functions of n. As before, we determine the degree d of s by considering Q(n, k) = q(n, k) - r(n, k) and R(n, k) = q(n, k) + r(n, k). Since deg(Q) = deg(R) = 1 (assuming that z ≠ ±1), we have  and s(n, k) = α0(n) is independent of k. Our equation becomes
The degree function deg(Q) refers here to the degree in k, treating n as constant.
(n + 1 - k)β0(n) + (n + 1)β1(n) = (n + 1 - k)zα0(n) - kα0(n) ;
and by equating powers of k we get the equivalent k-free equations
(n + 1)β0(n) + (n + 1)β1(n) - (n + 1)zα0(n)= 0,
-β0(n)                              + (z + 1)α0(n)= 0.
Hence we have a solution to (5.129) with
β0(n) = z + 1,          β1(n) = -1,          α0(n) = s(n, k) = 1.
(By chance, n has dropped out.)
We have discovered, by a purely mechanical method, that the term  is summable in hypergeometric terms. In other words,



where T(n, k) is a hypergeometric term in k. What is this T(n, k)? According to (5.121) and (5.128), we have



because . (Indeed,  almost always turns out to be 1 in practice.) Hence



And sure enough, everything checks out—equation (5.131) is true:



But we don't actually need to know T(n, k) precisely, because we are going to sum t(n, k) over all integers k. All we need to know is that T(n, k) is nonzero for only finitely many values of k, when n is any given nonnegative integer. Then the sum of T(n, k + 1) - T(n, k) over all k must telescope to 0.
Let ; this is the sum we started with, and we're now ready to compute it, because we now know a lot about t(n, k). The Gosper-Zeilberger procedure has deduced that



But this sum is (z + 1) ∑k t(n, k) - ∑k t(n + 1, k) = (z + 1)Sn - Sn+1. Therefore we have



In fact,  when |z| < 1 and n is any complex number. So (5.133) is true for all n, and in particular Sn = (z + 1)n when n is a negative integer.
Aha! This is a recurrence we know how to solve, provided that we know S0. And obviously S0 = 1. Hence we deduce that Sn = (z + 1)n, for all integers n ≥ 0. QED.
Let's look back at this computation and summarize what we did, in a form that will apply also to other summands t(n, k). The Gosper-Zeilberger algorithm can be formulated as follows, when t(n, k) is given:
0 Set l := 0. (We'll seek recurrences in n of order l.)
1 Let  = β0(n)t(n, k) +· · ·+ β1(n)t(n+l, k), where β0(n), . . . , βl(n) are unknown functions. Use properties of t(n, k) to find a linear combination p(n, k) of β0(n), . . . , βl(n) with coefficients that are polynomials in n and k, so that  can be written in the form , where  is a hypergeometric term in k. Find polynomials , q(n, k), r(n, k) so that the term ratio of  is expressed in the form (5.128), where q(n, k) and r(n, k) satisfy Gosper's condition (5.118). Set .
2a Set dQ := deg(q - r), dR := deg(q + r), and

2b If d ≥ 0, define s(n, k) by (5.130), and consider the linear equations in α0, . . . , αd, β0, . . . , βl obtained by equating coefficients of powers of k in the fundamental equation (5.129). If these equations have a solution with β0, . . . , βl not all zero, go to Step 4. Otherwise, if dQ < dR and if -2λ′/λ is an integer greater than d, where λ is the coefficient of kdR in q + r and λ′ is the coefficient of kdR-1 in q - r, set d := -2λ′/λ and repeat Step 2b.
3 (The term  isn't hypergeometrically summable.) Increase l by 1 and go back to Step 1.
4 (Success.) Set  . The algorithm has discovered that .
We'll prove later that this algorithm terminates successfully whenever t(n, k) belongs to a large class of terms called proper terms.
The binomial theorem can be derived in many ways, so our first example of the Gosper-Zeilberger approach was more instructive than impressive. Let's tackle Vandermonde's convolution next. Can Gosper and Zeilberger deduce algorithmically that  has a simple form? The algorithm starts with l = 0, which essentially reproduces Gosper's original algorithm, trying to see if  is summable in hypergeometric terms. Surprise: That term actually does turn out to be summable, if a + b is a specific nonnegative integer (see exercise 94). But we are interested in general values of a and b, and the algorithm quickly discovers that the indefinite sum is not a hypergeometric term in general. So l is increased from 0 to 1, and the algorithm proceeds to try  instead. The next step, as in our derivation of the binomial theorem, is to write , where p(n, k) is obtained by clearing fractions in t(n + 1, k)/t(n, k). In this case—the reader should please work along on a piece of scratch paper to check all these calculations—they aren't as hard as they look—everything goes through in an analogous fashion, but now with
p(n, k) = (n + 1 - k)β0(n) + (b - n + k)β1(n) = (n, k) ,(n, k) = t(n, k)/(n+1-k) = a! b!/(a-k)! k! (b-n+k)! (n+1-k)! ,q(n, k) = (n + 1 - k)(a - k) ,r(n, k) = (b - n + k)k .
Step 2a finds deg(q - r) < deg(q + r), and , so s(n, k) is again independent of k. Gosper's fundamental equation (5.129) is equivalent to two equations in three unknowns,
(n + 1)β0(n) + (b - n)β1(n)     - (n + 1)aα0(n)= 0,
-β0(n)              +β1(n) + (a + b + 1)α0(n)= 0,
which have the solution
β0(n) = a + b - n,          β1(n) = -n - 1,          α0(n) = 1.
The crucial point is that the Gosper-Zeilberger method always leads to equations that are linear in the unknown α's and β's, because the left side of (5.129) is linear in the β's and the right side is linear in the α's.
We conclude that (a + b - n)t(n, k) - (n + 1)t(n + 1, k) is summable with respect to k; hence if  the recurrence



holds; thus  since S0 = 1. A piece of cake.
What about the Saalschützian triple-binomial identity in (5.28)? The proof of (5.28) in exercise 43 is interesting, but it requires inspiration. When we transform an art into a science, we aim to replace inspiration by perspiration; so let's see if the Gosper-Zeilberger approach to summation is able to discover and prove (5.28) in a purely mechanical way. For convenience we make the substitutions m = b + d, n = a, r = a + b + c + d, s = a + b + c, so that (5.28) takes the more symmetrical form



To make the sum finite, we assume that either a or b is a nonnegative integer.
Let t(n, k) = (n + b + c + d + k)!/(n - k)! (b - k)! (c + k)! (d + k)! k! and . Proceeding along a path that is beginning to become well worn, we set
Deciding what parameter to call n is the only non-mechanical part.



and we try to solve (5.129) for s(n, k). Again deg(q-r) < deg(q+r), but this time  so it looks like we're stuck. However, Step 2b has an important second choice, d = -2λ′/λ, for the degree of s; we had better try it now before we give up. Here R(n, k) = q(n, k) + r(n, k) = 2k3 + · · · , so λ = 2, while the polynomial Q(n, k) = q(n, k) - r(n, k) almost miraculously turns out to have degree 1 in k—the coefficient of k2 vanishes! Therefore λ′ = 0; Gosper allows us to take d = 0 and s(n, k) = α0(n).
The equations to be solved are now
(n + 1)β0(n) + (n + 1 + b + c + d)β1(n)     - (n + 1)(n + 1 + b + c + d)bα0(n) = 0,- β0(n) + β1(n)     - ((n + 1)b - (n + 1 + b)(n + 1 + b + c + d) - cd)α0(n) = 0;
Notice that λ′ is not the leading coefficient of Q, although λ is the leading coefficient of R. The number λ′ is the coefficient of kdeg(R)-1 in Q.
and we find
β0(n) = (n + 1 + b + c)(n + 1 + b + d)(n + 1 + b + c + d),β1(n) = -(n + 1)(n + 1 + c)(n + 1 + d),α0(n) = 2n + 2 + b + c + d,
after only a modest amount of perspiration. The identity (5.134) follows immediately.
Perspiration flows, identity follows.
A similar proof of (5.134) can be obtained if we work with n = d instead of n = a. (See exercise 99.)
The Gosper-Zeilberger approach helps us evaluate definite sums over a restricted range as well as sums over all k. For example, let's consider



When  we obtained an "unexpected" result in (5.20); would Gosper and Zeilberger have expected it? Putting  leads us to
p(n, k) = (n + 1)β0(n) + (n + 1 + k)β1(n) = (n, k) ,(n, k) = t(n, k)/(n + 1) = (n + k)! zk/k! (n + 1)! ,q(n, k) = (n + 1 + k)z ,r(n, k) = k ,
and deg(s) = deg()-deg(q-r) = 0. Equation (5.129) is solved by β0(n) = 1, β1(n) = z - 1, s(n, k) = 1. Therefore we find



where . We can now sum (5.136) for 0 ≤ k ≤ n + 1, getting



But , so



We see immediately that the case  is special, and that . Moreover, the recurrence (5.137) can be simplified by applying the summation factor (1 - z)n+1 to both sides; this yields the general identity



which comparatively few people would have expected before Gosper and Zeilberger came along. Now the production of such identities is routine.
How about the similar sum



which we encountered in (5.74)? Flushed with confidence, we set  and proceed to calculate
p(n, k) = (n + 1 - 2k)β0(n) + (n + 1 - k)β1(n) = (n, k) ,(n, k) = t(n, k)/(n + 1 - 2k) = (n - k)! zk/k! (n + 1 - 2k)! ,q(n, k) = (n + 1 - 2k)(n - 2k)z ,r(n, k) = (n + 1 - k)k .
But whoa—there's no way to solve (5.129), if we assume that , because the degree of s would have to be deg() - deg(q - r) = -1.
 equals (n + 1)/2n.
No problem. We simply add another parameter β2(n) and try (n, k) = β0(n)t(n, k) + β1(n)t(n + 1, k) + β2(n)t(n + 2, k) instead:
p(n, k) = (n + 1 - 2k)(n + 2 - 2k)β0(n)                  + (n + 1 - k)(n + 2 - 2k)β1(n)                  + (n + 1 - k)(n + 2 - k)β2(n) = (n, k) ,(n, k) = t(n, k)/(n+1-2k)(n+2-2k) = (n-k)! zk/k! (n+2-2k)! ,q(n, k) = (n + 2 - 2k)(n + 1 - 2k)z ,r(n, k) = (n + 1 - k)k .
Now we can try s(n, k) = α0(n) and (5.129) does have a solution:
β0(n) = z,          β1(n) = 1,          β2(n) = -1,          α0(n) = 1.
We have discovered that
zt(n, k) + t(n + 1, k) - t(n + 2, k) = T(n, k + 1) - T(n, k) ,
where T(n, k) equals . Summing from k = 0 to k = n gives



And  for all n ≥ 0, so we obtain



We will study the solution of such recurrences in Chapters 6 and 7; the methods of those chapters lead directly from (5.140) to the closed form (5.74), when S0(z) = S1(z) = 1.
One more example—a famous one—will complete the picture. The French mathematician Roger Apéry solved a long-standing problem in 1978 when he proved that the number ζ(3) = 1 + 2-3 + 3-3 + 4-3 + · · · is irrational [14]. One of the main components of his proof involved the binomial sums



for which he announced a recurrence that other mathematicians were unable to verify at the time. (The numbers An have since become known as Apéry numbers; we have A0 = 1, A1 = 5, A2 = 73, A3 = 1445, A4 = 33001.) Finally [356] Don Zagier and Henri Cohen found a proof of Apéry's claim, and their proof for this special (but difficult) sum was one of the key clues that ultimately led Zeilberger to discover the general approach we are discussing.
By now, in fact, we have seen enough examples to make the sum in (5.141) almost trivial. Putting  and (n, k) = β0(n)t(n, k) + β1(n)t(n + 1, k) + β2(n)t(n + 2, k), we try to solve (5.129) with
p(n, k) = (n + 1 - k)2(n + 2 - k)2β0(n)
         + (n + 1 + k)2(n + 2 - k)2β1(n)
         + (n + 1 + k)2(n + 2 + k)2β2(n) = (n, k) ,
(n, k) = t(n, k)/(n+1-k)2(n+2-k)2 = (n+k)!2/k!4(n+2-k)!2 ,
q(n, k) = (n + 1 + k)2(n + 2 - k)2 ,
r(n, k) = k4 .
(First we try doing without β2, but that attempt quickly peters out.)
(We don't worry about the fact that q has the factor (k + n + 1) while r has the factor k; this does not violate (5.118), because we are regarding n as a variable parameter, not a fixed integer.) Since q(n, k) - r(n, k) = -2k3 + · · · , we are allowed to set deg(s) = -2λ′/λ = 2, so we take
s(n, k) = α2(n)k2 + α1(n)k + α0(n) .
With this choice of s, the recurrence (5.129) boils down to five equations in the six unknown quantities β0(n), β1(n), β2(n), α0(n), α1(n), α2(n). For example, the equation arising from the coefficients of k0 simplifies to
β0 + β1 + β2 - α0 - α1 - α2 = 0;
the equation arising from the coefficients of k4 is
β0 + β1 + β2 + α1 + (6 + 6n + 2n2)α2 = 0 .
The other three equations are more complicated. But the main point is that these linear equations—like all the equations that arise when we come to this stage of the Gosper-Zeilberger algorithm—are homogeneous (their right-hand sides are 0). So they always have a nonzero solution when the number of unknowns exceeds the number of equations. A solution, in our case, turns out to be
β0(n)   =   (n + 1)3 ,β1(n)   =   - (2n + 3)(17n2 + 51n + 39) ,β2(n)   =   (n + 2)3 ,α0(n)   =   -16(n + 1)(n + 2)(2n + 3) ,α1(n)   =   -12(2n + 3) ,α2(n)   =   8(2n + 3) .
Consequently
(n + 1)3t(n, k) - (2n + 3)(17n2 + 51n + 39)t(n + 1, k) + (n + 2)3t(n + 2, k) = T(n, k + 1) - T(n, k) ,
where T(n, k) = k4s(n, k)(n, k) = (2n + 3)(8k2 - 12k - 16(n + 1)(n + 2)) × (n + k)!2/(k - 1)!4(n + 2 - k)!2. Summing on k gives Apéry's once-incredible recurrence,



"Professor Littlewood, when he makes use of an algebraic identity, always saves himself the trouble of proving it; he maintains that an identity, if true, can be verified in a few lines by anybody obtuse enough to feel the need of verification. My object in the following pages is to confute this assertion."
—F. J. Dyson [89]
Does the Gosper-Zeilberger method work with all the sums we've encountered in this chapter? No. It doesn't apply when t(n, k) is the summand  in (5.65), because the term ratio t(n, k + 1)/t(n, k) is not a rational function of k. It also fails to handle cases like , because the other term ratio t(n + 1, k)/t(n, k) is not a rational function of k. (We can do that one, however, by summing  and then setting z = n.) And it fails on a comparatively simple summand like t(n, k) = 1/(nk + 1), even though both t(n, k + 1)/t(n, k) and t(n + 1, k)/t(n, k) are rational functions of n and k.
But the Gosper-Zeilberger algorithm is guaranteed to succeed in an enormous number of cases, namely whenever the summand t(n, k) is a so-called proper term—a term that can be written in the form



Here f(n, k) is a polynomial in n and k; the coefficients ,  are specific integer constants; the parameters w and z are nonzero; and the other quantities  are arbitrary complex numbers. We will prove that whenever t(n, k) is a proper term, there exist polynomials β0(n), . . . , βl(n), not all zero, and a proper term T(n, k), such that



What happens if t(n, k) is independent of n?
The following proof is due to Wilf and Zeilberger [374].
Let N be the operator that increases n by 1, and let K be the operator that increases k by 1, so that, for example, N2K3t(n, k) = t(n + 2, k + 3). We will study linear difference operators in N, K, and n, namely operator polynomials of the form



where each αi,j(n) is a polynomial in n. Our first observation is that, if t(n, k) is any proper term and H(N, K, n) is any linear difference operator, then H(N, K, n)t(n, k) is a proper term. Suppose t and H are given respectively by (5.143) and (5.145); then we define a "base term"



For example, if t(n, k) is , the base term corresponding to a linear difference operator of degrees I and J is (n, k)I,J = (n - 2k - 2J)!/(k + J)! (n - 3k + I)!. The point is that αi,j(n)NiKjt(n, k) is equal to (n, k)I,J times a polynomial in n and k, whenever 0 ≤ i ≤ I and 0 ≤ j ≤ J. A finite sum of polynomials is a polynomial, so H(N, K, n)t(n, k) has the required form (5.143).
The next step is to show that whenever t(n, k) is a proper term, there is always a nonzero linear difference operator H(N, K, n) such that
H(N, K, n)t(n, k) = 0 .
If 0 ≤ i ≤ I and 0 ≤ j ≤ J, the shifted term NiKjt(n, k) is  times a polynomial in n and k that has degree at most



in the variable k. Hence the desired H exists if we can solve DI,J + 1 homogeneous linear equations in the (I+1)(J+1) variables αi,j(n), with coefficients that are polynomials in n. All we need to do is choose I and J large enough that (I + 1)(J + 1) > DI,J + 1. For example, we can take I = 2A′ + 1 and J = 2A + deg(f), where



The last step in the proof is to go from the equation H(N, K, n)t(n, k) = 0 to a solution of (5.144). Let H be chosen so that J is minimized, i.e., so that H has the smallest possible degree in K. We can write
H(N, K, n) = H(N, 1, n) - (K - 1)G(N, K, n)
The trick here is based on regarding H as a polynomial in K and then replacing K by Δ + 1.
for some linear difference operator G(N, K, n). Let H(N, 1, n) = β0(n) + β1(n)N + · · · + βl(n)Nl and T(n, k) = G(N, K, n)t(n, k). Then T(n, k) is a proper term, and (5.144) holds.
The proof is almost complete; we still have to verify that H(N, 1, n) is not simply the zero operator. If it is, then T(n, k) is independent of k. So there are polynomials β0(n) and β1(n) such that (β0(n) + β1(n)N)T(n, k) = 0. But then (β0(n) + β1(n)N)G(N, K, n) is a nonzero linear difference operator of degree J - 1 that annihilates t(n, k); this contradicts the minimality of J, and our proof of (5.144) is complete.
Once we know that (5.144) holds, for some proper term T, we can be sure that Gosper's algorithm will succeed in finding T (or T plus a constant). Although we proved Gosper's algorithm only for the case of hypergeometric terms t(k) in a single variable k, our proof can be extended to the two-variable case, as follows: There are infinitely many complex numbers n for which condition (5.118) holds when q(n, k) and r(n, k) are completely factored as polynomials in k, and for which the calculations of d in Step 2 agree with the calculations of Gosper's one-variable algorithm. For all such n, our previous proof shows that a suitable polynomial s(n, k) in k exists; therefore a suitable polynomial s(n, k) in n and k exists; QED.
We have proved that the Gosper-Zeilberger algorithm will discover a solution to (5.144), for some l, where l is as small as possible. That solution gives us a recurrence in n for evaluating the sum over k of any proper term t(n, k), provided that t(n, k) is nonzero for only finitely many k. And the roles of n and k can, of course, be reversed, because the definition of proper term in (5.143) is symmetrical in n and k.
Exercises 98-108 provide additional examples of the Gosper-Zeilberger algorithm, illustrating some of its versatility. Wilf and Zeilberger [374] have significantly extended these results to methods that handle generalized binomial coefficients and multiple indices of summation.



Exercises

Warmups
1. What is 114 ? Why is this number easy to compute, for a person who knows binomial coefficients?
2. For which value(s) of k is  a maximum, when n is a given positive integer? Prove your answer.
3. Prove the hexagon property,



4. Evaluate  by negating (actually un-negating) its upper index.
5. Let p be prime. Show that  mod p = 0 for 0 < k < p. What does this imply about the binomial coefficients ?
6. Fix up the text's derivation in Problem 6, Section 5.2, by correctly applying symmetry.
A case of mistaken identity.
7. Is (5.34) true also when k < 0?
8. Evaluate



What is the approximate value of this sum, when n is very large? Hint: The sum is Δn f(0) for some function f.
9. Show that the generalized exponentials of (5.58) obey the law
ℇt(z) = ℇ(tz)1/t,          if t ≠ 0,
where ℇ(z) is an abbreviation for ℇ1(z).
10. Show that -2(ln(1 - z) + z)/z2 is a hypergeometric function.
11. Express the two functions



in terms of hypergeometric series.
12. Which of the following functions of k is a hypergeometric term, as defined in Section 5.7? Explain why or why not.
a nk.
b kn.
c (k! + (k + 1)!)/2.
d Hk, that is, .
e .
f t(k)T(k), when t and T are hypergeometric terms.
g t(k) + T(k), when t and T are hypergeometric terms.
h t(n - k), when t is a hypergeometric term.
i a t(k) + b t(k+1) + c t(k+2), when t is a hypergeometric term.
j k/2.
k k [k > 0].
(Here t and T aren't necessarily related as in (5.120).)


Basics
13. Find relations between the superfactorial function  of exercise 4.55, the hyperfactorial function , and the product .
14. Prove identity (5.25) by negating the upper index in Vandermonde's convolution (5.22). Then show that another negation yields (5.26).
15. What is  ? Hint: See (5.29).
16. Evaluate the sum



when a, b, c are nonnegative integers.
17. Find a simple relation between  and .
18. Find an alternative form analogous to (5.35) for the product



19. Show that the generalized binomials of (5.58) obey the law
t(z) = 1-t(-z)-1 .
20. Define a "generalized bloopergeometric series" by the formula



using falling powers instead of the rising ones in (5.76). Explain how G is related to F.
21. Show that Euler's definition of factorials is consistent with the ordinary definition, by showing that the limit in (5.83) is 1/m! when z = m is a positive integer.
22. Use (5.83) to prove the factorial duplication formula:
By the way, .



23. What is the value of F(-n, 1; ; 1)?
24. Find  by using hypergeometric series.
25. Show that



Find a similar relation between the hypergeometrics



26. Express the function G(z) in the formula



as a multiple of a hypergeometric series.
27. Prove that



28. Prove Euler's identity



by applying Pfaff's reflection law (5.101) twice.
29. Show that confluent hypergeometrics satisfy



30. What hypergeometric series F satisfies zF′(z) + F(z) = 1/(1 - z)?
31. Show that if f(k) is any function summable in hypergeometric terms, then f itself is a hypergeometric term. For example, if Σ f(k) δk = cF(A1, . . . , AM; B1, . . . , BN; Z)k+C, then there are constants a1, . . . , am, b1, . . . , bn, and z such that f(k) is a multiple of (5.115).
32. Find Σ k2 δk by Gosper's method.
33. Use Gosper's method to find Σ δk/(k2 - 1).
34. Show that a partial hypergeometric sum can always be represented as a limit of ordinary hypergeometrics:



when c is a nonnegative integer. (See (5.115).) Use this idea to evaluate .


Homework exercises
35. The notation  is ambiguous without context. Evaluate it
a as a sum on k;
b as a sum on n.
36. Let pk be the largest power of the prime p that divides , when m and n are nonnegative integers. Prove that k is the number of carries that occur when m is added to n in the radix p number system. Hint: Exercise 4.24 helps here.
37. Show that an analog of the binomial theorem holds for factorial powers. That is, prove the identities



for all nonnegative integers n.
38. Show that all nonnegative integers n can be represented uniquely in the form  where a, b, and c are integers with 0 ≤ a < b < c. (This is called the combinatorial number system.)
39. Show that if xy = ax + by then



for all n > 0. Find a similar formula for the more general product xmyn. (These formulas give useful partial fraction expansions, for example when x = 1/(z - c) and y = 1/(z - d).)
40. Find a closed form for



41. Evaluate  when n is a nonnegative integer.
42. Find the indefinite sum , and use it to compute the sum  in closed form when 0 ≤ m ≤ n.
43. Prove the triple-binomial identity (5.28). Hint: First replace  by .
44. Use identity (5.32) to find closed forms for the double sums



given integers m ≥ a ≥ 0 and n ≥ b ≥ 0.
45. Find a closed form for .
46. Evaluate the following sum in closed form, when n is a positive integer:



Hint: Generating functions win again.
47. The sum



is a polynomial in r and s. Show that it doesn't depend on s.
48. The identity  can be combined with the formula  to yield



What is the hypergeometric form of the latter identity?
49. Use the hypergeometric method to evaluate



50. Prove Pfaff's reflection law (5.101) by comparing the coefficients of zn on both sides of the equation.
51. The derivation of (5.104) shows that
lim→0 F(-m, -2m - 1 + ; -2m + ; 2) = 1/.
In this exercise we will see that slightly different limiting processes lead to distinctly different answers for the degenerate hypergeometric series F(-m, -2m - 1; -2m; 2).
a Show that lim→0 F(-m + , -2m - 1; -2m + 2; 2) = 0, by using Pfaff's reflection law to prove the identity F(a, -2m - 1; 2a; 2) = 0 for all integers m ≥ 0.
b What is lim→0 F(-m + , -2m - 1; -2m + ; 2)?
52. Prove that if N is a nonnegative integer,



53. If we put  and z = 1 in Gauss's identity (5.110), the left side reduces to -1 while the right side is +1. Why doesn't this prove that -1 = +1?
54. Explain how the right-hand side of (5.112) was obtained.
55. If the hypergeometric terms t(k) = F(a1, . . . , am; b1, . . . , bn; z)k and T(k) = F(A1, . . . , AM; B1, . . . , BN; Z)k satisfy t(k) = c (T(k + 1) - T(k)) for all k ≥ 0, show that z = Z and m - n = M - N.
56. Find a general formula for  using Gosper's method. Show that  is also a solution.
57. Given n and z, use Gosper's method to find a constant θ such that



is summable in hypergeometric terms.
58. If m and n are integers with 0 ≤ m ≤ n, let



Find a relation between Tm,n and Tm-1,n-1, then solve your recurrence by applying a summation factor.


Exam problems
59. Find a closed form for



when m and n are positive integers.
60. Use Stirling's approximation (4.23) to estimate  when m and n are both large. What does your formula reduce to when m = n?
61. Prove that when p is prime, we have



for all nonnegative integers m and n.
62. Assuming that p is prime and that m and n are positive integers, determine the value of  mod p2. Hint: You may wish to use the following generalization of Vandermonde's convolution:



63. Find a closed form for



given an integer n ≥ 0.
64. Evaluate , given an integer n ≥ 0.
65. Prove that



66. Evaluate "Harry's double sum,"



as a function of m. (The sum is over both j and k.)
67. Find a closed form for



68. Find a closed form for



69. Find a closed form for



as a function of m and n.
70. Find a closed form for



71. Let



where m and n are nonnegative integers, and let A(z) = Σk≥0 akzk be the generating function for the sequence a0, a1, a2, . . . .
a Express the generating function S(z) = Σn≥0 Snzn in terms of A(z).
b Use this technique to solve Problem 7 in Section 5.2.
72. Prove that, if m, n, and k are integers and n > 0,



where ν(k) is the number of 1's in the binary representation of k.
73. Use the repertoire method to solve the recurrence
X0 = α;          X1 = β;
Xn = (n - 1)(Xn-1 + Xn-2),          for n > 1.
Hint: Both n! and n¡ satisfy this recurrence.
74. This problem concerns a deviant version of Pascal's triangle in which the sides consist of the numbers 1, 2, 3, 4, . . . instead of all 1's, although the interior numbers still satisfy the addition formula:
1
2      2
3      4      3
4      7      7      4
5      11      14      11      5
.      .        .        .        .      .
If  denotes the kth number in row n, for 1 ≤ k ≤ n, we have , and  for 1 < k < n. Express the quantity  in closed form.
75. Find a relation between the functions



and the quantities 2n/3 and 2n/3.
76. Solve the following recurrence for n, k ≥ 0:
Qn,0 = 1;          Q0,k = [k = 0];
Qn,k = Qn-1,k + Qn-1,k-1 +  ,         for n, k > 0.
77. What is the value of



78. Assuming that m is a positive integer, find a closed form for



79.
a What is the greatest common divisor of ? Hint: Consider the sum of these n numbers.
b Show that the least common multiple of  is equal to L(n + 1)/(n + 1), where L(n) = lcm(1, 2, . . . , n).
Handy to know.
80. Prove that  for all integers k, n ≥ 0.
81. If 0 < θ < 1 and 0 ≤ x ≤ 1, and if l, m, n are nonnegative integers with m < n, prove the inequality



Hint: Consider taking the derivative with respect to x.


Bonus problems
82. Prove that Pascal's triangle has an even more surprising hexagon property than the one cited in the text:



if 0 < k < n. For example, gcd(56, 36, 210) = gcd(28, 120, 126) = 2.
83. Prove the amazing five-parameter double-sum identity (5.32).
84. Show that the second pair of convolution formulas, (5.61), follows from the first pair, (5.60). Hint: Differentiate with respect to z.
85. Prove that



(The left side is a sum of 2n - 1 terms.) Hint: Much more is true.
86. Let a1, . . . , an be nonnegative integers, and let C(a1, . . . , an) be the coefficient of the constant term  when the n(n - 1) factors



are fully expanded into positive and negative powers of the complex variables z1, . . . , zn.
a Prove that C(a1, . . . , an) equals the left-hand side of (5.31).
b Prove that if z1, . . . , zn are distinct complex numbers, then the polynomial



is identically equal to 1.
c Multiply the original product of n(n - 1) factors by f(0) and deduce that C(a1, a2, . . . , an) is equal to
C(a1 - 1, a2, . . . , an) + C(a1, a2 - 1, . . . , an)+ · · · + C(a1, a2, . . . , an - 1) .
(This recurrence defines multinomial coefficients, so C(a1, . . . , an) must equal the right-hand side of (5.31).)
87. Let m be a positive integer and let ζ = eπi/m. Show that



(This reduces to (5.74) in the special case m = 1.)
88. Prove that the coefficients sk in (5.47) are equal to



for all k > 1; hence |sk| < 1/(k - 1).
89. Prove that (5.19) has an infinite counterpart,



if |x| < |y| and |x| < |x + y|. Differentiate this identity n times with respect to y and express it in terms of hypergeometrics; what relation do you get?
90. Problem 1 in Section 5.2 considers  when r and s are integers with s ≥ r ≥ 0. What is the value of this sum if r and s aren't integers?
91. Prove Whipple's identity,



by showing that both sides satisfy the same differential equation.
92. Prove Clausen's product identities



What identities result when the coefficients of zn on both sides of these formulas are equated?
93. Show that the indefinite sum



has a (fairly) simple form, given any function f and any constant α ≠ 0.
94. Find  when n is a positive integer.
95. What conditions in addition to (5.118) will make the polynomials p, q, r of (5.117) uniquely determined?
96. Prove that if Gosper's algorithm finds no solution to (5.120), given a hypergeometric term t(k), then there is no solution to the more general equation
t(k) = T1(k + 1) + · · · + Tm(k + 1)) - (T1(k) + · · · + Tm(k)),
where T1(k), . . . , Tm(k) are hypergeometric terms.
97. Find all complex numbers z such that  is summable in hypergeometric terms.
98. What recurrence does the Gosper-Zeilberger method give for the sum ?
99. Use the Gosper-Zeilberger method to find a closed form for Σk t(n, k) when t(n, k) = (n + a + b + c + k)!/(n + k)! (c + k)! (b - k)! (a - k)! k!, assuming that a is a nonnegative integer.
100. Find a recurrence relation for the sum



and use the recurrence to find another formula for Sn.
Better use computer algebra for this one (and the next few).
101. Find recurrence relations satisfied by the sums
a 
b 
102. Use the Gosper-Zeilberger procedure to generalize the "useless" identity (5.113): Find additional values of a, b, and z such that



has a simple closed form.
103. Let t(n, k) be the proper term (5.143). What are the degrees of , q(n, k), and r(n, k) in terms of the variable k, when the procedure of Gosper and Zeilberger is applied to  = β0(n)t(n, k) + · · · + βl(n)t(n + l, k)? (Ignore the rare, exceptional cases.)
104. Use the Gosper-Zeilberger procedure to verify the remarkable identity



Explain why the simplest recurrence for this sum is not found.
105. Show that if ω = e2πi/3 we have



106. Prove the amazing identity (5.32) by letting t(r, j, k) be the summand divided by the right-hand side, then showing that there are functions T(r, j, k) and U(r, j, k) for which
t(r + 1, j, k) - t(r, j, k) = T(r, j + 1, k) - T(r, j, k)
   + U(r, j, k + 1) - U(r, j, k) .
107. Prove that 1/(nk + 1) is not a proper term.
108. Show that the Apéry numbers An of (5.141) are the diagonal elements An,n of a matrix of numbers defined by



Prove, in fact, that this matrix is symmetric, and that



109. Prove that the Apéry numbers (5.141) satisfy
An ≡ An/pAn mod p          (mod p)
for all primes p and all integers n ≥ 0.


Research problems
110. For what values of n is  (mod (2n + 1))?
111. Let q(n) be the smallest odd prime factor of the middle binomial coefficient . According to exercise 36, the odd primes p that do not divide  are those for which all digits in n's radix p representation are (p - 1)/2 or less. Computer experiments have shown that q(n) ≤ 11 for 1 < n < 1010000, except that q(3160) = 13.
a Is q(n) ≤ 11 for all n > 3160?
b Is q(n) = 11 for infinitely many n?
A reward of $7 · 11 · 13 is offered for a solution to either (a) or (b).
112. Is  divisible either by 4 or by 9, for all n > 4 except n = 64 and n = 256?
113. If t(n + 1, k)/t(n, k) and t(n, k + 1)/t(n, k) are rational functions of n and k, and if there is a nonzero linear difference operator H(N, K, n) such that H(N, K, n)t(n, k) = 0, does it follow that t(n, k) is a proper term?
114. Let m be a positive integer, and define the sequence  by the recurrence



Are these numbers  integers?











6. Special Numbers
Some sequences of numbers arise so often in mathematics that we recognize them instantly and give them special names. For example, everybody who learns arithmetic knows the sequence of square numbers 1, 4, 9, 16, . . .. In Chapter 1 we encountered the triangular numbers 1, 3, 6, 10, . . .; in Chapter 4 we studied the prime numbers 2, 3, 5, 7, . . .; in Chapter 5 we looked briefly at the Catalan numbers 1, 2, 5, 14, . . ..
In the present chapter we'll get to know a few other important sequences. First on our agenda will be the Stirling numbers  and , and the Eulerian numbers ; these form triangular patterns of coefficients analogous to the binomial coefficients  in Pascal's triangle. Then we'll take a good look at the harmonic numbers Hn, and the Bernoulli numbers Bn; these differ from the other sequences we've been studying because they're fractions, not integers. Finally, we'll examine the fascinating Fibonacci numbers Fn and some of their important generalizations.

6.1 Stirling Numbers
We begin with some close relatives of the binomial coefficients, the Stirling numbers, named after James Stirling (1692-1770). These numbers come in two flavors, traditionally called by the no-frills names "Stirling numbers of the first and second kind." Although they have a venerable history and numerous applications, they still lack a standard notation. Following Jovan Karamata, we will write  for Stirling numbers of the second kind and  for Stirling numbers of the first kind; these symbols turn out to be more user-friendly than the many other notations that people have tried.
Tables 258 and 259 show what  and  look like when n and k are small. A problem that involves the numbers "1, 7, 6, 1" is likely to be related to , and a problem that involves "6, 11, 6, 1" is likely to be related to , just as we assume that a problem involving "1, 4, 6, 4, 1" is likely to be related to ; these are the trademark sequences that appear when n = 4.
". . . par cette notation, les formules deviennent plus symétriques."
—J. Karamata [199]


Table 258 Stirling's triangle for subsets.



Stirling numbers of the second kind show up more often than those of the other variety, so let's consider last things first. The symbol  stands for the number of ways to partition a set of n things into k nonempty subsets. For example, there are seven ways to split a four-element set into two parts:
(Stirling himself considered this kind first in his book [343].)



thus . Notice that curly braces are used to denote sets as well as the numbers . This notational kinship helps us remember the meaning of , which can be read "n subset k."
Let's look at small k. There's just one way to put n elements into a single nonempty set; hence , for all n > 0. On the other hand , because a 0-element set is empty.
The case k = 0 is a bit tricky. Things work out best if we agree that there's just one way to partition an empty set into zero nonempty parts; hence . But a nonempty set needs at least one part, so  for n > 0.
What happens when k = 2? Certainly . If a set of n > 0 objects is divided into two nonempty parts, one of those parts contains the last object and some subset of the first n - 1 objects. There are 2n-1 ways to choose the latter subset, since each of the first n - 1 objects is either in it or out of it; but we mustn't put all of those objects in it, because we want to end up with two nonempty parts. Therefore we subtract 1:



(This tallies with our enumeration of  ways above.)


Table 259 Stirling's triangle for cycles.



A modification of this argument leads to a recurrence by which we can compute  for all k: Given a set of n > 0 objects to be partitioned into k nonempty parts, we either put the last object into a class by itself (in  ways), or we put it together with some nonempty subset of the first n - 1 objects. There are  possibilities in the latter case, because each of the  ways to distribute the first n - 1 objects into k nonempty parts gives subsets that the nth object can join. Hence



This is the law that generates Table 258; without the factor of k it would reduce to the addition formula (5.8) that generates Pascal's triangle.
And now, Stirling numbers of the first kind. These are somewhat like the others, but  counts the number of ways to arrange n objects into k cycles instead of subsets. We verbalize '' by saying "n cycle k."
Cycles are cyclic arrangements, like the necklaces we considered in Chapter 4. The cycle



can be written more compactly as '[A, B, C, D]', with the understanding that
[A, B, C, D] = [B, C, D, A] = [C, D, A, B] = [D, A, B, C];
a cycle "wraps around" because its end is joined to its beginning. On the other hand, the cycle [A, B, C, D] is not the same as [A, B, D, C] or [D, C, B, A].
There are eleven different ways to make two cycles from four elements:



"There are nine and sixty ways of constructing tribal lays, And-every-single-one-of-them-is-right."
—Rudyard Kipling
hence .
A singleton cycle (that is, a cycle with only one element) is essentially the same as a singleton set (a set with only one element). Similarly, a 2-cycle is like a 2-set, because we have [A, B] = [B, A] just as {A, B} = {B, A}. But there are two different 3-cycles, [A, B, C] and [A, C, B]. Notice, for example, that the eleven cycle pairs in (6.4) can be obtained from the seven set pairs in (6.1) by making two cycles from each of the 3-element sets.
In general, n!/n = (n - 1)! different n-cycles can be made from any n-element set, whenever n > 0. (There are n! permutations, and each n-cycle corresponds to n of them because any one of its elements can be listed first.) Therefore we have



This is much larger than the value  we had for Stirling subset numbers. In fact, it is easy to see that the cycle numbers must be at least as large as the subset numbers,



because every partition into nonempty subsets leads to at least one arrangement of cycles.
Equality holds in (6.6) when all the cycles are necessarily singletons or doubletons, because cycles are equivalent to subsets in such cases. This happens when k = n and when k = n - 1; hence



In fact, it is easy to see that



(The number of ways to arrange n objects into n - 1 cycles or subsets is the number of ways to choose the two objects that will be in the same cycle or subset.) The triangular numbers  = 1, 3, 6, 10, . . . are conspicuously present in both Table 258 and Table 259.
We can derive a recurrence for  by modifying the argument we used for . Every arrangement of n objects in k cycles either puts the last object into a cycle by itself (in  ways) or inserts that object into one of the  cycle arrangements of the first n - 1 objects. In the latter case, there are n - 1 different ways to do the insertion. (This takes some thought, but it's not hard to verify that there are j ways to put a new element into a j-cycle in order to make a (j + 1)-cycle. When j = 3, for example, the cycle [A, B, C] leads to
[A, B, C, D],     [A, B, D, C],     or     [A, D, B, C]
when we insert a new element D, and there are no other possibilities. Summing over all j gives a total of n - 1 ways to insert an nth object into a cycle decomposition of n - 1 objects.) The desired recurrence is therefore



This is the addition-formula analog that generates Table 259.
Comparison of (6.8) and (6.3) shows that the first term on the right side is multiplied by its upper index (n-1) in the case of Stirling cycle numbers, but by its lower index k in the case of Stirling subset numbers. We can therefore perform "absorption" in terms like  and , when we do proofs by mathematical induction.
Every permutation is equivalent to a set of cycles. For example, consider the permutation that takes 123456789 into 384729156. We can conveniently represent it in two rows,
1 2 3 4 5 6 7 8 93 8 4 7 2 9 1 5 6,
showing that 1 becomes 3 and 2 becomes 8, etc. The cycle structure comes about because 1 becomes 3, which becomes 4, which becomes 7, which becomes the original element 1; that's the cycle [1, 3, 4, 7]. Another cycle in this permutation is [2, 8, 5]; still another is [6, 9]. Therefore the permutation 384729156 is equivalent to the cycle arrangement
[1, 3, 4, 7] [2, 8, 5] [6, 9].
If we have any permutation π1π2 . . . πn of {1, 2, . . . , n}, every element is in a unique cycle. For if we start with m0 = m and look at m1 = πm0, m2 = πm1, etc., we must eventually come back to mk = m0. (The numbers must repeat sooner or later, and the first number to reappear must be m0 because we know the unique predecessors of the other numbers m1, m2, . . . , mk-1.) Therefore every permutation defines a cycle arrangement. Conversely, every cycle arrangement obviously defines a permutation if we reverse the construction, and this one-to-one correspondence shows that permutations and cycle arrangements are essentially the same thing.
Therefore  is the number of permutations of n objects that contain exactly k cycles. If we sum  over all k, we must get the total number of permutations:



For example, 6 + 11 + 6 + 1 = 24 = 4!.
Stirling numbers are useful because the recurrence relations (6.3) and (6.8) arise in a variety of problems. For example, if we want to represent ordinary powers xn by falling powers xn, we find that the first few cases are
x0 = x0;x1 = x1;x2 = x2 + x1;x3 = x3 + 3x2 + x1;x4 = x4 + 6x3 + 7x2 + x1.
These coefficients look suspiciously like the numbers in Table 258, reflected between left and right; therefore we can be pretty confident that the general formula is



We'd better define  when k < 0 and n ≥ 0.
And sure enough, a simple proof by induction clinches the argument: We have x·xk = xk+1 + kxk, because xk+1 = xk(x - k); hence x·xn-1 is



In other words, Stirling subset numbers are the coefficients of factorial powers that yield ordinary powers.
We can go the other way too, because Stirling cycle numbers are the coefficients of ordinary powers that yield factorial powers:



We have (x + n - 1)·xk = xk+1 + (n - 1)xk, so a proof like the one just given shows that



This leads to a proof by induction of the general formula



(Setting x = 1 gives (6.9) again.)
But wait, you say. This equation involves rising factorial powers , while (6.10) involves falling factorials . What if we want to express  in terms of ordinary powers, or if we want to express xn in terms of rising powers? Easy; we just throw in some minus signs and get






This works because, for example, the formula
x4 = x(x - 1)(x - 2)(x - 3) = x4 - 6x3 + 11x2 - 6x
is just like the formula
 = x(x + 1)(x + 2)(x + 3) = x4 + 6x3 + 11x2 + 6x
but with alternating signs. The general identity



of exercise 2.17 converts (6.10) to (6.12) and (6.11) to (6.13) if we negate x.


Table 264 Basic Stirling number identities, for integer n ≥ 0.






Table 265 Additional Stirling number identities, for integers l, m, n ≥ 0.







Also, , a generalization of (6.9).
We can remember when to stick the (-1)n-k factor into a formula like (6.12) because there's a natural ordering of powers when x is large:



The Stirling numbers  and  are nonnegative, so we have to use minus signs when expanding a "small" power in terms of "large" ones.
We can plug (6.11) into (6.12) and get a double sum:



This holds for all x, so the coefficients of x0, x1, . . . , xn-1, xn+1, xn+2, . . . on the right must all be zero and we must have the identity



Stirling numbers, like binomial coefficients, satisfy many surprising identities. But these identities aren't as versatile as the ones we had in Chapter 5, so they aren't applied nearly as often. Therefore it's best for us just to list the simplest ones, for future reference when a tough Stirling nut needs to be cracked. Tables 264 and 265 contain the formulas that are most frequently useful; the principal identities we have already derived are repeated there.
When we studied binomial coefficients in Chapter 5, we found that it was advantageous to define  for negative n in such a way that the identity  is valid without any restrictions. Using that identity to extend the 's beyond those with combinatorial significance, we discovered (in Table 164) that Pascal's triangle essentially reproduces itself in a rotated form when we extend it upward. Let's try the same thing with Stirling's triangles: What happens if we decide that the basic recurrences



are valid for all integers n and k? The solution becomes unique if we make the reasonable additional stipulations that





Table 267 Stirling's triangles in tandem.



In fact, a surprisingly pretty pattern emerges: Stirling's triangle for cycles appears above Stirling's triangle for subsets, and vice versa! The two kinds of Stirling numbers are related by an extremely simple law [220, 221]:



We have "duality," something like the relations between min and max, between x and x, between  and , between gcd and lcm. It's easy to check that both of the recurrences  and  amount to the same thing, under this correspondence.


6.2 Eulerian Numbers
Another triangle of values pops up now and again, this one due to Euler [104, §13; 110, page 485], and we denote its elements by . The angle brackets in this case suggest "less than" and "greater than" signs;  is the number of permutations π1π2 . . . πn of {1, 2, . . . , n} that have k ascents, namely, k places where πj < πj+1. (Caution: This notation is less standard than our notations ,  for Stirling numbers. But we'll see that it makes good sense.)
(Knuth [209, first edition] used  for .)
For example, eleven permutations of {1, 2, 3, 4} have two ascents:
1324,     1423,     2314,     2413,     3412;1243,     1342,     2341;         2134,     3124,     4123.
(The first row lists the permutations with π1 < π2 > π3 < π4; the second row lists those with π1 < π2 < π3 > π4 and π1 > π2 < π3 < π4.) Hence .


Table 268 Euler's triangle.



Table 268 lists the smallest Eulerian numbers; notice that the trademark sequence is 1, 11, 11, 1 this time. There can be at most n - 1 ascents, when n > 0, so we have  on the diagonal of the triangle.
Euler's triangle, like Pascal's, is symmetric between left and right. But in this case the symmetry law is slightly different:



The permutation π1π2 . . . πn has n-1-k ascents if and only if its "reflection" πn . . . π2π1 has k ascents.
Let's try to find a recurrence for . Each permutation ρ = ρ1 . . . ρn-1 of {1, . . . , n - 1} leads to n permutations of {1, 2, . . . , n} if we insert the new element n in all possible ways. Suppose we put n in position j, obtaining the permutation π = ρ1 . . . ρj-1 n ρj . . . ρn-1. The number of ascents in π is the same as the number in ρ, if j = 1 or if ρj-1 < ρj; it's one greater than the number in ρ, if ρj-1 > ρj or if j = n. Therefore π has k ascents in a total of  ways from permutations ρ that have k ascents, plus a total of  ways from permutations ρ that have k - 1 ascents. The desired recurrence is



Once again we start the recurrence off by setting



and we will assume that  when k < 0.
Eulerian numbers are useful primarily because they provide an unusual connection between ordinary powers and consecutive binomial coefficients:



(This is called "Worpitzky's identity" [378].) For example, we have



Western scholars have recently learned of a significant Chinese book by Li ShanLan [249; 265, pages 320-325], published in 1867, which contains the first known appearance of formula (6.37).
and so on. It's easy to prove (6.37) by induction (exercise 14).
Incidentally, (6.37) gives us yet another way to obtain the sum of the first n squares: We have , hence



The Eulerian recurrence (6.35) is a bit more complicated than the Stirling recurrences (6.3) and (6.8), so we don't expect the numbers  to satisfy as many simple identities. Still, there are a few:









If we multiply (6.39) by zn-m and sum on m, we get . Replacing z by z - 1 and equating coefficients of zk gives (6.40). Thus the last two of these identities are essentially equivalent. The first identity, (6.38), gives us special values when m is small:





Table 270 Second-order Eulerian triangle.



We needn't dwell further on Eulerian numbers here; it's usually sufficient simply to know that they exist, and to have a list of basic identities to fall back on when the need arises. However, before we leave this topic, we should take note of yet another triangular pattern of coefficients, shown in Table 270. We call these "second-order Eulerian numbers" , because they satisfy a recurrence similar to (6.35) but with n replaced by 2n - 1 in one place:



These numbers have a curious combinatorial interpretation, first noticed by Gessel and Stanley [147]: If we form permutations of the multiset {1, 1, 2, 2, . . . , n, n} with the special property that all numbers between the two occurrences of m are greater than m, for 1 ≤ m ≤ n, then  is the number of such permutations that have k ascents. For example, there are eight suitable single-ascent permutations of {1, 1, 2, 2, 3, 3}:
113322, 133221, 221331, 221133, 223311, 233211, 331122, 331221.
Thus . The multiset {1, 1, 2, 2, . . . , n, n} has a total of



suitable permutations, because the two appearances of n must be adjacent and there are 2n - 1 places to insert them within a permutation for n - 1. For example, when n = 3 the permutation 1221 has five insertion points, yielding 331221, 133221, 123321, 122331, and 122133. Recurrence (6.41) can be proved by extending the argument we used for ordinary Eulerian numbers.
Second-order Eulerian numbers are important chiefly because of their connection with Stirling numbers [148]: We have, by induction on n,






For example,



(We already encountered the case n = 1 in (6.7).) These identities hold whenever x is an integer and n is a nonnegative integer. Since the right-hand sides are polynomials in x, we can use (6.43) and (6.44) to define Stirling numbers  and  for arbitrary real (or complex) values of x.
If n > 0, these polynomials  and  are zero when x = 0, x = 1, . . . , and x = n; therefore they are divisible by (x-0), (x-1), . . . , and (x-n). It's interesting to look at what's left after these known factors are divided out. We define the Stirling polynomials σn(x) by the rule



(The degree of σn(x) is n - 1.) The first few cases are
σ0(x)   =   1/x ;σ1(x)   =   1/2 ;σ2(x)   =   (3x - 1)/24 ;σ3(x)   =   (x2 - x)/48 ;σ4(x)   =   (15x3 - 30x2 + 5x + 2)/5760 .
So 1/x is a polynomial?
(Sorry about that.)
They can be computed via the second-order Eulerian numbers; for example,
σ3(x) = ((x-4)(x-5) + 8(x-4)(x+1) + 6(x+2)(x+1)) /6!.


Table 272 Stirling convolution formulas.



It turns out that these polynomials satisfy two very pretty identities:






And in general, if St(z) is the power series that satisfies



then



Therefore we can obtain general convolution formulas for Stirling numbers, as we did for binomial coefficients in Table 202; the results appear in Table 272. When a sum of Stirling numbers doesn't fit the identities of Table 264 or 265, Table 272 may be just the ticket. (An example appears later in this chapter, following equation (6.100). Exercise 7.19 discusses the general principles of convolutions based on identities like (6.50) and (6.53).)


6.3 Harmonic Numbers
It's time now to take a closer look at harmonic numbers, which we first met back in Chapter 2:



These numbers appear so often in the analysis of algorithms that computer scientists need a special notation for them. We use Hn, the 'H' standing for "harmonic," since a tone of wavelength 1/n is called the nth harmonic of a tone whose wavelength is 1. The first few values look like this:



Exercise 21 shows that Hn is never an integer when n > 1.
Here's a card trick, based on an idea by R. T. Sharp [325], that illustrates how the harmonic numbers arise naturally in simple situations. Given n cards and a table, we'd like to create the largest possible overhang by stacking the cards up over the table's edge, subject to the laws of gravity:



This must be Table 273.
To define the problem a bit more, we assume that card k rests on card k + 1, for 1 ≤ k < n. We also require the right edge of each card to be parallel to the edge of the table; otherwise we could increase the overhang by rotating the cards so that their corners stick out a little farther. And to make the answer simpler, we assume that each card is 2 units long.
With one card, we get maximum overhang when its center of gravity is just above the edge of the table. The center of gravity is in the middle of the card, so we can create half a cardlength, or 1 unit, of overhang.
With two cards, it's not hard to convince ourselves that we get maximum overhang when the center of gravity of the top card is just above the edge of the second card, and the center of gravity of both cards combined is just above the edge of the table. The joint center of gravity of two cards will be in the middle of their common part, so we are able to achieve an additional half unit of overhang.
This pattern suggests a general method, where we place cards so that the center of gravity of the top k cards lies just above the edge of the k + 1st card (which supports those top k). The table plays the role of the n + 1st card. To express this condition algebraically, we can let dk be the distance from the extreme edge of the top card to the corresponding edge of the kth card from the top. Then d1 = 0, and we want to make dk+1 the center of gravity of the first k cards:



(The center of gravity of k objects, having respective weights w1, . . . , wk and having respective centers of gravity at positions p1, . . . , pk, is at position (w1p1 + · · · + wkpk)/(w1 + · · · + wk).) We can rewrite this recurrence in two equivalent forms


kdk+1
= k + d1 + · · · + dk-1 + dk ,          k ≥ 0 ;


(k - 1)dk
= k - 1 + d1 + · · · + dk-1 ,             k ≥ 1.


Subtracting these equations tells us that
kdk+1 - (k - 1)dk = 1 + dk ,               k ≥ 1;
hence dk+1 = dk + 1/k. The second card will be offset half a unit past the third, which is a third of a unit past the fourth, and so on. The general formula



follows by induction, and if we set k = n we get dn+1 = Hn as the total overhang when n cards are stacked as described.
Could we achieve greater overhang by holding back, not pushing each card to an extreme position but storing up "potential gravitational energy" for a later advance? No; any well-balanced card placement has



Furthermore d1 = 0. It follows by induction that dk+1 ≤ Hk.
Notice that it doesn't take too many cards for the top one to be completely past the edge of the table. We need an overhang of more than one cardlength, which is 2 units. The first harmonic number to exceed 2 is , so we need only four cards.
Anyone who actually tries to achieve this maximum overhang with 52 cards is probably not dealing with a full deck—or maybe he's a real joker.
And with 52 cards we have an H52-unit overhang, which turns out to be H52/2 ≈ 2.27 cardlengths. (We will soon learn a formula that tells us how to compute an approximate value of Hn for large n without adding up a whole bunch of fractions.)
An amusing problem called the "worm on the rubber band" shows harmonic numbers in another guise. A slow but persistent worm, W, starts at one end of a meter-long rubber band and crawls one centimeter per minute toward the other end. At the end of each minute, an equally persistent keeper of the band, K, whose sole purpose in life is to frustrate W, stretches it one meter. Thus after one minute of crawling, W is 1 centimeter from the start and 99 from the finish; then K stretches it one meter. During the stretching operation W maintains his relative position, 1% from the start and 99% from the finish; so W is now 2 cm from the starting point and 198 cm from the goal. After W crawls for another minute the score is 3 cm traveled and 197 to go; but K stretches, and the distances become 4.5 and 295.5. And so on. Does the worm ever reach the finish? He keeps moving, but the goal seems to move away even faster. (We're assuming an infinite longevity for K and W, an infinite elasticity of the band, and an infinitely tiny worm.)
Metric units make this problem more scientific.
Let's write down some formulas. When K stretches the rubber band, the fraction of it that W has crawled stays the same. Thus he crawls 1/100th of it the first minute, 1/200th the second, 1/300th the third, and so on. After n minutes the fraction of the band that he's crawled is



So he reaches the finish if Hn ever surpasses 100.
We'll see how to estimate Hn for large n soon; for now, let's simply check our analysis by considering how "Superworm" would perform in the same situation. Superworm, unlike W, can crawl 50 cm per minute; so she will crawl Hn/2 of the band length after n minutes, according to the argument we just gave. If our reasoning is correct, Superworm should finish before n reaches 4, since H4 > 2. And yes, a simple calculation shows that Superworm has only  cm left to travel after three minutes have elapsed. She finishes in 3 minutes and 40 seconds flat.
A flatworm, eh?
Harmonic numbers appear also in Stirling's triangle. Let's try to find a closed form for , the number of permutations of n objects that have exactly two cycles. Recurrence (6.8) tells us that



and this recurrence is a natural candidate for the summation factor technique of Chapter 2:



Unfolding this recurrence tells us that ; hence



We proved in Chapter 2 that the harmonic series ∑k1/k diverges, which means that Hn gets arbitrarily large as n → ∞. But our proof was indirect; we found that a certain infinite sum (2.58) gave different answers when it was rearranged, hence ∑k 1/k could not be bounded. The fact that Hn → ∞ seems counter-intuitive, because it implies among other things that a large enough stack of cards will overhang a table by a mile or more, and that the worm W will eventually reach the end of his rope. Let us therefore take a closer look at the size of Hn when n is large.
The simplest way to see that Hn → ∞ is probably to group its terms according to powers of 2. We put one term into group 1, two terms into group 2, four into group 3, eight into group 4, and so on:



Both terms in group 2 are between  and , so the sum of that group is between  and . All four terms in group 3 are between  and , so their sum is also between  and 1. In fact, each of the 2k-1 terms in group k is between 2-k and 21-k; hence the sum of each individual group is between  and 1.
This grouping procedure tells us that if 1/n is in group k, we must have Hn > k/2 and Hn ≤ k (by induction on k). Thus Hn → ∞, and in fact



We now know Hn within a factor of 2. Although the harmonic numbers approach infinity, they approach it only logarithmically—that is, quite slowly.
We should call them the worm numbers, they're so slow.
Better bounds can be found with just a little more work and a dose of calculus. We learned in Chapter 2 that Hn is the discrete analog of the continuous function ln n. The natural logarithm is defined as the area under a curve, so a geometric comparison is suggested:



The area under the curve between 1 and n, which is , is less than the area of the n rectangles, which is . Thus ln n < Hn; this is a sharper result than we had in (6.59). And by placing the rectangles a little differently, we get a similar upper bound:



"I now see a way too how ye aggregate of ye termes of Musicall progressions may bee found (much after ye same manner) by Logarithms, but ye calculations for finding out those rules would bee still more troublesom."
—I. Newton [280]
This time the area of the n rectangles, Hn, is less than the area of the first rectangle plus the area under the curve. We have proved that



We now know the value of Hn with an error of at most 1.
"Second order" harmonic numbers  arise when we sum the squares of the reciprocals, instead of summing simply the reciprocals:



Similarly, we define harmonic numbers of order r by summing (-r)th powers:



If r > 1, these numbers approach a limit as n → ∞; we noted in exercise 2.31 that this limit is conventionally called Riemann's zeta function:



Euler [103] discovered a neat way to use generalized harmonic numbers to approximate the ordinary ones, . Let's consider the infinite series



which converges when k > 1. The left-hand side is ln k - ln(k - 1); therefore if we sum both sides for 2 ≤ k ≤ n the left-hand sum telescopes and we get



Rearranging, we have an expression for the difference between Hn and ln n:



When n → ∞, the right-hand side approaches the limiting value



which is now known as Euler's constant and conventionally denoted by the Greek letter γ. In fact, ζ(r) - 1 is approximately 1/2r, so this infinite series converges rather rapidly and we can compute the decimal value



"Huius igitur quantitatis constantis C valorem deteximus, quippe est C = 0, 577218."
—L. Euler [103]
Euler's argument establishes the limiting relation



thus Hn lies about 58% of the way between the two extremes in (6.60). We are gradually homing in on its value.
Further refinements are possible, as we will see in Chapter 9. We will prove, for example, that



This formula allows us to conclude that the millionth harmonic number is
H1000000 ≈ 14.3927267228657236313811275 ,
without adding up a million fractions. Among other things, this implies that a stack of a million cards can overhang the edge of a table by more than seven cardlengths.
What does (6.66) tell us about the worm on the rubber band? Since Hn is unbounded, the worm will definitely reach the end, when Hn first exceeds 100. Our approximation to Hn says that this will happen when n is approximately
e100-γ ≈ e99.423 .
In fact, exercise 9.49 proves that the critical value of n is either e100-γ or e100-γ. We can imagine W's triumph when he crosses the finish line at last, much to K's chagrin, some 287 decillion centuries after his long crawl began. (The rubber band will have stretched to more than 1027 light years long; its molecules will be pretty far apart.)
Well, they can't really go at it this long; the world will have ended much earlier, when the Tower of Brahma is fully transferred.


6.4 Harmonic Summation
Now let's look at some sums involving harmonic numbers, starting with a review of a few ideas we learned in Chapter 2. We proved in (2.36) and (2.57) that






Let's be bold and take on a more general sum, which includes both of these as special cases: What is the value of



when m is a nonnegative integer?
The approach that worked best for (6.67) and (6.68) in Chapter 2 was called summation by parts. We wrote the summand in the form u(k)Δv(k), and we applied the general identity



Remember? The sum that faces us now, , is a natural for this method because we can let



(In other words, harmonic numbers have a simple Δ and binomial coefficients have a simple Δ-1, so we're in business.) Plugging into (6.69) yields



The remaining sum is easy, since we can absorb the (k + 1)-1 using our old standby, equation (5.5):



Thus we have the answer we seek:



(This checks nicely with (6.67) and (6.68) when m = 0 and m = 1.)
The next example sum uses division instead of multiplication: Let us try to evaluate



If we expand Hk by its definition, we obtain a double sum,



Now another method from Chapter 2 comes to our aid; equation (2.33) tells us that



It turns out that we could also have obtained this answer in another way if we had tried to sum by parts (see exercise 26).
Now let's try our hands at a more difficult problem [354], which doesn't submit to summation by parts:



(This sum doesn't explicitly mention harmonic numbers either; but who knows when they might turn up?)
(Not to give the answer away or anything.)
We will solve this problem in two ways, one by grinding out the answer and the other by being clever and/or lucky. First, the grinder's approach. We expand (n - k)n by the binomial theorem, so that the troublesome k in the denominator will combine with the numerator:



This isn't quite the mess it seems, because the kj-1 in the inner sum is a polynomial in k, and identity (5.40) tells us that we are simply taking the nth difference of this polynomial. Almost; first we must clean up a few things. For one, kj-1 isn't a polynomial if j = 0; so we will need to split off that term and handle it separately. For another, we're missing the term k = 0 from the formula for nth difference; that term is nonzero when j = 1, so we had better restore it (and subtract it out again). The result is



OK, now the top line (the only remaining double sum) is zero: It's the sum of multiples of nth differences of polynomials of degree less than n, and such nth differences are zero. The second line is zero except when j = 1, when it equals -nn. So the third line is the only residual difficulty; we have reduced the original problem to a much simpler sum:



For example, ; hence U3 = 27(T3 - 1) as claimed.
How can we evaluate Tn? One way is to replace  by , obtaining a simple recurrence for Tn in terms of Tn-1. But there's a more instructive way: We had a similar formula in (5.41), namely



If we subtract out the term for k = 0 and set x = 0, we get -Tn. So let's do it:



(We have used the expansion (6.11) of ; we can divide x out of the numerator because .) But we know from (6.58) that ; hence Tn = Hn, and we have the answer:



That's one approach. The other approach will be to try to evaluate a much more general sum,



the value of the original Un will drop out as the special case Un(n, -1). (We are encouraged to try for more generality because the previous derivation "threw away" most of the details of the given problem; somehow those details must be irrelevant, because the nth difference wiped them away.)
We could replay the previous derivation with small changes and discover the value of Un(x, y). Or we could replace (x + ky)n by (x + ky)n-1(x + ky) and then replace  by , leading to the recurrence



this can readily be solved with a summation factor (exercise 5).
But it's easiest to use another trick that worked to our advantage in Chapter 2: differentiation. The derivative of Un(x, y) with respect to y brings out a k that cancels with the k in the denominator, and the resulting sum is trivial:



(Once again, the nth difference of a polynomial of degree < n has vanished.)
We've proved that the derivative of Un(x, y) with respect to y is nxn-1, independent of y. In general, if f′(y) = c then f(y) = f(0) + cy; therefore we must have Un(x, y) = Un(x, 0) + nxn-1y.
The remaining task is to determine Un(x, 0). But Un(x, 0) is just xn times the sum Tn = Hn we've already considered in (6.72); therefore the general sum in (6.74) has the closed form



In particular, the solution to the original problem is Un(n,-1) = nn(Hn-1).


6.5 Bernoulli Numbers
The next important sequence of numbers on our agenda is named after Jakob Bernoulli (1654-1705), who discovered curious relationships while working out the formulas for sums of mth powers [26]. Let's write



(Thus, when m > 0 we have  in the notation of generalized harmonic numbers.) Bernoulli looked at the following sequence of formulas and spotted a pattern:



Can you see it too? The coefficient of nm+1 in Sm(n) is always 1/(m + 1). The coefficient of nm is always -1/2. The coefficient of nm-1 is always . . . let's see . . . m/12. The coefficient of nm-2 is always zero. The coefficient of nm-3 is always . . . let's see . . . hmmm . . . yes, it's -m(m-1)(m-2)/720. The coefficient of nm-4 is always zero. And it looks as if the pattern will continue, with the coefficient of nm-k always being some constant times mk.
That was Bernoulli's empirical discovery. (He did not give a proof.) In modern notation we write the coefficients in the form



Bernoulli numbers are defined by an implicit recurrence relation,



For example, . The first few values turn out to be



(All conjectures about a simple closed form for Bn are wiped out by the appearance of the strange fraction -691/2730.)
We can prove Bernoulli's formula (6.78) by induction on m, using the perturbation method (one of the ways we found S2(n) = n in Chapter 2):



Let Ŝm(n) be the right-hand side of (6.78); we wish to show that Sm(n) = Ŝm(n), assuming that Sj(n) = Ŝj(n) for 0 ≤ j < m. We begin as we did for m = 2 in Chapter 2, subtracting Sm+1(n) from both sides of (6.80). Then we expand each Sj(n) using (6.78), and regroup so that the coefficients of powers of n on the right-hand side are brought together and simplified:



(This derivation is a good review of the standard manipulations we learned in Chapter 5.) Thus Δ = 0 and Sm(n) = Ŝm(n), QED.
In Chapter 7 we'll use generating functions to obtain a much simpler proof of (6.78). The key idea will be to show that the Bernoulli numbers are the coefficients of the power series



Here's some more neat stuff that you'll probably want to skim through the first time.
—Friendly TA



Let's simply assume for now that equation (6.81) holds, so that we can derive some of its amazing consequences. If we add z to both sides, thereby cancelling the term B1z/1! = -  z from the right, we get



Here coth is the "hyperbolic cotangent" function, otherwise known in calculus books as cosh z/sinh z; we have



Changing z to -z gives  coth  coth ; hence every odd-numbered coefficient of  coth  must be zero, and we have



Furthermore (6.82) leads to a closed form for the coefficients of coth:



But there isn't much of a market for hyperbolic functions; people are more interested in the "real" functions of trigonometry. We can express ordinary trigonometric functions in terms of their hyperbolic cousins by using the rules



the corresponding power series are



Hence cot z = cos z/sin z = i cosh iz/ sinh iz = i coth iz, and we have



I see, we get "real" functions by using imaginary numbers.
Another remarkable formula for z cot z was found by Euler (exercise 73):



We can expand Euler's formula in powers of z2, obtaining



Equating coefficients of z2n with those in our other formula, (6.87), gives us an almost miraculous closed form for infinitely many infinite sums:



For example,






Formula (6.89) is not only a closed form for , it also tells us the approximate size of B2n, since  is very near 1 when n is large. And it tells us that (-1)n-1B2n > 0 for all n > 0; thus the nonzero Bernoulli numbers alternate in sign.
And that's not all. Bernoulli numbers also appear in the coefficients of the tangent function,






as well as other trigonometric functions (exercise 72). Formula (6.92) leads to another important fact about the Bernoulli numbers, namely that



We have, for example:



(The T's are called tangent numbers.)
One way to prove (6.93), following an idea of B. F. Logan, is to consider the power series



where Tn(x) is a polynomial in x; setting x = 0 gives Tn(0) = Tn, the nth tangent number. If we differentiate (6.94) with respect to x, we get



When x = tan w, this is tan(z + w). Hence, by Taylor's theorem, the nth derivative of tan w is Tn(tan w).
but if we differentiate with respect to z, we get



(Try it—the cancellation is very pretty.) Therefore we have



a simple recurrence from which it follows that the coefficients of Tn(x) are nonnegative integers. Moreover, we can easily prove that Tn(x) has degree n + 1, and that its coefficients are alternately zero and positive. Therefore T2n+1(0) = T2n+1 is a positive integer, as claimed in (6.93).
Recurrence (6.95) gives us a simple way to calculate Bernoulli numbers, via tangent numbers, using only simple operations on integers; by contrast, the defining recurrence (6.79) involves difficult arithmetic with fractions.
If we want to compute the sum of mth powers from a to b - 1 instead of from 0 to n - 1, the theory of Chapter 2 tells us that



This identity has interesting consequences when we consider negative values of k: We have



hence
Sm(0) - Sm(-n + 1) = (-1)m (Sm(n) - Sm(0)).
But Sm(0) = 0, so we have the identity



Therefore Sm(1) = 0. If we write the polynomial Sm(n) in factored form, it will always have the factors n and (n -1), because it has the roots 0 and 1. In general, Sm(n) is a polynomial of degree m + 1 with leading term . Moreover, we can set  in (6.97) to deduce that ; if m is even, this makes , so  will be an additional factor. These observations explain why we found the simple factorization



(Johann Faulhaber implicitly used (6.97) in 1631 [119] to find simple formulas for Sm(n) as polynomials in n(n + 1)/2 when m ≤ 17; see [222].)
in Chapter 2; we could have used such reasoning to deduce the value of S2(n) without calculating it! Furthermore, (6.97) implies that the polynomial with the remaining factors, Ŝm(n) = Sm(n)/, always satisfies
Ŝm(1 - n) = Ŝm(n),          m even,     m > 0.
It follows that Sm(n) can always be written in the factored form



Here , and α2, . . . , αm/2 are appropriate complex numbers whose values depend on m. For example,






If m is odd and greater than 1, we have Bm = 0; hence Sm(n) is divisible by n2 (and by (n - 1)2). Otherwise the roots of Sm(n) don't seem to obey a simple law.
Let's conclude our study of Bernoulli numbers by looking at how they relate to Stirling numbers. One way to compute Sm(n) is to change ordinary powers to falling powers, since the falling powers have easy sums. After doing those easy sums we can convert back to ordinary powers:



Therefore, equating coefficients with those in (6.78), we must have the identity



It would be nice to prove this relation directly, thereby discovering Bernoulli numbers in a new way. But the identities in Tables 264 or 265 don't give us any obvious handle on a proof by induction that the left-hand sum in (6.99) is a constant times mk-1. If k = m + 1, the left-hand sum is just , so that case is easy. And if k = m, the left-hand side sums to ; so that case is pretty easy too. But if k < m, the left-hand sum looks hairy. Bernoulli would probably not have discovered his numbers if he had taken this route.
One thing we can do is replace  by . The (j + 1) nicely cancels with the awkward denominator, and the left-hand side becomes



The second sum is zero, when k < m, by (6.31). That leaves us with the first sum, which cries out for a change in notation; let's rename all variables so that the index of summation is k, and so that the other parameters are m and n. Then identity (6.99) is equivalent to



Good, we have something that looks more pleasant—although Table 265 still doesn't suggest any obvious next step.
The convolution formulas in Table 272 now come to the rescue. We can use (6.49) and (6.48) to rewrite the summand in terms of Stirling polynomials:



Things are looking up; the convolution in (6.46), with t = 1, yields



Formula (6.100) is now verified, and we find that Bernoulli numbers are related to the constant terms in the Stirling polynomials:








6.6 Fibonacci Numbers
Now we come to a special sequence of numbers that is perhaps the most pleasant of all, the Fibonacci sequence Fn:



Unlike the harmonic numbers and the Bernoulli numbers, the Fibonacci numbers are nice simple integers. They are defined by the recurrence



The simplicity of this rule—the simplest possible recurrence in which each number depends on the previous two—accounts for the fact that Fibonacci numbers occur in a wide variety of situations.
"Bee trees" provide a good example of how Fibonacci numbers can arise naturally. Let's consider the pedigree of a male bee. Each male (also known as a drone) is produced asexually from a female (also known as a queen); each female, however, has two parents, a male and a female. Here are the first few levels of the tree:



The back-to-nature nature of this example is shocking. This book should be banned.
The drone has one grandfather and one grandmother; he has one greatgrandfather and two great-grandmothers; he has two great-great-grandfathers and three great-great-grandmothers. In general, it is easy to see by induction that he has exactly Fn+1 greatn-grandpas and Fn+2 greatn-grandmas.
Fibonacci numbers are often found in nature, perhaps for reasons similar to the bee-tree law. For example, a typical sunflower has a large head that contains spirals of tightly packed florets, usually with 34 winding in one direction and 55 in another. Smaller heads will have 21 and 34, or 13 and 21; a gigantic sunflower with 89 and 144 spirals was once exhibited in England. Similar patterns are found in some species of pine cones.
Phyllotaxis, n. The love of taxis.
And here's an example of a different nature [277]: Suppose we put two panes of glass back-to-back. How many ways an are there for light rays to pass through or be reflected after changing direction n times? The first few cases are:



When n is even, we have an even number of bounces and the ray passes through; when n is odd, the ray is reflected and it re-emerges on the same side it entered. The an's seem to be Fibonacci numbers, and a little staring at the figure tells us why: For n ≥ 2, the n-bounce rays either take their first bounce off the opposite surface and continue in an-1 ways, or they begin by bouncing off the middle surface and then bouncing back again to finish in an-2 ways. Thus we have the Fibonacci recurrence an = an-1 + an-2. The initial conditions are different, but not very different, because we have a0 = 1 = F2 and a1 = 2 = F3; therefore everything is simply shifted two places, and an = Fn+2.
Leonardo Fibonacci introduced these numbers in 1202, and mathematicians gradually began to discover more and more interesting things about them. Edouard Lucas, the perpetrator of the Tower of Hanoi puzzle discussed in Chapter 1, worked with them extensively in the last half of the nineteenth century (in fact it was Lucas who popularized the name "Fibonacci numbers"). One of his amazing results was to use properties of Fibonacci numbers to prove that the 39-digit Mersenne number 2127 - 1 is prime.
"La suite de Fibonacci possède des propriétés nombreuses fort intéressantes."
—E. Lucas [259]
One of the oldest theorems about Fibonacci numbers, first published by the Italian astronomer Gian Domenico Cassini in 1680 [51], is the identity



When n = 6, for example, Cassini's identity correctly claims that 13·5 - 82 equals 1. (Johannes Kepler knew this law already in 1608 [202].)
A polynomial formula that involves Fibonacci numbers of the form Fn±k for small values of k can be transformed into a formula that involves only Fn and Fn+1, because we can use the rule



to express Fm in terms of higher Fibonacci numbers when m < n, and we can use



to replace Fm by lower Fibonacci numbers when m > n+1. Thus, for example, we can replace Fn-1 by Fn+1 - Fn in (6.103) to get Cassini's identity in the form



Moreover, Cassini's identity reads
Fn+2 Fn -  = (-1)n+1
when n is replaced by n + 1; this is the same as (Fn+1 + Fn)Fn -  = (-1)n+1, which is the same as (6.106). Thus Cassini(n) is true if and only if Cassini(n+1) is true; equation (6.103) holds for all n by induction.
Cassini's identity is the basis of a geometrical paradox that was one of Lewis Carroll's favorite puzzles [63], [319], [364]. The idea is to take a chessboard and cut it into four pieces as shown here, then to reassemble the pieces into a rectangle:



Presto: The original area of 8 × 8 = 64 squares has been rearranged to yield 5 × 13 = 65 squares! A similar construction dissects any Fn × Fn square into four pieces, using Fn+1, Fn, Fn-1, and Fn-2 as dimensions wherever the illustration has 13, 8, 5, and 3 respectively. The result is an Fn-1 × Fn+1 rectangle; by (6.103), one square has therefore been gained or lost, depending on whether n is even or odd.
The paradox is explained because . . . well, magic tricks aren't supposed to be explained.
Strictly speaking, we can't apply the reduction (6.105) unless m ≥ 2, because we haven't defined Fn for negative n. A lot of maneuvering becomes easier if we eliminate this boundary condition and use (6.104) and (6.105) to define Fibonacci numbers with negative indices. For example, F-1 turns out to be F1 - F0 = 1; then F-2 is F0 - F-1 = -1. In this way we deduce the values



and it quickly becomes clear (by induction) that



Cassini's identity (6.103) is true for all integers n, not just for n > 0, when we extend the Fibonacci sequence in this way.
The process of reducing Fn±k to a combination of Fn and Fn+1 by using (6.105) and (6.104) leads to the sequence of formulas


Fn+2    =      Fn+1 +   Fn
 
Fn-1   =     Fn+1   -     Fn


Fn+3    =    2Fn+1 +   Fn
 
Fn-2   =   -Fn+1   +  2Fn


Fn+4    =    3Fn+1 + 2Fn
 
Fn-3   =     2Fn+1  -  3Fn


Fn+5    =    5Fn+1 + 3Fn
 
Fn-4   =   -3Fn+1  +  5Fn


in which another pattern becomes obvious:



This identity, easily proved by induction, holds for all integers k and n (positive, negative, or zero).
If we set k = n in (6.108), we find that



hence F2n is a multiple of Fn. Similarly,
F3n = F2nFn+1 + F2n-1Fn ,
and we may conclude that F3n is also a multiple of Fn. By induction,



for all integers k and n. This explains, for example, why F15 (which equals 610) is a multiple of both F3 and F5 (which are equal to 2 and 5). Even more is true, in fact; exercise 27 proves that



For example, gcd(F12, F18) = gcd(144, 2584) = 8 = F6.
We can now prove a converse of (6.110): If n > 2 and if Fm is a multiple of Fn, then m is a multiple of n. For if Fn\Fm then Fn\gcd(Fm, Fn) = Fgcd(m,n) ≤ Fn. This is possible only if Fgcd(m,n) = Fn; and our assumption that n > 2 makes it mandatory that gcd(m, n) = n. Hence n\m.
An extension of these divisibility ideas was used by Yuri Matijasevich in his famous proof [266] that there is no algorithm to decide if a given multivariate polynomial equation with integer coefficients has a solution in integers. Matijasevich's lemma states that, if n > 2, the Fibonacci number Fm is a multiple of  if and only if m is a multiple of nFn.
Let's prove this by looking at the sequence Fkn mod  for k = 1, 2, 3, . . . , and seeing when Fkn mod  = 0. (We know that m must have the form kn if Fm mod Fn = 0.) First we have Fn mod  = Fn; that's not zero. Next we have
F2n = FnFn+1 + Fn-1Fn ≡ 2FnFn+1          (mod ),
by (6.108), since Fn+1 ≡ Fn-1 (mod Fn). Similarly



This congruence allows us to compute


F3n
= F2n+1Fn + F2nFn-1


 
≡ Fn + (2FnFn+1)Fn+1 = 3Fn          (mod );


F3n+1
= F2n+1Fn+1 + F2nFn


 
≡  + (2FnFn+1)Fn ≡                           (mod ) .


In general, we find by induction on k that



Now Fn+1 is relatively prime to Fn, so


Fkn ≡ 0 (mod )

     kFn ≡ 0 (mod )


 

     k ≡ 0 (mod Fn) .


We have proved Matijasevich's lemma.
One of the most important properties of the Fibonacci numbers is the special way in which they can be used to represent integers. Let's write



Then every positive integer has a unique representation of the form



[The anonymous author of a classic Sanskrit work, Prākṛta Paiṅgala (c. 1320), actually knew this representation many centuries ago.]
(This is "Zeckendorf's theorem" [246], [381].) For example, the representation of one million turns out to be


1000000
= 832040 + 121393 + 46368 + 144 + 55


 
=    F30     +    F26     +    F24   +  F12 + F10 .


We can always find such a representation by using a "greedy" approach, choosing Fk1 to be the largest Fibonacci number ≤ n, then choosing Fk2 to be the largest that is ≤ n - Fk1, and so on. (More precisely, suppose that Fk ≤ n < Fk+1; then we have 0 ≤ n - Fk < Fk+1 - Fk = Fk-1. If n is a Fibonacci number, (6.113) holds with r = 1 and k1 = k. Otherwise n - Fk has a Fibonacci representation Fk2 + · · · + Fkr, by induction on n; and (6.113) holds if we set k1 = k, because the inequalities Fk2 ≤ n - Fk < Fk-1 imply that k ≫ k2.) Conversely, any representation of the form (6.113) implies that
Fk1 ≤ n < Fk1+1 ,
because the largest possible value of Fk2 + · · · + Fkr when k ≫ k2 ≫ · · · ≫ kr ≫ 0 is



(This formula is easy to prove by induction on k; the left-hand side is zero when k is 2 or 3.) Therefore k1 is the greedily chosen value described earlier, and the representation must be unique.
Any system of unique representation is a number system; therefore Zeckendorf's theorem leads to the Fibonacci number system. We can represent any nonnegative integer n as a sequence of 0's and 1's, writing



This number system is something like binary (radix 2) notation, except that there never are two adjacent 1's. For example, here are the numbers from 1 to 20, expressed Fibonacci-wise:


1 = (000001)F
6 = (001001)F
11 = (010100)F
16 = (100100)F


2 = (000010)F
7 = (001010)F
12 = (010101)F
17 = (100101)F


3 = (000100)F
8 = (010000)F
13 = (100000)F
18 = (101000)F


4 = (000101)F
9 = (010001)F
14 = (100001)F
19 = (101001)F


5 = (001000)F
10 = (010010)F
15 = (100010)F
20 = (101010)F


The Fibonacci representation of a million, shown a minute ago, can be contrasted with its binary representation 219 + 218 + 217 + 216 + 214 + 29 + 26:
(1000000)10 = (10001010000000000010100000000)F
       = (11110100001001000000)2 .
The Fibonacci representation needs a few more bits because adjacent 1's are not permitted; but the two representations are analogous.
To add 1 in the Fibonacci number system, there are two cases: If the "units digit" is 0, we change it to 1; that adds F2 = 1, since the units digit refers to F2. Otherwise the two least significant digits will be 01, and we change them to 10 (thereby adding F3 - F2 = 1). Finally, we must "carry" as much as necessary by changing the digit pattern '011' to '100' until there are no two 1's in a row. (This carry rule is equivalent to replacing Fm+1 + Fm by Fm+2.) For example, to go from 5 = (1000)F to 6 = (1001)F or from 6 = (1001)F to 7 = (1010)F requires no carrying; but to go from 7 = (1010)F to 8 = (10000)F we must carry twice.
So far we've been discussing lots of properties of the Fibonacci numbers, but we haven't come up with a closed formula for them. We haven't found closed forms for Stirling numbers, Eulerian numbers, or Bernoulli numbers either; but we were able to discover the closed form  for harmonic numbers. Is there a relation between Fn and other quantities we know? Can we "solve" the recurrence that defines Fn?
The answer is yes. In fact, there's a simple way to solve the recurrence by using the idea of generating function that we looked at briefly in Chapter 5. Let's consider the infinite series



If we can find a simple formula for F(z), chances are reasonably good that we can find a simple formula for its coefficients Fn.
"Sit 1 + x + 2xx + 3x3 + 5x4 + 8x5 + 13x6 + 21x7 + 34x8 &c Series nata ex divisione Unitatis per Trinomium 1 - x - xx."
—A. de Moivre [76]
"The quantities r, s, t, which show the relation of the terms, are the same as those in the denominator of the fraction. This property, howsoever obvious it may be, M. DeMoivre was the first that applied it to use, in the solution of problems about infinite series, which otherwise would have been very intricate."
—J. Stirling [343]
In Chapter 7 we will focus on generating functions in detail, but it will be helpful to have this example under our belts by the time we get there. The power series F(z) has a nice property if we look at what happens when we multiply it by z and by z2:


   F(z) =
F0 + F1z + F2z2 + F3z3 + F4z4 + F5z5 + · · · ,


  zF(z) =
F0z + F1z2 + F2z3 + F3z4 + F4z5 + · · · ,


z2F(z) =
       F0z2 + F1z3 + F2z4 + F3z5 + · · · .


If we now subtract the last two equations from the first, the terms that involve z2, z3, and higher powers of z will all disappear, because of the Fibonacci recurrence. Furthermore the constant term F0 never actually appeared in the first place, because F0 = 0. Therefore all that's left after the subtraction is (F1 - F0)z, which is just z. In other words,
F(z) - zF(z) - z2F(z) = z ,
and solving for F(z) gives us the compact formula



We have now boiled down all the information in the Fibonacci sequence to a simple (although unrecognizable) expression z/(1 - z - z2). This, believe it or not, is progress, because we can factor the denominator and then use partial fractions to achieve a formula that we can easily expand in power series. The coefficients in this power series will be a closed form for the Fibonacci numbers.
The plan of attack just sketched can perhaps be understood better if we approach it backwards. If we have a simpler generating function, say 1/(1 - αz) where α is a constant, we know the coefficients of all powers of z, because
 = 1 + αz + α2z2 + α3z3 + · · · .
Similarly, if we have a generating function of the form A/(1-αz)+B/(1-βz), the coefficients are easily determined, because



Therefore all we have to do is find constants A, B, α, and β such that



and we will have found a closed form Aαn + Bβn for the coefficient Fn of zn in F(z). The left-hand side can be rewritten



so the four constants we seek are the solutions to two polynomial equations:






We want to factor the denominator of F(z) into the form (1 - αz)(1 - βz); then we will be able to express F(z) as the sum of two fractions in which the factors (1 - αz) and (1 - βz) are conveniently separated from each other.
Notice that the denominator factors in (6.119) have been written in the form (1 - αz)(1 - βz), instead of the more usual form c(z - ρ1)(z - ρ2) where ρ1 and ρ2 are the roots. The reason is that (1 - αz)(1 - βz) leads to nicer expansions in power series.
We can find α and β in several ways, one of which uses a slick trick: Let us introduce a new variable w and try to find the factorization
w2 - wz - z2 = (w - αz)(w - βz) .
As usual, the authors can't resist a trick.
Then we can simply set w = 1 and we'll have the factors of 1 - z - z2. The roots of w2 - wz - z2 = 0 can be found by the quadratic formula; they are



Therefore



and we have the constants α and β we were looking for.
The number  is important in many parts of mathematics as well as in the art world, where it has been considered since ancient times to be the most pleasing ratio for many kinds of design. Therefore it has a special name, the golden ratio. We denote it by the Greek letter ϕ, in honor of Phidias who is said to have used it consciously in his sculpture. The other root  shares many properties of ϕ, so it has the special name , "phi hat." These numbers are roots of the equation w2 - w - 1 = 0, so we have



The ratio of one's height to the height of one's navel is approximately 1.618, according to extensive empirical observations by European scholars [136].
(More about ϕ and  later.)
We have found the constants α = ϕ and  needed in (6.119); now we merely need to find A and B in (6.120). Setting z = 0 in that equation tells us that B = -A, so (6.120) boils down to



The solution is ; the partial fraction expansion of (6.117) is therefore



Good, we've got F(z) right where we want it. Expanding the fractions into power series as in (6.118) gives a closed form for the coefficient of zn:



(This formula was first published by Daniel Bernoulli in 1728, but people forgot about it until it was rediscovered by Jacques Binet [31] in 1843.)
Before we stop to marvel at our derivation, we should check its accuracy. For n = 0 the formula correctly gives F0 = 0; for n = 1, it gives , which is indeed 1. For higher powers, equations (6.121) show that the numbers defined by (6.123) satisfy the Fibonacci recurrence, so they must be the Fibonacci numbers by induction. (We could also expand ϕn and  by the binomial theorem and chase down the various powers of ; but that gets pretty messy. The point of a closed form is not necessarily to provide us with a fast method of calculation, but rather to tell us how Fn relates to other quantities in mathematics.)
With a little clairvoyance we could simply have guessed formula (6.123) and proved it by induction. But the method of generating functions is a powerful way to discover it; in Chapter 7 we'll see that the same method leads us to the solution of recurrences that are considerably more difficult. Incidentally, we never worried about whether the infinite sums in our derivation of (6.123) were convergent; it turns out that most operations on the coefficients of power series can be justified rigorously whether or not the sums actually converge [182]. Still, skeptical readers who suspect fallacious reasoning with infinite sums can take comfort in the fact that equation (6.123), once found by using infinite series, can be verified by a solid induction proof.
One of the interesting consequences of (6.123) is that the integer Fn is extremely close to the irrational number  when n is large. (Since  is less than 1 in absolute value,  becomes exponentially small and its effect is almost negligible.) For example, F10 = 55 and F11 = 89 are very near



We can use this observation to derive another closed form,



because  for all n ≥ 0. When n is even, Fn is a little bit less than ; otherwise it is a little greater.
Cassini's identity (6.103) can be rewritten



When n is large, 1/Fn-1Fn is very small, so Fn+1/Fn must be very nearly the same as Fn/Fn-1; and (6.124) tells us that this ratio approaches ϕ. In fact, we have



(This identity is true by inspection when n = 0 or n = 1, and by induction when n > 1; we can also prove it directly by plugging in (6.123).) The ratio Fn+1/Fn is very close to ϕ, which it alternately overshoots and undershoots.
By coincidence, ϕ is also very nearly the number of kilometers in a mile. (The exact number is 1.609344, since 1 inch is exactly 2.54 centimeters.) This gives us a handy way to convert mentally between kilometers and miles, because a distance of Fn+1 kilometers is (very nearly) a distance of Fn miles.
Suppose we want to convert a non-Fibonacci number from kilometers to miles; what is 30 km, American style? Easy: We just use the Fibonacci number system and mentally convert 30 to its Fibonacci representation 21 + 8 + 1 by the greedy approach explained earlier. Now we can shift each number down one notch, getting 13 + 5 + 1. (The former '1' was F2, since kr ≫ 0 in (6.113); the new '1' is F1.) Shifting down divides by ϕ, more or less. Hence 19 miles is our estimate. (That's pretty close; the correct answer is about 18.64 miles.) Similarly, to go from miles to kilometers we can shift up a notch; 30 miles is approximately 34 + 13 + 2 = 49 kilometers. (That's not quite as close; the correct number is about 48.28.)
If the USA ever goes metric, our speed limit signs will go from 55 mi/hr to 89 km/hr. Or maybe the highway people will be generous and let us go 90.
It turns out that this shift-down rule gives the correctly rounded number of miles per n kilometers for all n ≤ 100, except in the cases n = 4, 12, 54, 62, 75, 83, 91, 96, and 99, when it is off by less than 2/3 mile. And the shift-up rule gives either the correctly rounded number of kilometers for n miles, or 1 km too many, for all n ≤ 113. (The only really embarrassing case is n = 4, where the individual rounding errors for n = 3 + 1 both go the same direction instead of cancelling each other out.)
The "shift down" rule changes n to f(n/ϕ) and the "shift up" rule changes n to f(nϕ), where f(x) = x + ϕ-1 .


6.7 Continuants
Fibonacci numbers have important connections to the Stern-Brocot tree that we studied in Chapter 4, and they have important generalizations to a sequence of polynomials that Euler studied extensively. These polynomials are called continuants, because they are the key to the study of continued fractions like



The continuant polynomial Kn(x1, x2, . . . , xn) has n parameters, and it is defined by the following recurrence:



For example, the next three cases after K1(x1) are


K2(x1, x2)
=
x1x2 + 1 ;


K3(x1, x2, x3)
=
x1x2x3 + x1 + x3 ;


K4(x1, x2, x3, x4)
=
x1x2x3x4 + x1x2 + x1x4 + x3x4 + 1 .


It's easy to see, inductively, that the number of terms is a Fibonacci number:



When the number of parameters is implied by the context, we can write simply 'K' instead of 'Kn', just as we can omit the number of parameters when we use the hypergeometric functions F of Chapter 5. For example, K(x1, x2) = K2(x1, x2) = x1x2 + 1. The subscript n is of course necessary in formulas like (6.128).
Euler [112] observed that K(x1, x2, . . . , xn) can be obtained by starting with the product x1x2 . . . xn and then striking out adjacent pairs xkxk+1 in all possible ways. We can represent Euler's rule graphically by constructing all "Morse code" sequences of dots and dashes having length n, where each dot contributes 1 to the length and each dash contributes 2; here are the Morse code sequences of length 4:



These dot-dash patterns correspond to the terms of K(x1, x2, x3, x4); a dot signifies a variable that's included and a dash signifies a pair of variables that's excluded. For example,  corresponds to x1x4.
A Morse code sequence of length n that has k dashes has n - 2k dots and n - k symbols altogether. These dots and dashes can be arranged in  ways; therefore if we replace each dot by z and each dash by 1 we get



We also know that the total number of terms in a continuant is a Fibonacci number; hence we have the identity



(A closed form for (6.129), generalizing the Euler-Binet formula (6.123) for Fibonacci numbers, appears in (5.74).)
The relation between continuant polynomials and Morse code sequences shows that continuants have a mirror symmetry:



Therefore they obey a recurrence that adjusts parameters at the left, in addition to the right-adjusting recurrence in definition (6.127):



Both of these recurrences are special cases of a more general law:



This law is easily understood from the Morse code analogy: The first product KmKn yields the terms of Km+n in which there is no dash in the [m, m + 1] position, while the second product yields the terms in which there is a dash there. If we set all the x's equal to 1, this identity tells us that Fm+n+1 = Fm+1Fn+1 + FmFn; thus, (6.108) is a special case of (6.133).
Euler [112] discovered that continuants obey an even more remarkable law, which generalizes Cassini's identity:



This law (proved in exercise 29) holds whenever the subscripts on the K's are all nonnegative. For example, when k = 2, m = 1, and n = 3, we have
K(x1, x2, x3, x4) K(x2, x3) = K(x1, x2, x3) K(x2, x3, x4) + 1 .
Continuant polynomials are intimately connected with Euclid's algorithm. Suppose, for example, that the computation of gcd(m, n) finishes in four steps:


gcd(m, n) = gcd(n0, n1)
    n0 = m ,         n1 = n ;


 
           = gcd(n1, n2)
n2 = n0 mod n1 = n0 - q1n1 ;


 
           = gcd(n2, n3)
n3 = n1 mod n2 = n1 - q2n2 ;


 
           = gcd(n3, n4)
n4 = n2 mod n3 = n2 - q3n3 ;


 
           = gcd(n4, 0) = n4 .
0 = n3 mod n4 = n3 - q4n4 .


Then we have


n4  =  n4
  =  K()n4 ;


n3  =  q4n4
  =  K(q4)n4 ;


n2  =  q3n3 + n4
  =  K(q3, q4)n4 ;


n1  =  q2n2 + n3
  =  K(q2, q3, q4)n4 ;


n0  =  q1n1 + n2
  =  K(q1, q2, q3, q4)n4 .


In general, if Euclid's algorithm finds the greatest common divisor d in k steps, after computing the sequence of quotients q1, . . . , qk, then the starting numbers were K(q1, q2, . . . , qk)d and K(q2, . . . , qk)d. (This fact was noticed early in the eighteenth century by Thomas Fantet de Lagny [232], who seems to have been the first person to consider continuants explicitly. Lagny pointed out that consecutive Fibonacci numbers, which occur as continuants when the q's take their minimum values, are therefore the smallest inputs that cause Euclid's algorithm to take a given number of steps.)
Continuants are also intimately connected with continued fractions, from which they get their name. We have, for example,



The same pattern holds for continued fractions of any depth. It is easily proved by induction; we have, for example,



because of the identity



(This identity is proved and generalized in exercise 30.)
Moreover, continuants are closely connected with the Stern-Brocot tree discussed in Chapter 4. Each node in that tree can be represented as a sequence of L's and R's, say



where a0 ≥ 0, a1 ≥ 1, a2 ≥ 1, a3 ≥ 1, . . . , an-2 ≥ 1, an-1 ≥ 0, and n is even. Using the 2 × 2 matrices L and R of (4.33), it is not hard to prove by induction that the matrix equivalent of (6.137) is



(The proof is part of exercise 87.) For example,



Finally, therefore, we can use (4.34) to write a closed form for the fraction in the Stern-Brocot tree whose L-and-R representation is (6.137):



(This is "Halphen's theorem" [174].) For example, to find the fraction for LRRL we have a0 = 0, a1 = 1, a2 = 2, a3 = 1, and n = 4; equation (6.139) gives



(We have used the rule Kn(x1, . . . , xn-1, xn + 1) = Kn+1(x1, . . . , xn-1, xn, 1) to absorb leading and trailing 1's in the parameter lists; this rule is obtained by setting y = 1 in (6.136).)
A comparison of (6.135) and (6.139) shows that the fraction corresponding to a general node (6.137) in the Stern-Brocot tree has the continued fraction representation



Thus we can convert at sight between continued fractions and the corresponding nodes in the Stern-Brocot tree. For example,



We observed in Chapter 4 that irrational numbers define infinite paths in the Stern-Brocot tree, and that they can be represented as an infinite string of L's and R's. If the infinite string for α is Ra0 La1 Ra2 La3 . . . , there is a corresponding infinite continued fraction



This infinite continued fraction can also be obtained directly: Let α0 = α and for k ≥ 0 let



The a's are called the "partial quotients" of α. If α is rational, say m/n, this process runs through the quotients found by Euclid's algorithm and then stops (with αk+1 = ∞).
Is Euler's constant γ rational or irrational? Nobody knows. We can get partial information about this famous unsolved problem by looking for γ in the Stern-Brocot tree; if it's rational we will find it, and if it's irrational we will find all the closest rational approximations to it. The continued fraction for γ begins with the following partial quotients:



Or if they do, they're not talking.
Therefore its Stern-Brocot representation begins LRLLRLLRLLLLRRRL . . . ; no pattern is evident. Calculations by Richard Brent [38] have shown that, if γ is rational, its denominator must be more than 10,000 decimal digits long. Therefore nobody believes that γ is rational; but nobody so far has been able to prove that it isn't.
Well, γ must be irrational, because of a little-known Einsteinian assertion: "God does not throw huge denominators at the universe."
Let's conclude this chapter by proving a remarkable identity that ties a lot of these ideas together. We introduced the notion of spectrum in Chapter 3; the spectrum of α is the multiset of numbers nα, where α is a given constant. The infinite series



can therefore be said to be the generating function for the spectrum of ϕ, where  is the golden ratio. The identity we will prove, discovered in 1976 by J. L. Davison [73], uses an infinite continued fraction to connect this generating function with the Fibonacci sequence:



Both sides of (6.143) are interesting; let's look first at the numbers nϕ. If the Fibonacci representation (6.113) of n is Fk1 + · · · + Fkr, we expect nϕ to be approximately Fk1+1 + · · · + Fkr+1, the number we get from shifting the Fibonacci representation left (as when converting from miles to kilometers). In fact, we know from (6.125) that
nϕ = Fk1+1 + · · · + Fkr+1 - (k1 + · · · + kr) .
Now  = -1/ϕ and k1 ≫ · · · ≫ kr ≫ 0, so we have



and k1 + · · · + kr has the same sign as (-1)kr, by a similar argument. Hence



Let us say that a number n is Fibonacci odd (or F-odd for short) if its least significant Fibonacci bit is 1; this is the same as saying that kr(n) = 2. Otherwise n is Fibonacci even (F-even). For example, the smallest F-odd numbers are 1, 4, 6, 9, 12, 14, 17, and 19. If kr(n) is even, then n - 1 is F-even, by (6.114); similarly, if kr(n) is odd, then n - 1 is F-odd. Therefore
kr(n) is even                    n - 1 is F-even.
Furthermore, if kr(n) is even, (6.144) implies that kr (nϕ) = 2; if kr(n) is odd, (6.144) says that kr (nϕ) = kr(n) + 1. Therefore kr (nϕ) is always even, and we have proved that
nϕ - 1 is always F-even.
Conversely, if m is any F-even number, we can reverse this computation and find an n such that m + 1 = nϕ. (First add 1 in F-notation as explained earlier. If no carries occur, n is (m + 2) shifted right; otherwise n is (m + 1) shifted right.) The right-hand sum of (6.143) can therefore be written



How about the fraction on the left? Let's rewrite (6.143) so that the continued fraction looks like (6.141), with all numerators 1:



(This transformation is a bit tricky! The numerator and denominator of the original fraction having zFn as numerator should be divided by zFn-1.) If we stop this new continued fraction at 1/z-Fn, its value will be a ratio of continuants,



as in (6.135). Let's look at the denominator first, in hopes that it will be tractable. Setting Qn = Kn+1(z-F0 , . . . , z-Fn ), we find Q0 = 1, Q1 = 1+z-1, Q2 = 1+z-1 +z-2, Q3 = 1+z-1 +z-2 +z-3 +z-4, and in general everything fits beautifully and gives a geometric series
Qn = 1 + z-1 + z-2 + · · · + z-(Fn+2-1) .
The corresponding numerator is Pn = Kn(z-F1 , . . . , z-Fn ); this turns out to be like Qn but with fewer terms. For example, we have
P5 = z-1 + z-2 + z-4 + z-5 + z-7 + z-9 + z-10 + z-12 ,
compared with Q5 = 1 + z-1 + · · · + z-12. A closer look reveals the pattern governing which terms are present: We have



and in general we can prove by induction that



Therefore



Taking the limit as n → ∞ now gives (6.146), because of (6.145).


Exercises

Warmups
1. What are the  permutations of {1, 2, 3, 4} that have exactly two cycles? (The cyclic forms appear in (6.4); non-cyclic forms like 2314 are desired instead.)
2. There are mn functions from a set of n elements into a set of m elements. How many of them range over exactly k different function values?
3. Card stackers in the real world know that it's wise to allow a bit of slack so that the cards will not topple over when a breath of wind comes along. Suppose the center of gravity of the top k cards is required to be at least  units from the edge of the k + 1st card. (Thus, for example, the first card can overhang the second by at most 1- units.) Can we still achieve arbitrarily large overhang, if we have enough cards?
4. Express 1/1 + 1/3 + · · · + 1/(2n+1) in terms of harmonic numbers.
5. Explain how to get the recurrence (6.75) from the definition of Un(x, y) in (6.74), and solve the recurrence.
6. An explorer has left a pair of baby rabbits on an island. If baby rabbits become adults after one month, and if each pair of adult rabbits produces one pair of baby rabbits every month, how many pairs of rabbits are present after n months? (After two months there are two pairs, one of which is newborn.) Find a connection between this problem and the "bee tree" in the text.
If the harmonic numbers are worm numbers, the Fibonacci numbers are rabbit numbers.
7. Show that Cassini's identity (6.103) is a special case of (6.108), and a special case of (6.134).
8. Use the Fibonacci number system to convert 65 mi/hr into an approximate number of km/hr.
9. About how many square kilometers are in 8 square miles?
10. What is the continued fraction representation of ϕ?


Basics
11. What is , the row sum of Stirling's cycle-number triangle with alternating signs, when n is a nonnegative integer?
12. Prove that Stirling numbers have an inversion law analogous to (5.48):



13. The differential operators  and ϑ = zD are mentioned in Chapters 2 and 5. We have
ϑ2 = z2D2 + zD ,
because ϑ2f(z) = ϑzf′(z) = z zf′(z) = z2f″(z) + zf′(z), which is (z2D2 +zD)f(z). Similarly it can be shown that ϑ3 = z3D3 +3z2D2 +zD. Prove the general formulas



for all n ≥ 0. (These can be used to convert between differential expressions of the forms ∑k αkzkf(k)(z) and ∑k βkϑkf(z), as in (5.109).)
14. Prove the power identity (6.37) for Eulerian numbers.
15. Prove the Eulerian identity (6.39) by taking the mth difference of (6.37).
16. What is the general solution of the double recurrence
An,0 = an [n ≥ 0];          A0,k = 0,          if k > 0;An,k = kAn-1,k + An-1,k-1 ,          integers k, n,
when k and n range over the set of all integers?
17. Solve the following recurrences, assuming that  is zero when n < 0 or k < 0:
a. 
b. 
c. 
18. Prove that the Stirling polynomials satisfy
(x + 1) σn(x + 1) = (x - n) σn(x) + xσn-1(x) .
19. Prove that the generalized Stirling numbers satisfy



20. Find a closed form for .
21. Show that if Hn = an/bn where an and bn are integers, the denominator bn is a multiple of 2lg n. Hint: Consider the number 2lg n-1Hn - .
22. Prove that the infinite sum



converges for all complex numbers z, except when z is a negative integer; and show that it equals Hz when z is a nonnegative integer. (Therefore we can use this formula to define harmonic numbers Hz when z is complex.)
23. Equation (6.81) gives the coefficients of z/(ez - 1), when expanded in powers of z. What are the coefficients of z/(ez + 1)? Hint: Consider the identity (ez + 1)(ez - 1) = e2z - 1.
24. Prove that the tangent number T2n+1 is a multiple of 2n. Hint: Prove that all coefficients of T2n(x) and T2n+1(x) are multiples of 2n.
25. Equation (6.57) proves that the worm will eventually reach the end of the rubber band at some time N. Therefore there must come a first time n when he's closer to the end after n minutes than he was after n - 1 minutes. Show that .
26. Use summation by parts to evaluate . Hint: Consider also the related sum .
27. Prove the gcd law (6.111) for Fibonacci numbers.
28. The Lucas number Ln is defined to be Fn+1 + Fn-1. Thus, according to (6.109), we have F2n = FnLn. Here is a table of the first few values:



a. Use the repertoire method to show that the solution Qn to the general recurrence
Q0 = α ;      Q1 = β ;      Qn =  Qn-1 + Qn-2 ,      n > 1
can be expressed in terms of Fn and Ln.
b. Find a closed form for Ln in terms of ϕ and .
29. Prove Euler's identity for continuants, equation (6.134).
30. Generalize (6.136) to find an expression for the incremented continuant K(x1, . . . , xm-1, xm + y, xm+1, . . . , xn), when 1 ≤ m ≤ n.


Homework exercises
31. Find a closed form for the coefficients  in the representation of rising powers by falling powers:



(For example, , hence .)
32. In Chapter 5 we obtained the formulas



by unfolding the recurrence  in two ways. What identities appear when the analogous recurrence  is unwound?
33. Table 264 gives the values of  and . What are closed forms (not involving Stirling numbers) for the next cases,  and ?
34. What are  and , if the basic recursion relation (6.35) is assumed to hold for all integers k and n, and if  for all k < 0?
35. Prove that, for every  > 0, there exists an integer n > 1 (depending on ) such that Hn mod 1 < .
36. Is it possible to stack n bricks in such a way that the topmost brick is not above any point of the bottommost brick, yet a person who weighs the same as 100 bricks can balance on the middle of the top brick without toppling the pile?
37. Express  in terms of harmonic numbers, assuming that m and n are positive integers. What is the limiting value as n → ∞?
38. Find the indefinite sum .
39. Express  in terms of n and Hn.
40. Prove that 1979 divides the numerator of , and give a similar result for 1987. Hint: Use Gauss's trick to obtain a sum of fractions whose numerators are 1979. See also exercise 4.
Ah! Those were prime years.
41. Evaluate the sum



in closed form, when n is an integer (possibly negative).
42. If S is a set of integers, let S + 1 be the "shifted" set {x + 1 | x ∈ S}. How many subsets of {1, 2, . . . , n} have the property that S ∪ (S + 1) = {1, 2, . . . , n + 1}?
43. Prove that the infinite sum
   .1+ .01+ .002+ .0003+ .00005+ .000008+ .0000013       .       .       .
converges to a rational number.
44. Prove the converse of Cassini's identity (6.106): If k and m are integers such that |m2-km-k2| = 1, then there is an integer n such that k = ±Fn and m = ±Fn+1.
45. Use the repertoire method to solve the general recurrence
X0 = α;     X1 = β;      Xn = Xn-1 + Xn-2 + γn + δ .
46. What are cos 36° and cos 72°?
47. Show that



and use this identity to deduce the values of Fp mod p and Fp+1 mod p when p is prime.
48. Prove that zero-valued parameters can be removed from continuant polynomials by collapsing their neighbors together:
Kn(x1, . . . , xm-1, 0, xm+1, . . . , xn)
   = Kn-2(x1, . . . , xm-2, xm-1+xm+1, xm+2, . . . , xn) , 1 < m < n.
49. Find the continued fraction representation of the number ∑n≥1 2-nϕ.
50. Define f(n) for all positive integers n by the recurrence
f(1) = 1 ;
f(2n) = f(n) ;
f(2n + 1) = f(n) + f(n + 1) .
a. For which n is f(n) even?
b. Show that f(n) can be expressed in terms of continuants.


Exam problems
51. Let p be a prime number.
a. Prove that  (mod p), for 1 < k < p.
b. Prove that , for 1 ≤ k < p.
c. Prove that , if p > 2.
d. Prove that if p > 3 we have . Hint: Consider .
52. Let Hn be written in lowest terms as an/bn.
a. Prove that , if p is prime.
b. Find all n > 0 such that an is divisible by 5.
53. Find a closed form for , when 0 ≤ m ≤ n. Hint: Exercise 5.42 has the sum without the Hk factor.
54. Let n > 0. The purpose of this exercise is to show that the denominator of B2n is the product of all primes p such that (p-1)\(2n).
a. Show that Sm(p) + [(p-1)\m] is a multiple of p, when p is prime and m > 0.
b. Use the result of part (a) to show that



Hint: It suffices to prove that, if p is any prime, the denominator of the fraction B2n + [(p-1)\(2n)]/p is not divisible by p.
c. Prove that the denominator of B2n is always an odd multiple of 6, and it is equal to 6 for infinitely many n.
55. Prove (6.70) as a corollary of a more general identity, by summing



and differentiating with respect to x.
56. Evaluate  in closed form as a function of the integers m and n. (The sum is over all integers k except for the value k = m.)
57. The "wraparound binomial coefficients of order 5" are defined by



and . Let Qn be the difference between the largest and smallest of these numbers in row n:



Find and prove a relation between Qn and the Fibonacci numbers.
58. Find closed forms for  and . What do you deduce about the quantity ?
59. Prove that if m and n are positive integers, there exists an integer x such that Fx ≡ m (mod 3n).
60. Find all positive integers n such that either Fn + 1 or Fn - 1 is a prime number.
61. Prove the identity



What is ?
62. Let An = ϕn + ϕ-n and Bn = ϕn - ϕ-n.
a. Find constants α and β such that An = αAn-1 + βAn-2 and Bn = αBn-1 + βBn-2 for all n ≥ 0.
b. Express An and Bn in terms of Fn and Ln (see exercise 28).
c. Prove that .
d. Find a closed form for .
Bogus problems


Bonus problems
63. How many permutations π1π2 . . . πn of {1, 2, . . . , n} have exactly k indices j such that
a. πi < πj for all i < j? (Such j are called "left-to-right maxima.")
b. πj > j? (Such j are called "excedances.")
64. What is the denominator of , when this fraction is reduced to lowest terms?
65. Prove the identity



66. What is , the nth alternating row sum of Euler's triangle?
67. Prove that



68. Show that , and find a closed form for .
69. Find a closed form for .
70. Show that the complex harmonic numbers of exercise 22 have the power series expansion .
71. Prove that the generalized factorial of equation (5.83) can be written



by considering the limit as n → ∞ of the first n factors of this infinite product. Show that  is related to the general harmonic numbers of exercise 22.
72. Prove that the tangent function has the power series (6.92), and find the corresponding series for z/sin z and ln ((tan z)/z).
73. Prove that z cot z is equal to



for all integers n ≥ 1, and show that the limit of the kth summand is 2z2/(z2 - k2π2) for fixed k as n → ∞.
74. Find a relation between the numbers Tn(1) and the coefficients of 1/cos z.
75. Prove that the tangent numbers and the coefficients of 1/cos z appear at the edges of the infinite triangle that begins as follows:
10       11      1      00      1      2      25      5      4      2      00     5     10     14     16     1661     61     56     46     32     16     0
Each row contains partial sums of the previous row, going alternately left-to-right and right-to-left. Hint: Consider the coefficients of the power series (sin z + cos z)/ cos(w + z).
76. Find a closed form for the sum



and show that it is zero when n is even.
77. When m and n are integers, n ≥ 0, the value of σn(m) is given by (6.48) if m < 0, by (6.49) if m > n, and by (6.101) if m = 0. Show that in the remaining cases we have



78. Prove the following relation that connects Stirling numbers, Bernoulli numbers, and Catalan numbers:



79. Show that the four chessboard pieces of the 64 = 65 paradox can also be reassembled to prove that 64 = 63.
80. A sequence defined by the recurrence A1 = x, A2 = y, and An = An-1 + An-2 has Am = 1000000 for some m. What positive integers x and y make m as large as possible?
81. The text describes a way to change a formula involving Fn±k to a formula that involves Fn and Fn+1 only. Therefore it's natural to wonder if two such "reduced" formulas can be equal when they aren't identical in form. Let P(x, y) be a polynomial in x and y with integer coefficients. Find a necessary and sufficient condition that P(Fn+1, Fn) = 0 for all n ≥ 0.
82. Explain how to add positive integers, working entirely in the Fibonacci number system.
83. Is it possible that a sequence An satisfying the Fibonacci recurrence An = An-1 + An-2 can contain no prime numbers, if A0 and A1 are relatively prime?
84. Let m and n be odd, positive integers. Find closed forms for



Hint: The sums in exercise 62 are  and .
85. Characterize all N such that the Fibonacci residues Fn mod N for n ≥ 0 form the complete set {0, 1, . . . , N - 1}. (See exercise 59.)
86. Let C1, C2, . . . be a sequence of nonzero integers such that
gcd(Cm, Cn) = Cgcd(m,n)
for all positive integers m and n. Prove that the generalized binomial coefficients



are all integers. (In particular, the "Fibonomial coefficients" formed in this way from Fibonacci numbers are integers, by (6.111).)
87. Show that continuant polynomials appear in the matrix product



and in the determinant



88. Generalizing (6.146), find a continued fraction related to the generating function ∑n≥1 znα, when α is any positive irrational number.
89. Let α be an irrational number in (0 . . 1) and let a1, a2, a3, . . . be the partial quotients in its continued fraction representation. Show that |D(α, n)| < 2 when n = K(a1, . . . , am), where D is the discrepancy defined in Chapter 3.
90. Let Qn be the largest denominator on level n of the Stern-Brocot tree. (Thus Q0, Q1, Q2, Q3, Q4, . . . = 1, 2, 3, 5, 8, . . . according to the diagram in Chapter 4.) Prove that Qn = Fn+2.


Research problems
91. What is the best way to extend the definition of  to arbitrary real values of n and k?
92. Let Hn be written in lowest terms as an/bn, as in exercise 52.
a. Are there infinitely many n with p\an, for some fixed prime p?
b. Are there infinitely many n with bn = lcm(1, 2, . . . , n)? (Two such values are n = 250 and n = 1000.)
93. Prove that γ and eγ are irrational.
94. Develop a general theory of the solutions to the two-parameter recurrence



assuming that  when n < 0 or k < 0. (Binomial coefficients, Stirling numbers, Eulerian numbers, and the sequences of exercises 17 and 31 are special cases.) What special values (α, β, γ, α′, β′, γ′) yield "fundamental solutions" in terms of which the general solution can be expressed?
95. Find an efficient way to extend the Gosper-Zeilberger algorithm from hypergeometric terms to terms that may involve Stirling numbers.











7. Generating Functions
The most powerful way to deal with sequences of numbers, as far as anybody knows, is to manipulate infinite series that "generate" those sequences. We've learned a lot of sequences and we've seen a few generating functions; now we're ready to explore generating functions in depth, and to see how remarkably useful they are.

7.1 Domino Theory and Change
Generating functions are important enough, and for many of us new enough, to justify a relaxed approach as we begin to look at them more closely. So let's start this chapter with some fun and games as we try to develop our intuitions about generating functions. We will study two applications of the ideas, one involving dominoes and the other involving coins.
How many ways Tn are there to completely cover a 2 × n rectangle with 2 × 1 dominoes? We assume that the dominoes are identical (either because they're face down, or because someone has rendered them indistinguishable, say by painting them all red); thus only their orientations—vertical or horizontal—matter, and we can imagine that we're working with domino-shaped tiles. For example, there are three tilings of a 2 × 3 rectangle, namely , , and ; so T3 = 3.
To find a closed form for general Tn we do our usual first thing, look at small cases. When n = 1 there's obviously just one tiling, ; and when n = 2 there are two,  and .
"Let me count the ways."
—E. B. Browning
How about when n = 0; how many tilings of a 2 × 0 rectangle are there? It's not immediately clear what this question means, but we've seen similar situations before: There is one permutation of zero objects (namely the empty permutation), so 0! = 1. There is one way to choose zero things from n things (namely to choose nothing), so . There is one way to partition the empty set into zero nonempty subsets, but there are no such ways to partition a nonempty set; so . By such reasoning we can conclude that there's just one way to tile a 2 × 0 rectangle with dominoes, namely to use no dominoes; therefore T0 = 1. (This spoils the simple pattern Tn = n that holds when n = 1, 2, and 3; but that pattern was probably doomed anyway, since T0 wants to be 1 according to the logic of the situation.) A proper understanding of the null case turns out to be useful whenever we want to solve an enumeration problem.
Let's look at one more small case, n = 4. There are two possibilities for tiling the left edge of the rectangle—we put either a vertical domino or two horizontal dominoes there. If we choose a vertical one, the partial solution is  and the remaining 2 × 3 rectangle can be covered in T3 ways. If we choose two horizontals, the partial solution  can be completed in T2 ways. Thus T4 = T3 + T2 = 5. (The five tilings are , , , , and .)
We now know the first five values of Tn:



These look suspiciously like the Fibonacci numbers, and it's not hard to see why: The reasoning we used to establish T4 = T3 + T2 easily generalizes to Tn = Tn-1 + Tn-2, for n ≥ 2. Thus we have the same recurrence here as for the Fibonacci numbers, except that the initial values T0 = 1 and T1 = 1 are a little different. But these initial values are the consecutive Fibonacci numbers F1 and F2, so the T's are just Fibonacci numbers shifted up one place:
Tn = Fn+1 ,    for n ≥ 0.
(We consider this to be a closed form for Tn, because the Fibonacci numbers are important enough to be considered "known." Also, Fn itself has a closed form (6.123) in terms of algebraic operations.) Notice that this equation confirms the wisdom of setting T0 = 1.
But what does all this have to do with generating functions? Well, we're about to get to that—there's another way to figure out what Tn is. This new way is based on a bold idea. Let's consider the "sum" of all possible 2 × n tilings, for all n ≥ 0, and call it T:
To boldly go where no tiling has gone before.



(The first term '|' on the right stands for the null tiling of a 2 × 0 rectangle.) This sum T represents lots of information. It's useful because it lets us prove things about T as a whole rather than forcing us to prove them (by induction) about its individual terms.
The terms of this sum stand for tilings, which are combinatorial objects. We won't be fussy about what's considered legal when infinitely many tilings are added together; everything can be made rigorous, but our goal right now is to expand our consciousness beyond conventional algebraic formulas.
We've added the patterns together, and we can also multiply them—by juxtaposition. For example, we can multiply the tilings  and  to get the new tiling . But notice that multiplication is not commutative; that is, the order of multiplication counts:  is different from .
Using this notion of multiplication it's not hard to see that the null tiling plays a special role—it is the multiplicative identity. For instance, .
Now we can use domino arithmetic to manipulate the infinite sum T:



Every valid tiling occurs exactly once in each right side, so what we've done is reasonable even though we're ignoring the cautions in Chapter 2 about "absolute convergence." The bottom line of this equation tells us that everything in T is either the null tiling, or is a vertical tile followed by something else in T, or is two horizontal tiles followed by something else in T.
I have a gut feeling that these sums must converge, as long as the dominoes are small enough.
So now let's try to solve the equation for T. Replacing the T on the left by |T and subtracting the last two terms on the right from both sides of the equation, we get



For a consistency check, here's an expanded version:



Every term in the top row, except the first, is cancelled by a term in either the second or third row, so our equation is correct.
So far it's been fairly easy to make combinatorial sense of the equations we've been working with. Now, however, to get a compact expression for T we cross a combinatorial divide. With a leap of algebraic faith we divide both sides of equation (7.3) by  to get



(Multiplication isn't commutative, so we're on the verge of cheating, by not distinguishing between left and right division. In our application it doesn't matter, because | commutes with everything. But let's not be picky, unless our wild ideas lead to paradoxes.)
The next step is to expand this fraction as a power series, using the rule



The null tiling |, which is the multiplicative identity for our combinatorial arithmetic, plays the part of 1, the usual multiplicative identity; and  plays z. So we get the expansion



This is T, but the tilings are arranged in a different order than we had before. Every tiling appears exactly once in this sum; for example,  appears in the expansion of .
We can get useful information from this infinite sum by compressing it down, ignoring details that are not of interest. For example, we can imagine that the patterns become unglued and that the individual dominoes commute with each other; then a term like  becomes , because it contains four verticals and six horizontals. Collecting like terms gives us the series
T = | +  + 2 + 2 + 3 + 2 2 + 4 + 32 2 + 4 + · · ·.
The  here represents the two terms of the old expansion,  and , that have one vertical and two horizontal dominoes; similarly  represents the three terms , , and . We're essentially treating  and  as ordinary (commutative) variables.
We can find a closed form for the coefficients in the commutative version of T by using the binomial theorem:



(The last step replaces k - j by m; this is legal because we have  when 0 ≤ k < j.) We conclude that  is the number of ways to tile a 2×(j+2m) rectangle with j vertical dominoes and 2m horizontal dominoes. For example, we recently looked at the 2 × 10 tiling , which involves four verticals and six horizontals; there are  such tilings in all, so one of the terms in the commutative version of T is .
We can suppress even more detail by ignoring the orientation of the dominoes. Suppose we don't care about the horizontal/vertical breakdown; we only want to know about the total number of 2 × n tilings. (This, in fact, is the number Tn we started out trying to discover.) We can collect the necessary information by simply substituting a single quantity, z, for  and . And we might as well also replace | by 1, getting
Now I'm disoriented.



This is the generating function (6.117) for Fibonacci numbers, except for a missing factor of z in the numerator; so we conclude that the coefficient of zn in T is Fn+1.
The compact representations , , and 1/(1-z-z2) that we have deduced for T are called generating functions, because they generate the coefficients of interest.
Incidentally, our derivation implies that the number of 2 × n domino tilings with exactly m pairs of horizontal dominoes is . (This follows because there are j = n - 2m vertical dominoes, hence there are



ways to do the tiling according to our formula.) We observed in Chapter 6 that  is the number of Morse code sequences of length n that contain m dashes; in fact, it's easy to see that 2×n domino tilings correspond directly to Morse code sequences. (The tiling  corresponds to ''.) Thus domino tilings are closely related to the continuant polynomials we studied in Chapter 6. It's a small world.
We have solved the Tn problem in two ways. The first way, guessing the answer and proving it by induction, was easier; the second way, using infinite sums of domino patterns and distilling out the coefficients of interest, was fancier. But did we use the second method only because it was amusing to play with dominoes as if they were algebraic variables? No; the real reason for introducing the second way was that the infinite-sum approach is a lot more powerful. The second method applies to many more problems, because it doesn't require us to make magic guesses.
Let's generalize up a notch, to a problem where guesswork will be beyond us. How many ways Un are there to tile a 3 × n rectangle with dominoes?
The first few cases of this problem tell us a little: The null tiling gives U0 = 1. There is no valid tiling when n = 1, since a 2 × 1 domino doesn't fill a 3 × 1 rectangle, and since there isn't room for two. The next case, n = 2, can easily be done by hand; there are three tilings, , , and , so U2 = 3. (Come to think of it we already knew this, because the previous problem told us that T3 = 3; the number of ways to tile a 3 × 2 rectangle is the same as the number to tile a 2 × 3.) When n = 3, as when n = 1, there are no tilings. We can convince ourselves of this either by making a quick exhaustive search or by looking at the problem from a higher level: The area of a 3 × 3 rectangle is odd, so we can't possibly tile it with dominoes whose area is even. (The same argument obviously applies to any odd n.) Finally, when n = 4 there seem to be about a dozen tilings; it's difficult to be sure about the exact number without spending a lot of time to guarantee that the list is complete.
So let's try the infinite-sum approach that worked last time:



Every non-null tiling begins with either  or  or ; but unfortunately the first two of these three possibilities don't simply factor out and leave us with U again. The sum of all terms in U that begin with  can, however, be written as  V, where



is the sum of all domino tilings of a mutilated 3 × n rectangle that has its lower left corner missing. Similarly, the terms of U that begin with  can be written  Λ, where



consists of all rectangular tilings lacking their upper left corner. The series Λ is a mirror image of V. These factorizations allow us to write



And we can factor V and Λ as well, because such tilings can begin in only two ways:



Now we have three equations in three unknowns (U, V, and Λ). We can solve them by first solving for V and Λ in terms of U, then plugging the results into the equation for U:



And the final equation can be solved for U, giving the compact formula



This expression defines the infinite sum U, just as (7.4) defines T.
The next step is to go commutative. Everything simplifies beautifully when we detach all the dominoes and use only powers of  and :
I learned in another class about "regular expressions." If I'm not mistaken, we can write



in the language of regular expressions; so there must be some connection between regular expressions and generating functions.



(This derivation deserves careful scrutiny. The last step uses the formula , identity (5.56).) Let's take a good look at the bottom line to see what it tells us. First, it says that every 3 × n tiling uses an even number of vertical dominoes. Moreover, if there are 2k verticals, there must be at least k horizontals, and the total number of horizontals must be k + 3m for some m ≥ 0. Finally, the number of possible tilings with 2k verticals and k + 3m horizontals is exactly .
We now are able to analyze the 3×4 tilings that left us doubtful when we began looking at the 3 × n problem. When n = 4 the total area is 12, so we need six dominoes altogether. There are 2k verticals and k + 3m horizontals, for some k and m; hence 2k + k + 3m = 6. In other words, k + m = 2. If we use no verticals, then k = 0 and m = 2; the number of possibilities is . (This accounts for the tiling .) If we use two verticals, then k = 1 and m = 1; there are  such tilings. And if we use four verticals, then k = 2 and m = 0; there are  such tilings, making a total of U4 = 11. In general if n is even, this reasoning shows that , hence  and the total number of 3 × n tilings is



As before, we can also substitute z for both  and , getting a generating function that doesn't discriminate between dominoes of particular persuasions. The result is



If we expand this quotient into a power series, we get
U = 1 + U2 z3 + U4 z6 + U6 z9 + U8 z12 + · · · ,
a generating function for the numbers Un. (There's a curious mismatch between subscripts and exponents in this formula, but it is easily explained. The coefficient of z9, for example, is U6, which counts the tilings of a 3 × 6 rectangle. This is what we want, because every such tiling contains nine dominoes.)
We could proceed to analyze (7.10) and get a closed form for the coefficients, but it's better to save that for later in the chapter after we've gotten more experience. So let's divest ourselves of dominoes for the moment and proceed to the next advertised problem, "change."
How many ways are there to pay 50 cents? We assume that the payment must be made with pennies , nickels , dimes , quarters , and half-dollars . George Pólya [298] popularized this problem by showing that it can be solved with generating functions in an instructive way.
Ah yes, I remember when we had half-dollars.
Let's set up infinite sums that represent all possible ways to give change, just as we tackled the domino problems by working with infinite sums that represent all possible domino patterns. It's simplest to start by working with fewer varieties of coins, so let's suppose first that we have nothing but pennies. The sum of all ways to leave some number of pennies (but just pennies) in change can be written



The first term stands for the way to leave no pennies, the second term stands for one penny, then two pennies, three pennies, and so on. Now if we're allowed to use both pennies and nickels, the sum of all possible ways is



since each payment has a certain number of nickels chosen from the first factor and a certain number of pennies chosen from P. (Notice that N is not the sum  , because such a sum includes many types of payment more than once. For example, the term  treats  and  as if they were different, but we want to list each set of coins only once without respect to order.)
Similarly, if dimes are permitted as well, we get the infinite sum



which includes terms like  when it is expanded in full. Each of these terms is a different way to make change. Adding quarters and then half-dollars to the realm of possibilities gives
Coins of the realm.



Our problem is to find the number of terms in C worth exactly 50¢.
A simple trick solves this problem nicely: We can replace  by z,  by z5,  by z10,  by z25, and  by z50. Then each term is replaced by zn, where n is the monetary value of the original term. For example, the term  becomes z50+10+5+5+1 = z71. The four ways of paying 13 cents, namely , , , and , each reduce to z13; hence the coefficient of z13 will be 4 after the z-substitutions are made.
Let Pn, Nn, Dn, Qn, and Cn be the numbers of ways to pay n cents when we're allowed to use coins that are worth at most 1, 5, 10, 25, and 50 cents, respectively. Our analysis tells us that these are the coefficients of zn in the respective power series
 P = 1 + z + z2 + z3 + z4 + · · · ,N = (1 + z5 + z10 + z15 + z20 + · · ·) P ,D = (1 + z10 + z20 + z30 + z40 + · · ·) N ,Q = (1 + z25 + z50 + z75 + z100 + · · ·) D ,C = (1 + z50 + z100 + z150 + z200 + · · ·) Q .
How many pennies are there, really? If n is greater than, say, 1010, I bet that Pn = 0 in the "real world."
Obviously Pn = 1 for all n ≥ 0. And a little thought proves that we have Nn = n/5 + 1: To make n cents out of pennies and nickels, we must choose either 0 or 1 or . . . or n/5 nickels, after which there's only one way to supply the requisite number of pennies. Thus Pn and Nn are simple; but the values of Dn, Qn, and Cn are increasingly more complicated.
One way to deal with these formulas is to realize that 1 + zm + z2m + · · · is just 1/(1 - zm). Thus we can write
 P = 1/(1 - z) ,N = P/(1 - z5) ,D = N/(1 - z10) ,Q = D/(1 - z25) ,C = Q/(1 - z50) .
Multiplying by the denominators, we have
    (1 - z) P = 1 ,  (1 - z5) N = P ,(1 - z10) D = N ,(1 - z25) Q = D ,(1 - z50) C = Q .
Now we can equate coefficients of zn in these equations, getting recurrence relations from which the desired coefficients can quickly be computed:
 Pn = Pn-1 + [n = 0] ,Nn = Nn-5 + Pn ,Dn = Dn-10 + Nn ,Qn = Qn-25 + Dn ,Cn = Cn-50 + Qn .
For example, the coefficient of zn in D = (1 - z25)Q is equal to Qn - Qn-25; so we must have Qn - Qn-25 = Dn, as claimed.
We could unfold these recurrences and find, for example, that Qn = Dn+Dn-25+Dn-50+Dn-75+ · · · , stopping when the subscripts get negative. But the non-iterated form is convenient because each coefficient is computed with just one addition, as in Pascal's triangle.
Let's use the recurrences to find C50. First, C50 = C0 + Q50; so we want to know Q50. Then Q50 = Q25 + D50, and Q25 = Q0 + D25; so we also want to know D50 and D25. These Dn depend in turn on D40, D30, D20, D15, D10, D5, and on N50, N45, . . . , N5. A simple calculation therefore suffices to determine all the necessary coefficients:



The final value in the table gives us our answer, C50: There are exactly 50 ways to leave a 50-cent tip.
(Not counting the option of charging the tip to a credit card.)
How about a closed form for Cn? Multiplying the equations together gives us the compact expression



but it's not obvious how to get from here to the coefficient of zn. Fortunately there is a way; we'll return to this problem later in the chapter.
More elegant formulas arise if we consider the problem of giving change when we live in a land that mints coins of every positive integer denomination ( ) instead of just the five we allowed before. The corresponding generating function is an infinite product of fractions,



and the coefficient of zn when these factors are fully multiplied out is called p(n), the number of partitions of n. A partition of n is a representation of n as a sum of positive integers, disregarding order. For example, there are seven different partitions of 5, namely
5 = 4+1 = 3+2 = 3+1+1 = 2+2+1 = 2+1+1+1 = 1+1+1+1+1 ;
hence p(5) = 7. (Also p(2) = 2, p(3) = 3, p(4) = 5, and p(6) = 11; it begins to look as if p(n) is always a prime number. But p(7) = 15, spoiling the pattern.) There is no closed form for p(n), but the theory of partitions is a fascinating branch of mathematics in which many remarkable discoveries have been made. For example, Ramanujan proved that p(5n + 4) ≡ 0 (mod 5), p(7n + 5) ≡ 0 (mod 7), and p(11n + 6) ≡ 0 (mod 11), by making ingenious transformations of generating functions (see Andrews [11, Chapter 10]).


7.2 Basic Maneuvers
Now let's look more closely at some of the techniques that make power series powerful.
First a few words about terminology and notation. Our generic generating function has the form



and we say that G(z), or G for short, is the generating function for the sequence g0, g1, g2, . . ., which we also call gn. The coefficient gn of zn in G(z) is often denoted [zn] G(z), as in Section 5.4.
The sum in (7.12) runs over all n ≥ 0, but we often find it more convenient to extend the sum over all integers n. We can do this by simply regarding g-1 = g-2 = · · · = 0. In such cases we might still talk about the sequence g0, g1, g2, . . ., as if the gn's didn't exist for negative n.
Two kinds of "closed forms" come up when we work with generating functions. We might have a closed form for G(z), expressed in terms of z; or we might have a closed form for gn, expressed in terms of n. For example, the generating function for Fibonacci numbers has the closed form z/(1 - z - z2); the Fibonacci numbers themselves have the closed form . The context will explain what kind of closed form is meant.
Now a few words about perspective. The generating function G(z) appears to be two different entities, depending on how we view it. Sometimes it is a function of a complex variable z, satisfying all the standard properties proved in calculus books. And sometimes it is simply a formal power series, with z acting as a placeholder. In the previous section, for example, we used the second interpretation; we saw several examples in which z was substituted for some feature of a combinatorial object in a "sum" of such objects. The coefficient of zn was then the number of combinatorial objects having n occurrences of that feature.
If physicists can get away with viewing light sometimes as a wave and sometimes as a particle, mathematicians should be able to view generating functions in two different ways.
When we view G(z) as a function of a complex variable, its convergence becomes an issue. We said in Chapter 2 that the infinite series ∑n≥0 gnzn converges (absolutely) if and only if there's a bounding constant A such that the finite sums ∑0≤n≤N |gnzn| never exceed A, for any N. Therefore it's easy to see that if ∑n≥0 gnzn converges for some value z = z0, it also converges for all z with |z| < |z0|. Furthermore, we must have ; hence, in the notation of Chapter 9, gn = O(|1/z0|n) if there is convergence at z0. And conversely if gn = O(Mn), the series ∑n≥0 gnzn converges for all |z| < 1/M. These are the basic facts about convergence of power series.
But for our purposes convergence is usually a red herring, unless we're trying to study the asymptotic behavior of the coefficients. Nearly every operation we perform on generating functions can be justified rigorously as an operation on formal power series, and such operations are legal even when the series don't converge. (The relevant theory can be found, for example, in Bell [23], Niven [282], and Henrici [182, Chapter 1].)
Furthermore, even if we throw all caution to the winds and derive formulas without any rigorous justification, we generally can take the results of our derivation and prove them by induction. For example, the generating function for the Fibonacci numbers converges only when |z| < 1/ϕ ≈ 0.618, but we didn't need to know that when we proved the formula . The latter formula, once discovered, can be verified directly, if we don't trust the theory of formal power series. Therefore we'll ignore questions of convergence in this chapter; it's more a hindrance than a help.
Even if we remove the tags from our mattresses.
So much for perspective. Next we look at our main tools for reshaping generating functions—adding, shifting, changing variables, differentiating, integrating, and multiplying. In what follows we assume that, unless stated otherwise, F(z) and G(z) are the generating functions for the sequences fn and gn. We also assume that the fn's and gn's are zero for negative n, since this saves us some bickering with the limits of summation.
It's pretty obvious what happens when we add constant multiples of F and G together:



This gives us the generating function for the sequence αfn + βgn.
Shifting a generating function isn't much harder. To shift G(z) right by m places, that is, to form the generating function for the sequence 0, . . . , 0, g0, g1, . . . = gn-m with m leading 0's, we simply multiply by zm:



This is the operation we used (twice), along with addition, to deduce the equation (1 - z - z2)F(z) = z on our way to finding a closed form for the Fibonacci numbers in Chapter 6.
And to shift G(z) left m places—that is, to form the generating function for the sequence gm, gm+1, gm+2, . . . = gn+m with the first m elements discarded—we subtract off the first m terms and then divide by zm:



(We can't extend this last sum over all n unless g0 = · · · = gm-1 = 0.)
Replacing the z by a constant multiple is another of our tricks:



this yields the generating function for the sequence cngn. The special case c = -1 is particularly useful.
Often we want to bring down a factor of n into the coefficient. Differentiation is what lets us do that:
I fear d generating-function dz's.



Shifting this right one place gives us a form that's sometimes more useful,



This is the generating function for the sequence ngn. Repeated differentiation would allow us to multiply gn by any desired polynomial in n.
Integration, the inverse operation, lets us divide the terms by n:



(Notice that the constant term is zero.) If we want the generating function for gn/n instead of gn-1/n, we should first shift left one place, replacing G(t) by (G(t) - g0)/t in the integral.
Finally, here's how we multiply generating functions together:



As we observed in Chapter 5, this gives the generating function for the sequence hn, the convolution of fn and gn. The sum hn = ∑k fkgn-k can also be written , because fk = 0 when k < 0 and gn-k = 0 when k > n. Multiplication/convolution is a little more complicated than the other operations, but it's very useful—so useful that we will spend all of Section 7.5 below looking at examples of it.
Multiplication has several special cases that are worth considering as operations in themselves. We've already seen one of these: When F(z) = zm we get the shifting operation (7.14). In that case the sum hn becomes the single term gn-m, because all fk's are 0 except for fm = 1.


Table 334 Generating function manipulations.



Another useful special case arises when F(z) is the familiar function 1/(1 - z) = 1 + z + z2 + · · · ; then all fk's (for k ≥ 0) are 1 and we have the important formula



Multiplying a generating function by 1/(1-z) gives us the generating function for the cumulative sums of the original sequence.
Table 334 summarizes the operations we've discussed so far. To use all these manipulations effectively it helps to have a healthy repertoire of generating functions in stock. Table 335 lists the simplest ones; we can use those to get started and to solve quite a few problems.
Each of the generating functions in Table 335 is important enough to be memorized. Many of them are special cases of the others, and many of


Table 335 Simple sequences and their generating functions.




Hint: If the sequence consists of binomial coefficients, its generating function usually involves a binomial, 1 ± z.
them can be derived quickly from the others by using the basic operations of Table 334; therefore the memory work isn't very hard.
For example, let's consider the sequence 1, 2, 3, 4, . . ., whose generating function 1/(1 - z)2 is often useful. This generating function appears near the middle of Table 335, and it's also the special case m = 1 of , which appears further down; it's also the special case c = 2 of the closely related sequence . We can derive it from the generating function for 1, 1, 1, 1, . . . by taking cumulative sums as in (7.21); that is, by dividing 1/(1-z) by (1-z). Or we can derive it from 1, 1, 1, 1, . . . by differentiation, using (7.17).
OK, OK, I'm convinced already.
The sequence 1, 0, 1, 0, . . . is another one whose generating function can be obtained in many ways. We can obviously derive the formula ∑n z2n = 1/(1 - z2) by substituting z2 for z in the identity ∑n zn = 1/(1 - z); we can also apply cumulative summation to the sequence 1, -1, 1, -1, . . ., whose generating function is 1/(1 + z), getting 1/(1 + z)(1 - z) = 1/(1 - z2). And there's also a third way, which is based on a general method for extracting the even-numbered terms g0, 0, g2, 0, g4, 0, . . . of any given sequence: If we add G(-z) to G(+z) we get



therefore



The odd-numbered terms can be extracted in a similar way,



In the special case where gn = 1 and G(z) = 1/(1-z), the generating function for 1, 0, 1, 0, . . . is .
Let's try this extraction trick on the generating function for Fibonacci numbers. We know that ∑n Fnzn = z/(1 - z - z2); hence



This generates the sequence F0, 0, F2, 0, F4, . . . ; hence the sequence of alternate F's, F0, F2, F4, F6, . . . = 0, 1, 3, 8, . . ., has a simple generating function:





7.3 Solving Recurrences
Now let's focus our attention on one of the most important uses of generating functions: the solution of recurrence relations.
Given a sequence gn that satisfies a given recurrence, we seek a closed form for gn in terms of n. A solution to this problem via generating functions proceeds in four steps that are almost mechanical enough to be programmed on a computer:
1 Write down a single equation that expresses gn in terms of other elements of the sequence. This equation should be valid for all integers n, assuming that g-1 = g-2 = · · · = 0.
2 Multiply both sides of the equation by zn and sum over all n. This gives, on the left, the sum ∑n gnzn, which is the generating function G(z). The right-hand side should be manipulated so that it becomes some other expression involving G(z).
3 Solve the resulting equation, getting a closed form for G(z).
4 Expand G(z) into a power series and read off the coefficient of zn; this is a closed form for gn.
This method works because the single function G(z) represents the entire sequence gn in such a way that many manipulations are possible.

Example 1: Fibonacci numbers revisited.
For example, let's rerun the derivation of Fibonacci numbers from Chapter 6. In that chapter we were feeling our way, learning a new method; now we can be more systematic. The given recurrence is
g0 = 0 ;     g1 = 1 ;gn = gn-1 + gn - 2 ,     for n ≥ 2.
We will find a closed form for gn by using the four steps above.
Step 1 tells us to write the recurrence as a "single equation" for gn. We could say



but this is cheating. Step 1 really asks for a formula that doesn't involve a case-by-case construction. The single equation
gn = gn-1 + gn-2
works for n ≥ 2, and it also holds when n ≤ 0 (because we have g0 = 0 and gnegative = 0). But when n = 1 we get 1 on the left and 0 on the right. Fortunately the problem is easy to fix, since we can add [n = 1] to the right; this adds 1 when n = 1, and it makes no change when n ≠ 1. So, we have
gn = gn-1 + gn-2 + [n = 1];
this is the equation called for in Step 1.
Step 2 now asks us to transform the equation for gn into an equation for G(z) = ∑n gnzn. The task is not difficult:



Step 3 is also simple in this case; we have



which of course comes as no surprise.
Step 4 is the clincher. We carried it out in Chapter 6 by having a sudden flash of inspiration; let's go more slowly now, so that we can get through Step 4 safely later, when we meet problems that are more difficult. What is



the coefficient of zn when z/(1 - z - z2) is expanded in a power series? More generally, if we are given any rational function



where P and Q are polynomials, what is the coefficient [zn] R(z)?
There's one kind of rational function whose coefficients are particularly nice, namely



(The case ρ = 1 appears in Table 335; we get the general formula by substituting ρz for z and multiplying by a.) A finite sum of functions like (7.25),



also has nice coefficients,



We will show that every rational function R(z) such that R(0) ≠ ∞ can be expressed in the form



where S(z) has the form (7.26) and T(z) is a polynomial. Therefore there is a closed form for the coefficients [zn] R(z). Finding S(z) and T(z) is equivalent to finding the "partial fraction expansion" of R(z).
Notice that S(z) = ∞ when z has the values 1/ρ1, . . . , 1/ρl. Therefore the numbers ρk that we need to find, if we're going to succeed in expressing R(z) in the desired form S(z) + T(z), must be the reciprocals of the numbers αk where Q(αk) = 0. (Recall that R(z) = P(z)/Q(z), where P and Q are polynomials; we have R(z) = ∞ only if Q(z) = 0.)
Suppose Q(z) has the form
Q(z) = q0 + q1z + · · · + qmzm ,    where q0 ≠ 0 and qm ≠ 0.
The "reflected" polynomial
QR(z) = q0zm + q1zm-1 + · · · + qm
has an important relation to Q(z):
QR(z) = q0(z - ρ1) . . . (z - ρm)
       Q(z) = q0(1 - ρ1z) . . . (1 - ρmz) .
Thus, the roots of QR are the reciprocals of the roots of Q, and vice versa. We can therefore find the numbers ρk we seek by factoring the reflected polynomial QR(z).
For example, in the Fibonacci case we have
Q(z) = 1 - z - z2 ;     QR(z) = z2 - z - 1 .
The roots of QR can be found by setting (a, b, c) = (1, -1, -1) in the quadratic formula ; we find that they are



Therefore  and .
Once we've found the ρ's, we can proceed to find the partial fraction expansion. It's simplest if all the roots are distinct, so let's consider that special case first. We might as well state and prove the general result formally:


Rational Expansion Theorem for Distinct Roots.
If R(z) = P(z)/Q(z), where Q(z) = q0(1 - ρ1z) . . . (1 - ρlz) and the numbers (ρ1, . . . , ρl) are distinct, and if P(z) is a polynomial of degree less than l, then



Proof: Let a1, . . . , al be the stated constants. Formula (7.29) holds if R(z) = P(z)/Q(z) is equal to



And we can prove that R(z) = S(z) by showing that the function T(z) = R(z) - S(z) is not infinite as z → 1/ρk . For this will show that the rational function T(z) is never infinite; hence T(z) must be a polynomial. We also can show that T(z) → 0 as z → ∞; hence T(z) must be zero.
Impress your parents by leaving the book open at this page.
Let αk = 1/ρk. To prove that limz→αk T(z) ≠ ∞, it suffices to show that limz→αk (z - αk)T(z) = 0, because T(z) is a rational function of z. Thus we want to show that



The right-hand limit equals limz→αk ak(z-αk)/(1-ρkz) = -ak/ρk, because (1 - ρkz) = -ρk(z - αk) and (z - αk)/(1 - ρjz) → 0 for j ≠ k. The left-hand limit is



by L'Hospital's rule. Thus the theorem is proved.
Returning to the Fibonacci example, we have P(z) = z and ; hence Q′(z) = -1 - 2z, and



According to (7.29), the coefficient of ϕn in [zn] R(z) is therefore ; the coefficient of  is . So the theorem tells us that , as in (6.123).
When Q(z) has repeated roots, the calculations become more difficult, but we can beef up the proof of the theorem and prove the following more general result:


General Expansion Theorem for Rational Generating Functions.
If R(z) = P(z)/Q(z), where Q(z) = q0(1 - ρ1z)d1 . . . (1 - ρlz)dl and the numbers (ρ1, . . . , ρl) are distinct, and if P(z) is a polynomial of degree less than d1 + · · · + dl, then



where each fk(n) is a polynomial of degree dk - 1 with leading coefficient



This can be proved by induction on max(d1, . . . , dl), using the fact that



is a rational function whose denominator polynomial is not divisible by (1 - ρkz)dk for any k.


Example 2: A more-or-less random recurrence.
Now that we've seen some general methods, we're ready to tackle new problems. Let's try to find a closed form for the recurrence



It's always a good idea to make a table of small cases first, and the recurrence lets us do that easily:



No closed form is evident, and this sequence isn't even listed in Sloane's Handbook [330]; so we need to go through the four-step process if we want to discover the solution.
Step 1 is easy, since we merely need to insert fudge factors to fix things when n < 2: The equation
gn = gn-1 + 2gn-2 + (-1)n[n ≥ 0] + [n = 1]
holds for all integers n. Now we can carry out Step 2:



N.B.: The upper index on ∑n=1 zn is not missing!
(Incidentally, we could also have used  instead of (-1)n[n ≥ 0], thereby getting  by the binomial theorem.) Step 3 is elementary algebra, which yields



And that leaves us with Step 4.
The squared factor in the denominator is a bit troublesome, since we know that repeated roots are more complicated than distinct roots; but there it is. We have two roots, ρ1 = 2 and ρ2 = -1; the general expansion theorem (7.30) tells us that
gn = a12n + (a2n + c)(-1)n
for some constant c, where



(The second formula for ak in (7.31) is easier to use than the first one when the denominator has nice factors. We simply substitute z = 1/ρk everywhere in R(z), except in the factor where this gives zero, and divide by (dk - 1)!; this gives the coefficient of .) Plugging in n = 0 tells us that the value of the remaining constant c had better be ; hence our answer is



It doesn't hurt to check the cases n = 1 and 2, just to be sure that we didn't foul up. Maybe we should even try n = 3, since this formula looks weird. But it's correct, all right.
Could we have discovered (7.33) by guesswork? Perhaps after tabulating a few more values we may have observed that gn+1 ≈ 2gn when n is large. And with chutzpah and luck we might even have been able to smoke out the constant . But it sure is simpler and more reliable to have generating functions as a tool.


Example 3: Mutually recursive sequences.
Sometimes we have two or more recurrences that depend on each other. Then we can form generating functions for both of them, and solve both by a simple extension of our four-step method.
For example, let's return to the problem of 3 × n domino tilings that we explored earlier this chapter. If we want to know only the total number of ways, Un, to cover a 3 × n rectangle with dominoes, without breaking this number down into vertical dominoes versus horizontal dominoes, we needn't go into as much detail as we did before. We can merely set up the recurrences



Here Vn is the number of ways to cover a 3 × n rectangle-minus-corner, using (3n - 1)/2 dominoes. These recurrences are easy to discover, if we consider the possible domino configurations at the rectangle's left edge, as before. Here are the values of Un and Vn for small n:



Let's find closed forms, in four steps. First (Step 1), we have
Un = 2Vn-1 + Un-2 + [n = 0] ,     Vn = Un-1 + Vn-2 ,
for all n. Hence (Step 2),
U(z) = 2zV(z) + z2U(z) + 1 ,     V(z) = zU(z) + z2V(z) .
Now (Step 3) we must solve two equations in two unknowns; but these are easy, since the second equation yields V(z) = zU(z)/(1 - z2); we find



(We had this formula for U(z) in (7.10), but with z3 instead of z2. In that derivation, n was the number of dominoes; now it's the width of the rectangle.)
The denominator 1 - 4z2 + z4 is a function of z2; this is what makes U2n+1 = 0 and V2n = 0, as they should be. We can take advantage of this nice property of z2 by retaining z2 when we factor the denominator: We need not take 1 - 4z2 + z4 all the way to a product of four factors (1 - ρkz), since two factors of the form (1 - ρkz2) will be enough to tell us the coefficients. In other words if we consider the generating function



we will have V(z) = zW(z2) and U(z) = (1 - z2)W(z2); hence V2n+1 = Wn and U2n = Wn - Wn-1. We save time and energy by working with the simpler function W(z).
The factors of 1 - 4z + z2 are  and , and they can also be written  and  because this polynomial is its own reflection. Thus it turns out that we have



This is the desired closed form for the number of 3 × n domino tilings.
Incidentally, we can simplify the formula for U2n by realizing that the second term always lies between 0 and 1. The number U2n is an integer, so we have



In fact, the other term  is extremely small when n is large, because . This needs to be taken into account if we try to use formula (7.38) in numerical calculations. For example, a fairly expensive name-brand hand calculator comes up with 413403.0005 when asked to compute . This is correct to nine significant figures; but the true value is slightly less than 413403, not slightly greater. Therefore it would be a mistake to take the ceiling of 413403.0005; the correct answer, U20 = 413403, is obtained by rounding to the nearest integer. Ceilings can be hazardous.
I've known slippery floors too.


Example 4: A closed form for change.
When we left the problem of making change, we had just calculated the number of ways to pay 50¢. Let's try now to count the number of ways there are to change a dollar, or a million dollars—still using only pennies, nickels, dimes, quarters, and halves.
The generating function derived earlier is



this is a rational function of z with a denominator of degree 91. Therefore we can decompose the denominator into 91 factors and come up with a 91-term "closed form" for Cn, the number of ways to give n cents in change. But that's too horrible to contemplate. Can't we do better than the general method suggests, in this particular case?
One ray of hope suggests itself immediately, when we notice that the denominator is almost a function of z5. The trick we just used to simplify the calculations by noting that 1 - 4z2 + z4 is a function of z2 can be applied to C(z), if we replace 1/(1 - z) by (1 + z + z2 + z3 + z4)/(1 - z5):



The compressed function Č(z) has a denominator whose degree is only 19, so it's much more tractable than the original. This new expression for C(z) shows us, incidentally, that C5n = C5n+1 = C5n+2 = C5n+3 = C5n+4; and indeed, this set of equations is obvious in retrospect: The number of ways to leave a 53¢ tip is the same as the number of ways to leave a 50¢ tip, because the number of pennies is predetermined modulo 5.
But Č(z) still doesn't have a really simple closed form based on the roots of the denominator. The easiest way to compute the coefficients of Č(z) is probably to recognize that each of the denominator factors is a divisor of 1 - z10. Hence we can write
Now we're also getting compressed reasoning.



The actual value of A(z), for the curious, is
(1 + z + ··· + z9)2(1 + z2 + ··· + z8)(1 + z5)
   = 1 + 2z + 4z2 + 6z3 + 9z4 + 13z5 + 18z6 + 24z7
     + 31z8 + 39z9 + 45z10 + 52z11 + 57z12 + 63z13 + 67z14 + 69z15
     + 69z16 + 67z17 + 63z18 + 57z19 + 52z20 + 45z21 + 39z22 + 31z23
     + 24z24 + 18z25 + 13z26 + 9z27 + 6z28 + 4z29 + 2z30 + z31 .
Finally, since , we can determine the coefficient Čn = [zn] Č(z) as follows, when n = 10q + r and 0 ≤ r < 10:



This gives ten cases, one for each value of r; but it's a pretty good closed form, compared with alternatives that involve powers of complex numbers.
For example, we can use this expression to deduce the value of C50q = Č10q. Then r = 0 and we have



The number of ways to change 50¢ is ; the number of ways to change $1 is ; and the number of ways to change $1,000,000 is





Example 5: A divergent series.
Now let's try to get a closed form for the numbers gn defined by
g0 = 1 ;
gn = ngn-1 ,     for n > 0.
Nowadays people are talking femto seconds.
After staring at this for a few nanoseconds we realize that gn is just n!; in fact, the method of summation factors described in Chapter 2 suggests this answer immediately. But let's try to solve the recurrence with generating functions, just to see what happens. (A powerful technique should be able to handle easy recurrences like this, as well as others that have answers we can't guess so easily.)
The equation
gn = ngn-1 + [n = 0]
holds for all n, and it leads to



To complete Step 2, we want to express ∑n ngn-1 zn in terms of G(z), and the basic maneuvers in Table 334 suggest that the derivative G′(z) = ∑n ngnzn-1 is somehow involved. So we steer toward that kind of sum:



Let's check this equation, using the values of gn for small n. Since
 G = 1 + z + 2z2 + 6z3 + 24z4 + · · · ,G′ =   1 + 4z + 18z2 + 96z3 + · · · ,
we have
z2G′ =       z2 + 4z3 + 18z4 + 96z5 + · · · ,  zG =   z + z2 + 2z3 + 6z4 + 24z5 + · · · ,   1 = 1 .
These three lines add up to G, so we're fine so far. Incidentally, we often find it convenient to write 'G' instead of 'G(z)'; the extra '(z)' just clutters up the formula when we aren't changing z.
Step 3 is next, and it's different from what we've done before because we have a differential equation to solve. But this is a differential equation that we can handle with the hypergeometric series techniques of Section 5.6; those techniques aren't too bad. (Readers who are unfamiliar with hypergeometrics needn't worry—this will be quick.)
"This will be quick." That's what the doctor said just before he stuck me with that needle. Come to think of it, "hypergeometric" sounds a lot like "hypodermic."
First we must get rid of the constant '1', so we take the derivative of both sides:
G′ = (z2G′ + zG + 1)′ = (2zG′ + z2G″) + (G + zG′)
                  = z2G″ + 3zG′ + G .
The theory in Chapter 5 tells us to rewrite this using the ϑ operator, and we know from exercise 6.13 that
ϑG = zG′ ,
    ϑ2G = z2G″ + zG′ .
Therefore the desired form of the differential equation is
ϑG = zϑ2G + 2zϑG + zG = z(ϑ + 1)2G .
According to (5.109), the solution with g0 = 1 is the hypergeometric series F(1, 1; ; z).
Step 3 was more than we bargained for; but now that we know what the function G is, Step 4 is easy—the hypergeometric definition (5.76) gives us the power series expansion:



We've confirmed the closed form we knew all along, gn = n!.
Notice that the technique gave the right answer even though G(z) diverges for all nonzero z. The sequence n! grows so fast, the terms |n! zn| approach ∞ as n → ∞, unless z = 0. This shows that formal power series can be manipulated algebraically without worrying about convergence.


Example 6: A recurrence that goes all the way back.
Let's close this section by applying generating functions to a problem in graph theory. A fan of order n is a graph on the vertices {0, 1, . . . , n} with 2n - 1 edges defined as follows: Vertex 0 is connected by an edge to each of the other n vertices, and vertex k is connected by an edge to vertex k + 1, for 1 ≤ k < n. Here, for example, is the fan of order 4, which has five vertices and seven edges.



The problem of interest: How many spanning trees fn are in such a graph? A spanning tree is a subgraph containing all the vertices, and containing enough edges to make the subgraph connected yet not so many that it has a cycle. It turns out that every spanning tree of a graph on n + 1 vertices has exactly n edges. With fewer than n edges the subgraph wouldn't be connected, and with more than n it would have a cycle; graph theory books prove this.
There are  ways to choose n edges from among the 2n - 1 present in a fan of order n, but these choices don't always yield a spanning tree. For instance the subgraph



has four edges but is not a spanning tree; it has a cycle from 0 to 4 to 3 to 0, and it has no connection between {1, 2} and the other vertices. We want to count how many of the  choices actually do yield spanning trees.
Let's look at some small cases. It's pretty easy to enumerate the spanning trees for n = 1, 2, and 3:



(We need not show the labels on the vertices, if we always draw vertex 0 at the left.) What about the case n = 0? At first it seems reasonable to set f0 = 1; but we'll take f0 = 0, because the existence of a fan of order 0 (which should have 2n - 1 = -1 edges) is dubious.
Our four-step procedure tells us to find a recurrence for fn that holds for all n. We can get a recurrence by observing how the topmost vertex (vertex n) is connected to the rest of the spanning tree. If it's not connected to vertex 0, it must be connected to vertex n - 1, since it must be connected to the rest of the graph. In this case, any of the fn-1 spanning trees for the remaining fan (on the vertices 0 through n - 1) will complete a spanning tree for the whole graph. Otherwise vertex n is connected to 0, and there's some number k ≤ n such that vertices n, n - 1, . . . , k are connected directly but the edge between k and k - 1 is not present. Then there can't be any edges between 0 and {n - 1, . . . , k}, or there would be a cycle. If k = 1, the spanning tree is therefore determined completely. And if k > 1, any of the fk-1 ways to produce a spanning tree on {0, 1, . . . , k-1} will yield a spanning tree on the whole graph. For example, here's what this analysis produces when n = 4:



The general equation, valid for n ≥ 1, is
fn = fn-1 + fn-1 + fn-2 + fn-3 + · · · + f1 + 1 .
(It almost seems as though the '1' on the end is f0 and we should have chosen f0 = 1; but we will doggedly stick with our choice.) A few changes suffice to make the equation valid for all integers n:



This is a recurrence that "goes all the way back" from fn-1 through all previous values, so it's different from the other recurrences we've seen so far in this chapter. We used a special method to get rid of a similar right-side sum in Chapter 2, when we solved the quicksort recurrence (2.12); namely, we subtracted one instance of the recurrence from another (fn+1 - fn). This trick would get rid of the ∑ now, as it did then; but we'll see that generating functions allow us to work directly with such sums. (And it's a good thing that they do, because we will be seeing much more complicated recurrences before long.)
Step 1 is finished; Step 2 is where we need to do a new thing:



The key trick here was to change zn to zk zn-k; this made it possible to express the value of the double sum in terms of F(z), as required in Step 2.
Now Step 3 is simple algebra, and we find



Those of us with a zest for memorization will recognize this as the generating function (7.24) for the even-numbered Fibonacci numbers. So, we needn't go through Step 4; we have found a somewhat surprising answer to the spans-of-fans problem:





7.4 Special Generating Functions
Step 4 of the four-step procedure becomes much easier if we know the coefficients of lots of different power series. The expansions in Table 335 are quite useful, as far as they go, but many other types of closed forms are possible. Therefore we ought to supplement that table with another one, which lists power series that correspond to the "special numbers" considered in Chapter 6.


Table 351 Generating functions for special numbers.




Table 351 is the database we need. The identities in this table are not difficult to prove, so we needn't dwell on them; this table is primarily for reference when we meet a new problem. But there's a nice proof of the first formula, (7.43), that deserves mention: We start with the identity



and differentiate it with respect to x. On the left, (1 - z)-x-1 is equal to e(x+1) ln(1/(1-z)), so d/dx contributes a factor of ln (1/(1 - z)). On the right, the numerator of  is (x + n) . . . (x + 1), and d/dx splits this into n terms whose sum is equivalent to multiplying  by



Replacing x by m gives (7.43). Notice that Hx+n - Hx is meaningful even when x is not an integer.
By the way, this method of differentiating a complicated product—leaving it as a product—is usually better than expressing the derivative as a sum. For example the right side of



would be a lot messier written out as a sum.
The general identities in Table 351 include many important special cases. For example, (7.43) simplifies to the generating function for Hn when m = 0:



This equation can also be derived in other ways; for example, we can take the power series for ln (1/(1 - z)) and divide it by 1 - z to get cumulative sums.
Identities (7.51) and (7.52) involve the respective ratios  and , which have the undefined form 0/0 when n ≥ m. However, there is a way to give them a proper meaning using the Stirling polynomials of (6.45), because we have






Thus, for example, the case m = 1 of (7.51) should not be regarded as the power series , but rather as



Identities (7.53), (7.54), (7.55), and (7.56) are "double generating functions" or "super generating functions" because they have the form G(w, z) = ∑m,n gm,nwmzn. The coefficient of wm is a generating function in the variable z; the coefficient of zn is a generating function in the variable w. Equation (7.56) can be put into the more symmetrical form





7.5 Convolutions
I always thought convolution was what happens to my brain when I try to do a proof.
The convolution of two given sequences f0, f1, . . . = fn and g0, g1, . . . = gn is the sequence f0g0, f0g1 + f1g0, . . . = ∑k fkgn-k. We have observed in Sections 5.4 and 7.2 that convolution of sequences corresponds to multiplication of their generating functions. This fact makes it easy to evaluate many sums that would otherwise be difficult to handle.


Example 1: A Fibonacci convolution.
For example, let's try to evaluate  in closed form. This is the convolution of Fn with itself, so the sum must be the coefficient of zn in F(z)2, where F(z) is the generating function for Fn. All we have to do is figure out the value of this coefficient.
The generating function F(z) is z/(1-z-z2), a quotient of polynomials; so the general expansion theorem for rational functions tells us that the answer can be obtained from a partial fraction representation. We can use the general expansion theorem (7.30) and grind away; or we can use the fact that



Instead of expressing the answer in terms of ϕ and , let's try for a closed form in terms of Fibonacci numbers. Recalling that , we have



Hence



and we have the answer we seek:



For example, when n = 3 this formula gives F0F3 + F1F2 + F2F1 + F3F0 = 0 + 1 + 1 + 0 = 2 on the left and (6F4 - 4F3)/5 = (18 - 8)/5 = 2 on the right.


Example 2: Harmonic convolutions.
The efficiency of a certain computer method called "samplesort" depends on the value of the sum



Exercise 5.58 obtains the value of this sum by a somewhat intricate double induction, using summation factors. It's much easier to realize that Tm,n is just the nth term in the convolution of  with . Both sequences have simple generating functions in Table 335:



Therefore, by (7.43),



In fact, there are many more sums that boil down to this same sort of convolution, because we have



for all r and s. Equating coefficients of zn gives the general identity



Because it's so harmonic.
This seems almost too good to be true. But it checks, at least when n = 2:



Special cases like s = 0 are as remarkable as the general case.
And there's more. We can use the convolution identity



to transpose Hr to the other side, since Hr is independent of k:



There's still more: If r and s are nonnegative integers l and m, we can replace  by  and  by ; then we can change k to k - l and n to n - m - l, getting



Even the special case l = m = 0 of this identity was difficult for us to handle in Chapter 2! (See (2.36).) We've come a long way.


Example 3: Convolutions of convolutions.
If we form the convolution of fn and gn, then convolve this with a third sequence hn, we get a sequence whose nth term is



The generating function of this three-fold convolution is, of course, the threefold product F(z)G(z)H(z). In a similar way, the m-fold convolution of a sequence gn with itself has nth term equal to



and its generating function is G(z)m.
We can apply these observations to the spans-of-fans problem considered earlier (Example 6 in Section 7.3). It turns out that there's another way to compute fn, the number of spanning trees of an n-fan, based on the configurations of tree edges between the vertices {1, 2, . . . , n}: The edge between vertex k and vertex k + 1 may or may not be selected for the tree; and each of the ways to select these edges connects up certain blocks of adjacent vertices. For example, when n = 10 we might connect vertices {1, 2}, {3}, {4, 5, 6, 7}, and {8, 9, 10}:
Concrete blocks.



How many spanning trees can we make, by adding additional edges to vertex 0? We need to connect 0 to each of the four blocks; and there are two ways to join 0 with {1, 2}, one way to join it with {3}, four ways with {4, 5, 6, 7}, and three ways with {8, 9, 10}, or 2 · 1 · 4 · 3 = 24 ways altogether. Summing over all possible ways to make blocks gives us the following expression for the total number of spanning trees:



For example, f4 = 4 + 3·1 + 2·2 + 1·3 + 2·1·1 + 1·2·1 + 1·1·2 + 1·1·1·1 = 21.
This is the sum of m-fold convolutions of the sequence 0, 1, 2, 3, . . ., for m = 1, 2, 3, . . . ; hence the generating function for fn is



where G(z) is the generating function for 0, 1, 2, 3, . . ., namely z/(1 - z)2. Consequently we have



as before. This approach to fn is more symmetrical and appealing than the complicated recurrence we had earlier.


Example 4: A convoluted recurrence.
Our next example is especially important. In fact, it's the "classic example" of why generating functions are useful in the solution of recurrences.
Suppose we have n + 1 variables x0, x1, . . . , xn whose product is to be computed by doing n multiplications. How many ways Cn are there to insert parentheses into the product x0· x1·. . .· xn so that the order of multiplication is completely specified? For example, when n = 2 there are two ways, x0· (x1· x2) and (x0 · x1) · x2. And when n = 3 there are five ways,
x0 · (x1 · (x2 · x3)) , x0 · ((x1 · x2) · x3) , (x0 · x1) · (x2 · x3) , (x0 · (x1 · x2)) · x3 , ((x0 · x1) · x2) · x3 .
Thus C2 = 2 , C3 = 5; we also have C1 = 1 and C0 = 1.
Let's use the four-step procedure of Section 7.3. What is a recurrence for the C's? The key observation is that there's exactly one ' · ' operation outside all of the parentheses, when n > 0; this is the final multiplication that ties everything together. If this ' · ' occurs between xk and xk+1, there are Ck ways to fully parenthesize x0 · . . . · xk, and there are Cn-k-1 ways to fully parenthesize xk+1 ·. . .· xn; hence
Cn = C0Cn-1 + C1Cn-2 + · · · + Cn-1C0 ,     if n > 0.
By now we recognize this expression as a convolution, and we know how to patch the formula so that it holds for all integers n:



Step 1 is now complete. Step 2 tells us to multiply by zn and sum:



Lo and behold, the convolution has become a product, in the generating-function world. Life is full of surprises.
The authors jest.
Step 3 is also easy. We solve for C(z) by the quadratic formula:



But should we choose the + sign or the - sign? Both choices yield a function that satisfies C(z) = zC(z)2 + 1, but only one of the choices is suitable for our problem. We might choose the + sign on the grounds that positive thinking is best; but we soon discover that this choice gives C(0) = ∞, contrary to the facts. (The correct function C(z) is supposed to have C(0) = C0 = 1.) Therefore we conclude that



Finally, Step 4. What is [zn] C(z)? The binomial theorem tells us that



hence, using (5.37),



The number of ways to parenthesize, Cn, is .
We anticipated this result in Chapter 5, when we introduced the sequence of Catalan numbers 1, 1, 2, 5, 14, . . . = Cn. This sequence arises in dozens of problems that seem at first to be unrelated to each other [46], because many situations have a recursive structure that corresponds to the convolution recurrence (7.66).
So the convoluted recurrence has led us to an oft-recurring convolution.
For example, let's consider the following problem: How many sequences a1, a2, . . . , a2n of +1's and -1's have the property that
a1 + a2 + · · · + a2n = 0
and have all their partial sums
a1,  a1 + a2, . . . , a1 + a2 + · · · + a2n
nonnegative? There must be n occurrences of +1 and n occurrences of -1. We can represent this problem graphically by plotting the sequence of partial sums  as a function of n: The five solutions for n = 3 are



These are "mountain ranges" of width 2n that can be drawn with line segments of the forms  and . It turns out that there are exactly Cn ways to do this, and the sequences can be related to the parenthesis problem in the following way: Put an extra pair of parentheses around the entire formula, so that there are n pairs of parentheses corresponding to the n multiplications. Now replace each ' · ' by +1 and each ')' by -1 and erase everything else. For example, the formula x0 · ((x1 · x2) · (x3 · x4)) corresponds to the sequence +1, +1, -1, +1, +1, -1, -1, -1 by this rule. The five ways to parenthesize x0 · x1 · x2 · x3 correspond to the five mountain ranges for n = 3 shown above.
Moreover, a slight reformulation of our sequence-counting problem leads to a surprisingly simple combinatorial solution that avoids the use of generating functions: How many sequences a0, a1, a2, . . . , a2n of +1's and -1's have the property that
a0 + a1 + a2 + · · · + a2n = 1 ,
when all the partial sums
a0,  a0 + a1,   a0 + a1 + a2,   . . . ,   a0 + a1 + · · · + a2n
are required to be positive? Clearly these are just the sequences of the previous problem, with the additional element a0 = +1 placed in front. But the sequences in the new problem can be enumerated by a simple counting argument, using a remarkable fact discovered by George Raney [302] in 1959: If x1, x2, . . . , xm is any sequence of integers whose sum is +1, exactly one of the cyclic shifts
x1, x2, . . . , xm, x2, . . . , xm, x1, . . . , xm, x1, . . . , xm-1
has all of its partial sums positive. For example, consider the sequence 3, -5, 2, -2, 3, 0. Its cyclic shifts are


3, -5, 2, -2, 3, 0
-2, 3, 0, 3, -5, 2


-5, 2, -2, 3, 0, 3
3, 0, 3, -5, 2, -2 √


2, -2, 3, 0, 3, -5
0, 3, -5, 2, -2, 3


and only the one that's checked has entirely positive partial sums.
Raney's lemma can be proved by a simple geometric argument. Let's extend the sequence periodically to get an infinite sequence
x1, x2, . . . , xm, x1, x2, . . . , xm, x1, x2, . . . ;
thus we let xm+k = xk for all k > 0. If we now plot the partial sums sn = x1 + · · · + xn as a function of n, the graph of sn has an "average slope" of 1/m, because sm+n = sn + 1. For example, the graph corresponding to our example sequence 3, -5, 2, -2, 3, 0, 3, -5, 2, . . . begins as follows:



The entire graph can be contained between two lines of slope 1/m, as shown; we have m = 6 in the illustration. In general these bounding lines touch the graph just once in each cycle of m points, since lines of slope 1/m hit points with integer coordinates only once per m units. The unique lower point of intersection is the only place in the cycle from which all partial sums will be positive, because every other point on the curve has an intersection point within m units to its right.
Ah, if stock prices would only continue to rise like this.
With Raney's lemma we can easily enumerate the sequences a0, . . . , a2n of +1's and -1's whose partial sums are entirely positive and whose total sum is +1. There are  sequences with n occurrences of -1 and n + 1 occurrences of +1, and Raney's lemma tells us that exactly 1/(2n + 1) of these sequences have all partial sums positive. (List all  of these sequences and all 2n + 1 of their cyclic shifts, in an N × (2n + 1) array. Each row contains exactly one solution. Each solution appears exactly once in each column. So there are N/(2n+1) distinct solutions in the array, each appearing (2n + 1) times.) The total number of sequences with positive partial sums is
(Attention, computer scientists: The partial sums in this problem represent the stack size as a function of time, when a product of n + 1 factors is evaluated, because each "push" operation changes the size by +1 and each multiplication changes it by -1.)





Example 5: A recurrence with m-fold convolution.
We can generalize the problem just considered by looking at sequences a0, . . . , amn of +1's and (1 - m)'s whose partial sums are all positive and whose total sum is +1. Such sequences can be called m-Raney sequences. If there are k occurrences of (1 - m) and mn + 1 - k occurrences of +1, we have
k(1 - m) + (mn + 1 - k) = 1 ,
(Attention, computer scientists: The stack interpretation now applies with respect to an m-ary operation, instead of the binary multiplication considered earlier.)
hence k = n. There are  sequences with n occurrences of (1 - m) and mn + 1 - n occurrences of +1, and Raney's lemma tells us that the number of such sequences with all partial sums positive is exactly



So this is the number of m-Raney sequences. Let's call it the Fuss-Catalan number , because the sequence  was first investigated by N. I. Fuss [135] in 1791 (many years before Catalan himself got into the act). The ordinary Catalan numbers are .
Now that we know the answer, (7.67), let's play "Jeopardy" and figure out a question that leads to it. In the case m = 2 the question was: "What numbers Cn satisfy the recurrence Cn = ∑k CkCn-1-k + [n = 0]?" We will try to find a similar question (a similar recurrence) in the general case.
The trivial sequence +1 of length 1 is clearly an m-Raney sequence. If we put the number (1-m) at the right of any m sequences that are m-Raney, we get an m-Raney sequence; the partial sums stay positive as they increase to +2, then +3, . . . , +m, and +1. Conversely, we can show that all m-Raney sequences a0, . . . , amn arise uniquely in this way, if n > 0: The last term amn must be (1 - m). The partial sums sj = a0 + · · · + aj-1 are positive for 1 ≤ j ≤ mn, and smn = m because smn + amn = 1. Let k1 be the largest index ≤ mn such that sk1 = 1; let k2 be largest such that sk2 = 2; and so on. Thus skj = j and sk > j, for kj < k ≤ mn and 1 ≤ j ≤ m. It follows that km = mn, and we can verify without difficulty that each of the subsequences a0, . . . , ak1-1, ak1, . . . , ak2-1, . . . , akm-1, . . . , akm-1 is an m-Raney sequence. We must have k1 = mn1 + 1, k2 - k1 = mn2 + 1, . . . , km - km-1 = mnm + 1, for some nonnegative integers n1, n2, . . . , nm.
Therefore  is the answer to the following two interesting questions: "What are the numbers  defined by the recurrence



for all integers n?" "If G(z) is a power series that satisfies



what is [zn] G(z)?"
Notice that these are not easy questions. In the ordinary Catalan case (m = 2), we solved (7.69) for G(z) and its coefficients by using the quadratic formula and the binomial theorem; but when m = 3, none of the standard techniques gives any clue about how to solve the cubic equation G = zG3 + 1. So it has turned out to be easier to answer this question before asking it.
Now, however, we know enough to ask even harder questions and deduce their answers. How about this one: "What is [zn] G(z)l, if l is a positive integer and if G(z) is the power series defined by (7.69)?" The argument we just gave can be used to show that [zn] G(z)l is the number of sequences of length mn + l with the following three properties:
• Each element is either +1 or (1 - m).
• The partial sums are all positive.
• The total sum is l.
For we get all such sequences in a unique way by putting together l sequences that have the m-Raney property. The number of ways to do this is



Raney proved a generalization of his lemma that tells us how to count such sequences: If x1, x2, . . . , xm is any sequence of integers with xj ≤ 1 for all j, and with x1 + x2 + · · · + xm = l > 0, then exactly l of the cyclic shifts
x1, x2, . . . , xm, x2, . . . , xm, x1, . . . , xm, x1, . . . , xm-1
have all partial sums positive.
For example, we can check this statement on the sequence -2, 1, -1, 0, 1, 1, -1, 1, 1, 1. The cyclic shifts are


-2, 1, -1, 0, 1, 1, -1, 1, 1, 1
 
1, -1, 1, 1, 1, -2, 1, -1, 0, 1


1, -1, 0, 1, 1, -1, 1, 1, 1, -2
 
-1, 1, 1, 1, -2, 1, -1, 0, 1, 1


-1, 0, 1, 1, -1, 1, 1, 1, -2, 1
 
1, 1, 1, -2, 1, -1, 0, 1, 1, -1  √


0, 1, 1, -1, 1, 1, 1, -2, 1, -1
 
1, 1, -2, 1, -1, 0, 1, 1, -1, 1


1, 1, -1, 1, 1, 1, -2, 1, -1, 0  √
 
1, -2, 1, -1, 0, 1, 1, -1, 1, 1


and only the two examples marked '√' have all partial sums positive. This generalized lemma is proved in exercise 13.
A sequence of +1's and (1 - m)'s that has length mn + l and total sum l must have exactly n occurrences of (1 - m). The generalized lemma tells us that l/(mn + l) of these  sequences have all partial sums positive; hence our tough question has a surprisingly simple answer:



for all integers l > 0.
Readers who haven't forgotten Chapter 5 might well be experiencing déjà vu: "That formula looks familiar; haven't we seen it before?" Yes, indeed; Lambert's equation (5.60) says that



Therefore the generating function G(z) in (7.69) must actually be the generalized binomial series . Sure enough, equation (5.59) says
m(z)1-m - m(z)-m = z ,
which is the same as
m(z) - 1 = zm(z)m .
Let's switch to the notation of Chapter 5, now that we know we're dealing with generalized binomials. Chapter 5 stated a bunch of identities without proof. We have now closed part of the gap by proving that the power series  defined by



has the remarkable property that



whenever t and r are positive integers.
Can we extend these results to arbitrary values of t and r? Yes; because the coefficients  are polynomials in t and r. The general rth power defined by



has coefficients that are polynomials in t and r; and those polynomials are equal to  for infinitely many values of t and r. So the two sequences of polynomials must be identically equal.
Chapter 5 also mentions the generalized exponential series



which is said in (5.60) to have an equally remarkable property:



We can prove this as a limiting case of the formulas for t(z), because it is not difficult to show that





7.6 Exponential GF's
Sometimes a sequence gn has a generating function whose properties are quite complicated, while the related sequence gn/n! has a generating function that's quite simple. In such cases we naturally prefer to work with gn/n! and then multiply by n! at the end. This trick works sufficiently often that we have a special name for it: We call the power series



the exponential generating function or "egf" of the sequence g0, g1, g2, . . .. This name arises because the exponential function ez is the egf of 1, 1, 1, . . ..
Many of the generating functions in Table 351 are actually egf's. For example, equation (7.50) says that  is the egf for the sequence . The ordinary generating function for this sequence is much more complicated (and also divergent).
Exponential generating functions have their own basic maneuvers, analogous to the operations we learned in Section 7.2. For example, if we multiply the egf of gn by z, we get



this is the egf of 0, g0, 2g1, . . . = ngn-1.
Differentiating the egf of g0, g1, g2, . . . with respect to z gives
Are we having fun yet?



this is the egf of g1, g2, . . .. Thus differentiation on egf's corresponds to the left-shift operation (G(z) - g0)/z on ordinary gf's. (We used this left-shift property of egf's when we studied hypergeometric series, (5.106).) Integration of an egf gives



this is a right shift, the egf of 0, g0, g1, . . ..
The most interesting operation on egf's, as on ordinary gf's, is multiplication. If  and  are egf's for fn and gn, then  is the egf for a sequence hn called the binomial convolution of fn and gn:



Binomial coefficients appear here because , hence



in other words, hn/n! is the ordinary convolution of fn/n! and gn/n!.
Binomial convolutions occur frequently in applications. For example, we defined the Bernoulli numbers in (6.79) by the implicit recurrence



this can be rewritten as a binomial convolution, if we substitute n for m + 1 and add the term Bn to both sides:



We can now relate this recurrence to power series (as promised in Chapter 6) by introducing the egf for Bernoulli numbers, . The left-hand side of (7.76) is the binomial convolution of Bn with the constant sequence 1, 1, 1, . . . ; hence the egf of the left-hand side is . The egf of the right-hand side is . Therefore we must have ; we have proved equation (6.81), which appears also in Table 351 as equation (7.44).
Now let's look again at a sum that has been popping up frequently in this book,



This time we will try to analyze the problem with generating functions, in hopes that it will suddenly become simpler. We will consider n to be fixed and m variable; thus our goal is to understand the coefficients of the power series



We know that the generating function for 1, k, k2, . . .  is



hence



by interchanging the order of summation. We can put this sum in closed form,



but we know nothing about expanding such a closed form in powers of z.
Exponential generating functions come to the rescue. The egf of our sequence S0(n), S1(n), S2(n), . . .  is



To get these coefficients Sm(n) we can use the egf for 1, k, k2, . . ., namely



and we have



And the latter sum is a geometric progression, so there's a closed form



Eureka! All we need to do is figure out the coefficients of this relatively simple function, and we'll know Sm(n), because Sm(n) = m![zm]Ŝ(z,n).
Here's where Bernoulli numbers come into the picture. We observed a moment ago that the egf for Bernoulli numbers is



hence we can write



The sum Sm(n) is m! times the coefficient of zm in this product. For example,



We have therefore derived the formula  for the umpteenth time, and this was the simplest derivation of all: In a few lines we have found the general behavior of Sm(n) for all m.
The general formula can be written



where Bm(x) is the Bernoulli polynomial defined by



Here's why: The Bernoulli polynomial is the binomial convolution of the sequence B0, B1, B2, . . .  with 1, x, x2, . . . ; hence the exponential generating function for B0(x), B1(x), B2(x), . . .  is the product of their egf's,



Equation (7.79) follows because the egf for 0, S0(n), 2S1(n), . . .  is, by (7.78),



Let's turn now to another problem for which egf's are just the thing: How many spanning trees are possible in the complete graph on n vertices {1, 2, . . . , n}? Let's call this number tn. The complete graph has  edges, one edge joining each pair of distinct vertices; so we're essentially looking for the total number of ways to connect up n given things by drawing n - 1 lines between them.
We have t1 = t2 = 1. Also t3 = 3, because a complete graph on three vertices is a fan of order 2; we know that f2 = 3. And there are sixteen spanning trees when n = 4:



Hence t4 = 16.
Our experience with the analogous problem for fans suggests that the best way to tackle this problem is to single out one vertex, and to look at the blocks or components that the spanning tree joins together when we ignore all edges that touch the special vertex. If the non-special vertices form m components of sizes k1, k2, . . . , km, then we can connect them to the special vertex in k1k2 . . . km ways. For example, in the case n = 4, we can consider the lower left vertex to be special. The top row of (7.82) shows 3t3 cases where the other three vertices are joined among themselves in t3 ways and then connected to the lower left in 3 ways. The bottom row shows 2·1 × t2t1 ×  solutions where the other three vertices are divided into components of sizes 2 and 1 in  ways; there's also the case  where the other three vertices are completely unconnected among themselves.
This line of reasoning leads to the recurrence



for all n > 1. Here's why: There are  ways to assign n-1 elements to a sequence of m components of respective sizes k1, k2, . . . , km; there are tk1 tk2 . . . tkm ways to connect up those individual components with spanning trees; there are k1k2 . . . km ways to connect vertex n to those components; and we divide by m! because we want to disregard the order of the components. For example, when n = 4 the recurrence says that



The recurrence for tn looks formidable at first, possibly even frightening; but it really isn't bad, only convoluted. We can define
un = n tn
and then everything simplifies considerably:



The inner sum is the coefficient of zn-1 in the egf (z), raised to the mth power; and we obtain the correct formula also when n = 1, if we add in the term (z)0 that corresponds to the case m = 0. So



for all n > 0, and we have the equation



Progress! Equation (7.84) is almost like
ℇ(z) = ez ℇ(z) ,
which defines the generalized exponential series ℇ(z) = ℇ1(z) in (5.59) and (7.71); indeed, we have
(z) = z ℇ (z) .
So we can read off the answer to our problem:



The complete graph on {1, 2, . . . , n} has exactly nn-2 spanning trees, for all n > 0.


7.7 Dirichlet Generating Functions
There are many other possible ways to generate a sequence from a series; any system of "kernel" functions Kn(z) such that



can be used, at least in principle. Ordinary generating functions use Kn(z) = zn, and exponential generating functions use Kn(z) = zn/n!; we could also try falling factorial powers zn, or binomial coefficients zn/n! = .
The most important alternative to gf's and egf's uses the kernel functions 1/nz; it is intended for sequences g1, g2, . . .  that begin with n = 1 instead of n = 0:



This is called a Dirichlet generating function (dgf), because the German mathematician Gustav Lejeune Dirichlet (1805-1859) made much of it.
For example, the dgf of the constant sequence 1, 1, 1, . . .  is



This is Riemann's zeta function, which we have also called the generalized harmonic number  when z > 1.
The product of Dirichlet generating functions corresponds to a special kind of convolution:



Thus  is the dgf of the sequence



For example, we know from (4.55) that ∑d\n μ(d) = [n = 1]; this is the Dirichlet convolution of the Möbius sequence μ(1), μ(2), μ(3), . . .  with 1, 1, 1, . . . , hence



In other words, the dgf of μ(1), μ(2), μ(3), . . .  is ζ(z)-1 .
Dirichlet generating functions are particularly valuable when the sequence g1, g2, . . .  is a multiplicative function, namely when
gmn = gm gn           for m ⊥ n.
In such cases the values of gn for all n are determined by the values of gn when n is a power of a prime, and we can factor the dgf into a product over primes:



If, for instance, we set gn = 1 for all n, we obtain a product representation of Riemann's zeta function:



The Möbius function has μ(p) = -1 and μ(pk) = 0 for k > 1, hence its dgf is



this agrees, of course, with (7.89) and (7.91). Euler's φ function has φ(pk) = pk - pk-1, hence its dgf has the factored form



We conclude that (z) = ζ(z - 1)/ζ(z).



Exercises

Warmups
1. An eccentric collector of 2 × n domino tilings pays $4 for each vertical domino and $1 for each horizontal domino. How many tilings are worth exactly $m by this criterion? For example, when m = 6 there are three solutions: , , and .
2. Give the generating function and the exponential generating function for the sequence 2, 5, 13, 35, . . . = 2n + 3n in closed form.
3. What is ∑n≥0 Hn/10n?
4. The general expansion theorem for rational functions P(z)/Q(z) is not completely general, because it restricts the degree of P to be less than the degree of Q. What happens if P has a larger degree than this?
5. Find a generating function S(z) such that





Basics
6. Show that the recurrence (7.32) can be solved by the repertoire method, without using generating functions.
7. Solve the recurrence



8. What is [zn] (ln(1 - z))2/(1 - z)m +1?
9. Use the result of the previous exercise to evaluate .
10. Set r = s = -1/2 in identity (7.62) and then remove all occurrences of 1/2 by using tricks like (5.36). What amazing identity do you deduce?
I deduce that Clark Kent is really Superman.
11. This problem, whose three parts are independent, gives practice in the manipulation of generating functions. We assume that A(z) = ∑n anzn, B(z) = ∑n bnzn, C(z) = ∑n cnzn, and that the coefficients are zero for negative n.
a. If cn = ∑j+2k≤n ajbk, express C in terms of A and B.
b. If , express A in terms of B.
c. If r is a real number and if , express A in terms of B; then use your formula to find coefficients fk(r) such that .
12. How many ways are there to put the numbers {1, 2, . . . , 2n} into a 2 × n array so that rows and columns are in increasing order from left to right and from top to bottom? For example, one solution when n = 5 is



13. Prove Raney's generalized lemma, which is stated just before (7.70).
14. Solve the recurrence



by using an exponential generating function.
15. The Bell number ϖn is the number of ways to partition n things into subsets. For example, ϖ3 = 5 because we can partition {1, 2, 3} in the following ways:
{1, 2, 3};   {1, 2}∪{3};   {1, 3}∪{2};   {1}∪{2, 3};   {1}∪{2}∪{3}.
Prove that , and use this recurrence to find a closed form for the exponential generating function P(z) = ∑n ϖnzn/n!.
16. Two sequences an and bn are related by the convolution formula



also a0 = 0 and b0 = 1. Prove that the corresponding generating functions satisfy ln  .
17. Show that the exponential generating function Ĝ(z) of a sequence is related to the ordinary generating function G(z) by the formula



if the integral exists.
18. Find the Dirichlet generating functions for the sequences
a. 
b. gn = ln n;
c. gn = [n is squarefree].
Express your answers in terms of the zeta function. (Squarefreeness is defined in exercise 4.13.)
19. Every power series F(z) = ∑n≥0 fnzn with f0 = 1 defines a sequence of polynomials fn(x) by the rule



where fn(1) = fn and fn(0) = [n = 0]. In general, fn(x) has degree n. Show that such polynomials always satisfy the convolution formulas
What do you mean, "in general"? If f1 = f2 = · · · = fm-1 = 0, the degree of fn(x) is at most n/m .



(The identities in Tables 202 and 272 are special cases of this trick.)
20. A power series G(z) is called differentiably finite if there exist finitely many polynomials P0(z), . . . , Pm(z), not all zero, such that
P0(z)G(z) + P1(z)G′(z) + · · · + Pm(z)G(m)(z) = 0 .
A sequence of numbers g0, g1, g2, . . . is called polynomially recursive if there exist finitely many polynomials p0(z), . . . , pm(z), not all zero, such that
p0(n)gn + p1(n)gn+1 + · · · + pm(n)gn+m = 0
for all integers n ≥ 0. Prove that a generating function is differentiably finite if and only if its sequence of coefficients is polynomially recursive.


Homework exercises
21. A robber holds up a bank and demands $500 in tens and twenties. He also demands to know the number of ways in which the cashier can give him the money. Find a generating function G(z) for which this number is [z500] G(z), and a more compact generating function (z) for which this number is [z50] (z). Determine the required number of ways by (a) using partial fractions; (b) using a method like (7.39).
Will he settle for 2 × n domino tilings?
22. Let P be the sum of all ways to "triangulate" polygons:



(The first term represents a degenerate polygon with only two vertices; every other term shows a polygon that has been divided into triangles. For example, a pentagon can be triangulated in five ways.) Define a "multiplication" operation AΔB on triangulated polygons A and B so that the equation
P = __ + P△P
is valid. Then replace each triangle by 'z'; what does this tell you about the number of ways to decompose an n-gon into triangles?
23. In how many ways can a 2 × 2 × n pillar be built out of 2 × 1 × 1 bricks?
At union rates, as many as you can afford, plus a few.
24. How many spanning trees are in an n-wheel (a graph with n "outer" vertices in a cycle, each connected to an (n + 1)st "hub" vertex), when n ≥ 3?
25. Let m ≥ 2 be an integer. What is a closed form for the generating function of the sequence n mod m, as a function of z and m? Use this generating function to express 'n mod m' in terms of the complex number ω = e2πi/m. (For example, when m = 2 we have ω = -1 and n mod .)
26. The second-order Fibonacci numbers  are defined by the recurrence



Express  in terms of the usual Fibonacci numbers Fn and Fn+1.
27. A 2 × n domino tiling can also be regarded as a way to draw n disjoint lines in a 2 × n array of points:



If we superimpose two such patterns, we get a set of cycles, since every point is touched by two lines. For example, if the lines above are combined with the lines



the result is



The same set of cycles is also obtained by combining



But we get a unique way to reconstruct the original patterns from the superimposed ones if we assign orientations to the vertical lines by using arrows that go alternately up/down/up/down/· · · in the first pattern and alternately down/up/down/up/· · · in the second. For example,



The number of such oriented cycle patterns must therefore be , and we should be able to prove this via algebra. Let Qn be the number of oriented 2 × n cycle patterns. Find a recurrence for Qn, solve it with generating functions, and deduce algebraically that  .
28. The coefficients of A(z) in (7.39) satisfy Ar+Ar+10+Ar+20+Ar+30 = 100 for 0 ≤ r < 10. Find a "simple" explanation for this.
29. What is the sum of Fibonacci products



30. If the generating function G(z) = 1/(1 - αz)(1 - βz) has the partial fraction decomposition a/(1-αz)+b/(1-βz), what is the partial fraction decomposition of G(z)n?
31. What function g(n) of the positive integer n satisfies the recurrence



where φ is Euler's totient function?
32. An arithmetic progression is an infinite set of integers
{an + b} = {b, a + b, 2a + b, 3a + b, . . . } .
A set of arithmetic progressions {a1n + b1}, . . . , {amn + bm} is called an exact cover if every nonnegative integer occurs in one and only one of the progressions. For example, the three progressions {2n}, {4n + 1}, {4n + 3} constitute an exact cover. Show that if {a1n + b1}, . . . , {amn + bm} is an exact cover such that 2 ≤ a1 ≤ · · · ≤ am, then am-1 = am. Hint: Use generating functions.


Exam problems
33. What is [wmzn] (ln(1 + z))/(1 - wz)?
34. Find a closed form for the generating function ∑n≥0 Gn(z)wn, if



(Here m is a fixed positive integer.)
35. Evaluate the sum ∑0<k<n 1/k(n - k) in two ways:
a. Expand the summand in partial fractions.
b. Treat the sum as a convolution and use generating functions.
36. Let A(z) be the generating function for a0, a1, a2, a3, . . . . Express ∑n an/mzn in terms of A, z, and m.
37. Let an be the number of ways to write the positive integer n as a sum of powers of 2, disregarding order. For example, a4 = 4, since 4 = 2 + 2 = 2 + 1 + 1 = 1 + 1 + 1 + 1. By convention we let a0 = 1. Let  be the cumulative sum of the first a's.
a. Make a table of the a's and b's up through n = 10. What amazing relation do you observe in your table? (Don't prove it yet.)
b. Express the generating function A(z) as an infinite product.
c. Use the expression from part (b) to prove the result of part (a).
38. Find a closed form for the double generating function



Generalize your answer to obtain, for fixed m ≥ 2, a closed form for



39. Given positive integers m and n, find closed forms for



(For example, when m = 2 and n = 3 the sums are 1·2 + 1·3 + 2·3 and 1·1+1·2+1·3+2·2+2·3+3·3.) Hint: What are the coefficients of zm in the generating functions (1 + a1z) . . . (1 + anz) and 1/(1 - a1z) . . . (1 - anz)?
40. Express  in closed form.
41. An up-down permutation of order n is an arrangement a1a2 . . . an of the integers {1, 2, . . . , n} that goes alternately up and down:
a1 < a2 > a3 < a4 > · · · .
For example, 35142 is an up-down permutation of order 5. If An denotes the number of up-down permutations of order n, show that the exponential generating function of An is (1 + sin z)/cos z.
42. A space probe has discovered that organic material on Mars has DNA composed of five symbols, denoted by (a, b, c, d, e), instead of the four components in earthling DNA. The four pairs cd, ce, ed, and ee never occur consecutively in a string of Martian DNA, but any string without forbidden pairs is possible. (Thus bbcda is forbidden but bbdca is OK.) How many Martian DNA strings of length n are possible? (When n = 2 the answer is 21, because the left and right ends of a string are distinguishable.)
43. The Newtonian generating function of a sequence gn is defined to be



Find a convolution formula that defines the relation between sequences fn, gn, and hn whose Newtonian generating functions are related by the equation . Try to make your formula as simple and symmetric as possible.
44. Let qn be the number of possible outcomes when n numbers {x1, . . . , xn} are compared with each other. For example, q3 = 13 because the possibilities are



Find a closed form for the egf (z) = Σn qnzn/n!. Also find sequences an, bn, cn such that



45. Evaluate ∑m,n>0[m ⊥ n]/m2n2.
46. Evaluate



in closed form. Hint: .
47. Show that the numbers Un and Vn of 3 × n domino tilings, as given in (7.34), are closely related to the fractions in the Stern-Brocot tree that converge to .
48. A certain sequence gn satisfies the recurrence
agn + bgn+1 + cgn+2 + d = 0 ,        integer n ≥ 0,
for some integers (a, b, c, d) with gcd(a, b, c, d) = 1. It also has the closed form
gn = α(1 + )n ,       integer n ≥ 0,
for some real number α between 0 and 1. Find a, b, c, d, and α.
49. This is a problem about powers and parity.
Kissinger, take note.
a. Consider the sequence a0, a1, a2, . . . = 2, 2, 6, . . . defined by the formula
an = (1 + )n + (1 - )n .
Find a simple recurrence relation that is satisfied by this sequence.
b. Prove that (1 + )n ≡ n (mod 2) for all integers n > 0.
c. Find a number α of the form , where p and q are positive integers, such that αn ≡ n (mod 2) for all integers n > 0.


Bonus problems
50. Continuing exercise 22, consider the sum of all ways to decompose polygons into polygons:



Find a symbolic equation for Q and use it to find a generating function for the number of ways to draw nonintersecting diagonals inside a convex n-gon. (Give a closed form for the generating function as a function of z; you need not find a closed form for the coefficients.)
51. Prove that the product



is the generating function for tilings of an m×n rectangle with dominoes. (There are mn factors, which we can imagine are written in the mn cells of the rectangle. If mn is odd, the middle factor is zero. The coefficient of  is the number of ways to do the tiling with j vertical and k horizontal dominoes.) Hint: This is a difficult problem, really beyond the scope of this book. You may wish to simply verify the formula in the case m = 3, n = 4.
Is this a hint or a warning?
52. Prove that the polynomials defined by the recurrence



have the form , where  is a positive integer for 1 ≤ m ≤ n. Hint: This exercise is very instructive but not very easy.
53. The sequence of pentagonal numbers 1, 5, 12, 22, . . . generalizes the triangular and square numbers in an obvious way:



Let the nth triangular number be Tn = n(n+1)/2; let the nth pentagonal number be Pn = n(3n - 1)/2; and let Un be the 3 × n domino-tiling number defined in (7.38). Prove that the triangular number T(U4n+2-1)/2 is also a pentagonal number. Hint: .
54. Consider the following curious construction:



(Start with a row containing all the positive integers. Then delete every mth column; here m = 5. Then replace the remaining entries by partial sums. Then delete every (m - 1)st column. Then replace with partial sums again, and so on.) Use generating functions to show that the final result is the sequence of mth powers. For example, when m = 5 we get 15, 25, 35, 45, . . . as shown.
55. Prove that if the power series F(z) and G(z) are differentiably finite (as defined in exercise 20), then so are F(z) + G(z) and F(z)G(z).


Research problems
56. Prove that there is no "simple closed form" for the coefficient of zn in (1 + z + z2)n, as a function of n, in some large class of "simple closed forms."
57. Prove or disprove: If all the coefficients of G(z) are either 0 or 1, and if all the coefficients of G(z)2 are less than some constant M, then infinitely many of the coefficients of G(z)2 are zero.











8. Discrete Probability
The element of chance enters into many of our attempts to understand the world we live in. A mathematical theory of probability allows us to calculate the likelihood of complex events if we assume that the events are governed by appropriate axioms. This theory has significant applications in all branches of science, and it has strong connections with the techniques we have studied in previous chapters.
Probabilities are called "discrete" if we can compute the probabilities of all events by summation instead of by integration. We are getting pretty good at sums, so it should come as no great surprise that we are ready to apply our knowledge to some interesting calculations of probabilities and averages.

8.1 Definitions
Probability theory starts with the idea of a probability space, which is a set Ω of all things that can happen in a given problem together with a rule that assigns a probability Pr(ω) to each elementary event ω  Ω. The probability Pr(ω) must be a nonnegative real number, and the condition



must hold in every discrete probability space. Thus, each value Pr(ω) must lie in the interval [0 . . 1]. We speak of Pr as a probability distribution, because it distributes a total probability of 1 among the events ω.
(Readers unfamiliar with probability theory will, with high probability, benefit from a perusal of Feller's classic introduction to the subject [120].)
Here's an example: If we're rolling a pair of dice, the set Ω of elementary events is , where



Never say die.
is the set of all six ways that a given die can land. Two rolls such as  and  are considered to be distinct; hence this probability space has a total of 62 = 36 elements.
We usually assume that dice are "fair"—that each of the six possibilities for a particular die has probability , and that each of the 36 possible rolls in Ω has probability . But we can also consider "loaded" dice in which there is a different distribution of probabilities. For example, let
Careful: They might go off.



Then ΣdDPr1(d) = 1, so Pr1 is a probability distribution on the set D, and we can assign probabilities to the elements of Ω = D2 by the rule



For example, . This is a valid distribution because



We can also consider the case of one fair die and one loaded die,



in which case . Dice in the "real world" can't really be expected to turn up equally often on each side, because they aren't perfectly symmetrical; but  is usually pretty close to the truth.
If all sides of a cube were identical, how could we tell which side is face up?
An event is a subset of Ω. In dice games, for example, the set



is the event that "doubles are thrown." The individual elements ω of Ω are called elementary events because they cannot be decomposed into smaller subsets; we can think of ω as a one-element event {ω}.
The probability of an event A is defined by the formula



and in general if R(ω) is any statement about ω, we write 'Pr (R(ω))' for the sum of all Pr(ω) such that R(ω) is true. Thus, for example, the probability of doubles with fair dice is ; but when both dice are loaded with probability distribution Pr1 it is . Loading the dice makes the event "doubles are thrown" more probable.
(We have been using Σ-notation in a more general sense here than defined in Chapter 2: The sums in (8.1) and (8.4) occur over all elements ω of an arbitrary set, not over integers only. However, this new development is not really alarming; we can agree to use special notation under a Σ whenever nonintegers are intended, so there will be no confusion with our ordinary conventions. The other definitions in Chapter 2 are still valid; in particular, the definition of infinite sums in that chapter gives the appropriate interpretation to our sums when the set Ω is infinite. Each probability is nonnegative, and the sum of all probabilities is bounded, so the probability of event A in (8.4) is well defined for all subsets A ⊆ Ω.)
A random variable is a function defined on the elementary events ω of a probability space. For example, if Ω = D2 we can define S(ω) to be the sum of the spots on the dice roll ω, so that  = 6 + 3 = 9. The probability that the spots total seven is the probability of the event S(ω) = 7, namely



With fair dice (Pr = Pr00), this happens with probability ; but with loaded dice (Pr = Pr11), it happens with probability , the same as we observed for doubles.
It's customary to drop the '(ω)' when we talk about random variables, because there's usually only one probability space involved when we're working on any particular problem. Thus we say simply 'S = 7' for the event that a 7 was rolled, and 'S = 4' for the event .
A random variable can be characterized by the probability distribution of its values. Thus, for example, S takes on eleven possible values {2, 3, . . . , 12}, and we can tabulate the probability that S = s for each s in this set:



If we're working on a problem that involves only the random variable S and no other properties of dice, we can compute the answer from these probabilities alone, without regard to the details of the set Ω = D2. In fact, we could define the probability space to be the smaller set Ω = {2, 3, . . . , 12}, with whatever probability distribution Pr(s) is desired. Then 'S = 4' would be an elementary event. Thus we can often ignore the underlying probability space Ω and work directly with random variables and their distributions.
If two random variables X and Y are defined over the same probability space Ω, we can characterize their behavior without knowing everything about Ω if we know the "joint distribution"
Just Say No.
Pr(X = x and Y = y)
for each x in the range of X and each y in the range of Y. We say that X and Y are independent random variables if



for all x and y. Intuitively, this means that the value of X has no effect on the value of Y.
For example, if Ω is the set of dice rolls D2, we can let S1 be the number of spots on the first die and S2 the number of spots on the second. Then the random variables S1 and S2 are independent with respect to each of the probability distributions Pr00, Pr11, and Pr01 discussed earlier, because we defined the dice probability for each elementary event dd′ as a product of a probability for S1 = d multiplied by a probability for S2 = d′. We could have defined probabilities differently so that, say,
A dicey inequality.



but we didn't do that, because different dice aren't supposed to influence each other. With our definitions, both of these ratios are Pr(S2 = 5)/ Pr(S2 = 6).
We have defined S to be the sum of the two spot values, S1 + S2. Let's consider another random variable P, the product S1S2. Are S and P independent? Informally, no; if we are told that S = 2, we know that P must be 1. Formally, no again, because the independence condition (8.5) fails spectacularly (at least in the case of fair dice): For all legal values of s and p, we have 0 < Pr00(S = s) ̭ Pr00(P = p) ; this can't equal Pr00(S = s and P = p), which is a multiple of .
If we want to understand the typical behavior of a given random variable, we often ask about its "average" value. But the notion of "average" is ambiguous; people generally speak about three different kinds of averages when a sequence of numbers is given:
• the mean (which is the sum of all values, divided by the number of values);
• the median (which is the middle value, numerically);
• the mode (which is the value that occurs most often).
For example, the mean of (3, 1, 4, 1, 5) is ; the median is 3; the mode is 1.
But probability theorists usually work with random variables instead of with sequences of numbers, so we want to define the notion of an "average" for random variables too. Suppose we repeat an experiment over and over again, making independent trials in such a way that each value of X occurs with a frequency approximately proportional to its probability. (For example, we might roll a pair of dice many times, observing the values of S and/or P.) We'd like to define the average value of a random variable so that such experiments will usually produce a sequence of numbers whose mean, median, or mode is approximately the same as the mean, median, or mode of X, according to our definitions.
Here's how it can be done: The mean of a random real-valued variable X on a probability space Ω is defined to be



if this potentially infinite sum exists. (Here X(Ω) stands for the set of all real values x for which Pr(X = x) is nonzero.) The median of X is defined to be the set of all x  X(Ω) such that



And the mode of X is defined to be the set of all x  X(Ω) such that



In our dice-throwing example, the mean of S turns out to be  in distribution Pr00, and it also turns out to be 7 in distribution Pr11. The median and mode both turn out to be {7} as well, in both distributions. So S has the same average under all three definitions. On the other hand the P in distribution Pr00 turns out to have a mean value of  = 12.25; its median is {10}, and its mode is {6, 12}. The mean of P is unchanged if we load the dice with distribution Pr11, but the median drops to {8} and the mode becomes {6} alone.
Probability theorists have a special name and notation for the mean of a random variable: They call it the expected value, and write



In our dice-throwing example, this sum has 36 terms (one for each element of Ω), while (8.6) is a sum of only eleven terms. But both sums have the same value, because they're both equal to



The mean of a random variable turns out to be more meaningful in applications than the other kinds of averages, so we shall largely forget about medians and modes from now on. We will use the terms "expected value," "mean," and "average" almost interchangeably in the rest of this chapter.
I get it: On average, "average" means "mean."
If X and Y are any two random variables defined on the same probability space, then X + Y is also a random variable on that space. By formula (8.9), the average of their sum is the sum of their averages:



Similarly, if α is any constant we have the simple rule



But the corresponding rule for multiplication of random variables is more complicated in general; the expected value is defined as a sum over elementary events, and sums of products don't often have a simple form. In spite of this difficulty, there is a very nice formula for the mean of a product in the special case that the random variables are independent:



We can prove this by the distributive law for products,



For example, we know that S = S1 + S2 and P = S1S2, when S1 and S2 are the numbers of spots on the first and second of a pair of fair dice. We have ES1 = ES2 = , hence ES = 7; furthermore S1 and S2 are independent, so , as claimed earlier. We also have E(S+P) = ES+EP = 7+ . But S and P are not independent, so we cannot assert that . In fact, the expected value of SP turns out to equal  in distribution Pr00, while it equals 112 (exactly) in distribution Pr11.


8.2 Mean and Variance
The next most important property of a random variable, after we know its expected value, is its variance, defined as the mean square deviation from the mean:



If we denote EX by μ, the variance VX is the expected value of (X - μ)2. This measures the "spread" of X's distribution.
As a simple example of variance computation, let's suppose we have just been made an offer we can't refuse: Someone has given us two gift certificates for a certain lottery. The lottery organizers sell 100 tickets for each weekly drawing. One of these tickets is selected by a uniformly random process—that is, each ticket is equally likely to be chosen—and the lucky ticket holder wins a hundred million dollars. The other 99 ticket holders win nothing.
(Slightly subtle point: There are two probability spaces, depending on what strategy we use; but EX1 and EX2 are the same in both.)
We can use our gift in two ways: Either we buy two tickets in the same lottery, or we buy one ticket in each of two lotteries. Which is a better strategy? Let's try to analyze this by letting X1 and X2 be random variables that represent the amount we win on our first and second ticket. The expected value of X1, in millions, is



and the same holds for X2. Expected values are additive, so our average total winnings will be
E(X1 + X2) = EX1 + EX2 = 2 million dollars,
regardless of which strategy we adopt.
Still, the two strategies seem different. Let's look beyond expected values and study the exact probability distribution of X1 + X2:



If we buy two tickets in the same lottery we have a 98% chance of winning nothing and a 2% chance of winning $100 million. If we buy them in different lotteries we have a 98.01% chance of winning nothing, so this is slightly more likely than before; and we have a 0.01% chance of winning $200 million, also slightly more likely than before; and our chances of winning $100 million are now 1.98%. So the distribution of X1 + X2 in this second situation is slightly more spread out; the middle value, $100 million, is slightly less likely, but the extreme values are slightly more likely.
It's this notion of the spread of a random variable that the variance is intended to capture. We measure the spread in terms of the squared deviation of the random variable from its mean. In case 1, the variance is therefore
.98(0M - 2M)2 + .02(100M - 2M)2 = 196M2;
in case 2 it is
.9801(0M - 2M)2 + .0198(100M - 2M)2 + .0001(200M - 2M)2 = 198M2.
As we expected, the latter variance is slightly larger, because the distribution of case 2 is slightly more spread out.
When we work with variances, everything is squared, so the numbers can get pretty big. (The factor M2 is one trillion, which is somewhat imposing even for high-stakes gamblers.) To convert the numbers back to the more meaningful original scale, we often take the square root of the variance. The resulting number is called the standard deviation, and it is usually denoted by the Greek letter σ:



Interesting: The variance of a dollar amount is expressed in units of square dollars.
The standard deviations of the random variables X1 + X2 in our two lottery strategies are  = 14.00M and  ≈ 14.071247M. In some sense the second alternative is about $71,247 riskier.
How does the variance help us choose a strategy? It's not clear. The strategy with higher variance is a little riskier; but do we get the most for our money by taking more risks or by playing it safe? Suppose we had the chance to buy 100 tickets instead of only two. Then we could have a guaranteed victory in a single lottery (and the variance would be zero); or we could gamble on a hundred different lotteries, with a .99100 ≈ .366 chance of winning nothing but also with a nonzero probability of winning up to $10,000,000,000. To decide between these alternatives is beyond the scope of this book; all we can do here is explain how to do the calculations.
Another way to reduce risk might be to bribe the lottery officials. I guess that's where probability becomes indiscreet.
(N.B.: Opinions expressed in these margins do not necessarily represent the opinions of the management.)
In fact, there is a simpler way to calculate the variance, instead of using the definition (8.13). (We suspect that there must be something going on in the mathematics behind the scenes, because the variances in the lottery example magically came out to be integer multiples of M2.) We have


E([X - EX]2)
=
E(X2 - 2X[EX] + [EX]2)


 
=
E(X2 - 2X[EX][EX] + [EX]2,


since (EX) is a constant; hence



"The variance is the mean of the square minus the square of the mean."
For example, the mean of (X1 + X2)2 comes to .98(0M)2 + .02(100M)2 = 200M2 or to .9801(0M)2 + .0198(100M)2 + .0001(200M)2 = 202M2 in the lottery problem. Subtracting 4M2 (the square of the mean) gives the results we obtained the hard way.
There's an even easier formula yet, if we want to calculate V(X+Y) when X and Y are independent: We have


E([X + Y]2)
=
E(X2 + 2XY + Y2)


 
=
E[X2] + 2[EX][EY] + E[Y2],


since we know that E(XY) = (EX)(EY) in the independent case. Therefore



"The variance of a sum of independent random variables is the sum of their variances." For example, the variance of the amount we can win with a single lottery ticket is
 - [EX1]2 = .99[OM]2 + .01[100M]2 - [1M]2 = 99M2.
Therefore the variance of the total winnings of two lottery tickets in two separate (independent) lotteries is 2×99M2 = 198M2. And the corresponding variance for n independent lottery tickets is n × 99M2.
The variance of the dice-roll sum S drops out of this same formula, since S = S1 + S2 is the sum of two independent random variables. We have



when the dice are fair; hence . The loaded die has



hence VS =  = 7.5 when both dice are loaded. Notice that the loaded dice give S a larger variance, although S actually assumes its average value 7 more often than it would with fair dice. If our goal is to shoot lots of lucky 7's, the variance is not our best indicator of success.
OK, we have learned how to compute variances. But we haven't really seen a good reason why the variance is a natural thing to compute. Everybody does it, but why? The main reason is Chebyshev's inequality ([29] and [57]), which states that the variance has a significant property:



If he proved it in 1867, it's a classic '67 Chebyshev.
(This is different from the monotonic inequalities of Chebyshev that we encountered in Chapter 2.) Very roughly, (8.17) tells us that a random variable X will rarely be far from its mean EX if its variance VX is small. The proof is amazingly simple. We have



dividing by α finishes the proof.
If we write μ for the mean and σ for the standard deviation, and if we replace α by c2VX in (8.17), the condition (X - EX)2 ≥ c2VX is the same as (X - μ)2 ≥ (cσ)2; hence (8.17) says that



Thus, X will lie within c standard deviations of its mean value except with probability at most 1/c2. A random variable will lie within 2σ of μ at least 75% of the time; it will lie between μ - 10σ and μ + 10σ at least 99% of the time. These are the cases α = 4VX and α = 100VX of Chebyshev's inequality.
If we roll a pair of fair dice n times, the total value of the n rolls will almost always be near 7n, for large n. Here's why: The variance of n independent rolls is n. A variance of n means a standard deviation of only



So Chebyshev's inequality tells us that the final sum will lie between



in at least 99% of all experiments when n fair dice are rolled. For example, the odds are better than 99 to 1 that the total value of a million rolls will be between 6.975 million and 7.025 million.
In general, let X be any random variable over a probability space Ω, having finite mean μ and finite standard deviation σ. Then we can consider the probability space Ωn whose elementary events are n-tuples (ω1, ω2, . . . , ωn) with each ωk  Ω, and whose probabilities are
Pr(ω1, ω2, . . . , ωn) = Pr(ω1) Pr(ω2) . . . Pr(ωn) .
If we now define random variables Xk by the formula
Xk(ω1, ω2, . . . , ωn) = X(ωk) ,
the quantity
X1 + X2 + · · · + Xn
is a sum of n independent random variables, which corresponds to taking n independent "samples" of X on Ω and adding them together. The mean of X1 + X2 + · · · + Xn is nμ, and the standard deviation is  σ; hence the average of the n samples,
 [X1 + X2 + ... + Xn),
will lie between μ - 10σ/ and μ + 10σ/ at least 99% of the time. In other words, if we choose a large enough value of n, the average of n independent samples will almost always be very near the expected value EX. (An even stronger theorem called the Strong Law of Large Numbers is proved in textbooks of probability theory; but the simple consequence of Chebyshev's inequality that we have just derived is enough for our purposes.)
(That is, the average will fall between the stated limits in at least 99% of all cases when we look at a set of n independent samples, for any fixed value of n. Don't misunderstand this as a statement about the averages of an infinite sequence X1, X2 , X3, . . . as n varies.)
Sometimes we don't know the characteristics of a probability space, and we want to estimate the mean of a random variable X by sampling its value repeatedly. (For example, we might want to know the average temperature at noon on a January day in San Francisco; or we may wish to know the mean life expectancy of insurance agents.) If we have obtained independent empirical observations X1, X2, . . . , Xn, we can guess that the true mean is approximately



And we can also make an estimate of the variance, using the formula



The (n-1)'s in this formula look like typographic errors; it seems they should be n's, as in (8.19), because the true variance VX is defined by expected values in (8.15). Yet we get a better estimate with n - 1 instead of n here, because definition (8.20) implies that



Here's why:



(This derivation uses the independence of the observations when it replaces E(XjXk) by (EX)2[j ≠ k] + E(X2)[j = k].)
In practice, experimental results about a random variable X are usually obtained by calculating a sample mean  = ÊX and a sample standard deviation , and presenting the answer in the form . For example, here are ten rolls of two supposedly fair dice:



The sample mean of the spot sum S is
 = (7 + 11 + 8 + 5 + 4 + 6 + 10 + 8 + 8 + 7)/10 = 7.4;
the sample variance is
(72 + 112 + 82 + 52 + 42 + 62 + 102 + 82 + 82 + 72 - 102)/9 ≈ 2.12 .
We estimate the average spot sum of these dice to be 7.4±2.1/ ≈ 7.4±0.7, on the basis of these experiments.
Let's work one more example of means and variances, in order to show how they can be calculated theoretically instead of empirically. One of the questions we considered in Chapter 5 was the "football victory problem," where n hats are thrown into the air and the result is a random permutation of hats. We showed in equation (5.51) that there's a probability of n¡/n! ≈ 1/e that nobody gets the right hat back. We also derived the formula



for the probability that exactly k people end up with their own hats.
Restating these results in the formalism just learned, we can consider the probability space Πn of all n! permutations π of {1, 2, . . . , n}, where Pr(π) = 1/n! for all π  Πn. The random variable
Fn(π) = number of "fixed points" of π ,    for π  Πn,
measures the number of correct hat-falls in the football victory problem. Equation (8.22) gives Pr(Fn = k), but let's pretend that we don't know any such formula; we merely want to study the average value of Fn, and its standard deviation.
Not to be confused with a Fibonacci number.
The average value is, in fact, extremely easy to calculate, avoiding all the complexities of Chapter 5. We simply observe that


Fn[π]
=
Fn, 1[π] + Fn,2[π] + ... + Fn,n[π],
 


Fn,k[π]
=
[position k of π is a fixed point],
for π  Πn.


Hence
EFn = EFn,1 + EFn,2 + · · · + EFn,n .
And the expected value of Fn,k is simply the probability that Fn,k = 1, which is 1/n because exactly (n - 1)! of the n! permutations π = π1π2 . . . πn  Πn have πk = k. Therefore



On the average, one hat will be in its correct place. "A random permutation has one fixed point, on the average."
One the average.
Now what's the standard deviation? This question is more difficult, because the Fn,k's are not independent of each other. But we can calculate the variance by analyzing the mutual dependencies among them:



(We used a similar trick when we derived (2.33) in Chapter 2.) Now ,k = Fn,k, since Fn,k is either 0 or 1; hence E(,k) = EFn,k = 1/n as before. And if j < k we have E(Fn,j Fn,k) = Pr(π has both j and k as fixed points) = (n - 2)!/n! = 1/n(n - 1). Therefore



(As a check when n = 3, we have ) The variance is E() - (EFn)2 = 1, so the standard deviation (like the mean) is 1. "A random permutation of n ≥ 2 elements has 1 ± 1 fixed points."


8.3 Probability Generating Functions
If X is a random variable that takes only nonnegative integer values, we can capture its probability distribution nicely by using the techniques of Chapter 7. The probability generating function or pgf of X is



This power series in z contains all the information about the random variable X. We can also express it in two other ways:



The coefficients of GX(z) are nonnegative, and they sum to 1; the latter condition can be written



Conversely, any power series G(z) with nonnegative coefficients and with G(1) = 1 is the pgf of some random variable.
The nicest thing about pgf's is that they usually simplify the computation of means and variances. For example, the mean is easily expressed:



We simply differentiate the pgf with respect to z and set z = 1.
The variance is only slightly more complicated:



Therefore



Equations (8.28) and (8.29) tell us that we can compute the mean and variance if we can compute the values of two derivatives,  and . We don't have to know a closed form for the probabilities; we don't even have to know a closed form for GX(z) itself.
It is convenient to write






when G is any function, since we frequently want to compute these combinations of derivatives.
The second-nicest thing about pgf's is that they are comparatively simple functions of z, in many important cases. For example, let's look at the uniform distribution of order n, in which the random variable takes on each of the values {0, 1, . . . , n - 1} with probability 1/n. The pgf in this case is



We have a closed form for Un(z) because this is a geometric series.
But this closed form proves to be somewhat embarrassing: When we plug in z = 1 (the value of z that's most critical for the pgf), we get the undefined ratio 0/0, even though Un(z) is a polynomial that is perfectly well defined at any value of z. The value Un(1) = 1 is obvious from the non-closed form (1 + z + · · · + zn-1)/n, yet it seems that we must resort to L'Hospital's rule to find limz→1 Un(z) if we want to determine Un(1) from the closed form. The determination of  by L'Hospital's rule will be even harder, because there will be a factor of (z-1)2 in the denominator;  will be harder still.
Luckily there's a nice way out of this dilemma. If G(z) = Σn≥0 gnzn is any power series that converges for at least one value of z with |z| > 1, the power series G′(z) = Σn≥0 gnzn-1 will also have this property, and so will G″(z), G″′(z), etc. Therefore by Taylor's theorem we can write



all derivatives of G(z) at z = 1 will appear as coefficients, when G(1 + t) is expanded in powers of t.
For example, the derivatives of the uniform pgf Un(z) are easily found in this way:



Comparing this to (8.33) gives



and in general  (1) = (n - 1)m/(m + 1), although we need only the cases m = 1 and m = 2 to compute the mean and the variance. The mean of the uniform distribution is



and the variance is



The third-nicest thing about pgf's is that the product of pgf's corresponds to the sum of independent random variables. We learned in Chapters 5 and 7 that the product of generating functions corresponds to the convolution of sequences; but it's even more important in applications to know that the convolution of probabilities corresponds to the sum of independent random variables. Indeed, if X and Y are random variables that take on nothing but integer values, the probability that X + Y = n is



If X and Y are independent, we now have



a convolution. Therefore—and this is the punch line —



Earlier this chapter we observed that V(X + Y) = VX + VY when X and Y are independent. Let F(z) and G(z) be the pgf's for X and Y, and let H(z) be the pgf for X + Y. Then
H(z) = F(z)G(z) ,
and our formulas (8.28) through (8.31) for mean and variance tell us that we must have






These formulas, which are properties of the derivatives Mean(H) = H′(1) and Var(H) = H″(1) + H′(1) - H′(1)2, aren't valid for arbitrary function products H(z) = F(z)G(z); we have


H′[z]
=
F′[z]G[z] + F[z]G′[z],


H˝[z]
=
F˝[z]G[z] + 2F′[z]G′[z] + F[z] G˝ [z].


But if we set z = 1, we can see that (8.38) and (8.39) will be valid in general provided only that



and that the derivatives exist. The "probabilities" don't have to be in [0 . . 1] for these formulas to hold. We can normalize the functions F(z) and G(z) by dividing through by F(1) and G(1) in order to make this condition valid, whenever F(1) and G(1) are nonzero.
I'll graduate magna cum ulant.
Mean and variance aren't the whole story. They are merely two of an infinite series of so-called cumulant statistics introduced by the Danish astronomer Thorvald Nicolai Thiele [351] in 1903. The first two cumulants κ1 and κ2 of a random variable are what we have called the mean and the variance; there also are higher-order cumulants that express more subtle properties of a distribution. The general formula



defines the cumulants of all orders, when G(z) is the pgf of a random variable.
Let's look at cumulants more closely. If G(z) is the pgf for X, we have



where



This quantity μm is called the "mth moment" of X. We can take exponentials on both sides of (8.41), obtaining another formula for G(et):



Equating coefficients of powers of t leads to a series of formulas















defining the cumulants in terms of the moments. Notice that κ2 is indeed the variance, E(X2) - (EX)2, as claimed.
Equation (8.41) makes it clear that the cumulants defined by the product F(z)G(z) of two pgf's will be the sums of the corresponding cumulants of F(z) and G(z), because logarithms of products are sums. Therefore all cumulants of the sum of independent random variables are additive, just as the mean and variance are. This property makes cumulants more important than moments.
"For these higher half-invariants we shall propose no special names."    —T. N. Thiele [351]
If we take a slightly different tack, writing



equation (8.33) tells us that the α's are the "factorial moments"



It follows that



and we can express the cumulants in terms of the derivatives G(m)(1):









This sequence of formulas yields "additive" identities that extend (8.38) and (8.39) to all the cumulants.
Let's get back down to earth and apply these ideas to simple examples. The simplest case of a random variable is a "random constant," where X has a certain fixed value x with probability 1. In this case GX(z) = zx, and ln GX(et) = xt; hence the mean is x and all other cumulants are zero. It follows that the operation of multiplying any pgf by zx increases the mean by x but leaves the variance and all other cumulants unchanged.
How do probability generating functions apply to dice? The distribution of spots on one fair die has the pgf



where U6 is the pgf for the uniform distribution of order 6. The factor 'z' adds 1 to the mean, so the mean is 3.5 instead of  = 2.5 as given in (8.35); but an extra 'z' does not affect the variance (8.36), which equals .
The pgf for total spots on two independent dice is the square of the pgf for spots on one die,


Gs[z]
=



 
=
z2U6[z]2.


If we roll a pair of fair dice n times, the probability that we get a total of k spots overall is, similarly,


[zK] Gs (z)n
=
[zk] z2n U6 (z)2n


 
=
[zk-2n] U6 (z)2n.


Hat distribution is a different kind of uniform distribution.
In the hats-off-to-football-victory problem considered earlier, otherwise known as the problem of enumerating the fixed points of a random permutation, we know from (5.49) that the pgf is



Therefore



Without knowing the details of the coefficients, we can conclude from this recurrence  = Fn-1[z] that  [z] = Fn-m [z]; hence



This formula makes it easy to calculate the mean and variance; we find as before (but more quickly) that they are both equal to 1 when n ≥ 2.
In fact, we can now show that the mth cumulant κm of this random variable is equal to 1 whenever n ≥ m. For the mth cumulant depends only on , and these are all equal to 1; hence we obtain the same answer for the mth cumulant as we do when we replace Fn(z) by the limiting pgf



which has  (1) = 1 for derivatives of all orders. The cumulants of F∞ are identically equal to 1, because





8.4 Flipping Coins
Now let's turn to processes that have just two outcomes. If we flip a coin, there's probability p that it comes up heads and probability q that it comes up tails, where
p + q = 1 .
(We assume that the coin doesn't come to rest on its edge, or fall into a hole, etc.) Throughout this section, the numbers p and q will always sum to 1. If the coin is fair, we have p = q = ; otherwise the coin is said to be biased.
Con artists know that p ≈ 0.1 when you spin a newly minted U.S. penny on a smooth table. (The weight distribution makes Lincoln's head fall downward.)
The probability generating function for the number of heads after one toss of a coin is



If we toss the coin n times, always assuming that different coin tosses are independent, the number of heads is generated by



according to the binomial theorem. Thus, the chance that we obtain exactly k heads in n tosses is  pk qn-k. This sequence of probabilities is called the binomial distribution.
Suppose we toss a coin repeatedly until heads first turns up. What is the probability that exactly k tosses will be required? We have k = 1 with probability p (since this is the probability of heads on the first flip); we have k = 2 with probability qp (since this is the probability of tails first, then heads); and for general k the probability is qk-1p. So the generating function is



Repeating the process until n heads are obtained gives the pgf



This, incidentally, is zn times



the generating function for the negative binomial distribution.
The probability space in example (8.59), where we flip a coin until n heads have appeared, is different from the probability spaces we've seen earlier in this chapter, because it contains infinitely many elements. Each element is a finite sequence of heads and/or tails, containing precisely n heads in all, and ending with heads; the probability of such a sequence is pnqk-n, where k - n is the number of tails. Thus, for example, if n = 3 and if we write H for heads and T for tails, the sequence THTTTHH is an element of the probability space, and its probability is qpqqqpp = p3q4.
Heads I win, tails you lose.No? OK; tails you lose, heads I win.No? Well, then, heads you lose, tails I win.
Let X be a random variable with the binomial distribution (8.57), and let Y be a random variable with the negative binomial distribution (8.60). These distributions depend on n and p. The mean of X is nH′(1) = np, since its pgf is H(z)n; the variance is



Thus the standard deviation is : If we toss a coin n times, we expect to get heads about np ±  times. The mean and variance of Y can be found in a similar way: If we let



we have



hence G′(1) = pq/p2 = q/p and G″(1) = 2pq2/p3 = 2q2/p2. It follows that the mean of Y is nq/p and the variance is nq/p2.
A simpler way to derive the mean and variance of Y is to use the reciprocal generating function



and to write



This polynomial F(z) is not a probability generating function, because it has a negative coefficient. But it does satisfy the crucial condition F(1) = 1. Thus F(z) is formally a binomial that corresponds to a coin for which we get heads with "probability" equal to -q/p; and G(z) is formally equivalent to flipping such a coin -1 times(!). The negative binomial distribution with parameters (n, p) can therefore be regarded as the ordinary binomial distribution with parameters (n′, p′) = (-n, -q/p). Proceeding formally, the mean must be n′p′ = (-n)(-q/p) = nq/p, and the variance must be n′p′q′ = (-n)(-q/p)(1 + q/p) = nq/p2. This formal derivation involving negative probabilities is valid, because our derivation for ordinary binomials was based on identities between formal power series in which the assumption 0 ≤ p ≤ 1 was never used.
The probability is negative that I'm getting younger.
Oh? Then it's > 1 that you're getting older, or staying the same.
Let's move on to another example: How many times do we have to flip a coin until we get heads twice in a row? The probability space now consists of all sequences of H's and T's that end with HH but have no consecutive H's until the final position:
Ω = {HH, THH, TTHH, HTHH, TTTHH, THTHH, HTTHH, . . . } .
The probability of any given sequence is obtained by replacing H by p and T by q; for example, the sequence THTHH will occur with probability
Pr(THTHH) = qpqpp = p3q2 .
We can now play with generating functions as we did at the beginning of Chapter 7, letting S be the infinite sum
S = HH + THH + TTHH + HTHH + TTTHH + THTHH + HTTHH + · · ·
of all the elements of Ω. If we replace each H by pz and each T by qz, we get the probability generating function for the number of flips needed until two consecutive heads turn up.
There's a curious relation between S and the sum of domino tilings



in equation (7.1). Indeed, we obtain S from T if we replace each  by T and each  by HT, then tack on an HH at the end. This correspondence is easy to prove because each element of Ω has the form (T + HT)nHH for some n ≥ 0, and each term of T has the form . Therefore by (7.4) we have
S = (1 - T - HT)-1HH ,
and the probability generating function for our problem is



Our experience with the negative binomial distribution gives us a clue that we can most easily calculate the mean and variance of (8.64) by writing


G[z]
=



where


F[z]
=



and by calculating the "mean" and "variance" of this pseudo-pgf F(z). (Once again we've introduced a function with F(1) = 1.) We have


F′[1]
=
[-q - 2pq]/p2 = 2 - p-1 -p-2 ;


F˝[1]
=
-2pq/p2 = 2 - 2p-1.


Therefore, since z2 = F(z)G(z), Mean(z2) = 2, and Var(z2) = 0, the mean and variance of distribution G(z) are






When p =  the mean and variance are 6 and 22, respectively. (Exercise 4 discusses the calculation of means and variances by subtraction.)
Now let's try a more intricate experiment: We will flip coins until the pattern THTTH is first obtained. The sum of winning positions is now


S
=
THTTH + HTHTTH + TTHTTH


 
 
+ HHTHTTH + HTTHTTH + THTHTTH + TTTHTTH + · · · ;


this sum is more difficult to describe than the previous one. If we go back to the method by which we solved the domino problems in Chapter 7, we can obtain a formula for S by considering it as a "finite state language" defined by the following "automaton":



"'You really are an automaton—a calculating machine,' I cried. 'There is something positively inhuman in you at times.'"    —J. H. Watson [83]
The elementary events in the probability space are the sequences of H's and T's that lead from state 0 to state 5. Suppose, for example, that we have just seen THT; then we are in state 3. Flipping tails now takes us to state 4; flipping heads in state 3 would take us to state 2 (not all the way back to state 0, since the TH we've just seen may be followed by TTH).
In this formulation, we can let Sk be the sum of all sequences of H's and T's that lead to state k; it follows that


S0
=
1 + S0 H + S2 H,


S1
=
S0 T + S1 T + S4 T,


S2
=
S1 H + S3 H,


S3
=
S2 T,


S4
=
S3 T,


S5
=
S4 H,


Now the sum S in our problem is S5; we can obtain it by solving these six equations in the six unknowns S0, S1, . . . , S5. Replacing H by pz and T by qz gives generating functions where the coefficient of zn in Sk is the probability that we are in state k after n flips.
In the same way, any diagram of transitions between states, where the transition from state j to state k occurs with given probability pj,k, leads to a set of simultaneous linear equations whose solutions are generating functions for the state probabilities after n transitions have occurred. Systems of this kind are called Markov processes, and the theory of their behavior is intimately related to the theory of linear equations.
But the coin-flipping problem can be solved in a much simpler way, without the complexities of the general finite-state approach. Instead of six equations in six unknowns S0, S1, . . . , S5, we can characterize S with only two equations in two unknowns. The trick is to consider the auxiliary sum N = S0 + S1 + S2 + S3 + S4 of all flip sequences that don't contain any occurrences of the given pattern THTTH:
N = 1 + H + T + HH + · · · + THTHT + THTTT + · · · .
We have



because every term on the left either ends with THTTH (and belongs to S) or doesn't (and belongs to N); conversely, every term on the right is either empty or belongs to N H or N T. And we also have the important additional equation



because every term on the left completes a term of S after either the first H or the second H, and because every term on the right belongs to the left.
The solution to these two simultaneous equations is easily obtained: We have N = (1 - S)(1 - H - T)-1 from (8.67), hence
(1 - S)(1 - T - H)-1 THTTH = S(1 + TTH) .
As before, we get the probability generating function G(z) for the number of flips if we replace H by pz and T by qz. A bit of simplification occurs since p + q = 1, and we find



hence the solution is



Notice that G(1) = 1, if pq ≠ 0; we do eventually encounter the pattern THTTH, with probability 1, unless the coin is rigged so that it always comes up heads or always tails.
To get the mean and variance of the distribution (8.69), we invert G(z) as we did in the previous problem, writing G(z) = z5/F(z) where F is a polynomial:



The relevant derivatives are


F′(1)
=
5 - (1 + pq2)/p2q3,


F′(1)
=
20 - 6pq2/p2q3;


and if X is the number of flips we get






When , the mean and variance are 36 and 996.
Let's get general: The problem we have just solved was "random" enough to show us how to analyze the case that we are waiting for the first appearance of an arbitrary pattern A of heads and tails. Again we let S be the sum of all winning sequences of H's and T's, and we let N be the sum of all sequences that haven't encountered the pattern A yet. Equation (8.67) will remain the same; equation (8.68) will become



where m is the length of A, and where A(k) and A(k) denote respectively the last k characters and the first k characters of A. For example, if A is the pattern THTTH we just studied, we have
A(1) = H , A(2) = TH , A(3) = TTH , A(4) = HTTH;
A(1) = T , A(2) = TH , A(3) = THT , A(4) = THTT.
Since the only perfect match is A(2) = A(2), equation (8.73) reduces to (8.68).
Let Ã be the result of substituting p-1 for H and q-1 for T in the pattern A. Then it is not difficult to generalize our derivation of (8.71) and (8.72) to conclude (exercise 20) that the general mean and variance are






In the special case  we can interpret these formulas in a particularly simple way. Given a pattern A of m heads and tails, let



We can easily find the binary representation of this number by placing a '1' under each position such that the string matches itself perfectly when it is superimposed on a copy of itself that has been shifted to start in this position:



Equation (8.74) now tells us that the expected number of flips until pattern A appears is exactly 2(A:A), if we use a fair coin, because Ã(k) = 2k when p = q = . This result, first discovered by the Soviet mathematician A. D. Solov'ev in 1966 [331], seems paradoxical at first glance: Patterns with no self-overlaps occur sooner than overlapping patterns do! It takes almost twice as long to encounter HHHHH as it does to encounter HHHHT or THHHH.
"Chem bol'she periodov u nashego slova, tem pozzhe ono poıavlıaetsıa."    —A. D. Solov'ev
Now let's consider an amusing game that was invented by (of all people) Walter Penney [289] in 1969. Alice and Bill flip a coin until either HHT or HTT occurs; Alice wins if the pattern HHT comes first, Bill wins if HTT comes first. This game—now called "Penney ante"—certainly seems to be fair, if played with a fair coin, because both patterns HHT and HTT have the same characteristics if we look at them in isolation: The probability generating function for the waiting time until HHT first occurs is
G(z) = 
and the same is true for HTT. Therefore neither Alice nor Bill has an advantage, if they play solitaire.
Of course not! Who could they have an advantage over?
But there's an interesting interplay between the patterns when both are considered simultaneously. Let SA be the sum of Alice's winning configurations, and let SB be the sum of Bill's:
SA = HHT + HHHT + THHT + HHHHT + HTHHT + THHHT + ···;
SB = HTT + THTT + HTHTT + TTHTT + THTHTT + TTTHTT + ···.
Also—taking our cue from the trick that worked when only one pattern was involved—let us denote by N the sum of all sequences in which neither player has won so far:



Then we can easily verify the following set of equations:



If we now set H = T = , the resulting value of SA becomes the probability that Alice wins, and SB becomes the probability that Bill wins. The three equations reduce to



and we find SA = , SB = . Alice will win about twice as often as Bill!
In a generalization of this game, Alice and Bill choose patterns A and B of heads and tails, and they flip coins until either A or B appears. The two patterns need not have the same length, but we assume that A doesn't occur within B, nor does B occur within A. (Otherwise the game would be degenerate. For example, if A = HT and B = THTH, poor Bill could never win; and if A = HTH and B = TH, both players might claim victory simultaneously.) Then we can write three equations analogous to (8.73) and (8.78):



Here l is the length of A and m is the length of B. For example, if we have A = HTTHTHTH and B = THTHTTH, the two pattern-dependent equations are
N HTTHTHTH = SA TTHTHTH + SA + SB TTHTHTH + SB THTH;
  N THTHTTH = SA THTTH + SA TTH + SB THTTH + SB.
We obtain the victory probabilities by setting H = T = , if we assume that a fair coin is being used; this reduces the two crucial equations to



We can see what's going on if we generalize the A:A operation of (8.76) to a function of two independent strings A and B:



Equations (8.80) now become simply
SA(A:A) + SB(B:A) = SA(A:B) + SB(B:B);
the odds in Alice's favor are



(This beautiful formula was discovered by John Horton Conway [137].)
For example, if A = HTTHTHTH and B = THTHTTH as above, we have A:A = (10000001)2 = 129, A:B = (0001010)2 = 10, B:A = (0001001)2 = 9, and B:B = (1000010)2 = 66; so the ratio SA/SB is (66-9)/(129-10) = 57/119. Alice will win this one only 57 times out of every 176, on the average.
Strange things can happen in Penney's game. For example, the pattern HHTH wins over the pattern HTHH with 3/2 odds, and HTHH wins over THHH with 7/5 odds. So HHTH ought to be much better than THHH. Yet THHH actually wins over HHTH, with 7/5 odds! The relation between patterns is not transitive. In fact, exercise 57 proves that if Alice chooses any pattern τ1τ2 . . . τl of length l ≥ 3, Bill can always ensure better than even chances of winning if he chooses the pattern 2τ1τ2 . . . τl-1, where  is the heads/tails opposite of τ2.
Odd, odd.


8.5 Hashing
Let's conclude this chapter by applying probability theory to computer programming. Several important algorithms for storing and retrieving information inside a computer are based on a technique called "hashing." The general problem is to maintain a set of records that each contain a "key" value, K, and some data D(K) about that key; we want to be able to find D(K) quickly when K is given. For example, each key might be the name of a student, and the associated data might be that student's homework grades.
"Somehow the verb 'to hash' magically became standard terminology for key transformation during the mid-1960s, yet nobody was rash enough to use such an undignified word publicly until 1967."    —D. E. Knuth [209]
In practice, computers don't have enough capacity to set aside one memory cell for every possible key; billions of keys are possible, but comparatively few keys are actually present in any one application. One solution to the problem is to maintain two tables KEY[j] and DATA[j] for 1 ≤ j ≤ N, where N is the total number of records that can be accommodated; another variable n tells how many records are actually present. Then we can search for a given key K by going through the table sequentially in an obvious way:
S1 Set j := 1. (We've searched through all positions < j.)
S2 If j > n, stop. (The search was unsuccessful.)
S3 If KEY[j] = K, stop. (The search was successful.)
S4 Increase j by 1 and return to step S2. (We'll try again.)
After a successful search, the desired data entry D(K) appears in DATA[j]. After an unsuccessful search, we can insert K and D(K) into the table by setting
n  :=  j,  KEY[n]  :=  K,  DATA[n]  :=  D(K),
assuming that the table was not already filled to capacity.
This method works, but it can be dreadfully slow; we need to repeat step S2 a total of n + 1 times whenever an unsuccessful search is made, and n can be quite large.
Hashing was invented to speed things up. The basic idea, in one of its popular forms, is to use m separate lists instead of one giant list. A "hash function" transforms every possible key K into a list number h(K) between 1 and m. An auxiliary table FIRST[i] for 1 ≤ i ≤ m points to the first record in list i; another auxiliary table NEXT[j] for 1 ≤ j ≤ N points to the record following record j in its list. We assume that


FIRST[i]
=
-1,
if list i is empty;


NEXT[j]
=
0 ,
if record j is the last in its list.


As before, there's a variable n that tells how many records have been stored altogether.
For example, suppose the keys are names, and suppose that there are m = 4 lists based on the first letter of a name:



We start with four empty lists and with n = 0. If, say, the first record has Nora as its key, we have h(Nora) = 3, so Nora becomes the key of the first item in list 3. If the next two names are Glenn and Jim, they both go into list 2. Now the tables in memory look like this:
FIRST[1] = -1,  FIRST[2] = 2,  FIRST[3] = 1,  FIRST[4] = -1.
KEY[1]  =  Nora,     NEXT[1] = 0;
KEY[2]  =  Glenn,    NEXT[2] = 3;
KEY[3]  =  Jim,      NEXT[3] = 0 ;     n = 3.
(The values of DATA[1], DATA[2], and DATA[3] are confidential and will not be shown.) After 18 records have been inserted, the lists might contain the names



and these names would appear intermixed in the KEY array with NEXT entries to keep the lists effectively separate. If we now want to search for John, we have to scan through the six names in list 2 (which happens to be the longest list); but that's not nearly as bad as looking at all 18 names.
Let's hear it for the Concrete Math students who sat in the front rows and lent their names to this experiment.
Here's a precise specification of the algorithm that searches for key K in accordance with this scheme:
H1 Set i := h(K) and j := FIRST[i].
H2 If j ≤ 0, stop. (The search was unsuccessful.)
H3 If KEY[j] = K, stop. (The search was successful.)
H4 Set i := j, then set j := NEXT[i] and return to step H2. (We'll try again.)
For example, to search for Jennifer in the example given, step H1 would set i := 2 and j := 2; step H3 would find that Glenn ≠ Jennifer; step H4 would set j := 3; and step H3 would find Jim ≠ Jennifer. One more iteration of steps H4 and H3 would locate Jennifer in the table.
I bet their parents are glad about that.
After a successful search, the desired data D(K) appears in DATA[j], as in the previous algorithm. After an unsuccessful search, we can enter K and D(K) in the table by doing the following operations:



Now the table will once again be up to date.
We hope to get lists of roughly equal length, because this will make the task of searching about m times faster. The value of m is usually much greater than 4, so a factor of 1/m will be a significant improvement.
We don't know in advance what keys will be present, but it is generally possible to choose the hash function h so that we can consider h(K) to be a random variable that is uniformly distributed between 1 and m, independent of the hash values of other keys that are present. In such cases computing the hash function is like rolling a die that has m faces. There's a chance that all the records will fall into the same list, just as there's a chance that a die will always turn up ; but probability theory tells us that the lists will almost always be pretty evenly balanced.

Analysis of Hashing: Introduction.
"Algorithmic analysis" is a branch of computer science that derives quantitative information about the efficiency of computer methods. "Probabilistic analysis of an algorithm" is the study of an algorithm's running time, considered as a random variable that depends on assumed characteristics of the input data. Hashing is an especially good candidate for probabilistic analysis, because it is an extremely efficient method on the average, even though its worst case is too horrible to contemplate. (The worst case occurs when all keys have the same hash value.) Indeed, a computer programmer who uses hashing had better be a believer in probability theory.
Let P be the number of times step H3 is performed when the algorithm above is used to carry out a search. (Each execution of H3 is called a "probe" in the table.) If we know P, we know how often each step is performed, depending on whether the search is successful or unsuccessful:



Thus the main quantity that governs the running time of the search procedure is the number of probes, P.
We can get a good mental picture of the algorithm by imagining that we are keeping an address book that is organized in a special way, with room for only one entry per page. On the cover of the book we note down the page number for the first entry in each of m lists; each name K determines the list h(K) that it belongs to. Every page inside the book refers to the successor page in its list. The number of probes needed to find an address in such a book is the number of pages we must consult.
If n items have been inserted, their positions in the table depend only on their respective hash values, h1, h2, . . . , hn. Each of the mn possible sequences h1, h2, . . . , hn is considered to be equally likely, and P is a random variable depending on such a sequence.


Case 1: The key is not present.
Let's consider first the behavior of P in an unsuccessful search, assuming that n records have previously been inserted into the hash table. In this case the relevant probability space consists of mn+1 elementary events
ω = (h1, h2, . . . , hn, hn+1)
where hj is the hash value of the jth key inserted, and where hn+1 is the hash value of the key for which the search is unsuccessful. We assume that the hash function h has been chosen properly so that Pr(ω) = 1/mn+1 for every such ω.
Check under the doormat.
For example, if m = n = 2, there are eight equally likely possibilities:



If h1 = h2 = h3 we make two unsuccessful probes before concluding that the new key K is not present; if h1 = h2 ≠ h3 we make none; and so on. This list of all possibilities shows that P has a probability distribution given by the pgf , when m = n = 2.
An unsuccessful search makes one probe for every item in list number hn+1, so we have the general formula



The probability that hj = hn+1 is 1/m, for 1 ≤ j ≤ n; so it follows that
EP = E[h1 = hn+1] + E[h2 =hn+1] + ... + E[hn =hn+1] = 
Maybe we should do that more slowly: Let Xj be the random variable
Xj = Xj(ω) = [hj = hn+1].
Then P = X1 + ··· + Xn, and EXj = 1/m for all j ≤ n; hence
EP = EX1 + ··· + EXn = n/m.
Good: As we had hoped, the average number of probes is 1/m times what it was without hashing. Furthermore the random variables Xj are independent, and they each have the same probability generating function


Xj(z)
=



therefore the pgf for the total number of probes in an unsuccessful search is



This is a binomial distribution, with p = 1/m and q = (m - 1)/m; in other words, the number of probes in an unsuccessful search behaves just like the number of heads when we toss a biased coin whose probability of heads is 1/m on each toss. Equation (8.61) tells us that the variance of P is therefore



When m is large, the variance of P is approximately n/m, so the standard deviation is approximately .


Case 2: The key is present.
Now let's look at successful searches. In this case the appropriate probability space is a bit more complicated, depending on our application: We will let Ω be the set of all elementary events



where hj is the hash value for the jth key as before, and where k is the index of the key being sought (the key whose hash value is hk). Thus we have 1 ≤ hj ≤ m for 1 ≤ j ≤ n, and 1 ≤ k ≤ n; there are mn · n elementary events ω in all.
Let sj be the probability that we are searching for the jth key that was inserted into the table. Then



if ω is the event (8.86). (Some applications search most often for the items that were inserted first, or for the items that were inserted last, so we will not assume that each sj = 1/n.) Notice that ∑ωΩ Pr(ω) =  sk = 1, hence (8.87) defines a legal probability distribution.
The number P of probes in a successful search is p if key K was the pth key to be inserted into its list. Therefore



or, if we let Xj be the random variable [hj = hk], we have



Suppose, for example, that we have m = 10 and n = 16, and that the hash values have the following "random" pattern:
(h1, . . . , h16) = 3 1 4 1 5 9 2 6 5 3 5 8 9 7 9 3;
(P1, . . . , P16) = 1 1 1 2 1 1 1 1 2 2 3 1 2 1 3 3.
Where have I seen that pattern before?
The number of probes needed to find the jth key is shown below hj.
Equation (8.89) represents P as a sum of random variables, but we can't simply calculate EP as EX1+· · ·+EXk because the quantity k itself is a random variable. What is the probability generating function for P? To answer this question we should digress a moment to talk about conditional probability.
Equation (8.43) was also a momentary digression.
If A and B are events in a probability space, we say that the conditional probability of A, given B, is



For example, if X and Y are random variables, the conditional probability of the event X = x, given that Y = y, is



For any fixed y in the range of Y, the sum of these conditional probabilities over all x in the range of X is Pr(Y = y)/Pr(Y = y) = 1; therefore (8.91) defines a probability distribution, and we can define a new random variable 'X|y' such that Pr((X|y) = x) = Pr(X = x | Y = y).
If X and Y are independent, the random variable X|y will be essentially the same as X, regardless of the value of y, because Pr(X = x | Y = y) is equal to Pr(X = x) by (8.5); that's what independence means. But if X and Y are dependent, the random variables X|y and X|y′ need not resemble each other in any way when y ≠ y′.
If X takes only nonnegative integer values, we can decompose its pgf into a sum of conditional pgf's with respect to any other random variable Y:



This holds because the coefficient of zx on the left side is Pr(X = x), for all x  X(Ω), and on the right it is



For example, if X is the product of the spots on two fair dice and if Y is the sum of the spots, the pgf for X|6 is



because the conditional probabilities for Y = 6 consist of five equally probable events . Equation (8.92) in this case reduces to



a formula that is obvious once you understand it. (End of digression.)
Oh, now I understand what mathematicians mean when they say something is "obvious," "clear," or "trivial."
In the case of hashing, (8.92) tells us how to write down the pgf for probes in a successful search, if we let X = P and Y = K. For any fixed k between 1 and n, the random variable P|k is defined as a sum of independent random variables X1 + · · · + Xk; this is (8.89). So it has the pgf
GP|k(z)  =  
Therefore the pgf for P itself is clearly



where



is the pgf for the search probabilities sk (divided by z for convenience).
"By clearly, I mean a good freshman should be able to do it, although it's not completely trivial."    —Paul Erd˝os [94].
Good. We have a probability generating function for P; we can now find the mean and variance by differentiation. It's somewhat easier to remove the z factor first, as we've done before, thus finding the mean and variance of P - 1 instead:



Therefore






These are general formulas expressing the mean and variance of the number of probes P in terms of the mean and variance of the assumed search distribution S.
For example, suppose we have sk = 1/n for 1 ≤ k ≤ n. This means we are doing a purely "random" successful search, with all keys in the table equally likely. Then S(z) is the uniform probability distribution Un(z) in (8.32), and we have Mean(S) = (n - 1)/2, Var(S) = (n2 - 1)/12. Hence






Once again we have gained the desired speedup factor of 1/m. If m ≈ n/ln n and n → ∞, the average number of probes per successful search in this case is about  ln n, and the standard deviation is asymptotically (ln n)/.
On the other hand, we might suppose that sk = (kHn)-1 for 1 ≤ k ≤ n; this distribution is called "Zipf's law." Then Mean (S) = n/Hn - 1, Var(S) = n(n + 1)/Hn - n2/. The average number of probes for m ≈ n/ln n as n → ∞ is approximately 2, with standard deviation asymptotic to .
In both cases the analysis allows the cautious souls among us, who fear the worst case, to rest easily: Chebyshev's inequality tells us that the lists will be nice and short, except in extremely rare cases.


Case 2, continued: Variants of the variance.
We have just computed the variance of the number of probes in a successful search, by considering P to be a random variable over a probability space with mn·n elements (h1, . . . , hn; k). But we could have adopted another point of view: Each pattern (h1, ..., hn) of hash values defines a random variable P|(h1, ..., hn), representing the probes we make in a successful search of a particular hash table on n given keys. The average value of P|(h1, ..., hn),



can be said to represent the running time of a successful search. This quantity A(h1, . . . , hn) is a random variable that depends only on (h1, . . . , hn), not on the final component k. We can write it in the form



where P(h1, . . . , hn; k) is defined in (8.88), since P|(h1, . . . , hn) = p with probability



OK, gang, time to put on your skim suits again.    — Friendly TA
The mean value of A(h1, . . . , hn), obtained by summing over all mn possibilities (h1, . . . , hn) and dividing by mn, will be the same as the mean value we obtained before in (8.95). But the variance of A(h1, . . . , hn) is something different; this is a variance of mn averages, not a variance of mn ·n probe counts. For example, if m = 1 (so that there is only one list), the "average" value A(h1, . . . , hn) = A(1, . . . , 1) is actually constant, so its variance VA is zero; but the number of probes in a successful search is not constant, so the variance VP is nonzero.
But the VP is nonzero only in an election year.
We can illustrate this difference between variances by carrying out the calculations for general m and n in the simplest case, when sk = 1/n for 1 ≤ k ≤ n. In other words, we will assume temporarily that there is a uniform distribution of search keys. Any given sequence of hash values (h1, . . . , hn) defines m lists that contain respectively (n1, n2, . . . , nm) entries for some numbers nj, where
n1 + n2 + · · · + nm = n.
A successful search in which each of the n keys in the table is equally likely will have an average running time of



probes. Our goal is to calculate the variance of this quantity A(h1, . . . , hn), over the probability space consisting of all mn sequences (h1, . . . , hn).
The calculations will be simpler, it turns out, if we compute the variance of a slightly different quantity,



We have
A(h1, ..., hn) = 1 + B(h1, ..., hn)/n,
hence the mean and variance of A satisfy



The probability that the list sizes will be n1, n2, . . . , nm is the multinomial coefficient



divided by mn; hence the pgf for B(h1, . . . , hn) is



This sum looks a bit scary to inexperienced eyes, but our experiences in Chapter 7 have taught us to recognize it as an m-fold convolution. Indeed, if we consider the exponential super-generating function



we can readily verify that G(w, z) is simply an mth power:



As a check, we can try setting z = 1; we get G(w, 1) = (ew)m, so the coefficient of mnwn/n! is Bn(1) = 1.
If we knew the values of  and , we would be able to calculate Var(Bn). So we take partial derivatives of G(w, z) with respect to z:



Complicated, yes; but everything simplifies greatly when we set z = 1. For example, we have



and it follows that



The expression for EA in (8.100) now gives EA = 1+(n-1)/2m, in agreement with (8.97).
The formula for  involves the similar sum



hence we find that



Now we can put all the pieces together and evaluate the desired variance VA. Massive cancellation occurs, and the result is surprisingly simple:



When such "coincidences" occur, we suspect that there's a mathematical reason; there might be another way to attack the problem, explaining why the answer has such a simple form. And indeed, there is another approach (in exercise 61), which shows that the variance of the average successful search has the general form



when sk is the probability that the kth-inserted element is being sought. Equation (8.103) is the special case sk = 1/n for 1 ≤ k ≤ n.
Besides the variance of the average, we might also consider the average of the variance. In other words, each sequence (h1, . . . , hn) that defines a hash table also defines a probability distribution for successful searching, and the variance of this probability distribution tells how spread out the number of probes will be in different successful searches. For example, let's go back to the case where we inserted n = 16 things into m = 10 lists:
(h1, . . . , h16) = 3 1 4 1 5 9 2 6 5 3 5 8 9 7 9 3;
(P1, . . . , P16) = 1 1 1 2 1 1 1 1 2 2 3 1 2 1 3 3.
Where have I seen that pattern before?
Where have I seen that graffito before?
IηνPπ.
A successful search in the resulting hash table has the pgf


G(3, 1, 4, 1, ..., 3)
=



 
=
s1z + s2z + s3z + s4z2 + · · · + s16z3.


We have just considered the average number of probes in a successful search of this table, namely A(3, 1, 4, 1, . . . , 3) = Mean (G(3, 1, 4, 1, . . . , 3)). We can also consider the variance,



This variance is a random variable, depending on (h1, . . . , hn), so it is natural to consider its average value.
In other words, there are three natural kinds of variance that we may wish to know, in order to understand the behavior of a successful search: The overall variance of the number of probes, taken over all (h1, . . . , hn) and k; the variance of the average number of probes, where the average is taken over all k and the variance is then taken over all (h1, . . . , hn); and the average of the variance of the number of the probes, where the variance is taken over all k and the average is then taken over all (h1, . . . , hn). In symbols, the overall variance is



the variance of the average is



and the average of the variance is



It turns out that these three quantities are interrelated in a simple way:



In fact, conditional probability distributions always satisfy the identity



if X and Y are random variables in any probability space and if X takes real values. (This identity is proved in exercise 22.) Equation (8.105) is the special case where X is the number of probes in a successful search and Y is the sequence of hash values (h1, . . . , hn).
The general equation (8.106) needs to be understood carefully, because the notation tends to conceal the different random variables and probability spaces in which expectations and variances are being calculated. For each y in the range of Y, we have defined the random variable X|y in (8.91), and this random variable has an expected value E(X|y) depending on y. Now E(X|Y) denotes the random variable whose values are E(X|y) as y ranges over all possible values of Y, and V(E(X|Y)) is the variance of this random variable with respect to the probability distribution of Y. Similarly, E(V(X|Y)) is the average of the random variables V(X|y) as y varies. On the left of (8.106) is VX, the unconditional variance of X. Since variances are nonnegative, we always have



(Now is a good time to do warmup exercise 6.)


Case 1, again: Unsuccessful search revisited.
Let's bring our microscopic examination of hashing to a close by doing one more calculation typical of algorithmic analysis. This time we'll look more closely at the total running time associated with an unsuccessful search, assuming that the computer will insert the previously unknown key into its memory.
The insertion process in (8.83) has two cases, depending on whether j is negative or zero. We have j < 0 if and only if P = 0, since a negative value comes from the FIRST entry of an empty list. Thus, if the list was previously empty, we have P = 0 and we must set FIRST[hn+1] := n + 1. (The new record will be inserted into position n + 1.) Otherwise we have P > 0 and we must set a NEXT entry to n + 1. These two cases may take different amounts of time; therefore the total running time for an unsuccessful search has the form



where α, β, and δ are constants that depend on the computer being used and on the way in which hashing is encoded in that machine's internal language. It would be nice to know the mean and variance of T, since such information is more relevant in practice than the mean and variance of P.
P is still the number of probes.
So far we have used probability generating functions only in connection with random variables that take nonnegative integer values. But it turns out that we can deal in essentially the same way with



when X is any real-valued random variable, because the essential characteristics of X depend only on the behavior of GX near z = 1, where powers of z are well defined. For example, the running time (8.108) of an unsuccessful search is a random variable, defined on the probability space of equally likely hash values (h1, . . . , hn, hn+1) with 1 ≤ hj ≤ m; we can consider the series



to be a pgf even when α, β, and δ are not integers. (In fact, the parameters α, β, δ are physical quantities that have dimensions of time; they aren't even pure numbers! Yet we can use them in the exponent of z.) We can still calculate the mean and variance of T, by evaluating  and  and combining these values in the usual way.
The generating function for P instead of T is



Therefore we have



The determination of Mean(GT ) and Var(GT ) is now routine:






In Chapter 9 we will learn how to estimate quantities like this when m and n are large. If, for example, m = n and n → ∞, the techniques of Chapter 9 will show that the mean and variance of T are respectively α + β + δe-1 + O(n-1) and β2 - 2βδe-1 + δ2(e-1 - e-2) + O(n-1). If m = n/ln n + O(1) and n → ∞ the corresponding results are


Mean(GT)
=
β ln n + α + O ([log n]2/n);


Var(GT)
=
β2 ln n + O ([log n]2/n).





Exercises

Warmups
1. What's the probability of doubles in the probability distribution Pr01 of (8.3), when one die is fair and the other is loaded? What's the probability that S = 7 is rolled?
2. What's the probability that the top and bottom cards of a randomly shuffled deck are both aces? (All 52! permutations have probability 1/52!.)
3. Stanford's Concrete Math students were asked in 1979 to flip coins until they got heads twice in succession, and to report the number of flips required. The answers were
3, 2, 3, 5, 10, 2, 6, 6, 9, 2 .
Princeton's Concrete Math students were asked in 1987 to do a similar thing, with the following results:
10, 2, 10, 7, 5, 2, 10, 6, 10, 2 .
Estimate the mean and variance, based on (a) the Stanford sample; (b) the Princeton sample.
Why only ten numbers?
The other students either weren't empiricists or they were just too flipped out.
4. Let H(z) = F(z)/G(z), where F(1) = G(1) = 1. Prove that
Mean(H) = Mean(F) - Mean(G),
Var(H) = Var(F) - Var(G),
in analogy with (8.38) and (8.39), if the indicated derivatives exist at z = 1.
5. Suppose Alice and Bill play the game (8.78) with a biased coin that comes up heads with probability p. Is there a value of p for which the game becomes fair?
6. What does the conditional variance law (8.106) reduce to, when X and Y are independent random variables?


Basics
7. Show that if two dice are loaded with the same probability distribution, the probability of doubles is always at least .
8. Let A and B be events such that A ∪ B = Ω. Prove that
Pr(ω  A ∩ B) = Pr(ω  A) Pr(ω  B) - Pr(ω ∉ A) Pr(ω ∉ B) .
9. Prove or disprove: If X and Y are independent random variables, then so are F(X) and G(Y), when F and G are any functions.
10. What's the maximum number of elements that can be in the median set of a random variable X, according to definition (8.7)?
11. Construct a random variable that has finite mean and infinite variance.
12. a If P(z) is the pgf for the random variable X, prove that
Pr(X  ≤  r)  ≤  x-rP(x)    for 0 < x ≤ 1;
Pr(X  ≥  r)  ≤  x-rP(x)    for x ≥ 1.
(These important relations are called the tail inequalities.)
b In the special case P(z) = (1 + z)n/2n, use the first tail inequality to prove that ∑k≤αn() ≤ 1/ααn(1 - α)(1-α)n when 0 < α < .
13. If X1, . . . , X2n are independent random variables with the same distribution, and if α is any real number whatsoever, prove that



14. Let F(z) and G(z) be probability generating functions, and let
H(z) = p F(z) + q G(z)
where p + q = 1. (This is called a mixture of F and G; it corresponds to flipping a coin and choosing probability distribution F or G depending on whether the coin comes up heads or tails.) Find the mean and variance of H in terms of p, q, and the mean and variance of F and G.
15. If F(z) and G(z) are probability generating functions, we can define another pgf H(z) by "composition":
H(z) = F(G(z)).
Express Mean(H) and Var(H) in terms of Mean(F), Var(F), Mean(G), and Var(G). (Equation (8.93) is a special case.)
16. Find a closed form for the super generating function Σn≥0 Fn(z)wn, when Fn(z) is the football-fixation generating function defined in (8.53).
17. Let Xn,p and Yn,p have the binomial and negative binomial distributions, respectively, with parameters (n, p). (These distributions are defined in (8.57) and (8.60).) Prove that Pr(Yn,p ≤ m) = Pr(Xm+n,p ≥ n). What identity in binomial coefficients does this imply?
18. A random variable X is said to have the Poisson distribution with mean μ if Pr(X = k) = e-μμk/k! for all k ≥ 0.
a What is the pgf of such a random variable?
b What are its mean, variance, and other cumulants?
The distribution of fish per unit volume of water.
19. Continuing the previous exercise, let X1 be a random Poisson variable with mean μ1, and let X2 be a random Poisson variable with mean μ2, independent of X1.
a What is the probability that X1 + X2 = n?
b What are the mean, variance, and other cumulants of 2X1 + 3X2?
20. Prove (8.74) and (8.75), the general formulas for mean and variance of the time needed to wait for a given pattern of heads and tails.
21. What does the value of N represent, if H and T are both set equal to  in (8.77)?
22. Prove (8.106), the law of conditional expectations and variances.


Homework exercises
23. Let Pr00 be the probability distribution of two fair dice, and let Pr11 be the probability distribution of two loaded dice as given in (8.2). Find all events A such that Pr00(A) = Pr11(A). Which of these events depend only on the random variable S? (A probability space with Ω = D2 has 236 events; only 211 of those events depend on S alone.)
24. Player J rolls 2n + 1 fair dice and removes those that come up . Player K then calls a number between 1 and 6, rolls the remaining dice, and removes those that show the number called. This process is repeated until no dice remain. The player who has removed the most total dice (n + 1 or more) is the winner.
a. What are the mean and variance of the total number of dice that J removes? Hint: The dice are independent.
b. What's the probability that J wins, when n = 2?
25. Consider a gambling game in which you stake a given amount A and you roll a fair die. If k spots turn up, you multiply your stake by 2(k - 1)/5. (In particular, you double the stake whenever you roll , but you lose everything if you roll .) You can stop at any time and reclaim the current stake. What are the mean and variance of your stake after n rolls? (Ignore any effects of rounding to integer amounts of currency.)
26. Find the mean and variance of the number of l-cycles in a random permutation of n elements. (The football victory problem discussed in (8.23), (8.24), and (8.53) is the special case l = 1.)
27. Let X1, X2, . . . , Xn be independent samples of the random variable X. Equations (8.19) and (8.20) explain how to estimate the mean and variance of X on the basis of these observations; give an analogous formula for estimating the third cumulant κ3. (Your formula should be an "unbiased" estimate, in the sense that its expected value should be κ3.)
28. What is the average length of the coin-flipping game (8.78),
a given that Alice wins?
b given that Bill wins?
29. Alice, Bill, and Computer flip a fair coin until one of the respective patterns A = HHTH, B = HTHH, or C = THHH appears for the first time. (If only two of these patterns were involved, we know from (8.82) that A would probably beat B, that B would probably beat C, and that C would probably beat A; but all three patterns are simultaneously in the game.) What are each player's chances of winning?
30. The text considers three kinds of variances associated with successful search in a hash table. Actually there are two more: We can consider the average (over k) of the variances (over h1, . . . , hn) of P(h1, . . . , hn; k); and we can consider the variance (over k) of the averages (over h1, . . . , hn). Evaluate these quantities.
31. An apple is located at vertex A of pentagon ABCDE, and a worm is located two vertices away, at C. Every day the worm crawls with equal probability to one of the two adjacent vertices. Thus after one day the worm is at vertex B or vertex D, each with probability . After two days, the worm might be back at C again, because it has no memory of previous positions. When it reaches vertex A, it stops to dine.
Schrödinger's worm.
a What are the mean and variance of the number of days until dinner?
b Let p be the probability that the number of days is 100 or more. What does Chebyshev's inequality say about p?
c What do the tail inequalities (exercise 12) tell us about p?
32. Alice and Bill are in the military, stationed in one of the five states Kansas, Nebraska, Missouri, Oklahoma, or Colorado. Initially Alice is in Nebraska and Bill is in Oklahoma. Every month each person is reassigned to an adjacent state, each adjacent state being equally likely. (Here's a diagram of the adjacencies:



The initial states are circled.) For example, Alice is restationed after the first month to Colorado, Kansas, or Missouri, each with probability 1/3. Find the mean and variance of the number of months it takes Alice and Bill to find each other. (You may wish to enlist a computer's help.)
Definitely a finite-state situation.
33. Are the random variables X1 and X2 in (8.89) independent?
34. Gina is a golfer who has probability p = .05 on each stroke of making a "supershot" that gains a stroke over par, probability q = .91 of making an ordinary shot, and probability r = .04 of making a "subshot" that costs her a stroke with respect to par. (Non-golfers: At each turn she advances 2, 1, or 0 steps toward her goal, with probability p, q, or r, respectively. On a par-m hole, her score is the minimum n such that she has advanced m or more steps after taking n turns. A low score is better than a high score.)
a. Show that Gina wins a par-4 hole more often than she loses, when she plays against a player who shoots par. (In other words, the probability that her score is less than 4 is greater than the probability that her score is greater than 4.)
b. Show that her average score on a par-4 hole is greater than 4. (Therefore she tends to lose against a "steady" player on total points, although she would tend to win in match play by holes.)
(Use a calculator for the numerical work on this problem.)


Exam problems
35. A die has been loaded with the probability distribution



Let Sn be the sum of the spots after this die has been rolled n times. Find a necessary and sufficient condition on the "loading distribution" such that the two random variables Sn mod 2 and Sn mod 3 are independent of each other, for all n.
36. The six faces of a certain die contain the spot patterns



instead of the usual  through .
a. Show that there is a way to assign spots to the six faces of another die so that, when these two dice are thrown, the sum of spots has the same probability distribution as the sum of spots on two ordinary dice. (Assume that all 36 face pairs are equally likely.)
b. Generalizing, find all ways to assign spots to the 6n faces of n dice so that the distribution of spot sums will be the same as the distribution of spot sums on n ordinary dice. (Each face should receive a positive integer number of spots.)
37. Let pn be the probability that exactly n tosses of a fair coin are needed before heads are seen twice in a row, and let qn = Σk≥n pk. Find closed forms for both pn and qn in terms of Fibonacci numbers.
38. What is the probability generating function for the number of times you need to roll a fair die until all six faces have turned up? Generalize to m-sided fair dice: Give closed forms for the mean and variance of the number of rolls needed to see l of the m faces. What is the probability that this number will be exactly n?
39. A Dirichlet probability generating function has the form



Thus P(0) = 1. If X is a random variable with Pr(X = n) = pn, express E(X), V(X), and E(ln X) in terms of P(z) and its derivatives.
40. The mth cumulant κm of the binomial distribution (8.57) has the form nfm(p), where fm is a polynomial of degree m. (For example, f1(p) = p and f2(p) = p - p2, because the mean and variance are np and npq.)
a Find a closed form for the coefficient of pk in fm(p).
b Prove that fm() = (2m - 1)Bm/m + [m = 1], where Bm is the mth Bernoulli number.
41. Let the random variable Xn be the number of flips of a fair coin until heads have turned up a total of n times. Show that  = (-1)n(ln 2 + Hn/2 - Hn).. Use the methods of Chapter 9 to estimate this value with an absolute error of O(n-3).
42. A certain man has a problem finding work. If he is unemployed on any given morning, there's constant probability ph (independent of past history) that he will be hired before that evening; but if he's got a job when the day begins, there's constant probability pf that he'll be laid off by nightfall. Find the average number of evenings on which he will have a job lined up, assuming that he is initially employed and that this process goes on for n days. (For example, if n = 1 the answer is 1 - pf.)
Does TEX choose optimal line breaks?
43. Find a closed form for the pgf Gn(z) = Σk≥0pk,nZk, where pk,n is the probability that a random permutation of n objects has exactly k cycles. What are the mean and standard deviation of the number of cycles?
44. The athletic department runs an intramural "knockout tournament" for 2n tennis players as follows. In the first round, the players are paired off randomly, with each pairing equally likely, and 2n-1 matches are played. The winners advance to the second round, where the same process produces 2n-2 winners. And so on; the kth round has 2n-k randomly chosen matches between the 2n-k+1 players who are still undefeated. The nth round produces the champion. Unbeknownst to the tournament organizers, there is actually an ordering among the players, so that x1 is best, x2 is second best, . . . , x2n is worst. When xj plays xk and j < k, the winner is xj with probability p and xk with probability 1 - p, independent of the other matches. We assume that the same probability p applies to all j and k.
a. What's the probability that x1 wins the tournament?
b. What's the probability that the nth round (the final match) is between the top two players, x1 and x2?
c. What's the probability that the best 2k players are the competitors in the kth-to-last round? (The previous questions were the cases k = 0 and k = 1.)
d. Let N(n) be the number of essentially different tournament results; two tournaments are essentially the same if the matches take place between the same players and have the same winners. Prove that N(n) = 2n!.
e. What's the probability that x2 wins the tournament?
f. Prove that if  < p < 1, the probability that xj wins is strictly greater than the probability that xj+1 wins, for 1 ≤ j < 2n.
A peculiar set of tennis players.
45. True sherry is made in Spain according to a multistage system called "Solera." For simplicity we'll assume that the winemaker has only three barrels, called A, B, and C. Every year a third of the wine from barrel C is bottled and replaced by wine from B; then B is topped off with a third of the wine from A; finally A is topped off with new wine. Let A(z), B(z), C(z) be probability generating functions, where the coefficient of zn is the fraction of n-year-old wine in the corresponding barrel just after the transfers have been made.
a. Assume that the operation has been going on since time immemorial, so that we have a steady state in which A(z), B(z), and C(z) are the same at the beginning of each year. Find closed forms for these generating functions.
b. Find the mean and standard deviation of the age of the wine in each barrel, under the same assumptions. What is the average age of the sherry when it is bottled? How much of it is exactly 25 years old?
c. Now take the finiteness of time into account: Suppose that all three barrels contained new wine at the beginning of year 0. What is the average age of the sherry that is bottled at the beginning of year n?
"Une rapide opération arithmétique montre que, grâce à cette ingénieuse cascade, les xérès ont toujours au moins trois ans. Pousser plus loin le calcul de leur âge donne le vertige."—Revue du vin de France (Nov 1984)
46. Stefan Banach used to carry two boxes of matches, each containing n matches initially. Whenever he needed a light he chose a box at random, each with probability , independent of his previous choices. After taking out a match he'd put the box back in its pocket (even if the box became empty—all famous mathematicians used to do this). When his chosen box was empty he'd throw it away and reach for the other box.
a. Once he found that the other box was empty too. What's the probability that this occurs? (For n = 1 it happens half the time and for n = 2 it happens 3/8 of the time.) To answer this part, find a closed form for the generating function P(w, z) = Σm,n pm,nwmzn, where pm,n is the probability that, starting with m matches in one box and n in the other, both boxes are empty when an empty box is first chosen. Then find a closed form for pn,n.
b. Generalizing your answer to part (a), find a closed form for the probability that exactly k matches are in the other box when an empty one is first thrown away.
c. Find a closed form for the average number of matches in that other box.
And for the number in the empty box.
47. Some physicians, collaborating with some physicists, recently discovered a pair of microbes that reproduce in a peculiar way. The male microbe, called a diphage, has two receptors on its surface; the female microbe, called a triphage, has three:



When a culture of diphages and triphages is irradiated with a psi-particle, exactly one of the receptors on one of the phages absorbs the particle; each receptor is equally likely. If it was a diphage receptor, that diphage changes to a triphage; if it was a triphage receptor, that triphage splits into two diphages. Thus if an experiment starts with one diphage, the first psi-particle changes it to a triphage, the second particle splits the triphage into two diphages, and the third particle changes one of the diphages to a triphage. The fourth particle hits either the diphage or the triphage; then there are either two triphages (probability ) or three diphages (probability ). Find a closed form for the average number of diphages present, if we begin with a single diphage and irradiate the culture n times with single psi-particles.
48. Five people stand at the vertices of a pentagon, throwing frisbees to each other.



Or, if this pentagon is in Arlington, throwing missiles at each other.
They have two frisbees, initially at adjacent vertices as shown. In each time interval, each frisbee is thrown either to the left or to the right (along an edge of the pentagon) with equal probability. This process continues until one person is the target of two frisbees simultaneously; then the game stops. (All throws are independent of past history.)
a. Find the mean and variance of the number of pairs of throws.
b. Find a closed form for the probability that the game lasts more than 100 steps, in terms of Fibonacci numbers.
Frisbee is a trademark of Wham-O Manufacturing Company.
49. Luke Snowwalker spends winter vacations at his mountain cabin. The front porch has m pairs of boots and the back porch has n pairs. Every time he goes for a walk he flips a (fair) coin to decide whether to leave from the front porch or the back, and he puts on a pair of boots at that porch and heads off. There's a 50/50 chance that he returns to each porch, independent of his starting point, and he leaves the boots at the porch he returns to. Thus after one walk there will be m + [-1, 0, or +1] pairs on the front porch and n - [-1, 0, or +1] pairs on the back porch. If all the boots pile up on one porch and if he decides to leave from the other, he goes without boots and gets frostbite, ending his vacation. Assuming that he continues his walks until the bitter end, let PN(m, n) be the probability that he completes exactly N nonfrostbitten trips, starting with m pairs on the front porch and n on the back. Thus, if both m and n are positive,



this follows because this first trip is either front/back, front/front, back/back, or back/front, each with probability , and N - 1 trips remain.
a. Complete the recurrence for PN(m, n) by finding formulas that hold when m = 0 or n = 0. Use the recurrence to obtain equations that hold among the probability generating functions



b. Differentiate your equations and set z = 1, thereby obtaining relations among the quantities ,n(1). Solve these equations, thereby determining the mean number of trips before frostbite.
c. Show that gm,n has a closed form if we substitute z = 1/cos2 θ:



50. Consider the function



The purpose of this problem is to prove that H(z) = Σk≥0hkZk is a probability generating function, and to obtain some basic facts about it.
a. Let (1 - z)3/2(9 - z)1/2 = Σk≥0 ckzk. Prove that c0 = 3, c1 = -14/3, c2 = 37/27, and  for all l ≥ 0. Hint: Use the identity
(9 - z)1/2  =  3(1 - z)1/2 (1 +  z/(1 - z))1/2
and expand the last factor in powers of z/(1 - z).
b. Use part (a) and exercise 5.81 to show that the coefficients of H(z) are all positive.
c. Prove the amazing identity



d. What are the mean and variance of H?
51. The state lottery in El Dorado uses the payoff distribution H defined in the previous problem. Each lottery ticket costs 1 doubloon, and the payoff is k doubloons with probability hk. Your chance of winning with each ticket is completely independent of your chance with other tickets; in other words, winning or losing with one ticket does not affect your probability of winning with any other ticket you might have purchased in the same lottery.
a. Suppose you start with one doubloon and play this game. If you win k doubloons, you buy k tickets in the second game; then you take the total winnings in the second game and apply all of them to the third; and so on. If none of your tickets is a winner, you're broke and you have to stop gambling. Prove that the pgf of your current holdings after n rounds of such play is



b. Let gn be the probability that you lose all your money for the first time on the nth game, and let G(z) = g1z + g2z2 + · · · . Prove that G(1) = 1. (This means that you're bound to lose sooner or later, with probability 1, although you might have fun playing in the meantime.) What are the mean and the variance of G?
c. What is the average total number of tickets you buy, if you continue to play until going broke?
d. What is the average number of games until you lose everything if you start with two doubloons instead of just one?
A doubledoubloon.


Bonus problems
52. Show that the text's definitions of median and mode for random variables correspond in some meaningful sense to the definitions of median and mode for sequences, when the probability space is finite.
53. Prove or disprove: If X, Y, and Z are random variables with the property that all three pairs (X, Y), (X, Z), and (Y, Z) are independent, then X + Y is independent of Z.
54. Equation (8.20) proves that the average value of X is VX. What is the variance X?
55. A normal deck of playing cards contains 52 cards, four each with face values in the set {A, 2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K}. Let X and Y denote the respective face values of the top and bottom cards, and consider the following algorithm for shuffling:
S1 Permute the deck randomly so that each arrangement occurs with probability 1/52!.
S2 If X ≠ Y, flip a biased coin that comes up heads with probability p, and go back to step S1 if heads turns up. Otherwise stop.
Each coin flip and each permutation is assumed to be independent of all the other randomizations. What value of p will make X and Y independent random variables after this procedure stops?
56. Generalize the frisbee problem of exercise 48 from a pentagon to an m-gon. What are the mean and variance of the number of collision-free throws in general, when the frisbees are initially at adjacent vertices? Show that, if m is odd, the pgf for the number of throws can be written as a product of coin-flipping distributions:



Hint: Try the substitution z = 1/cos2 θ.
57. Prove that the Penney-ante pattern τ1τ2 . . . τl-1τl is always inferior to the pattern  when a fair coin is flipped, if l ≥ 3.
58. Is there any sequence A = τ1τ2 . . . τl-1τl of l ≥ 3 heads and tails such that the sequences Hτ1τ2 . . . τl-1 and Tτ1τ2 . . . τl-1 both perform equally well against A in the game of Penney ante?
59. Are there patterns A and B of heads and tails such that A is longer than B, yet A appears before B more than half the time when a fair coin is being flipped?
60. Let k and n be fixed positive integers with k < n.
a. Find a closed form for the probability generating function



for the joint distribution of the numbers of probes needed to find the kth and nth items that have been inserted into a hash table with m lists.
b. Although the random variables P(h1, . . . , hn; k) and P(h1, . . . , hn; n) are dependent, show that they are somewhat independent:
E(P(h1, . . . , hn; k)P(h1, . . . , hn; n))     =   (EP(h1, . . . , hn; k)) (EP(h1, . . . , hn; n))
61. Use the result of the previous exercise to prove (8.104).
62. Continuing exercise 47, find the variance of the number of diphages after n irradiations.


Research problem
63. The normal distribution is a non-discrete probability distribution characterized by having all its cumulants zero except the mean and the variance. Is there an easy way to tell if a given sequence of cumulants κ1, κ2, κ3, . . .  comes from a discrete distribution? (All the probabilities must be "atomic" in a discrete distribution.)











9. Asymptotics
Exact answers are great when we can find them; there's something very satisfying about complete knowledge. But there's also a time when approximations are in order. If we run into a sum or a recurrence whose solution doesn't have a closed form (as far as we can tell), we still would like to know something about the answer; we don't have to insist on all or nothing. And even if we do have a closed form, our knowledge might be imperfect, since we might not know how to compare it with other closed forms.
Uh oh . . . here comes that A-word.
For example, there is (apparently) no closed form for the sum



But it is nice to know that



we say that the sum is "asymptotic to" . It's even nicer to have more detailed information, like



which gives us a "relative error of order 1/n2." But even this isn't enough to tell us how big Sn is, compared with other quantities. Which is larger, Sn or the Fibonacci number F4n? Answer: We have S2 = 22 > F8 = 21 when n = 2; but F4n is eventually larger, because F4n ∼ ϕ4n/  and ϕ4 ≈ 6.8541, while



Our goal in this chapter is to learn how to understand and to derive results like this without great pain.
Other words like 'symptom' and 'ptomaine' also come from this root.
The word asymptotic stems from a Greek root meaning "not falling together." When ancient Greek mathematicians studied conic sections, they considered hyperbolas like the graph of ,



which has the lines y = x and y = -x as "asymptotes." The curve approaches but never quite touches these asymptotes, when x → ∞. Nowadays we use "asymptotic" in a broader sense to mean any approximate value that gets closer and closer to the truth, when some parameter approaches a limiting value. For us, asymptotics means "almost falling together."
Some asymptotic formulas are very difficult to derive, well beyond the scope of this book. We will content ourselves with an introduction to the subject; we hope to acquire a suitable foundation on which further techniques can be built. We will be particularly interested in understanding the definitions of '∼' and 'O' and similar symbols, and we'll study basic ways to manipulate asymptotic quantities.

9.1 A Hierarchy
Functions of n that occur in practice usually have different "asymptotic rates of growth"; one of them will approach infinity faster than another. We formalize this by saying that



This relation is transitive: If f(n) ≺ g(n) and g(n) ≺ h(n) then f(n) ≺ h(n). We also may write g(n) ≻ f(n) if f(n) ≺ g(n). This notation was introduced in 1871 by Paul du Bois-Reymond [85].
All functions great and small.
For example, n ≺ n2; informally we say that n grows more slowly than n2. In fact,



when α and β are arbitrary real numbers.
There are, of course, many functions of n besides powers of n. We can use the ≺ relation to rank lots of functions into an asymptotic pecking order that includes entries like this:
1 ≺ log log n ≺ log n ≺ n ≺ nc ≺ nlog n ≺ cn ≺ nn ≺ ccn .
(Here  and c are arbitrary constants with 0 <  < 1 < c.)
All functions listed here, except 1, go to infinity as n goes to infinity. Thus when we try to place a new function in this hierarchy, we're not trying to determine whether it becomes infinite but rather how fast.
It helps to cultivate an expansive attitude when we're doing asymptotic analysis: We should THINK BIG, when imagining a variable that approaches infinity. For example, the hierarchy says that log n ≺ n0.0001; this might seem wrong if we limit our horizons to teeny-tiny numbers like one googol, n = 10100. For in that case, log n = 100, while n0.0001 is only 100.01 ≈ 1.0233. But if we go up to a googolplex, n = 1010100, then log n = 10100 pales in comparison with n0.0001 = 101096.
Even if  is extremely small (smaller than, say, 1/1010100), the value of log n will be much smaller than the value of n, if n is large enough. For if we set n = 10102k, where k is so large that  ≥ 10-k, we have log n = 102k but n ≥ 1010k. The ratio (log n)/n therefore approaches zero as n → ∞.
A loerarchy?
The hierarchy shown above deals with functions that go to infinity. Often, however, we're interested in functions that go to zero, so it's useful to have a similar hierarchy for those functions. We get one by taking reciprocals, because when f(n) and g(n) are never zero we have



Thus, for example, the following functions (except 1) all go to zero:



Let's look at a few other functions to see where they fit in. The number π(n) of primes less than or equal to n is known to be approximately n/ln n. Since 1/n ≺ 1/ln n ≺ 1, multiplying by n tells us that
n1- ≺ π(n) ≺ n.
We can in fact generalize (9.4) by noticing, for example, that



Here '(α1, α2, α3) < (β1, β2, β3)' means lexicographic order (dictionary order); in other words, either α1 < β1, or α1 = β1 and α2 < β2, or α1 = β1 and α2 = β2 and α3 < β3.
How about the function ; where does it live in the hierarchy? We can answer questions like this by using the rule



which follows in two steps from definition (9.3) by taking logarithms. Consequently
1 ≺ f(n) ≺ g(n) = e|f (n)| ≺ e|g(n)| .
And since , we have .
When two functions f(n) and g(n) have the same rate of growth, we write 'f(n) ≍ g(n)'. The official definition is:



This holds, for example, if f(n) = cos n + arctan n and g(n) is a nonzero constant. We will prove shortly that it holds whenever f(n) and g(n) are polynomials of the same degree. There's also a stronger relation, defined by the rule



In this case we say that "f(n) is asymptotic to g(n)."
G. H. Hardy [179] introduced an interesting and important concept called the class of logarithmico-exponential functions, defined recursively as the smallest family  of functions satisfying the following properties:
• The constant function f(n) = α is in , for all real α.
• The identity function f(n) = n is in .
• If f(n) and g(n) are in , so is f(n) - g(n).
• If f(n) is in , so is ef(n).
• If f(n) is in  and is "eventually positive," then ln f(n) is in .
A function f(n) is called "eventually positive" if there is an integer n0 such that f(n) > 0 whenever n ≥ n0.
We can use these rules to show, for example, that f(n) + g(n) is in  whenever f(n) and g(n) are, because f(n) + g(n) = f(n) - (0 - g(n)). If f(n) and g(n) are eventually positive members of , their product f(n)g(n) = e ln f(n)+ln g(n) and quotient f(n)/g(n) = e ln f(n)-ln g(n) are in ; so are functions like , etc. Hardy proved that every logarithmico-exponential function is eventually positive, eventually negative, or identically zero. Therefore the product and quotient of any two -functions are in , except that we cannot divide by a function that's identically zero.
Hardy's main theorem about logarithmico-exponential functions is that they form an asymptotic hierarchy: If f(n) and g(n) are any functions in , then either f(n) ≺ g(n), or f(n) ≻ g(n), or f(n) ≍ g(n). In the last case there is, in fact, a constant α such that
f(n) ∼ α g(n).
The proof of Hardy's theorem is beyond the scope of this book; but it's nice to know that the theorem exists, because almost every function we ever need to deal with is in . In practice, we can generally fit a given function into a given hierarchy without great difficulty.


9.2 O Notation
A wonderful notational convention for asymptotic analysis was introduced by Paul Bachmann in 1894 and popularized in subsequent years by Edmund Landau and others. We have seen it in formulas like



which tells us that the nth harmonic number is equal to the natural logarithm of n plus Euler's constant, plus a quantity that is "Big Oh of 1 over n." This last quantity isn't specified exactly; but whatever it is, the notation claims that its absolute value is no more than a constant times 1/n.
". . . , wenn wir durch das Zeichen O(n) eine Grosse ausdrucken, deren Ordnung in Bezug auf n die Ordnung von n nicht uberschreitet; ob sie wirklich Glieder von der Ordnung n in sich enthalt, bleibt bei dem bisherigen Schlussverfahren dahingestellt."
—P. Bachmann [17]
The beauty of O-notation is that it suppresses unimportant detail and lets us concentrate on salient features: The quantity O(1/n) is negligibly small, if constant multiples of 1/n are unimportant.
Furthermore we get to use O right in the middle of a formula. If we want to express (9.10) in terms of the notations in Section 9.1, we must transpose 'ln n + γ' to the left side and specify a weaker result like
Hn - ln n - γ ≺ 
or a stronger result like
Hn - ln n - γ ≍ 
The Big Oh notation allows us to specify an appropriate amount of detail in place, without transposition.
The idea of imprecisely specified quantities can be made clearer if we consider some additional examples. We occasionally use the notation '±1' to stand for something that is either +1 or -1; we don't know (or perhaps we don't care) which it is, yet we can manipulate it in formulas.
N. G. de Bruijn begins his book Asymptotic Methods in Analysis [74] by considering a Big Ell notation that helps us understand Big Oh. If we write L(5) for a number whose absolute value is less than 5 (but we don't say what the number is), then we can perform certain calculations without knowing the full truth. For example, we can deduce formulas such as 1 + L(5) = L(6); L(2) + L(3) = L(5); L(2)L(3) = L(6); eL(5) = L(e5); and so on. But we cannot conclude that L(5) - L(3) = L(2), since the left side might be 4 - 0. In fact, the most we can say is L(5) - L(3) = L(8).
It's not nonsense, but it is pointless.
Bachmann's O-notation is similar to L-notation but it's even less precise: O(α) stands for a number whose absolute value is at most a constant times |α|. We don't say what the number is and we don't even say what the constant is. Of course the notion of a "constant" is nonsense if there is nothing variable in the picture, so we use O-notation only in contexts when there's at least one quantity (say n) whose value is varying. The formula



means in this context that there is a constant C such that



and when O (g(n)) stands in the middle of a formula it represents a function f(n) that satisfies (9.12). The values of f(n) are unknown, but we do know that they aren't too large. Similarly, the quantity L(n) above represents an unspecified function f(n) whose values satisfy |f(n)| < |n|. The main difference between L and O is that O-notation involves an unspecified constant C; each appearance of O might involve a different C, but each C is independent of n.
For example, we know that the sum of the first n squares is



We can write
□n = O(n3)
because  for all integers n. Similarly, we have the more specific formula
□n = n3 + O(n2);
we can also be sloppy and throw away information, saying that
□n = O(n10).
Nothing in the definition of O requires us to give a best possible bound.
I've got a little list—I've got a little list, Of annoying terms and details that might well be under ground, And that never would be missed—that never would be missed.
But wait a minute. What if the variable n isn't an integer? What if we have a formula like S(x) =  x3 +  x2 + x, where x is a real number? Then we cannot say that S(x) = O(x3), because the ratio S(x)/x3 =  +  x-1 +  x-2 becomes unbounded when x → 0. And we cannot say that S(x) = O(x), because the ratio S(x)/x =  x2 +  x +  becomes unbounded when x → ∞. So we apparently can't use O-notation with S(x).
The answer to this dilemma is that variables used with O are generally subject to side conditions. For example, if we stipulate that |x| ≥ 1, or that x ≥  where  is any positive constant, or that x is an integer, then we can write S(x) = O(x3). If we stipulate that |x| ≤ 1, or that |x| ≤ c where c is any positive constant, then we can write S(x) = O(x). The O-notation is governed by its environment, by constraints on the variables involved.
You are the fairest of your sex, Let me be your hero; I love you as one over x, As x approaches zero.
—Michael Stueben Positively.
These constraints are often specified by a limiting relation. For example, we might say that



This means that the O-condition is supposed to hold when n is "near" ∞; we don't care what happens unless n is quite large. Moreover, we don't even specify exactly what "near" means; in such cases each appearance of O implicitly asserts the existence of two constants C and n0, such that



The values of C and n0 might be different for each O, but they do not depend on n. Similarly, the notation


f(x) = O (g(x))
as x → 0


means that there exist two constants C and  such that



The limiting value does not have to be ∞ or 0; we can write


ln z = z - 1 + O ((z - 1)2)
as z → 1,


because it can be proved that | ln z - z + 1| ≤ |z - 1|2 when |z - 1| ≤ .
Our definition of O has gradually developed, over a few pages, from something that seemed pretty obvious to something that seems rather complex; we now have O representing an undefined function and either one or two unspecified constants, depending on the environment. This may seem complicated enough for any reasonable notation, but it's still not the whole story! Another subtle consideration lurks in the background. Namely, we need to realize that it's fine to write
 n3 +  n2 +  n = O(n3),
but we should never write this equality with the sides reversed. Otherwise we could deduce ridiculous things like n = n2 from the identities n = O(n2) and n2 = O(n2). When we work with O-notation and any other formulas that involve imprecisely specified quantities, we are dealing with one-way equalities. The right side of an equation does not give more information than the left side, and it may give less; the right is a "crudification" of the left.
"And to auoide the tediouse repetition of these woordes: is equalle to: I will sette as I doe often in woorke use, a paire of paralleles, or Gemowe lines of one lengthe, thus: ====, bicause noe .2. thynges, can be moare equalle."
—R. Recorde [305]
From a strictly formal point of view, the notation O (g(n)) does not stand for a single function f(n), but for the set of all functions f(n) such that |f(n)| ≤ C| g(n)| for some constant C. An ordinary formula g(n) that doesn't involve O-notation stands for the set containing a single function f(n) = g(n). If S and T are sets of functions of n, the notation S + T stands for the set of all functions of the form f(n) + g(n), where f(n)  S and g(n)  T; other notations like S-T, ST, S/T, , eS, ln S are defined similarly. Then an "equation" between such sets of functions is, strictly speaking, a set inclusion; the '=' sign really means '⊆'. These formal definitions put all of our O manipulations on firm logical ground.
For example, the "equation"
n3 + O(n2) = O(n3)
means that S1 ⊆ S2, where S1 is the set of all functions of the form n3 + f1(n) such that there exists a constant C1 with |f1(n)| ≤ C1|n2|, and where S2 is the set of all functions f2(n) such that there exists a constant C2 with |f2(n)| ≤ C2|n3|. We can formally prove this "equation" by taking an arbitrary element of the left-hand side and showing that it belongs to the right-hand side: Given n3 + f1 (n) such that |f1(n)| ≤ C1|n2|, we must prove that there's a constant C2 such that n3 + f1 (n)| ≤ C2|n3|. The constant C2 =  + C1 does the trick, since n2 ≤ |n3| for all integers n.
If '=' really means '⊆', why don't we use '⊆' instead of abusing the equals sign? There are four reasons.
First, tradition. Number theorists started using the equals sign with O-notation and the practice stuck. It's sufficiently well established by now that we cannot hope to get the mathematical community to change.
Second, tradition. Computer people are quite used to seeing equals signs abused—for years FORTRAN and BASIC programmers have been writing assignment statements like 'N = N + 1'. One more abuse isn't much.
Third, tradition. We often read '=' as the word 'is'. For instance we verbalize the formula Hn = O(log n) by saying "H sub n is Big Oh of log n." And in English, this 'is' is one-way. We say that a bird is an animal, but we don't say that an animal is a bird; "animal" is a crudification of "bird."
Fourth, for our purposes it's natural. If we limited our use of O-notation to situations where it occupies the whole right side of a formula—as in the harmonic number approximation Hn = O(log n), or as in the description of a sorting algorithm's running time T(n) = O(n log n)—it wouldn't matter whether we used '=' or something else. But when we use O-notation in the middle of an expression, as we usually do in asymptotic calculations, our intuition is well satisfied if we think of the equals sign as an equality, and if we think of something like O(1/n) as a very small quantity.
"It is obvious that the sign = is really the wrong sign for such relations, because it suggests symmetry, and there is no such symmetry. . . . Once this warning has been given, there is, however, not much harm in using the sign =, and we shall maintain it, for no other reason than that it is customary."
—N. G. de Bruijn [74]
So we'll continue to use '=', and we'll continue to regard O (g(n)) as an incompletely specified function, knowing that we can always fall back on the set-theoretic definition if we must.
But we ought to mention one more technicality while we're picking nits about definitions: If there are several variables in the environment, O-notation formally represents sets of functions of two or more variables, not just one. The domain of each function is every variable that is currently "free" to vary.
This concept can be a bit subtle, because a variable might be defined only in parts of an expression, when it's controlled by a Σ or something similar. For example, let's look closely at the equation



The expression k2 + O(k) on the left stands for the set of all two-variable functions of the form k2 + f(k, n) such that there exists a constant C with |f(k, n)| ≤ Ck for 0 ≤ k ≤ n. The sum of this set of functions, for 0 ≤ k ≤ n, is the set of all functions g(n) of the form



where f has the stated property. Since we have


| n2 +  n + f(0, n) + f(1, n) + · · · + f(n, n)|


 
≤  n2 +  n2 + C·0 + C·1 + · · · + C·n


 
< n2 + C(n2 + n)/2 < (C + 1)n2,


all such functions g(n) belong to the right-hand side of (9.16); therefore (9.16) is true.
(Now is a good time to do warmup exercises 3 and 4.)
People sometimes abuse O-notation by assuming that it gives an exact order of growth; they use it as if it specifies a lower bound as well as an upper bound. For example, an algorithm to sort n numbers might be called inefficient "because its running time is O(n2)." But a running time of O(n2) does not imply that the running time is not also O(n). There's another notation, Big Omega, for lower bounds:



We have f(n) = Ω (g(n)) if and only if g(n) = O (f(n)) . A sorting algorithm whose running time is Ω(n2) is inefficient compared with one whose running time is O(n log n), if n is large enough.
Since Ω and Θ are uppercase Greek letters, the O in O-notation must be a capital Greek Omicron. After all, Greeks invented asymptotics.
Finally there's Big Theta, which specifies an exact order of growth:



We have f(n) = Θ (g(n)) if and only if f(n) ≍ g(n) in the notation we saw previously, equation (9.8).
Edmund Landau [238] invented a "little oh" notation,



This is essentially the relation f(n) ≺ g(n) of (9.3). We also have



Many authors use 'o' in asymptotic formulas, but a more explicit 'O' expression is almost always preferable. For example, the average running time of a computer method called "bubblesort" depends on the asymptotic value of the sum . Elementary asymptotic methods suffice to prove the formula , which means that the ratio  approaches 1 as n → ∞. However, the true behavior of P(n) is best understood by considering the difference, , not the ratio:



The numerical evidence in the middle column is not very compelling; it certainly is far from a dramatic proof that  approaches 1 rapidly, if at all. But the right-hand column shows that P(n) is very close indeed to . Thus we can characterize the behavior of P(n) much better if we can derive formulas of the form



or even sharper estimates like



Stronger methods of asymptotic analysis are needed to prove O-results, but the additional effort required to learn these stronger methods is amply compensated by the improved understanding that comes with O-bounds.
Moreover, many sorting algorithms have running times of the form
T(n) = A n lg n + B n + O(log n)
for some constants A and B. Analyses that stop at T(n) ∼ A n lg n don't tell the whole story, and it turns out to be a bad strategy to choose a sorting algorithm based just on its A value. Algorithms with a good 'A' often achieve this at the expense of a bad 'B'. Since n lg n grows only slightly faster than n, the algorithm that's faster asymptotically (the one with a slightly smaller A value) might be faster only for values of n that never actually arise in practice. Thus, asymptotic methods that allow us to go past the first term and evaluate B are necessary if we are to make the right choice of method.
Also lD, the Duraflame logarithm.
Before we go on to study O, let's talk about one more small aspect of mathematical style. Three different notations for logarithms have been used in this chapter: lg, ln, and log. We often use 'lg' in connection with computer methods, because binary logarithms are often relevant in such cases; and we often use 'ln' in purely mathematical calculations, since the formulas for natural logarithms are nice and simple. But what about 'log'? Isn't this the "common" base-10 logarithm that students learn in high school—the "common" logarithm that turns out to be very uncommon in mathematics and computer science? Yes; and many mathematicians confuse the issue by using 'log' to stand for natural logarithms or binary logarithms. There is no universal agreement here. But we can usually breathe a sigh of relief when a logarithm appears inside O-notation, because O ignores multiplicative constants. There is no difference between O(lg n), O(ln n), and O(log n), as n → ∞; similarly, there is no difference between O(lg lg n), O(ln ln n), and O(log log n). We get to choose whichever we please; and the one with 'log' seems friendlier because it is more pronounceable. Therefore we generally use 'log' in all contexts where it improves readability without introducing ambiguity.
Notice that log log log n is undefined when n ≤ 10.


9.3 O Manipulation
Like any mathematical formalism, the O-notation has rules of manipulation that free us from the grungy details of its definition. Once we prove that the rules are correct, using the definition, we can henceforth work on a higher plane and forget about actually verifying that one set of functions is contained in another. We don't even need to calculate the constants C that are implied by each O, as long as we follow rules that guarantee the existence of such constants.
The secret of being a bore is to tell everything.
—Voltaire
For example, we can prove once and for all that






Then we can say immediately that  n3+  n2+  n = O(n3)+O(n3)+O(n3) = O(n3), without the laborious calculations in the previous section.
Here are some more rules that follow easily from the definition:















Exercise 9 proves (9.22), and the proofs of the others are similar. We can always replace something of the form on the left by what's on the right, regardless of the side conditions on the variable n.
(Note: The formula O(f(n))2 does not denote the set of all functions g(n)2 where g(n) is in O(f(n)); such functions g(n)2 cannot be negative, but the set O(f(n))2 includes negative functions. In general, when S is a set, the notation S2 stands for the set of all products s1s2 with s1 and s2 in S, not for the set of all squares s2 with s  S.)
Equations (9.27) and (9.23) allow us to derive the identity O (f(n)2) = O (f(n))2. This sometimes helps avoid parentheses, since we can write


O(log n)2
instead of
O((log n)2).


Both of these are preferable to 'O(log2 n)', which is ambiguous because some authors use it to mean 'O(log log n)'.
Can we also write


O(log n)-1
instead of
O((log n)-1) ?


No! This is an abuse of notation, since the set of functions 1/O(log n) is neither a subset nor a superset of O(1/log n). We could legitimately substitute Ω(log n)-1 for O ((log n)-1), but this would be awkward. So we'll restrict our use of "exponents outside the O" to constant, positive integer exponents.
Power series give us some of the most useful operations of all. If the sum



converges absolutely for some complex number z = z0, then


S(z) = O(1),
for all |z| ≤ |z0|.


This is obvious, because



In particular, S(z) = O(1) as z → 0, and S(1/n) = O(1) as n → ∞, provided only that S(z) converges for at least one nonzero value of z. We can use this principle to truncate a power series at any convenient point and estimate the remainder with O. For example, not only is S(z) = O(1), but
S(z) = a0 + O(z) ,
S(z) = a0 + a1z + O(z2),
and so on, because



and the latter sum, like S(z) itself, converges absolutely for z = z0 and is O(1). Table 452 lists some of the most useful asymptotic formulas, half of which are simply based on truncation of power series according to this rule.
Dirichlet series, which are sums of the form Σk≥1 ak/kz, can be truncated in a similar way: If a Dirichlet series converges absolutely when z = z0, we can truncate it at any term and get the approximation



valid for ℜz ≥ ℜz0. The asymptotic formula for Bernoulli numbers Bn in Table 452 illustrates this principle.
Remember that ℜ stands for "real part."
On the other hand, the asymptotic formulas for Hn, n!, and π(n) in Table 452 are not truncations of convergent series; if we extended them indefinitely they would diverge for all values of n. This is particularly easy to see in the case of π(n), since we have already observed in Section 7.3, Example 5, that the power series Σk≥0 k!/(ln n)k is everywhere divergent. Yet these truncations of divergent series turn out to be useful approximations.


Table 452 Asymptotic approximations, valid as n → ∞ and z → 0.



An asymptotic approximation is said to have absolute error O(g(n)) if it has the form f(n) + O(g(n)) where f(n) doesn't involve O. The approximation has relative error O(g(n)) if it has the form f(n) (1 + O (g(n))) where f(n) doesn't involve O. For example, the approximation for Hn in Table 452 has absolute error O(n-6); the approximation for n! has relative error O(n-4). (The right-hand side of (9.29) doesn't actually have the required form f(n) (1 + O(n-4)), but we could rewrite it



if we wanted to; a similar calculation is the subject of exercise 12.) The absolute error of this approximation is O(nn-3.5e-n). Absolute error is related to the number of correct decimal digits to the right of the decimal point if the O term is ignored; relative error corresponds to the number of correct "significant figures."
(Relative error is nice for taking reciprocals, because 1/(1 + O()) = 1 + O().)
We can use truncation of power series to prove the general laws






(Here we assume that n → ∞; similar formulas hold for ln(1 + O(f(x))) and eO(f(x)) as x → 0.) For example, let ln(1 + g(n)) be any function belonging to the left side of (9.36). Then there are constants C, n0, and c such that


|g(n)| ≤ C|f(n)| ≤ c < 1 ,
for all n ≥ n0.


It follows that the infinite sum



converges for all n ≥ n0, and the parenthesized series is bounded by the constant . This proves (9.36), and the proof of (9.37) is similar. Equations (9.36) and (9.37) combine to give the useful formula




Problem 1: Return to the Wheel of Fortune.
Let's try our luck now at a few asymptotic problems. In Chapter 3 we derived equation (3.13) for the number of winning positions in a certain game:



And we promised that an asymptotic version of W would be derived in Chapter 9. Well, here we are in Chapter 9; let's try to estimate W, as N → ∞.
The main idea here is to remove the floor brackets, replacing K by N1/3 + O(1). Then we can go further and write
K = N1/3 (1 + O(N-1/3));
this is called "pulling out the large part." (We will be using this trick a lot.) Now we have


K2
=
N2/3(1 + O(N-1/3))2


 
=
N2/3(1 + O(N-1/3)) = N2/3 + O(N1/3)


by (9.38) and (9.26). Similarly


N/K
=
N1 -1/3 (1 + O(N-1/3)) -1 + O(1)


 
=
N2/3 (1 + O(N-1/3))+ O(1) = N2/3 + O(N1/3) .


It follows that the number of winning positions is



Notice how the O terms absorb one another until only one remains; this is typical, and it illustrates why O-notation is useful in the middle of a formula.


Problem 2: Perturbation of Stirling's formula.
Stirling's approximation for n! is undoubtedly the most famous asymptotic formula of all. We will prove it later in this chapter; for now, let's just try to get better acquainted with its properties. We can write one version of the approximation in the form



for certain constants a and b. Since this holds for all large n, it must also be asymptotically true when n is replaced by n - 1:



We know, of course, that (n - 1)! = n!/n; hence the right-hand side of this formula must simplify to the right-hand side of (9.40), divided by n.
Let us therefore try to simplify (9.41). The first factor becomes tractable if we pull out the large part:



Equation (9.35) has been used here.
Similarly we have



The only thing in (9.41) that's slightly tricky to deal with is the factor (n - 1)n-1, which equals
nn-1(1 - n-1)n-1 = nn-1(1 - n-1)n (1 + n-1 + n-2 + O(n-3)).
(We are expanding everything out until we get a relative error of O(n-3), because the relative error of a product is the sum of the relative errors of the individual factors. All of the O(n-3) terms will coalesce.)
In order to expand (1 - n-1)n, we first compute ln(1 - n-1) and then form the exponential, en ln(1-n-1):


(1 - n-1)n
=
exp (n ln(1 - n-1))


 
=
exp (n (-n-1 -  n-2 -  n-3 + O(n-4)))


 
=
exp (-1 -  n-1 -  n-2 + O(n-3))


 
=
exp(-1) · exp(-  n-1) · exp(-  n-2) · exp (O(n-3))


 
=
exp(-1) ·( 1 -  n-1 +  n-2 + O(n-3))· (1 -  n-2 + O(n-4)) · (1 + O(n-3))


 
=
e-1 (1 -  n-1 - n-2 + O(n-3)).


Here we use the notation exp z instead of ez, since it allows us to work with a complicated exponent on the main line of the formula instead of in the superscript position. We must expand ln(1-n-1) with absolute error O(n-4) in order to end with a relative error of O(n-3), because the logarithm is being multiplied by n.
The right-hand side of (9.41) has now been reduced to  times nn-1/en times a product of several factors:


( 1 -  n-1 -  n-2 + O(n-3))


 
· (1 + n-1 + n-2 + O(n-3))


 
· (1 -  n-1 - n-2 + O(n-3))


 
· (1 + an-1 + (a + b)n-2 + O(n-3)).


Multiplying these out and absorbing all asymptotic terms into one O(n-3) yields
1 + an-1 + (a + b - )n-2 + O(n-3) .
Hmmm; we were hoping to get 1 + an-1 + bn-2 + O(n-3), since that's what we need to match the right-hand side of (9.40). Has something gone awry? No, everything is fine, provided that .
This perturbation argument doesn't prove the validity of Stirling's approximation, but it does prove something: It proves that formula (9.40) cannot be valid unless . If we had replaced the O(n-3) in (9.40) by cn-3 + O(n-4) and carried out our calculations to a relative error of O(n-4), we could have deduced that b must be , as claimed in Table 452. (This is not the easiest way to determine the values of a and b, but it works.)


Problem 3: The nth prime number.
Equation (9.31) is an asymptotic formula for π(n), the number of primes that do not exceed n. If we replace n by p = Pn, the nth prime number, we have π(p) = n; hence



as n → ∞. Let us try to "solve" this equation for p; then we will know the approximate size of the nth prime.
The first step is to simplify the O term. If we divide both sides by p/ln p, we find that n ln p/p → 1; hence p/ln p = O(n) and



(We have (log p)-1 ≤ (log n)-1 because p ≥ n.)
The second step is to transpose the two sides of (9.42), except for the O term. This is legal because of the general rule



(Each of these equations follows from the other if we multiply both sides by -1 and then add an + bn to both sides.) Hence



and we have



This is an "approximate recurrence" for p = Pn in terms of itself. Our goal is to change it into an "approximate closed form," and we can do this by unfolding the recurrence asymptotically. So let's try to unfold (9.44).
By taking logarithms of both sides we deduce that



This value can be substituted for ln p in (9.44), but we would like to get rid of all p's on the right before making the substitution. Somewhere along the line, that last p must disappear; we can't get rid of it in the normal way for recurrences, because (9.44) doesn't specify initial conditions for small p.
One way to do the job is to start by proving the weaker result p = O(n2).
This follows if we square (9.44) and divide by pn2,



since the right side approaches zero as n → ∞. OK, we know that p = O(n2); therefore log p = O(log n) and log log p = O(log log n). We can now conclude from (9.45) that
ln p = ln n + O(log log n);
in fact, with this new estimate in hand we can conclude that ln ln p = ln ln n+ O(log log n/log n), and (9.45) now yields
ln p = ln n + ln ln n + O(log log n/log n).
And we can plug this into the right-hand side of (9.44), obtaining
p = n ln n + n ln ln n + O(n).
This is the approximate size of the nth prime.
Get out the scratch paper again, gang.
Boo, Hiss.
We can refine this estimate by using a better approximation of π(p) in place of (9.42). The next term of (9.31) tells us that



proceeding as before, we obtain the recurrence



which has a relative error of O(1/log n)2 instead of O(1/log n). Taking logarithms and retaining proper accuracy (but not too much) now yields



Finally we substitute these results into (9.47) and our answer finds its way out:



For example, when n = 106 this estimate comes to 15631363.6 + O(n/log n); the millionth prime is actually 15485863. Exercise 21 shows that a still more accurate approximation to Pn results if we begin with a still more accurate approximation to π(p) in place of (9.46).


Problem 4: A sum from an old final exam.
When Concrete Mathematics was first taught at Stanford University during the 1970-1971 term, students were asked for the asymptotic value of the sum



with an absolute error of O(n-7). Let's imagine that we've just been given this problem on a (take-home) final; what is our first instinctive reaction?
No, we don't panic. Our first reaction is to THINK BIG. If we set n = 10100, say, and look at the sum, we see that it consists of n terms, each of which is slightly less than 1/n2; hence the sum is slightly less than 1/n. In general, we can usually get a decent start on an asymptotic problem by taking stock of the situation and getting a ballpark estimate of the answer.
Let's try to improve the rough estimate by pulling out the largest part of each term. We have



and so it's natural to try summing all these approximations:



It looks as if we're getting Sn = n-1 -  n-2 + O(n-3), based on the sums of the first two columns; but the calculations are getting hairy.
If we persevere in this approach, we will ultimately reach the goal; but we won't bother to sum the other columns, for two reasons: First, the last column is going to give us terms that are O(n-6), when n/2 ≤ k ≤ n, so we will have an error of O(n-5); that's too big, and we will have to include yet another column in the expansion. Could the exam-giver have been so sadistic? We suspect that there must be a better way. Second, there is indeed a much better way, staring us right in the face.
Do pajamas have buttons?
Namely, we know a closed form for Sn: It's just Hn2+n - Hn2 . And we know a good approximation for harmonic numbers, so we just apply it twice:



Now we can pull out large terms and simplify, as we did when looking at Stirling's approximation. We have



So there's lots of helpful cancellation, and we find


Sn = n-1 -  n-2 +  n-3 -  n-4 +  n-5 -  n-6


-  n-3 +  n-4 -  n-5 +  n-6


+  n-5 -  n-6


plus terms that are O(n-7). A bit of arithmetic and we're home free:



It would be nice if we could check this answer numerically, as we did when we derived exact results in earlier chapters. Asymptotic formulas are harder to verify; an arbitrarily large constant may be hiding in a O term, so any numerical test is inconclusive. But in practice, we have no reason to believe that an adversary is trying to trap us, so we can assume that the unknown O-constants are reasonably small. With a pocket calculator we find that ; and our asymptotic estimate when n = 4 comes to



If we had made an error of, say,  in the term for n-6, a difference of  would have shown up in the fifth decimal place; so our asymptotic answer is probably correct.


Problem 5: An infinite sum.
We turn now to an asymptotic question posed by Solomon Golomb [152]: What is the approximate value of



where Nn(k) is the number of digits required to write k in radix n notation?
First let's try again for a ballpark estimate. The number of digits, Nn(k), is approximately logn k = log k/log n; so the terms of this sum are roughly (log n)2/k(log k)2. Summing on k gives ≈ (log n)2 Σk≥2 1/k(log k)2, and this sum converges to a constant value because it can be compared to the integral



Therefore we expect Sn to be about C(log n)2, for some constant C.
Hand-wavy analyses like this are useful for orientation, but we need better estimates to solve the problem. One idea is to express Nn(k) exactly:



Thus, for example, k has three radix n digits when n2 ≤ k < n3, and this happens precisely when logn k = 2. It follows that Nn(k) > logn k, hence Sn =Σk≥1 1/kNn(k)2 < 1 + (log n)2 Σk≥2 1/k(log k)2.
Proceeding as in Problem 1, we can try to write Nn(k) = logn k + O(1) and substitute this into the formula for Sn. The term represented here by O(1) is always between 0 and 1, and it is about  on the average, so it seems rather well-behaved. But still, this isn't a good enough approximation to tell us about Sn; it gives us zero significant figures (that is, high relative error) when k is small, and these are the terms that contribute the most to the sum. We need a different idea.
The key (as in Problem 4) is to use our manipulative skills to put the sum into a more tractable form, before we resort to asymptotic estimates. We can introduce a new variable of summation, m = Nn(k):



This may look worse than the sum we began with, but it's actually a step forward, because we have very good approximations for the harmonic numbers.
Still, we hold back and try to simplify some more. No need to rush into asymptotics. Summation by parts allows us to group the terms for each value of Hnm-1 that we need to approximate:



For example, Hn2-1 is multiplied by 1/22 and then by -1/32. (We have used the fact that Hn0-1 = H0 = 0.)
Now we're ready to expand the harmonic numbers. Our experience with estimating (n - 1)! has taught us that it will be easier to estimate Hnk than Hnk-1, since the (nk - 1)'s will be messy; therefore we write



Our sum now reduces to



There are four easy pieces left: Σ1, Σ2, Σ3(n), and Σ3(n2).
Into a Big Oh.
Let's do the Σ3's first, since Σ3(n2) is the O term; then we'll see what sort of error we're getting. (There's no sense carrying out other calculations with perfect accuracy if they will be absorbed into a O anyway.) This sum is simply a power series,



and the series converges when x ≥ 1 so we can truncate it at any desired point. If we stop Σ3(n2) at the term for k = 1, we get Σ3(n2) = O(n-2); hence (9.53) has an absolute error of O(n-2). (To decrease this absolute error, we could use a better approximation to Hnk; but O(n-2) is good enough for now.) If we truncate Σ3(n) at the term for k = 2, we get
Σ3(n) = n-1 + O(n-2);
this is all the accuracy we need.
We might as well do Σ2 now, since it is so easy:



This is the telescoping series .
Finally, Σ1 gives us the leading term of Sn, the coefficient of ln n in (9.53):



This is . (If we hadn't applied summation by parts earlier, we would have seen directly that Sn ∼ Σk≥1(ln n)/k2, because Hnk-1 - Hnk-1-1 ∼ ln n; so summation by parts didn't help us to evaluate the leading term, although it did make some of our other work easier.)
Now we have evaluated each of the Σ's in (9.53), so we can put everything together and get the answer to Golomb's problem:



Notice that this grows more slowly than our original hand-wavy estimate of C(log n)2. Sometimes a discrete sum fails to obey a continuous intuition.


Problem 6: Big Phi.
Near the end of Chapter 4, we observed that the number of fractions in the Farey series  is 1 + Φ(n), where
Φ(n) = φ(1) + φ(2) + · · · + φ(n);
and we showed in (4.62) that



Let us now try to estimate Φ(n) when n is large. (It was sums like this that led Bachmann to invent O-notation in the first place.)
Thinking BIG tells us that Φ(n) will probably be proportional to n2. For if the final factor were just n/k instead of 1 + n/k, we would have the upper bound | Φ(n)| ≤  Σk≥1n/k2 ≤  Σk≥1(n/k)2 =  n2, because the Möbius function μ(k) is either -1, 0, or +1. The additional '1 + ' in that final factor adds Σk≥1 μ(k)n/k; but this is zero for k > n, so it cannot be more than nHn = O(n log n) in absolute value.
This preliminary analysis indicates that we'll find it advantageous to write



This removes the floors; the remaining problem is to evaluate the unfloored sum  with an accuracy of O(n log n); in other words, we want to evaluate  with an accuracy of O(n-1 log n). But that's easy; we can simply run the sum all the way up to k = ∞, because the newly added terms are



We proved in (7.89) that Σk≥1 μ(k)/kz = 1/ζ(z). Hence Σk≥1 μ(k)/k2 = 1/(Σk≥1 1/k2) = 6/π2, and we have our answer:



(The error term was shown to be at most O(n(log n)2/3 × (log log n)1+) by Saltykov in 1960 [316]. On the other hand, it is not as small as o(n(log log n)1/2), according to Montgomery [275].)


9.4 Two Asymptotic Tricks
Now that we have some facility with O manipulations, let's look at what we've done from a slightly higher perspective. Then we'll have some important weapons in our asymptotic arsenal, when we need to do battle with tougher problems.


Trick 1: Bootstrapping.
When we estimated the nth prime Pn in Problem 3 of Section 9.3, we solved an asymptotic recurrence of the form
Pn = n ln Pn (1 + O(1/log n)).
We proved that Pn = n ln n + n ln ln n + O(n) by first using the recurrence to show the weaker result O(n2). This is a special case of a general method called bootstrapping, in which we solve a recurrence asymptotically by starting with a rough estimate and plugging it into the recurrence; in this way we can often derive better and better estimates, "pulling ourselves up by our bootstraps."
Here's another problem that illustrates bootstrapping nicely: What is the asymptotic value of the coefficient gn = [zn] G(z) in the generating function



as n → ∞? If we differentiate this equation with respect to z, we find



equating coefficients of zn-1 on both sides gives the recurrence



Our problem is equivalent to finding an asymptotic formula for the solution to (9.58), with the initial condition g0 = 1. The first few values



don't reveal much of a pattern, and the integer sequence n!2gn doesn't appear in Sloane's Handbook [330]; therefore a closed form for gn seems out of the question, and asymptotic information is probably the best we can hope to derive.
Our first handle on this problem is the observation that 0 < gn ≤ 1 for all n ≥ 0; this is easy to prove by induction. So we have a start:
gn = O(1).
This equation can, in fact, be used to "prime the pump" for a bootstrapping operation: Plugging it in on the right of (9.58) yields



hence we have



And we can bootstrap yet again:



obtaining



Will this go on forever? Perhaps we'll have gn = O(n-1 log n)m for all m.
Actually no; we have just reached a point of diminishing returns. The next attempt at bootstrapping involves the sum



which is Ω(n-1); so we cannot get an estimate for gn that falls below Ω(n-2).
In fact, we now know enough about gn to apply our old trick of pulling out the largest part:



The first sum here is G(1) = exp(  +  +  + · · · ) = eπ2/6, because G(z) converges for all |z| ≤ 1. The second sum is the tail of the first; we can get an upper bound by using (9.59):



This last estimate follows because, for example,



(Exercise 54 discusses a more general way to estimate such tails.)
The third sum in (9.60) is



by an argument that's already familiar. So (9.60) proves that



Finally, we can feed this formula back into the recurrence, bootstrapping once more; the result is



(Exercise 23 peeks inside the remaining O term.)


Trick 2: Trading tails.
We derived (9.62) in somewhat the same way we derived the asymptotic value (9.56) of Φ(n): In both cases we started with a finite sum but got an asymptotic value by considering an infinite sum. We couldn't simply get the infinite sum by introducing O into the summand; we had to be careful to use one approach when k was small and another when k was large.
(This important method was pioneered by Laplace [240].)
Those derivations were special cases of an important three-step asymptotic summation method we will now discuss in greater generality. Whenever we want to estimate the value of Σk ak(n), we can try the following approach:
1 First break the sum into two disjoint ranges, Dn and Tn. The summation over Dn should be the "dominant" part, in the sense that it includes enough terms to determine the significant digits of the sum, when n is large. The summation over the other range Tn should be just the "tail" end, which contributes little to the overall total.
2 Find an asymptotic estimate
ak(n) = bk(n) + O(ck(n))
that is valid when k  Dn. The O bound need not hold when k  Tn.
3 Now prove that each of the following three sums is small:



If all three steps can be completed successfully, we have a good estimate:



Here's why. We can "chop off" the tail of the given sum, getting a good estimate in the range Dn where a good estimate is necessary:



And we can replace the tail with another one, even though the new tail might be a terrible approximation to the old, because the tails don't really matter:



When we evaluated the sum in (9.60), for example, we had


ak(n)
=
[0 ≤ k < n]gk/(n - k),


bk(n)
=
gk/n,


ck(n)
=
kgk/n(n - k) ;


the ranges of summation were


Dn = {0, 1, . . . , n - 1},
Tn = {n, n + 1, . . . } ;


and we found that
Σa(n) = 0, Σb(n) = O ((log n)2/n2), Σc(n) = O((log n)3/n2).
This led to (9.61).
Similarly, when we estimated Φ(n) in (9.55) we had
ak(n) = µ(k)n/k1+n/k,    bk(n) = µ(k)n2/k2,    ck(n) = n/k;
Dn = {1, 2, . . . , n},    Tn = {n + 1, n + 2, . . . } .
We derived (9.56) by observing that Σa(n) = 0, Σb(n) = O(n), and Σc(n) = O(n log n).
Asymptotics is the art of knowing where to be sloppy and where to be precise.
Also, horses switch their tails when feeding time approaches.
Here's another example where tail switching is effective. (Unlike our previous examples, this one illustrates the trick in its full generality, with Σa(n) ≠ 0.) We seek the asymptotic value of



The big contributions to this sum occur when k is small, because of the k! in the denominator. In this range we have



We can prove that this estimate holds for 0 ≤ k < lg n, since the original terms that have been truncated with O are bounded by the convergent series



(In this range, .)
Therefore we can apply the three-step method just described, with


ak(n)
=
ln(n + 2k)/k!,


bk(n)
=
(ln n + 2k/n - 4k/2n2)/k!,


ck(n)
=
8k/n3k! ;


Dn
=
{0, 1, . . . , lg n - 1},


Tn
=
{lg n, lg n + 1, . . . } .


All we have to do is find good bounds on the three Σ's in (9.63), and we'll know that Σk≥0 ak(n) ≈ Σk≥0 bk(n).
The error we have committed in the dominant part of the sum, Σc(n) = ΣkDn 8k/n3k!, is obviously bounded by Σk≥0 8k/n3k! = e8/n3, so it can be replaced by O(n-3). The new tail error is



"We may not be big, but we're small."
Since lg n! grows faster than any power of n, this minuscule error is over-whelmed by Σc(n) = O(n-3). The error that comes from the original tail,



is smaller yet.
Finally, it's easy to sum Σk≥0 bk(n) in closed form, and we have obtained the desired asymptotic formula:



The method we've used makes it clear that, in fact,



for any fixed m > 0. (This is a truncation of a series that diverges for all fixed n if we let m → ∞ .)
There's only one flaw in our solution: We were too cautious. We derived (9.64) on the assumption that k < lg n, but exercise 53 proves that the stated estimate is actually valid for all values of k. If we had known the stronger general result, we wouldn't have had to use the two-tail trick; we could have gone directly to the final formula! But later we'll encounter problems where exchange of tails is the only decent approach available.


9.5 Euler's Summation Formula
And now for our next trick—which is, in fact, the last important technique that will be discussed in this book—we turn to a general method of approximating sums that was first published by Leonhard Euler [101] in 1732. (The idea is sometimes also associated with the name of Colin Maclaurin, a professor of mathematics at Edinburgh who discovered it independently a short time later [263, page 305].)
Here's the formula:






On the left is a typical sum that we might want to evaluate. On the right is another expression for that sum, involving integrals and derivatives. If f(x) is a sufficiently "smooth" function, it will have m derivatives f′(x), . . . , f(m) (x), and this formula turns out to be an identity. The right-hand side is often an excellent approximation to the sum on the left, in the sense that the remainder Rm is often small. For example, we'll see that Stirling's approximation for n! is a consequence of Euler's summation formula; so is our asymptotic approximation for the harmonic number Hn.
The numbers Bk in (9.67) are the Bernoulli numbers that we met in Chapter 6; the function Bm ({x}) in (9.68) is the Bernoulli polynomial that we met in Chapter 7. The notation {x} stands for the fractional part x - x, as in Chapter 3. Euler's summation formula sort of brings everything together.
Let's recall the values of small Bernoulli numbers, since it's always handy to have them listed near Euler's general formula:
B0 = 1, B1 = -, B2 = , B4 = - , B6 = , B8 = - ;
B3 = B5 = B7 = B9 = B11 = · · · = 0.
Jakob Bernoulli discovered these numbers when studying the sums of powers of integers, and Euler's formula explains why: If we set f(x) = xm-1, we have f(m)(x) = 0; hence Rm = 0, and (9.67) reduces to



For example, when m = 3 we have our favorite example of summation:



(This is the last time we shall derive this famous formula in this book.)
All good things must come to an end.
Before we prove Euler's formula, let's look at a high-level reason (due to Lagrange [234]) why such a formula ought to exist. Chapter 2 defines the difference operator Δ and explains that Σ is the inverse of Δ, just as ∫ is the inverse of the derivative operator D. We can express Δ in terms of D using Taylor's formula as follows:
f(x + ) = f(x) +  +  2 + · · · .
Setting  = 1 tells us that



Here eD stands for the differential operation 1 + D/1! + D2/2! + D3/3! + · · · . Since Δ = eD - 1, the inverse operator Σ = 1/Δ should be 1/(eD - 1); and we know from Table 351 that z/(ez - 1) = Σk≥0 Bkzk/k! is a power series involving Bernoulli numbers. Thus



Applying this operator equation to f(x) and attaching limits yields



which is exactly Euler's summation formula (9.67) without the remainder term. (Euler did not, in fact, consider the remainder, nor did anybody else until S. D. Poisson [295] published an important memoir about approximate summation in 1823. The remainder term is important, because the infinite sum  often diverges. Our derivation of (9.71) has been purely formal, without regard to convergence.)
Now let's prove (9.67), with the remainder included. It suffices to prove the case a = 0 and b = 1, namely



because we can then replace f(x) by f(x + l) for any integer l, getting



The general formula (9.67) is just the sum of this identity over the range a ≤ l < b, because intermediate terms telescope nicely.
The proof when a = 0 and b = 1 is by induction on m, starting with m = 1:



(The Bernoulli polynomial Bm(x) is defined by the equation



in general, hence B1(x) = x -  in particular.) In other words, we want to prove that



But this is just a special case of the formula



for integration by parts, with u(x) = f(x) and v(x) = x - . Hence the case m = 1 is easy.
Will the authors never get serious?
To pass from m - 1 to m and complete the induction when m > 1, we need to show that , namely that



This reduces to the equation



Once again (9.73) applies to these two integrals, with u(x) = f(m-1)(x) and v(x) = Bm(x), because the derivative of the Bernoulli polynomial (9.72) is



(The absorption identity (5.7) was useful here.) Therefore the required formula will hold if and only if



In other words, we need to have



This is a bit embarrassing, because Bm(0) is obviously equal to Bm, not to (-1)mBm. But there's no problem really, because m > 1; we know that Bm is zero when m is odd. (Still, that was a close call.)
To complete the proof of Euler's summation formula we need to show that Bm(1) = Bm(0), which is the same as saying that
 Bk = Bm,         for m > 1.
But this is just the definition of Bernoulli numbers, (6.79), so we're done.
The identity  = mBm-1(x) implies that



and we know now that this integral is zero when m ≥ 1. Hence the remainder term in Euler's formula,



multiplies f(m)(x) by a function Bm ({x}) whose average value is zero. This means that Rm has a reasonable chance of being small.
Let's look more closely at Bm(x) for 0 ≤ x ≤ 1, since Bm(x) governs the behavior of Rm. Here are the graphs for Bm(x) for the first twelve values of m:



Although B3(x) through B9(x) are quite small, the Bernoulli polynomials and numbers ultimately get quite large. Fortunately Rm has a compensating factor 1/m!, which helps to calm things down.
The graph of Bm(x) begins to look very much like a sine wave when m ≥ 3; exercise 58 proves that Bm(x) can in fact be well approximated by a negative multiple of cos(2πx -  πm), with error O(2-m maxx Bm ({x})).
In general, B4k+1(x) is negative for for 0 < x <  and positive for  < x < 1. Therefore its integral, B4k+2(x)/(4k+2), decreases for 0 < x <  and increases for  < x < 1. Moreover, we have
B4k+1(1 - x) = -B4k+1(x),         for 0 ≤ x ≤ 1,
and it follows that
B4k+2(1 - x) = B4k+2(x),         for 0 ≤ x ≤ 1.
The constant term B4k+2 causes the integral  to be zero; hence B4k+2 > 0. The integral of B4k+2(x) is B4k+3(x)/(4k + 3), which must therefore be positive when 0 < x <  and negative when  < x < 1; furthermore B4k+3(1-x) = -B4k+3(x), so B4k+3(x) has the properties stated for B4k+1(x), but negated. Therefore B4k+4(x) has the properties stated for B4k+2(x), but negated. Therefore B4k+5(x) has the properties stated for B4k+1(x); we have completed a cycle that establishes the stated properties inductively for all k.
According to this analysis, the maximum value of B2m(x) must occur either at x = 0 or at x = . Exercise 17 proves that



hence we have



This can be used to establish a useful upper bound on the remainder in Euler's summation formula, because we know from (6.89) that



Therefore we can rewrite Euler's formula (9.67) as follows:



For example, if f(x) = ex, all derivatives are the same and this formula tells us that ek = (eb - ea) (1 -  + B2/2! + B4/4! + · · · + B2m/(2m)!+ O (2π)-2m . Of course, we know that this sum is actually a geometric series, equal to (eb - ea)/(e - 1) = (eb - ea) Σk≥0 Bk/k!.
If f(2m)(x) ≥ 0 for a ≤ x ≤ b, the integral  is just , so we have



in other words, the remainder is bounded by the magnitude of the final term (the term just before the remainder), in this case. We can give an even better estimate if we know that



For it turns out that this implies the relation



in other words, the remainder will then lie between 0 and the first discarded term in (9.78)—the term that would follow the final term if we increased m.
Here's the proof: Euler's summation formula is valid for all m, and B2m+1 = 0 when m > 0; hence R2m = R2m+1, and the first discarded term must be
R2m - R2m+2.
We therefore want to show that R2m lies between 0 and R2m - R2m+2; and this is true if and only if R2m and R2m+2 have opposite signs. We claim that



This, together with (9.79), will prove that R2m and R2m+2 have opposite signs, so the proof of (9.80) will be complete.
It's not difficult to prove (9.81) if we recall the definition of R2m+1 and the facts we proved about the graph of B2m+1(x). Namely, we have



and f(2m+1) (x) is increasing because its derivative f(2m+2) (x) is positive. (More precisely, f(2m+1) (x) is nondecreasing because its derivative is nonnegative.) The graph of B2m+1 ({x}) looks like (-1)m+1 times a sine wave, so it is geometrically obvious that the second half of each sine wave is more influential than the first half when it is multiplied by an increasing function. This makes (-1)mR2m+1 ≥ 0, as desired. Exercise 16 proves the result formally.


9.6 Final Summations
Now comes the summing up, as we prepare to conclude this book. We will apply Euler's summation formula to some interesting and important examples.


Summation 1: This one is too easy.
But first we will consider an interesting unimportant example, namely a sum that we already know how to do. Let's see what Euler's summation formula tells us if we apply it to the telescoping sum



It can't hurt to embark on our first serious application of Euler's formula with the asymptotic equivalent of training wheels.
We might as well start by writing the function f(x) = 1/x(x+1) in partial fraction form,



since this makes it easier to integrate and differentiate. Indeed, we have f′(x) = -1/x2 + 1/(x + 1)2 and f″(x) = 2/x3 - 2/(x + 1)3; in general



Furthermore



Plugging this into the summation formula (9.67) gives



where .
For example, the right-hand side when m = 4 is



This is kind of a mess; it certainly doesn't look like the real answer 1 - n-1. But let's keep going anyway, to see what we've got. We know how to expand the right-hand terms in negative powers of n up to, say, O(n-5):



=
-n-1 +  n-2 -  n-3 +  n-4 + O(n-5);



=
n-1 - n-2 + n-3 - n-4 + O(n-5);



=
n-2 - 2n-3 + 3n-4 + O(n-5);



=
n-4 + O(n-5) .


Therefore the terms on the right of our approximation add up to



The coefficients of n-2, n-3, and n-4 cancel nicely, as they should.
If all were well with the world, we would be able to show that R4(n) is asymptotically small, maybe O(n-5), and we would have an approximation to the sum. But we can't possibly show this, because we happen to know that the correct constant term is 1, not ln 2 +  (which is approximately 0.9978). So R4(n) is actually equal to  - ln 2 + O(n-5), but Euler's summation formula doesn't tell us this.
In other words, we lose.
One way to try fixing things is to notice that the constant terms in the approximation form a pattern, if we let m get larger and larger:



Perhaps we can show that this series approaches 1 as the number of terms becomes infinite? But no; the Bernoulli numbers get very large. For example, ; therefore |R22(n)| will be much larger than |R4(n)|. We lose totally.
There is a way out, however, and this escape route will turn out to be important in other applications of Euler's formula. The key is to notice that R4(n) approaches a definite limit as n → ∞:



The integral  will exist whenever f(m)(x) = O(x-2) as x → ∞, and in this case f(4)(x) surely qualifies. Moreover, we have



Thus we have used Euler's summation formula to prove that



for some constant C. We do not know what the constant is—some other method must be used to establish it—but Euler's summation formula is able to let us deduce that the constant exists.
Suppose we had chosen a much larger value of m. Then the same reasoning would tell us that
Rm(n) = Rm(∞) + O(n-m-1),
and we would have the formula



for certain constants c2, c3, . . . . We know that the c's happen to be zero in this case; but let's prove it, just to restore some of our confidence (in Euler's formula if not in ourselves). The term ln  contributes (-1)m/m to cm; the term (-1)m+1(Bm/m)n-m contributes (-1)m+1 Bm/m; and the term (-1)k(Bk/k)(n + 1)-k contributes . Therefore



Sure enough, it's zero, when m > 1. We have proved that



This is not enough to prove that the sum is exactly equal to C - n-1; the actual value might be C - n-1 + 2-n or something. But Euler's summation formula does give us the error bound O(n-m-1) for arbitrarily large m, even though we haven't evaluated any remainders explicitly.


Summation 1, again: Recapitulation and generalization.
Before we leave our training wheels, let's review what we just did from a somewhat higher perspective. We began with a sum



and we used Euler's summation formula to write



where F(x) was ∫ f(x) dx and where Tk(x) was a certain term involving Bk and f(k-1)(x). We also noticed that there was a constant c such that
f(m)(x) = O(xc-m) as x → ∞,        for all large m.
Namely, f(k) was 1/k(k + 1); F(x) was ln(x/(x + 1)); c was -2; and Tk(x) was (-1)k+1(Bk/k) (x-k - (x + 1)-k). For all large enough values of m, this implied that the remainders had a small tail,



Therefore we were able to conclude that there exists a constant C such that



(Notice that C nicely absorbed the Tk(1) terms, which were a nuisance.)
We can save ourselves unnecessary work in future problems by simply asserting the existence of C whenever Rm(∞) exists.
Now let's suppose that f(2m+2)(x) ≥ 0 and f(2m+4)(x) ≥ 0 for 1 ≤ x ≤ n. We have proved that this implies a simple bound (9.80) on the remainder,
R2m(n) = θm,n (T2m+2(n) - T2m+2(1)),
where θm,n lies somewhere between 0 and 1. But we don't really want bounds that involve R2m(n) and T2m+2(1); after all, we got rid of Tk(1) when we introduced the constant C. What we really want is a bound like
 = ϕm,nT2m+2(n),
where 0 < ϕm,n < 1; this will allow us to conclude from (9.85) that



hence the remainder will truly be between zero and the first discarded term.
A slight modification of our previous argument will patch things up perfectly. Let us assume that



The right-hand side of (9.85) is just like the negative of the right-hand side of Euler's summation formula (9.67) with a = n and b = ∞, as far as remainder terms are concerned, and successive remainders are generated by induction on m. Therefore our previous argument can be applied.


Summation 2: Harmonic numbers harmonized.
Now that we've learned so much from a trivial (but safe) example, we can readily do a nontrivial one. Let us use Euler's summation formula to derive the approximation for Hn that we have been claiming for some time.
In this case, f(x) = 1/x. We already know about the integral and derivatives of f, because of Summation 1; also f(m)(x) = O(x-m-1) as x → ∞.
Therefore we can immediately plug into formula (9.85):



for some constant C. The sum on the left is Hn-1, not Hn; but it's more convenient to work with Hn-1 and to add 1/n later, than to mess around with (n + 1)'s on the right-hand side. The B1n-1 will then become (B1 + 1)n-1 = 1/(2n). Let us call the constant γ instead of C, since Euler's constant γ is, in fact, defined to be limn→∞(Hn - ln n).
The remainder term can be estimated nicely by the theory we developed a minute ago, because f(2m)(x) = (2m)!/x2m+1 ≥ 0 for all x > 0. Therefore (9.86) tells us that



where θm,n is some fraction between 0 and 1. This is the general formula whose first few terms are listed in Table 452. For example, when m = 2 we get



This equation, incidentally, gives us a good approximation to γ even when n = 2:
γ = H2 - ln 2 -  +  -  +  = 0.577165 . . . + ,
where  is between zero and . If we take n = 104 and m = 250, we get the value of γ correct to 1271 decimal places, beginning thus:



But Euler's constant appears also in other formulas that allow it to be evaluated even more efficiently [345].


Summation 3: Stirling's approximation.
If f(x) = ln x, we have f′(x) = 1/x, so we can evaluate the sum of logarithms using almost the same calculations as we did when summing reciprocals. Euler's summation formula yields



where σ is a certain constant, "Stirling's constant," and 0 < φm,n < 1. (In this case f(2m)(x) is negative, not positive; but we can still say that the remainder is governed by the first discarded term, because we could have started with f(x) = - ln x instead of f(x) = ln x.) Adding ln n to both sides gives



when m = 2. And we can get the approximation in Table 452 by taking 'exp' of both sides. (The value of eσ turns out to be , but we aren't quite ready to derive that formula. In fact, Stirling didn't discover the closed form for σ until several years after de Moivre [76] had proved that the constant exists.)
If m is fixed and n → ∞, the general formula gives a better and better approximation to ln n! in the sense of absolute error, hence it gives a better and better approximation to n! in the sense of relative error. But if n is fixed and m increases, the error bound |B2m+2|/(2m + 2)(2m + 1)n2m+1 decreases to a certain point and then begins to increase. Therefore the approximation reaches a point beyond which a sort of uncertainty principle limits the amount by which n! can be approximated.
Heisenberg may have been here.
In Chapter 5, equation (5.83), we generalized factorials to arbitrary real α by using a definition



suggested by Euler. Suppose α is a large number; then



and Euler's summation formula can be used with f(x) = ln(x + α) to estimate this sum:



(Here we have used (9.67) with a = 0 and b = n, then added ln(n + α) - ln α to both sides.) If we subtract this approximation for  from Stirling's approximation for ln n!, then add α ln n and take the limit as n → ∞, we get



because α ln n+n ln n-n+  ln n-(n+α) ln(n+α)+n-  ln(n+α) → -α and the other terms not shown here tend to zero. Thus Stirling's approximation behaves for generalized factorials (and for the Gamma function Γ(α + 1) = α!) exactly as for ordinary factorials.


Summation 4: A bell-shaped summand.
Let's turn now to a sum that has quite a different flavor:



This is a doubly infinite sum, whose terms reach their maximum value e0 = 1 when k = 0. We call it Θn because it is a power series involving the quantity e-1/n raised to the p(k)th power, where p(k) is a polynomial of degree 2; such power series are traditionally called "theta functions." If n = 10100, we have



So the summand stays very near 1 until k gets up to about , when it drops off and stays very near zero. We can guess that Θn will be proportional to . Here is a graph of e-k2/n when n = 10:



Larger values of n just stretch the graph horizontally by a factor of .
We can estimate Θn by letting f(x) = e-x2/n and taking a = -∞, b = + ∞ in Euler's summation formula. (If infinities seem too scary, let a = -A and b = +B, then take limits as A, B → ∞.) The integral of f(x) is



if we replace x by . The value of  du is well known, but we'll call it C for now and come back to it after we have finished plugging into Euler's summation formula.
The next thing we need to know is the sequence of derivatives f′(x), f″(x), . . . , and for this purpose it's convenient to set



Then the chain rule of calculus says that



and this is the same as saying that



By induction we have



For example, we have g′(x) = -2xe-x2 and g″(x) = (4x2 - 2)e-x2 ; hence



It's easier to see what's going on if we work with the simpler function g(x).
We don't have to evaluate the derivatives of g(x) exactly, because we're only going to be concerned about the limiting values when x = ±∞. And for this purpose it suffices to notice that every derivative of g(x) is e-x2 times a polynomial in x:
g(k)(x) = Pk(x)e-x2,        where Pk is a polynomial of degree k.
This follows by induction.
The negative exponential e-x2 goes to zero much faster than Pk(x) goes to infinity, when x → ±, ∞ so we have
f(k)(+∞) = f(k) (-∞) = 0
for all k ≥ 0. Therefore all of the terms



vanish, and we are left with the term from ∫ f(x) dx and the remainder:



The O estimate here follows since  is bounded and the integral  exists whenever P is a polynomial. (The constant implied by this O depends on m.)
We have proved that , for arbitrarily large M; the difference between Θn and  is "exponentially small." Let us therefore determine the constant C that plays such a big role in the value of Θn.
One way to determine C is to look the integral up in a table; but we prefer to know how the value can be derived, so that we can do integrals even when they haven't been tabulated. Elementary calculus suffices to evaluate C if we are clever enough to look at the double integral



Converting to polar coordinates gives



So . The fact that x2 + y2 = r2 is the equation of a circle whose circumference is 2πr somehow explains why π gets into the act.
Another way to evaluate C is to replace x by  and dx by t-1/2 dt:



This integral equals , since  according to (5.84).
Therefore we have demonstrated that .
Our final formula, then, is



The constant in the O depends on M; that's why we say that M is "fixed."
When n = 2, for example, the infinite sum Θ2 is approximately equal to 2.506628288; this is already very close to  ≈ 2.506628275, even though n is quite small. The value of Θ100 agrees with  to 427 decimal places! Exercise 59 uses advanced methods to derive a rapidly convergent series for Θn; it turns out that





Summation 5: The clincher.
Now we will do one last sum, which will turn out to tell us the value of Stirling's constant σ. This last sum also illustrates many of the other techniques of this last chapter (and of this whole book), so it will be a fitting way for us to conclude our explorations of Concrete Mathematics.
The final task seems almost absurdly easy: We will try to find the asymptotic value of



by using Euler's summation formula.
This is another case where we already know the answer (right?); but it's always interesting to try new methods on old problems, so that we can compare facts and maybe discover something new.
So we THINK BIG and realize that the main contribution to An comes from the middle terms, near k = n. It's almost always a good idea to choose notation so that the biggest contribution to a sum occurs near k = 0, because we can then use the tail-exchange trick to get rid of terms that have large |k|. Therefore we replace k by n + k:



Things are looking reasonably good, since we know how to approximate (n ± k)! when n is large and k is small.
Now we want to carry out the three-step procedure associated with the tail-exchange trick. Namely, we want to write
 = ak(n) = bk(n) + O ck(n),         for k  Dn,
so that we can obtain the estimate



Let us therefore try to estimate  in the region where |k| is small. We could use Stirling's approximation as it appears in Table 452, but it's easier to work with the logarithmic equivalent in (9.91):



We want to convert this to a nice, simple O estimate.
The tail-exchange method allows us to work with estimates that are valid only when k is in the "dominant" set Dn. But how should we define Dn?
Actually I'm not into dominance.
We have to make Dn small enough that we can make a good estimate; for example, we had better not let k get near n, or the term O((n - k)-1) in (9.95) will blow up. Yet Dn must be large enough that the tail terms (the terms with k ∉ Dn) are negligibly small compared with the overall sum. Trial and error is usually necessary to find an appropriate set Dn; in this problem the calculations we are about to make will show that it's wise to define things as follows:



Here  is a small positive constant that we can choose later, after we get to know the territory. (Our O estimates will depend on the value of .) Equation (9.95) now reduces to



(We have pulled out the large parts of the logarithms, writing
ln(n ± k) = ln n + ln(1 ± k/n),
and this has made a lot of ln n terms cancel out.)
Now we need to expand the terms ln(1 ± k/n) asymptotically, until we have an error term that approaches zero as n → ∞. We are multiplying ln(1 ± k/n) by (1 ± k + ), so we should expand the logarithm until we reach o(n-1), using the assumption that |k| ≤ n1/2+:



Multiplication by 1 ± k +  yields



plus other terms that are absorbed in the O(n-1/2+3). So (9.97) becomes
ln ak(n) = (2n +) ln 2 - σ -  ln n - k2/n + O(n-1/2+3) .
Taking exponentials, we have



This is our approximation, with
,         ck(n) = 22n n-1+3 e-k2/n .
Notice that k enters bk(n) and ck(n) in a very simple way. We're in luck, because we will be summing over k.
The tail-exchange trick tells us that Σk ak(n) will be approximately Σk bk(n) if we have done a good job of estimation. Let us therefore evaluate



(Another stroke of luck: We get to use the sum Θn from the previous example.) This is encouraging, because we know that the original sum is actually
An = = (1 + 1)2n = 22n .
Therefore it looks as if we will have eσ = , as advertised.
What an amazing coincidence.
But there's a catch: We still need to prove that our estimates are good enough. So let's look first at the error contributed by ck(n):



Good; this is asymptotically smaller than the previous sum, if 3 < .
I'm tired of getting to the end of long, hard books and not even getting a word of good wishes from the author. It would be nice to read a "thanks for reading this, hope it comes in handy," instead of just running into a hard, cold, cardboard cover at the end of a long, dry proof. You know?
Next we must check the tails. We have



which is O(n-M) for all M; so Σk∉Dn bk(n) is asymptotically negligible. (We chose the cutoff at n1/2+ just so that e-k2/n would be exponentially small outside of Dn. Other choices like n1/2 log n would have been good enough too, and the resulting estimates would have been slightly sharper, but the formulas would have come out more complicated. We need not make the strongest possible estimates, since our main goal is to establish the value of the constant σ.) Similarly, the other tail



is bounded by 2n times its largest term, which occurs at the cutoff point k ≈ n1/2+. This term is known to be approximately bk(n), which is exponentially small compared with An; and an exponentially small multiplier wipes out the factor of 2n.
Thus we have successfully applied the tail-exchange trick to prove the estimate



We may choose  =  and conclude that
σ =  ln 2π.
QED.
Thanks for reading this, hope it comes in handy.
—The authors



Exercises

Warmups
1. Prove or disprove: If f1(n) ≺ g1(n) and f2(n) ≺ g2(n), then we have f1(n) + f2(n) ≺ g1(n) + g2(n).
2. Which function grows faster:
a. n(ln n) or (ln n)n?
b. n(ln ln ln n) or (ln n)!?
c. (n!)! or ((n - 1)!) ! (n - 1)!n!?
d.  or HFn?
3. What's wrong with the following argument? "Since n = O(n) and 2n = O(n) and so on, we have ."
4. Give an example of a valid equation that has O-notation on the left but not on the right. (Do not use the trick of multiplying by zero; that's too easy.) Hint: Consider taking limits.
5. Prove or disprove: O(f(n) + g(n)) = f(n) + O( g(n)), if f(n) and g(n) are positive for all n. (Compare with (9.27).)
6. Multiply (ln n + γ + O(1/n)) by (n + O()), and express your answer in O-notation.
7. Estimate Σk≥0 e-k/n with absolute error O(n-1).


Basics
8. Give an example of functions f(n) and g(n) such that none of the three relations f(n) ≺ g(n), f(n) ≻ g(n), f(n) ≍ g(n) is valid, although f(n) and g(n) both increase monotonically to ∞.
9. Prove (9.22) rigorously by showing that the left side is a subset of the right side, according to the set-of-functions definition of O.
10. Prove or disprove: cos O(x) = 1 + O(x2) for all real x.
11. Prove or disprove: O(x + y)2 = O(x2) + O(y2).
12. Prove that
1 + + O(n-2) =  (1 + O(n-2)),
as n → ∞.
13. Evaluate (n + 2 + O(n-1))n with relative error O(n-1).
14. Prove that (n + α)n+β = nn+βeα (1 + α(β -  α)n-1 + O(n-2)).
15. Give an asymptotic formula for the "middle" trinomial coefficient , correct to relative error O(n-3).
16. Show that if B(1 - x) = -B(x) ≥ 0 for 0 < x < , we have



if we assume also that f′(x) ≥ 0 for a ≤ x ≤ b.
17. Use generating functions to show that Bm(  ) = (21-m - 1)Bm, for all m ≥ 0.
18. Find  with relative error O(n-1/4), when α > 0.


Homework exercises
19. Use a computer to compare the left and right sides of the approximations in Table 452, when n = 10, z = α = 0.1, and O(f(n)) = O (f(z)) = 0.
20. Prove or disprove the following estimates, as n → ∞:
a. 
b. e(1+O(1/n))2 = e + O(1/n).
c. n! = O (((1 - 1/n)nn)n).
21. Equation (9.48) gives the nth prime with relative error O(log n)-2. Improve the relative error to O(log n)-3 by starting with another term of (9.31) in (9.46).
22. Improve (9.54) to O(n-3).
23. Push the approximation (9.62) further, getting absolute error O(n-3). Hint: Let gn = c/(n + 1)(n + 2) + hn; what recurrence does hn satisfy?
24. Suppose an = O (f(n)) and bn = O(f(n)). Prove or disprove that the convolution  is also O(f(n)), in the following cases:
a. f(n) = n-α, α > 1.
b. f(n) = α-n, α > 1.
25. Prove (9.1) and (9.2), with which we opened this chapter.
26. Equation (9.91) shows how to evaluate ln 10! with an absolute error . Therefore if we take exponentials, we get 10! with a relative error that is less than e1/126000000 - 1 < 10-8. (In fact, the approximation gives 3628799.9714.) If we now round to the nearest integer, knowing that 10! is an integer, we get an exact result.
Is it always possible to calculate n! in a similar way, if enough terms of Stirling's approximation are computed? Estimate the value of m that gives the best approximation to ln n!, when n is a fixed (large) integer. Compare the absolute error in this approximation with n! itself.
27. Use Euler's summation formula to find the asymptotic value of , where α is any fixed real number. (Your answer may involve a constant that you do not know in closed form.)
28. Exercise 5.13 defines the hyperfactorial function Qn = 1122 . . . nn. Find the asymptotic value of Qn with relative error O(n-1). (Your answer may involve a constant that you do not know in closed form.)
29. Estimate the function 11/121/2 . . . n1/n as in the previous exercise.
30. Find the asymptotic value of Σk≥0 kle-k2/n with absolute error O(n-3), when l is a fixed nonnegative integer.
31. Evaluate Σk≥0 1/(ck + cm) with absolute error O(c-3m), when c > 1 and m is a positive integer.


Exam problems
32. Evaluate  with absolute error O(n-1).
33. Evaluate  with absolute error O(n-3).
34. Determine values A through F such that (1 + 1/n)nHn is



35. Evaluate  with absolute error O(1).
36. Evaluate  with absolute error O(n-5).
37. Evaluate  with absolute error O(n log n).
38. Evaluate  with relative error O(n-1).
39. Evaluate Σ0≤k<n ln(n - k)(ln n)k/k! with absolute error O(n-1). Hint: Show that the terms for k ≥ 10 ln n are negligible.
40. Let m be a (fixed) positive integer. Evaluate  with absolute error O(1).
41. Evaluate the "Fibonacci factorial"  with relative error O(n-1) or better. Your answer may involve a constant whose value you do not know in closed form.
42. Let α be a constant in the range 0 < α < . We've seen in previous chapters that there is no general closed form for the sum . Show that there is, however, an asymptotic formula



where  Hint: Show that  for 0 < k ≤ αn.
43. Show that Cn, the number of ways to change n cents (as considered in Chapter 7) is asymptotically cn4 + O(n3) for some constant c. What is that constant?
44. Prove that



as x → ∞. (Recall the definition  in (5.88), and the definition of generalized Stirling numbers in Table 272.)
45. Let α be an irrational number between 0 and 1. Chapter 3 discusses the quantity D(α, n), which measures the maximum discrepancy by which the fractional parts {kα} for 0 ≤ k < n deviate from a uniform distribution. The recurrence
D(α, n) ≤ D({α-1}, αn) + α-1 + 2
was proved in (3.31); we also have the obvious bounds
0 ≤ D(α, n) ≤ n.
Prove that limn→∞ D(α, n)/n = 0. Hint: Chapter 6 discusses continued fractions.
46. Show that the Bell number ϖn = e-1 Σk≥0 kn/k! of exercise 7.15 is asymptotically equal to



where m(n) ln m(n) = n - , and estimate the relative error in this approximation.
47. Let m be an integer ≥ 2. Analyze the two sums



which is asymptotically closer to logm n! ?
48. Consider a table of the harmonic numbers Hk for 1 ≤ k ≤ n in decimal notation. The kth entry  has been correctly rounded to dk significant digits, where dk is just large enough to distinguish this value from the values of Hk-1 and Hk+1. For example, here is an extract from the table, showing five entries where Hk passes 10:



Estimate the total number of digits in the table, , with an absolute error of O(n).
49. In Chapter 6 we considered the tale of a worm that reaches the end of a stretching band after n seconds, where Hn-1 < 100 ≤ Hn. Prove that if n is a positive integer such that Hn-1 ≤ α ≤ Hn, then
eα -γ ≤ n ≤ eα-γ.
50. Venture capitalists in Silicon Valley are being offered a deal giving them a chance for an exponential payoff on their investments: For an n million dollar investment, where n ≥ 2, the GKP consortium promises to pay up to N million dollars after one year, where N = 10n. Of course there's some risk; the actual deal is that GKP pays k million dollars with probability , for each integer k in the range 1 ≤ k ≤ N. (All payments are in megabucks, that is, in exact multiples of $1,000,000; the payoff is determined by a truly random process.) Notice that an investor always gets at least a million dollars back.
I once earned O(10-n) dollars.
a. What is the asymptotic expected return after one year, if n million dollars are invested? (In other words, what is the mean value of the payment?) Your answer should be correct within an absolute error of O(10-n) dollars.
b. What is the asymptotic probability that you make a profit, if you invest n million? (In other words, what is the chance that you get back more than you put in?) Your answer here should be correct within an absolute error of O(n-3).


Bonus problems
51. Prove or disprove:  as n → ∞.
52. Show that there exists a power series A(z) = Σk≥0 anzn, convergent for all complex z, such that



53. Prove that if f(x) is a function whose derivatives satisfy
f′(x) ≤ 0 , -f″(x) ≤ 0 , f″′(x) ≤ 0, . . . , (-1)mf(m+1) (x) ≤ 0
for all x ≥ 0, then we have



In particular, the case f(x) = - ln(1 + x) proves (9.64) for all k, n > 0.
54. Let f(x) be a positive, differentiable function such that xf′(x) ≺ f(x) as x → ∞. Prove that



Hint: Consider the quantity .
55. Improve (9.99) to relative error O(n-3/2+5).
56. The quantity  occurs in the analysis of many algorithms. Find its asymptotic value, with absolute error o(1).
57. An asymptotic formula for Golomb's sum Σk≥1 1/k1 + logn k2 is derived in (9.54). Find an asymptotic formula for the analogous sum without floor brackets, Σk≥1 1/k(1 + logn k)2. Hint: Consider the integral .
58. Prove that



by using residue calculus, integrating



on the square contour z = x+iy, where , and letting the integer M tend to ∞
59. Let Θn(t) = Σk e-(k+t)2/n, a periodic function of t. Show that the expansion of Θn(t) as a Fourier series is



(This formula gives a rapidly convergent series for the sum Θn = Θn(0) in equation (9.93).)
60. Explain why the coefficients in the asymptotic expansion



all have denominators that are powers of 2.
61. Exercise 45 proves that the discrepancy D(α, n) is o(n) for all irrational numbers α. Exhibit an irrational α such that D(α, n) is not O(n1-) for any  > 0.
62. Given n, let  be the largest entry in row n of Stirling's subset triangle. Show that for all sufficiently large n, we have  or , where



Hint: This is difficult.
63. Prove that S. W. Golomb's self-describing sequence of exercise 2.36 satisfies f(n) = ϕ2-ϕnϕ-1 + O(nϕ-1/log n).
64. Find a proof of the identity



that uses only "Eulerian" (eighteenth-century) mathematics.
65. What are the coefficients of the asymptotic series





Research problems
66. Find a "combinatorial" proof of Stirling's approximation. (Note that nn is the number of mappings of {1, 2, . . . , n} into itself, and n! is the number of mappings of {1, 2, . . . , n} onto itself.)
67. Consider an n × n array of dots, n ≥ 3, in which each dot has four neighbors. (At the edges we "wrap around" modulo n.) Let χn be the number of ways to assign the colors red, white, and blue to these dots in such a way that no neighboring dots have the same color. (Thus χ3 = 12.) Prove that



68. Let Qn be the least integer m such that Hm > n. Find the smallest integer n such that Qn ≠ en-γ +  , or prove that no such n exist.
Th-th-th-that's all, folks!











A. Answers to Exercises
Every exercise is answered here (at least briefly), and some of these answers go beyond what was asked. Readers will learn best if they make a serious attempt to find their own answers BEFORE PEEKING at this appendix.
The authors will be interested to learn of any solutions (or partial solutions) to the research problems, or of any simpler (or more correct) ways to solve the non-research ones.
(The first finder of every error in this book will receive a reward of $2.56.)
1.1 The proof is fine except when n = 2. If all sets of two horses have horses of the same color, the statement is true for any number of horses.
Does that mean I have to find every error?
1.2 If Xn is the number of moves, we have X0 = 0 and Xn = Xn-1 + 1 + Xn-1 + 1 + Xn-1 when n > 0. It follows (for example by adding 1 to both sides) that Xn = 3n -1. (After  moves, it turns out that the entire tower will be on the middle peg, halfway home!)
(We meant to say "any error.")
Does that mean only one person gets a reward?
1.3 There are 3n possible arrangements, since each disk can be on any of the pegs. We must hit them all, since the shortest solution takes 3n -1 moves. (This construction is equivalent to a "ternary Gray code," which runs through all numbers from (0 . . . 0)3 to (2 . . . 2)3, changing only one digit at a time.)
(Hmmm. Try it and see.)
1.4 No. If the largest disk doesn't have to move, 2n-1 -1 moves will suffice (by induction); otherwise (2n-1 - 1) + 1 + (2n-1 - 1) will suffice (again by induction).
1.5 No; different circles can intersect in at most two points, so the fourth circle can increase the number of regions to at most 14. However, it is possible to do the job with ovals:



The number of intersection points turns out to give the whole story; convexity was a red herring.
Venn [359] claimed that there is no way to do the five-set case with ellipses, but a five-set construction with ellipses was found by Grünbaum [167].
1.6 If the nth line intersects the previous lines in k > 0 distinct points, we get k - 1 new bounded regions (assuming that none of the previous lines were mutually parallel) and two new infinite regions. Hence the maximum number of bounded regions is (n-2)+(n-3)+· · · = Sn-2 = (n-1)(n-2)/2 = Ln-2n.
This answer assumes that n > 0.
1.7 The basis is unproved; and in fact, H(1) ≠ 2.
1.8 Q2 = (1 + β)/α; Q3 = (1 + α + β)/αβ; Q4 = (1 + α)/β; Q5 = α; Q6 = β. So the sequence is periodic!
1.9 (a) We get P(n - 1) from the inequality



(b) x1 . . . xnxn+1 . . . x2n ≤ (((x1 + · · · + xn)/n)((xn+1 + · · · + x2n)/n))n by P(n); the product inside is ≤ ((x1 + · · · +x2n)/2n)2 by P(2). (c) For example, P(5) follows from P(6) from P(3) from P(4) from P(2).
1.10 First show that Rn = Rn-1 + 1 + Qn-1 + 1 + Rn-1, when n > 0. Incidentally, the methods of Chapter 7 will tell us that Qn = ((1 + )n+1 - (1 - )n+1)/(2) - 1.
1.11 (a) We cannot do better than to move a double (n - 1)-tower, then move (and invert the order of) the two largest disks, then move the double (n - 1)-tower again; hence An = 2An-1 + 2 and An = 2Tn = 2n+1 - 2. This solution interchanges the two largest disks but returns the other 2n - 2 to their original order.
(b) Let Bn be the minimum number of moves. Then B1 = 3, and it can be shown that no strategy does better than Bn = An-1 +2+An-1 +2+Bn-1 when n > 1. Hence Bn = 2n+2-5, for all n > 0. Curiously this is just 2An-1, and we also have Bn = An-1 + 1 + An-1 + 1 + An-1 + 1 + An-1.
1.12 If all mk > 0, then A(m1, . . . , mn) = 2A(m1, . . . , mn-1)+mn. This is an equation of the "generalized Josephus" type, with solution (m1 . . . mn)2 = 2n-1m1 + · · · + 2mn-1 + mn.
Incidentally, the corresponding generalization of exercise 11b appears to satisfy the recurrence



1.13 Given n straight lines that define Ln regions, we can replace them by extremely narrow zig-zags with segments sufficiently long that there are nine intersections between each pair of zig-zags. This shows that ZZn = ZZn-1+9n-8, for all n > 0; consequently ZZn = 9Sn - 8n + 1 = n2 - n + 1.
1.14 The number of new 3-dimensional regions defined by each new cut is the number of 2-dimensional regions defined in the new plane by its intersections with the previous planes. Hence Pn = Pn-1 + Ln-1, and it turns out that P5 = 26. (Six cuts in a cubical piece of cheese can make 27 cubelets, or up to P6 = 42 cuts of weirder shapes.)
Incidentally, the solution to this recurrence fits into a nice pattern if we express it in terms of binomial coefficients (see Chapter 5):



I bet I know what happens in four dimensions!
Here Xn is the maximum number of 1-dimensional regions definable by n points on a line.
1.15 The function I satisfies the same recurrence as J when n > 1, but I(1) is undefined. Since I(2) = 2 and I(3) = 1, there's no value of I(1) = α that will allow us to use our general method; the "end game" of unfolding depends on the two leading bits in n's binary representation.
If n = 2m + 2m-1 + k, where 0 ≤ k < 2m+1 + 2m - (2m + 2m-1) = 2m +2m-1, the solution is I(n) = 2k+1 for all n > 2. Another way to express this, in terms of the representation n = 2m + l, is to say that



1.16 Let g(n) = a(n)α + b(n)β0 + c(n)β1 + d(n)γ. We know from (1.18) that a(n)α + b(n)β0 + c(n)β1 = (α βbm-1 βbm-2 . . . βb1 βb0)3 when n = (1 bm-1 . . . b1 b0)2; this defines a(n), b(n), and c(n). Setting g(n) = n in the recurrence implies that a(n) + c(n) - d(n) = n; hence we know everything. [Setting g(n) = 1 gives the additional identity a(n) - 2b(n) - 2c(n) = 1, which can be used to define b(n) in terms of the simpler functions a(n) and a(n) + c(n).]
1.17 In general we have Wm ≤ 2Wm-k + Tk, for 0 ≤ k ≤ m. (This relation corresponds to transferring the top m - k, then using only three pegs to move the bottom k, then finishing with the top m - k.) The stated relation turns out to be based on the unique value of k that minimizes the right-hand side of this general inequality, when m = n(n + 1)/2. (However, we cannot conclude that equality holds; many other strategies for transferring the tower are conceivable.) If we set Yn = (Wn(n+1)/2 - 1)/2n, we find that Yn ≤ Yn-1 + 1; hence Wn(n+1)/2 ≤ 2n(n - 1) + 1.
1.18 It suffices to show that both of the lines from (n2j, 0) intersect both of the lines from (n2k, 0), and that all these intersection points are distinct.
A line from (xj, 0) through (xj - aj, 1) intersects a line from (xk, 0) through (xk - ak, 1) at the point (xj - taj, t) where t = (xk - xj)/(ak - aj). Let xj = n2j and aj = nj + (0 or n-n). Then the ratio t = (n2k - n2j)/(nk - nj + (-n-n or 0 or n-n)) lies strictly between nj + nk - 1 and nj + nk + 1; hence the y coordinate of the intersection point uniquely identifies j and k. Also the four intersections that have the same j and k are distinct.
1.19 Not when n > 5. A bent line whose half-lines run at angles θ and θ + 30° from its apex can intersect four times with another whose half-lines run at angles ϕ and ϕ + 30° only if 30° < |θ - ϕ| < 150°. We can't choose more than 5 angles this far apart from each other. (It is possible to choose 5.)
1.20 Let h(n) = a(n)α + b(n)β0 + c(n)β1 + d(n)γ0 + e(n)γ1. We know from (1.18) that a(n)α + b(n)β0 + c(n)β1 = (α βbm-1 βbm-2 . . . βb1 βb0)4 when n = (1 bm-1 . . . b1 b0)2; this defines a(n), b(n), and c(n). Setting h(n) = n in the recurrence implies that a(n) + c(n) - 2d(n) - 2e(n) = n; setting h(n) = n2 implies that a(n) + c(n) + 4e(n) = n2. Hence d(n) = (3a(n) + 3c(n) - n2 - 2n)/4; e(n) = (n2 - a(n) - c(n)) /4.
1.21 We can let q be the least (or any) common multiple of 2n, 2n - 1, . . . , n + 1. [A non-rigorous argument suggests that a "random" value of q will succeed with probability



so we might expect to find such a q less than 4n.]
1.22 Take a regular polygon with 2n sides and label the sides with the elements of a "de Bruijn cycle" of length 2n. (This is a cyclic sequence of 0's and 1's in which all n-tuples of adjacent elements are different; see [207, exercise 2.3.4.2-23] and [208, exercise 3.2.2-17].) Attach a very thin convex extension to each side that's labeled 1. The n sets are copies of the resulting polygon, rotated by the length of k sides for k = 0, 1, . . . , n - 1.
I once rode a de Bruijn cycle (when visiting at his home in Nuenen, The Netherlands).
1.23 Yes. (We need principles of elementary number theory from Chapter 4.) Let L(n) = lcm(1, 2, . . . , n). We can assume that n > 2; hence by Bertrand's postulate there is a prime p between n/2 and n. We can also assume that j > n/2, since q′ = L(n) + 1 - q leaves j′ = n + 1 - j if and only if q leaves j. Choose q so that q ≡ 1 (mod L(n)/p) and q ≡ j + 1 - n (mod p). The people are now removed in order 1, 2, . . . , n - p, j + 1, j + 2, . . . , n, n - p + 1, . . . , j - 1.
1.24 The only known examples are: Xn = 2i sin πr + 1/Xn-1, where r is rational and 0 ≤ r <  (all period lengths ≥ 2 occur as r varies); Gauss's recurrence of period 5 in exercise 8; H. Todd's even more remarkable recurrence Xn = (1+Xn-1+Xn-2)/Xn-3, which has period 8 (see [261]); and recurrences derived from these when we replace Xn by a constant times Xmn. We can assume that the first nonzero coefficient in the denominator is unity, and that the first nonzero coefficient in the numerator (if any) has nonnegative real part. Computer algebra shows easily that there are no further solutions of period ≤ 5 when k = 2. A partial theory has been developed by Lyness [261, 262] and by Kurshan and Gopinath [231].
An interesting example of another type, with period 9 when the starting values are real, is the recurrence Xn = |Xn-1|-Xn-2 discovered by Morton Brown [43]. Nonlinear recurrences having any desired period ≥ 5 can be based on continuants [65].
1.25 If T(k)(n) denotes the minimum number of moves needed to transfer n disks with k auxiliary pegs (hence T(1)(n) = Tn and T(2)(n) = Wn), we have . No examples (n, k) are known where this inequality fails to be an equality. When k is small compared with n, the formula 2n+1-k gives a convenient upper bound on .
1.26 The execution-order permutation can be computed in O(n log n) steps for all q and n [209, exercises 5.1.1-2 and 5.1.1-5]. Bjorn Poonen has proved that non-Josephus sets with exactly four "bad guys" exist whenever n ≡ 0 (mod 3) and n ≥ 9; in fact, the number of such sets is at least  for some  > 0. He also found by extensive computations that the only other n < 24 with non-Josephus sets is n = 20, which has 236 such sets with k = 14 and two with k = 13. (One of the latter is {1, 2, 3, 4, 5, 6, 7, 8, 11, 14, 15, 16, 17}; the other is its reflection with respect to 21.) There is a unique non-Josephus set with n = 15 and k = 9, namely {3, 4, 5, 6, 8, 10, 11, 12, 13}.
2.1 There's no agreement about this; three answers are defensible: (1) We can say that  is always equivalent to ∑m≤k≤n qk; then the stated sum is zero. (2) A person might say that the given sum is q4 + q3 + q2 + q1 + q0, by summing over decreasing values of k. But this conflicts with the generally accepted convention that  when n = 0. (3) We can say that ; then the stated sum is equal to - q1 - q2 - q3. This convention may appear strange, but it obeys the useful law  for all a, b, c.
It's best to use the notation  only when n - m ≥ -1; then both conventions (1) and (3) agree.
2.2 This is |x|. Incidentally, the quantity ([x > 0] - [x < 0]) is often called sign(x) or signum(x); it is +1 when x > 0, 0 when x = 0, and -1 when x < 0.
2.3 The first sum is, of course, a0 + a1 + a2 + a3 + a4 + a5; the second is a4 + a1 + a0 + a1 + a4, because the sum is over the values k ∈ {-2, -1, 0, +1, +2}. The commutative law doesn't apply here because the function p(k) = k2 is not a permutation. Some values of n (e.g., n = 3) have no k such that p(k) = n; others (e.g., n = 4) have two such k.
2.4 (a) 
(b) 
2.5 The same index 'k' is being used for two different index variables, although k is bound in the inner sum. This is a famous mistake in mathematics (and computer programming). The result turns out to be correct if aj = ak for all j and k, 1 ≤ j, k ≤ n.
2.6 It's [1 ≤ j ≤ n](n - j + 1). The first factor is necessary here because we should get zero when j < 1 or j > n.
2.7 . A version of finite calculus based on ∇ instead of Δ would therefore give special prominence to rising factorial powers.
2.8 0, if m ≥ 1; 1/|m|!, if m ≤ 0.
2.9 , for integers m and n. Setting m = -n tells us that .
2.10 Another possible right-hand side is Eu Δv + v Δu.
2.11 Break the left-hand side into two sums, and change k to k + 1 in the second of these.
2.12 If p(k) = n then n + c = k + ((-1)k + 1)c and ((-1)k + 1) is even; hence (-1)n+c = (-1)k and k = n - (-1)n+cc. Conversely, this value of k yields p(k) = n.
2.13 Let R0 = α, and Rn = Rn-1 + (-1)n(β + nγ + n2δ) for n > 0. Then R(n) = A(n)α + B(n)β + C(n)γ + D(n)δ. Setting Rn = 1 yields A(n) = 1. Setting Rn = (-1)n yields A(n) + 2B(n) = (-1)n. Setting Rn = (-1)nn yields -B(n)+2C(n) = (-1)nn. Setting Rn = (-1)nn2 yields B(n)-2C(n) + 2D(n) = (-1)nn2. Therefore 2D(n) = (-1)n(n2+n); the stated sum is D(n).
2.14 The suggested rewrite is legitimate since we have k = ∑1≤j≤k 1 when 1 ≤ k ≤ n. Sum first on k; the multiple sum reduces to



2.15 The first step replaces k(k + 1) by 2∑1≤j≤k j. The second step gives .
2.16 , by (2.52).
2.17 Use induction for the first two ='s, and (2.52) for the third. The second line follows from the first.
2.18 Use the facts that (ℜz)+ ≤ |z|, (ℜz)- ≤ |z|, (ℑz)+ ≤ |z|, (ℑz)- ≤ |z|, and |z| ≤ (ℜz)+ + (ℜz)- + (ℑz)+ + (ℑz)-.
2.19 Multiply both sides by 2n-1/n! and let Sn = 2nTn/n! = Sn-1 + 3 · 2n-1 = 3(2n - 1) + S0. The solution is Tn = 3 · n! + n!/2n-1. (We'll see in Chapter 4 that Tn is an integer only when n is 0 or a power of 2.)
"It is a profoundly erroneous truism, repeated by all copybooks and by eminent people when they are making speeches, that we should cultivate the habit of thinking of what we are doing. The precise opposite is the case. Civilization advances by extending the number of important operations which we can perform without thinking about them. Operations of thought are like cavalry charges in a battle—they are strictly limited in number, they require fresh horses, and must only be made at decisive moments."
—A. N. Whitehead [370]
2.20 The perturbation method gives



2.21 Extracting the final term of Sn+1 gives Sn+1 = 1 - Sn; extracting the first term gives



Hence 2Sn = 1 + (-1)n and we have Sn = [n is even]. Similarly, we find



hence 2Tn = n + 1 - Sn and we have Tn = (n + [n is odd]). Finally, the same approach yields


Un+1 = (n + 1)2 - Un
=
Un + 2Tn + Sn


 
=
Un + n + [n is odd] + [n is even]


 
=
Un + n + 1.


Hence Un is the triangular number .
2.22 Twice the general sum gives a "vanilla" sum over 1 ≤ j, k ≤ n, which splits and yields twice (∑k akAk)(∑k bkBk) - (∑k akBk) (∑k bkAk).
2.23 (a) This approach gives four sums that evaluate to 2n + Hn - 2n + (Hn +  - 1). (It would have been easier to replace the summand by 1/k + 1/(k + 1).) (b) Let u(x) = 2x + 1 and Δv(x) = 1/x(x + 1) = (x - 1)-2; then Δu(x) = 2 and v(x) = -(x - 1)-1 = -1/x. The answer is .
2.24 Summing by parts, ΣxmHx δx = xm + 1Hx/(m+1)-xm+1/(m+1)2 +C; hence Σ0≤k<n kmHk = nm + 1 (Hn - 1/(m + 1))/(m + 1) + 0m + 1/(m + 1)2. In our case m = -2, so the sum comes to 1 - (Hn + 1)/(n + 1).
2.25 Here are some of the basic analogies:



2.26 P2 = (∏1≤j,k≤n ajak)(∏1≤j=k≤n ajak). The first factor is equal to ; the second factor is . Hence .
2.27 Δ(cx) = cx(c - x - 1) = cx+2/(c - x). Setting c = -2 and decreasing x by 2 yields Δ(-(-2)x-2) = (-2)x/x, hence the stated sum is (-2)-1 - (-2)n - 1 = (-1)nn! - 1.
2.28 The interchange of summation between the second and third lines is not justifiable; the terms of this sum do not converge absolutely. Everything else is perfectly correct, except that the result of ∑k≥1[k = j - 1]k/j should perhaps have been written [j - 1 ≥ 1](j - 1)/j and simplified explicitly.
As opposed to imperfectly correct.
2.29 Use partial fractions to get



The (-1)k factor now makes the two halves of each term cancel with their neighbors. Hence the answer is -1/4 + (-1)n/(8n + 4).
2.30 . So we have
(b - a)(b + a - 1) = 2100 = 22 ·3·52 ·7.
There is one solution for each way to write 2100 = x · y where x is even and y is odd; we let  and . So the number of solutions is the number of divisors of 3 · 52 · 7, namely 12. In general, there are ∏p>2(np + 1) ways to represent ∏p pnp, where the products range over primes.
2.31 ∑j,k≥2 j-k = ∑j≥2 1/j2(1 - 1/j) = ∑j≥2 1/j(j - 1). The second sum is, similarly, 3/4.
2.32 If 2n ≤ x < 2n+1, the sums are 0+· · ·+n+(x-n-1)+· · ·+(x-2n) = n(x-n) = (x-1) + (x-3) + · · · + (x-2n+1). If 2n - 1 ≤ x < 2n they are, similarly, both equal to n(x - n). (Looking ahead to Chapter 3, the formula (x + 1)(x - (x + 1) covers both cases.)
2.33 If K is empty, ∧k∈K ak = ∞. The basic analogies are:



A permutation that consumes terms of one sign faster than those of the other can steer the sum toward any value that it likes.
2.34 Let K+ = {k | ak ≥ 0} and K- = {k | ak < 0}. Then if, for example, n is odd, we choose Fn to be Fn-1 ∪ En, where En ⊆ K- is sufficiently large that ∑k∈(Fn-1∩K+) ak - ∑k∈En (-ak) < A-.
2.35 Goldbach's sum can be shown to equal



as follows: By unsumming a geometric series, it equals ∑k∈P, l≥1 k-l; therefore the proof will be complete if we can find a one-to-one correspondence between ordered pairs (m, n) with m, n ≥ 2 and ordered pairs (k, l) with k ∈ P and l ≥ 1, where mn = kl when the pairs correspond. If m ∉ P we let (m, n) ↔ (mn, 1); but if m = ab ∈ P, we let (m, n) ↔ (an, b).
With this self-description, Golomb's sequence wouldn't do too well on the Dating Game.
2.36 (a) By definition, g(n) - g(n - 1) = f(n). (b) By part (a), g(g(n)) - g(g(n - 1)) = ∑k f(k)[g(n - 1) < k ≤ g(n)] = n(g(n) - g(n - 1)) = nf(n). (c) By part (a) again, g(g(g(n))) - g(g(g(n - 1))) is



Colin Mallows observes that the sequence can also be defined by the recurrence
f(1) = 1;     f(n + 1) = 1 + f (n + 1 - f(f(n))),   for n ≥ 0.
2.37 (RLG thinks they probably won't fit; DEK thinks they probably will; OP is not committing himself.)
3.1 m = lg n; l = n - 2m = n - 2lg n.
3.2 (a) x + .5. (b) x - .5.
3.3 This is mn - {mα}n/α = mn - 1, since 0 < {mα} < 1.
3.4 Something where no proof is required, only a lucky guess (I guess).
3.5 We have nx = nx + n{x} = nx + n{x} by (3.8) and (3.6). Therefore nx = nx ⇔ n{x} = 0 ⇔ 0 ≤ n{x} < 1 ⇔ {x} < 1/n, assuming that n is a positive integer. (Notice that nx ≤ nx for all x in this case.)
3.6 f(x) = f(x).
3.7 n/m + n mod m.
3.8 If all boxes contain < n/m objects, then n ≤ (n/m - 1) m, so n/m + 1 ≤ n/m, contradicting (3.5). The other proof is similar.
3.9 We have m/n-1/q = (n mumble m)/qn. The process must terminate, because 0 ≤ n mumble m < m. The denominators of the representation are strictly increasing, hence distinct, because qn/(n mumble m) > q.
3.10 x +  - [(2x + 1)/4 is not an integer] is the nearest integer to x, if {x} ≠ ; otherwise it's the nearest even integer. (See exercise 2.) Thus the formula gives an "unbiased" way to round.
3.11 If n is an integer, α < n < β ⇔ α < n < β. The number of integers satisfying a < n < b when a and b are integers is (b - a - 1)[b > a]. We would therefore get the wrong answer if α = β = integer.
3.12 Subtract n/m from both sides, by (3.6), getting (n mod m)/m = (n mod m + m - 1)/m. Both sides are now equal to [n mod m > 0], since 0 ≤ n mod m < m.
A shorter but less direct proof simply observes that the first term in (3.24) must equal the last term in (3.25).
3.13 If they form a partition, the text's formula for N(α, n) implies that 1/α + 1/β = 1, because the coefficients of n in the equation N(α, n) + N(β, n) = n must agree if the equation is to hold for large n. Hence α and β are both rational or both irrational. If both are irrational, we do get a partition, as shown in the text. If both can be written with numerator m, the value m - 1 occurs in neither spectrum, and m occurs in both. (However, Golomb [151] has observed that the sets {nα | n ≥ 1} and {nβ - 1 | n ≥ 1} always do form a partition, when 1/α + 1/β = 1.)
3.14 It's obvious by (3.22) if ny = 0, otherwise true by (3.21) and (3.6).
3.15 Plug in mx for n in (3.24): mx = x + x -  + ··· + x - .
3.16 The formula n mod 3 = 1 + ((ω - 1)ωn - (ω + 2)ω2n) can be verified by checking it when 0 ≤ n < 3.
A general formula for n mod m, when m is any positive integer, appears in exercise 7.25.
3.17 Σj,k[0 ≤ k < m][1 ≤ j ≤ x + k/m] = Σj,k[0 ≤ k< m][1 ≤ j ≤ x] × [k ≥ m(j - x)] = Σ1≤j≤x Σk[0 ≤ k < m] - Σj=x Σk [0 ≤ k < m(j - x)] = mx - m(x - x) = --mx = mx.
3.18 We have



If j ≤ nα - 1 ≤ nα - v, there is no contribution, because (j + v)α-1 ≤ n. Hence j = nα is the only case that matters, and the value in that case equals (nα + v)α-1 - n ≤ vα-1.
3.19 If and only if b is an integer. (If b is an integer, logb x is a continuous, increasing function that takes integer values only at integer points. If b is not an integer, the condition fails when x = b.)
3.20 We have ∑k kx[α ≤ kx ≤ β] = x ∑k k[α/x ≤ k ≤ β/x], which sums to x(β/x β/x + 1 - α/x α/x - 1).
3.21 If 10n ≤ 2M < 10n+1, there are exactly n+1 such powers of 2, because there's exactly one such k-digit power of 2 for each k. Therefore the answer is 1 + M log 2.
Note: The number of powers of 2 with leading digit l is more difficult, when l > 1; it's ∑0≤n≤M (n log 2 - log l - n log 2 - log(l + 1)).
3.22 All terms are the same for n and n-1 except the kth, where n = 2k-1q and q is odd; we have Sn = Sn-1 + 1 and Tn = Tn-1 + 2kq. Hence Sn = n and Tn = n(n + 1).
3.23 Xn = m ⇔ m(m - 1) < n ≤ m(m + 1) ⇔ m2 - m +  < 2n < m2 + m +  ⇔ m -  <  < m + .
3.24 Let β = α/(α + 1). Then the number of times the nonnegative integer m occurs in Spec(β) is exactly one more than the number of times it occurs in Spec(α). Why? Because N(β, n) = N(α, n) + n + 1.
3.25 Continuing the development in the text, if we could find a value of m such that Km ≤ m, we could violate the stated inequality at n + 1 when n = 2m + 1. (Also when n = 3m + 1 and n = 3m + 2.) But the existence of such an m = n′ + 1 requires that 2Kn′/2 ≤ n′ or 3Kn′/3 ≤ n′, i.e., that
Kn′/2 ≤ n′/2    or    Kn′/3 ≤ n′/3.
Aha. This goes down further and further, implying that K0 ≤ 0; but K0 = 1.
What we really want to prove is that Kn is strictly greater than n, for all n ≥ 0. In fact, it's easy to prove this by induction, although it's a stronger result than the one we couldn't prove!
(This exercise teaches an important lesson. It's more an exercise about the nature of induction than about properties of the floor function.)
"In trying to devise a proof by mathematical induction, you may fail for two opposite reasons. You may fail because you try to prove too much: Your P(n) is too heavy a burden. Yet you may also fail because you try to prove too little: Your P(n) is too weak a support. In general, you have to balance the statement of your theorem so that the support is just enough for the burden."
—G. Pólya [297]
3.26 Induction, using the stronger hypothesis



3.27 If  = 2mb - a, where a is 0 or 1, then  = 3mb - a.
3.28 The key observation is that an = m2 implies an+2k+1 = (m + k)2 + m-k and an+2k+2 = (m+k)2+2m, for 0 ≤ k ≤ m; hence an+2m+1 = (2m)2. The solution can be written in a nice form discovered by Carl Witty:



3.29 D(α′, αn) is at most the maximum of the absolute value of s(α′, nα, v′) = -s(α, n, v) - S +  + {0 or 1} + v′ - {0 or 1}.
3.30 Xn = α2n + α-2n, by induction; and Xn is an integer.
This logic is seriously floored.
3.31 Here's an "elegant," "impressive" proof that gives no clue about how it was discovered:


x + y + x + y
=
x + y + x + y


 
≤
x + 2y + x + 2y + 


 
=
2x + 2y = 2x + 2y.


But there's also a simple, graphical proof based on the observation that we need to consider only the case 0 ≤ x, y < 1. Then the functions look like this in the plane:



A slightly stronger result is possible, namely
x + y + x + y ≤ 2x + 2y;
but this is stronger only when . If we replace (x, y) by (-x, x + y) in this identity and apply the reflective law (3.4), we get
y + x + y + 2x ≤ x + 2x + 2y.
3.32 Let f(x) be the sum in question. Since f(x) = f(-x), we may assume that x ≥ 0. The terms are bounded by 2k as k → -∞ and by x2/2k as k → +∞, so the sum exists for all real x.
We have f(2x) = 2 ∑k 2k-1∥x/2k-1∥2 = 2f(x). Let f(x) = l(x) + r(x) where l(x) is the sum for k ≤ 0 and r(x) is the sum for k > 0. Then l(x + 1) = l(x), and l(x) ≤ 1/2 for all x. When 0 ≤ x < 1, we have r(x) = x2/2 + x2/4 + · · · = x2, and r(x + 1) = (x - 1)2/2 + (x + 1)2/4 + (x + 1)2/8 + · · · = x2 + 1. Hence f(x + 1) = f(x) + 1, when 0 ≤ x < 1.
We can now prove by induction that f(x + n) = f(x) + n for all integers n ≥ 0, when 0 ≤ x < 1. In particular, f(n) = n. Therefore in general, f(x) = 2-mf(2mx) = 2-m2mx + 2-mf ({2mx}). But f({2mx}) = l({2mx}) + r({2mx}) ≤  + 1; so |f(x) - x| ≤ |2-m 2mx - x| + 2-m· ≤ 2-m ·  for all integers m.
The inescapable conclusion is that f(x) = |x| for all real x.
3.33 Let r = n - be the radius of the circle. (a) There are 2n-1 horizontal lines and 2n-1 vertical lines between cells of the board, and the circle crosses each of these lines twice. Since r2 is not an integer, the Pythagorean theorem tells us that the circle doesn't pass through the corner of any cell. Hence the circle passes through as many cells as there are crossing points, namely 8n - 4 = 8r. (The same formula gives the number of cells at the edge of the board.) (b) f(n,k) = 4.
It follows from (a) and (b) that



The task of obtaining more precise estimates of this sum is a famous problem in number theory, investigated by Gauss and many others; see Dickson [78, volume 2, chapter 6].
3.34 (a) Let m = lg n. We can add 2m - n terms to simplify the calculations at the boundary:



Consequently f(n) = nm - 2m + 1.
(b) We have n/2 = (n+1)/2, and it follows that the solution to the general recurrence g(n) = a(n) + g (n/2) + g(n/2) must satisfy Δg(n) = Δa(n)+Δg (n/2). In particular, when a(n) = n-1, Δf(n) = 1+Δf (n/2) is satisfied by the number of bits in the binary representation of n, namely lg(n + 1). Now convert from Δ to Σ.
A more direct solution can be based on the identities lg 2j = lg j + 1 and lg(2j - 1) = lg j + [j > 1], for j ≥ 1.
3.35 (n + 1)2n!e = An + (n + 1)2 + (n + 1) + Bn, where



is a multiple of n and



is less than 1. Hence the answer is 2 mod n.
3.36 The sum is



3.37 First consider the case m < n, which breaks into subcases based on whether ; then show that both sides change in the same way when m is increased by n.
This is really only a level 4 problem, in spite of the way it's stated.
3.38 At most one xk can be noninteger. Discard all integer xk, and suppose that n are left. When {x} ≠ 0, the average of {mx} as m → ∞ lies between  and ; hence {mx1} + · · · + {mxn} - {mx1 + · · · + mxn} cannot have average value zero when n > 1.
But the argument just given relies on a difficult theorem about uniform distribution. An elementary proof is possible, sketched here for n = 2: Let Pm be the point ({mx}, {my}). Divide the unit square 0 ≤ x, y < 1 into triangular regions A and B according as x + y < 1 or x + y ≥ 1. We want to show that Pm ∈ B for some m, if {x} and {y} are nonzero. If P1 ∈ B, we're done. Otherwise there is a disk D of radius  > 0 centered at P1 such that D ⊆ A. By Dirichlet's box principle, the sequence P1, . . . , PN must contain two points with |Pk - Pj| <  and k > j, if N is large enough.



It follows that Pk-j-1 is within  of (1, 1) - P1; hence Pk-j-1 ∈ B.
3.39 Replace j by b - j and add the term j = 0 to the sum, so that exercise 15 can be used for the sum on j. The result,
x/bk - x/bk+1 + b - 1,
telescopes when summed on k.
3.40 Let 2 = 4k + r where -2 ≤ r < 2, and let m = . Then the following relationships can be proved by induction:



Thus, when k ≥ 1, Wk is a segment of length 2k - 1 where the path travels west and y(n) = k; Sk is the interior of a segment of length 2k where the path travels south and x(n) = -k; etc. (a) The desired formula is therefore
y(n) = (-1)m((n - m(m + 1))·[2 is odd] - m).
(b) On all segments, k = max(|x(n)|, |y(n)|). On segments Wk and Sk we have x < y and n + x + y = m(m + 1) = (2k)2 - 2k; on segments Ek and Nk we have x ≥ y and n - x - y = m(m + 1) = (2k)2 + 2k. Hence the sign is (-1)[x(n)<y(n)].
3.41 Since 1/ϕ + 1/ϕ2 = 1, the stated sequences do partition the positive integers. Since the condition g(n) = f(f(n)) + 1 determines f and g uniquely, we need only show that nϕϕ + 1 = nϕ2 for all n > 0. This follows from exercise 3, with α = ϕ and n = 1.
3.42 No; an argument like the analysis of the two-spectrum case in the text and in exercise 13 shows that a tripartition occurs if and only if 1/α + 1/β + 1/γ = 1 and



for all n > 0. But the average value of {(n + 1)/α} is 1/2 if α is irrational, by the theorem on uniform distribution. The parameters can't all be rational, and if γ = m/n the average is 3/2 - 1/(2n). Hence γ must be an integer, but this doesn't work either. (There's also a proof of impossibility that uses only simple principles, without the theorem on uniform distribution; see [155].)
3.43 One step of unfolding the recurrence for Kn gives the minimum of the four numbers 1 + a + a · b · K(n-1-a)/(a·b), where a and b are each 2 or 3. (This simplification involves an application of (3.11) to remove floors within floors, together with the identity x + min(y, z) = min(x + y, x + z). We must omit terms with negative subscripts; i.e., with n - 1 - a < 0.)
Continuing along such lines now leads to the following interpretation: Kn is the least number > n in the multiset S of all numbers of the form
1 + a1 + a1a2 + a1a2a3 + · · · + a1a2a3 . . . am ,
where m ≥ 0 and each ak is 2 or 3. Thus,
S = {1, 3, 4, 7, 9, 10, 13, 15, 19, 21, 22, 27, 28, 31, 31, . . . };
the number 31 is in S "twice" because it has two representations 1 + 2 + 4 + 8 + 16 = 1 + 3 + 9 + 18. (Incidentally, Michael Fredman [134] has shown that limn→∞ Kn/n = 1, i.e., that S has no enormous gaps.)
3.44 Let  mumble(q-1), so that  and . Now , and the results follow. (This is the solution found by Euler [116], who determined the a's and d's sequentially without realizing that a single sequence  would suffice.)
Too easy.
3.45 Let α > 1 satisfy α + 1/α = 2m. Then we find 2Yn = α2n + α-2n, and it follows that Yn = α2n/2.
3.46 The hint follows from (3.9), since 2n(n + 1) = 2(n + )2. Let n + θ = (l + l-l)m and n′ + θ′ = (l+1 + l)m, where 0 ≤ θ, θ′ < 1. Then θ′ = 2θ mod 1 = 2θ - d, where d is 0 or 1. We want to prove that n′ = (n + ); this equality holds if and only if
0 ≤ θ′(2 - ) + (1 - d) < 2.
To solve the recurrence, note that Spec(1 + 1/) and Spec(1 + ) partition the positive integers; hence any positive integer a can be written uniquely in the form a = (l + l-1)m, where l and m are integers with m odd and l ≥ 0. It follows that Ln = (l+n + l+n-1)m.
3.47 (a) . (b) c is an integer. (c) c = 0. (d) c is arbitrary. See the answer to exercise 1.2.4-40 in [207] for more general results.
3.48 Let x:0 = 1 and x:(k+1) = xx:k; also let ak = {x:k} and bk = x:k, so that the stated identity reads x3 = 3x:3 + 3a1a2 +  - 3b1b2 + . Since ak + bk = x:k = xbk-1 for k ≥ 0, we have (1 - xz)(1 + b1z + b2z2 + · · ·) = 1 - a1z - a2z2 - · · · ; thus



Take the logarithm of both sides, to separate the a's from the b's. Then differentiate with respect to z, obtaining



The coefficient of zn-1 on the left is xn; on the right it is a formula that matches the given identity when n = 3.
Similar identities for the more general product x0x1 . . . xn-1 can also be derived [170].
3.49 (Solution by Heinrich Rolletschek.) We can replace (α, β) by ({β}, α + β) without changing nα + nβ. Hence the condition α = {β} is necessary. It is also sufficient: Let m = β be the least element of the given multiset, and let S be the multiset obtained from the given one by subtracting mn from the nth smallest element, for all n. If α = {β}, consecutive elements of S differ by either 0 or 2, hence the multiset S = Spec(α) determines α.
A more interesting (still unsolved) problem: Restrict both α and β to be < 1, and ask when the given multiset determines the unordered pair {α, β}.
3.50 According to unpublished notes of William A. Veech, it is sufficient to have αβ, β, and 1 linearly independent over the rationals.
3.51 H. S. Wilf observes that the functional equation f(x2-1) = f(x)2 would determine f(x) for all x ≥ ϕ if we knew f(x) on any interval (ϕ . . ϕ + ).
3.52 There are infinitely many ways to partition the positive integers into three or more generalized spectra with irrational αk; for example,
Spec(2α; 0) ∪ Spec(4α; - α) ∪ Spec(4α; - 3α) ∪ Spec(β; 0)
works. But there's a precise sense in which all such partitions arise by "expanding" a basic one, Spec(α) ∪ Spec(β); see [158]. The only known rational examples, e.g.,
Spec(7; - 3) ∪ Spec(; - 1) ∪ Spec(; 0) ,
are based on parameters like those in the stated conjecture, which is due to A. S. Fraenkel [128].
3.53 Partial results are discussed in [95, pages 30-31]. The greedy algorithm probably does not terminate.
4.1 1, 2, 4, 6, 16, 12.
4.2 Note that mp + np = min(mp, np) + max(mp, np). The recurrence lcm(m, n) = (n/(n mod m)) lcm(n mod m, m) is valid but not really advisable for computing lcm's; the best way known to compute lcm(m, n) is to compute gcd(m, n) first and then to divide mn by the gcd.
4.3 This holds if x is an integer, but π(x) is defined for all real x. The correct formula,
π(x) - π(x - 1) = [x is prime] ,
is easy to verify.
4.4 Between  and  we'd have a left-right reflected Stern-Brocot tree with all denominators negated, etc. So the result is all fractions m/n with m ⊥ n. The condition m′n-mn′ = 1 still holds throughout the construction. (This is called the Stern-Brocot wreath, because we can conveniently regard the final  as identical to the first , thereby joining the trees in a cycle at the top. The Stern-Brocot wreath has interesting applications to computer graphics because it represents all rational directions in the plane.)
4.5  and ; this holds even when k < 0. (We will find a general formula for any product of L's and R's in Chapter 6.)
After all, 'mod y' sort of means "pretend y is zero." So if it already is, there's nothing to pretend.
4.6 a = b. (Chapter 3 defined x mod 0 = x, primarily so that this would be true.)
4.7 We need m mod 10 = 0, m mod 9 = k, and m mod 8 = 1. But m can't be both even and odd.
4.8 We want 10x + 6y ≡ 10x + y (mod 15); hence 5y ≡ 0 (mod 15); hence y ≡ 0 (mod 3). We must have y = 0 or 3, and x = 0 or 1.
4.9 32k+1 mod 4 = 3, so (32k+1 - 1)/2 is odd. The stated number is divisible by (37 - 1)/2 and (311 - 1)/2 (and by other numbers).
4.10 .
4.11 σ(0) = 1; σ(1) = -1; σ(n) = 0 for n > 1. (Generalized Möbius functions defined on arbitrary partially ordered structures have interesting and important properties, first explored by Weisner [366] and developed by many other people, notably Gian-Carlo Rota [313].)
4.12 ∑d\m ∑k\d μ(d/k) g(k) = ∑k\m ∑d\(m/k) μ(d) g(k) = ∑k\mg(k)×[m/k = 1] = g(m), by (4.7) and (4.9).
4.13 (a) np ≤ 1 for all p; (b) μ(n) ≠ 0.
4.14 True when k > 0. Use (4.12), (4.14), and (4.15).
4.15 No. For example, en mod 5 = [2 or 3]; en mod 11 = [2, 3, 7, or 10].
4.16 1/e1 + 1/e2 + · · · + 1/en = 1 - 1/(en(en - 1)) = 1 - 1/(en+1 - 1).
4.17 We have fn mod fm = 2; hence gcd(fn, fm) = gcd(2, fm) = 1. (Incidentally, the relation fn = f0f1 . . . fn-1 + 2 is very similar to the recurrence that defines the Euclid numbers en.)
4.18 If n = qm and q is odd, 2n+1 = (2m+1)(2n-m -2n-2m +· · ·-2m+1).
4.19 The first sum is π(n), since the summand is [k + 1 is prime]. The inner sum in the second is ∑1≤k<m[k\m], so it is greater than 1 if and only if m is composite; again we get π(n). Finally , so the third sum is an application of Wilson's theorem. To evaluate π(n) by any of these formulas is, of course, sheer lunacy.
4.20 Let p1 = 2 and let pn be the smallest prime greater than 2pn-1. Then 2pn-1 < pn < 2pn-1 +1, and it follows that we can take b = limn→∞ lg(n) pn where lg(n) is the function lg iterated n times. The stated numerical value comes from p2 = 5, p3 = 37. It turns out that p4 = 237 + 9, and this gives the more precise value
b ≈ 1.2516475977905
(but no clue about p5).
4.21 By Bertrand's postulate, Pn < 10n. Let



Then 10n2K ≡ Pn + fraction (mod 102n-1).
4.22 (bmn - 1)/(b - 1) = ((bm - 1)/(b - 1)) (bmn -m + · · · + 1). [The only prime numbers of the form (10p - 1)/9 for p < 49081 occur when p = 2, 19, 23, 317, 1031.] Numbers of this form are called "repunits."
4.23 ρ(2k + 1) = 0; ρ(2k) = ρ(k) + 1, for k ≥ 1. By induction we can show that ρ(n) = ρ(n - 2m), if n > 2m and m > ρ(n). The kth Hanoi move is disk ρ(k), if we number the disks 0, 1, . . . , n - 1. This is clear if k is a power of 2. And if 2m < k < 2m+1, we have ρ(k) < m; moves k and k - 2m correspond in the sequence that transfers m + 1 disks in Tm + 1 + Tm steps.
4.24 The digit that contributes dpm to n contributes dpm-1 + · · · + d = d(pm - 1)/(p - 1) to p(n!), hence p(n!) = (n - νp(n))/(p - 1).
4.25 m\\n ⇔ mp = 0 or mp = np, for all p. It follows that (a) is true. But (b) fails, in our favorite example m = 12, n = 18. (This is a common fallacy.)
4.26 Yes, since  defines a subtree of the Stern-Brocot tree.
4.27 Extend the shorter string with M's (since M lies alphabetically between L and R) until both strings are the same length, then use dictionary order. For example, the topmost levels of the tree are LL < LM < LR < MM < RL < RM < RR. (Another solution is to append the infinite string RL∞ to both inputs, and to keep comparing until finding L < R.)
4.28 We need to use only the first part of the representation:



The fraction  appears because it's a better upper bound than , not because it's closer than . Similarly,  is a better lower bound than . The simplest upper bounds and the simplest lower bounds all appear, but the next really good approximation doesn't occur until just before the string of R's switches back to L.
4.29 1/α. To get 1 - x from x in binary notation, we interchange 0 and 1; to get 1/α from α in Stern-Brocot notation, we interchange L and R. (The finite cases must also be considered, but they must work since the correspondence is order preserving.)
4.30 The m integers x  [A . . A + m) are different mod m; so their residues (x mod m1, . . . , x mod mr) run through all m1 . . . mr = m possible values, one of which must be equal to (a1 mod m1, . . . , ar mod mr) by the pigeonhole principle.
4.31 A number in radix b notation is divisible by d if and only if the sum of its digits is divisible by d, whenever b ≡ 1 (mod d). This follows because (am . . . a0)b = ambm + · · · + a0b0 ≡ am + · · · + a0.
4.32 The φ(m) numbers { kn mod m | k ⊥ m and 0 ≤ k < m } are the numbers { k | k ⊥ m and 0 ≤ k < m } in some order. Multiply them together and divide by ∏0≤k<m, k⊥m k.
4.33 Obviously h(1) = 1. If m ⊥ n then h(mn) = ∑d\mn f(d) g(mn/d) = ∑c\m,d\n f(cd) g ((m/c)(n/d)) = ∑c\m ∑d\n f(c) g(m/c) f(d) g(n/d); this is h(m) h(n), since c ⊥ d for every term in the sum.
4.34 g(m) = ∑d\m f(d) = ∑d\m f(m/d) = ∑d≥1 f(m/d) if f(x) is zero when x is not an integer.
4.35 The base cases are
I (0, n) = 0 ;    I(m, 0) = 1 .
When m, n > 0, there are two rules, where the first is trivial if m > n and the second is trivial if m < n:
I(m, n) = I(m, n mod m) - n/mI(n mod m, m) ;I(m, n) = I(m mod n, n) .
4.36 A factorization of any of the given quantities into nonunits must have m2 - 10n2 = ±2 or ±3, but this is impossible mod 10.
4.37 Let an = 2-n ln(en - ) and bn = 2-n ln(en + ). Then
en = E2n+ ⇔ an ≤ ln E < bn .
And an-1 < an < bn < bn-1, so we can take E = limn→∞ ean. In fact, it turns out that



a product that converges rapidly to (1.26408473530530111 . . . )2. But these observations don't tell us what en is, unless we can find another expression for E that doesn't depend on Euclid numbers.
4.38 Let r = n mod m. Then an - bn = (am - bm)(an-mb0 + an-2mbm + · · · + arbn-m-r) + bmn/m(ar - br).
4.39 If a1 . . . at and b1 . . . bu are perfect squares, so is



where {a1, . . . , at}∩{b1, . . . , bu} = {c1, . . . , cv}. (It can be shown, in fact, that the sequence S(1), S(2), S(3), . . .  contains every nonprime positive integer exactly once.)
4.40 Let f(n) = П 1≤k≤n,pχk k = n!/Pn/p n/p! and g(n) = n!/pp(n!). Then
g(n) = f(n)f(n/p)f(n/p2) . . . = f(n)g(n/p) .
Also f(n) ≡ a0!(p - 1)!n/p ≡ a0!(-1)n/p (mod p), and p(n!) = n/p + p (n/p!). These recurrences make it easy to prove the result by induction. (Several other solutions are possible.)
4.41 (a) If n2 ≡ -1 (mod p) then (n2)(p-1)/2 ≡ -1; but Fermat says it's +1. (b) Let n = ((p - 1)/2)!; we have n ≡ (-1)(p-1)/2 ∏1≤k<p/2(p - k) = (p - 1)!/n, hence n2 ≡ (p - 1)!.
4.42 First we observe that k ⊥ l ⇔ k ⊥ l + ak for any integer a, since gcd(k, l) = gcd(k, l + ak) by Euclid's algorithm. Now


m ⊥ n and n′ ⊥ n          mn′ ⊥ n


 
     mn′ + nm′ ⊥ n .


Similarly


m′ ⊥ n′ and n ⊥ n′                mn′ + nm′ ⊥ n′ .


Hence


m ⊥ n and m′ ⊥ n′ and n ⊥ n′                mn′ + nm′ ⊥ nn′.


4.43 We want to multiply by L-1R, then by R-1L-1RL, then L-1R, then R-2L-1RL2, etc.; the nth multiplier is R-ρ(n)L-1RLρ(n), since we must cancel ρ(n) R's. And R-mL-1RLm =.
4.44 We can find the simplest rational number that lies in
[0.3155..0.3165) = 
by looking at the Stern-Brocot representations of  and  and stopping just before the former has L where the latter has R:



The output is LLLRRRRR =  ≈ .3158. Incidentally, an average of .334 implies at least 287 at bats.
John .316
—banner displayed during the 1993 World Series, when John Kruk came to bat.
4.45 x2 ≡ x (mod 10n)  x(x - 1) ≡ 0 (mod 2n) and x(x - 1) ≡ 0 (mod 5n)  x mod 2n = [0 or 1] and x mod 5n = [0 or 1]. (The last step is justified because x(x - 1) mod 5 = 0 implies that either x or x - 1 is a multiple of 5, in which case the other factor is relatively prime to 5n and can be divided from the congruence.)
So there are at most four solutions, of which two (x = 0 and x = 1) don't qualify for the title "n-digit number" unless n = 1. The other two solutions have the forms x and 10n + 1 - x, and at least one of these numbers is ≥ 10n-1. When n = 4 the other solution, 10001 - 9376 = 625, is not a four-digit number. We expect to get two n-digit solutions for about 80% of all n, but this conjecture has not been proved.
(Such self-reproducing numbers have been called "automorphic.")
4.46 (a) If j′j - k′k = gcd(j, k), we have nk′kngcd(j,k) = nj′j ≡ 1 and nk′k ≡ 1. (b) Let n = pq, where p is the smallest prime divisor of n. If 2n ≡ 1 (mod n) then 2n ≡ 1 (mod p). Also 2p-1 ≡ 1 (mod p); hence 2gcd(p-1,n) ≡ 1 (mod p). But gcd(p - 1, n) = 1 by the definition of p.
4.47 If nm-1 ≡ 1 (mod m) we must have n ⊥ m. If nk ≡ nj for some 1 ≤ j < k < m, then nk-j ≡ 1 because we can divide by nj. Therefore if the numbers n1 mod m, . . . , nm-1 mod m are not distinct, there is a k < m - 1 with nk ≡ 1. The least such k divides m-1, by exercise 46(a). But then kq = (m - 1)/p for some prime p and some positive integer q; this is impossible, since nkq ≢ 1. Therefore the numbers n1 mod m, . . . , nm-1 mod m are distinct and relatively prime to m. Therefore the numbers 1, . . . , m - 1 are relatively prime to m, and m must be prime.
4.48 By pairing numbers up with their inverses, we can reduce the product (mod m) to ∏1≤n<m, n2 mod m=1 n. Now we can use our knowledge of the solutions to n2 mod m = 1. By residue arithmetic we find that the result is m - 1 if m = 4, pk, or 2pk (p > 2); otherwise it's +1.
4.49 (a) Either m < n (Φ(N) - 1 cases) or m = n (one case) or m > n (Φ(N) - 1 again). Hence R(N) = 2Φ(N) - 1. (b) From (4.62) we get



hence the stated result holds if and only if



And this is a special case of (4.61) if we set f(x) = [x ≥ 1].
4.50 (a) If f is any function,



we saw a special case of this in the derivation of (4.63). An analogous derivation holds for ∏ instead of ∑. Thus we have



because ωm/d = e2πi/d.
Part (b) follows from part (a) by the analog of (4.56) for products instead of sums. Incidentally, this formula shows that Ψm(z) has integer coefficients, since Ψm(z) is obtained by multiplying and dividing polynomials whose leading coefficient is 1.
4.51 (x1 + · · · + xn)p = ∑k1+···+kn=p p!/(k1! . . . kn!), and the coefficient is divisible by p unless some kj = p. Hence (x1 + · · · + xn)p ≡  (mod p). Now we can set all the x's to 1, obtaining np ≡ n.
4.52 If p > n there is nothing to prove. Otherwise x ⊥ p, so xk(p-1) ≡ 1 (mod p); this means that at least (n - 1)/(p - 1) of the given numbers are multiples of p. And (n - 1)/(p - 1) ≥ n/p since n ≥ p.
"Die ganzen Zahlen hat der liebe Gott gemacht, alles andere ist Menschenwerk."
—L. Kronecker [365]
4.53 First show that if m ≥ 6 and m is not prime then (m-2)! ≡ 0 (mod m). (If m = p2, the product for (m - 2)! includes p and 2p; otherwise it includes d and m/d where d < m/d.) Next consider cases:
Case 0, n < 5. The condition holds for n = 1 only.
Case 1, n ≥ 5 and n is prime. Then (n - 1)!/(n + 1) is an integer and it can't be a multiple of n.
Case 2, n ≥ 5, n is composite, and n + 1 is composite. Then n and n + 1 divide (n - 1)!, and n ⊥ n + 1; hence n(n + 1)\(n - 1)!.
Case 3, n ≥ 5, n is composite, and n + 1 is prime. Then (n - 1)! ≡ 1 (mod n + 1) by Wilson's theorem, and
(n - 1)!/(n + 1) = ((n - 1)! + n)/(n + 1);
this is divisible by n.
Therefore the answer is: Either n = 1 or n ≠ 4 is composite.
4.54 2(1000!) > 500 and 5(1000!) = 249, hence 1000! = a·10249 for some even integer a. Since 1000 = (13000)5, exercise 40 tells us that a · 2249 = 1000!/5249 ≡ -1 (mod 5). Also 2249 ≡ 2, hence a ≡ 2, hence a mod 10 = 2 or 7; hence the answer is 2·10249.
4.55 One way is to prove by induction that  is an integer; this stronger result helps the induction go through. Another way is based on showing that each prime p divides the numerator at least as often as it divides the denominator. This reduces to proving the inequality



which follows from
 .
The latter is true when 0 ≤ n < m, and both sides increase by 4 when n is increased by m.
4.56 Let f (m) =  min (k, 2n-k)[m\k], g(m) =  (2n-2k-1) × [m\(2k + 1)]. The number of times p divides the numerator of the stated fraction is f(p) + f(p2) + f(p3) + · · · , and the number of times p divides the denominator is g(p) + g(p2) + g(p3) + · · · . But f(m) = g(m) whenever m is odd, by exercise 2.32. The stated fraction therefore reduces to 2n(n-1), by exercise 3.22.
4.57 The hint suggests a standard interchange of summation, since



Calling the hinted sum Σ(n), we have



On the other hand, we know from (4.54) that ∑(n) = n(n + 1). Hence Σ(m + n) - Σ(m) - Σ(n) = mn.
4.58 The function f(m) is multiplicative, and when m = pk it equals 1 + p + · · · + pk. This is a power of 2 if and only if p is a Mersenne prime and k = 1. For k must be odd, and in that case the sum is
(1 + p)(1 + p2 + p4 + · · · + pk-1)
and (k - 1)/2 must be odd, etc. The necessary and sufficient condition is that m be a product of distinct Mersenne primes.
4.59 Proof of the hint: If n = 1 we have x1 = α = 2, so there's no problem. If n > 1 we can assume that x1 ≤ · · · ≤ xn. Case 1:  + (xn - 1)-1 ≥ 1 and xn > xn-1. Then we can find β ≥ xn - 1 ≥ xn-1 such that ; hence xn ≤ β + 1 ≤ en and x1 . . . xn ≤ x1 . . . xn-1(β + 1) ≤ e1 . . . en, by induction. There is a positive integer m such that α = x1 . . . xn/m; hence α ≤ e1 . . . en = en+1 - 1, and we have x1 . . . xn(α + 1) ≤ e1 . . . en en+1. Case 2:  and xn = xn-1. Let a = xn and a-1 + (a - 1)-1 = (a - 2)-1 + ζ-1. Then we can show that a ≥ 4 and (a - 2)(ζ + 1) ≥ a2. So there's a β ≥ ζ such that ; it follows by induction that x1 . . . xn ≤ x1 . . . xn-2(a - 2)(ζ + 1) ≤ x1 . . . xn-2(a - 2)(β + 1) ≤ e1 . . . en, and we can finish as before. Case 3: . Let a = xn, and let a-1 + α-1 = (a - 1)-1 + β-1. It can be shown that (a - 1)(β + 1) > a(α + 1), because this identity is equivalent to
aα2 - a2α + aα - a2 + α + a > 0,
which is a consequence of aα(α - a) + (1 + a)α ≥ (1 + a)α > a2 - a. Hence we can replace xn and α by a - 1 and β, repeating this transformation until cases 1 or 2 apply.
Another consequence of the hint is that 1/x1 + · · · + 1/xn < 1 implies 1/x1 + · · · + 1/xn ≤ 1/e1 + · · · + 1/en; see exercise 16.
4.60 The main point is that . Then we can take p1 sufficiently large (to meet the conditions below) and pn to be the least prime greater than . With this definition let an = 3-n ln pn and bn = 3-n ln(pn + 1). If we can show that an-1 ≤ an < bn ≤ bn-1, we can take P = limn→∞ ean as in exercise 37. But this hypothesis is equivalent to  ≤ pn < (pn-1 + 1)3. If there's no prime pn in this range, there must be a prime  such that p + pθ > (pn-1 + 1)3. But this implies that pθ > 3p2/3, which is impossible when p is sufficiently large.
We can almost certainly take p1 = 2, since all available evidence indicates that the known bounds on gaps between primes are much weaker than the truth (see exercise 69). Then p2 = 11, p3 = 1361, p4 = 2521008887, and 1.306377883863 < P < 1.306377883869.
"Man made the integers: All else is Dieudonné."
—R. K. Guy
4.61 Let  and  be the right-hand sides; observe that , hence . Also  and N = ((n + N)/n′)n′ - n ≥  > ((n + N)/n′ - 1)n′ - n = N - n′ ≥ 0. So we have . If equality doesn't hold, we have , a contradiction.
Incidentally, this exercise implies that (m + m″)/(n + n″) = m′/n′, although the former fraction is not always reduced.
4.62 2-1 + 2-2 + 2-3 - 2-6 - 2-7 + 2-12 + 2-13 - 2-20 - 2-21 + 2-30 + 2-31 - 2-42 - 2-43 + · · · can be written



This sum, incidentally, can be expressed in closed form using the "theta function" θ(z, λ) = ∑k e-πλk2+2izk; we have



4.63 Any n > 2 either has a prime divisor d > 2 or is divisible by d = 4. In either case, a solution with exponent n implies a solution (an/d)d+(bn/d)d = (cn/d)d with exponent d. Since d = 4 has no solutions, d must be prime.
The hint follows from the binomial theorem, since (ap + (x - a)p)/x ≡ pap-1 (mod x) when p is odd. The smallest counterexample, if (4.46) fails, has a ⊥ x. If x is not divisible by p then x is relatively prime to cp/x; this means that whenever q is prime and qe\\x and qf\\c, we have e = fp. Hence x = mp for some m. On the other hand if x is divisible by p, then cp/x is divisible by p but not by p2, and cp has no other factors in common with x.
I have discovered a wonderful proof of Fermat's Last Theorem, but there's no room for it here.
4.64 Equal fractions in N appear in "organ-pipe order":



Suppose that N is correct; we want to prove that N+1 is correct. This means that if kN is odd, we want to show that



if kN is even, we want to show that



In both cases it will be helpful to know the number of fractions that are strictly less than (k - 1)/(N + 1) in N; this is



by (3.32), where d = gcd(k - 1, N + 1). And this reduces to  (kN - d + 1), since N mod d = d - 1.
Furthermore, the number of fractions equal to (k - 1)/(N + 1) in N that should precede it in N+1 is , by the nature of organpipe order.
If kN is odd, then d is even and (k-1)/(N+1) is preceded by  elements of N; this is just the correct number to make things work. If kN is even, then d is odd and (k - 1)/(N + 1) is preceded by  elements of N. If d = 1, none of these equals (k - 1)/(N + 1) and N,kN is '<'; otherwise (k - 1)/(N + 1) falls between two equal elements and N,kN is '='. (C. S. Peirce [288] independently discovered the Stern-Brocot tree at about the same time as he discovered N.)
"No square less than 25 × 1014 divides a Euclid number."
—Ilan Vardi
4.65 The analogous question for the (analogous) Fermat numbers fn is a famous unsolved problem. This one might be easier or harder.
4.66 It is known that no square less than 36 × 1018 divides a Mersenne number or Fermat number. But there has still been no proof of Schinzel's conjecture that there exist infinitely many squarefree Mersenne numbers. It is not even known if there are infinitely many p such that p\\(a ± b), where all prime factors of a and b are ≤ 31.
4.67 M. Szegedy proved this conjecture for all large n; then R. Balasubramanian and K. Soundararajan found a complete solution. See [95, pp. 78-79], [348], [55], and [19′].
4.68 This is a much weaker conjecture than the result in the following exercise.
4.69 Cramér [66] showed that this conjecture is plausible on probabilistic grounds, and computational experience bears this out: Brent [37] has shown that Pn+1 - Pn ≤ 602 for Pn+1 < 2.686 × 1012. But the much weaker bounds in exercise 60 are the best that were known in 1994 [255]. Exercise 68 has a "yes" answer if  for all sufficiently large n. According to Guy [169, problem A8], Paul Erdős offers $10,000 for proof that there are infinitely many n such that



for all c > 0.
4.70 This holds if and only if ν2(n) = ν3(n), according to exercise 24. The methods of [96] may help to crack this conjecture.
4.71 When k = 3 the smallest solution is n = 4700063497 = 19·47·5263229; no other solutions are known in this case.
4.72 This is known to be true for infinitely many values of a, including -1 (of course) and 0 (not so obviously). Lehmer [244] has a famous conjecture that φ(n)\(n - 1) if and only if n is prime.
4.73 This is known to be equivalent to the Riemann hypothesis (that the complex zeta function ζ(z) is nonzero when the real part of z is greater than 1/2).
4.74 Experimental evidence suggests that there are about p(1 - 1/e) distinct values, just as if the factorials were randomly distributed modulo p.
5.1 , in any number system of radix r ≥ 7, because of the binomial theorem.
What's 114 in radix 11?
5.2 The ratio  is ≤ 1 when k ≥ n/2 and ≥ 1 when k < n/2, so the maximum occurs when k = n/2 and k = n/2.
5.3 Expand into factorials. Both products are equal to f(n)/f(n - k)f(k), where f(n) = (n + 1)! n! (n - 1)!.
5.4 
5.5 If 0 < k < p, there's a p in the numerator of  with nothing to cancel it in the denominator. Since , we must have , for 0 ≤ k < p.
5.6 The crucial step (after second down) should be



The original derivation forgot to include this extra term, which is [n = 0].
5.7 Yes, because . We also have



5.8 f(k) = (k/n - 1)n is a polynomial of degree n whose leading coefficient is n-n. By (5.40), the sum is n!/nn. When n is large, Stirling's approximation says that this is approximately . (This is quite different from (1 - 1/e), which is what we get if we use the approximation (1-k/n)n ∼ e-k, valid for fixed k as n → ∞.)
5.9 ℇt(z)t = ∑k≥0 t(tk + t)k-1zk/k! = ∑k≥0(k + 1)k-1(tz)k/k! = ℇ1(tz), by (5.60).
5.10 ∑k≥0 2zk/(k + 2) = F(2, 1; 3; z), since tk+1/tk = (k + 2)z/(k + 3).
But not Imbesselian.
5.11 The first is Besselian and the second is Gaussian:



5.12 (a) Yes, if n ≠ 0, since the term ratio is n. (b) Yes, when n is an integer; the term ratio is (k + 1)n/kn. Notice that we get this term from (5.115) by setting m = n + 1, a1 = · · · = am = 1, b1 = · · · = bn = 0, z = 1, and multiplying by 0n. (c) Yes, the term ratio is (k+1)(k+3)/(k+2). (d) No, the term ratio is 1 + 1/(k + 1)Hk; and Hk ∼ ln k isn't a rational function. (e) Yes, the reciprocal of any hypergeometric term is a hypergeometric term. The fact that t(k) = ∞ when k < 0 or k > n does not exclude t(k) from hypergeometric termhood. (f) Of course. (g) Not when, say, t(k) = 2k and T(k) = 1. (h) Yes; the term ratio t(n - 1 - k)/t(n - 1 - (k + 1)) is a rational function (the reciprocal of the term ratio for t, with k replaced by n - 1 - k), for arbitrary n. (i) Yes; the term ratio can be written



and t(k + m)/t(k) = (t(k + m)/t(k + m - 1)) . . . (t(k + 1)/t(k)) is a rational function of k. (j) No. Whenever two rational functions p1(k)/q1(k) and p2(k)/q2(k) are equal for infinitely many k, they are equal for all k, because p1(k)q2(k) = q1(k)p2(k) is a polynomial identity. Therefore the term ratio (k + 1)/2/k/2 would have to equal 1 if it were a rational function. (k) No. The term ratio would have to be (k + 1)/k, since it is (k + 1)/k for all k > 0; but then t(-1) can be zero only if t(0) is a multiple of 02, while t(1) can be 1 only if t(0) = 01.
Each value of a hypergeometric term t(k) can be written 0e(k)v(k), where e(k) is an integer and v(k) ≠ 0. Suppose the term ratio t(k + 1)/t(k) is p(k)/q(k), and that p and q have been completely factored over the complex numbers. Then, for each k, e(k + 1) is e(k) plus the number of zero factors of p(k) minus the number of zero factors of q(k), and v(k + 1) is v(k) times the product of the nonzero factors of p(k) divided by the product of the nonzero factors of q(k).
5.13 
5.14 The first factor in (5.25) is  when k ≤ l, so it's . The sum for k ≤ l is the sum over all k, since m ≥ 0. (The condition n ≥ 0 isn't really needed, although k must assume negative values if n < 0.)
To go from (5.25) to (5.26), first replace s by -1 - n - q.
5.15 If n is odd, the sum is zero, since we can replace k by n-k. If n = 2m, the sum is (-1)m(3m)!/m!3, by (5.29) with a = b = c = m.
5.16 This is just (2a)! (2b)! (2c)!/(a + b)! (b + c)! (c + a)! times (5.29), if we write the summands in terms of factorials.
5.17 The formulas  and  yield .
5.18 
5.19 , by (5.60), and this is .
5.20 It equals F(-a1, . . . , -am; -b1, . . . , -bn; (-1)m+nz); see exercise 2.17.
5.21 limn→∞(n + m)m/nm = 1.
5.22 Multiplying and dividing instances of (5.83) gives



by (5.34) and (5.36). Also



Hence, etc. The Gamma function equivalent, incidentally, is



5.23 (-1)nn¡, see (5.50).
5.24 This sum is , by (5.35) and (5.93).
5.25 This is equivalent to the easily proved identity



as well as to the operator formula a - b = (ϑ + a) - (ϑ + b).
Similarly, we have



because a1-a2 = (a1+k)-(a2+k). If a1-b1 is a nonnegative integer d, this second identity allows us to express F(a1, . . . , am; b1, . . . , bn; z) as a linear combination of F(a2 + j, a3, . . . , am; b2, . . . , bn; z) for 0 ≤ j ≤ d, thereby eliminating an upper parameter and a lower parameter. Thus, for example, we get closed forms for F(a, b; a - 1; z), F(a, b; a - 2; z), etc.
Gauss [143, §7] derived analogous relations between F(a, b; c; z) and any two "contiguous" hypergeometrics in which a parameter has been changed by ±1. Rainville [301] generalized this to cases with more parameters.
5.26 If the term ratio in the original hypergeometric series is tk+1/tk = r(k), the term ratio in the new one is tk+2/tk+1 = r(k + 1). Hence



5.27 This is the sum of the even terms of F(2a1, . . . , 2am; 2b1, . . . , 2bm; z). We have , etc.
Equating coefficients of zn gives the Pfaff-Saalschütz formula (5.97).
5.28 . (Euler proved the identity by showing that both sides satisfy the same differential equation. The reflection law is often attributed to Euler, but it does not seem to appear in his published papers.)
5.29 The coefficients of zn are equal, by Vandermonde's convolution. (Kummer's original proof was different: He considered limm→∞ F(m, b - a; b; z/m) in the reflection law (5.101).)
5.30 Differentiate again to get z(1 - z)F″(z) + (2 - 3z)F′(z) - F(z) = 0. Therefore F(z) = F(1, 1; 2; z) by (5.108).
5.31 The condition f(k) = T(k + 1) - T(k) implies that f(k + 1)/f(k) = (T(k + 2)/T(k + 1) - 1 )/(1 - T(k)/T(k + 1)) is a rational function of k.
5.32 When summing a polynomial in k, Gosper's method reduces to the "method of undetermined coefficients." We have q(k) = r(k) = 1, and we try to solve p(k) = s(k + 1) - s(k). The method suggests letting s(k) be a polynomial whose degree is d = deg(p) + 1.
5.33 The solution to k = (k - 1)s(k + 1) - (k + 1)s(k) is ; hence the answer is (1 - 2k)/2k(k - 1) + C.
5.34 The limiting relation holds because all terms for k > c vanish, and  - c cancels with -c in the limit of the other terms. Therefore the second partial sum is lim→0 F(-m, -n;  - m; 1) = lim→0 ( + n - m)m/( - m)m = (-1)m 
5.35 (a) 2-n3n[n ≥ 0]. (b) (1 - )-k-1[k≥0] = 2k+1 [k ≥ 0].
5.36 The sum of the digits of m + n is the sum of the digits of m plus the sum of the digits of n, minus p - 1 times the number of carries, because each carry decreases the digit sum by p - 1. [See [226] for extensions of this result to generalized binomial coefficients.]
5.37 Dividing the first identity by n! yields , Vandermonde's convolution. The second identity follows, for example, from the formula  if we negate both x and y.
5.38 Choose c as large as possible such that . Then ; replace n by  and continue in the same fashion. Conversely, any such representation is obtained in this way. (We can do the same thing with



for any fixed m.)
5.39  for all m > 0 and n > 0, by induction on m + n.
5.40 .
5.41 , which is 22nn!/(2n + 1)!.
5.42 We treat n as an indeterminate real variable. Gosper's method with q(k) = k + 1 and r(k) = k - 1 - n has the solution s(k) = 1/(n + 2); hence the desired indefinite sum is . And if 0 ≤ m ≤ n,



This exercise, incidentally, implies the formula



a "dual" to the basic recurrence (5.8).
5.43 After the hinted first step we can apply (5.21) and sum on k. Then (5.21) applies again and Vandermonde's convolution finishes the job. (A combinatorial proof of this identity has been given by Andrews [10]. There's a quick way to go from this identity to a proof of (5.29), explained in [207, exercise 1.2.6-62].)
5.44 Cancellation of factorials shows that



so the second sum is  times the first. And the first is just the special case l = 0, n = b, r = a, s = m + n - b of (5.32), so it is .
5.45 According to (5.9), . If this form of the answer isn't "closed" enough, we can apply (5.35) and get .
5.46 By (5.69), this convolution is the negative of the coefficient of z2n in -1(z)-1(-z). Now ; hence . By the binomial theorem,



so the answer is .
The boxed sentence on the other side of this page is true.
5.47 It's the coefficient of zn in (r(z)s/Qr(z))(r(z)-s/Qr(z)) = Qr(z)-2, where Qr(z) = 1 - r + rr(z)-1, by (5.61).
5.48 , a special case of (5.111).
5.49 Saalschütz's identity (5.97) yields



5.50 The left-hand side is



and the coefficient of zn is



by Vandermonde's convolution (5.92).
5.51 (a) Reflection gives F(a, -n; 2a; 2) = (-1)nF(a, -n; 2a; 2). (Incidentally, this formula implies the remarkable identity Δ2m+1 f(0) = 0, when f(n) = 2nxn/(2x)n.)
(b) The term-by-term limit is  plus an additional term for k = 2m - 1. The additional term is



hence, by (5.104), this limit is , the negative of what we had.
Term limits?
5.52 The terms of both series are zero for k > N. This identity corresponds to replacing k by N - k. Notice that



5.53 When , the left side of (5.110) is 1 - 2z and the right side is (1 - 4z + 4z2)1/2, independent of a. The right side is the formal power series



which can be expanded and rearranged to give 1-2z+0z2 +0z3 + · · · ; but the rearrangement involves divergent series in its intermediate steps when z = 1, so it is not legitimate.
The boxed sentence on the other side of this page is false.
5.54 If m + n is odd, say 2N - 1, we want to show that



Equation (5.92) applies, since -m +  > -m -  + , and the denominator factor Γ(c - b) = Γ(N - m) is infinite since N ≤ m; the other factors are finite. Otherwise m + n is even; setting n = m - 2N we have



by (5.93). The remaining job is to show that



and this is the case x = N of exercise 22.
5.55 Let Q(k) = (k + A1) . . . (k + AM)Z and R(k) = (k + B1) . . . (k + BN). Then t(k + 1)/t(k) = P(k)Q(k - 1)/P(k - 1)R(k), where P(k) = Q(k) - R(k) is a nonzero polynomial.
5.56 The solution to -(k+1)(k+2) = s(k+1)+s(k) is (k) = -k2 - k - ; hence  δk =  (-1)k-1(2k2 + 4k + 1) + C. Also



5.57 We have t(k+1)/t(k) = (k-n)(k+1+θ)(-z)/(k+1)(k+θ). Therefore we let p(k) = k + θ, q(k) = (k - n)(-z), r(k) = k. The secret function s(k) must be a constant α0, and we have
k + θ = (-z(k - n) - k) α0 ;
hence α0 = -1/(1 + z) and θ = -nz/(1 + z). The sum is



(The special case z = 1 was mentioned in (5.18).)
5.58 If m > 0 we can replace  by  and derive the formula . The summation factor  is therefore appropriate:



We can unfold this to get



Finally T0,n-m = Hn-m, so . (It's also possible to derive this result by using generating functions; see Example 2 in Section 7.5.)
5.59 , which is .
5.60  is the case m = n of



5.61 Let n/p = q and n mod p = r. The polynomial identity (x + 1)p ≡ xp + 1 (mod p) implies that
(x + 1)pq+r ≡ (x + 1)r(xp + 1)q  (mod p).
The coefficient of xm on the left is . On the right it's , which is just  because 0 ≤ r < p.
5.62 , because all terms of the sum are multiples of p2 except for the  terms in which exactly m of the k's are equal to p. (Stanley [335, exercise 1.6(d)] shows that the congruence actually holds modulo p3 when p > 3.)
5.63 This is . The denominator of (5.74) is zero when z = -1/4, so we can't simply plug into that formula. The recurrence Sn = -2Sn-1 - Sn-2 leads to the solution Sn = (-1)n(2n + 1).
5.64 , which is 
5.65 Multiply both sides by nn-1 and replace k by n - 1 - k to get



(The partial sums can, in fact, be found by Gosper's algorithm.) Alternatively,  can be interpreted as the number of mappings of {1, . . . , n} into itself with f(1), . . . , f(k) distinct but f(k + 1) ∈ {f(1), . . . , f(k)}; summing on k must give nn.
5.66 This is a walk-the-garden-path problem where there's only one "obvious" way to proceed at every step. First replace k - j by l, then replace  by k, getting



The infinite series converges because the terms for fixed j are dominated by a polynomial in j divided by 2j. Now sum over k, getting



Absorb the j + 1 and apply (5.57) to get the answer, 4(m + 1).
5.67  by (5.26), because



5.68 Using the fact that



we get .
The boxed sentence on the other side of this page is not a sentence.
5.69 Since , the minimum occurs when the k's are as equal as possible. Hence, by the equipartition formula of Chapter 3, the minimum is



A similar result holds for any lower index in place of 2.
5.70 This is ; but it's also  if we replace k by n - k. Now  by Gauss's identity (5.111). (Alternatively,  by the reflection law (5.101), and Kummer's formula (5.94) relates this to (5.55).) The answer is 0 when n is odd,  when n is even. (See [164, §1.2] for another derivation. This sum arises in the study of a simple search algorithm [195].)
5.71 (a) Observe that



(b) Here , so we have A(z/(1 - z)2) = 1 - z. Thus .
5.72 The stated quantity is m(m - n) . . . (m - (k - 1)n)nk -ν(k)/k!. Any prime divisor p of n divides the numerator at least k - ν(k) times and divides the denominator at most k - ν(k) times, since this is the number of times 2 divides k!. A prime p that does not divide n must divide the product m(m - n) . . . (m - (k - 1)n) at least r times if pr divides k!, because we have  when nn′ ≡ 1.
5.73 Plugging in Xn = n! yields α = β = 1; plugging in Xn = n¡ yields α = 1, β = 0. Therefore the general solution is Xn = αn¡ + β(n! - n¡).
5.74 
5.75 The recurrence Sk(n+1) = Sk(n)+S(k-1) mod 3 (n) makes it possible to verify inductively that two of the S's are equal and that S(-n) mod 3 (n) differs from them by (-1)n. These three values split their sum S0(n) + S1(n) + S2(n) = 2n as equally as possible, so there must be 2n mod 3 occurrences of 2n/3 and 3 - (2n mod 3) occurrences of 2n/3.
5.76 
5.77 The terms are zero unless k1 ≤ · · · ≤ km, when the product is the multinomial coefficient



Therefore the sum over k1, . . . , km-1 is mkm, and the final sum over km yields (mn+1 - 1)/(m - 1).
The boxed sentence on the other side of this page is not boxed.
5.78 Extend the sum to k = 2m2 + m - 1; the new terms are . Since m ⊥ (2m + 1), the pairs (k mod m, k mod (2m + 1)) are distinct. Furthermore, the numbers (2j + 1) mod (2m + 1) as j varies from 0 to 2m are the numbers 0, 1, . . . , 2m in some order. Hence the sum is



5.79 (a) The sum is 22n-1, so the gcd must be a power of 2. If n = 2kq where q is odd,  is divisible by 2k+1 and not by 2k+2. Each  is divisible by 2k+1 (see exercise 36), so this must be the gcd. (b) If pr ≤ n+1 < pr +1, we get the most radix p carries by adding k to n - k when k = pr - 1. The number of carries in this case is r - p(n + 1), and r = p (L(n + 1)).
5.80 First prove by induction that k! ≥ (k/e)k.
5.81 Let fl,m,n(x) be the left-hand side. It is sufficient to show that we have fl,m,n(1) > 0 and that . The value of fl,m,n(1) is  by (5.23), and this is positive because the binomial coefficient has exactly n - m - 1 negative factors. The inequality is true when l = 0, for the same reason. If l > 0, we have , which is negative by induction.
5.82 Let p(a) be the exponent by which the prime p divides a, and let m = n - k. The identity to be proved reduces to
min(p(m)-p(m+k), p(m+k+1)-p(k+1), p(k)-p(m+1))
  = min(p(k)-p(m+k), p(m)-p(k+1), p(m+k+1)-p(m+1)) .
For brevity let's write this as min(x1, y1, z1) = min(x2, y2, z2). Notice that x1 + y1 + z1 = x2 + y2 + z2. The general relation
p(a) < p(b) ⇒ p(a) = p(|a ± b|)
allows us to conclude that x1 ≠ x2 ⇒ min(x1, x2) = 0; the same holds also for (y1, y2) and (z1, z2). It's now a simple matter to complete the proof.
5.83 (Solution by P. Paule.) Let r be a nonnegative integer. The given sum is the coefficient of xlym in



so it is clearly . (See also exercise 106.)
5.84 Following the hint, we get



and a similar formula is obtained for ℇt(z). Thus the respective formulas  and  give the respective right-hand sides of (5.61). We must therefore prove that



and these follow from (5.59).
5.85 If f(x) = anxn + · · · + a1x + a0 is any polynomial of degree ≤ n, we can prove inductively that



The stated identity is the special case where an = 1/n! and xk = k3.
5.86 (a) First expand with n(n - 1) index variables lij for all i ≠ j. Setting kij = lij - lji for 1 ≤ i < j < n and using the constraints ∑i≠j (lij - lji) = 0 for all i < n allows us to carry out the sums on ljn for 1 ≤ j < n and then on lji for 1 ≤ i < j < n by Vandermonde's convolution. (b) f(z) - 1 is a polynomial of degree < n that has n roots, so it must be zero. (c) Consider the constant terms in



5.87 The first term is , by (5.61). The summands in the second term are



Since ∑0≤j<m(ζ2j +1)k = m(-1)l[k = ml], these terms sum to



Incidentally, the functions m(zm) and ς2j+1z1+1/m(ς2j+1z)1/m are the m + 1 complex roots of the equation wm+1 - wm = zm.
5.88 Use the facts that  and (1 - e-t)/t ≤ 1. (We have  as k → ∞, by (5.83); so this bound implies that Stirling's series  converges when x > -1. Hermite [186] showed that the sum is ln Γ(1 + x).)
5.89 Adding this to (5.19) gives y-r(x + y)m+r on both sides, by the binomial theorem. Differentiation gives



and we can replace k by k + m + 1 and apply (5.15) to get



In hypergeometric form, this reduces to



which is the special case (a, b, c, z) = (n + 1, m + 1 + r, m + 2, -x/y) of the reflection law (5.101). (Thus (5.105) is related to reflection and to the formula in exercise 52.)
The boxed sentence on the other side of this page is self-referential.
5.90 If r is a nonnegative integer, the sum is finite, and the derivation in the text is valid as long as none of the terms of the sum for 0 ≤ k ≤ r has zero in the denominator. Otherwise the sum is infinite, and the kth term  is approximately ks-r(-s - 1)!/(-r - 1)! by (5.83). So we need r > s+1 if the infinite series is going to converge. (If r and s are complex, the condition is ℜr > ℜs + 1, because |kz| = kℜz.) The sum is



by (5.92); this is the same formula we found when r and s were integers.
5.91 (It's best to have computer help for this.) Incidentally, when c = (a + 1)/2, this reduces to an identity that's equivalent to Gauss's identity (5.110), in view of Pfaff's reflection law. For if w = -z/(1 - z) we have 4w(1 - w) = -4z/(1 - z)2, and



5.92 The identities can be proved, as Clausen proved them more than 150 years ago, by showing that both sides satisfy the same differential equation. One way to write the resulting equations between coefficients of zn is in terms of binomial coefficients:



Another way is in terms of hypergeometrics:



The boxed sentence on the other side of this page is not self-referential.
5.93 
5.94 Gosper's algorithm finds the answer . Consequently, when m ≥ 0 is an integer less than n, we have



5.95 The leading coefficients of p and r should be unity, and p should have no factors in common with q or r. It is easy to fulfill these additional conditions by shuffling factors around.
Now suppose p(k + 1)q(k)/p(k)r(k + 1) = P(k + 1)Q(k)/P(k)R(k + 1), where the polynomials (p, q, r) and (P, Q, R) both satisfy the new criteria. Let p0(k) = p(k)/g(k) and P0(k) = P(k)/g(k), where g(k) = gcd(p(k), P(k)) is the product of all common factors of p and P. Then
p0(k + 1)q(k)P0(k)R(k + 1) = p0(k)r(k + 1)P0(k + 1)Q(k) .
Suppose p0(k) ≠ 1. Then there is a complex number α such that p0(α) = 0; this implies q(α) ≠ 0, r(α) ≠ 0, and P0(α) ≠ 0. Hence we must have p0(α + 1)R(α + 1) = 0 and p0(α - 1)Q(α - 1) = 0. Let N be a positive integer such that p0(α+N) ≠ 0 and p0(α-N) ≠ 0. Repeating the argument N times, we find R(α+1) . . . R(α+N) = 0 = Q(α-1) . . . Q(α-N), contradicting (5.118). Therefore p0(k) = 1. Similarly P0(k) = 1, so p(k) = P(k). Now q(α) = 0 implies r(α + 1) ≠ 0, by (5.118), hence q(k)\Q(k). Similarly Q(k)\q(k), so q(k) = Q(k) since they have the same leading coefficient. That leaves r(k) = R(k).
5.96 If r(k) is a nonzero rational function and T(k) is a hypergeometric term, then r(k)T(k) is a hypergeometric term, which is called similar to T(k). (We allow r(k) to be ∞ and T(k) to be 0, or vice versa, for finitely many values of k.) In particular, T(k + 1) is always similar to T(k). If T1(k) and T2(k) are similar hypergeometric terms, then T1(k) + T2(k) is a hypergeometric term. If T1(k), . . . , Tm(k) are mutually dissimilar, and m > 1, then T1(k) + · · · + Tm(k) cannot be zero for all but finitely many k. For if it could, consider a counterexample for which m is minimum, and let rj(k) = Tj(k + 1)/Tj(k). Since T1(k) + · · · + Tm(k) = 0, we have rm(k)T1(k) + · · · + rm(k)Tm(k) = 0 and r1(k)T1(k) + · · · + rm(k)Tm(k) = T1(k + 1) + · · · + Tm(k + 1) = 0; hence (rm(k) - r1(k)) T1(k) + · · · + (rm(k) - rm-1(k)) Tm-1(k) = 0. We cannot have rm(k) - rj(k) = 0, for any j < m, since Tj and Tm are dissimilar. But m was minimum, so this cannot be a counterexample; it follows that m = 2. But then T1(k) and T2(k) must be similar, since they are both zero for all but finitely many k.
Now let t(k) be any hypergeometric term with t(k + 1)/t(k) = r(k), and suppose that t(k) = (T1(k + 1) + · · · + Tm(k + 1)) - (T1(k) + · · · + Tm(k)), where m is minimal. Then T1, . . . , Tm must be mutually dissimilar. Let rj(k) be the rational function such that
r(k) (Tj(k + 1) - Tj(k)) - (Tj(k + 2) - Tj(k + 1)) = rj(k)Tj(k) .
Suppose m > 1. Since 0 = r(k)t(k)-t(k+1) = r1(k)T1(k)+· · ·+rm(k)Tm(k), we must have rj(k) = 0 for all but at most one value of j. If rj(k) = 0, the function  satisfies . So Gosper's algorithm will find a solution.
Burma-Shave
5.97 Suppose first that z is not equal to -d - 1/d for any integer d > 0. Then in Gosper's algorithm we have p(k) = 1, q(k) = (k + 1)2, r(k) = k2 + kz + 1. Since deg(Q) < deg(R) and deg(p) - deg(R) + 1 = -1, the only possibility is z = d + 2 where d is a nonnegative integer. Trying s(k) = αdkd + · · · + α0 fails when d = 0 but succeeds whenever d > 0. (The linear equations obtained by equating coefficients of kd, kd-1, . . . , k1 in (5.122) express αd-1, . . . , α0 as positive multiples of αd, and the remaining equation 1 = αd + · · · + α1 then defines αd.) For example, when z = 3 the indefinite sum is .
If z = -d - 1/d, on the other hand, the stated terms t(k) are infinite for k ≥ d. There are two reasonable ways to proceed: We can cancel the zero in the denominator by redefining



thereby making t(k) = 0 for 0 ≤ k < d and positive for k ≥ d. Then Gosper's algorithm gives p(k) = kd, q(k) = k + 1, r(k) = k - 1/d, and we can solve (5.122) for s(k) because the coefficient of kj on the right is (j + 1 + 1/d)αj plus multiples of {αj+1, . . . , αd}. For example, when d = 2 the indefinite sum is .
Alternatively, we can try to sum the original terms, but only in the range 0 ≤ k < d. Then we can replace p(k) = kd by



This is justified since (5.117) still holds for 0 ≤ k < d - 1; we have , so this trick essentially cancels a 0 from the numerator and denominator of (5.117) as in L'Hospital's rule. Gosper's method now yields an indefinite sum.
Look, any finite sequence is trivially summable, because we can find a polynomial that matches t(k) for 0 ≤ k < d.
5.98 nSn+1 = 2nSn. (Beware: This gives no information about S1/S0.)
5.99 Let p(n, k) = (n + 1 + k)β0(n) + (n + 1 + a + b + c + k)β1(n) = (n, k), (n, k) = t(n, k)/(n + 1 + k), q(n, k) = (n + 1 + a + b + c + k)(a - k)(b - k), r(n, k) = (n + 1 + k)(c + k)k. Then (5.129) is solved by β0(n) = (n + 1 + a + b + c)(n + 1 + a + b), β1(n) = -(n + 1 + a)(n + 1 + b), α0(n) = s(n, k) = -1. We discover (5.134) by observing that it is true when n = -a and using induction on n.
5.100 The Gosper-Zeilberger algorithm discovers easily that



Summing from k = 0 to n-1 yields . Hence (2n + 2)Sn+1 = (n + 2)Sn + 2n + 2. Applying a summation factor now leads to the expression .
5.101 (a) If we hold m fixed, the Gosper-Zeilberger algorithm discovers that (n + 2)Sm,n+2(z) = (z - 1)(n + 1)Sm,n(z) + (2n + 3 - z(n - m + 1))Sm,n+1(z). We can also apply the method to the term
β0(m, n)t(m, n, k) + β1(m, n)t(m+1, n, k) + β2(m, n)t(m, n+1, k),
in which case we get a simpler recurrence,
(m + 1)Sm+1,n(z) - (n + 1)Sm,n+1(z) = (1 - z)(m - n)Sm,n(z).
(b) Now we must work a little harder, with five equations in six unknowns. The algorithm finds



Therefore (n + 1)(z - 1)2Sn(z) - (2n + 3)(z + 1)Sn+1(z) + (n + 2)Sn+2(z) = 0. Incidentally, this recurrence holds also for negative n, and we have S-n-1(z) = Sn(z)/(1 - z)2n+1.
The sum Sn(z) can be regarded as a modified form of the Legendre polynomial , since we can write . Similarly,  is a modified Jacobi polynomial.
How about z = 0?
5.102 The sum is , so we need not consider the case z = -1. Let n = 3m. We seek solutions to (5.129) when
p(m, k) = (3m + 3 - k)3(m + 1 - k)β0 + (4m + 4 - b - k)4β1 ,
q(m, k) = (3m + 3 - k)(m + 1 - a - k)z ,
r(m, k) = k(4m + 1 - b - k) ,
s(m, k) = α2k2 + α1k + α0 .
The resulting five homogeneous equations have a nonzero solution (α0, α1, α2, β0, β1) if and only if the determinant of coefficients is zero; and this determinant, a polynomial in m, vanishes only in eight cases. One of those cases is, of course, (5.113); but we can now evaluate the sum for all nonnegative integers n, not just n ≢ 2 (mod 3):



Here the notation [c0, c1, c2] stands for the single value cn mod 3. Another case, , yields the identity



(This sum, amazingly, is zero unless n is a multiple of 3; and then the identity can be written



which might even be useful.) The remaining six cases generate even weirder sums



where the respective values of (a, b, z, c0, c1, c2, a′, b′, x) are



5.103 We assume that each  and  is nonzero, since the corresponding factors would otherwise have no influence on the degrees in k. Let  where



Then we have , , except in unusual cases where cancellation occurs in the leading coefficient. And , again except in unusual cases.
(These estimates can be used to show directly that, as l increases, the degree of  eventually becomes large enough to make a polynomial s(n, k) possible, and the number of unknown αj and βj eventually becomes larger than the number of homogeneous linear equations to be solved. So we obtain another proof that the Gosper-Zeilberger algorithm succeeds, if we argue as in the text that there must be a solution with β0(n), . . . , βl(n) not all zero.)
5.104 Let t(n, k) = (-1)k(r-s-k)! (r-2k)!/ ((r-s-2k)! (r-n-k+1)! (n-k)! k!). Then β0(n)t(n, k) + β1(n)t(n + 1, k) is not summable in hypergeometric terms, because deg() = 1, deg(q - r) = 3, deg(q + r) = 4, λ = -8, λ′ = -4; but β0(n)t(n, k) + β1(n)t(n + 1, k) + β2(n)t(n + 2, k) is—basically because λ′ = 0 when q(n, k) = -(r - s - 2k)(r - s - 2k - 1)(n + 2 - k)(r - n - k + 1) and r(k) = (r - s - k + 1)(r - 2k + 2)(r - 2k + 1)k. The solution is
β0(n) = (s - n)(r - n + 1)(r - 2n + 1) ,
β1(n) = (rs - s2 - 2rn + 2n2 - 2r + 2n)(r - 2n - 1) ,
β2(n) = (s - r + n + 1)(n + 2)(r - 2n - 3) ,
α0(n) = r - 2n - 1 ,
and we may conclude that β0(n)Sn + β1(n)Sn+1 + β2(n)Sn+2 = 0 when Sn denotes the stated sum. This suffices to prove the identity by induction, after verifying the cases n = 0 and n = 1.
But Sn also satisfies the simpler recurrence , where  and . Why didn't the method discover this? Well, nobody ever said that such a recurrence necessarily forces the terms  to be indefinitely summable. The surprising thing is that the Gosper-Zeilberger method actually does find the simplest recurrence in so many other cases.
Notice that the second-order recurrence we found can be factored:



where N is the shift operator in (5.145).
5.105 Set a = 1 and compare the coefficients of z3n on both sides of Henrici's "friendly monster" identity,



where f(a, z) = F(1; a, 1; z). The identity can be proved by showing that both sides satisfy the same differential equation.
Peter Paule has found another interesting way to evaluate the sum:



using the binomial theorem, Vandermonde's convolution, and the fact that [z0]g(az) = [z0]g(z). We can now set N = 3n and apply the Gosper-Zeilberger algorithm to this sum Sn, miraculously obtaining the first-order recurrence (n+1)2Sn+1 = 4(4n+1)(4n+3)Sn; the result follows by induction.
If 3n is replaced by 3n + 1 or 3n + 2, the stated sum is zero. Indeed, ∑k+l+m=N t(k, l, m)ωl-m is always zero when N mod 3 ≠ 0 and t(k, l, m) = t(l, m, k).
5.106 (Solution by Shalosh B Ekhad.) Let



The stated equality is routinely verifiable, and (5.32) follows by summing with respect to j and k. (We sum T(r, j + 1, k) - T(r, j, k) first with respect to j, then with respect to k; we sum the other terms U(r, j, k + 1) - U(r, j, k) first with respect to k, then with respect to j.)
Well, we also need to verify (5.32) when r = 0. In that case it reduces via trinomial revision to . We are assuming that l, m, and n are integers and n ≥ 0. Both sides are clearly zero unless n + l ≥ 0. Otherwise we can replace k by n - k and use (5.24).
Notice that 1/nk is proper, since it's (n - 1)!(k - 1)!/ n! k!. Also 1/(n2 - k2) is proper. But 1/(n2 + k2) isn't.
5.107 If it were proper, there would be a linear difference operator that annihilates it. In other words, we would have a finite summation identity



where the α's are polynomials in n, not all zero. Choose integers i, j, and n such that n > 1 and αi,j(n) ≠ 0. Then when k = -1/(n + i) - j, the (i, j) term in the sum is infinite but the other terms are finite.
5.108 Replace k by m - k in the double sum, then use (5.28) to sum on k, getting



trinomial revision (5.21) then yields one of the desired formulas.
It appears to be difficult to find a direct proof that the two symmetrical sums for Am,n are equal. We can, however, prove the equation indirectly with the Gosper-Zeilberger algorithm, by showing that both sums satisfy the recurrence
(n + 1)3Am,n - f(m, n)Am,n+1 + (n + 2)3Am,n+2 = 0,
where f(m, n) = (2n + 3)(n2 + 3n + 2m2 + 2m + 3). Setting  and , we find
(n + 1)2tj(n, k) - f(m, n)tj(n + 1, k) + (n + 2)2tj(n + 2, k)= Tj(n, k + 1) - Tj(n, k) ,
where T1(n, k) = -2(2n + 3)k4t1(n, k)/(n + 1 - k)(n + 2 - k) and T2(n, k) = - ((n + 2)(4mn + n + 3m2 + 8m + 2) - 2(3mn + n + m2 + 6m + 2)k + (2m + 1)k2)k2(m+n+1-k)2t2(n, k)/(n+2-k)2. This proves the recurrence, so we need only verify equality when n = 0 and n = 1. (We could also have used the simpler recurrence
m3Am,n-1 - n3Am-1,n = (m - n)(m2 + n2 - mn)Am-1,n-1 ,
which can be discovered by the method of exercise 101.)
The fact that the first formula for Am,n equals the third implies a remarkable identity between the generating functions ∑m,n Am,nwmzn:



where . It turns out, in fact, that



this is a special case of an identity discovered by Bailey [19].
5.109 Let  for any positive integers a0, a1, . . . , al, and any integer x. Then if 0 ≤ m < p we have



And corresponding terms are congruent (mod p), because exercise 36 implies that they are multiples of p when lj + m ≥ p, exercise 61 implies that the binomials are congruent when lj + m < p, and (4.48) implies that xp ≡ x.
5.110 The congruence surely holds if 2n + 1 is prime. Steven Skiena has also found the example n = 2953, when 2n + 1 = 3·11·179.
Ilan Vardi notes that the condition holds for 2n + 1 = p2, where p is prime, if and only if 2p-1 mod p2 = 1. This yields two more examples: n = (10932-1)/2; n = (35112-1)/2.
5.111 See [96] for partial results. The computer experiments were done by V. A. Vyssotsky.
5.112 If n is not a power of 2, we know from exercise 36 that  is a multiple of 4. Otherwise the stated phenomenon was verified for all values of n ≤ 222000 by A. Granville and O. Ramaré, who also sharpened a theorem of Sárközy [317] by showing that  is divisible by the square of a prime for all n > 222000. This established a long-standing conjecture that  is never squarefree when n > 4.
The analogous conjectures for cubes are that  is divisible by the cube of a prime for all n > 1056, and by either 23 or 33 for all n > 229 + 223. This has been verified for all n < 210000. Paul Erdős conjectures that, in fact,  tends to infinity as n → ∞; this might be true even if we restrict p to the values 2 and 3.
5.113 The theorem about generating functions in exercise 7.20 may help resolve this conjecture.
5.114 Strehl [344] has shown that  is a so-called Franel number [132], and that . In another direction, H. S. Wilf has shown that  is an integer for all m when n ≤ 9.
6.1 2314, 2431, 3241, 1342, 3124, 4132, 4213, 1423, 2143, 3412, 4321.
6.2 , because every such function partitions its domain into k nonempty subsets, and there are mk ways to assign function values for each partition. (Summing over k gives a combinatorial proof of (6.10).)
6.3 Now dk+1 ≤ (center of gravity) -  = 1 -  + (d1 + · · · + dk)/k. This recurrence is like (6.55) but with 1 -  in place of 1; hence the optimum solution is dk+1 = (1 - )Hk. This is unbounded as long as  < 1.
6.4 . (Similarly .)
6.5 Un(x, y) is equal to



and the first sum is



The remaining k-1 can be absorbed, and we have



This proves (6.75). Let Rn(x, y) = x-nUn(x, y); then R0(x, y) = 0 and Rn(x, y) = Rn-1(x, y)+1/n+y/x, hence Rn(x, y) = Hn+ny/x. (Incidentally, the original sum Un = Un(n, -1) doesn't lead to a recurrence such as this; therefore the more general sum, which detaches x from its dependence on n, is easier to solve inductively than its special case. This is another instructive example where a strong induction hypothesis makes the difference between success and failure.)
The Fibonacci recurrence is additive, but the rabbits are multiplying.
6.6 Each pair of babies bb present at the end of a month becomes a pair of adults aa at the end of the next month; and each pair aa becomes an aa and a bb. Thus each bb behaves like a drone in the bee tree and each aa behaves like a queen, except that the bee tree goes backward in time while the rabbits are going forward. There are Fn+1 pairs of rabbits after n months; Fn of them are adults and Fn-1 are babies. (This is the context in which Fibonacci originally introduced his numbers.)
6.7 (a) Set k = 1 - n and apply (6.107). (b) Set m = 1 and k = n - 1 and apply (6.128).
6.8 55 + 8 + 2 becomes 89 + 13 + 3 = 105; the true value is 104.60736.
That "true value" is the length of 65 international miles, but the international mile is actually only .999998 as big as a U. S. statute mile. There are exactly 6336 kilometers in 3937 U. S. statute miles; the Fibonacci method converts 3937 to 6370.
6.9 21. (We go from Fn to Fn+2 when the units are squared. The true answer is about 20.72.)
6.10 The partial quotients a0, a1, a2, . . . are all equal to 1, because ϕ = 1 + 1/ϕ. (The Stern-Brocot representation is therefore RLRLRLRLRL . . . .)
6.11 ; see (6.11).
6.12 This is a consequence of (6.31) and its dual in Table 264.
6.13 The two formulas are equivalent, by exercise 12. We can use induction. Or we can observe that znDn applied to f(z) = zx gives xnzx while ϑn applied to the same function gives xnzx; therefore the sequence ϑ0, ϑ1, ϑ2, . . .  must relate to z0D0, z1D1, z2D2, . . .  as x0, x1, x2, . . .  relates to x0, x1, x2, . . . .
6.14 We have



because (n + 1)x = (k + 1)(x + k - n) + (n - k)(x + k + 1). (It suffices to verify the latter identity when k = 0, k = -1, and k = n.)
6.15 Since , we have the general formula



Set x = 0 and appeal to (6.19).
6.16 ; this sum is always finite.
6.17 (a)  (b)  (c) 
6.18 This is equivalent to (6.3) or (6.8).
6.19 Use Table 272.
6.20 
6.21 The hinted number is a sum of fractions with odd denominators, so it has the form a/b with b odd. (Incidentally, Bertrand's postulate implies that bn is also divisible by at least one odd prime, whenever n > 2.)
6.22 |z/k(k + z)| ≤ 2|z|/k2 when k > 2|z|, so the sum is well defined when the denominators are not zero. If z = n we have , which approaches Hn as m → ∞. (The quantity Hz-1 - γ is often called the psi function ψ(z).)
6.23 z/(ez + 1) = z/(ez - 1) - 2z/(e2z - 1) = ∑n≥0(1 - 2n)Bnzn/n!.
6.24 When n is odd, Tn(x) is a polynomial in x2, hence its coefficients are multiplied by even numbers when we form the derivative and compute Tn+1(x) by (6.95). (In fact we can prove more: The Bernoulli number B2n always has 2 to the first power in its denominator, by exercise 54; hence 22n-k \\T2n+1 ⇔ 2k\\(n + 1). The odd positive integers (n + 1)T2n+1/22n are called Genocchi numbers 1, 1, 3, 17, 155, 2073, . . . , after Genocchi [145].)
(Of course Euler knew the Genocchi numbers long before Genocchi was born; see [110], Volume 2, Chapter 7, §181.)
6.25 100n - nHn < 100(n - 1) - (n - 1)Hn-1 ⇔ Hn-1 > 99. (The least such n is approximately e99-γ, while he finishes at N ≈ e100-γ, about e times as long. So he is getting closer during the final 63% of his journey.)
6.26 Let u(k) = Hk-1 and Δv(k) = 1/k, so that u(k) = v(k). Then we have .
6.27 Observe that when m > n we have gcd(Fm, Fn) = gcd(Fm-n, Fn) by (6.108). This yields a proof by induction.
6.28 (a) Qn = α(Ln - Fn)/2 + βFn. (The solution can also be written .
6.29 When k = 0 the identity is (6.133). When k = 1 it is, essentially,



in Morse code terms, the second product on the right subtracts out the cases where the first product has intersecting dashes. When k > 1, an induction on k suffices, using both (6.127) and (6.132). (The identity is also true when one or more of the subscripts on K become -1, if we adopt the convention that K-1 = 0. When multiplication is not commutative, Euler's identity remains valid for k = n - 1 if we write it in the form



For example, we obtain the somewhat surprising noncommutative factorizations
(abc + a + c)(1 + ba) = (ab + 1)(cba + a + c)
from the case m = 0, n = 3.)
6.30 The derivative of K(x1, . . . , xn) with respect to xm is
K(x1, . . . , xm-1) K(xm+1, . . . , xn),
and the second derivative is zero; hence the answer is
K(x1, . . . , xn) + K(x1, . . . , xm-1) K(xm+1, . . . , xn)y.
6.31 Since , we have . These coefficients, incidentally, satisfy the recurrence



.
6.32  and , both of which appear in Table 265.
6.33 If n > 0, we have , by (6.71); , by (6.19).
6.34 We have ; and in general  is given by (6.38) for all integers n.
6.35 Let n be the least integer > 1/ such that Hn > Hn-1.
6.36 Now dk+1 = (100+(1+d1)+· · ·+(1+dk))/(100+k), and the solution is dk+1 = Hk+100 - H101 + 1 for k ≥ 1. This exceeds 2 when k ≥ 176.
6.37 The sum (by parts) is . The infinite sum is therefore ln m. (It follows that



because νm(k) = (m - 1) ∑j≥1(k mod mj)/mj.)
6.38 . (By parts, using (5.16).)
6.39 Write it as ∑1≤j≤n j-1 ∑j≤k≤n Hk and sum first on k via (6.67), to get



6.40 If 6n - 1 is prime, the numerator of



is divisible by 6n - 1, because the sum is



Similarly if 6n + 1 is prime, the numerator of  is a multiple of 6n + 1. For 1987 we sum up to k = 1324.
6.41 , hence we have Sn+1 + . The answer is Fn+2.
6.42 Fn.
6.43 Set  in Σn≥0 Fnzn = z/(1 - z - z2) to get . The sum is a repeating decimal with period length 44:
0.11235 95505 61797 75280 89887 64044 94382 02247 19101 12359 55+.
6.44 Replace (m, k) by (-m, -k) or (k, -m) or (-k, m), if necessary, so that m ≥ k ≥ 0. The result is clear if m = k. If m > k, we can replace (m, k) by (m - k, m) and use induction.
6.45 Xn = A(n)α+B(n)β+C(n)γ+D(n)δ, where B(n) = Fn, A(n) = Fn-1, A(n) + B(n) - D(n) = 1, and B(n) - C(n) + 3D(n) = n.
6.46 ϕ/2 and ϕ-1/2. Let u = cos 72° and v = cos 36°; then u = 2v2 - 1 and v = 1-2 sin2 18° = 1-2u2. Hence u+v = 2(u+v)(v-u), and 4v2-2v-1 = 0. We can pursue this investigation to find the five complex fifth roots of unity:



"Let p be any old prime."(See [171], p. 419.)
6.47 , and the even powers of  cancel out. Now let p be an odd prime. Then  except when k = (p-1)/2, and  except when k = 0 or k = (p - 1)/2; hence Fp ≡ 5(p-1)/2 and 2Fp+1 ≡ 1 + 5(p-1)/2 (mod p). It can be shown that 5(p-1)/2 ≡ 1 when p has the form 10k ± 1, and 5(p-1)/2 ≡ -1 when p has the form 10k ± 3.
6.48 Let Ki,j = Kj-i+1(xi, . . . , xj). Using (6.133) repeatedly, both sides expand to (K1,m-2(xm-1 + xm+1) + K1,m-3)Km+2,n + K1,m-2Km+3,n.
6.49 Set  in (6.146); the partial quotients are 0, 2F0, 2F1, 2F2, . . .  . (Knuth [206] noted that this number is transcendental.)
6.50 (a) f(n) is even ⇔ 3\n. (b) If the binary representation of n is (1a1 0a2 . . . 1am-1 0am )2, where m is even, we have f(n)=K(a1, a2, . . . , am-1).
6.51 (a) Combinatorial proof: The arrangements of {1, 2, . . . , p} into k subsets or cycles are divided into "orbits" of 1 or p arrangements each, if we add 1 to each element modulo p. For example,



We get an orbit of size 1 only when this transformation takes an arrangement into itself; but then k = 1 or k = p. Alternatively, there's an algebraic proof: We have xp ≡ xp + x1 and xp ≡ xp - x (mod p), since Fermat's theorem tells us that xp - x is divisible by (x - 0)(x - 1) . . . (x - (p-1)).
(b) This result follows from (a) and Wilson's theorem; or we can use .
(c) We have  for 3 ≤ k ≤ p, then  for 4 ≤ k ≤ p, etc. (Similarly, we have .)
(d) . But , so



is a multiple of p2. (This is called Wolstenholme's theorem.)
(Attention, computer programmers: Here's an interesting condition to test, for as many primes as you can.)
6.52 (a) Observe that , where . (b) Working mod 5 we have Hr = 0, 1, 4, 1, 0 for 0 ≤ r ≤ 4. Thus the first solution is n = 4. By part (a) we know that 5\an ⇒ 5\an/5; so the next possible range is n = 20 + r, 0 ≤ r ≤ 4, when we have Hn = . The numerator of , like the numerator of H4, is divisible by 25. Hence the only solutions in this range are n = 20 and n = 24. The next possible range is n = 100 + r; now , which is  plus a fraction whose numerator is a multiple of 5. If , where m is an integer, the harmonic number H100+r will have a numerator divisible by 5 if and only if m + Hr ≡ 0 (mod 5); hence m must be ≡ 0, 1, or 4. Working modulo 5 we find ; hence there are no solutions for 100 ≤ n ≤ 104. Similarly there are none for 120 ≤ n ≤ 124; we have found all three solutions.
(By exercise 6.51(d), we always have p2\ap-1, p\ap2-p, and p\ap2-1, if p is any prime ≥ 5. The argument just given shows that these are the only solutions to p\an if and only if there are no solutions to p-2Hp-1 + Hr ≡ 0 (mod p) for 0 ≤ r < p. The latter condition holds not only for p = 5 but also for p = 13, 17, 23, 41, and 67—perhaps for infinitely many primes. The numerator of Hn is divisible by 3 only when n = 2, 7, and 22; it is divisible by 7 only when n = 6, 42, 48, 295, 299, 337, 341, 2096, 2390, 14675, 16731, 16735, and 102728. See the answer to exercise 92.)
6.53 Summation by parts yields



(The numerators of Bernoulli numbers played an important role in early studies of Fermat's Last Theorem; see Ribenboim [308].)
6.54 (a) If m ≥ p we have Sm(p) ≡ Sm-(p-1)(p) (mod p), since kp-1 ≡ 1 when 1 ≤ k < p. Also Sp-1(p) ≡ p - 1 ≡ -1. If 0 < m < p - 1, we can write



(b) The condition in the hint implies that the denominator of I2n is not divisible by any prime p; hence I2n must be an integer. To prove the hint, we may assume that n>1. Then



is an integer, by (6.78), (6.84), and part (a). So we want to verify that none of the fractions  has a denominator divisible by p. The denominator of  isn't divisible by p, since Bk has no p2 in its denominator (by induction); and the denominator of p2n-k-1/(2n - k + 1) isn't divisible by p, since 2n - k + 1 < p2n-k when k ≤ 2n-2; QED. (The numbers I2n are tabulated in [224]. Hermite calculated them through I18 in 1875 [184]. It turns out that I2 = I4 = I6 = I8 = I10 = I12 = 1; hence there is actually a "simple" pattern to the Bernoulli numbers displayed in the text, including . But the numbers I2n don't seem to have any memorable features when 2n > 12. For example, , and 86579 is prime.)
(c) The numbers 2-1 and 3-1 always divide 2n. If n is prime, the only divisors of 2n are 1, 2, n, and 2n, so the denominator of B2n for prime n > 2 will be 6 unless 2n+1 is also prime. In the latter case we can try 4n+3, 8n+7, . . . , until we eventually hit a nonprime (since n divides 2n-1n + 2n-1 - 1). (This proof does not need the more difficult, but true, theorem that there are infinitely many primes of the form 6k + 1.) The denominator of B2n can be 6 also when n has nonprime values, such as 49.
6.55 The stated sum is , by Vandermonde's convolution. To get (6.70), differentiate and set x = 0.
6.56 First replace kn+1 by ((k - m) + m)n+1 and expand in powers of k - m; simplifications occur as in the derivation of (6.72). If m > n or m < 0, the answer is . Otherwise we need to take the limit of (5.41) minus the term for k = m, as x → -m; the answer comes to .
6.57 First prove by induction that the nth row contains at most three distinct values An ≥ Bn ≥ Cn; if n is even they occur in the cyclic order [Cn, Bn, An, Bn, Cn], while if n is odd they occur in the cyclic order [Cn, Bn, An, An, Bn]. Also


A2n+1 =
A2n + B2n ;
A2n =
2A2n-1 ;


B2n+1 =
B2n + C2n ;
B2n =
A2n-1 + B2n-1 ;


C2n+1 =
2C2n ;
C2n =
B2n-1 + B2n-1 .


It follows that Qn = An - Cn = Fn+1. (See exercise 5.75 for wraparound binomial coefficients of order 3.)
6.58 (a) . (Square Binet's formula (6.123) and sum on n, then combine terms so that ϕ and  disappear.) (b) Similarly,



It follows that . (The corresponding recurrence for mth powers involves the Fibonomial coefficients of exercise 86; it was discovered by Jarden and Motzkin [194].)
6.59 Let m be fixed. We can prove by induction on n that it is, in fact, possible to find such an x with the additional condition x ≢ 2 (mod 4). If x is such a solution, we can move up to a solution modulo 3n+1 because



either x or x + 8·3n-1 or x + 16·3n-1 will do the job.
6.60 F1 + 1, F2 + 1, F3 + 1, F4 - 1, and F6 - 1 are the only cases. Otherwise the Lucas numbers of exercise 28 arise in the factorizations



(We have Fm+n - (-1)nFm-n = LmFn in general.)
6.61 1/F2m = Fm-1/Fm - F2m-1/F2m when m is even and positive. The second sum is 5/4 - F3·2n-1/F3·2n, for n ≥ 1.
6.62 (a)  and . Incidentally, we also have  and . (b) A table of small values reveals that



(c) Bn/An+1 - Bn-1/An = 1/(F2n+1 + 1) because  and . Notice that Bn/An+1 = (Fn/Fn+1)[n even]+(Ln/Ln+1)[n odd]. (d) Similarly, . This quantity can also be expressed as (5Fn/Ln+1)[n even] + (Ln/Fn+1)[n odd].
6.63 (a) . There are  with πn = n and  with πn < n. (b) . Each permutation ρ1 . . . ρn-1 of {1, . . . , n-1} leads to n permutations π1π2 . . . πn = ρ1 . . . ρj-1 n ρj+1 . . . ρn-1ρj. If ρ1 . . . ρn-1 has k excedances, there are k+1 values of j that yield k excedances in π1π2 . . . πn; the remaining n-1-k values yield k+1. Hence the total number of ways to get k excedances in π1π2 . . . πn is .
6.64 The denominator of , by the proof in exercise 5.72. The denominator of  is the same, by (6.44), because  and  is even for k > 0.
6.65 This is equivalent to saying that  is the probability that we have x1 + · · · + xn = k, when x1, . . . , xn are independent random numbers uniformly distributed between 0 and 1. Let yj = (x1 + · · · + xj) mod 1. Then y1, . . . , yn are independently and uniformly distributed, and x1 + · · · + xn is the number of descents in the y's. The permutation of the y's is random, and the probability of k descents is the same as the probability of k ascents.
6.66 2n+1(2n+1 - 1)Bn+1/(n + 1), if n > 0. (See (7.56) and (6.92); the desired numbers are essentially the coefficients of 1 - tanh z.)
6.67 It is  by (6.3) and (6.40). Now use (6.34). (This identity has a combinatorial interpretation [59].)
6.68 We have the general formula



analogous to (6.38). When m = 2 this equals



6.69 . (It would be nice to automate the derivation of formulas such as this.)
6.70 1/k - 1/(k + z) = z/k2 - z2/k3 + · · · , which converges when |z| < 1.
6.71 Note that . If  we find f(z)/z! + γ = Hz.
6.72 For tan z, we can use tan z = cot z - 2 cot 2z (which is equivalent to the identity of exercise 23). Also  has the power series ∑n≥0(-1)n-1(4n - 2)B2nz2n/(2n)!; and



because  and .
6.73 cot(z + π) = cot z and ; hence the identity is equivalent to



which follows by induction from the case n = 1. The stated limit follows since z cot z → 1 as z → 0. It can be shown that term-by-term passage to the limit is justified, hence (6.88) is valid. (Incidentally, the general formula



is also true. It can be proved from (6.88), or from



which is equivalent to the partial fraction expansion of 1/(zn - 1).)
6.74 Since tan 2z + sec 2z = (sin z + cos z)/(cos z - sin z), setting x = 1 in (6.94) gives Tn(1) = 2nEn, where 1/cos z = ∑n≥0 E2nz2n/(2n)!. (The coefficients En are called Euler numbers in combinatorics, not to be confused with the Eulerian numbers . We have E0, E1, E2, . . .  = 1, 1, 1, 2, 5, 16, 61, 272, 1385, 7936, 50521, . . . . Numerical analysts define Euler numbers differently: Their En is (-1)n/2En[n even] in the notation above.)
6.75 Let G(w, z) = sin z/ cos(w+z) and H(w, z) = cos z/ cos(w+z), and let G(w, z) + H(w, z) = ∑m,n Em,nwmzn/m! n!. Then the equations G(w, 0) = 0 and  imply that Em,0 = 0 when m is odd, Em,n+1 = Em+1,n + Em,n when m + n is even; the equations H(0, z) = 1 and  imply that E0,n = [n = 0] when n is even, Em+1,n = Em,n+1+Em,n when m+n is odd. Consequently the nth row below the apex of the triangle contains the numbers En,0, En-1,1, . . . , E0,n. At the left, En,0 is the secant number En [n even]; at the right, E0,n = Tn + [n = 0].
6.76 Let An denote the sum. Looking ahead to equation (7.49), we see that . Therefore, by exercise 23 or 72,
An = (2n+1 - 4n+1)Bn+1/(n + 1) = (-1)(n+1)/2Tn + [n = 0].
6.77 This follows by induction on m, using the recurrence in exercise 18. It can also be proved from (6.50), using the fact that



The latter equation, incidentally, is equivalent to



6.78 If p(x) is any polynomial of degree ≤ n, we have



because this equation holds for x = 0, -1, . . . , -n. The stated identity is the special case where p(x) = xσn(x) and x = 1. Incidentally, we obtain a simpler expression for Bernoulli numbers in terms of Stirling numbers by setting k = 1 in (6.99):



6.79 Sam Loyd [256, pages 288 and 378] gave the construction



and claimed to have invented (but not published) the 64 = 65 arrangement in 1858. (Similar paradoxes go back at least to the eighteenth century, but Loyd found better ways to present them.)
He also published it in the Brooklyn Daily Eagle (28 August 1904), 39; (11 September 1904), 37.
6.80 We expect Am/Am-1 ≈ ϕ, so we try Am-1 = 618034 + r and Am-2 = 381966-r. Then Am-3 = 236068+2r, etc., and we find Am-18 = 144-2584r, Am-19 = 154 + 4181r. Hence r = 0, x = 154, y = 144, m = 20.
6.81 If P(Fn+1, Fn) = 0 for infinitely many even values of n, then P(x, y) is divisible by U(x, y) - 1, where U(x, y) = x2 - xy - y2. For if t is the total degree of P, we can write



Then



and we have  by taking the limit as n → ∞. Hence Q(x, y) is a multiple of U(x, y), say A(x, y)U(x, y). But U(Fn+1, Fn) = (-1)n and n is even, so P0(x, y) = P(x, y) - (U(x, y) - 1)A(x, y) is another polynomial such that P0(Fn+1, Fn) = 0. The total degree of P0 is less than t, so P0 is a multiple of U - 1 by induction on t.
Similarly, P(x, y) is divisible by U(x, y) + 1 if P(Fn+1, Fn) = 0 for infinitely many odd values of n. A combination of these two facts gives the desired necessary and sufficient condition: P(x, y) is divisible by U(x, y)2 - 1.
6.82 First add the digits without carrying, getting digits 0, 1, and 2. Then use the two carry rules



always applying the leftmost applicable carry. This process terminates because the binary value obtained by reading (bm . . . b2)F as (bm . . . b2)2 increases whenever a carry is performed. But a carry might propagate to the right of the "Fibonacci point"; for example, (1)F +(1)F becomes (10.01)F. Such rightward propagation extends at most two positions; and those two digit positions can be zeroed again by using the text's "add 1" algorithm if necessary.
Incidentally, there's a corresponding "multiplication" operation on nonnegative integers: If m = Fj1 +· · ·+Fjq and n = Fk1 +· · ·+Fkr in the Fibonacci number system, let , by analogy with multiplication of binary numbers. (This definition implies that  when m and n are large, although 1 ο n ≈ ϕ2n.) Fibonacci addition leads to a proof of the associative law l ο (m ο n) = (l ο m) ο n.
Exercise: m ο n = mn + (m+1)/ϕn + m (n+1)/ϕ.
6.83 Yes; for example, we can take


A0 =
  331635635998274737472200656430763 ;


A1 =
1510028911088401971189590305498785 .


The resulting sequence has the property that An is divisible by (but unequal to) pk when n mod mk = rk, where the numbers (pk, mk, rk) have the following 18 respective values:


(3, 4, 1)
 
(2, 3, 2)
 
(5, 5, 1)


(7, 8, 3)
 
(17, 9, 4)
 
(11, 10, 2)


(47, 16, 7)
 
(19, 18, 10)
 
(61, 15, 3)


(2207, 32, 15)
 
(53, 27, 16)
 
(31, 30, 24)


(1087, 64, 31)
 
(109, 27, 7)
 
(41, 20, 10)


(4481, 64, 63)
 
(5779, 54, 52)
 
(2521, 60, 60)


One of these triples applies to every integer n; for example, the six triples in the first column cover every odd value of n, and the middle column covers all even n that are not divisible by 6. The remainder of the proof is based on the fact that Am+n = AmFn-1 + Am+1Fn, together with the congruences
A0 ≡ Fmk-rk mod pk ,
A1 ≡ Fmk-rk+1 mod pk ,
for each of the triples (pk, mk, rk). (An improved solution, in which A0 and A1 are numbers of "only" 17 digits each, is also possible [218].)
6.84 The sequences of exercise 62 satisfy A-m = Am, B-m = -Bm, and
AmAn = Am+n + Am-n ;
AmBn = Bm+n - Bm-n ;
BmBn = Am+n - Am-n .
Let fk = Bmk/Amk+l and gk = Amk/Bmk+l, where . Then fk+1 - fk = AlBm/(A2mk+n + Am) and gk - gk+1 = AlBm/(A2mk+n - Am); hence we have



6.85 The property holds if and only if N has one of the seven forms 5k, 2·5k, 4·5k, 3j·5k, 6·5k, 7·5k, 14·5k.
6.86 For any positive integer m, let r(m) be the smallest index j such that Cj is divisible by m; if no such j exists, let r(m) = ∞. Then Cn is divisible by m if and only if gcd(Cn, Cr(m)) is divisible by m if and only if Cgcd(n,r(m)) is divisible by m if and only if gcd(n, r(m)) = r(m) if and only if n is divisible by r(m).
(Conversely, the gcd condition is easily seen to be implied by the condition that the sequence C1, C2, . . . has a function r(m), possibly infinite, such that Cn is divisible by m if and only if n is divisible by r(m).)
Now let Π(n) = C1C2 . . . Cn, so that



If p is prime, the number of times p divides Π(n) is fp(n) = ∑k≥1 n/r(pk), since n/pk is the number of elements {C1, . . . , Cn} that are divisible by pk. Therefore fp(m + n) ≥ fp(m) + fp(n) for all p, and  is an integer.
6.87 The matrix product is



This relates to products of L and R as in (6.137), because we have



The determinant is Kn(x1, . . . , xn); the more general tridiagonal determinant



satisfies the recurrence Dn = xnDn-1 - ynDn-2.
6.88 Let α-1 = a0 + 1/(a1 + 1/(a2 + · · · )) be the continued fraction representation of α-1. Then we have



where



A proof analogous to the text's proof of (6.146) uses a generalization of Zeckendorf's theorem (Fraenkel [129, §4]). If z = 1/b, where b is an integer ≥ 2, this gives the continued fraction representation of the transcendental number (b - 1) ∑n≥1 b-nα, as in exercise 49.
6.89 Let p = K(0, a1, a2, . . . , am), so that p/n is the mth convergent to the continued fraction. Then α = p/n + (-1)m/nq, where q = K(a1, . . . , am, β) and β > 1. The points {kα} for 0 ≤ k < n can therefore be written



where π1 . . . πn-1 is a permutation of {1, . . . , n - 1}. Let f(v) be the number of such points < v; then f(v) and vn both increase by 1 when v increases from k/n to (k + 1)/n, except when k = 0 or k = n - 1, so they never differ by 2 or more.
6.90 By (6.139) and (6.136), we want to maximize K(a1, . . . , am) over all sequences of positive integers whose sum is ≤ n + 1. The maximum occurs when all the a's are 1, for if j ≥ 1 and a ≥ 1 we have


Kj+k+1(1, . . . , 1, a + 1, b1, . . . , bk)


=
Kj+k+1(1, . . . , 1, a, b1, . . . , bk) + Kj(1, . . . , 1) Kk(b1, . . . , bk)


≤
Kj+k+1(1, . . . , 1, a, b1, . . . , bk) + Kj+k(1, . . . , 1, a, b1, . . . , bk)


=
Kj+k+2(1, . . . , 1, a, b1, . . . , bk).


(Motzkin and Straus [278] show how to solve more general maximization problems on continuants.)
6.91 A candidate for the case n mod  appears in [213, §6], although it may be best to multiply the integers discussed there by some constant involving . An elegant and more convincing proposal has been put forward by Philippe Flajolet and Helmut Prodinger in SIAM Journal on Discrete Mathematics 12 (1999), 155-159.
6.92 (a) David Boyd has shown that there are only finitely many solutions for all p < 500, except perhaps p = 83, 127, 397. (b) The behavior of bn is quite strange: We have bn = lcm(1, . . . , n) for 968 ≤ n ≤ 1066; on the other hand, b600 = lcm(1, . . . , 600)/(33 · 52 · 43). Andrew Odlyzko observes that p divides lcm(1, . . . , n)/bn if and only if kpm ≤ n < (k + 1)pm for some m ≥ 1 and some k < p such that p divides the numerator of Hk. Therefore infinitely many such n exist if it can be shown, for example, that almost all primes have only one such value of k (namely k = p - 1).
Another reason to remember 1066?
6.93 (Brent [38] found the surprisingly large partial quotient 1568705 in eγ, but this seems to be just a coincidence. For example, Gosper has found even larger partial quotients in π: The 453,294th is 12996958 and the 11,504,931st is 878783625.)
6.94 Consider the generating function , which equals ∑k (wF(α′ + β′ + γ′, α′ + β′, α′) + zF(α + γ, α + β, α))k(1), where F(a, b, c) is the differential operator a + bϑw + cϑz.
6.95 An elegant solution to this research problem has been found by Manuel Kauers, Journal of Symbolic Computation 42 (2007), 948-970.
Kauers succeeded even though Stirling numbers are not "holonomic" in the sense of [383].
7.1 Substitute z4 for  and z for  in the generating function, getting 1/(1 - z4 - z2). This is like the generating function for T, but with z replaced by z2. Therefore the answer is zero if m is odd, otherwise Fm/2+1.
7.2 G(z) = 1/(1 - 2z) + 1/(1 - 3z); Ĝ(z) = e2z + e3z.
7.3 Set z = 1/10 in the generating function, getting  ln .
7.4 Divide P(z) by Q(z), getting a quotient T(z) and a remainder P0(z) whose degree is less than the degree of Q. The coefficients of T(z) must be added to the coefficients [zn] P0(z)/Q(z) for small n. (This is the polynomial T(z) in (7.28).)
7.5 This is the convolution of (1 + z2)r with (1 + z)r, so
S(z) = (1 + z + z2 + z3)r.
Incidentally, no simple form is known for the coefficients of this generating function; hence the stated sum probably has no simple closed form. (We can use generating functions to obtain negative results as well as positive ones.)
7.6 Let the solution to g0 = α, g1 = β, gn = gn-1 + 2gn-2 + (-1)nγ be gn = A(n)α + B(n)β + C(n)γ. The function 2n works when α = 1, β = 2, γ = 0; the function (-1)n works when α = 1, β = -1, γ = 0; the function (-1)nn works when α = 0, β = -1, γ = 3. Hence A(n) + 2B(n) = 2n, A(n) - B(n) = (-1)n, and -B(n) + 3C(n) = (-1)nn.
7.7 G(z) = (z/(1 - z)2)G(z) + 1, hence



we have gn = F2n + [n = 0].
I bet that the controversial "fan of order zero" does have one spanning tree.
7.8 Differentiate (1 - z)-x-1 twice with respect to x, obtaining



Now set x = m.
7.9 
7.10 The identity  implies that .
7.11 (a) C(z) = A(z)B(z2)/(1 - z). (b) zB′(z) = A(2z)ez, hence . (c) A(z) = B(z)/(1 - z)r+1, hence B(z) = (1 - z)r+1A(z) and we have .
7.12 Cn. The numbers in the upper row correspond to the positions of +1's in a sequence of +1's and -1's that defines a "mountain range"; the numbers in the lower row correspond to the positions of -1's. For example, the given array corresponds to



7.13 Extend the sequence periodically (let xm+k = xk) and define sn = x1 + · · · + xn. We have sm = l, s2m = 2l, etc. There must be a largest index kj such that skj = j, skj+m = l + j, etc. These indices k1, . . . , kl (mod m) specify the cyclic shifts in question.
For example, in the sequence -2, 1, -1, 0, 1, 1, -1, 1, 1, 1 with m = 10 and l = 2 we have k1 = 17, k2 = 24.
7.14 Ĝ(z) = -2zĜ(z) + Ĝ(z)2 + z (be careful about the final term!) leads via the quadratic formula to



Hence g2n+1 = 0 and g2n = (-1)n(2n)! Cn-1, for all n > 0.
7.15 There are  partitions with k other objects in the subset containing n+1. Hence . The solution to this differential equation is , and c = -1 since . (We can also get this result by summing (7.49) on m, since .)
7.16 One way is to take the logarithm of
B(z) = 1/((1 - z)a1 (1 - z2)a2 (1 - z3)a3 (1 - z4)a4 . . .),
then use the formula for ln  and interchange the order of summation.
7.17 This follows since . There's also a formula that goes in the other direction:



7.18 (a)  (b) -ζ′(z); (c) ζ(z)/ζ(2z). Every positive integer is uniquely representable as m2q, where q is squarefree.
7.19 If n > 0, the coefficient [zn] exp(x ln F(z)) is a polynomial of degree n in x that's a multiple of x. The first convolution formula comes from equating coefficients of zn in F(z)xF(z)y = F(z)x+y. The second comes from equating coefficients of zn-1 in F′(z)F(z)x-1F(z)y = F′(z)F(z)x+y-1, because we have



(Further convolutions follow by taking ∂/∂x, as in (7.43).)
Still more is true, as shown in [221]: We have



for arbitrary x, y, and t. In fact, xfn(x + tn)/(x + tn) is the sequence of polynomials for the coefficients of Ft(z)x, where
Ft(z) = F(zFt(z)t) .
(We saw special cases in (5.59) and (6.52).)
7.20 Let G(z) = ∑n≥0 gnzn. Then



for all k, l ≥ 0, if we regard gn = 0 for n < 0. Hence if P0(z), . . . , Pm(z) are polynomials, not all zero, having maximum degree d, then there are polynomials p0(n), . . . , pm+d(n) such that



Therefore a differentiably finite G(z) implies that



The converse is similar. (One consequence is that G(z) is differentiably finite if and only if the corresponding egf, Ĝ(z), is differentiably finite.)
7.21 This is the problem of giving change with denominations 10 and 20, so G(z) = 1/(1 - z10)(1 - z20) = (z10), where (z) = 1/(1 - z)(1 - z2). (a) The partial fraction decomposition of (z) is , so . Setting n = 50 yields 26 ways to make the payment. (b) (z) = (1 + z)/(1 - z2)2 = (1 + z)(1 + 2z2 + 3z4 + · · · ), so [zn] (z) = n/2 + 1. (Compare this with the value Nn = n/5 + 1 in the text's coin-changing problem. The bank robber's problem is equivalent to the problem of making change with pennies and tuppences.)
This slow method of finding the answer is just the cashier's way of stalling until the police come.
The USA has two-cent pieces, but they haven't been minted since 1873.
7.22 Each polygon has a "base" (the line segment at the bottom). If A and B are triangulated polygons, let AB be the result of pasting the base of A to the upper left diagonal of , and pasting the base of B to the upper right diagonal. Thus, for example,



(The polygons might need to be warped a bit and/or banged into shape.) Every triangulation arises in this way, because the base line is part of a unique triangle and there are triangulated polygons A and B at its left and right.
Replacing each triangle by z gives a power series in which the coefficient of zn is the number of triangulations with n triangles, namely the number of ways to decompose an (n + 2)-gon into triangles. Since P = 1 + zP2, this is the generating function for Catalan numbers C0 + C1z + C2z2 + · · · ; the number of ways to triangulate an n-gon is .
7.23 Let an be the stated number, and bn the number of ways with a 2×1×1 notch missing at the top. By considering the possible patterns visible on the top surface, we have
an = 2an-1 + 4bn-1 + an-2 + [n = 0] ;bn = an-1 + bn-1 .
Hence the generating functions satisfy A = 2zA + 4zB + z2A + 1, B = zA + zB, and we have



This formula relates to the problem of 3 × n domino tilings; we have an = , which is  rounded to the nearest integer.
"Curiously, a2n is equal to , the square of the number of ways to tile a 3 × 2n rectangle with dominoes; and ."
—I. Kaplansky
7.24 n∑k1+···+km=n k1 · . . . · km/m = F2n+1 + F2n-1 - 2. (Consider the coefficient , where G(z) = z/(1 - z)2.)
7.25 The generating function is P(z)/(1 - zm), where P(z) = z + 2z2 + · · · + (m - 1)zm-1 = ((m - 1)zm+1 - mzm + z)/(1 - z)2. The denominator is Q(z) = 1 - zm = (1 - ω0z)(1 - ω1z) . . . (1 - ωm-1z). By the rational expansion theorem for distinct roots, we obtain



7.26  leads to  as in equation (7.61).
7.27 Each oriented cycle pattern begins with  or  or a 2 × k cycle (for some k ≥ 2) oriented in one of two ways. Hence
Qn = Qn-1 + Qn-2 + 2Qn-2 + 2Qn-3 + · · · + 2Q0
for n ≥ 2; Q0 = Q1 = 1. The generating function is therefore



and .
7.28 In general if A(z) = (1 + z + · · · + zm-1)B(z), we have Ar + Ar+m + Ar+2m + · · · = B(1) for 0 ≤ r < m. In this case m = 10 and B(z) = (1 + z + · · · + z9)(1 + z2 + z4 + z6 + z8)(1 + z5).
7.29 , so the answer is .
7.30 , by exercise 5.39.
7.31 The dgf is ζ(z)2/ζ(z-1); hence we find g(n) is the product of (k+1-kp) over all prime powers pk that exactly divide n.
7.32 We may assume that each bk ≥ 0. A set of arithmetic progressions forms an exact cover if and only if



Subtract zbm/(1 - zam) from both sides and set z = e2πi/am. The left side is infinite, and the right side will be finite unless am-1 = am.
7.33 (-1)n-m+1[n > m]/(n - m).
7.34 We can also write . In general, if



we have Gn = z1Gn-1 + z2Gn-2 + · · · + zrGn-r + [n = 0], and the generating function is 1/(1 - z1w - z2w2 - · · · - zrwr). In the stated special case the answer is 1/(1 - w - zmwm+1). (See (5.74) for the case m = 1.)
7.35 (a) . (b)  by (7.50) and (6.58). Another way to do part (b) is to use the rule  with .
7.36 
7.37 (a) The amazing identity a2n = a2n+1 = bn holds in the table



(b) A(z) = 1/((1 - z)(1 - z2)(1 - z4)(1 - z8) . . .). (c) B(z) = A(z)/(1 - z), and we want to show that A(z) = (1 + z)B(z2). This follows from A(z) = A(z2)/(1 - z).
7.38 (1 - wz)M(w, z) = ∑m,n≥1(min(m, n) - min(m-1, n-1))wmzn = ∑m,n≥1 wmzn = wz/(1 - w)(1 - z). In general,



7.39 The answers to the hint are



respectively. Therefore: (a) We want the coefficient of zm in the product (1 + z)(1 + 2z) . . . (1 + nz). This is the reflection of , so it is  and the answer is . (b) The coefficient of zm in 1/((1- z)(1 - 2z)...(1 - nz)) is  by (7.47).
7.40 The egf for nFn-1 - Fn is  where . The egf for n¡ is e-z/(1 - z). The product is



We have . So the answer is (-1)nFn.
7.41 The number of up-down permutations with the largest element n in position 2k is . Similarly, the number of up-down permutations with the smallest element 1 in position 2k + 1 is , because down-up permutations and up-down permutations are equally numerous. Summing over all possibilities gives



The egf Â therefore satisfies 2Â′(z) = Â(z)2 +1 and Â(0) = 1; the given function solves this differential equation. (Consequently An is the Euler number En of exercise 6.74, namely a secant number when n is even, a tangent number when n is odd.)
7.42 Let an be the number of Martian DNA strings that don't end with c or e; let bn be the number that do. Then



and the total number is [zn](1 + z)/(1 - 4z - z2) = F3n+2.
7.43 By (5.45), gn = ΔnĠ(0). The nth difference of a product can be written



and . Therefore we find



This is a sum over all trinomial coefficients; it can be put into the more symmetric form



The empty set is pointless.
7.44 Each partition into k nonempty subsets can be ordered in k! ways, so bk = k!. Thus . And this is the geometric series ∑k≥0 ekz/2k+1, hence ak = 1/2k+1. Finally, ck = 2k; consider all permutations when the x's are distinct, change each '>' between subscripts to '<' and allow each '<' between subscripts to become either '<' or '='. (For example, the permutation x1x3x2 produces x1 < x3 < x2 and x1 = x3 < x2, because 1 < 3 > 2.)
7.45 This sum is ∑n≥1 r(n)/n2, where r(n) is the number of ways to write n as a product of two relatively prime factors. If n is divisible by t distinct primes, r(n) = 2t. Hence r(n)/n2 is multiplicative and the sum is



7.46 Let . Then Sn = Sn-1 + αSn-3 + [n = 0], and the generating function is 1/(1 - z - αz3). When , the hint tells us that this has a nice factorization . The general expansion theorem now tells us that , and the remaining constant c turns out to be .
7.47 The Stern-Brocot representation of  is R(LR2)∞, because



The fractions are  ; they eventually have the cyclic pattern



7.48 We have g0 = 0, and if g1 = m the generating function satisfies



Hence G(z) = P(z)/(az2 + bz + c)(1 - z) for some polynomial P(z). Let ρ1 and ρ2 be the roots of cz2 + bz + a, with |ρ1| ≥ |ρ2|. If b2 - 4ac ≤ 0 then |ρ1|2 = ρ1ρ2 = a/c is rational, contradicting the fact that  approaches . Hence ; and this implies that a = -c, b = -2c, . The generating function now takes the form



where r = d/c. Since g2 is an integer, r is an integer. We also have



and this can hold only if r = -1, because  alternates in sign as it approaches zero. Hence (a, b, c, d) = ±(1, 2, -1, 1). Now we find , which is between 0 and 1 only if 0 ≤ m ≤ 2. Each of these values actually gives a solution; the sequences gn are 0, 0, 1, 3, 8, . . . , 0, 1, 3, 8, 20, . . . , and 0, 2, 5, 13, 32, . . . .
7.49 (a) The denominator of  is 1 - 2z - z2; hence an = 2an-1 + an-2 for n ≥ 2. (b) True because an is even and . (c) Let



We would like bn to be odd for all n > 0, and . Working as in part (a), we find b0 = 2, b1 = p, and  for n ≥ 2. One satisfactory solution has p = 3 and q = 17.
7.50 Extending the multiplication idea of exercise 22, we have



Replace each n-gon by zn-2. This substitution behaves properly under multiplication, because the pasting operation takes an m-gon and an n-gon into an (m + n - 2)-gon. Thus the generating function is



and the quadratic formula gives . The coefficient of zn-2 in this power series is the number of ways to put nonoverlapping diagonals into a convex n-gon. These coefficients apparently have no closed form in terms of other quantities that we have discussed in this book, but their asymptotic behavior is known [207, exercise 2.2.1-12].
Incidentally, if each n-gon in Q is replaced by wzn-2 we get



a formula in which the coefficient of wmzn-2 is the number of ways to divide an n-gon into m polygons by nonintersecting diagonals.
Give me Legendre polynomials and I'll give you a closed form.
7.51 The key first step is to observe that the square of the number of ways is the number of cycle patterns of a certain kind, generalizing exercise 27. These can be enumerated by evaluating the determinant of a matrix whose eigenvalues are not difficult to determine. When m = 3 and n = 4, the fact that cos 36° = ϕ/2 is helpful (exercise 6.46).
7.52 The first few cases are p0(y) = 1, p1(y) = y, p2(y) = y2 + y, p3(y) = y3 + 3y2 + 3y. Let pn(y) = q2n(x) where y = x(1 - x); we seek a generating function that defines q2n+1(x) in a convenient way. One such function is ∑n qn(x)zn/n! = 2eixz/(eiz + 1), from which it follows that qn(x) = inEn(x), where En(x) is called an Euler polynomial. We have , so Euler polynomials are analogous to Bernoulli polynomials, and they have factors analogous to those in (6.98). By exercise 6.23 we have ; this polynomial has integer coefficients by exercise 6.54. Hence q2n(x), whose coefficients have denominators that are powers of 2, must have integer coefficients. Hence pn(y) has integer coefficients. Finally, the relation  shows that



and it follows that the 's are positive. (A similar proof shows that the related quantity (-1)n(2n + 2)E2n+1(x)/(2x - 1) has positive integer coefficients, when expressed as an nth degree polynomial in y.) It can be shown that  is the Genocchi number (-1)n-1(22n+1 - 2)B2n (see exercise 6.24), and that , etc.
7.53 It is P(1+V4n+1+V4n+3)/6. Thus, for example, T20 = P12 = 210; T285 = P165 = 40755.
7.54 Let Ek be the operation on power series that sets all coefficients to zero except those of zn where n mod m = k. The stated construction is equivalent to the operation
E0 S E0 S (E0 + E1) S ... S (E0 + E1 + · · · + Em-1)
applied to 1/(1 - z), where S means "multiply by 1/(1 - z)." There are m! terms
E0 S Ek1 S Ek2 S ... S Ekm
where 0 ≤ kj < j, and every such term evaluates to zrm/(1 - zm)m+1 if r is the number of places where kj < kj+1. Exactly  terms have a given value of r, so the coefficient of zmn is  by (6.37). (The fact that operation Ek can be expressed with complex roots of unity seems to be of no help in this problem.)
7.55 Suppose that P0(z)F(z) + · · · + Pm(z)F(m)(z) = Q0(z)G(z) + · · · + Qn(z)G(n)(z) = 0, where Pm(z) and Qn(z) are nonzero. (a) Let H(z) = F(z) + G(z). Then there are rational functions Rk,l(z) for 0 ≤ l < m + n such that H(k)(z) = Rk,0(z)F(0)(z) + · · · + Rk,m-1(z)F(m-1)(z) + Rk,m(z)G(0)(z) + · · · + Rk,m+n-1(z)G(n-1)(z). The m + n + 1 vectors (Rk,0(z), . . . , Rk,m+n-1(z)) are linearly dependent in the (m + n)-dimensional vector space whose components are rational functions; hence there are rational functions Sl(z), not all zero, such that S0(z)H(0)(z) + · · · + Sm+n(z)H(m+n)(z) = 0. (b) Similarly, let H(z) = F(z)G(z). There are rational Rk,l(z) for 0 ≤ l < mn with , hence S0(z)H(0)(z) + · · · + Smn(z)H(mn)(z) = 0 for some rational Sl(z), not all zero. (A similar proof shows that if fn and gn are polynomially recursive, so are fn + gn and fngn. Incidentally, there is no similar result for quotients; for example, cos z is differentiably finite, but 1/cos z is not.)
7.56 Euler [113] showed that this number is also , and he gave the formula . He also discovered a "memorable failure of induction" while examining these numbers: Although 3tn - tn+1 is equal to Fn-1(Fn-1 + 1) for 0 ≤ n ≤ 8, this empirical law mysteriously breaks down when n is 9 or more! George Andrews [12] has explained the mystery by showing that the sum ∑k[zn+10k] (1 + z + z2)n can be expressed as a closed form in terms of Fibonacci numbers.
H. S. Wilf observes that [zn] (a + bz + cz2)n= [zn] 1/f(z), where  (see [373, page 159]), and it follows that the coefficients satisfy
(n + 1)An+1 - (2n + 1)bAn + n(b2 - 4ac)An-1 = 0.
The algorithm of Petkovšek [291] can be used to prove that this recurrence has a closed form solution as a finite sum of hypergeometric terms if and only if abc(b2 - 4ac) = 0. Therefore in particular, the middle trinomial coefficients have no such closed form. The next step is presumably to extend this result to a larger class of closed forms (including harmonic numbers and/or Stirling numbers, for example).
Give me Legendre polynomials and I'll give you a closed form.
7.57 (Paul Erdős repeatedly offered $500 for a solution to this problem.)
8.1 . (In fact, we always get doubles with probability  when at least one of the dice is fair.) Any two faces whose sum is 7 have the same probability in distribution Pr1, so S = 7 has the same probability as doubles.
8.2 There are 12 ways to specify the top and bottom cards and 50! ways to arrange the others; so the probability is 12.50!/52! = 12/(51.52) = .
8.3 (3 + 2 + · · · + 9 + 2) = 4.8; (32 + 22 + · · · + 92 + 22 - 10(4.8)2) = , which is approximately 8.6. The true mean and variance with a fair coin are 6 and 22, so Stanford had an unusually heads-up class. The corresponding Princeton figures are 6.4 and . 12.5 (This distribution has κ4 = 2974, which is rather large. Hence the standard deviation of this variance estimate when n = 10 is also rather large,  20.1 according to exercise 54. One cannot complain that the students cheated.)
8.4 This follows from (8.38) and (8.39), because F(z) = G(z)H(z). (A similar formula holds for all the cumulants, even though F(z) and G(z) may have negative coefficients.)
8.5 Replace H by p and T by q = 1 - p. If SA = SB = we have p2qN =  and pq2N =  q + ; the solution is p = 1/φ2, q = 1/φ.
8.6 In this case X|y has the same distribution as X, for all y, hence E(X|Y) = EX is constant and V(E(X|Y)) = 0. Also V(X|Y) is constant and equal to its expected value.
8.7 We have  by Chebyshev's monotonic inequality of Chapter 2.
8.8 Let p = Pr(ω  A ∩ B), q = Pr(ω ∉A), and r = Pr(ω ∉B). Then p + q + r = 1, and the identity to be proved is p = (p + r)(p + q) - qr.
8.9 This is true (subject to the obvious proviso that F and G are defined on the respective ranges of X and Y), because



8.10 Two. Let x1 < x2 be distinct median elements; then 1 ≤ Pr(X ≤ x1) + Pr(X ≥ x2) ≤ 1, hence equality holds. (Some discrete distributions have no median elements. For example, let Ω be the set of all fractions of the form ±1/n, with Pr(+1/n) = Pr(-1/n)= .)
8.11 For example, let K = k with probability 4/(k + 1)(k + 2)(k + 3), for all integers k ≥ 0. Then EK = 1, but E(K2) = ∞. (Similarly we can construct random variables with finite cumulants through κm but with κm+1 = ∞.)
8.12 (a) Let pk = Pr(X = k). If 0 < x ≤ 1, we have Pr(X ≤ r) = ∑k≤rpk ≤ ∑k≤r xk-rpk ≤ ∑k xk-rpk = x-rP(x). The other inequality has a similar proof. (b) Let x = α/(1-α) to minimize the right-hand side. (A more precise estimate for the given sum is obtained in exercise 9.42.)
8.13 (Solution by Boris Pittel.) Let us set Y = (X1 + · · · + Xn)/n and Z = (Xn+1 + · · · + X2n)/n. Then






The last inequality is, in fact, '>' in any discrete probability distribution, because Pr(Y = Z) > 0.
8.14 Mean(H) = p Mean(F) + q Mean(G); Var(H) = p Var(F) + q Var(G) + pq (Mean(F)-Mean(G))2. (A mixture is actually a special case of conditional probabilities: Let Y be the coin, let X|H be generated by F(z), and let X|T be generated by G(z). Then VX = EV(X|Y) + VE(X|Y), where EV(X|Y) = pV(X|H) + qV(X|T) and VE(X|Y) is the variance of pzMean(F)+ qzMean(G).)
8.15 By the chain rule, H′(z) = G′(z)F′ (G(z)); H″(z) = G″(z)F′ (G(z)) + G′(z)2F′′ (G(z)). Hence
Mean(H) = Mean(F) Mean(G);
   Var(H) = Var(F) Mean(G)2 + Mean(F) Var(G).
(The random variable corresponding to probability distribution H can be understood as follows: Determine a nonnegative integer n by distribution F; then add the values of n independent random variables that have distribution G. The identity for variance in this exercise is a special case of (8.106), when X has distribution H and Y has distribution F.)
8.16 ew(z-1)/(1 - w).
8.17 Pr(Yn,p ≤ m) = Pr(Yn,p + n ≤ m + n) = probability that we need ≤ m + n tosses to obtain n heads = probability that m + n tosses yield ≥ n heads = Pr(Xm+n,p ≥ n). Thus



and this is (5.19) with n = r, x = q, y = p.
8.18 (a) GX(z) = eμ(z-1). (b) The mth cumulant is μ, for all m ≥ 1. (The case μ = 1 is called F∞ in (8.55).)
8.19 (a) GX1+X2(z) = GX1(z)GX2(z) = e(μ1+μ2)(z-1). Hence the probability is e-μ1-μ2 (μ1 + μ2)n/n!; the sum of independent Poisson variables is Poisson. (b) In general, if KmX denotes the mth cumulant of a random variable X, we have Km(aX1 + bX2) = am(KmX1) + bm(KmX2), when a, b ≥ 0. Hence the answer is 2mμ1 + 3mμ2.
8.20 The general pgf will be G(z) = zm/F(z), where



8.21 This is ∑n≥0 qn, where qn is the probability that the game between Alice and Bill is still incomplete after n flips. Let pn be the probability that the game ends at the nth flip; then pn + qn = qn-1. Hence the average time to play the game is ∑n≥1npn = (q0 - q1) + 2(q1 - q2) + 3(q2 - q3) + · · · = q0 + q1 + q2 + · · · = N, since limn→∞ nqn = 0.
Another way to establish this answer is to replace H and T by z. Then the derivative of the first equation in (8.78) tells us that .
By the way, .
8.22 By definition we have V(X|Y) = E(X2|Y)-(E(X|Y))2) and V(E(X|Y)) = E((E(X|Y))2)-(E(E(X|Y)))2; hence E(V(X|Y))+ V(E(X|Y))= E(E(X2|Y))-(E(E(X|Y)))2. But E(E(X|Y))= ∑y Pr(Y = y)E(X|y) = ∑x,y Pr(Y = y) × Pr((X|y) = x)x = EX and E(E(X2|Y)) = E(X2), so the result is just VX.
8.23 Let  and ; and let Ω2 be the other 16 elements of Ω. Then  according as ω  Ω0, Ω1, Ω2. The events A must therefore be chosen with kj elements from Ωj, where (k0, k1, k2) is one of the following: (0, 0, 0), (0, 2, 7), (0, 4, 14), (1, 4, 4), (1, 6, 11), (2, 6, 1), (2, 8, 8), (2, 10, 15), (3, 10, 5), (3, 12, 12), (4, 12, 2), (4, 14, 9), (4, 16, 16). For example, there are  events of type (2, 6, 1). The total number of such events is [z0](1 + z20)4(1 + z-7)16(1 + z2)16, which turns out to be 1304872090. If we restrict ourselves to events that depend on S only, we get 40 solutions S  A, where , and the complements of these sets. (Here the notation '' means either 2 or 12 but not both.)
8.24 (a) Any one of the dice ends up in J's possession with probability ; hence . Let . Then the pgf for J's total holdings is (q + pz)2n+1, with mean (2n + 1)p and variance (2n + 1)pq, by (8.61). (b) .
8.25 The pgf for the current stake after n rolls is Gn(z), where



(The noninteger exponents cause no trouble.) It follows that Mean(Gn) = Mean(Gn-1), and . So the mean is always A, but the variance grows to .
This problem can perhaps be solved more easily without generating functions than with them.
8.26 The pgf Fl,n(z) satisfies ; hence  and ; the variance is easily computed. (In fact, we have



which approaches a Poisson distribution with mean 1/l as n → ∞.)
8.27 (n2Σ3 - 3nΣ2Σ1 + 2Σ)/n(n - 1)(n - 2) has the desired mean, where . This follows from the identities


EΣ3 =
nμ3;


E(Σ2Σ1) =
nμ3 + n(n - 1)μ2μ1;


E(Σ) =
nμ3 + 3n(n - 1)μ2μ1 + n(n - 1)(n - 2)μ.


Incidentally, the third cumulant is κ3 = E((X-EX)3), but the fourth cumulant does not have such a simple expression; we have κ4 = E((X - EX)4)- 3(VX)2.
8.28 (The exercise implicitly calls for p = q =, but the general answer is given here for completeness.) Replace H by pz and T by qz, getting SA(z) = p2qz3/(1 - pz)(1 - qz)(1 - pqz2) and SB(z) = pq2z3/(1 - qz)(1 - pqz2). The pgf for the conditional probability that Alice wins at the nth flip, given that she wins the game, is



This is a product of pseudo-pgf's, whose mean is 3+p/q+q/p+2pq/(1-pq). The formulas for Bill are the same but without the factor q/(1 - pz), so Bill's mean is 3 + q/p + 2pq/(1 - pq). When p = q =, the answer in case (a) is ; in case (b) it is . Bill wins only half as often, but when he does win he tends to win sooner. The overall average number of flips is , agreeing with exercise 21. The solitaire game for each pattern has a waiting time of 8.
8.29 Set H = T =  in
1 + N(H + T) = N + SA + SB + SC
 N HHTH = SA(HTH + 1) + SB(HTH + TH) + SC(HTH + TH)
 N HTHH = SA(THH + H) + SB(THH + 1) + SC(THH)
 N THHH = SA(HH) + SB(H) + SC
to get the winning probabilities. In general we will have SA + SB + SC = 1 and
SA(A:A) + SB(B:A) + SC(C:A) = SA(A:B) + SB(B:B) + SC(C:B)
      = SA(A:C) + SB(B:C) + SC(C:C).
In particular, the equations 9SA + 3SB + 3SC = 5SA + 9SB + SC = 2SA + 4SB + 8SC imply that , , .
8.30 The variance of P(h1, . . . , hn; k)|k is the variance of the shifted binomial distribution ((m - 1 + z)/m)k-1z, which is  by (8.61). Hence the average of the variance is Mean(S)(m - 1)/m2. The variance of the average is the variance of (k - 1)/m, namely Var(S)/m2. According to (8.106), the sum of these two quantities should be VP, and it is. Indeed, we have just replayed the derivation of (8.96) in slight disguise. (See exercise 15.)
8.31 (a) A brute force solution would set up five equations in five unknowns:
A = 1;   B =  zA +  zC;   C =  zB +  zD;
D =  zC +  zE;   E =  zD +  zA.
But positions C and D are equidistant from the goal, as are B and E, so we can set C = D and B = E in these generating functions for the probabilities. Only two equations now remain to be solved:
B =  z +  zC;   C =  zB +  zC.
Hence C = z2/(4 - 2z - z2); we have Mean(C) = 6 and Var(C) = 22. (Rings a bell? In fact, this problem is equivalent to flipping a fair coin until getting heads twice in a row: Heads means "advance toward the apple" and tails means "start over.") (b) Chebyshev's inequality says that Pr(C ≥ 100) = Pr((C - 6)2 ≥ 942) ≤ 22/942 ≈ .0025. (c) The second tail inequality says that Pr(C ≥ 100) ≤ 1/(x98(4 - 2x - x2)) for all x ≥ 1, and we get the upper bound 0.00000005 when . (The actual probability is ∑n≥100 Fn-1/2n = F101/299 ≈ 0.0000000009, according to exercise 37.)
8.32 By symmetry, we can reduce each month's situation to one of four possibilities:
D, the states are diagonally opposite;
A, the states are adjacent and not Kansas;
K, the states are Kansas and one other;
S, the states are the same.
"Toto, I've a feeling we're not in Kansas anymore."
—Dorothy
Considering the Markovian transitions, we get four equations



whose sum is D + K + A + S = 1 + z(D + A + K). The solution is



but the simplest way to find the mean and variance may be to write z = 1+w and expand in powers of w, ignoring multiples of w2:



Now , and . The mean is  and the variance is . (Is there a simpler way?)
8.33 First answer: Clearly yes, because the hash values h1, . . . , hn are independent. Second answer: Certainly no, even though the hash values h1, . . . , hn are independent. We have Pr(Xj = 0) =  sk [j ≠ k](m-1)/m = (1 - sj)(m - 1)/m, but Pr(X1 = X2 = 0) = sk[k > 2](m - 1)2/m2 = (1 - s1 - s2)(m - 1)2/m2 ≠ Pr(X1 = 0) Pr(X2 = 0).
8.34 Let [zn] Sm(z) be the probability that Gina has advanced < m steps after taking n turns. Then Sm(1) is her average score on a par-m hole; [zm] Sm(z) is the probability that she loses such a hole against a steady player; and 1 - [zm-1] Sm(z) is the probability that she wins it. We have the recurrence
 S0(z) = 0;
Sm(z) = 1 + pzSm-2(z) + qzSm-1(z)/(1 - rz),   for m > 0.
To solve part (a), it suffices to compute the coefficients for m, n ≤ 4; it is convenient to replace z by 100w so that the computations involve nothing but integers. We obtain the following tableau of coefficients:


S0
0
0
0
0
0


S1
1
4
16
64
256


S2
1
95
744
4432
23552


S3
1
100
9065
104044
819808


S4
1
100
9975
868535
12964304


Therefore Gina wins with probability 1 - .868535 = .131465; she loses with probability .12964304. (b) To find the mean number of strokes, we compute



(Incidentally, S5(1) ≈ 4.9995; she wins with respect to both holes and strokes on a par-5 hole, but loses either way when par is 3.)
8.35 The condition will be true for all n if and only if it is true for n = 1, by the Chinese remainder theorem. One necessary and sufficient condition is the polynomial identity
(p2+p4+p6 + (p1+p3+p5)w)(p3+p6 + (p1+p4)z + (p2+p5)z2) = (p1wz + p2z2 + p3w + p4z + p5wz2 + p6),
but that just more-or-less restates the problem. A simpler characterization is


(p2 + p4 + p6)(p3 + p6) = p6,
(p1 + p3 + p5)(p2 + p5) = p5,


which checks only two of the coefficients in the former product. The general solution has three degrees of freedom: Let a0 + a1 = b0 + b1 + b2 = 1, and put p1 = a1b1, p2 = a0b2, p3 = a1b0, p4 = a0b1, p5 = a1b2, p6 = a0b0.
8.36 (a)  (b) If the kth die has faces with s1, . . . , s6 spots, let pk(z) = zs1 + · · · + zs6. We want to find such polynomials with p1(z) . . . pn(z) = (z + z2 + z3 + z4 + z5 + z6)n. The irreducible factors of this polynomial with rational coefficients are zn(z + 1)n × (z2 + z + 1)n(z2 - z + 1)n; hence pk(z) must be of the form zak (z + 1)bk × (z2 + z + 1)ck (z2 - z + 1)dk. We must have ak ≥ 1, since pk(0) = 0; and in fact ak = 1, since a1 + · · · + an = n. Furthermore the condition pk(1) = 6 implies that bk = ck = 1. It is now easy to see that 0 ≤ dk ≤ 2, since dk > 2 gives negative coefficients. When d = 0 and d = 2, we get the two dice in part (a); therefore the only solutions have k pairs of dice as in (a), plus n - 2k ordinary dice, for some .
8.37 The number of coin-toss sequences of length n is Fn-1, for all n > 0, because of the relation between domino tilings and coin flips. Therefore the probability that exactly n tosses are needed is Fn-1/2n, when the coin is fair. Also qn = Fn+1/2n-1, since ∑k≥n Fkzk = (Fnzn + Fn-1zn+1)/(1 - z - z2). (A systematic solution via generating functions is, of course, also possible.)
8.38 When k faces have been seen, the task of rolling a new one is equivalent to flipping coins with success probability pk = (m - k)/m. Hence the pgf is . The mean is ; the variance is ; and equation (7.47) provides a closed form for the requested probability, namely . (The problem discussed in this exercise is traditionally called "coupon collecting.")
8.39 E(X) = P(-1); V(X) = P(-2) - P(-1)2; E(ln X) = -P′(0).
8.40 (a) We have , by (7.49). Incidentally, the third cumulant is npq(q-p) and the fourth is npq(1-6pq). The identity q+pet = (p+qe-t)et shows that fm(p) = (-1)mfm(q)+[m = 1]; hence we can write fm(p) = gm(pq)(q - p)[m odd], where gm is a polynomial of degree m/2, whenever m > 1. (b) Let p =  and F(t) = ln(  +  et). Then ∑m≥1 κmtm -1/(m-1)! = F′(t) = 1-1/(et+1), and we can use exercise 6.23.
8.41 If G(z) is the pgf for a random variable X that assumes only positive integer values, then  G(z) dz/z = Pk-1 Pr(X = k)/k = E(X-1). If X is the distribution of the number of flips to obtain n + 1 heads, we have G(z) = (pz/(1 - qz))n+1 by (8.59), and the integral is



if we substitute w = pz/(1 - qz). When p = q the integrand can be written (-1)n((1+w)-1-1+w-w2+· · ·+(-1)nwn-1), so the integral is . We have  by (9.28), and it follows that .
8.42 Let Fn(z) and Gn(z) be pgf's for the number of employed evenings, if the man is initially unemployed or employed, respectively. Let qh = 1 - ph and qf = 1 - pf. Then F0(z) = G0(z) = 1, and
Fn(z) = phzGn-1(z) + qhFn-1(z);
Gn(z) = pfFn-1(z) + qfzGn-1(z).
The solution is given by the super generating function



where B(w) = w(qf -(qf -ph)w)/(1-qhw) and A(w) =(1-B(w))/(1-w).
Now ∑n≥0(1)wn = αw/(1 - w)2 + β/(1 - w) - β/1 - (qf - ph)w) where



hence (1) = αn + β(1 - (qf - ph)n). (Similarly (1) = α2n2 + O(n), so the variance is O(n).)
8.43 Gn(z) = ∑k≥0  zk/n! = z/n!, by (6.11). This is a product of binomial pgf's, , where the kth has mean 1/k and variance (k - 1)/k2; hence Mean(Gn) = Hn and Var (Gn) = Hn - .
8.44 (a) The champion must be undefeated in n rounds, so the answer is pn. (b,c) Players x1, . . . , x2k must be "seeded" (by chance) in distinct subtournaments and they must win all 2k(n - k) of their matches. The 2n leaves of the tournament tree can be filled in 2n! ways; to seed it we have 2k!(2n-k)2k ways to place the top 2k players, and (2n - 2k)! ways to place the others. Hence the probability is (2p)2k(n-k)/. When k = 1 this simplifies to (2p2)n-1/(2n - 1). (d) Each tournament outcome corresponds to a permutation of the players: Let y1 be the champ; let y2 be the other finalist; let y3 and y4 be the players who lost to y1 and y2 in the semifinals; let (y5, . . . , y8) be those who lost respectively to (y1, . . . , y4) in the quarterfinals; etc. (Another proof shows that the first round has 2n!/2n-1! essentially different outcomes; the second round has 2n-1!/2n-2!; and so on.) (e) Let Sk be the set of 2k-1 potential opponents of x2 in the kth round. The conditional probability that x2 wins, given that x1 belongs to Sk, is
Pr(x1 plays x2)·pn-1(1 - p) + Pr(x1 doesn't play x2)·pn
                   = pk-1pn-1(1 - p) + (1 - pk-1)pn.
The chance that x1  Sk is 2k-1/(2n - 1); summing on k gives the answer:



(f) Each of the 2n! tournament outcomes has a certain probability of occurring, and the probability that xj wins is the sum of these probabilities over all (2n - 1)! tournament outcomes in which xj is champion. Consider interchanging xj with xj+1 in all those outcomes; this change doesn't affect the probability if xj and xj+1 never meet, but it multiplies the probability by (1 - p)/p < 1 if they do meet.
8.45 (a) A(z) = 1/(3 - 2z); B(z) = zA(z)2; C(z) = z2A(z)3. The pgf for sherry when it's bottled is z3A(z)3, which is z3 times a negative binomial distribution with parameters n = 3, p = . (b) Mean(A) = 2, Var(A) = 6; Mean(B) = 5, Var(B) = 2 Var(A) = 12; Mean(C) = 8, Var(C) = 18. The sherry is nine years old, on the average. The fraction that's 25 years old is . (c) Let the coefficient of wn be the pgf for the beginning of year n. Then



Differentiate with respect to z and set z = 1; this makes



The average age of bottled sherry n years after the process started is 1 greater than the coefficient of wn-1, namely 9-()n(3n2+21n+72)/8. (This already exceeds 8 when n = 11.)
8.46 (a) P(w, z) = 1 +  (wP(w, z) + zP(w, z)) = (1 -  (w + z))-1, hence pm,n = 2-m-n. (b) Pk(w, z) =  (wk + zk)P(w, z); hence



(c) ; this can be summed using (5.20):



(The methods of Chapter 9 show that this is  - 1 + O(n-1/2).
8.47 After n irradiations there are n + 2 equally likely receptors. Let the random variable Xn denote the number of diphages present; then Xn+1 = Xn + Yn, where Yn = -1 if the (n + 1)st particle hits a diphage receptor (conditional probability 2Xn/(n + 2)) and Yn = +2 otherwise. Hence
EXn+1 = EXn + EYn = EXn - 2EXn/(n+2) + 2(1 - 2EXn/(n+2)).
The recurrence (n+2)EXn+1 = (n-4)EXn+2n+4 can be solved if we multiply both sides by the summation factor (n + 1)5; or we can guess the answer and prove it by induction: EXn = (2n + 4)/7 for all n > 4. (Incidentally, there are always two diphages and one triphage after five steps, regardless of the configuration after four.)
8.48 (a) The distance between frisbees (measured so as to make it an even number) is either 0, 2, or 4 units, initially 4. The corresponding generating functions A, B, C (where, say, [zn] C is the probability of distance 4 after n throws) satisfy
A = zB,   B = zB + zC,   C = 1 + zB + zC.
It follows that A = z2/(16 - 20z + 5z2) = z2/F(z), and we have Mean(A) = 2 - Mean(F) = 12, Var(A) = - Var(F) = 100. (A more difficult but more amusing solution factors A as follows:



where , and p1 + q1 = p2 + q2 = 1. Thus, the game is equivalent to having two biased coins whose heads probabilities are p1 and p2; flip the coins one at a time until they have both come up heads, and the total number of flips will have the same distribution as the number of frisbee throws. The mean and variance of the waiting times for these two coins are respectively  and , hence the total mean and variance are 12 and 100 as before.)
(b) Expanding the generating function in partial fractions makes it possible to sum the probabilities. (Note that /(4ϕ) + ϕ2/4 = 1, so the answer can be stated in terms of powers of φ.) The game will last more than n steps with probability 5(n-1)/24-n(φn+2 - φ-n-2); when n is even this is 5n/24-nFn+2. So the answer is 5504-100F102 ≈ .00006.
8.49 (a) If n > 0, PN(0, n) =  [N = 0] +  PN-1(0, n) +  PN-1(1, n-1); PN(m, 0) is similar; PN(0, 0) = [N = 0]. Hence
gm,n =  zgm-1,n+1 +  zgm,n +  zgm+1,n-1;
 g0,n =  +  zg0,n +  g1,n-1;   etc.
(b) ; ; etc. By induction on m, we have  for all m, n ≥ 0. And since , we must have . (c) The recurrence is satisfied when mn > 0, because



this is a consequence of the identity sin(x - y) + sin(x + y) = 2 sin x cos y. So all that remains is to check the boundary conditions.
8.50 (a) Using the hint, we get



now look at the coefficient of z3+l. (b) . (c) Let . One can show that (z-3+r)(z-3-r) = 4z, and hence that (r/(1 - z) + 2)2 = (13 - 5z + 4r)/(1 - z) = (9 - H(z))/(1 - H(z)).
(d) Evaluating the first derivative at z = 1 shows that Mean(H) = 1. The second derivative diverges at z = 1, so the variance is infinite.
8.51 (a) Let Hn(z) be the pgf for your holdings after n rounds of play, with H0(z) = z. The distribution for n rounds is
Hn+1(z) = Hn(H(z)),
so the result is true by induction (using the amazing identity of the preceding problem). (b) gn = Hn(0) - Hn-1(0) = 4/n(n + 1)(n + 2) = 4(n - 1)-3. The mean is 2, and the variance is infinite. (c) The expected number of tickets you buy on the nth round is Mean(Hn) = 1, by exercise 15. So the total expected number of tickets is infinite. (Thus, you almost surely lose eventually, and you expect to lose after the second game, yet you also expect to buy an infinite number of tickets.) (d) Now the pgf after n games is Hn(z)2, and the method of part (b) yields a mean of 16 -  π2 ≈ 2.8. (The sum ∑k≥11/k2 = π2/6 shows up here.)
8.52 If ω and ω′ are events with Pr(ω) > Pr(ω′), then a sequence of n independent experiments will encounter ω more often than ω′, with high probability, because ω will occur very nearly n Pr(ω) times. Consequently, as n → ∞, the probability approaches 1 that the median or mode of the values of X in a sequence of independent trials will be a median or mode of the random variable X.
8.53 We can disprove the statement, even in the special case that each variable is 0 or 1. Let p0 = Pr(X = Y = Z = 0),  , , where . Then p0 + p1 + · · · + p7 = 1, and the variables are independent in pairs if and only if we have
(p4 + p5 + p6 + p7)(p2 + p3 + p6 + p7) = p6 + p7,
(p4 + p5 + p6 + p7)(p1 + p3 + p5 + p7) = p5 + p7,
(p2 + p3 + p6 + p7)(p1 + p3 + p5 + p7) = p3 + p7.
But Pr(X + Y = Z = 0) ≠ Pr(X + Y = 0) Pr(Z = 0)    p0 ≠ (p0 + p1)(p0 + p2 + p4 + p6). One solution is
p0 = p3 = p5 = p6 = 1/4;      p1 = p2 = p4 = p7 = 0.
This is equivalent to flipping two fair coins and letting X = (the first coin is heads), Y = (the second coin is heads), Z = (the coins differ). Another example, with all probabilities nonzero, is
p0 = 4/64, p1 = p2 = p4 = 5/64,
p3 = p5 = p6 = 10/64, p7 = 15/64.
For this reason we say that n variables X1, . . . , Xn are independent if
Pr(X1 = x1 and · · · and Xn = xn) = Pr(X1 = x1) . . . Pr(Xn = xn);
pairwise independence isn't enough to guarantee this.
8.54 (See exercise 27 for notation.) We have
     E(Σ) = nμ4 + n(n-1)μ;
   E(Σ2Σ) = nμ4 + 2n(n-1)μ3μ1 + n(n-1)μ + n(n-1)(n-2)μ2μ;
     E(Σ) = nμ4 + 4n(n-1)μ3μ1 + 3n(n-1)μ
              + 6n(n-1)(n-2)μ2μ + n(n-1)(n-2)(n-3)μ;
it follows that V(X) = κ4/n + 2κ/(n - 1).
8.55 There are  permutations with X = Y, and  permutations with X ≠ Y. After the stated procedure, each permutation with X = Y occurs with probability , because we return to step S1 with probability . Similarly, each permutation with X ≠ Y occurs with probability . Choosing p =  makes Pr(X = x and Y = y) =  for all x and y. (We could therefore make two flips of a fair coin and go back to S1 if both come up heads.)
8.56 If m is even, the frisbees always stay an odd distance apart and the game lasts forever. If m = 2l + 1, the relevant generating functions are


Gm
=
 zA1;


A1
=
 zA1 +  zA2,


Ak
=
 zAk-1 +  zAk +  zAk+1,
for 1 < k < l,


Al
=
 zAl-1 + 34 zAl + 1.


(The coefficient [zn] Ak is the probability that the distance between frisbees is 2k after n throws.) Taking a clue from the similar equations in exercise 49, we set z = 1/cos2 θ and A1 = X sin 2θ, where X is to be determined. It follows by induction (not using the equation for Al) that Ak = X sin 2kθ. Therefore we want to choose X such that



It turns out that X = 2 cos2 θ/sin θ cos(2l + 1)θ, hence



The denominator vanishes when θ is an odd multiple of π/(2m); thus 1-qkz is a root of the denominator for 1 ≤ k ≤ l, and the stated product representation must hold. To find the mean and variance we can write
Trigonometry wins again. Is there a connection with pitching pennies along the angles of the m-gon?



because tan2 θ = z - 1 and tan θ = θ +  θ3 + · · ·. So we have Mean(Gm) =  (m2 -1) and Var(Gm) =  m2(m2 -1). (Note that this implies the identities



The third cumulant of this distribution is m2 (m2-1) (4m2 - 1); but the pattern of nice cumulant factorizations stops there. There's a much simpler way to derive the mean: We have Gm + A1 + · · · + Al = z(A1 + · · · + Al) + 1, hence when z = 1 we have . Since Gm = 1 when z = 1, an easy induction shows that Ak = 4k.)
8.57 We have A:A ≥ 2l-1 and B:B < 2l-1 + 2l-3 and B:A ≥ 2l-2, hence B:B - B:A ≥ A:A - A:B is possible only if A:B > 2l-3. This means that , τ1 = τ4, τ2 = τ5, . . . , τl-3 = τl. But then A:A ≈ 2l-1 + 2l-4 + · · ·, A:B ≈ 2l-3 +2l-6 +· · ·, B:A ≈ 2l-2 +2l-5 +· · ·, and B:B ≈ 2l-1 +2l-4 +· · ·; hence B:B - B:A is less than A:A - A:B after all. (Sharper results have been obtained by Guibas and Odlyzko [168], who show that Bill's chances are always maximized with one of the two patterns Hτ1 . . . τl-1 or Tτ1 . . . τl-1. Bill's winning strategy is, in fact, unique; see the following exercise.)
8.58 (Solution by J. Csirik.) If A is Hl or Tl, one of the two sequences matches A and cannot be used. Otherwise let Â = τ1 . . . τl-1, H = HÂ, and T = T Â. It is not difficult to verify that H:A = T:A = Â:Â, H:H + T:T = 2l-1 + 2( Â:Â) + 1, and A:H + A:T = 1 + 2(A:A) - 2l. Therefore the equation



implies that both fractions equal



Then we can rearrange the original fractions to show that



where pq > 0 and p + q = gcd(2l-1 + 1, 2l - 1) = gcd(3, 2l - 1); so we may assume that l is even and that p = 1, q = 2. It follows that A:A - A:H = (2l - 1)/3 and A:A-A:T = (2l+1-2)/3, hence A:H-A:T = (2l - 1)/3 ≥ 2l-2. We have A:H ≥ 2l-2 if and only if A = (TH)l/2. But then H:H - H:A = A:A - A:H, so 2l-1 + 1 = 2l - 1 and l = 2.
(Csirik [69] goes on to show that, when l ≥ 4, Alice can do no better than to play HTl-3H2. But even with this strategy, Bill wins with probability nearly .)
8.59 According to (8.82), we want B:B - B:A > A:A - A:B. One solution is A = TTHH, B = HHH.
8.60 (a) Two cases arise depending on whether hk ≠ hn or hk = hn:



(b) We can either argue algebraically, taking partial derivatives of G(w, z) with respect to w and z and setting w = z = 1; or we can argue combinatorially: Whatever the values of h1, . . . , hn-1, the expected value of P(h1, . . . , hn-1, hn; n) is the same (averaged over hn), because the hash sequence (h1, . . . , hn-1) determines a sequence of list sizes (n1, n2, . . . , nm) such that the stated expected value is ((n1+1)+(n2+1)+· · ·+(nm+1) /m (n - 1 + m)/m. Therefore the random variable EP(h1, . . . , hn; n) is independent of (h1, . . . , hn-1), hence independent of P(h1, . . . , hn; k).
8.61 If 1 ≤ k < l ≤ n, the previous exercise shows that the coefficient of sksl in the variance of the average is zero. Therefore we need only consider the coefficient of , which is



the variance of ((m - 1 + z)/m)k-1z; and this is (k - 1)(m - 1)/m2 as in exercise 30.
8.62 The pgf Dn(z) satisfies the recurrence



We can now derive the recurrence



which has the solution  (n+2) (26n + 15) for all n ≥ 11 (regardless of initial conditions). Hence the variance comes to  (n + 2) for n ≥ 11.
8.63 (Another question asks if a given sequence of purported cumulants comes from any distribution whatever; for example, κ2 must be nonnegative, and  must be at least , etc. A necessary and sufficient condition for this other problem was found by Hamburger [6], [175].)
9.1 True if the functions are all positive. But otherwise we might have, say, f1(n) = n3 + n2, f2(n) = -n3, g1(n) = n4 + n, g2(n) = -n4.
9.2 (a) We have nlnn ≺ cn ≺ (ln n)n, since (ln n)2 ≺ n ln c ≺ n ln ln n.
(b) nlnlnlnn ≺ (ln n)! ≺ nlnlnn. (c) Take logarithms to show that (n!)! wins. (d) ; HFn ∼ n ln φ wins because φ2 = φ + 1 < e.
9.3 Replacing kn by O(n) requires a different C for each k; but each O stands for a single C. In fact, the context of this O requires it to stand for a set of functions of two variables k and n. It would be correct to write .
9.4 For example, limn→∞ O(1/n) = 0. On the left, O(1/n) is the set of all functions f(n) such that there are constants C and n0 with |f(n)| ≤ C/n for all n ≥ n0. The limit of all functions in that set is 0, so the left-hand side is the singleton set {0}. On the right, there are no variables; 0 represents {0}, the (singleton) set of all "functions of no variables, whose value is zero." (Can you see the inherent logic here? If not, come back to it next year; you probably can still manipulate O-notation even if you can't shape your intuitions into rigorous formalisms.)
9.5 Let f(n) = n2 and g(n) = 1; then n is in the left set but not in the right, so the statement is false.
9.6 nln + γn + O(lnn).
9.7 (1 - e-1/n)-1 = nB0 - B1 + B2n-1/2! + · · · = n +  + O(n-1).
9.8 For example, let f(n) = n/2!2 + n, g(n) = (n/2 - 1) ! n/2! + n. These functions, incidentally, satisfy f(n) = O (ng(n)) and g(n) = O) nf(n)); more extreme examples are clearly possible.
9.9 (For completeness, we assume that there is a side condition n → ∞, so that two constants are implied by each O.) Every function on the left has the form a(n) + b(n), where there exist constants m0, B, n0, C such that |a(n) |≤ B |f(n) |for n ≥ m0 and |b(n) |≤ C |g(n) |for n ≥ n0. Therefore the left-hand function is at most max(B, C) (|f(n)| +| g(n)|), for n ≥ max(m0, n0), so it is a member of the right side.
9.10 If g(x) belongs to the left, so that g(x) = cos y for some y, where |y| ≤ C|x| for some C, then 0 ≤ 1 - g(x) = 2 sin2(y/2) ≤  y2 ≤  C2x2; hence the set on the left is contained in the set on the right, and the formula is true.
9.11 The proposition is true. For if, say, |x| ≤ |y|, we have (x + y)2 ≤ 4y2. Thus (x + y)2 = O(x2) + O(y2). Thus O(x + y)2 = O ((x + y)2) = O(O(x2) + O(y2)) = O(O(x2)) + O(O(y2)) = O(x2) + O(y2).
9.12 1 + 2/n + O(n-2) = (1 + 2/n) (1 + O(n-2)/(1 + 2/n)) by (9.26), and 1/(1 + 2/n) = O(1); now use (9.26).
9.13 nn(1 + 2n-1 + O(n-2))n = nn exp (n(2n-1 + O(n-2))) = e2nn + O(nn-1).
9.14 It is nn+β exp ((n + β) (α/n -  α2/n2 + O(n-3))).
(It's interesting to compare this formula with the corresponding result for the middle binomial coefficient, exercise 9.60.)
9.15 , so the answer is



9.16 If l is any integer in the range a ≤ l < b we have



Since l + x ≥ l + 1 - x when , this integral is positive when f(x) is nondecreasing.
9.17 
9.18 The text's derivation for the case α = 1 generalizes to give



the answer is 22nα(πn)(1-α)/2α-1/2(1 + O(n-1/2+3)).
9.19 H10 = 2.928968254 ≈ 2.928968256; 10! = 3628800 ≈ 3628712.4; B10 = 0.075757576 ≈ 0.075757494; π(10) = 4 ≈ 10.0017845; e0.1 = 1.10517092 ≈ 1.10517083; ln 1.1 = 0.0953102 ≈ 0.0953083; 1.1111111 . . . ≈ 1.1111; 1.10.1 = 1.00957658 ≈ 1.00957643. (The approximation to π(n) gives more significant figures when n is larger; for example, π(109) = 50847534 ≈ 50840742.)
9.20 (a) Yes; the left side is o(n) while the right side is equivalent to O(n). (b) Yes; the left side is e · eO(1/n). (c) No; the left side is about  times the bound on the right.
9.21 We have Pn = p = n (ln p - 1 - 1/ln p + O(1/log n)2), where



It follows that



(A slightly better approximation replaces this O(1/log n)2 by the quantity -5.5/(ln n)2 + O(log log n/log n)3; then we estimate P1000000 ≈ 15480992.8.)
What does a drowning analytic number theorist say?
log log log log . . .
9.22 Replace O(n-2k) by -  n-2k + O(n-4k) in the expansion of Hnk ; this replaces O(∑3(n2)) by  in (9.53). We have



hence the term O(n-2) in (9.54) can be replaced by -  n-2 + O(n-3).
9.23 nhn = ∑0≤k<n hk/(n-k)+2cHn/(n+1)(n+2). Choose c = eπ2/6 = ∑k≥0 gk so that ∑k≥0 hk = 0 and hn = O(log n)/n3. The expansion of ∑0≤k<n hk/(n - k) as in (9.60) now yields nhn = 2cHn/(n + 1)(n + 2) + O(n-2), hence



9.24 (a) If ∑k≥0|f(k)| < ∞ and if f(n - k) = O(f(n)) when 0 ≤ k ≤ n/2, we have



which is 2O(f(n) ∑k≥0|f(k)|), so this case is proved. (b) But in this case if an = bn = α-n, the convolution (n + 1)α-n is not O(α-n).
9.25 . We may restrict the range of summation to 0 ≤ k ≤ (log n)2, say. In this range  and , so the summand is



Hence the sum over k is 2-4/n + O(1/n2). Stirling's approximation can now be applied to , proving (9.2).
9.26 The minimum occurs at a term B2m/(2m)(2m-1)n2m-1 where 2m ≈ 2πn+ , and this term is approximately equal to . The absolute error in ln n! is therefore too large to determine n! exactly by rounding to an integer, when n is greater than about e2π+1.
9.27 We may assume that α ≠ -1. Let f(x) = xα; the answer is



(The constant Cα turns out to be ζ(-α), which is in fact defined by this formula when α > -1.)
In particular, ζ(0) = -1/2, and ζ(-n) = -Bn+1/(n+1) for integer n > 0.
9.28 In general, suppose f(x) = xα ln x in Euler's summation formula, when α ≠ -1. Proceeding as in the previous exercise, we find



the constant  can be shown [74, §3.7] to be -ζ′(-α). (The log n factor in the O term can be removed when α is a positive integer ≤ 2m; in that case we also replace the kth term of the right sum by B2kα! (2k - 2 - α)! × (-1)αnα-2k+1/(2k)! when α < 2k - 1.) To solve the stated problem, we let α = 1 and m = 1, taking the exponential of both sides to get
Qn = A · nn2/2+n/2+1/12e-n2/4(1 + O(n-2)),
where A = e1/12-ζ ′(-1) ≈ 1.2824271291 is "Glaisher's constant."
9.29 Let f(x) = x-1 ln x. A slight modification of the calculation in the previous exercise gives



where γ1 ≈ -0.07281584548367672486 is a "Stieltjes constant" (see the answer to 9.57). Taking exponentials gives



9.30 Let g(x) = xle-x2 and . Then n-l/2 ∑k≥0 kle-k2/n is



Since g(x) = xl - x2+l/1! + x4+l/2! - x6+l/3! + · · · , the derivatives g(m)(x) obey a simple pattern, and the answer is



9.31 The somewhat surprising identity 1/(cm-k + cm) + 1/(cm+k + cm) = 1/cm makes the terms for 0 ≤ k ≤ 2m sum to . The remaining terms are



and this series can be truncated at any desired point, with an error not exceeding the first omitted term.
9.32  by Euler's summation formula, since we know the constant; and Hn is given by (9.89). So the answer is
The world's top three constants, (e, π, γ), all appear in this answer.
neγ+π2/6 (1 -  n-1 + O(n-2)).
9.33 We have ; dividing by k! and summing over k ≥ 0 yields e - en-1 +  en-2 + O(n-3).
9.34 .
9.35 Since 1/k(ln k + O(1)) = 1/k ln k + O(1/k(log k)2), the given sum is . The remaining sum is ln ln n + O(1) by Euler's summation formula.
9.36 This works out beautifully with Euler's summation formula:



Hence .
9.37 This is






The remaining sum is like (9.55) but without the factor μ(q). The same method works here as it did there, but we get ζ(2) in place of 1/ζ(2), so the answer comes to .
9.38 Replace k by n - k and let . Then ln ak(n) = n ln n - ln k! - k + O(kn-1), and we can use tail-exchange with bk(n) = nne-k/k!, ck(n) = kbk(n)/n, Dn = {k | k ≤ ln n }, to get .
9.39 Tail-exchange with , ck(n) = n-3(ln n)k+3/k!, Dn = { k | 0 ≤ k ≤ 10 ln n }. When k ≈ 10 ln n we have , so the kth term is O(n-10 ln(10/e) log n). The answer is .
9.40 Combining terms two by two, we find that  plus terms whose sum over all k ≥ 1 is O(1). Suppose n is even. Euler's summation formula implies that



hence the sum is . In general the answer is .
9.41 Let . We have



The latter sum is ∑k>n O(αk) = O(αn). Hence the answer is
φn (n+1)/25-n/2C + O(φn(n-3)/25-n/2),
where C = (1 - α)(1 - α2)(1 - α3) . . . ≈ 1.226742.
9.42 The hint follows since . Let m = αn = αn - . Then



So , and it remains to estimate . By Stirling's approximation we have  = = - 12 ln n-(αn-) ln(α-/n)-- (1-α)n+- × ln(1 - α + /n) + O(1) = - 12 ln n - αn ln α - (1 - α)n ln(1 - α) + O(1).
9.43 The denominator has factors of the form z - ω, where ω is a complex root of unity. Only the factor z - 1 occurs with multiplicity 5. Therefore by (7.31), only one of the roots has a coefficient Ω(n4), and the coefficient is c = 5/(5!·1·5·10·25·50) = 1/1500000.
9.44 Stirling's approximation says that ln(x-αx!/(x - α)!) has an asymptotic series



in which each coefficient of x-k is a polynomial in α. Hence x-αx!/(x - α)! = c0(α) + c1(α)x-1 + · · · + cn(α)x-n + O(x-n-1) as x → ∞, where cn(α) is a polynomial in α. We know that  whenever α is an integer, and  is a polynomial in α of degree 2n; hence  for all real α. In other words, the asymptotic formulas
(See [220] for further discussion.)



generalize equations (6.13) and (6.11), which hold in the all-integer case.
9.45 Let the partial quotients of α be a1, a2, . . . , and let αm be the continued fraction 1/(am + αm+1) for m ≥ 1. Then D(α, n) = D(α1, n) < D(α2, α1n) + a1 + 3 < D (α3, α2α1n) + a1 + a2 + 6 < · · · < D(αm+1, αm. . . α1n . . .) +a1 +· · ·+am +3m < α1 . . . αm n+a1 +· · ·+am +3m, for all m. Divide by n and let n → ∞; the limit points are bounded above by α1 . . . αm for all m. Finally we have



9.46 For convenience we write just m instead of m(n). By Stirling's approximation, the maximum value of kn/k! occurs when k ≈ m ≈ n/ln n, so we replace k by m + k and find that



Actually we want to replace k by m + k; this adds a further O(km-1 log n). The tail-exchange method with |k| ≤ m1/2+ now allows us to sum on k, giving a fairly sharp asymptotic estimate in terms of the quantity Θ in (9.93):
A truly Bell-shaped summand.



The requested formula follows, with relative error O(log log n/log n).
9.47 Let logm n = l + θ, where 0 ≤ θ < 1. The floor sum is l(n + 1) + 1 - (ml + 1 - 1)/(m - 1); the ceiling sum is (l + 1)n - (ml+1 - 1)/(m - 1); the exact sum is (l + θ)n - n/ln m + O(log n). Ignoring terms that are o(n), the difference between ceiling and exact is (1-f(θ))n, and the difference between exact and floor is f(θ)n, where



This function has maximum value f(0) = f(1) = m/(m - 1) - 1/ln m, and its minimum value is ln ln m/ln m + 1 - (ln(m - 1))/ln m. The ceiling value is closer when n is nearly a power of m, but the floor value is closer when θ lies somewhere between 0 and 1.
9.48 Let dk = ak + bk, where ak counts digits to the left of the decimal point. Then ak = 1 + log Hk = log log k + O(1), where 'log' denotes log10. To estimate bk, let us look at the number of decimal places necessary to distinguish y from nearby numbers y -  and y + ′: Let δ = 10-b be the length of the interval of numbers that round to ŷ. We have ; also  and . Therefore  + ′ > δ. And if δ < min(, ′), the rounding does distinguish ŷ from both y -  and y + ′. Hence 10-bk < 1/(k - 1) + 1/k and 101-bk ≥ 1/k; we have bk = log k + O(1). Finally, therefore, , which is n log n + n log log n + O(n) by Euler's summation formula.
9.49 We have , where f(x) is increasing for all x > 0; hence if n ≥ eα-γ we have Hn ≥ f(eα-γ) > α. Also , where g(x) is increasing for all x > 0; hence if n ≤ eα-γ we have Hn-1 ≤ g(eα-γ) < α. Therefore Hn-1 ≤ α ≤ Hn implies that eα-γ + 1 > n > eα+γ - 1. (Sharper results have been obtained by Boas and Wrench [33].)
9.50 (a) The expected return is , and we want the asymptotic value to O(N-1):



The coefficient (6 ln 10)/π2 ≈ 1.3998 says that we expect about 40% profit.
(b) The probability of profit is , and since  this is



actually decreasing with n. (The expected value in (a) is high because it includes payoffs so huge that the entire world's economy would be affected if they ever had to be made.)
9.51 Strictly speaking, this is false, since the function represented by O(x-2) might not be integrable. (It might be '[x  S]/x2', where S is not a measurable set.) But if we stipulate that f(x) is an integrable function such that f(x) = O(x-2) as x → ∞, then .
(As opposed to an execrable function.)
9.52 In fact, the stack of n's can be replaced by any function f(n) that approaches infinity, however fast. Define the sequence m0, m1, m2, . . .  by setting m0 = 0 and letting mk be the least integer > mk-1 such that



Now let A(z) = ∑k≥1(z/k)mk. This power series converges for all z, because the terms for k > |z| are bounded by a geometric series. Also A(n + 1) ≥ ((n + 1)/n)mn ≥ f(n + 1)2, hence limn→∞ f(n)/A(n) = 0.
9.53 By induction, the O term is . Since f(m+1) has the opposite sign to f(m), the absolute value of this integral is bounded by ; so the error is bounded by the absolute value of the first discarded term.
9.54 Let g(x) = f(x)/xα. Then g′(x) ∼ -αg(x)/x as x → ∞. By the mean value theorem,  for some y between  and . Now g(y) = g(x) 1 + O(1/x), so . Therefore



Sounds like a nasty theorem.
9.55 The estimate of (n + k + ) ln (1 + k/n) + (n - k + ) ln (1 - k/n) is extended to k2/n + k4/6n3 + O(n-3/2+5), so we apparently want to have an extra factor e-k4/6n3 in bk(n), and ck(n) = 22nn-2+5e-k2/n. But it turns out to be better to leave bk(n) untouched and to let
ck(n) = 22nn-2+5e-k2/n + 22nn-5+5k4e-k2/n,
thereby replacing e-k4/6n3 by 1+O(k4/n3). The sum Σkk4e-k2/n is O(n5/2), as shown in exercise 30.
9.56 If k ≤ n1/2+ we have  by Stirling's approximation, hence
nk/nk = e-k2/2n (1 + k/2n -  k3/(2n)2 + O(n-1+4)).
Summing with the identity in exercise 30, and remembering to omit the term for k = 0, gives .
9.57 Using the hint, the given sum becomes . The zeta function can be defined by the series



where γ0 = γ and γm is the Stieltjes constant [341, 201]



Hence the given sum is
ln n + γ - 2γ1(ln n)-1 + 3γ2(ln n)-2 - · · ·.
9.58 Let 0 ≤ θ ≤ 1 and f(z) = e2πizθ/(e2πiz - 1). We have



Therefore |f(z)| is bounded on the contour, and the integral is O(M1-m). The residue of 2πif(z)/zm at z = k ≠ 0 is e2πikθ/km; the residue at z = 0 is the coefficient of z-1 in



namely (2πi)mBm(θ)/m!. Therefore the sum of residues inside the contour is



This equals the contour integral O(M1-m), so it approaches zero as M → ∞.
9.59 If F(x) is sufficiently well behaved, we have the general identity



where . (This is "Poisson's summation formula," which can be found in standard texts such as Henrici [182, Theorem 10.6e].)
9.60 The stated formula is equivalent to



by exercise 5.22. Hence the result follows from exercises 6.64 and 9.44.
9.61 The idea is to make α "almost" rational. Let ak = 222k be the kth partial quotient of α, and let , where qm = K(a1, . . . , am) and m is even. Then 0 < {qmα} < 1/K(a1, . . . , am+1) < 1/(2n), and if we take v = am+1/(4n) we get a discrepancy . If this were less than n1- we would have ; but in fact .
9.62 See Canfield [48]; see also David and Barton [71, Chapter 16] for asymptotics of Stirling numbers of both kinds.
9.63 Let c = φ2-φ. The estimate cnφ-1+o(nφ-1) was proved by Fine [150]. Ilan Vardi observes that the sharper estimate stated can be deduced from the fact that the error term e(n) = f(n) - cnφ-1 satisfies the approximate recurrence cφn2-φe(n) ≈ - ∑k e(k)[1 ≤ k < cnφ-1 ]. The function



satisfies this recurrence asymptotically, if u(x + 1) = -u(x). (Vardi conjectures that
Additional progress on this problem has been made by Jean-Luc Rémy, Journal of Number Theory, vol. 66 (1997), 1-28.



for some such function u.) Calculations for small n show that f(n) equals the nearest integer to cnφ-1 for 1 ≤ n ≤ 400 except in one case: f(273) = 39 > c · 273φ -1 ≈ 38.4997. But the small errors are eventually magnified, because of results like those in exercise 2.36. For example, e(201636503) ≈ 35.73; e(919986484788) ≈ -1959.07.
9.64 (From this identity for B2(x) we can easily derive the identity of exercise 58 by induction on m.) If 0 < x < 1, the integral  can be expressed as a sum of N integrals that are each O(N-2), so it is O(N-1); the constant implied by this O may depend on x. Integrating the identity  and letting N → ∞ now gives , a relation that Euler knew ([107] and [110, part 2, §92]). Integrating again yields the desired formula. (This solution was suggested by E. M. E. Wermuth [367]; Euler's original derivation did not meet modern standards of rigor.)
9.65 Since a0+a1n-1 +a2n-2 +· · · = 1+(n-1)-1(a0+a1(n-1)-1 +a2(n- 1)-2 + · · · ), we obtain the recurrence , which matches the recurrence for the Bell numbers. Hence am = ϖm.
A slightly longer but more informative proof can be based on the fact that 1/(n-1)...(n-m)=, by (7.47).
"The paradox is now fully established that the utmost abstractions are the true weapons with which to control our thought of concrete fact."
—A. N. Whitehead [372]
9.66 The expected number of distinct elements in the sequence 1, f(1), f(f(1)), . . . , when f is a random mapping of {1, 2, . . . , n} into itself, is the function Q(n) of exercise 56, whose value is ; this might account somehow for the factor .
9.67 It is known that ; the constant e-π/6 has been verified empirically to eight significant digits.
9.68 This would fail if, for example,  for some integer m and some ; but no counterexamples are known.









B. Bibliography
Here are the works cited in this book. Numbers in the margin specify the page numbers where citations occur.
"This paper fills a much-needed gap in the literature."
— Math. Reviews
References to published problems are generally made to the places where solutions can be found, instead of to the original problem statements.
Wherever possible, names and titles are spelled here as they appeared in the original publication.
1. N. H. Abel, letter to B. Holmboe (1823), in his Œuvres Complètes, first edition, 1839, volume 2, 264-265. Reprinted in the second edition, 1881, volume 2, 254-255.
634.
2. Milton Abramowitz and Irene A. Stegun, editors, Handbook of Mathematical Functions. United States Government Printing Office, 1964. Reprinted by Dover, 1965.
42.
3. William W. Adams and J. L. Davison, "A remarkable class of continued fractions," Proceedings of the American Mathematical Society 65 (1977), 194-198. [See also P. E. Böhmer, "Über die Transzendenz gewisser dyadischer Brüche," Mathematische Annalen 96 (1927), 367-377, 735.]
635.
4. A. V. Aho and N. J. A. Sloane, "Some doubly exponential sequences," Fibonacci Quarterly 11 (1973), 429-437.
633.
5. W. Ahrens, Mathematische Unterhaltungen und Spiele. Teubner, Leipzig, 1901. Second edition, in two volumes, 1910 and 1918.
8.
6. Naum Il'ich Akhiezer,  problema momentov i nekotorye voprosy analiza, . Moscow, 1961. English translation, The Classical Moment Problem and Some Related Questions in Analysis, Hafner, 1965.
591.
7. R. E. Allardice and A. Y. Fraser, "La Tour d'Hanoï," Proceedings of the Edinburgh Mathematical Society 2 (1884), 50-53.
2.
8. Désiré André, "Sur les permutations alternées," Journal de Mathématiques pures et appliquées, series 3, 7 (1881), 167-184.
635.
9. George E. Andrews, "Applications of basic hypergeometric functions," SIAM Review 16 (1974), 441-484.
215, 634.
10. George E. Andrews, "On sorting two ordered sets," Discrete Mathematics 11 (1975), 97-106.
530.
11. George E. Andrews, The Theory of Partitions. Addison-Wesley, 1976.
330.
12. George E. Andrews, "Euler's 'exemplum memorabile inductionis fallacis' and q-trinomial coefficients," Journal of the American Mathematical Society 3 (1990), 653-669.
575.
13. George E. Andrews and K. Uchimura, "Identities in combinatorics IV: Differentiation and harmonic numbers," Utilitas Mathematica 28 (1985), 265-269.
635.
14. Roger Apéry, "Interpolation de fractions continues et irrationalité de certaines constantes," in Mathématiques, Ministère des universités (France), Comité des travaux historiques et scientifiques, Section des sciences, Bulletin de la Section des Sciences 3 (1981), 37-53.
238, 634.
15. M. D. Atkinson, "The cyclic towers of Hanoi," Information Processing Letters 13 (1981), 118-119.
633.
16. M. D. Atkinson, "How to compute the series expansions of sec x and tan x," American Mathematical Monthly 93 (1986), 387-389. [This triangle was first found by L. Seidel, "Ueber eine einfache Entstehungsweise der Bernoulli'schen Zahlen und einiger verwandten Reihen," Sitzungsberichte der mathematisch-physikalischen Classe der königlich bayerischen Akademie der Wissenschaften zu München 7 (1877), 157-187.]
635.
17. Paul Bachmann, Die analytische Zahlentheorie. Teubner, Leipzig, 1894.
443.
18. W. N. Bailey, Generalized Hypergeometric Series. Cambridge University Press, 1935; second edition, 1964.
223, 634.
19. W. N. Bailey, "The generating function for Jacobi polynomials," Journal of the London Mathematical Society 13 (1938), 243-246.
548.
19′ R. Balasubramanian and K. Soundararajan, "On a conjecture of R. L. Graham," Acta Arithmetica 75 (1996), 1-38.
525.
20. W. W. Rouse Ball and H. S. M. Coxeter, Mathematical Recreations and Essays, twelfth edition. University of Toronto Press, 1974. (A revision of Ball's Mathematical Recreations and Problems, first published by Macmillan, 1892.)
633.
21. P. Barlow, "Demonstration of a curious numerical proposition," Journal of Natural Philosophy, Chemistry, and the Arts 27 (1810), 193-205.
634.
22. Samuel Beatty, "Problem 3177," American Mathematical Monthly 34 (1927), 159-160.
633.
23. E. T. Bell, "Euler algebra," Transactions of the American Mathematical Society 25 (1923), 135-154.
332.
24. E. T. Bell, "Exponential numbers," American Mathematical Monthly 41 (1934), 411-419.
635.
25. Edward A. Bender, "Asymptotic methods in enumeration," SIAM Review 16 (1974), 485-515.
636.
26. Jacobi Bernoulli, Ars Conjectandi, opus posthumum. Basel, 1713. Reprinted in Die Werke von Jakob Bernoulli, volume 3, 107-286.
283.
27. J. Bertrand, "Mémoire sur le nombre de valeurs que peut prendre une fonction quand on y permute les lettres qu'elle renferme," Journal de l'École Royale Polytechnique 18, cahier 30 (1845), 123-140.
633.
28. William H. Beyer, editor, CRC Standard Mathematical Tables and Formulae, 29th edition. CRC Press, Boca Raton, Florida, 1991.
42.
29. J. Bienaymé, "Considérations à l'appui de la découverte de Laplace sur la loi de probabilité dans la méthode des moindres carrés," Comptes Rendus hebdomadaires des séances de l'Académie des Sciences (Paris) 37 (1853), 309-324.
390.
30. J. Binet, "Mémoire sur un système de Formules analytiques, et leur application à des considérations géométriques," Journal de l'École Polytechnique 9, cahier 16 (1812), 280-354.
633.
31. J. Binet, "Mémoire sur l'intégration des équations linéaires aux différences finies, d'un ordre quelconque, à coefficients variables," Comptes Rendus hebdomadaires des séances de l'Académie des Sciences (Paris) 17 (1843), 559-567.
299.
32. Gunnar Blom, "Problem E 3043: Random walk until no shoes," American Mathematical Monthly 94 (1987), 78-79.
636.
33. R. P. Boas, Jr. and J. W. Wrench, Jr., "Partial sums of the harmonic series," American Mathematical Monthly 78 (1971), 864-870.
600, 636.
34. P. Bohl, "Über ein in der Theorie der säkularen Störungen vorkommendes Problem," Journal für die reine und angewandte Mathematik 135 (1909), 189-283.
87.
35. Émile Borel, Leçons sur les séries à termes positifs. Paris, 1902.
636.
36. Jonathan M. Borwein and Peter B. Borwein, Pi and the AGM. Wiley, 1987.
635.
37. Richard P. Brent, "The first occurrence of large gaps between successive primes," Mathematics of Computation 27 (1973), 959-963.
525.
38. Richard P. Brent, "Computation of the regular continued fraction for Euler's constant," Mathematics of Computation 31 (1977), 771-777.
306, 564.
39. John Brillhart, "Some miscellaneous factorizations," Mathematics of Computation 17 (1963), 447-450.
633.
40. Achille Brocot, "Calcul des rouages par approximation, nouvelle méthode," Revue Chronométrique 3 (1861), 186-194. (He also published a 97-page monograph with the same title in 1862.)
116.
41. Maxey Brooke and C. R. Wall, "Problem B-14: A little surprise," Fibonacci Quarterly 1, 3 (1963), 80.
635.
42. Brother U. Alfred [Brousseau], "A mathematician's progress," Mathematics Teacher 59 (1966), 722-727.
633.
43. Morton Brown, "Problem 6439: A periodic sequence," American Mathematical Monthly 92 (1985), 218.
501.
44. T. Brown, "Infinite multi-variable subpolynormal Woffles which do not satisfy the lower regular Q-property (Piffles)," in A Collection of 250 Papers on Woffle Theory Dedicated to R. S. Green on His 23rd Birthday. Cited in A. K. Austin, "Modern research in mathematics," The Mathematical Gazette 51 (1967), 149-150.
(Such papers aren't cited in this book.)
45. Thomas C. Brown, "Problem E 2619: Squares in a recursive sequence," American Mathematical Monthly 85 (1978), 52-53.
633.
46. William G. Brown, "Historical note on a recurrent combinatorial problem," American Mathematical Monthly 72 (1965), 973-977.
358.
47. S. A. Burr, "On moduli for which the Fibonacci sequence contains a complete system of residues," Fibonacci Quarterly 9 (1971), 497-504.
635.
48. E. Rodney Canfield, "On the location of the maximum Stirling number(s) of the second kind," Studies in Applied Mathematics 59 (1978), 83-93.
602, 636.
49. L. Carlitz, "The generating function for max(n1, n2, · · · , nk)," Portugaliae Mathematica 21 (1962), 201-207.
635.
50. Lewis Carroll [pseudonym of C. L. Dodgson], Through the Looking Glass and What Alice Found There. Macmillan, 1871.
31.
51. Jean-Dominique Cassini, "Une nouvelle progression de nombres," Histoire de l'Académie Royale des Sciences, Paris, volume 1, 201. (Cassini's work is summarized here as one of the mathematical results presented to the academy in 1680. This volume was published in 1733.)
292.
52. E. Catalan, "Note sur une Équation aux différences finies," Journal de Mathématiques pures et appliquées 3 (1838), 508-516.
203.
53. Augustin-Louis Cauchy, Cours d'analyse de l'École Royale Polytechnique. Imprimerie Royale, Paris, 1821. Reprinted in his Œuvres complètes, series 2, volume 3.
633.
54. Arnold Buffum Chace, The Rhind Mathematical Papyrus, volume 1. Mathematical Association of America, 1927. (Includes an excellent bibliography of Egyptian mathematics by R. C. Archibald.)
633.
55. M. Chaimovich, G. Freiman, and J. Schönheim, "On exceptions to Szegedy's theorem," Acta Arithmetica 49 (1987), 107-112.
525.
56. P. L. Tchebichef [Chebyshev], "Mémoire sur les nombres premiers," Journal de Mathématiques pures et appliquées 17 (1852), 366-390. Reprinted in his Œuvres, volume 1, 51-70. Russian translation, "O prostykh chislakh," in his Polnoe sobranie sochinenĭı, volume 1, 191-207.
633.
57. P. L. Chebyshev˝, "O srednikh˝ velichinakh˝," Matematicheskiĭ Sbornik˝ 2,1 (1867), 1-9. Reprinted in his Polnoe sobranie sochineniĭ, volume 2, 431-437. French translation, "Des valeurs moyennes," Journal de Mathématiques pures et appliquées, series 2, 12 (1867), 177-184; reprinted in his Œuvres, volume 1, 685-694.
390.
58. P. L. Chebyshev˝, "O priblizhennykh˝  odnikh˝ integralov˝ cherez˝ drugīe,  v˝  zhe ,"  i protokoly  matematicheskago obshchestva pri Imperatorskom˝ Khar'kovskom˝  4,2 (1882), 93-98. Reprinted in his Polnoe sobranie sochinenĭı, volume 3, 128-131. French translation, "Sur les expressions approximatives des intégrales définies par les autres prises entre les mêmes limites," in his Œuvres, volume 2, 716-719.
38.
59. F. R. K. Chung and R. L. Graham, "On the cover polynomial of a digraph," Journal of Combinatorial Theory, series B, 65 (1995), 273-290.
557, 635.
60. Th. Clausen, "Ueber die Fälle, wenn die Reihe von der Form
634.



ein Quadrat von der Form



Journal für die reine und angewandte Mathematik 3 (1828), 89-91.
61. Th. Clausen, "Beitrag zur Theorie der Reihen," Journal für die reine und angewandte Mathematik 3 (1828), 92-95.
634.
62. Th. Clausen, "Theorem," Astronomische Nachrichten 17 (1840), columns 351-352.
635.
63. Stuart Dodgson Collingwood, The Lewis Carroll Picture Book. T. Fisher Unwin, 1899. Reprinted by Dover, 1961, with the new title Diversions and Digressions of Lewis Carroll.
293.
64. Louis Comtet, Advanced Combinatorics. Dordrecht, Reidel, 1974.
636.
65. J. H. Conway and R. L. Graham, "Problem E 2567: A periodic recurrence," American Mathematical Monthly 84 (1977), 570-571.
501.
66. Harald Cramér, "On the order of magnitude of the difference between consecutive prime numbers," Acta Arithmetica 2 (1937), 23-46.
525, 634.
67. A. L. Crelle, "Démonstration élémentaire du théorème de Wilson généralisé," Journal für die reine und angewandte Mathematik 20 (1840), 29-56.
633.
68. D. W. Crowe, "The n-dimensional cube and the Tower of Hanoi," American Mathematical Monthly 63 (1956), 29-30.
633.
69. János A. Csirik, "Optimal strategy for the first player in the Penney ante game," Combinatorics, Probability and Computing 1 (1992), 311-321.
590.
70. D. R. Curtiss, "On Kellogg's Diophantine problem," American Mathematical Monthly 29 (1922), 380-387.
634.
71. F. N. David and D. E. Barton, Combinatorial Chance. Hafner, 1962.
602.
72. Philip J. Davis, "Leonhard Euler's integral: A historical profile of the Gamma function," American Mathematical Monthly 66 (1959), 849-869.
210.
73. J. L. Davison, "A series and its associated continued fraction," Proceedings of the American Mathematical Society 63 (1977), 29-32.
307, 635.
74. N. G. de Bruijn, Asymptotic Methods in Analysis. North-Holland, 1958; third edition, 1970. Reprinted by Dover, 1981.
444, 447, 595, 636.
75. N. G. de Bruijn, "Problem 9," Nieuw Archief voor Wiskunde, series 3, 12 (1964), 68.
635.
76. Abraham de Moivre, Miscellanea analytica de seriebus et quadraturis. London, 1730.
297, 481.
77. R. Dedekind, "Abriß einer Theorie der höheren Congruenzen in Bezug auf einen reellen Primzahl-Modulus," Journal für die reine und angewandte Mathematik 54 (1857), 1-26. Reprinted in his Gesammelte mathematische Werke, volume 1, 40-67.
136.
78. Leonard Eugene Dickson, History of the Theory of Numbers. Carnegie Institution of Washington, volume 1, 1919; volume 2, 1920; volume 3, 1923. Reprinted by Stechert, 1934, and by Chelsea, 1952, 1971.
510.
79. Edsger W. Dijkstra, Selected Writings on Computing: A Personal Perspective. Springer-Verlag, 1982.
635.
80. G. Lejeune Dirichlet, "Verallgemeinerung eines Satzes aus der Lehre von den Kettenbrüchen nebst einigen Anwendungen auf die Theorie der Zahlen," Bericht über die Verhandlungen der Königlich-Preußischen Akademie der Wissenschaften zu Berlin (1842), 93-95. Reprinted in his Werke, volume 1, 635-638.
633.
81. A. C. Dixon, "On the sum of the cubes of the coefficients in a certain expansion by the binomial theorem," The Messenger of Mathematics, new series, 20 (1891), 79-80.
634.
82. John Dougall, "On Vandermonde's theorem, and some more general expansions," Proceedings of the Edinburgh Mathematical Society 25 (1907), 114-132.
171.
83. A. Conan Doyle, "The sign of the four; or, The problem of the Sholtos," Lippincott's Monthly Magazine (Philadelphia) 45 (1890), 145-223.
228, 405.
84. A. Conan Doyle, "The adventure of the final problem," The Strand Magazine 6 (1893), 558-570.
162.
85. P. du Bois-Reymond, "Sur la grandeur relative des infinis des fonctions," Annali di Matematica pura ed applicata, series 2, 4 (1871), 338-353.
440.
86. Harvey Dubner, "Generalized repunit primes," Mathematics of Computation 61 (1993), 927-930.
633.
87. Henry Ernest Dudeney, The Canterbury Puzzles and Other Curious Problems. E. P. Dutton, New York, 1908; 4th edition, Dover, 1958. (Dudeney had first considered the generalized Tower of Hanoi in The Weekly Dispatch, on 15 November 1896, 25 May 1902, and 15 March 1903.)
633.
88. G. Waldo Dunnington, Carl Friedrich Gauss: Titan of Science. Exposition Press, New York, 1955.
6.
89. F. J. Dyson, "Some guesses in the theory of partitions," Eureka 8 (1944), 10-15.
239.
90. A. W. F. Edwards, Pascal's Arithmetical Triangle. Oxford University Press, 1987.
155.
91. G. Eisenstein, "Entwicklung von ααα...," Journal für die reine und angewandte Mathematik 28 (1844), 49-52. Reprinted in his Mathematische Werke 1, 122-125.
202.
92. Noam D. Elkies, "On A4 + B4 + C4 = D4," Mathematics of Computation 51 (1988), 825-835.
131.
93. Erdős Pál, " egyenlet egész számú meg oldásairól," Matematikai Lapok 1 (1950), 192-209. English abstract on page 210.
634.
94. Paul Erdös, "My Scottish Book 'problems'," in The Scottish Book: Mathematics from the Scottish Café, edited by R. Daniel Mauldin, 1981, 35-45.
418.
95. P. Erdös and R. L. Graham, Old and New Problems and Results in Combinatorial Number Theory. Université de Genève, L'Enseignement Mathématique, 1980.
515, 525, 634, 635, 636.
96. P. Erdös, R. L. Graham, I. Z. Ruzsa, and E. G. Straus, "On the prime factors of ," Mathematics of Computation 29 (1975), 83-92.
525, 548.
97. Arulappah Eswarathasan and Eugene Levine, "p-integral harmonic sums," Discrete Mathematics 91 (1991), 249-257.
635.
98. Euclid, ΣTOIXEIA. Ancient manuscript first printed in Basel, 1533. Scholarly edition (Greek and Latin) by J. L. Heiberg in five volumes, Teubner, Leipzig, 1883-1888.
108.
99. Leonhard Euler, letter to Christian Goldbach (13 October 1729), in Correspondance mathématique et physique de quelques célèbres géomètres du XVIIIème siècle, edited by P. H. Fuss, St. Petersburg, 1843, volume 1, 3-7.
210, 634.
100. L. Eulero, "De progressionibus transcendentibus seu quarum termini generales algebraice dari nequeunt," Commentarii academiæ scientiarum imperialis Petropolitanæ 5 (1730), 36-57. Reprinted in his Opera Omnia, series 1, volume 14, 1-24.
210.
101. Leonh. Eulero, "Methodus generalis summandi progressiones," Commentarii academiæ scientiarum imperialis Petropolitanæ 6 (1732), 68-97. Reprinted in his Opera Omnia, series 1, volume 14, 42-72.
469.
102. Leonh. Eulero, "Observationes de theoremate quodam Fermatiano, aliisque ad numeros primos spectantibus," Commentarii academiæ scientiarum imperialis Petropolitanæ 6 (1732), 103-107. Reprinted in his Opera Omnia, series 1, volume 2, 1-5. Reprinted in his Commentationes arithmeticæ collectæ, volume 1, 1-3.
132.
103. Leonh. Eulero, "De progressionibus harmonicis observationes," Commentarii academiæ scientiarum imperialis Petropolitanæ 7 (1734), 150-161. Reprinted in his Opera Omnia, series 1, volume 14, 87-100.
277, 278.
104. Leonh. Eulero, "Methodus universalis series summandi ulterius promota," Commentarii academiæ scientiarum imperialis Petropolitanæ 8 (1736), 147-158. Reprinted in his Opera Omnia, series 1, volume 14, 124-137.
267.
105. Leonh. Euler, "De fractionibus continuis, Dissertatio," Commentarii academiæ scientiarum imperialis Petropolitanæ 9 (1737), 98-137. Reprinted in his Opera Omnia, series 1, volume 14, 187-215.
122.
106. Leonh. Euler, "Variæ observationes circa series infinitas," Commentarii academiæ scientiarum imperialis Petropolitanæ 9 (1737), 160-188. Reprinted in his Opera Omnia, series 1, volume 14, 216-244.
633.
107. Leonhard Euler, letter to Christian Goldbach (4 July 1744), in Correspondance mathématique et physique de quelques célèbres géomètres du XVIIIème siècle, edited by P. H. Fuss, St. Petersburg, 1843, volume 1, 278-293.
603.
108. Leonhardo Eulero, Introductio in Analysin Infinitorum. Tomus primus, Lausanne, 1748. Reprinted in his Opera Omnia, series 1, volume 8. Translated into French, 1786; German, 1788; Russian, 1936; English, 1988.
635.
109. L. Eulero, "De partitione numerorum," Novi commentarii academiæ scientiarum imperialis Petropolitanæ 3 (1750), 125-169. Reprinted in his Commentationes arithmeticæ collectæ, volume 1, 73-101. Reprinted in his Opera Omnia, series 1, volume 2, 254-294.
635.
110. Leonhardo Eulero, Institutiones Calculi Differentialis cum eius usu in Analysi Finitorum ac Doctrina Serierum. St. Petersburg, Academiæ Imperialis Scientiarum Petropolitanæ, 1755. Reprinted in his Opera Omnia, series 1, volume 10. Translated into German, 1790.
48, 267, 551, 603, 635.
111. L. Eulero, "Theoremata arithmetica nova methodo demonstrata," Novi commentarii academiæ scientiarum imperialis Petropolitanæ 8 (1760), 74-104. (Also presented in 1758 to the Berlin Academy.) Reprinted in his Commentationes arithmeticæ collectæ, volume 1, 274-286. Reprinted in his Opera Omnia, series 1, volume 2, 531-555.
133, 134.
112. L. Eulero, "Specimen algorithmi singularis," Novi commentarii academiæ scientiarum imperialis Petropolitanæ 9 (1762), 53-69. (Also presented in 1757 to the Berlin Academy.) Reprinted in his Opera Omnia, series 1, volume 15, 31-49.
302, 303.
113. L. Eulero, "Observationes analyticæ," Novi commentarii academiæ scientiarum imperialis Petropolitanæ 11 (1765), 124-143. Reprinted in his Opera Omnia, series 1, volume 15, 50-69.
575, 636.
114. Leonhard Euler, Vollständige Anleitung zur Algebra. Erster Theil. Von den verschiedenen Rechnungs-Arten, Verhältnissen und Proportionen. St. Petersburg, 1770. Reprinted in his Opera Omnia, series 1, volume 1. Translated into Russian, 1768; Dutch, 1773; French, 1774; Latin, 1790; English, 1797.
636.
115. L. Eulero, "Observationes circa bina biquadrata quorum summam in duo alia biquadrata resolvere liceat," Novi commentarii academiæ scientiarum imperialis Petropolitanæ 17 (1772), 64-69. Reprinted in his Commentationes arithmeticæ collectæ, volume 1, 473-476. Reprinted in his Opera Omnia, series 1, volume 3, 211-217.
131.
116. L. Eulero, "Observationes circa novum et singulare progressionum genus," Novi commentarii academiæ scientiarum imperialis Petropolitanæ 20 (1775), 123-139. Reprinted in his Opera Omnia, series 1, volume 7, 246-261.
513.
117. L. Eulero, "De serie Lambertina, plurimisque eius insignibus proprietatibus," Acta academiæ scientiarum imperialis Petropolitanæ 3,2 (1779), 29-51. Reprinted in his Opera Omnia, series 1, volume 6, 350-369.
202.
118. L. Eulero, "Specimen transformationis singularis serierum," Nova acta academiæ scientiarum imperialis Petropolitanæ 12 (1794), 58-70. Submitted for publication in 1778. Reprinted in his Opera Omnia, series 1, volume 16(2), 41-55.
207, 634.
119. Johann Faulhaber, Academia Algebræ, Darinnen die miraculosische Inventiones zu den höchsten Cossen weiters continuirt und profitiert werden, . . . biß auff die regulierte Zensicubiccubic Coß durch offnen Truck publiciert worden. Augsburg, 1631.
288.
120. William Feller, An Introduction to Probability Theory and Its Applications, volume 1. Wiley, 1950; second edition, 1957; third edition, 1968.
381, 636.
121. Pierre de Fermat, letter to Marin Mersenne (25 December 1640), in Œuvres de Fermat, volume 2, 212-217.
131.
122. Leonardo filio Bonacii Pisano [Fibonacci], Liber Abaci. First edition, 1202 (now lost); second edition, 1228. Reprinted in Scritti di Leonardo Pisano, edited by Baldassarre Boncompagni, 1857, volume 1.
633, 634.
123. Bruno de Finetti, Teoria delle Probabilità. Turin, 1970. English translation, Theory of Probability, Wiley, 1974-1975.
24.
124. Michael E. Fisher, "Statistical mechanics of dimers on a plane lattice," Physical Review 124 (1961), 1664-1672.
636.
125. R. A. Fisher, "Moments and product moments of sampling distributions," Proceedings of the London Mathematical Society, series 2, 30 (1929), 199-238.
636.
126. Pierre Forcadel, L'arithmeticque. Paris, 1557.
634.
127. J. Fourier, "Refroidissement séculaire du globe terrestre," Bulletin des Sciences par la Société philomathique de Paris, series 3, 7 (1820), 58-70. Reprinted in Œuvres de Fourier, volume 2, 271-288.
22.
128. Aviezri S. Fraenkel, "Complementing and exactly covering sequences," Journal of Combinatorial Theory, series A, 14 (1973), 8-20.
515, 633.
129. Aviezri S. Fraenkel, "How to beat your Wythoff games' opponent on three fronts," American Mathematical Monthly 89 (1982), 353-361.
563.
130. J. S. Frame, B. M. Stewart, and Otto Dunkel, "Partial solution to problem 3918," American Mathematical Monthly 48 (1941), 216-219.
633.
131. Piero della Francesca, Libellus de quinque corporibus regularibus. Vatican Library, manuscript Urbinas 632. Translated into Italian by Luca Pacioli, as part 3 of Pacioli's Diuine Proportione, Venice, 1509.
635.
132. J. Franel, Solutions to questions 42 and 170, in L'Intermédiaire des Math ématiciens 1 (1894), 45-47; 2 (1895), 33-35.
549.
133. W. D. Frazer and A. C. McKellar, "Samplesort: A sampling approach to minimal storage tree sorting," Journal of the Association for Computing Machinery 27 (1970), 496-507.
634.
134. Michael Lawrence Fredman, Growth Properties of a Class of Recursively Defined Functions. Ph.D. thesis, Stanford University, Computer Science Department, 1972.
513.
135. Nikolao Fuss, "Solutio quæstionis, quot modis polygonum n laterum in polygona m laterum, per diagonales resolvi quæat," Nova acta academiæ scientiarum imperialis Petropolitanæ 9 (1791), 243-251.
361.
136. Martin Gardner, "About phi, an irrational number that has some re markable geometrical expressions," Scientific American 201, 2 (August 1959), 128-134. Reprinted with additions in his book The 2nd Scientific American Book of Mathematical Puzzles & Diversions, 1961, 89-103.
299.
137. Martin Gardner, "On the paradoxical situations that arise from non transitive relations," Scientific American 231, 4 (October 1974), 120-124. Reprinted with additions in his book Time Travel and Other Mathematical Bewilderments, 1988, 55-69.
410.
138. Martin Gardner, "From rubber ropes to rolling cubes, a miscellany of refreshing problems," Scientific American 232, 3 (March 1975), 112-114; 232, 4 (April 1975), 130, 133. Reprinted with additions in his book Time Travel and Other Mathematical Bewilderments, 1988, 111-124.
634.
139. Martin Gardner, "On checker jumping, the amazon game, weird dice, card tricks and other playful pastimes," Scientific American 238, 2 (February 1978), 19, 22, 24, 25, 30, 32. Reprinted with additions in his book Penrose Tiles to Trapdoor Ciphers, 1989, 265-280.
636.
140. J. Garfunkel, "Problem E 1816: An inequality related to Stirling's formula," American Mathematical Monthly 74 (1967), 202.
636.
141. George Gasper and Mizan Rahman, Basic Hypergeometric Series. Cambridge University Press, 1990.
223.
142. Carolo Friderico Gauss, Disquisitiones Arithmeticæ. Leipzig, 1801. Reprinted in his Werke, volume 1.
123, 633.
143. Carolo Friderico Gauss, "Disquisitiones generales circa seriem infinitam
207, 212, 222, 529, 634.



Pars prior," Commentationes societatis regiæ scientiarum Gottingensis recentiores 2 (1813). (Thesis delivered to the Royal Society in Göttingen, 20 January 1812.) Reprinted in his Werke, volume 3, 123-163, together with an unpublished sequel on pages 207-229.
144. C. F. Gauss, "Pentagramma mirificum," written prior to 1836. Published posthumously in his Werke, volume 3, 480-490.
633.
145. Angelo Genocchi, "Intorno all'espressione generale de'numeri Bernulliani," Annali di Scienze Matematiche e Fisiche 3 (1852), 395-405.
551.
146. Ira Gessel, "Some congruences for Apéry numbers," Journal of Number Theory 14 (1982), 362-368.
634.
147. Ira Gessel and Richard P. Stanley, "Stirling polynomials," Journal of Combinatorial Theory, series A, 24 (1978), 24-33.
270.
148. Jekuthiel Ginsburg, "Note on Stirling's numbers," American Mathematical Monthly 35 (1928), 77-80.
271.
149. J. W. L. Glaisher, "On the product 11.22.33 . . . nn," The Messenger of Mathematics, new series, 7 (1877), 43-47.
636.
150. Solomon W. Golomb, "Problem 5407: A nondecreasing indicator function," American Mathematical Monthly 74 (1967), 740-743.
603, 633.
151. Solomon W. Golomb, "The 'Sales Tax' theorem," Mathematics Magazine 49 (1976), 187-189.
507.
152. Solomon W. Golomb, "Problem E 2529: An application of ψ(x)," American Mathematical Monthly 83 (1976), 487-488.
460.
153. I. J. Good, "Short proof of a conjecture by Dyson," Journal of Mathematical Physics 11 (1970), 1884.
634.
154. R. William Gosper, Jr., "Decision procedure for indefinite hypergeo-metric summation," Proceedings of the National Academy of Sciences of the United States of America 75 (1978), 40-42.
224, 634.
155. R. L. Graham, "On a theorem of Uspensky," American Mathematical Monthly 70 (1963), 407-409.
513.
156. R. L. Graham, "A Fibonacci-like sequence of composite numbers," Mathematics Magazine 37 (1964), 322-324.
635.
157. R. L. Graham, "Problem 5749," American Mathematical Monthly 77 (1970), 775.
634.
158. Ronald L. Graham, "Covering the positive integers by disjoint sets of the form {[nα + β] : n = 1, 2, . . . }," Journal of Combinatorial Theory, series A, 15 (1973), 354-358.
514.
159. R. L. Graham, "Problem 1242: Bijection between integers and composites," Mathematics Magazine 60 (1987), 180.
633.
160. R. L. Graham and D. E. Knuth, "Problem E 2982: A double infinite sum for |x|," American Mathematical Monthly 96 (1989), 525-526.
633.
161. Ronald L. Graham, Donald E. Knuth, and Oren Patashnik, Concrete Mathematics: A Foundation for Computer Science. Addison-Wesley, 1989; second edition, 1994.
102.
162. R. L. Graham and H. O. Pollak, "Note on a nonlinear recurrence related to ," Mathematics Magazine 43 (1970), 143-145.
633.
163. Guido Grandi, letter to Leibniz (July 1713), in Leibnizens mathematische Schriften, volume 4, 215-217.
58.
164. Daniel H. Greene and Donald E. Knuth, Mathematics for the Analysis of Algorithms. Birkhäuser, Boston, 1981; third edition, 1990.
535, 636.
165. Samuel L. Greitzer, International Mathematical Olympiads, 1959-1977. Mathematical Association of America, 1978.
633.
166. Oliver A. Gross, "Preferential arrangements," American Mathematical Monthly 69 (1962), 4-8.
635.
167. Branko Grünbaum, "Venn diagrams and independent families of sets," Mathematics Magazine 48 (1975), 12-23.
498.
168. L. J. Guibas and A. M. Odlyzko, "String overlaps, pattern matching, and nontransitive games," Journal of Combinatorial Theory, series A, 30 (1981), 183-208.
590, 636.
169. Richard K. Guy, Unsolved Problems in Number Theory. Springer Verlag, 1981.
525.
170. Inger Johanne Håland and Donald E. Knuth, "Polynomials involving the floor function," Mathematica Scandinavica 76 (1995), 194-200. Reprinted in Knuth's Selected Papers on Discrete Mathematics, 257-264.
514, 633.
171. Marshall Hall, Jr., The Theory of Groups. Macmillan, 1959.
553.
172. P. R. Halmos, "How to write mathematics," L'Enseignement Mathématique, series 2, 16 (1970), 123-152. Reprinted in How to Write Mathematics, American Mathematical Society, 1973, 19-48.
vi.
173. Paul R. Halmos, I Want to Be a Mathematician: An Automathography. Springer-Verlag, 1985. Reprinted by Mathematical Association of America, 1988.
v.
174. G. H. Halphen, "Sur des suites de fractions analogues à la suite de Farey," Bulletin de la Société mathématique de France 5 (1876), 170-175. Reprinted in his Œuvres, volume 2, 102-107.
305.
175. Hans Hamburger, "Über eine Erweiterung des Stieltjesschen Momenten-problems," Mathematische Annalen 81 (1920), 235-319; 82 (1921), 120-164, 168-187.
591.
176. J. M. Hammersley, "On the enfeeblement of mathematical skills by 'Modern Mathematics' and by similar soft intellectual trash in schools and universities," Bulletin of the Institute of Mathematics and Its Applications 4, 4 (October 1968), 66-85.
v.
177. J. M. Hammersley, "An undergraduate exercise in manipulation," The Mathematical Scientist 14 (1989), 1-23.
636.
178. Eldon R. Hansen, A Table of Series and Products. Prentice-Hall, 1975.
42.
179. G. H. Hardy, Orders of Infinity: The 'Infinitärcalcül' of Paul du Bois-Reymond. Cambridge University Press, 1910; second edition, 1924.
442, 636.
180. G. H. Hardy, "A mathematical theorem about golf," The Mathematical Gazette 29 (1945), 226-227. Reprinted in his Collected Papers, volume 7, 488.
636.
181. G. H. Hardy and E. M. Wright, An Introduction to the Theory of Numbers. Clarendon Press, Oxford, 1938; fifth edition, 1979.
111, 633.
182. Peter Henrici, Applied and Computational Complex Analysis. Wiley, volume 1, 1974; volume 2, 1977; volume 3, 1986.
300, 332, 602, 636.
183. Peter Henrici, "De Branges' proof of the Bieberbach conjecture: A view from computational analysis," Sitzungsberichte der Berliner Mathematischen Gesellschaft (1987), 105-121.
634.
184. Charles Hermite, letter to C. W. Borchardt (8 September 1875), in Journal für die reine und angewandte Mathematik 81 (1876), 93-95. Reprinted in his Œuvres, volume 3, 211-214.
555.
185. Charles Hermite, Cours de M. Hermite. Faculté des Sciences de Paris, 1882. Third edition, 1887; fourth edition, 1891.
634.
186. Charles Hermite, letter to S. Pincherle (10 May 1900), in Annali di Matematica pura ed applicata, series 3, 5 (1901), 57-60. Reprinted in his Œuvres, volume 4, 529-531.
538, 634.
187. I. N. Herstein and I. Kaplansky, Matters Mathematical. Harper & Row, 1974.
8.
188. A. P. Hillman and V. E. Hoggatt, Jr., "A proof of Gould's Pascal hexagon conjecture," Fibonacci Quarterly 10 (1972), 565-568, 598.
634.
189. C. A. R. Hoare, "Quicksort," The Computer Journal 5 (1962), 10-15.
28.
190. L. C. Hsu, "Note on a combinatorial algebraic identity and its application," Fibonacci Quarterly 11 (1973), 480-484.
634.
191. Kenneth E. Iverson, A Programming Language. Wiley, 1962.
24, 67, 633.
192. C. G. J. Jacobi, Fundamenta nova theoriæ functionum ellipticarum. Königsberg, Bornträger, 1829. Reprinted in his Gesammelte Werke, volume 1, 49-239.
64.
193. Svante Janson, Donald E. Knuth, Tomasz Łuczak, and Boris Pittel, "The birth of the giant component," Random Structures & Algorithms 4 (1993), 233-358. Reprinted with corrections in Knuth's Selected Papers on Discrete Mathematics, 643-792.
202.
194. Dov Jarden and Theodor Motzkin, "The product of sequences with a common linear recursion formula of order 2," Riveon Lematematika 3 (1949), 25-27, 38 (Hebrew with English summary). English version reprinted in Dov Jarden, Recurring Sequences, Jerusalem, 1958, 42-45; second edition, 1966, 30-33.
556.
195. Arne Jonassen and Donald E. Knuth, "A trivial algorithm whose analysis isn't," Journal of Computer and System Sciences 16 (1978), 301-322. Reprinted with an addendum in Knuth's Selected Papers on Analysis of Algorithms, 257-282.
535.
196. Bush Jones, "Note on internal merging," Software — Practice and Experience 2 (1972), 241-243.
175.
197. Flavius Josephus, . English translation, History of the Jewish War against the Romans, by H. St. J. Thackeray, in the Loeb Classical Library edition of Josephus's works, volumes 2 and 3, Heinemann, London, 1927-1928. (The "Josephus problem" may be based on an early manuscript now preserved only in the Slavonic version; see volume 2, page xi, and volume 3, page 654.)
8.
198. R. Jungen, "Sur les séries de Taylor n'ayant que des singularités algébricologarithmiques sur leur cercle de convergence," Commentarii Mathematici Helvetici 3 (1931), 266-306.
635.
199. J. Karamata, "Théorèmes sur la sommabilité exponentielle et d'autres sommabilités rattachant," Mathematica (Cluj) 9 (1935), 164-178.
257.
200. I. Kaucký, "Problem E 2257: A harmonic identity," American Mathematical Monthly 78 (1971), 908.
635.
201. J. B. Keiper, "Power series expansions of Riemann's ξ function," Mathematics of Computation 58 (1992), 765-773.
601.
202. Johannes Kepler, letter to Joachim Tancke (12 May 1608), in his Gesammelte Werke, volume 16, 154-165.
292.
203. Murray S. Klamkin, International Mathematical Olympiads, 1978-1985, and Forty Supplementary Problems. Mathematical Association of America, 1986.
633, 635.
204. R. Arthur Knoebel, "Exponentials reiterated," American Mathematical Monthly 88 (1981), 235-252.
202.
205. Konrad Knopp, Theorie und Anwendung der unendlichen Reihen. Julius Springer, Berlin, 1922; second edition, 1924. Reprinted by Dover, 1945. Fourth edition, 1947; fifth edition, 1964. English translation, Theory and Application of Infinite Series, 1928; second edition, 1951.
636.
206. Donald Knuth, "Transcendental numbers based on the Fibonacci sequence," Fibonacci Quarterly 2 (1964), 43-44, 52. Reprinted with an addendum in his Selected Papers on Fun and Games, 99-102.
553.
207. Donald E. Knuth, The Art of Computer Programming, volume 1: Fundamental Algorithms. Addison-Wesley, 1968; third edition, 1997.
vi, 500, 514, 530, 573, 633, 634, 635, 636.
208. Donald E. Knuth, The Art of Computer Programming, volume 2: Seminumerical Algorithms. Addison-Wesley, 1969; third edition, 1997.
110, 128, 500, 633, 635, 636.
209. Donald E. Knuth, The Art of Computer Programming, volume 3: Sorting and Searching. Addison-Wesley, 1973; second edition, 1998.
267, 411, 501, 634, 635, 636.
210. Donald E. Knuth, "Problem E 2492: Some sum," American Mathematical Monthly 82 (1975), 855.
634.
211. Donald E. Knuth, Mariages stables et leurs relations avec d'autres problèmes combinatoires. Les Presses de l'Université de Montréal, 1976. Revised and corrected edition, 1980. English translation, Stable Marriage and its Relation to Other Combinatorial Problems, 1997.
636.
212. Donald E. Knuth, The TEXbook. Addison-Wesley, 1984. Reprinted as volume A of Computers & Typesetting, 1986.
633.
213. Donald E. Knuth, "An analysis of optimum caching," Journal of Algorithms 6 (1985), 181-199. Reprinted with an addendum in his Selected Papers on Analysis of Algorithms, 235-255.
564.
214. Donald E. Knuth, Computers & Typesetting, volume D: METAFONT: The Program. Addison-Wesley, 1986.
633.
215. Donald E. Knuth, "Problem 1280: Floor function identity," Mathematics Magazine 61 (1988), 319-320.
633.
216. Donald E. Knuth, "Problem E 3106: A new sum for n2," American Mathematical Monthly 94 (1987), 795-797.
634
217. Donald E. Knuth, "Fibonacci multiplication," Applied Mathematics Letters 1 (1988), 57-60. Reprinted with an addendum in his Selected Papers on Fun and Games, 87-92.
635.
218. Donald E. Knuth, "A Fibonacci-like sequence of composite numbers," Mathematics Magazine 63 (1990), 21-25. Reprinted with an addendum in his Selected Papers on Fun and Games, 93-98.
562.
219. Donald E. Knuth, "Problem E3309: A binomial coefficient inequality," American Mathematical Monthly 97 (1990), 614.
634.
220. Donald E. Knuth, "Two notes on notation," American Mathematical Monthly 99 (1992), 403-422. Reprinted with an addendum in his Selected Papers on Discrete Mathematics, 15-44.
24, 162, 267, 598.
221. Donald E. Knuth, "Convolution polynomials," The Mathematica Journal 2,4 (Fall 1992), 67-78. Reprinted with an addendum in his Selected Papers on Discrete Mathematics, 225-256.
267, 566, 635.
222. Donald E. Knuth, "Johann Faulhaber and sums of powers," Mathematics of Computation 61 (1993), 277-294. Reprinted with an addendum in his Selected Papers on Discrete Mathematics, 61-84.
288.
223. Donald E. Knuth, "Bracket notation for the coefficient-of operator," in A Classical Mind, essays in honour of C. A. R. Hoare, edited by A. W. Roscoe, Prentice-Hall, 1994, 247-258. Reprinted with an addendum in his Selected Papers on Discrete Mathematics, 45-59.
197.
224. Donald E. Knuth and Thomas J. Buckholtz, "Computation of Tangent, Euler, and Bernoulli numbers," Mathematics of Computation 21 (1967), 663-688. Reprinted with an addendum in Knuth's Selected Papers on Design of Algorithms, 359-372.
555.
225. Donald E. Knuth and Ilan Vardi, "Problem 6581: The asymptotic expansion of the middle binomial coefficient," American Mathematical Monthly 97 (1990), 626-630.
636.
226. Donald E. Knuth and Herbert S. Wilf, "The power of a prime that divides a generalized binomial coefficient," Journal für die reine und angewandte Mathematik 396 (1989), 212-219. Reprinted in Knuth's Selected Papers on Discrete Mathematics, 511-524.
530, 635.
227. Donald E. Knuth and Hermann Zapf, "AMS Euler — A new typeface for mathematics," Scholarly Publishing 20 (1989), 131-157. Reprinted in Knuth's Digital Typography, 339-365.
viii.
228. C. Kramp, Élémens d'arithmétique universelle. Cologne, 1808.
111.
229. E. E. Kummer, "Über die hypergeometrische Reihe
213, 634.



Journal für die reine und angewandte Mathematik 15 (1836), 39-83, 127-172. Reprinted in his Collected Papers, volume 2, 75-166.
230. E. E. Kummer, "Über die Ergänzungssätze zu den allgemeinen Reciprocitätsgesetzen," Journal für die reine und angewandte Mathematik 44 (1852), 93-146. Reprinted in his Collected Papers, volume 1, 485-538.
634.
231. R. P. Kurshan and B. Gopinath, "Recursively generated periodic sequences," Canadian Journal of Mathematics 26 (1974), 1356-1371.
501.
232. Thomas Fantet de Lagny, Analyse générale ou Méthodes nouvelles pour résoudre les problèmes de tous les genres et de tous les degrés à l'infini. Published as volume 11 of Mémoires de l'Académie Royale des Sciences, Paris, 1733.
304.
233. de la Grange [Lagrange], "Démonstration d'un théorème nouveau concernant les nombres premiers," Nouveaux Mémoires de l'Académie royale des Sciences et Belles-Lettres, Berlin (1771), 125-137. Reprinted in his Œuvres, volume 3, 425-438.
635.
234. de la Grange [Lagrange], "Sur une nouvelle espèce de calcul rélatif à la différentiation & à l'intégration des quantités variables," Nouveaux Mémoires de l'Académie royale des Sciences et Belles-Lettres, Berlin (1772), 185-221. Reprinted in his Œuvres, volume 3, 441-476.
470.
235. I. Lah, "Eine neue Art von Zahlen, ihre Eigenschaften und Anwendung in der mathematischen Statistik," Mitteilungsblatt für Mathematische Statistik 7 (1955), 203-212. [More general formulas had been published by L. Toscano, Commentationes 3 (Vatican City: Accademia della Scienze, 1939), 721-757, Equations 17 and 117.]
634.
236. I. H. Lambert, "Observationes variæ in Mathesin puram," Acta Helvetica 3 (1758), 128-168. Reprinted in his Opera Mathematica, volume 1, 16-51.
201.
237. Lambert, "Observations analytiques," Nouveaux Mémoires de l'Académie royale des Sciences et Belles-Lettres, Berlin (1770), 225-244. Reprinted in his Opera Mathematica, volume 2, 270-290.
201.
238. Edmund Landau, Handbuch der Lehre von der Verteilung der Primzahlen, two volumes. Teubner, Leipzig, 1909.
448, 636.
239. Edmund Landau, Vorlesungen über Zahlentheorie, three volumes. Hirzel, Leipzig, 1927.
634.
240. P. S. de la Place [Laplace], "Mémoire sur les approximations des Formules qui sont fonctions de très-grands nombres," Mémoires de l'Academie royale des Sciences de Paris (1782), 1-88. Reprinted in his Œuvres Complètes 10, 207-291.
466.
241. Adrien-Marie Legendre, Essai sur la Théorie des Nombres. Paris, 1798; second edition, 1808. Third edition (retitled Théorie des Nombres, in two volumes), 1830; fourth edition, Blanchard, 1955.
633.
242. D. H. Lehmer, "Tests for primality by the converse of Fermat's theorem," Bulletin of the American Mathematical Society, series 2, 33 (1927), 327- 340. Reprinted in his Selected Papers, volume 1, 69-82.
633.
243. D. H. Lehmer, "On Stern's diatomic series," American Mathematical Monthly 36 (1929), 59-67.
635.
244. D. H. Lehmer, "On Euler's totient function," Bulletin of the American Mathematical Society, series 2, 38 (1932), 745-751. Reprinted in his Selected Papers, volume 1, 319-325.
526.
245. G. W. Leibniz, letter to Johann Bernoulli (May 1695), in Leibnizens mathematische Schriften, volume 3, 174-179.
168.
246. C. G. Lekkerkerker, "Voorstelling van natuurlijke getallen door een som van getallen van Fibonacci," Simon Stevin 29 (1952), 190-195.
295.
247. Tamás Lengyel, "A combinatorial identity and the world series," SIAM Review 35 (1993), 294-297.
167.
248. Tamás Lengyel, "On some properties of the series  and the Stirling numbers of the second kind," Discrete Mathematics 150 (1996), 281-292.
635.
249. Li Shan-Lan, Duò Jī Bĭ Lèi [Sums of Piles Obtained Inductively]. In his Zégŭxī Zhāi Suànxué [Classically Inspired Meditations on Mathematics], Nanjing, 1867.
269.
250. Elliott H. Lieb, "Residual entropy of square ice," Physical Review 162 (1967), 162-172.
636.
251. J. Liouville, "Sur l'expression φ(n), qui marque combien la suite 1, 2, 3, . . . , n contient de nombres premiers à n," Journal de Mathématiques pures et appliquées, series 2, 2 (1857), 110-112.
136.
252. B. F. Logan, "The recovery of orthogonal polynomials from a sum of squares," SIAM Journal on Mathematical Analysis 21 (1990), 1031-1050.
634.
253. B. F. Logan, "Polynomials related to the Stirling numbers," AT&T Bell Laboratories internal technical memorandum, August 10, 1987.
635.
254. Calvin T. Long and Verner E. Hoggatt, Jr., "Sets of binomial coefficients with equal products," Fibonacci Quarterly 12 (1974), 71-79.
634.
255. Shituo Lou and Qi Yao, "A Chebychev's type of prime number theorem in a short interval-II," Hardy-Ramanujan Journal 15 (1992), 1-33.
525.
256. Sam Loyd, Cyclopedia of Puzzles. Franklin Bigelow Corporation, Morningside Press, New York, 1914.
560.
257. E. Lucas, "Sur les rapports qui existent entre la théorie des nombres et le Calcul intégral," Comptes Rendus hebdomadaires des séances de l'Académie des Sciences (Paris) 82 (1876), 1303-1305.
633, 634, 635.
258. Édouard Lucas, "Sur les congruences des nombres eulériens et des coefficients différentiels des fonctions trigonométriques, suivant un module premier," Bulletin de la Société mathématique de France 6 (1877), 49-54.
634.
259. Edouard Lucas, Théorie des Nombres, volume 1. Paris, 1891.
292, 634.
260. Édouard Lucas, Récréations mathématiques, four volumes. Gauthier-Villars, Paris, 1891-1894. Reprinted by Albert Blanchard, Paris, 1960. (The Tower of Hanoi is discussed in volume 3, pages 55-59.)
1.
261. R. C. Lyness, "Cycles," The Mathematical Gazette 29 (1945), 231-233.
501.
262. R. C. Lyness, "Cycles," The Mathematical Gazette 45 (1961), 207-209.
501.
263. Colin MacLaurin, Collected Letters, edited by Stella Mills. Shiva Publishing, Nantwich, Cheshire, 1982.
469.
264. P. A. MacMahon, "Application of a theory of permutations in circular procession to the theory of numbers," Proceedings of the London Mathematical Society 23 (1892), 305-313.
140.
265. J.-C. Martzloff, Histoire des Mathématiques Chinoises. Paris, 1988. English translation, A History of Chinese Mathematics, Springer-Verlag, 1997.
269.
266. , "Diofantovost' perechislimykh mnozhestv," Doklady Akademii Nauk SSSR 191 (1970), 279-282. English translation, with amendments by the author, "Enumerable sets are diophantine," Soviet Mathematics — Doklady 11 (1970), 354-357.
294, 635.
267. Z. A. Melzak, Companion to Concrete Mathematics. Volume 1, Mathematical Techniques and Various Applications, Wiley, 1973; volume 2, Mathematical Ideas, Modeling & Applications, Wiley, 1976.
vi.
268. N. S. Mendelsohn, "Problem E 2227: Divisors of binomial coefficients," American Mathematical Monthly 78 (1971), 201.
634.
269. Marini Mersenni, Cogitata Physico-Mathematica. Paris, 1644.
109.
270. F. Mertens, "Ueber einige asymptotische Gesetze der Zahlentheorie," Journal für die reine und angewandte Mathematik 77 (1874), 289-338.
139.
271. Mertens, "Ein Beitrag zur analytischen Zahlentheorie," Journal für die reine und angewandte Mathematik 78 (1874), 46-62.
23.
272. W. H. Mills, "A prime representing function," Bulletin of the American Mathematical Society, series 2, 53 (1947), 604.
634.
273. A. F. Möbius, "Über eine besondere Art von Umkehrung der Reihen," Journal für die reine und angewandte Mathematik 9 (1832), 105-123. Reprinted in his Gesammelte Werke, volume 4, 589-612.
138.
274. A. Moessner, "Eine Bemerkung über die Potenzen der natürlichen Zahlen," Sitzungsberichte der Mathematisch-Naturwissenschaftlichen Klasse der Bayerischen Akademie der Wissenschaften, 1951, Heft 3, 29.
636.
275. Hugh L Montgomery, "Fluctuations in the mean of Euler's phi function," Proceedings of the Indian Academy of Sciences, Mathematical Sciences, 97 (1987), 239-245.
463.
276. Peter L. Montgomery, "Problem E 2686: LCM of binomial coefficients," American Mathematical Monthly 86 (1979), 131.
634.
277. Leo Moser, "Problem B-6: Some reflections," Fibonacci Quarterly 1, 4 (1963), 75-76.
291.
278. T. S. Motzkin and E. G. Straus, "Some combinatorial extremum problems," Proceedings of the American Mathematical Society 7 (1956), 1014-1021.
564.
279. B. R. Myers, "Problem 5795: The spanning trees of an n-wheel," American Mathematical Monthly 79 (1972), 914-915.
635.
280. Isaac Newton, letter to John Collins (18 February 1670), in The Correspondence of Isaac Newton, volume 1, 27. Excerpted in The Mathematical Papers of Isaac Newton, volume 3, 563.
277.
281. Ivan Niven, Diophantine Approximations. Interscience, 1963.
633.
282. Ivan Niven, "Formal power series," American Mathematical Monthly 76 (1969), 871-889.
332.
283. Andrew M. Odlyzko and Herbert S. Wilf, "Functional iteration and the Josephus problem," Glasgow Mathematical Journal 33 (1991), 235-240.
81.
284. Blaise Pascal, "De numeris multiplicibus," presented to Académie Parisienne in 1654 and published with his Traité du triangle arithmétique [285]. Reprinted in Œuvres de Blaise Pascal, volume 3, 314-339.
633.
285. Blaise Pascal, "Traité du triangle arithmetique," in his Traité du Triangle Arithmetique, avec quelques autres petits traitez sur la mesme matiere, Paris, 1665. Reprinted in Œuvres de Blaise Pascal (Hachette, 1904-1914), volume 3, 445-503; Latin editions from 1654 in volume 11, 366-390.
155, 156, 624.
286. G. P. Patil, "On the evaluation of the negative binomial distribution with examples," Technometrics 2 (1960), 501-505.
636.
287. C. S. Peirce, letter to E. S. Holden (January 1901). In The New Elements of Mathematics, edited by Carolyn Eisele, Mouton, The Hague, 1976, volume 1, 247-253. (See also page 211.)
634.
288. C. S. Peirce, letter to Henry B. Fine (17 July 1903). In The New Elements of Mathematics, edited by Carolyn Eisele, Mouton, The Hague, 1976, volume 3, 781-784. (See also "Ordinals," an unpublished manuscript from circa 1905, in Collected Papers of Charles Sanders Peirce, volume 4, 268-280.)
525.
289. Walter Penney, "Problem 95: Penney-Ante," Journal of Recreational Mathematics 7 (1974), 321.
408.
290. J. K. Percus, Combinatorial Methods. Springer-Verlag, 1971.
636.
291. Marko Petkovšek, "Hypergeometric solutions of linear recurrences with polynomial coefficients," Journal of Symbolic Computation 14 (1992), 243-264.
229, 575, 634.
292. J. F. Pfaff, "Observationes analyticæ ad L. Euleri institutiones calculi integralis, Vol. IV, Supplem. II & IV," Nova acta academiæ scientiarum imperialis Petropolitanæ 11, Histoire section, 37-57. (This volume, printed in 1798, contains mostly proceedings from 1793, although Pfaff's memoir was actually received in 1797.)
207, 214, 217, 634.
293. L. Pochhammer, "Ueber hypergeometrische Functionen nter Ordnung," Journal für die reine und angewandte Mathematik 71 (1870), 316-352.
48.
294. H. Poincaré, "Sur les fonctions à espaces lacunaires," American Journal of Mathematics 14 (1892), 201-221.
636.
295. S. D. Poisson, "Mémoire sur le calcul numérique des intégrales définies," Mémoires de l'Académie Royale des Sciences de l'Institut de France, series 2, 6 (1823), 571-602.
471.
296. G. Pólya,"Kombinatorische Anzahlbestimmungen für Gruppen, Graphen und chemische Verbindungen," Acta Mathematica 68 (1937), 145-254. English translation, with commentary by Ronald C. Read, Combinatorial Enumeration of Groups, Graphs, and Chemical Compounds, Springer-Verlag, 1987.
635.
297. George Pólya, Induction and Analogy in Mathematics. Princeton University Press, 1954.
vi, 16, 508, 633.
298. G. Pólya, "On picture-writing," American Mathematical Monthly 63 (1956), 689-697.
327, 635.
299. G. Pólya and G. Szegö, Aufgaben und Lehrsätze aus der Analysis, two volumes. Julius Springer, Berlin, 1925; fourth edition, 1970 and 1971. English translation, Problems and Theorems in Analysis, 1972 and 1976.
636.
300. R. Rado, "A note on the Bernoullian numbers," Journal of the London Mathematical Society 9 (1934), 88-90.
635.
301. Earl D. Rainville, "The contiguous function relations for pFq with applications to Bateman's  and Rice's Hn(ζ, p, v)," Bulletin of the American Mathematical Society, series 2, 51 (1945), 714-723.
529.
302. George N. Raney, "Functional composition patterns and power series reversion," Transactions of the American Mathematical Society 94 (1960), 441-451.
359, 635.
303. D. Rameswar Rao, "Problem E2208: A divisibility problem," American Mathematical Monthly 78 (1971), 78-79.
633.
304. John William Strutt, Third Baron Rayleigh, The Theory of Sound. First edition, 1877; second edition, 1894. (The cited material about irrational spectra is from section 92a of the second edition.)
77.
305. Robert Recorde, The Whetstone of Witte. London, 1557.
446.
306. Simeon Reich, "Problem 6056: Truncated exponential-type series," American Mathematical Monthly 84 (1977), 494-495.
636.
307. Georges de Rham, "Un peu de mathématiques à propos d'une courbe plane," Elemente der Mathematik 2 (1947), 73-76, 89-97. Reprinted in his Œuvres Mathématiques, 678-689.
635.
308. Paulo Ribenboim, 13 Lectures on Fermat's Last Theorem. Springer-Verlag, 1979.
555, 634.
309. Bernhard Riemann, "Ueber die Darstellbarkeit einer Function durch eine trigonometrische Reihe," Habilitationsschrift, Göttingen, 1854. Published in Abhandlungen der mathematischen Classe der Königlichen Gesellschaft der Wissenschaften zu Göttingen 13 (1868), 87-132. Reprinted in his Gesammelte Mathematische Werke, 227-264.
633.
310. Samuel Roberts, "On the figures formed by the intercepts of a system of straight lines in a plane, and on analogous relations in space of three dimensions," Proceedings of the London Mathematical Society 19 (1889), 405-422.
633.
311. Øystein Rødseth, "Problem E 2273: Telescoping Vandermonde convolutions," American Mathematical Monthly 79 (1972), 88-89.
634.
312. J. Barkley Rosser and Lowell Schoenfeld, "Approximate formulas for some functions of prime numbers," Illinois Journal of Mathematics 6 (1962), 64-94.
111.
313. Gian-Carlo Rota, "On the foundations of combinatorial theory. I. Theory of Möbius functions," Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 2 (1964), 340-368.
516.
314. Ranjan Roy, "Binomial identities and hypergeometric series," American Mathematical Monthly 94 (1987), 36-46.
634.
315. Louis Saalschütz, "Eine Summationsformel," Zeitschrift für Mathematik und Physik 35 (1890), 186-188.
214.
316. A. I. Saltykov, "O funktsii Éĭlera," Vestnik Moskovskogo Universiteta, series 1, Matematika, Mekhanika (1960), number 6, 34-50.
463.
317. A. Sárközy, "On divisors of binomial coefficients, I," Journal of Number Theory 20 (1985), 70-80.
548.
318. W. W. Sawyer, Prelude to Mathematics. Baltimore, Penguin, 1955.
207.
319. O. Schlömilch, "Ein geometrisches Paradoxon," Zeitschrift für Mathematik und Physik 13 (1868), 162.
293.
320. Ernst Schröder, "Vier combinatorische Probleme," Zeitschrift für Mathematik und Physik 15 (1870), 361-376.
635.
321. Heinrich Schröter, "Ableitung der Partialbruch- und Produkt-Entwickelungen für die trigonometrischen Funktionen," Zeitschrift für Mathematik und Physik 13 (1868), 254-259.
635.
322. R. S. Scorer, P. M. Grundy, and C. A. B. Smith, "Some binary games," The Mathematical Gazette 28 (1944), 96-103.
633.
323. J. Sedláček, "On the skeletons of a graph or digraph," in Combinatorial Structures and their Applications, Gordon and Breach, 1970, 387-391. (This volume contains proceedings of the Calgary International Conference on Combinatorial Structures and their Applications, 1969.)
635.
324. J. O. Shallit, "Problem 6450: Two series," American Mathematical Monthly 92 (1985), 513-514.
635.
325. R. T. Sharp, "Problem 52: Overhanging dominoes," Pi Mu Epsilon Journal 1, 10 (1954), 411-412.
273.
326. W. Sierpiński, "Sur la valeur asymptotique d'une certaine somme," Bulletin International de l'Académie Polonaise des Sciences et des Lettres (Cracovie), series A (1910), 9-11.
87.
327. W. Sierpiński, "Sur les nombres dont la somme de diviseurs est une puissance du nombre 2," Calcutta Mathematical Society Golden Jubilee Commemorative Volume (1958-1959), part 1, 7-9.
634.
328. Wacław Sierpiński, A Selection of Problems in the Theory of Numbers. Macmillan, 1964.
634.
329. David L. Silverman, "Problematical Recreations 447: Numerical links," Aviation Week & Space Technology 89, 10 (1 September 1968), 71. Reprinted as Problem 147 in Second Book of Mathematical Bafflers, edited by Angela Fox Dunn, Dover, 1983.
635.
330. N. J. A. Sloane, A Handbook of Integer Sequences. Academic Press, 1973. Sequel, with Simon Plouffe, The Encyclopedia of Integer Sequences, Academic Press, 1995. http://www.research.att.com/~njas/sequences.
42, 341, 464.
331. A. D. Solov'ev, "Odno kombinatornoe tozhdestvo i ego primenenie k zadache o pervom nastuplenii redkogo ,"  i ee  11 (1966), 313-320. English translation, "A combinatorial identity and its application to the problem concerning the first occurrence of a rare event," Theory of Probability and Its Applications 11 (1966), 276-282.
408.
332. William G. Spohn, Jr., "Can mathematics be saved?" Notices of the American Mathematical Society 16 (1969), 890-894.
v.
333. Richard P. Stanley, "Differentiably finite power series," European Journal of Combinatorics 1 (1980), 175-188.
636.
334. Richard P. Stanley, "On dimer coverings of rectangles of fixed width," Discrete Applied Mathematics 12 (1985), 81-87.
636.
335. Richard P. Stanley, Enumerative Combinatorics, volume 1. Wadsworth & Brooks/Cole, 1986.
534, 635, 636.
336. K. G. C. von Staudt, "Beweis eines Lehrsatzes, die Bernoullischen Zahlen betreffend," Journal für die reine und angewandte Mathematik 21 (1840), 372-374.
635.
336′ Tor B. Staver, "Om summasjon av potenser av binomiaalkoeffisientene," Norsk Matematisk Tidsskrift 29 (1947), 97-103.
634.
337. Guy L. Steele Jr., Donald R. Woods, Raphael A. Finkel, Mark R. Crispin, Richard M. Stallman, and Geoffrey S. Goodfellow, The Hacker's Dictionary: A Guide to the World of Computer Wizards. Harper & Row, 1983.
124.
338. J. Steiner, "Einige Gesetze über die Theilung der Ebene und des Raumes," Journal für die reine und angewandte Mathematik 1 (1826), 349-364. Reprinted in his Gesammelte Werke, volume 1, 77-94.
5, 633.
339. M. A. Stern, "Ueber eine zahlentheoretische Funktion," Journal für die reine und angewandte Mathematik 55 (1858), 193-220.
116.
340. L. Stickelberger, "Ueber eine Verallgemeinerung der Kreistheilung," Mathematische Annalen 37 (1890), 321-367.
633.
341. T. J. Stieltjes, letters to Hermite (June 1885), in Correspondance d'Hermite et de Stieltjes, volume 1, 146-159.
601.
342. T. J. Stieltjes, "Table des valeurs des sommes ," Acta Mathematica 10 (1887), 299-302. Reprinted in his Œuvres Complètes, volume 2, 100-103.
633.
343. James Stirling, Methodus Differentialis. London, 1730. English translation, The Differential Method, 1749.
192, 258, 297.
344. Volker Strehl, "Binomial identities — combinatorial and algorithmic aspects," Discrete Mathematics 136 (1994), 309-346.
549, 634.
345. Dura W. Sweeney, "On the computation of Euler's constant," Mathematics of Computation 17 (1963), 170-178.
481.
346. J. J. Sylvester, "Problem 6919," Mathematical Questions with their Solutions from the 'Educational Times' 37 (1882), 42-43, 80.
633.
347. J. J. Sylvester, "On the number of fractions contained in any 'Farey series' of which the limiting number is given," The London, Edinburgh and Dublin Philosophical Magazine and Journal of Science, series 5, 15 (1883), 251-257. Reprinted in his Collected Mathematical Papers, volume 4, 101-109.
133.
348. M. Szegedy, "The solution of Graham's greatest common divisor problem," Combinatorica 6 (1986), 67-71.
525.
349. S. Tanny, "A probabilistic interpretation of Eulerian numbers," Duke Mathematical Journal 40 (1973), 717-722.
635.
350. L. Theisinger, "Bemerkung über die harmonische Reihe," Monatshefte für Mathematik und Physik 26 (1915), 132-134.
634.
351. T. N. Thiele, The Theory of Observations. Charles & Edwin Layton, London, 1903. Reprinted in The Annals of Mathematical Statistics 2 (1931), 165-308.
397, 398.
352. E. C. Titchmarsh, The Theory of the Riemann Zeta-Function. Clarendon Press, Oxford, 1951; second edition, revised by D. R. Heath-Brown, 1986.
636.
353. F. G. Tricomi and A. Erdélyi, "The asymptotic expansion of a ratio of gamma functions," Pacific Journal of Mathematics 1 (1951), 133-142.
636.
354. Peter Ungar, "Problem E 3052: A sum involving Stirling numbers," American Mathematical Monthly 94 (1987), 185-186.
280.
355. J. V. Uspensky, "On a problem arising out of the theory of a certain game," American Mathematical Monthly 34 (1927), 516-521.
633.
356. Alfred van der Poorten, "A proof that Euler missed . . . Apéry's proof of the irrationality of ζ(3), an informal report," The Mathematical Intelligencer 1 (1979), 195-203.
238.
357. A. Vandermonde, "Mémoire sur des irrationnelles de différens ordres avec une application au cercle," Mémoires de Mathématique et de Physique, tirés des registres de l'Académie Royale des Sciences (1772), part 1, 489- 498.
169, 634.
358. Ilan Vardi, "The error term in Golomb's sequence," Journal of Number Theory 40 (1992), 1-11.
633, 636.
359. J. Venn, "On the diagrammatic and mechanical representation of propositions and reasonings," The London, Edinburgh and Dublin Philosophical Magazine and Journal of Science, series 5, 10 (1880), 1-18.
498, 633.
360. John Wallis, A Treatise of Angular Sections. Oxford, 1684.
635.
361. Edward Waring, Meditationes Algebraïcæ. Cambridge, 1770; third edition, 1782.
635.
361′ J. Wasteels, "Quelques propriétés des nombres de Fibonacci," Mathesis, series 3, 11 (1902), 60-62.
635.
362. William C. Waterhouse, "Problem E 3117: Even odder than we thought," American Mathematical Monthly 94 (1987), 691-692.
635.
363. Frederick V. Waugh and Margaret W. Maxfield, "Side-and-diagonal numbers," Mathematics Magazine 40 (1967), 74-83.
635.
364. Warren Weaver, "Lewis Carroll and a geometrical paradox," American Mathematical Monthly 45 (1938), 234-236.
293.
365. H. Weber, "Leopold Kronecker," Jahresbericht der Deutschen Mathe matiker-Vereinigung 2 (1892), 5-31. Reprinted in Mathematische Annalen 43 (1893), 1-25.
521.
366. Louis Weisner, "Abstract theory of inversion of finite series," Transactions of the American Mathematical Society 38 (1935), 474-484.
516.
367. Edgar M. E. Wermuth, "Die erste Fourierreihe," Mathematische Semesterberichte 40 (1993), 133-145.
603.
368. Hermann Weyl, "Über die Gibbs'sche Erscheinung und verwandte Konvergenzphänomene," Rendiconti del Circolo Matematico di Palermo 30 (1910), 377-407.
87.
369. F. J. W. Whipple, "Some transformations of generalized hypergeometric series," Proceedings of the London Mathematical Society, series 2, 26 (1927), 257-272.
634.
370. Alfred North Whitehead, An Introduction to Mathematics. London and New York, 1911.
503.
371. Alfred North Whitehead, "Technical education and its relation to science and literature," chapter 2 in The Organization of Thought, Educational and Scientific, London and New York, 1917. Reprinted as chapter 4 of The Aims of Education and Other Essays, New York, 1929.
91.
372. Alfred North Whitehead, Science and the Modern World. New York, 1925. Chapter 2 reprinted in The World of Mathematics, edited by James R. Newman, 1956, volume 1, 402-416.
603.
373. Herbert S. Wilf, generatingfunctionology. Academic Press, 1990; second edition, 1994.
575, 634.
374. Herbert S. Wilf and Doron Zeilberger, "An algorithmic proof theory for hypergeometric (ordinary and 'q') multisum/integral identities," Inventiones Mathematicae 108 (1992), 575-633.
240, 241, 634.
375. H. C. Williams and Harvey Dubner, "The primality of R1031," Mathematics of Computation 47 (1986), 703-711.
633.
376. J. Wolstenholme, "On certain properties of prime numbers," Quarterly Journal of Pure and Applied Mathematics 5 (1862), 35-39.
635.
377. Derick Wood, "The Towers of Brahma and Hanoi revisited," Journal of Recreational Mathematics 14 (1981), 17-24.
633.
378. J. Worpitzky, "Studien über die Bernoullischen und Euler schen Zahlen," Journal für die reine und angewandte Mathematik 94 (1883), 203-232.
269.
379. E. M. Wright, "A prime-representing function," American Mathematical Monthly 58 (1951), 616-618; errata in 59 (1952), 99.
633.
380. Derek A. Zave, "A series expansion involving the harmonic numbers," Information Processing Letters 5 (1976), 75-77.
635.
381. E. Zeckendorf, "Représentation des nombres naturels par une somme de nombres de Fibonacci ou de nombres de Lucas," Bulletin de la Société Royale des Sciences de Liège 41 (1972), 179-182.
295.
382. Doron Zeilberger, "Sister Celine's technique and its generalizations," Journal of Mathematical Analysis and Applications 85 (1982), 114-145. See also Sister Mary Celine Fasenmyer, "A note on pure recurrence relations," American Mathematical Monthly 56 (1949), 14-17.
230.
383. Doron Zeilberger, "A holonomic systems approach to special functions identities," Journal of Computational and Applied Mathematics 32 (1990), 321-368.
564.
384. Doron Zeilberger, "The method of creative telescoping," Journal of Symbolic Computation 11 (1991), 195-204.
229.









C. Credits for Exercises
The exercises in this book have been drawn from many sources. The authors have tried to trace the origins of all the problems that have been published before, except in cases where the exercise is so elementary that its inventor would probably not think anything was being invented.
Many of the exercises come from examinations in Stanford's Concrete Mathematics classes. The teaching assistants and instructors often devised new problems for those exams, so it is appropriate to list their names here:


Year
Instructor
Teaching Assistant(s)


1970
Don Knuth
Vaughan Pratt


1971
Don Knuth
Leo Guibas


1973
Don Knuth
Henson Graves, Louis Jouaillec


1974
Don Knuth
Scot Drysdale, Tom Porter


1975
Don Knuth
Mark Brown, Luis Trabb Pardo


1976
Andy Yao
Mark Brown, Lyle Ramshaw


1977
Andy Yao
Yossi Shiloach


1978
Frances Yao
Yossi Shiloach


1979
Ron Graham
Frank Liang, Chris Tong, Mark Haiman


1980
Andy Yao
Andrei Broder, Jim McGrath


1981
Ron Graham
Oren Patashnik


1982
Ernst Mayr
Joan Feigenbaum, Dave Helmbold


1983
Ernst Mayr
Anna Karlin


1984
Don Knuth
Oren Patashnik, Alex Schäffer


1985
Andrei Broder
Pang Chen, Stefan Sharkansky


1986
Don Knuth
Arif Merchant, Stefan Sharkansky


The TA sessions were invaluable, I mean really great.
Keep the same instructor and the same TAs next year.
Class notes very good and useful.
I never "got" Stirling numbers.
In addition, David Klarner (1971), Bob Sedgewick (1974), Leo Guibas (1975), and Lyle Ramshaw (1979) each contributed to the class by giving six or more guest lectures. Detailed lecture notes taken each year by the teaching assistants and edited by the instructors have served as the basis of this book.
1.1     Pólya [297, p. 120].
1.2     Scorer, Grundy, and Smith [322].
1.5     Venn [359].
1.6     Steiner [338]; Roberts [310].
1.8     Gauss [144].
1.9     Cauchy [53, note 2, theorem 17].
1.10   Atkinson [15].
1.11   Inspired by Wood [377].
1.14   Steiner [338]; Pólya [297, chapter 3]; Brother Alfred [42].
1.17   Dudeney [87, puzzle 1].
1.21   Ball [20] credits B. A. Swinden.
1.22   Based on an idea of Peter Shor.*
1.23   Bjorn Poonen.*
1.25   Frame, Stewart, and Dunkel [130].
2.2     Iverson [191, p. 11].
2.3     [207, exercise 1.2.3-2].
2.5     [207, exercise 1.2.3-25].
2.22   Binet [30, §4].
2.23   1982 final.
2.26   [207, exercise 1.2.3-26].
2.29   1979 midterm.
2.30   1973 midterm.
2.31   Stieltjes [342].
2.34   Riemann [309, §3].
2.35   Euler [106] gave a fallacious "proof" using divergent series.
2.36   Golomb [150]; Vardi [358].
2.37   Leo Moser.*
3.6     Ernst Mayr, 1982 homework.
3.8     Dirichlet [80].
3.9     Chace [54]; Fibonacci [122, pp. 77-83].
3.12   [207, exercise 1.2.4-48(a)].
3.13   Beatty [22]; Niven [281, theorem 3.7].
3.19   [207, exercise 1.2.4-34].
3.21   1975 midterm.
3.23   [207, exercise 1.2.4-41].
3.28   Brown [45].
3.30   Aho and Sloane [4].
3.31   Greitzer [165, problem 1972/3, solution 2].
3.32   [160].
3.33   1984 midterm.
3.34   1970 midterm.
3.35   1975 midterm.
3.36   1976 midterm.
3.37   1986 midterm; [215].
3.38   1974 midterm.
3.39   1971 midterm.
3.40   1980 midterm.
3.41   Klamkin [203, problem 1978/3].
3.42   Uspensky [355].
3.45   Aho and Sloane [4].
3.46   Graham and Pollak [162].
3.48   Håland and Knuth [170].
3.49   R. L. Graham and D. R. Hofstadter.*
3.52   Fraenkel [128].
3.53   S. K. Stein.*
4.4     [214, §526].
4.16   Sylvester [346].
4.19   [212, pp. 148-149].
4.20   Bertrand [27, p. 129]; Chebyshev [56]; Wright [379].
4.22   Brillhart [39]; Williams and Dubner [375]; Dubner [86].
4.23   Crowe [68].
4.24   Legendre [241, second edition, introduction].
4.26   [208, exercise 4.5.3-43].
4.31   Pascal [284].
4.36   Hardy and Wright [181, §14.5].
4.37   Aho and Sloane [4].
4.38   Lucas [257].
4.39   [159].
4.40   Stickelberger [340].
4.41   Legendre [241, §135]; Hardy and Wright [181, theorem 82].
4.42   [208, exercise 4.5.1-6].
4.44   [208, exercise 4.5.3-39].
4.45   [208, exercise 4.3.2-13].
4.47   Lehmer [242].
4.48   Gauss [142, §78]; Crelle [67].
4.52   1974 midterm.
4.53   1973 midterm, inspired by Rao [303].
4.54   1974 midterm.
4.56   Logan [252, eq. (6.15)].
4.57   A special case appears in [216].
4.58   Sierpiński [327].
4.59   Curtiss [70]; Erdős [93].
4.60   Mills [272].
4.61   [207, exercise 1.3.2-19].
4.63   Barlow [21]; Abel [1].
4.64   Peirce [287].
4.66   Ribenboim [308]; Sierpiński [328, problem P210].
4.67   [157].
4.69   Cramér [66].
4.70   Paul Erdős.*
4.71   [95, p. 96].
4.72   [95, p. 103].
4.73   Landau [239, volume 2, eq. 648].
5.1     Forcadel [126].
5.3     Long and Hoggatt [254].
5.5     1983 in-class final.
5.13   1975 midterm.
5.14   [207, exercise 1.2.6-20].
5.15   Dixon [81].
5.21   Euler [99].
5.25   Gauss [143, §7].
5.28   Euler [118].
5.29   Kummer [229, eq. 26.4].
5.31   Gosper [154].
5.34   Bailey [18, §10.4].
5.36   Kummer [230, p. 116].
5.37   Vandermonde [357].
5.38   [207, exercise 1.2.6-56].
5.40   Rødseth [311].
5.43   Pfaff [292]; [207, exercise 1.2.6-31].
5.48   Ranjan Roy.*
5.49   Roy [314, eq. 3.13].
5.53   Gauss [143]; Richard Askey.*
5.58   Frazer and McKellar [133].
5.59   Stanford Computer Science Comprehensive Exam, Winter 1987.
5.60   [207, exercise 1.2.6-41].
5.61   Lucas [258].
5.62   1971 midterm.
5.63   1974 midterm.
5.64   1980 midterm.
5.65   1983 midterm.
5.66   1984 midterm.
5.67   1976 midterm.
5.68   1985 midterm.
5.69   Lyle Ramshaw, guest lecture in 1986.
5.70   Andrews [9, theorem 5.4].
5.71   Wilf [373, exercise 4.16].
5.72   Hermite [185].
5.74   1979 midterm.
5.75   1971 midterm.
5.76   [207, exercise 1.2.6-59 (corrected)].
5.77   1986 midterm.
5.78   [210].
5.79   Mendelsohn [268]; Montgomery [276].
5.81   1986 final exam; [219].
5.82   Hillman and Hoggatt [188].
5.85   Hsu [190].
5.86   Good [153].
5.88   Hermite [186].
5.91   Whipple [369].
5.92   Clausen [60], [61].
5.93   Gosper [154].
5.95   Petkovšek [291, Corollary 3.1].
5.96   Petkovšek [291, Corollary 5.1].
5.98   Ira Gessel.*
5.100 Staver [336′].
5.102 H. S. Wilf.*
5.104 Volker Strehl.*
5.105 Henrici [183, p. 118].
5.108 Apéry [14].
5.109 Gessel [146].
5.110 R. William Gosper, Jr.*
5.111 [95, p. 71].
5.112 [95, p. 71].
5.113 Wilf and Zeilberger [374].
5.114 Strehl [344] credits A. Schmidt.
6.6     Fibonacci [122, p. 283].
6.15   [209, exercise 5.1.3-2].
6.21   Theisinger [350].
6.25   Gardner [138] credits Denys Wilquin.
6.27   Lucas [257].
6.28   Lucas [259, chapter 18].
6.31   Lah [235]; R. W. Floyd.*
6.35   1977 midterm.
6.37   Shallit [324].
6.39   [207, exercise 1.2.7-15].
6.40   Klamkin [203, problem 1979/1].
6.41   1973 midterm.
6.43   Brooke and Wall [41].
6.44   Wasteels [361′].
6.46   Francesca [131]; Wallis [360, chapter 4].
6.47   Lucas [257].
6.48   [208, exercise 4.5.3-9(c)].
6.49   Davison [73].
6.50   1985 midterm; Rham [307]; Dijkstra [79, pp. 230-232].
6.51   Waring [361]; Lagrange [233]; Wolstenholme [376].
6.52   Eswarathasan and Levine [97].
6.53   Kaucký [200] treats a special case.
6.54   Staudt [336]; Clausen [62]; Rado [300].
6.55   Andrews and Uchimura [13].
6.56   1986 midterm.
6.57   1984 midterm, suggested by R. W. Floyd.*
6.58   [207, exercise 1.2.8-30]; 1982 midterm.
6.59   Burr [47].
6.61   1976 final exam.
6.62   Borwein and Borwein [36, §3.7].
6.63   [207, section 1.2.10]; Stanley [335, proposition 1.3.12].
6.65   Tanny [349].
6.66   [209, exercise 5.1.3-3].
6.67   Chung and Graham [59].
6.68   Logan [253].
6.69   [209, exercise 6.1-13].
6.72   Euler [110, part 2, chapter 8].
6.73   Euler [108, chapters 9 and 10]; Schröter [321].
6.75   Atkinson [16].
6.76   [209, answer 5.1.3-3]; Lengyel [248].
6.78   Logan [253].
6.79   Comic section, Boston Herald, August 21, 1904.
6.80   Silverman and Dunn [329].
6.82   [217].
6.83   [156], modulo a numerical error.
6.85   Burr [47].
6.86   [226].
6.87   [208, exercises 4.5.3-2 and 3].
6.88   Adams and Davison [3].
6.90   Lehmer [243].
6.92   Part (a) is from Eswarathasan and Levine [97].
7.2     [207, exercise 1.2.9-1].
7.8     Zave [380].
7.9     [207, exercise 1.2.7-22].
7.11   1971 final exam.
7.12   [209, pp. 63-64].
7.13   Raney [302].
7.15   Bell [24].
7.16   Pólya [296, p. 149]; [207, exercise 2.3.4.4-1].
7.19   [221].
7.20   Jungen [198, p. 299] credits A. Hurwitz.
7.22   Pólya [298].
7.23   1983 homework.
7.24   Myers [279]; Sedláček [323].
7.25   [208, Carlitz's proof of lemma 3.3.3B].
7.26   [207, exercise 1.2.8-12].
7.32   [95, pp. 25-26] credits L. Mirsky and M. Newman.
7.33   1971 final exam.
7.34   Tomás Feder.*
7.36   1974 final exam.
7.37   Euler [109, §50]; 1971 final exam.
7.38   Carlitz [49].
7.39   [207, exercise 1.2.9-18].
7.41   André [8]; [209, exercise 5.1.4-22].
7.42   1974 final exam.
7.44   Gross [166]; [209, exercise 5.3.1-3].
7.45   de Bruijn [75].
7.47   Waugh and Maxfield [363].
7.48   1984 final exam.
7.49   Waterhouse [362].
7.50   Schröder [320]; [207, exercise 2.3.4.4-31].
7.51   Fisher [124]; Percus [290, pp. 89-123]; Stanley [334].
7.52   Hammersley [177].
7.53   Euler [114, part 2, section 2, chapter 6, §91].
7.54   Moessner [274].
7.55   Stanley [333].
7.56   Euler [113].
7.57   [95, p. 48] credits P. Erdős and P. Turán.
8.13   Thomas M. Cover.*
8.15   [207, exercise 1.2.10-17].
8.17   Patil [286].
8.24   John Knuth (age 4) and DEK; 1975 final.
8.26   [207, exercise 1.3.3-18].
8.27   Fisher [125].
8.29   Guibas and Odlyzko [168].
8.32   1977 final exam.
8.34   Hardy [180] has an incorrect analysis leading to the opposite conclusion.
8.35   1981 final exam.
8.36   Gardner [139] credits George Sicherman.
8.38   [208, exercise 3.3.2-10].
8.39   [211, exercise 4.3(a)].
8.41   Feller [120, exercise IX.33].
8.43   [207, sections 1.2.10 and 1.3.3].
8.44   1984 final exam.
8.45   1985 final exam.
8.46   Feller [120] credits Hugo Steinhaus.
8.47   1974 final, suggested by "fringe analysis" of 2-3 trees.
8.48   1979 final exam.
8.49   Blom [32]; 1984 final exam.
8.50   1986 final exam.
8.51   1986 final exam.
8.53   Feller [120] credits S. N. Bernstein.
8.57   Lyle Ramshaw.*
8.58   Guibas and Odlyzko [168].
9.1     Hardy [179, 1.3(g)].
9.2     Part (c) is from Garfunkel [140].
9.3     [207, exercise 1.2.11.1-6].
9.6     [207, exercise 1.2.11.1-3].
9.8     Hardy [179, 1.2(iv)].
9.9     Landau [238, vol. 1, p. 60].
9.14   [207, exercise 1.2.11.3-6].
9.16   Knopp [205, edition ≥ 2, §64C].
9.18   Bender [25, §3.1].
9.20   1971 final exam.
9.24   [164, §4.1.6].
9.27   Titchmarsh [352].
9.28   Glaisher [149].
9.29   de Bruijn [74, §3.7].
9.32   1976 final exam.
9.34   1973 final exam.
9.35   1975 final exam.
9.36   1980 class notes.
9.37   [208, eq. 4.5.3-21].
9.38   1977 final exam.
9.39   1975 final exam, inspired by Reich [306].
9.40   1977 final exam.
9.41   1980 final exam.
9.42   1979 final exam.
9.44   Tricomi and Erdélyi [353].
9.46   de Bruijn [74, §6.3].
9.47   1980 homework; [209, eq. 5.3.1-34].
9.48   1980 final exam.
9.49   1974 final exam.
9.50   1984 final exam.
9.51   [164, §4.2.1].
9.52   Poincaré [294]; Borel [35, p. 27].
9.53   Pólya/Szegő [299, part 1, prob. 140].
9.57   Andrew M. Odlyzko.*
9.58   Henrici [182, exercise 4.9.8].
9.60   [225].
9.62   Canfield [48].
9.63   Vardi [358].
9.65   Comtet [64, chapter 5, exercise 24].
9.66   M. P. Schützenberger.*
9.67   Lieb [250]; Stanley [335, exercise 4.37(c)].
9.68   Boas and Wrench [33].
* Unpublished personal communication.









Index
When an index entry refers to a page containing a relevant exercise, the answer to that exercise (in Appendix A) might divulge further information; an answer page is not indexed here unless it refers to a topic that isn't included in the statement of the relevant exercise. Some notations not indexed here (like xn, x, and ) are listed on pages x and xi, just before the table of contents.
(Graffiti have been indexed too.)
00, 162
 (≈ 1.41421), 100
 (≈ 1.73205), 378
ℑ: imaginary part, 64
: logarithmico-exponential functions, 442-443
ℜ: real part, 64, 212, 451
γ (≈ 0.57722), see Euler's constant
Γ, see Gamma function
δ, 47-56
Δ: difference operator, 47-55, 241, 470-471
p(n): largest power of p dividing n, 112-114, 146
ζ, see zeta function
ϑ, 219-221, 310, 347
Θ: Big Theta notation, 448
κm, see cumulants
μ, see Möbius function
ν, see nu function
π (≈ 3.14159), 26, 70, 146, 244, 485, 564, 596
π(x), see pi function
σ: standard deviation, 388; see also Stirling's constant
σn(x), see Stirling polynomials
ϕ (≈ 1.61803): golden ratio, 70, 97, 299-301, 310, 553
φ, see phi function
Φ: sum of φ, 138-139, 462-463
Ω: Big Omega notation, 448
∑-notation, 22-25, 245
∏-notation, 64, 106
∧-notation, 65
⇔: if and only if, 68
⇒: implies, 71
\: divides, 102
\\: exactly divides, 146
⊥: is relatively prime to, 115
≺: grows slower than, 440-443
≻: grows faster than, 440-443
≍: grows as fast as, 442-443
∽: is asymptotic to, 8, 110, 439-443, 448-449
≈: approximates, 23
≡: is congruent to, 123-126
#: cardinality, 39
!: factorial, 111-115
¡: subfactorial, 194-200
..: interval notation, 73-74
...: ellipsis, 21, 50, 108, ...
Aaronson, Bette Jane, ix
Abel, Niels Henrik, 604, 634
Abramowitz, Milton, 42, 604
absolute convergence, 60-62, 64
absolute error, 452, 455
absolute value of complex number, 64
absorption identities, 157-158, 261
Acton, John Emerich Edward Dalberg, Baron, 66
Adams, William Wells, 604, 635
Addison-Wesley, ix
addition formula for , 158-159
analog for , 268
analogs for  and , 259, 261
dual, 530
Aho, Alfred Vaino, 604, 633
Ahrens, Wilhelm Ernst Martin Georg, 8, 604
Akhiezer, Naum Il'ich, 604
Alfred [Brousseau], Brother Ulbertus, 607, 633
algebraic integers, 106, 147
algorithms,
analysis of, 138, 413-426
divide and conquer, 79
Euclid's, 103-104, 123, 303-304
Fibonacci's, 95, 101
Gosper's, 224-227
Gosper-Zeilberger, 229-241, 254-255, 319, 547
greedy, 101, 295
self-certifying, 104
Alice, 31, 408-410, 427, 430
Allardice, Robert Edgar, 2, 604
ambiguous notation, 245
American Mathematical Society, viii
AMS Euler, ix, 657
analysis of algorithms, 138, 413-426
analytic functions, 196
ancestor, 117, 291
André, Antoine Désiré, 604, 635
Andrews, George W. Eyre, 215, 330, 530, 575, 605, 634, 635
answers, notes on, 497, 637, viii
anti-derivative operator, 48, 470-471
anti-difference operator, 48, 54, 470-471
Apéry, Roger, 238, 605, 630, 634
numbers, 238-239, 255
approximation, see asymptotics
of sums by integrals, 45, 276-277, 469-475
Archibald, Raymond Clare, 608
Archimedes of Syracuse, 6
argument of hypergeometric, 205
arithmetic progression, 30, 376
floored, 89-94
sum of, 6, 26, 30-31
Armageddon, 85
Armstrong, Daniel Louis (= Satchmo), 80
art and science, 234
ascents, 267-268, 270
Askey, Richard Allen, 634
associative law, 30, 61, 64
asymptotics, 439-496
from convergent series, 451
of Bernoulli numbers, 286, 452
of binomial coefficients, 248, 251, 495, 598
of discrepancies, 492, 495
of factorials, 112, 452, 481-482, 491
of harmonic numbers, 276-278, 452, 480-481, 491
of hashing, 426
of nth prime, 110-111, 456-457, 490
of Stirling numbers, 495, 602
of sums, using Euler's summation formula, 469-489
of sums, using tail-exchange, 466-469, 486-489
of sums of powers, 491
of wheel winners, 76, 453-454
table of expansions, 452
usefulness of, 76, 439
Atkinson, Michael David, 605, 633, 635
Austin, Alan Keith, 607
automaton, 405
automorphic numbers, 520
average, 384
of a reciprocal, 432
variance, 423-425
Bn, see Bernoulli numbers
Bachmann, Paul Gustav Heinrich, 443, 462, 605
Bailey, Wilfrid Norman, 223, 548, 605, 634
Balasubramanian, Ramachandran, 525, 605.
Ball, Walter William Rouse, 605, 633
ballot problem, 362
Banach, Stefan, 433
Barlow, Peter, 605, 634
Barton, David Elliott, 602, 609
base term, 240
baseball, 73, 148, 195, 519, 622, 648, 653
BASIC, 173, 446
basic fractions, 134, 138
basis of induction, 3, 10-11, 320-321
Bateman, Harry, 626
Baum, Lyman Frank, 581
Beatty, Samuel, 605, 633
bee trees, 291
Beeton, Barbara Ann Neuhaus Friend Smith, viii
Bell, Eric Temple, 332, 606, 635
numbers, 373, 493, 603
Bender, Edward Anton, 606, 636
Bernoulli, Daniel, 299
Bernoulli, Jakob (= Jacobi = Jacques = James), 283, 470, 606
numbers, see Bernoulli numbers
polynomials, 367-368, 470-475
polynomials, graphs of, 473
trials, 402; see also coins, flipping
Bernoulli, Johann (= Jean), 622
Bernoulli numbers, 283-290
asymptotics of, 286, 452
calculation of, 288, 620
denominators of, 315, 551, 574
generalized, see Stirling polynomials
generating function for, 285, 351, 365
numerators of, 555
relation to tangent numbers, 287
table of, 284, 620
Bernshten (= Bernstein), Serge Natanovich, 636
Bertrand, Joseph Louis François, 145, 606, 633
postulate, 145, 500, 550
Bessel, Friedrich Wilhelm, functions, 206, 527
Beyer, William Hyman, 606
biased coin, 401
bicycles, 260, 500
Bieberbach, Ludwig, 617
Bienaymé, Irénée Jules, 606
Big Ell notation, 444
Big Oh notation, 76, 443-449
Big Omega notation, 448
Big Theta notation, 448
bijection, 39
Bill, 408-410, 427, 430
binary logarithm, 70, 449
binary notation (radix 2), 11-13, 15-16, 70, 113-114
binary partitions, 377
binary search, 121, 183
binary trees, 117
Binet, Jacques Philippe Marie, 299, 303, 606, 633
binomial coefficients, 153-242
addition formula, 158-159
asymptotics of, 248, 251, 495, 598
combinatorial interpretation, 153, 158, 160, 169-170
definition, 154, 211
dual, 530
fractional, 250
generalized, 211, 318, 530
indices of, 154
middle, 187, 255-256, 495
reciprocal of, 188-189, 246, 254
top ten identities of, 174
wraparound, 250 (exercise 75), 315
binomial convolution, 365, 367
binomial distribution, 401-402, 415, 428, 432
negative, 402-403, 428
binomial series, generalized, 200-204, 243, 252, 363
binomial theorem, 162-163
as hypergeometric series, 206, 221
discovered mechanically, 230-233
for factorial powers, 245
special cases, 163, 199
Blom, Carl Gunnar, 606, 636
bloopergeometric series, 243
Boas, Ralph Philip, Jr., 600, 606, 636, viii
Boggs, Wade Anthony, 195
Bohl, Piers Paul Felix (= Bol', Pirs Georgievich), 87, 606
Böhmer, Paul Eugen, 604
Bois-Reymond, Paul David Gustav du, 440, 610, 617
Boncompagni, Prince Baldassarre, 613
bootstrapping, 463-466
to estimate nth prime, 456-457
Borchardt, Carl Wilhelm, 617
Borel, Émile Félix Édouard Justin, 606, 636
Borwein, Jonathan Michael, 606, 635
Borwein, Peter Benjamin, 606, 635
bound variables, 22
boundary conditions on sums,
can be difficult, 75, 86
made easier, 24-25, 159
bowling, 6
box principle, 95, 130, 512
Boyd, David William, 564
bracket notation,
for coefficients, 197, 331
for true/false values, 24-25
Brahma, Tower of, 1, 4, 278
Branges, Louis de, 617
Brent, Richard Peirce, 306, 525, 564, 606
bricks, 313, 374
Brillhart, John David, 607, 633
Brocot, Achille, 116, 607
Broder, Andrei Zary, 632, ix
Brooke, Maxey, 607, 635
Brousseau, Brother Alfred, 607, 633
Brown, Mark Robbin, 632
Brown, Morton, 501, 607
Brown, Roy Howard, ix
Brown, Thomas Craig, 607, 633
Brown, Trivial, 607
Brown, William Gordon, 607
Brown University, ix
Browning, Elizabeth Barrett, 320
Bruijn, Nicolaas Govert de, 444, 447, 500, 609, 635, 636
cycle, 500
bubblesort, 448
Buckholtz, Thomas Joel, 620
Bulwer-Lytton, Edward George Earle Lytton, Baron, v
Burma-Shave, 541
Burr, Stefan Andrus, 607, 635
calculators, 67, 77, 459
failure of, 344
calculus, vi, 33
finite and infinite, 47-56
candy, 36
Canfield, Earl Rodney, 602, 607, 636
cards,
shuffling, 437
stacking, 273-274, 278, 309
Carlitz, Leonard, 607, 635
Carroll, Lewis (= Dodgson, Rev. Charles Lutwidge), 31, 293, 607, 608, 630
carries,
across the decimal point, 70
in divisibility of , 245, 536
in Fibonacci number system, 297, 561
Cassini, Gian (= Giovanni = Jean) Domenico (= Dominique), 292, 607
identity, 292-293, 300
identity, converse, 314
identity, generalized, 303, 310
Catalan, Eugène Charles, 203, 361, 607
Catalan numbers, 203
combinatorial interpretations, 358-360, 565, 568
generalized, 361
in sums, 181, 203, 317
table of, 203
Cauchy, Augustin Louis, 607, 633
Čech, Eduard, vi
ceiling function, 67-69
converted to floor, 68, 96
graph of, 68
center of gravity, 273-274, 309
certificate of correctness, 104
Chace, Arnold Buffum, 608, 633
Chaimovich, Mark, 608
chain rule, 54, 483
change, 327-330, 374
large amounts of, 344-346, 492
changing the index of summation, 30-31, 39
changing the tails of a sum, 466-469, 486-489
cheating, viii, 195, 388, 401
not, 158, 323
Chebyshev, Pafnuti L'vovich, 38, 145, 608, 633
inequality, 390-391, 428, 430
monotonic inequalities, 38, 576
cheese slicing, 19
Chen, Pang-Chieh, 632
Chinese Remainder Theorem, 126, 146
Chu Shih-Chieh (= Zhū Shìjié), 169
Chung, Fan-Rong King, ix, 608, 635
Clausen, Thomas, 608, 634, 635
product identities, 253
clearly, clarified, 417-418, 581
clichés, 166, 324, 357
closed form, 3, 7, 321, 331
for generating functions, 317
not, 108, 573
pretty good, 346
closed interval, 73-74
Cobb, Tyrus Raymond, 195
coefficient extraction, 197, 331
Cohen, Henri José, 238
coins, 327-330
biased, 401
fair, 401, 430
flipping, 401-410, 430-432, 437-438
spinning, 401
Collingwood, Stuart Dodgson, 608
Collins, John, 624
Colombo, Cristoforo (= Columbus, Christopher), 74
coloring, 496
Columbia University, ix
combinations, 153
combinatorial number system, 245
common logarithm, 449
commutative law, 30, 61, 64
failure of, 322, 502, 551
relaxed, 31
complete graph, 368
complex factorial powers, 211
complex numbers, 64
roots of unity, 149, 204, 375, 553, 574, 598
composite numbers, 105, 518
composition of generating functions, 428
computer algebra, 42, 254, 501, 539
Comtet, Louis, 609, 636
Concrete Math Club, 74, 453
concrete mathematics, defined, vi
conditional convergence, 59
conditional probability, 416-419, 424-425
confluent hypergeometric series, 206, 245
congruences, 123-126
Connection Machine, 131
contiguous hypergeometrics, 529
continuants, 301-309, 501
and matrices, 318-319
Euler's identity for, 303, 312
zero parameters in, 314
continued fractions, 301, 304-309, 319
large partial quotients of, 553, 563, 564, 602
convergence,
absolute, 60-62, 64
conditional, 59
of power series, 206, 331-332, 348, 451, 532
convex regions, 5, 20, 497
convolution, 197, 246, 333, 353-364
binomial, 365, 367
identities for, 202, 272, 373
polynomials, 373
Stirling, 272, 290
Vandermonde, see Vandermonde convolution
Conway, John Horton, 410, 609
cotangent function, 286, 317
counting,
combinations, 153
cycle arrangements, 259-262
derangements, 193-196, 199-200
integers in intervals, 73-74
necklaces, 139-141
parenthesized formulas, 357-359
permutations, 111
permutations by ascents, 267-268
permutations by cycles, 262
set partitions, 258-259
spanning trees, 348-350, 356, 368-369, 374
with generating functions, 320-330
coupon collecting, 583
Cover, Thomas Merrill, 636
Coxeter, Harold Scott MacDonald, 605
Cramér, Carl Harald, 525, 609, 634
Cray X-MP, 109
Crelle, August Leopold, 609, 633
cribbage, 65
Crispin, Mark Reed, 628
Crowe, Donald Warren, 609, 633
crudification, 447
Csirik, János András, 590, 609
cubes, sum of consecutive, 51, 63, 283, 289, 367
cumulants, 397-401
infinite, 576
of binomial distribution, 432
of discrete distribution, 438
of Poisson distribution, 428-429
third and fourth, 429, 579, 589
CUNY (= City University of New York), ix
Curtiss, David Raymond, 609, 634
cycles,
de Bruijn, 500
of beads, 139-140
of permutations, 259-262
cyclic shift, 12, 359, 362
cyclotomic polynomials, 149
D, see derivative operator
Dating Game, 506
David, Florence Nightingale, 602, 609
Davis, Philip Jacob, 609
Davison, John Leslie, 307, 604, 609, 635
de Branges, Louis, 617
de Bruijn, Nicolaas Govert, 444, 447, 500, 609, 635, 636
cycle, 500
de Finetti, Bruno, 24, 613
de Lagny, Thomas Fantet, 304, 621
de Moivre, Abraham, 297, 481, 609
Dedekind, Julius Wilhelm Richard, 136-137, 609
definite sums, analogous to definite integrals, 49-50
deg, 227, 232
degenerate hypergeometric series, 209-210, 216, 222, 247
derangements, 194-196, 250
generating function, 199-200
derivative operator, 47-49
converting between D and Δ, 470-471
converting between D and , 310
with generating functions, 33, 333, 364-365
with hypergeometric series, 219-221
descents, see ascents
dgf: Dirichlet generating function, 370
dice, 381-384
fair, 382, 417, 429
loaded, 382, 429, 431
nonstandard, 431
pgf for, 399-400
probability of doubles, 427
supposedly fair, 392
Dickson, Leonard Eugene, 510, 609
Dieudonné, Jean Alexandre, 523
difference operator, 47-55, 241
converting between D and Δ, 470-471
nth difference, 187-192, 280-281
nth difference of product, 571
differentiably finite power series, 374, 380
differential operators, see derivative operator, theta operator
difficulty measure for summation, 181
Dijkstra, Edsger Wybe, 173, 609, 635
dimers and dimes, 320, see dominoes and change
diphages, 434, 438
Dirichlet, Johann Peter Gustav Lejeune, 370, 610, 633
box principle, 95, 130, 512
generating functions, 370-371, 373, 432, 451
probability generating functions, 432
discrepancy, 88-89, 97
and continued fractions, 319, 492, 602
asymptotics of, 492, 495
discrete probability, 381-438
defined, 381
distribution,
of fractional parts, 87
of primes, 111
of probabilities, see probability distributions
of things into groups, 83-85
distributive law, 30, 35, 60, 64
for gcd and lcm, 145
for mod, 83
divergent sums, 57, 60
considered useful, 346-348, 451
illegitimate, 504, 532
divide and conquer, 79
divides exactly, 146
in binomial coefficients, 245
in factorials, 112-114, 146
divisibility, 102-105
by 3, 147
of polynomials, 225
Dixon, Alfred Cardew, 610, 634
formula, 214
DNA, Martian, 377
Dodgson, Charles Lutwidge, see Carroll
domino tilings, 320-327, 371, 379
ordered pairs of, 375
Dorothy Gale, 581
double generating functions, see super generating functions
double sums, 34-41, 246, 249
considered useful, 46, 183-185
faulty use of, 63, 65
infinite, 61
over divisors, 105
telescoping, 255
doubloons, 436-437
doubly exponential recurrences, 97, 100, 101, 109
doubly infinite sums, 59, 98, 482-483
Dougall, John, 171, 610
downward generalization, 2, 95, 320-321
Doyle, Sir Arthur Ignatius Conan, 162, 228-229, 405, 610
drones, 291
Drysdale, Robert Lewis (Scot), III, 632
du Bois-Reymond, Paul David Gustav, 440, 610, 617
duality, 69
between  and , 530
between factorial and Gamma functions, 211
between floors and ceilings, 68-69, 96
between gcd and lcm, 107
between rising and falling powers, 63
between Stirling numbers of different kinds, 267
Dubner, Harvey, 610, 631, 633
Dudeney, Henry Ernest, 610, 633
Dunkel, Otto, 614, 633
Dunn, Angela Fox, 628, 635
Dunnington, Guy Waldo, 610
duplication formulas, 186, 244
Dupré, Lyn Oppenheim, ix
Durst, Lincoln Kearney, viii
Dyson, Freeman John, 172, 239, 610, 615
e (≈ 2.718281828459045),
as canonical constant, 70, 596
representations of, 122, 150
en, see Euclid numbers
E: expected value, 385-386
E: shift operator, 55, 188, 191
En, see Euler numbers
Edwards, Anthony William Fairbank, 610
eeny-meeny-miny-mo, see Josephus problem
efficiency, different notions of, 24, 133
egf: exponential generating function, 364
eggs, 158
Egyptian mathematics, 95, 150
bibliography of, 608
Einstein, Albert, 72, 307
Eisele, Carolyn, 625
Eisenstein, Ferdinand Gotthold Max, 202, 610
Ekhad, Shalosh B, 546
elementary events, 381-382
Elkies, Noam David, 131, 610
ellipsis (···), 21
advantage of, 21, 25, 50
disadvantage of, 25
elimination of, 108
empirical estimates, 391-393, 427
empty case,
for spanning trees, 349, 565
for Stirling numbers, 258
for tilings, 320-321
for Tower of Hanoi, 2
empty product, 48, 106, 111
empty sum, 24, 48
entier function, see floor function
equality, one-way, 446-447, 489-490
equivalence relation, 124
Eratosthenes, sieve of, 111
Erdélyi, Arthur, 629, 636
Erdős, Pál (= Paul), 418, 525, 548, 575, 610-611, 634, 636
error function, 166
errors, absolute versus relative, 452, 455
errors, locating our own, 183
Eswarathasan, Arulappah, 611, 635
Euclid (= ), 107-108, 147, 611
algorithm, 103-104, 123, 303-304
numbers, 108-109, 145, 147, 150, 151
Euler, Leonhard, i, vii, ix, 48, 122, 132-134, 202, 205, 207, 210, 267, 277, 278, 286, 301-303, 469, 471, 513, 529, 551, 575, 603, 605, 609, 611-613, 629, 630, 633-636
constant (≈ 0.57722), 278, 306-307, 316, 319, 481, 596
disproved conjecture, 131
identity for continuants, 303, 312
identity for hypergeometrics, 244
numbers, 559, 570, 620; see also Eulerian numbers
polynomials, 574
pronunciation of name, 147
summation formula, 469-475
theorem, 133, 142, 147
totient function, see phi function
triangle, 268, 316
Eulerian numbers, 267-271, 310, 316, 378, 574
combinatorial interpretations, 267-268, 557
generalized, 313
generating function for, 351
second-order, 270-271
table of, 268
event, 382
eventually positive function, 442
exact cover, 376
exactly divides, 146
in binomial coefficients, 245
in factorials, 112-114, 146
excedances, 316
exercises, levels of, viii, 72-73, 95, 511
exp: exponential function, 455
expectation, see expected value
expected value, 385-387
using a pgf, 395
exponential function, discrete analog of, 54
exponential generating functions, 364-369, 421-422
exponential series, generalized, 200-202, 242, 364, 369
exponents, laws of, 52, 63
F, see hypergeometric series
Fn, see Fibonacci numbers
factorial expansion of binomial coefficients, 156, 211
factorial function, 111-115, 346-348
approximation to, see Stirling's approximation
duplication formula, 244
generalized to nonintegers, 192, 210-211, 213-214, 316
factorial powers, see falling factorial powers, rising factorial powers
factorization into primes, 106-107, 110
factorization of summation conditions, 36
fair coins, 401, 430
fair dice, 382, 386, 392, 417, 429
falling factorial powers, 47
binomial theorem for, 245
complex, 211
difference of, 48, 53, 188
negative, 52, 63, 188
related to ordinary powers, 51, 262-263, 598
related to rising powers, 63, 312
summation of, 50-53
fans, ix, 193, 348
Farey, John, series, 118-119, 617
consecutive elements of, 118-119, 150
distribution of, 152
enumeration of, 134, 137-139, 462-463
Fasenmyer, Mary Celine, 230, 631
Faulhaber, Johann, 288, 613, 620
Feder Bermann, Tomás, 635
Feigenbaum, Joan, 632
Feller, William, 381, 613, 636
Fermat, Pierre de, 130, 131, 613
numbers, 131-132, 145, 525
Fermat's Last Theorem, 130-131, 150, 524, 555
Fermat's theorem (= Fermat's Little Theorem), 131-133, 141-143, 149
converse of, 132, 148
Fibonacci, Leonardo, of Pisa (= Leonardo filio Bonacii Pisano), 95, 292, 549, 613, 633, 634
addition, 296-297, 318
algorithm, 95, 101
factorial, 492
multiplication, 561
number system, 296-297, 301, 307, 310, 318
odd and even, 307-308
Fibonacci numbers, 290-301, 575
and continuants, 302
and sunflowers, 291
closed forms for, 299-300, 331
combinatorial interpretations of, 291-292, 302, 321, 549
egf for, 570
ordinary generating functions for, 297-300, 337-340, 351
second-order, 375
table of, 290, 293
Fibonomial coefficients, 318, 556
Fine, Henry Burchard, 625
Fine, Nathan Jacob, 603
Finetti, Bruno de, 24, 613
finite calculus, 47-56
finite state language, 405
Finkel, Raphael Ari, 628
Fisher, Michael Ellis, 613, 636
Fisher, Sir Ronald Aylmer, 613, 636
fixed points, 12, 393-394
pgf for, 400-401, 428
Flajolet, Philippe Patrick Michel, 564
flipping coins, 401-410, 430-432, 437-438
floor function, 67-69
converted to ceiling, 68, 96
graph of, 68
Floyd, Robert W, 635
food, see candy, cheese, eggs, pizza, sherry
football, 182
football victory problem, 193-196, 199-200, 428
generalized, 429
mean and variance, 393-394, 400-401
Forcadel, Pierre, 613, 634
formal power series, 206, 331, 348, 532
FORTRAN, 446
Fourier, Jean Baptiste Joseph, 22, 613
series, 495
fractional parts, 70
in Euler's summation formula, 470
in polynomials, 100
related to mod, 83
uniformly distributed, 87
fractions, 116-123
basic, 134, 138
continued, 301, 304-309, 319, 564
partial, see partial fraction expansions
unit, 95, 101, 150
unreduced, 134-135, 151
Fraenkel, Aviezri S, 515, 563, 613-614, 633
Frame, James Sutherland, 614, 633
Francesca, Piero della, 614, 635
Franel, Jérome, 549, 614
Fraser, Alexander Yule, 2, 604
Frazer, William Donald, 614, 634
Fredman, Michael Lawrence, 513, 614
free variables, 22
Freman, Grigori Abelevich, 608
friendly monster, 545
frisbees, 434-435, 437
Frye, Roger Edward, 131
Fundamental Theorem of Algebra, 207
Fundamental Theorem of Arithmetic, 106-107
Fundamental Theorem of Calculus, 48
Fuss, Nicola Ivanovich, 361, 614
Fuss-Catalan numbers, 361
Fuss, Paul Heinrich von (= Fus, Pavel Nikolaeich), 611-612
Gale, Dorothy, 581
games, see bowling, cards, cribbage, dice, Penney ante, sports
Gamma function, 210-214, 609
duplication formula for, 528
Stirling's approximation for, 482
gaps between primes, 150-151, 525
Gardner, Martin, 614, 634, 636
Garfunkel, Jack, 614, 636
Gasper, George, Jr., 223, 614
Gauß (= Gauss), Johann Friderich Carl (= Carl Friedrich), vii, 6, 7, 123, 205, 207, 212, 501, 510, 529, 610, 615, 633, 634
hypergeometric series, 207
identity for hypergeometrics, 222, 247, 539
trick, 6, 30, 112, 313
gcd, 103, see greatest common divisor
generalization, 11, 13, 16
downward, 2, 95, 320-321
generalized binomial coefficients, 211, 318, 530
generalized binomial series, 200-204, 243, 252, 363
generalized exponential series, 200-202, 242, 364, 369
generalized factorial function, 192, 210-211, 213-214, 316
generalized harmonic numbers, 277, 283, 286, 370
generalized Stirling numbers, 271-272, 311, 316, 319, 598
generating functions, 196-204, 297-300, 320-380
composition of, 428
Dirichlet, 370-371, 373, 432, 451
exponential, 364-369, 421-422
for Bernoulli numbers, 285, 351, 365
for convolutions, 197, 333-334, 353-364, 369, 421
for Eulerian numbers, 351, 353
for Fibonacci numbers, 297-300, 337-340, 351, 570
for harmonic numbers, 351-352
for minima, 377
for probabilities, 394-401
for simple sequences, 335
for special numbers, 351-353
for spectra, 307, 319
for Stirling numbers, 351-352, 559
Newtonian, 378
of generating functions, 351, 353, 421
super, 353, 421
table of manipulations, 334
Genocchi, Angelo, 615
numbers, 551, 574
geometric progression, 32
floored, 114
generalized, 205-206
sum of, 32-33, 54
Gessel, Ira Martin, 270, 615, 634
Gibbs, Josiah Willard, 630
Gilbert, William Schwenck, 444
Ginsburg, Jekuthiel, 615
Glaisher, James Whitbread Lee, 615, 636
constant (≈ 1.28243), 595
God, 1, 307, 521
Goldbach, Christian, 611-612
theorem, 66
golden ratio, 299, see phi
golf, 431
Golomb, Solomon Wolf, 460, 507, 615, 633
digit-count sum, 460-462, 490 (exercise 22), 494
self-describing sequence, 66, 495, 630
Good, Irving John, 615, 634
Goodfellow, Geoffrey Scott, 628
Gopinath, Bhaskarpillai, 501, 621
Gordon, Peter Stuart, ix
Gosper, Ralph William, Jr., 224, 564, 615, 634
algorithm, 224-227
algorithm, examples, 227-229, 245, 247-248, 253-254, 530, 534
Gosper-Zeilberger algorithm, 229-241, 319
examples, 254-255, 547
summary, 233
goto, considered harmful, 173
Gottschalk, Walter Helbig, vii
graffiti, vii, ix, 59, 637
Graham, Cheryl, ix
Graham, Ronald Lewis, iii, iv, vi, ix, 102, 506, 605, 608-609, 611, 615-616, 629, 632, 633, 635
Grandi, Luigi Guido, 58, 616
Granville, Andrew James, 548
graph theory, see spanning trees
graphs of functions,
1/x, 276-277
e-x2/10, 483
Bernoulli polynomials, 473
floor and ceiling, 68
hyperbola, 440
partial sums of a sequence, 359-360
Graves, William Henson, 632
gravity, center of, 273-274, 309
Gray, Frank, code, 497
greatest common divisor, 92, 103-104, 107, 145
greatest integer function, see floor function
greatest lower bound, 65
greed, 74, 387-388; see also rewards
greedy algorithm, 101, 295
Green, Research Sink, 607
Greene, Daniel Hill, 616
Greitzer, Samuel Louis, 616, 633
Gross, Oliver Alfred, 616, 635
Grünbaum, Branko, 498, 616
Grundy, Patrick Michael, 627, 633
Guibas, Leonidas Ioannis (= Leo John), 590, 616, 632, 636
Guy, Richard Kenneth, 523, 525, 616
Hn, see harmonic numbers
Haar, Alfréd, vii
Hacker's Dictionary, 124, 628
Haiman, Mark, 632
Håland Knutson, Inger Johanne, 616, 633
half-open interval, 73-74
Hall, Marshall, Jr., 616
Halmos, Paul Richard, v, vi, 616-617
Halphen, Georges Henri, 305, 617
halving, 79, 186-187
Hamburger, Hans Ludwig, 591, 617
Hammersley, John Michael, v, 617, 636
Hanoi, Tower of, 1-4, 26-27, 109, 146
variations on, 17-20
Hansen, Eldon Robert, 42, 617
Hardy, Godfrey Harold, 111, 442-443, 617, 633, 636
harmonic numbers, 29, 272-282
analogous to logarithms, 53
asymptotics of, 276-278, 452, 480-481, 491
complex, 311, 316
divisibility of, 311, 314, 319
generalized, 277, 283, 286, 370
generating function for, 351-352
second-order, 277, 280, 311, 550-552
sums of, 41, 313, 316, 354-355
sums using summation by parts, 56, 279-282, 312
table of, 273
harmonic series, divergence of, 62, 275-276
Harry, Matthew Arnold, double sum, 249
hashing, 411-426, 430
hat-check problem, see football victory problem
hcf, 103, see greatest common divisor
Heath-Brown, David Rodney, 629
Heiberg, Johan Ludvig, 611
Heisenberg, Werner Karl, 481
Helmbold, David Paul, 632
Henrici, Peter Karl Eugen, 332, 545, 602, 617, 634, 636
Hermite, Charles, 538, 555, 617, 629, 634
herring, red, 497
Herstein, Israel Nathan, 8, 618
hexagon property, 155-156, 242, 251
highest common factor, see greatest common divisor
Hillman, Abraham P, 618, 634
Hoare, Sir Charles Antony Richard, 28, 73, 618, 620
Hofstadter, Douglas Richard, 633
Hoggatt, Verner Emil, Jr., 618, 623, 634
Holden, Edward Singleton, 625
Holmboe, Berndt Michael, 604
Holmes, Thomas Sherlock Scott, 162, 228-229
holomorphic functions, 196
homogeneous linear equations, 239, 543
horses, 17, 18, 468, 503
Hsu, Lee-Tsch (= Lietz = Leetch) Ching-Siur, 618, 634
Hurwitz, Adolf, 635
hyperbola, 440
hyperbolic functions, 285-286
hyperfactorial, 243, 491
hypergeometric series, 204-223
confluent, 206, 245
contiguous, 529
degenerate, 209-210, 216, 222, 247
differential equation for, 219-221
Gaussian, 207
partial sums of, 165-166, 223-230, 245
transformations of, 216-223, 247, 253
hypergeometric terms, 224, 243, 245, 527, 575
similar, 541
i, 22
implicit recurrences, 136-139, 193-195, 284
indefinite summation, 48-49
by parts, 54-56
of binomial coefficients, 161, 223-224, 246, 248, 313
of hypergeometric terms, 224-229
independent random variables, 384, 427
pairwise, 437
products of, 386
sums of, 386, 396-398
index set, 22, 30, 61
index variable, 22, 34, 60
induction, 3, 7, 10-11, 43
backwards, 18
basis of, 3, 320-321
failure of, 17, 575
important lesson about, 508, 549
inductive leap, 4, 43
infinite sums, 56-62, 64
doubly, 59, 98, 482-483
information retrieval, 411-413
INT function, 67
insurance agents, 391
integer part, 70
integration, 45-46, 48
by parts, 54, 472
of generating functions, 333, 365
interchanging the order of summation, 34-41, 105, 136, 183, 185, 546
interpolation, 191-192
intervals, 73-74
invariant relation, 117
inverse modulo m, 125, 132, 147
inversion formulas, 193
for binomial coefficients, 192-196
for Stirling numbers, 264, 310
for sums over divisors, 136-139
irrational numbers, 238
continued fraction representations, 306
rational approximations to, 122-123
spectra of, 77, 96, 514
Stern-Brocot representations, 122-123
Iverson, Kenneth Eugene, 24, 67, 618, 633
convention, 24-25, 31, 34, 68, 75
Jacobi, Carl Gustav Jacob, 64, 618
polynomials, 543, 605
Janson, Carl Svante, 618
Jarden, Dov, 556, 618
Jeopardy, 361
joint distribution, 384
Jonassen, Arne Tormod, 618
Jones, Bush, 618
Josephus, Flavius, 8, 12, 19-20, 618
numbers, 81, 97, 100
problem, 8-17, 79-81, 95, 100, 144
recurrence, generalized, 13-16, 79-81, 498
subset, 20
Jouaillec, Louis Maurice, 632
Jungen, Reinwald, 618, 635
K, see continuants
Kaplansky, Irving, 8, 568, 618
Karamata, Jovan, 257, 618
Karlin, Anna Rochelle, 632
Kaucký, Josef, 619, 635
Kauers, Manuel, 564
Keiper, Jerry Bruce, 619
Kellogg, Oliver Dimon, 609
Kent, Clark (= Kal-El), 372
Kepler, Johannes, 292, 619
kernel functions, 370
Ketcham, Henry King, 148
kilometers, 301, 310, 550
Kilroy, James Joseph, vii
Kipling, Joseph Rudyard, 260
Kissinger, Henry Alfred, 379
Klamkin, Murray Seymour, 619, 633, 635
Klarner, David Anthony, 632
knockout tournament, 432-433
Knoebel, Robert Arthur, 619
Knopp, Konrad, 619, 636
Knuth, Donald Ervin, iii-ix, 102, 267, 411, 506, 553, 616, 618-620, 632, 633, 636, 657
numbers, 78, 97, 100
Knuth, John Martin, 636
Knuth, Nancy Jill Carter, ix
Kramp, Christian, 111, 620
Kronecker, Leopold, 521
delta notation, 24
Kruk, John Martin, 519
Kummer, Ernst Eduard, 206, 529, 621, 634
formula for hypergeometrics, 213, 217, 535
Kurshan, Robert Paul, 501, 621
Ln, see Lucas numbers
Lagny, Thomas Fantet de, 304, 621
Lagrange (= de la Grange), Joseph Louis, comte, 470, 621, 635
identity, 64
Lah, Ivo, 621, 635
Lambert, Johann Heinrich, 201, 363, 613, 621
Landau, Edmund Georg Hermann, 443, 448, 622, 634, 636
Laplace, Pierre Simon, marquis de, 466, 606, 622
last but not least, 132, 469
Law of Large Numbers, 391
lcm, 103, see least common multiple
leading coefficient, 235
least common multiple, 103, 107, 145
of{1,...,n}, 251, 319, 500
least integer function, see ceiling function
least upper bound, 57, 61
LeChiffre, Mark Well, 148
left-to-right maxima, 316
Legendre, Adrien Marie, 622, 633
polynomials, 543, 573, 575
Lehmer, Derrick Henry, 526, 622, 633, 635
Leibniz, Gottfried Wilhelm, Freiherr von, vii, 168, 616, 622
Lekkerkerker, Cornelis Gerrit, 622
Lengyel, Tamás Lóránt, 622, 635
levels of problems, viii, 72-73, 95, 511
Levine, Eugene, 611, 635
lexicographic order, 441
lg: binary logarithm, 70, 449
L'Hospital, Guillaume François Antoine de, marquis de Sainte Mesme, rule, 340, 396, 542
L Shànlán (= Rénshū = Qiūrèn), 269, 622
Liang, Franklin Mark, 632
Lieb, Elliott Hershel, 622, 636
lies, and statistics, 195
Lincoln, Abraham, 401
linear difference operators, 240
lines in the plane, 4-8, 17, 19
Liouville, Joseph, 136-137, 622
little oh notation, 448
considered harmful, 448-449
Littlewood, John Edensor, 239
ln: natural logarithm, 276, 449
discrete analog of, 53-54
sum of, 481-482
log: common logarithm, 449
Logan, Benjamin Franklin (= Tex), Jr., 287, 622-623, 634-635
logarithmico-exponential functions, 442-443
logarithms, 449
binary, 70
discrete analog of, 53-54
in O-notation, 449
natural, 276
Long, Calvin Thomas, 623, 634
lottery, 387-388, 436-437
Lóu, Shìtuó, 623
lower index of binomial coefficient, 154
complex valued, 211
lower parameters of hypergeometric series, 205
Loyd, Samuel, 560, 623
Lucas, François Édouard Anatole, 1, 292, 623, 633-635
numbers, 312, 316, 556
Łuczak, Tomasz Jan, 618
Lyness, Robert Cranston, 501, 623
Maclaurin (= Mac Laurin), Colin, 469, 623
MacMahon, Maj. Percy Alexander, 140, 623
magic tricks, 293
Mallows, Colin Lingwood, 506
Markov, Andre Andreevich (the elder), processes, 405
Martian DNA, 377
Martzloff, Jean-Claude, 623
mathematical induction, 3, 7, 10-11, 43
backwards, 18
basis of, 3, 320-321
failure of, 17, 575
important lesson about, 508, 549
Mathews, Edwin Lee (= 41), 8, 21, 94, 105, 106, 343
 (=Matijasevich),  (=Yuri) Vladimirovich, 294, 623, 635
Mauldin, Richard Daniel, 611
Maxfield, Margaret Waugh, 630, 635
Mayr, Ernst, ix, 632, 633
McEliece, Robert James, 71
McGrath, James Patrick, 632
McKellar, Archie Charles, 614, 634
mean (average) of a probability distribution, 384-399
median, 384, 385, 437
mediant, 116
Melzak, Zdzislaw Alexander, vi, 623
Mendelsohn, Nathan Saul, 623, 634
Merchant, Arif Abdulhussein, 632
merging, 79, 175
Mersenne, Marin, 109-110, 131, 613, 624
numbers, 109-110, 151, 292
primes, 109-110, 127, 522-523
Mertens, Franz Carl Joseph, 23, 139, 624
miles, 301, 310, 550
Mills, Stella, 624
Mills, William Harold, 624, 634
minimum, 65, 249, 377
Minkowski, Hermann, 122
Mirsky, Leon, 635
mixture of probability distributions, 428
mnemonics, 74, 164
Möbius, August Ferdinand, 136, 138, 624
function, 136-139, 145, 149, 370-371, 462-463
mod: binary operation, 81-85
mod: congruence relation, 123-126
mod 0, 82-83, 515
mode, 384, 385, 437
modular arithmetic, 123-129
modulus, 82
Moessner, Alfred, 624, 636
Moivre, Abraham de, 297, 481, 609
moments, 398-399
Montgomery, Hugh Lowell, 463, 624
Montgomery, Peter Lawrence, 624, 634
Moriarty, James, 162
Morse, Samuel Finley Breese, code, 302-303, 324, 551
Moser, Leo, 624, 633
Motzkin, Theodor Samuel, 556, 564, 618, 624
mountain ranges, 359, 565
mu function, see Möbius function
multinomial coefficients, 168, 171-172, 569
recurrence for, 252
multinomial theorem, 149, 168
multiple of a number, 102
multiple sums, 34-41, 61; see also double sums
multiple-precision numbers, 127
multiplicative functions, 134-136, 144, 371
multisets, 77, 270
mumble function, 83, 84, 88, 507, 513
Murdock, Phoebe James, viii
Murphy's Law, 74
Myers, Basil Roland, 624, 635
name and conquer, 2, 32, 88, 139
National Science Foundation, ix
natural logarithm, 53-54, 276, 449, 481-482
Naval Research, ix
navel research, 299
nearest integer, 95
rounding to, 195, 300, 344, 491
unbiased, 507
necessary and sufficient conditions, 72
necklaces, 139-141, 259
negating the upper index, 164-165
negative binomial distribution, 402-403, 428
negative factorial powers, 52, 63, 188
Newman, James Roy, 631
Newman, Morris, 635
Newton, Sir Isaac, 189, 277, 624
series, 189-192
Newtonian generating function, 378
Niven, Ivan Morton, 332, 624, 633
nonprime numbers, 105, 518
nontransitive paradox, 410
normal distribution, 438
notation, x-xi, 2, 637
extension of, 49, 52, 154, 210-211, 266, 271, 311, 319
ghastly, 67, 175
need for new, 83, 115, 267
nu function: sum of digits,
binary (radix 2), 12, 114, 250, 525, 557
other radices, 146, 525, 552
null case, for spanning trees, 349, 565
for Stirling numbers, 258
for tilings, 320-321
for Tower of Hanoi, 2
number system, 107, 119
combinatorial, 245
Fibonacci, 296-297, 301, 307, 310, 318
prime-exponent, 107, 116
radix, see radix notation residue, 126-129, 144
Stern-Brocot, see Stern-Brocot number system
number theory, 102-152
o, considered harmful, 448-449
O-notation, 76, 443-449
abuse of, 447-448, 489
one-way equalities with, 446-447, 489-490
obvious, clarified, 417, 526
odds, 410
Odlyzko, Andrew Michael, 81, 564, 590, 616, 624, 636
Office of Naval Research, ix
one-way equalities, 446-447, 489-490
open interval, 73-74, 96
operators, 47
anti-derivative (∫), 48
anti-difference (∑), 48
derivative (D), 47, 310
difference (Δ), 47
equations of, 188, 191, 241, 310, 471
shift (E, K, N), 55, 240
theta (ϑ), 219, 310
optical illusions, 292, 293, 560
organ-pipe order, 524
Oz, Wizard of, 581
Pacioli, Luca, 614
Palais, Richard Sheldon, viii
paradoxes,
chessboard, 293, 317
coin flipping, 408-410
pair of boxes, 531, 535, 539
paradoxical sums, 57
parallel summation, 159, 174, 208-210
parentheses, 357-359
parenthesis conventions, xi
partial fraction expansions, 298-299, 338-341
for easy summation and differentiation, 64, 376, 476, 504, 586
not always easiest, 374
of , 189
of 1/(zn- 1), 558
powers of, 246, 376
partial quotients, 306
and discrepancies, 319, 598-599, 602
large, 553, 563, 564, 602
partial sums, see indefinite summation
required to be positive, 359-362
partition into nearly equal parts, 83-85
partitions, of the integers, 77-78, 96, 99, 101
of a number, 330, 377
of a set, 258-259, 373
Pascal, Blaise, 155, 156, 624-625, 633
Pascal's triangle, 155
extended upward, 164
hexagon property, 155-156, 242, 251
row lcms, 251
row products, 243
row sums, 163, 165-166
variant of, 250
Patashnik, Amy Markowitz, ix
Patashnik, Oren, iii, iv, vi, ix, 102, 506, 616, 632
Patil, Ganapati Parashuram, 625, 636
Paule, Peter, 537, 546
Peirce, Charles Santiago Sanders, 151, 525, 625, 634
Penney, Walter Francis, 408, 625
Penney ante, 408-410, 430, 437, 438
pentagon, 314 (exercise 46), 430, 434
pentagonal numbers, 380
Percus, Jerome Kenneth, 625, 636
perfect powers, 66
periodic recurrences, 20, 179, 498
permutations, 111-112
ascents in, 267-268, 270
cycles in, 259-262
excedances in, 316
fixed points in, 193-196, 393-394, 400-401, 428
left-to-right maxima in, 316
random, 393-394, 400-401, 428
up-down, 377
without fixed points, see derangements
personal computer, 109
perspiration, 234-235
perturbation method, 32-33, 43-44, 64, 179, 284-285
Petkovšek, Marko, 229, 575, 625, 634
Pfaff, Johann Friedrich, 207, 214, 217, 529, 625, 634
reflection law, 217, 244, 247, 539
pgf: probability generating function, 394
phages, 434, 438
phi (≈ 1.61803), 299-301
as canonical constant, 70
continued fraction for, 310
in fifth roots of unity, 553
in solutions to recurrences, 97, 99, 299-301
Stern-Brocot representation of, 550
phi function, 133-135
dgf for, 371
divisibility by, 151
Phi function: sum of φ, 138-139, 462-463
Phidias, 299
philosophy, vii, 11, 16, 46, 71, 72, 75, 91, 170, 181, 194, 331, 467, 503, 508, 603
phyllotaxis, 291
pi (≈ 3.14159), 26, 286
as canonical constant, 70, 416, 423
large partial quotients of, 564
Stern-Brocot representation of, 146
pi function, 110-111, 452, 593
preposterous expressions for, 516
Pig, Porky, 496
pigeonhole principle, 130
Pincherle, Salvatore, 617
Pisano, Leonardo filio Bonacii, 613, see Fibonacci
Pittel, Boris Gershon, 576, 618
pizza, 4, 423
planes, cutting, 19
Plouffe, Simon, 628
pneumathics, 164
Pochhammer, Leo, 48, 625
symbol, 48
pocket calculators, 67, 77, 459
failure of, 344
Poincaré, Jules Henri, 625, 636
Poisson, Siméon Denis, 471, 625
distribution, 428-429, 579
summation formula, 602
Pollak, Henry Otto, 616, 633
Pólya, George (= György), vi, 16, 327, 508, 625-626, 633, 635, 636
polygons, dissection of, 379
triangulation of, 374
Venn diagrams with, 20
polynomial argument, 158, 163
for rational functions, 527
opposite of, 210
polynomially recursive sequence, 374
polynomials, 189
Bernoulli, 367-368, 470-475
continuant, 301-309
convolution, 373
cyclotomic, 149
degree of, 158, 226
divisibility of, 225
Euler, 574
Jacobi, 543, 605
Legendre, 543, 573, 575
Newton series for, 189-191
reflected, 339
Stirling, 271-272, 290, 311, 317, 352
Poonen, Bjorn, 501, 633
Poorten, Alfred Jacobus van der, 630
Porter, Thomas K, 632
Portland cement, see concrete (in another book)
power series, 196, see generating functions formal, 206, 331, 348, 532
Pr, 381-382
Pratt, Vaughan Ronald, 632
preferential arrangements, 378 (exercise 44)
primality testing, 110, 148
impractical method, 133
prime algebraic integers, 106, 147
prime numbers, 105-111
gaps between, 150-151, 525
largest known, 109-110
Mersenne, 109-110, 127, 522-523
size of nth, 110-111, 456-457
sum of reciprocals, 22-25
prime to, 115
prime-exponent representation, 107, 116
Princeton University, ix, 427
probabilistic analysis of an algorithm, 413-426
probability, 195, 381-438
conditional, 416-419, 424-425
discrete, 381-438
generating functions, 394-401
spaces, 381
probability distributions, 381
binomial, 401-402, 415, 428, 432
composition or mixture of, 428
joint, 384
negative binomial, 402-403, 428
normal, 438
Poisson, 428-429, 579
uniform, 395-396, 418-421
problems, levels of, viii, 72-73, 95, 511
Prodinger, Helmut, 564
product notation, 64, 106
product of consecutive odd numbers, 186, 270
progression, see arithmetic progression, geometric progression
proof, 4, 7
proper terms, 239-241, 255-256
properties, 23, 34, 72-73
prove or disprove, 71-72
psi function, 551
pulling out the large part, 453, 458
puns, ix, 220
Pythagoras of Samos, theorem, 510
quadratic domain, 147
quicksort, 28-29, 54
quotation marks, xi
quotient, 81
rabbits, 310
radix notation, 11-13, 15-16, 109, 195, 526
length of, 70, 460
related to prime factors, 113-114, 146-148, 245
Rado, Richard, 625, 635
Rahman, Mizan, 223, 614
Rainville, Earl David, 529, 626
Ramanujan Iyengar, Srinivasa, 330
Ramaré, Olivier, 548
Ramshaw, Lyle Harold, 73, 632, 634, 636
random constant, 399
random variables, 383-386; see also independent random variables
Raney, George Neal, 359, 362, 626, 635
lemma, 359-360
lemma, generalized, 362, 372
sequences, 360-361
Rao, Dekkata Rameswar, 626, 633
rational functions, 207-208, 224-226, 338, 527
rational generating functions, 338-346
expansion theorems for, 340-341
Rayleigh, John William Strutt, 3rd Baron, 77, 626
Read, Ronald Cedric, 625
real part, 64, 212, 451
reciprocity law, 94
Recorde, Robert, 446, 626
recurrences, 1-20
and sums, 25-29
doubly exponential, 97, 100, 101, 109
floor/ceiling, 78-81
implicit, 136-139, 193-195, 284
periodic, 20, 179, 498
solving, 337-350
unfolding, 6, 100, 159-160, 312
unfolding asymptotically, 456
referee, 175
reference books, 42, 223, 616, 619
reflected light rays, 291-292
reflected polynomials, 339
reflection law for hypergeometrics, 217, 247, 539
regions, 4-8, 17, 19
regular expressions, 278
Reich, Simeon, 626, 636
relative error, 452, 455
relatively prime integers, 108, 115-123
remainder after division, 81-82
remainder in Euler's summation formula, 471, 474-475, 479-480
Rémy, Jean-Luc, 603
Renz, Peter Lewis, viii
repertoire method, 14-15, 19, 250
for Fibonacci-like recurrences, 312, 314, 372
for sums, 26, 44-45, 63
replicative function, 100
repunit primes, 516
residue calculus, 495
residue number system, 126-129, 144
retrieving information, 411-413
rewards, monetary, ix, 256, 497, 525, 575
Rham, Georges de, 626, 635
Ribenboim, Paulo, 555, 626, 634
Rice, Stephan Oswald, 626
Rice University, ix
Riemann, Georg Friedrich Bernhard, 205, 626, 633
hypothesis, 526
Riemann's zeta function, 65, 595
as generalized harmonic number, 277-278, 286
as infinite product, 371
as power series, 601
dgf's involving, 370-371, 373, 463, 566, 569
evaluated at integers, 238, 286, 571, 595, 597
rising factorial powers, 48
binomial theorem for, 245
complex, 211
negative, 63
related to falling powers, 63, 312
related to ordinary powers, 263, 598
Roberts, Samuel, 626, 633
rocky road, 36, 37
Rødseth, Øystein Johan, 627, 634
Rolletschek, Heinrich Franz, 514
roots of unity, 149, 204, 375, 574, 598
fifth, 553
modulo m, 128-129
Roscoe, Andrew William, 620
Rosser, John Barkley, 111, 627
Rota, Gian-Carlo, 516, 627
roulette wheel, 74-76, 453
rounding to nearest integer, 95, 195, 300, 344, 491
unbiased, 507
Roy, Ranjan, 627, 634
rubber band, 274-275, 278, 312, 493
ruler function, 113, 146, 148
running time, 413, 425-426
O-notation for, abused, 447-448
Ruzsa, Imre Zoltán, 611
Saalschütz, Louis, 214, 627
identity, 214-215, 234-235, 529, 531
Saltykov, Al'bert Ivanovich, 463, 627
sample mean and variance, 391-393, 427
sample third cumulant, 429
samplesort, 354
sandwiching, 157, 165
Sárközy, András, 548, 627
Sawyer, Walter Warwick, 207, 627
Schäffer, Alejandro Alberto, 632
Schinzel, Andrzej, 525
Schlömilch, Oscar Xaver, 627
Schmidt, Asmus Lorenzen, 634
Schoenfeld, Lowell, 111, 627
Schönheim, Johanen, 608
Schröder, Ernst, 627, 635
Schrödinger, Erwin, 430
Schröter, Heinrich Eduard, 627, 635
Schützenberger, Marcel Paul, 636
science and art, 234
Scorer, Richard Segar, 627, 633
searching a table, 411-413
Seaver, George Thomas (= 41), 8, 21, 94, 105, 106, 343
secant numbers, 317, 559, 570, 620
second-order Eulerian numbers, 270-271
second-order Fibonacci numbers, 375
second-order harmonic numbers, 277, 280, 311, 550-552
Sedgewick, Robert, 632
Sedláček, Jiří, 627, 635
Seidel, Philipp Ludwig von, 605
self-certifying algorithms, 104
self-describing sequence, 66, 495
self reference, 59, 95, 531-540, 616, 653
set inclusion in O-notation, 446-447, 490
Shallit, Jeffrey Outlaw, 627, 635
Sharkansky, Stefan Michael, 632
Sharp, Robert Thomas, 273, 627
sherry, 433
shift operator, 55, 240
binomial theorems for, 188, 191
Shiloach, Joseph (= Yossi), 632
Shor, Peter Williston, 633
Sicherman, George Leprechaun, 636
sideways addition, 12, 114, 146, 250, 552
Sierpiński, Wacław Franciszek, 87, 627-628, 634
sieve of Eratosthenes, 111
Sigma-notation, 22-25
ambiguity of, 245
signum function, 502
Silverman, David L, 628, 635
similar hypergeometric terms, 541
skepticism, 71
Skiena, Steven Sol, 548
Sloane, Neil James Alexander, 42, 341, 464, 604, 628, 633
Slowinski, David Allen, 109
small cases, 2, 5, 9, 155, 320-321; see also empty case
Smith, Cedric Austen Bardell, 627, 633
Snowwalker, Luke, 435
Solov'ev, Aleksandr Danilovitch, 408, 628
solution, 3, 337
sorting,
asymptotic efficiency of, 447-449
bubblesort, 448
merge sort, 79, 175
possible outcomes, 378
quicksort, 28-29, 54
samplesort, 354
Soundararajan, Kannan, 525, 605.
spanning trees,
of complete graphs, 368-369
of fans, 348-350, 356
of wheels, 374
Spec, see spectra
special numbers, 257-319
spectra, 77-78, 96, 97, 99, 101
generating functions for, 307, 319
spinning coins, 401
spiral function, 99
Spohn, William Gideon, Jr., 628
sports, see baseball, football, frisbees, golf, tennis
square pyramidal numbers, 42
square root,
of 1 (mod m), 128-129
of 2, 100
of 3, 378
of -1, 22
squarefree, 145, 151, 373, 525, 548
squares, sum of consecutive, 41-46, 51, 180, 245, 269, 284, 288, 367, 444, 470
stack size, 360-361
stacking bricks, 313, 374
stacking cards, 273-274, 278, 309
Stallman, Richard Matthew, 628
standard deviation, 388, 390-394
Stanford University, v, vii, ix, 427, 458, 632, 634, 657
Stanley, Richard Peter, 270, 534, 615, 628, 635, 636
Staudt, Karl Georg Christian von, 628, 635
Staver, Tor Bøhm, 628, 634
Steele, Guy Lewis, Jr., 628
Stegun, Irene Anne, 42, 604
Stein, Sherman Kopald, 633
Steiner, Jacob, 5, 628, 633
Steinhaus, Hugo Dyonizy, 636
Stengel, Charles Dillon (= Casey), 42
step functions, 87
Stern, Moritz Abraham, 116, 629
Stern-Brocot number system, 119-123
related to continued fractions, 306
representation of , 572
representation of γ, 306
representation of π, 146
representation of ϕ, 550
representation of e, 122, 150
simplest rational approximations from, 122-123, 146, 519
Stern-Brocot tree, 116-123, 148, 525
largest denominators in, 319
related to continued fractions, 305-306
Stern-Brocot wreath, 515
Stewart, Bonnie Madison, 614, 633
Stickelberger, Ludwig, 629, 633
Stieltjes, Thomas Jan, 617, 629, 633
constants, 595, 601
Stirling, James, 192, 195, 210, 257, 258, 297, 481, 629
approximation, 112, 452, 481-482, 491, 496
approximation, perturbed, 454-455
constant, 481, 485-489
polynomials, 271-272, 290, 311, 317, 352
triangles, 258, 259, 267
Stirling numbers, 257-267
as sums of products, 570
asymptotics of, 495, 602
combinatorial interpretations, 258-262
convolution formulas, 272, 290
duality of, 267
generalized, 271-272, 311, 316, 319, 598
generating functions for, 351-352, 559
identities for, 264-265, 269, 272, 290, 311, 317, 378
inversion formulas for, 310
of the first kind, 259
of the second kind, 258
related to Bernoulli numbers, 289-290, 317 (exercise 76)
table of, 258, 259, 267
Stone, Marshall Harvey, vi
Straus, Ernst Gabor, 564, 611, 624
Strehl, Karl Ernst Volker, 549, 629, 634
Stueben, Michael A., 445
subfactorial, 194-196, 250
summand, 22
summation, 21-66
asymptotic, 87-89, 466-496
by parts, 54-56, 63, 279
changing the index of, 30-31, 39
definite, 49-50, 229-241
difficulty measure for, 181
factors, 27-29, 64, 236, 248, 275, 543
in hypergeometric terms, 224-229
indefinite, see indefinite summation
infinite, 56-62, 64
interchanging the order of, 34-41, 105, 136, 183, 185, 546
mechanical, 229-241
on the upper index, 160-161, 175-176
over divisors, 104-105, 135-137, 141, 370
over triangular arrays, 36-41
parallel, 159, 174, 208-210
sums, 21-66; see also summation
absolutely convergent, 60-62, 64
and recurrences, 25-29
approximation of, by integrals, 45, 276-277, 469-475
divergent, see divergent sums
double, see double sums
doubly infinite, 59, 98, 482-483
empty, 24, 48
floor/ceiling, 86-94
formal, 321; see also formal power series
hypergeometric, see hypergeometric series
infinite, 56-62, 64
multiple, 34-41, 61; see also double sums
notations for, 21-25
of consecutive cubes, 51, 63, 283, 289, 367
of consecutive integers, 6, 44, 65
of consecutive mth powers, 42, 283-285, 288-290, 366-368
of consecutive squares, 41-46, 51, 180, 245, 269, 284, 288, 367, 444, 470
of harmonic numbers, 41, 56, 279-282, 312-313, 316, 354-355
paradoxical, 57
tails of, 466-469, 488-489, 492
Sun Tsŭ (= Sūnz, Master Sun), 126
sunflower, 291
super generating functions, 353, 421
superfactorials, 149, 243
Swanson, Ellen Esther, viii
Sweeney, Dura Warren, 629
Swinden, Benjamin Alfred, 633
Sylvester, James Joseph, 133, 629, 633
symmetry identities,
for binomial coefficients, 156-157, 183
for continuants, 303
for Eulerian numbers, 268
Szegedy, Márió, 525, 608, 629
Szegő, Gábor, 626, 636
Tn, see tangent numbers
tail exchange, 466-469, 486-489
tail inequalities, 428, 430
tail of a sum, 466-469, 488-489, 492
tale of a sum, see squares
Tancke, Joachim, 619
tangent function, 287, 317
tangent numbers, 287, 312, 317, 570, 620
Tanny, Stephen Michael, 629, 635
Tartaglia, Nicolò, triangle, 155
Taylor, Brook, series, 163, 191, 287, 396, 470-471
telescoping, 50, 232, 236, 255
tennis, 432-433
term, 21
hypergeometric, 224, 243, 245, 527, 575
term ratio, 207-209, 211-212, 224-225
TEX, 219, 432, 657
Thackeray, Henry St. John, 618
Theisinger, Ludwig, 629, 634
theory of numbers, 102-152
theory of probability, 381-438
theta functions, 483, 524
theta operator, 219-221, 347
converting between D and , 310
Thiele, Thorvald Nicolai, 397, 398, 629
thinking, 503
big, 2, 441, 458, 483, 486
not at all, 56, 230, 503
small, see downward generalization, small cases
three-dots (···) notation, 21
advantage of, 21, 25, 50
disadvantage of, 25
elimination of, 108
tilings, see domino tilings
Titchmarsh, Edward Charles, 629, 636
Todd, Horace, 501
Toledo, Ohio, 73
Tong, Christopher Hing, 632
Toscano, Letterio, 621
totient function, 133-135
dgf for, 371
divisibility by, 151
summation of, 137-144, 150, 462-463
Toto, 581
tournament, 432-433
Tower of Brahma, 1, 4, 278
Tower of Hanoi, 1-4, 26-27, 109, 146
variations on, 17-20
Trabb Pardo, Luis Isidoro, 632
transitive law, 124
failure of, 410
traps, 154, 157, 183, 222, 542
trees,
2-3 trees, 636
binary, 117
of bees, 291
spanning, 348-350, 356, 368-369, 374
Stern-Brocot, see Stern-Brocot tree
triangular array, summation over, 36-41
triangular numbers, 6, 155, 195-196, 260, 380
triangulation, 374
Tricomi, Francesco Giacomo Filippo, 629, 636
tridiagonal matrix, 319
trigonometric functions,
related to Bernoulli numbers, 286-287, 317
related to probabilities, 435, 437
related to tilings, 379
trinomial coefficients, 168, 171, 255, 571
middle, 490
trinomial theorem, 168
triphages, 434
trivial, clarified, 105, 129, 417-418, 618
Turán, Paul, 636
typefaces, viii-ix, 657
Uchimura, Keisuke, 605, 635
umop-apısdn function, 193
unbiased estimate, 392, 429
unbiased rounding, 507
uncertainty principle, 481
undetermined coefficients, 529
unexpected sum, 167, 215-216, 236, 247
unfolding a recurrence, 6, 100, 159-160, 312
asymptotically, 456
Ungar, Peter, 629
uniform distribution, 395-396, 418-421
uniformity, deviation from, 152; see also discrepancy
unique factorization, 106-107, 147
unit, 147
unit fractions, 95, 101, 150
unwinding a recurrence, see unfolding a recurrence
up-down permutations, 377
upper index of binomial coefficient, 154
upper negation, 164-165
upper parameters of hypergeometric series, 205
upper summation, 160-161, 176
useless identity, 223, 254
Uspensky, James Victor, 615, 630, 633
V: variance, 387-398, 419-425
van der Poorten, Alfred Jacobus, 630
Vandermonde, Alexandre Théophile, 169, 630, 634
Vandermonde's convolution, 169-170, 610, 627
as a hypergeometric series, 211-213
combinatorial interpretation, 169-170
derived mechanically, 234
derived from generating functions, 198
generalized, 201-202, 218-219, 248
with half-integers, 187
vanilla, 36
Vardi, Ilan, 525, 548, 603, 620, 630, 633, 636
variance of a probability distribution, 387-398, 419-425
infinite, 428, 587
Veech, William Austin, 514
Venn, John, 498, 630, 633
diagram, 17, 20
venture capitalists, 493-494
violin string, 29
vocabulary, 75
Voltaire, de (= Arouet, François Marie), 450
von Seidel, Philipp Ludwig, 605
von Staudt, Karl Georg Christian, 628, 635
Vyssotsky, Victor Alexander, 548
Wall, Charles Robert, 607, 635
Wallis, John, 630, 635
Wapner, Joseph Albert, 43
war, 8, 16, 85, 434
Waring, Edward, 630, 635
Wasteels, Joseph, 630, 635
Waterhouse, William Charles, 630, 635
Watson, John Hamish, 229, 405
Waugh, Frederick Vail, 630, 635
Weaver, Warren, 630
Weber, Heinrich, 630
Weisner, Louis, 516, 630
Wermuth, Edgar Martin Emil, 603, 630
Weyl, Claus Hugo Hermann, 87, 630
Wham-O, 435, 443
wheel, 74, 374
big, 75
of Fortune, 453
Whidden, Samuel Blackwell, viii
Whipple, Francis John Welsh, 630, 634
identity, 253
Whitehead, Alfred North, 91, 503, 603, 631
Wiles, Andrew John, 131
Wilf, Herbert Saul, 81, 240, 241, 514, 549, 575, 620, 624, 631, 634
Williams, Hugh Cowie, 631, 633
Wilquin, Denys, 634
Wilson, George and Martha, 148
Wilson, Sir John, theorem, 132-133, 148, 516, 609
wine, 433
Witty, Carl Roger, 509
Wolstenholme, Joseph, 631, 635
theorem, 554
Wood, Derick, 631, 633
Woods, Donald Roy, 628
Woolf, William Blauvelt, viii
worm,
and apple, 430
on rubber band, 274-275, 278, 312, 493
Worpitzky, Julius Daniel Theodor, 631
identity, 269
wraparound, 250 (exercise 75), 315
wreath, 515
Wrench, John William, Jr., 600, 606, 636
Wright, Sir Edward Maitland, 111, 617, 631, 633
Wythoff (= Wijthoff), Willem Abraham, 614
Yao, Andrew Chi-Chih, ix, 632
Yao, Frances Foong Chu, ix, 632
Yáo, Qí, 623
Youngman, Henry (= Henny), 175
zag, see zig
Zagier, Don Bernard, 238
Zapf, Hermann, viii, 620, 657
Zave, Derek Alan, 631, 635
Zeckendorf, Edouard, 631
theorem, 295-296, 563
Zeilberger, Doron, ix, 229-231, 238, 240, 241, 631, 634
zero, not considered harmful, 24-25, 159
strongly, 24-25
zeta function, 65, 595
and the Riemann hypothesis, 526
as generalized harmonic number, 277-278, 286
as infinite product, 371
as power series, 601
dgf's involving, 370-371, 373, 463, 566, 569
evaluated at integers, 238, 286, 571, 595, 597
Zhu Shijie, see Chu Shih-Chieh
zig, 7-8, 19
zig-zag, 19
Zipf, George Kingsley, law, 419









List of Tables
Sums and differences 55
Pascal's triangle 155
Pascal's triangle extended upward 164
Sums of products of binomial coefficients 169
The top ten binomial coefficient identities 174
General convolution identities 202
Stirling's triangle for subsets 258
Stirling's triangle for cycles 259
Basic Stirling number identities 264
Additional Stirling number identities 265
Stirling's triangles in tandem 267
Euler's triangle 268
Second-order Eulerian triangle 270
Stirling convolution formulas 272
Generating function manipulations 334
Simple sequences and their generating functions 335
Generating functions for special numbers 351
Asymptotic approximations 452
This book was composed at Stanford University using the TEX system for technical text developed by D. E. Knuth. The mathematics is set in a new typeface called AMS Euler (Version 2.1), designed by Hermann Zapf for the American Mathematical Society. The text is set in a new typeface called Concrete Roman and Italic, a special version of Knuth's Computer Modern family with weights designed to blend with AMS Euler. The paper is 50-lb.-basis Bright White Finch Opaque, which has a neutral pH and a life expectancy of several hundred years. The offset printing and notch binding were done by the Courier Corporation of Westford, Massachusetts.
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills — the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists — the authors themselves rely heavily on it! — but for serious users of mathematics in virtually every discipline.
Concrete mathematics is a blending of CONtinuous and disCRETE mathematics. "More concretely," the authors explain, "it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems." The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study.

Major topics include
Sums • Recurrences • Integer functions • Elementary number theory • Binomial coefficients • Generating functions • Discrete probability • Asymptotic methods This second edition includes important new material about mechanical summation. In response to the widespread use of the first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. Graham, Knuth, and Patashnik want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.


About the authors
RONALD L. GRAHAM is Chief Scientist at AT&T Labs Research. He is also University Professor of Mathematical Sciences at Rutgers University, and a former President of the American Mathematical Society. Dr. Graham is the author of six other mathematics books.
DONALD E. KNUTH is Professor Emeritus of The Art of Computer Programming at Stanford University. His prolific writings include three volumes on the Art of Computer Programming, and five books related to his TEX and META-FONT typesetting systems.
OREN PATASHNIK is a member of the research staff at the Center for Communications Research, La Jolla. He is also the author of BibTEX, a widely used bibliography processor.





