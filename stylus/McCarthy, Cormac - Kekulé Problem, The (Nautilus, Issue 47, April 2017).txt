



Editor's Note
 
Consciousness

By Michael SegalEditor in ChiefCover Illustration by Len Small
♦


Consciousness is a hard problem because it is emergent, mixes software and hardware, and is dizzyingly self-referential. It's harder still because, in a sense, it impossible to study directly.
	
We can measure how some living (or even inanimate) thing interacts with the world. We can learn to recognize intelligence and reflexivity in that behavior. But how can we tell what it is like to be that thing?
	
Christof Koch, one of today's leading thinkers on consciousness, describes it as "physics from the inside." It's a different category of question than we're used to, and one that has a growing set of intersections with other sciences.
	
It is even a short distance from some common laments. Why don't we understand each other, or even ourselves? This issue is full of discovery, re-discovery and argument by analogy.
	
Because that's a good place to start.
	
Welcome to "Consciousness."
	








Chapter One
Inner Spaces











Biology | Neuroscience
The Spiritual, Reductionist Consciousness of Christof Koch
What the neuroscientist is discovering is both humbling and frightening him.
By Steve Paulson
♦ 

Consciousness is a thriving industry. It's not just the meditation retreats and ayahuasca shamans. Or the conferences with a heady mix of philosophers, quantum physicists, and Buddhist monks. Consciousness is a buzzing business in neuroscience labs and brain institutes. But it wasn't always this way. Just a few decades ago, consciousness barely registered as a credible subject for science. 
Perhaps no one did more to legitimize its study than Francis Crick, who launched a second career in neurobiology after cracking the genetic code. In the 1980s Crick found a brilliant collaborator in the young scientist Christof Koch. In some ways, they made an unlikely team. Crick, a legend in science, was an outspoken atheist, while Koch, 40 years younger, was a Catholic yearning for ultimate meaning. Together, they published a series of pioneering articles on the neural correlates of consciousness until Crick died in 2004. 



WHAT'S THE BUZZ: Bees have all the complicated brain components that humans have, but in a smaller package. "So yes, I do believe it feels like something to be a honey bee," Christof Koch says.
Pixabay


Koch went on to a distinguished career at Caltech before joining the Allen Institute for Brain Science in Seattle. Today, as the president and chief scientific officer, he supervises several hundred scientists, engineers, and informatics experts trying to map the brain and figure out how our neural circuits process information. The Institute recently made news with the discovery of three giant neurons connecting many regions of the mouse brain, including one that wraps around the entire brain. The neurons extend from a set of cells known as the claustrum, which Crick and Koch maintained could act as a seat of consciousness. 
Koch is one of the great thinkers about consciousness. He has a philosophical frame of mind and jumps readily from one big idea to the next. He can talk about the tough ethical decisions regarding brain-impaired patients and also zoom out to give a quick history of Christian thinking on the soul. In our conversation, he ranged over a number of far-out ideas—from panpsychism and runaway artificial intelligence to the consciousness of bees and even bacteria. 




You've said you always loved dogs. Did growing up with a dog lead to your fascination with consciousness?

I've wondered about dogs since early childhood. I grew up in a devout Roman Catholic family, and I asked my father and then my priest, "Why don't dogs go to heaven?" That never made sense to me. They're like us in certain ways. They don't talk, but they obviously have strong emotions of love and fear, hate and excitement, of happiness. Why couldn't they be resurrected at the end of time? 

Are scientific attitudes about animal consciousness simplistic?

The fact is, I don't even know that you're conscious. The only thing I know beyond any doubt—and this is one of the central insights of Western philosophy—is Cogito ergo sum. What Descartes meant is the only thing I'm absolutely sure of is my own consciousness. I assume you're conscious because your behavior is similar to mine, and I could see your brain if I put you in an MRI scanner. When you have a patient who's locked-in, who can't talk to me, I have to infer it. The same with animals. I can see they're afraid when it's appropriate to be afraid, and they display all the behavioral traits of being afraid, including the release of hormones in their bloodstream. If you look at a piece of dog brain or mouse brain and compare that to a piece of human brain the same size, only an expert with a microscope can tell for sure that this is a dog brain or a human brain. You really have to be an expert neuroanatomist. 

We share much of our evolutionary history with dogs and even dolphins. But what about lizards or ants? What about bacteria? Can they be conscious?

It becomes progressively more difficult. The brain of a bird or a lizard has a very different evolutionary history, so it becomes more difficult to assert without having a general theory. Ultimately, you need a theory that tells us which physical systems can be conscious. By the time you get to a worm, let alone to bacteria, you can believe that it feels like something to be a worm because that's ultimately what consciousness is. If it feels like something to be a worm, then it's conscious. Right now, most people believe it doesn't feel like anything to be my iPhone. Yet it may well be true that it feels like something to be a bee. But it's not easy to test that assertion in a scientific way. 

What do you mean when you say "it feels like something?"

It feels like something to be you. I can't describe it to you if you're a zombie. If you were born blind, I can never describe what it means to see colors. You are simply unable to comprehend that. So it is with consciousness. It's impossible to describe it unless you have it. And we have these states of consciousness unless we are deeply asleep or anesthetized or in a coma. In fact, it's impossible not to be conscious of something. Even if you wake up discombobulated in a dark hotel room, you're jet-lagged and your eyes are still closed, you are already there. Before there was just nothing, nada, rien. Then slowly some of your brain boots up and you realize, "Oh, I'm here. I'm in Beijing and I flew in last night." The difference between nothing and something is a base-level consciousness. 

Is this self-awareness?

It's even much simpler. I might not even know who I am when I'm waking up. It takes time to boot up and realize who you are, where you are, what time of day it is. First, you open your eyes and just see darkness. Darkness is different from nothing. It's not that I see darkness behind my head; I just don't see at all. That's what consciousness is. It's a basic feeling. 

We might be surrounded by consciousness everywhere and find it in places where we don't expect.


You said bees could be conscious. They do amazing things, and yet they have tiny brains.

Yes, they do very complicated things. We know that individual bees can fly mazes. They can remember scents. They can return to a distant flower. In fact, they can communicate with each other, through a dance, about the location and quality of a distant food source. They have facial recognition and can recognize their beekeeper. Under normal conditions, they would never sting their beekeeper; it's probably a combination of visual and olfactory cues. 
Their brains contain roughly a million neurons. By comparison, our brains contain about 100 billion, so a hundred thousand times more. Yet the complexity of the bee's brain is staggering, even though it's smaller than a piece of quinoa. It's roughly 10 times higher in terms of density than our cortex. They have all the complicated components that we have in our brains, but in a smaller package. So yes, I do believe it feels like something to be a honey bee. It probably feels very good to be dancing in the sunlight and to drink nectar and carry it back to their hive. I try not to kill bees or wasps or other insects anymore. 

You're talking about the consciousness of an individual bee—not the hive, which has another level of complexity.

I'm talking about the potential for sentience in individual bees. Would we exclude them because they can't talk? Well, lots of people can't talk. Babies can't talk, impaired patients can't talk. Because they don't have a human brain? Well, that's completely arbitrary. Yes, their evolution diverged from us 250 million years ago or so, but they share with us a lot of the basic metabolism and machinery of the brain. They have neurons, ionic channels, neurotransmitters, and dopamine just like we have. 

So brain size is not the key factor in consciousness?

That's entirely correct. In fact, there's no principal reason to assume that brain size should be the be-all and end-all of consciousness. 

We also know Neanderthals had bigger brains than the Homo sapiens who lived near them in Europe. Yet we survived and they didn't.

Their brain was maybe 10 percent larger than our brain. We don't know why we survived. Did we just outbreed them? Were we more aggressive? There's some research showing that dogs play a role here. At the same time when Homo neanderthalensis became extinct—around 35,000 years ago—Homo sapiens domesticated the wolf and they became the two apex hunters. Homo sapiens and wolves/dogs started to collaborate. We became this ultra-efficient hunting cooperative because we now had the ability to be much more efficient at hunting down prey over long distances and exhausting them. So the creature with the larger brain didn't survive and the one with the smaller brain did. 



FATAL INTELLIGENCE: Given the probable existence of trillions of planets, why haven't we detected life elsewhere? It's likely, Christof Koch says, that sufficiently complex and intelligent life would destroy itself.
NASA



Why were humans able to create civilizations that have transformed the planet?

We don't have a precise answer. We have big brains and are, by some measure, the most intelligent species, at least in the short term. We'll see whether we'll actually survive in the long term, given our propensity for mass violence. And we've manipulated the planet to such an extent that we are now talking about entering a new geological age, the Anthropocene. But it's unclear why whales or dolphins—some of which have bigger brains and more neurons in their cortex than we do—why they are not called smarter or more successful. Maybe because they have flippers and live in the ocean, which is a relatively static environment. With flippers, you're unable to build sophisticated tools. Of course, human civilization is all about tools, whether it's a little stone, an arrow, a bomb, or a computer. 

So hands are crucial for their ability to manipulate tools.

You need not only a brain, but also hands that can manipulate the environment. Otherwise, you can think about the world but you can't act upon it. That's probably why this particular species of primate excelled and took over the planet. 

There are fascinating questions about how deep consciousness goes. You've embraced the old philosophy of panpsychism. Isn't this the idea that everything in nature has some degree of consciousness or mind?

Yes, there's this ancient belief in panpsychism: "Pan" meaning "every," "psyche" meaning "soul." There are different versions of it depending on which philosophical or religious tradition you follow, but basically it meant that everything is ensouled. Now, I don't believe that a stone is ensouled or a planet is ensouled. But if you take a more conceptual approach to consciousness, the evidence suggests there are many more systems that have consciousness—possibly all animals, all unicellular bacteria, and at some level maybe even individual cells that have an autonomous existence. We might be surrounded by consciousness everywhere and find it in places where we don't expect it because our intuition says we'll only see it in people and maybe monkeys and also dogs and cats. But we know our intuition is fallible, which is why we need science to tell us what the actual state of the universe is. 

The Internet and runaway AI will not have our value system. It may not care at all about humans. Why should it?


Most scientists would dismiss panpsychism as ancient mythology. Why does this idea resonate for you?

It's terribly elegant in its simplicity. You don't say consciousness only exists if you have more than 42 neurons or 2 billion neurons or whatever. Instead, the system is conscious if there's a certain type of complexity. And we live in a universe where certain systems have consciousness. It's inherent in the design of the universe. Why is that so? I don't know. Why does the universe follow the laws of quantum mechanics? I don't know. Can I imagine a universe where the laws of quantum mechanics don't hold? Yes, but I don't happen to live in such a universe, so I believe our universe has certain types of complexity and a system that gives rise to consciousness. Suddenly the world is populated by entities that have conscious awareness, and that one simple principle leads to a number of very counterintuitive predictions that can, in principle, be verified. 

So it all comes down to how complex the system is? And for the human brain, how its neurons and synapses are wired together?

It comes down to the circuitry of the brain. We know that most organs in your body do not give rise to consciousness. Your liver, for example, is very complicated, but it doesn't seem to have any feelings. We also know that consciousness does not require your entire brain. You can lose 80 percent of your neurons. You can lose the little brain at the back of your brain called the cerebellum. There was recently a 24-year-old Chinese woman who discovered, when she had to get a brain scan, that she has absolutely no cerebellum. She's one of the extremely rare cases of people born without a cerebellum, including deep cerebellar nuclei. She never had one. She talks in a somewhat funny way and she's a bit ataxic. It took her several years to learn how to walk and speak, but you can communicate with her. She's married and has a child. She can talk to you about her conscious experiences. So clearly you don't need the cerebellum. 
Yet the cerebellum has everything you expect of neurons. It has gorgeous neurons. In fact, some of the most beautiful neurons in the brain, so-called Purkinje cells, are found in the cerebellum. Why does the cerebellum not contribute to consciousness? It has a very repetitive and monotonous circuitry. It has 69 billion neurons, but they have simple feed-forward loops. So I believe the way the cerebellum is wired up does not give rise to consciousness. Yet another part of the brain, the cerebral cortex, seems to be wired up in a much more complicated way. We know it's really the cortex that gives rise to conscious experience. 

It sounds like you're saying our intelligence comes from this wiring, not from some special substance in the neurons. Could a conscious system be made of something totally different?

That's correct. There's nothing inherently magical about the human brain. It obeys all the laws of physics like everything else in the universe. There isn't anything supernatural that's added to my brain or my cortex that gives rise to a conscious experience. 

Is it like a computer?

A computer shares some similarities with the brain, but this is a metaphor and that can be dangerous. One is evolved, the other one is constructed. In the one case you have software and hardware. It's much more difficult to make that distinction in the brain. I think we have to be cautious about comparisons between a brain and a computer. But in theory, a system that's complex enough could be conscious. It may be possible that human-built artifacts would feel like something and would also experience the world. 

The Internet is an extremely complex system. Could it feel happy or depressed?

If a computer or the Internet has sentience, the challenge is how we relate its conscious state to ours because its evolutionary history is radically different. It doesn't have our senses or our reward systems. Of course, this is also a threat. The Internet and runaway AI will not have our value system. It may not care at all about humans. Why should it? We don't care about ants or bugs. Most of us don't even care about chickens or cows except when we want to eat them. This is a concern moving forward if we endow these entities not just with consciousness but intelligence. Is that really such a good idea? 
We're not the dominant species on the planet because we are wiser or swifter or more powerful. It's because we're more intelligent and ruthless. If we build intelligent systems that exceed even our intelligence, we may believe we can control them. "Oh yeah, I always have this kill-switch. Don't worry, it'll be OK." Well, one day somebody's going to say, "Oops, I didn't want that. I didn't mean that to happen." And it may be our last invention. 

I'm not a mystic. I'm a scientist. But this is a feeling I have. I find myself in a wonderful universe with a very positive and romantic outlook on life.


That's the scenario in a lot of science fiction. But you really believe artificial intelligence could develop a certain level of complexity and wipe us out?

This is independent of the question of computer consciousness. Yes, if you have an entity that has enough AI and deep machine learning and access to the Cloud, etc., it's possible in our lifetime that we'll see creatures that we can talk to with almost the same range of fluidity and depth of conversation that you and I have. Once you have one of them, you replicate them in software and you can have billions of them. If you link them together, you could get superhuman intelligence. That's why I think it behooves all of us to think hard about this before it may be too late. Yes, there's a promise of untold benefits, but we all know human nature. It has its dark side. People will misuse it for their own purposes. 

How do we build in those checks to make sure computers don't rule the world?

That's a very good question. The only reason we don't have a nuclear bomb in every backyard is because you can't build it easily. It's hard to get the material. It takes a nation state and tens of thousands of people. But that may be different with AI. If current trends accelerate, it may be that 10 programmers in Timbuktu could unleash something truly malevolent onto mankind. These days, I'm getting more pessimistic about the fate of a technological species such as ours. Of course, this might also explain the Fermi paradox. 

Remind us what the Fermi paradox is.

We have yet to detect a single intelligent species, even though we know there are probably trillions of planets. Why is that? Well, one explanation is it's just extremely unlikely for life to arise and we're the only one. But I think a more likely possibility is that any time you get life that's sufficiently complex, with advanced technology, it has somehow managed to annihilate itself, either by nuclear war or by the rise of machines. 



JUST LIKE HEAVEN: "In a cathedral, I get a feeling of luminosity out of the numinous," says Christof Koch. Gaudi's La Sagrada Familia is seen above. "You can get that feeling without being a Catholic."
Pixabay



You are a pessimist! You really think any advanced civilization is going to destroy itself?

If it's very aggressive like ours and it's based in technology. You can imagine other civilizations that are not nearly as aggressive and live more in harmony with themselves and nature. Some people have thought of it as a bottleneck. As soon as you develop technology to escape the boundary of the planet, there's an argument that civilization will also develop computers and nuclear fusion and fission. Then the question is, can it grow up? Can it become a full-grown, mature adult without killing itself? 

You have embraced Integrated Information Theory, which was developed by your colleague Giulio Tononi. What can this tell us about consciousness?

The Integrated Information Theory of consciousness derives a mathematical calculus and gives rise to something known as a consciousness meter, which a variety of clinical groups are now testing. If you have an anesthetized patient, or a patient who's been in a really bad traffic accident, you don't really know if this person is minimally conscious or in a vegetative state; you treat them as if they're conscious, but they don't respond in any meaningful way. 

How can you be sure they're conscious?

You're never really sure. So you want a brain-based test that tells you if this person is capable of some experience. People have developed that based on this integrated information series. That's big progress. The current state of my brain influences what happens in my brain the next second, and the past state of my brain influences what my brain does right now. Any system that has this cause-effect power upon itself is conscious. It derives from a mathematical measure. It could be a number that's zero, which means a system with no cause-effect power upon itself. It's not conscious. Or you have systems that are "Phi," different from zero. The Phi measures, in some sense, the maximum capacity of the system to experience something. The higher the number, the more conscious the system. 

So you could assign a number to everything that might have some degree of consciousness—whether it's an ant, a lizard, bacteria, or a vegetative human being?

Yes, you or me, the Dalai Lama or Albert Einstein. 

The higher the number, the more conscious?

The number by itself doesn't tell you it's now thinking, or is conscious of an image or a smell. But it tells you the capacity of the system to have a conscious experience. In some deep philosophical sense, the number tells you how much it exists. The higher the number, the more the system exists for itself. There isn't a Turing Test for consciousness. You have to look at the way the system is built. You have to look at the circuitry, not its behavior, whether it's a computer or a biological brain. This has now been tested and validated in many patients, including locked-in patients who are fully conscious, people under anesthesia who are not conscious, people in deep sleep, and those in vegetative states or minimal-conscious states. So the question now is whether this can be turned into something practical that can be used at every clinic in the country or the world to test patients who've just been in a bad traffic accident. 

Obviously, there are huge implications. Do you turn off the life-support machines?

First, does the patient suffer or is nobody home anymore? In the famous case of Terri Schiavo, we could tell the brain stem was still functioning but there wasn't anybody home. Her consciousness had disappeared 15 years earlier. 

Isn't there still the old "mind-body problem?" How do three pounds of goo in the human brain, with its billions of neurons and synapses, generate our thoughts and feelings? There seems to be an unbridgeable gap between the physical world and the mental world.

No, it's just how you look at it. The philosopher Bertrand Russell had this idea that physics is really just about external relationships—between a proton and electron, between planets and stars. But consciousness is really physics from the inside. Seen from the inside, it's experience. Seen from the outside, it's what we know as physics, chemistry, and biology. So there aren't two substances. Of course, a number of mystics throughout the ages have taken this point of view. 
It does look strange if you grew up like me, as a Roman Catholic, believing in a body and a soul. But it's unclear how the body and the soul should interact. After a while, you realize this entire notion of a special substance that can't be tracked by science—that I have but animals don't have, which gets inserted during the developmental process and then leaves my body—sounds like wishful thinking and just doesn't cohere with what we know about the actual world. 

It sounds like you lost your religious faith as you learned about science.

I lost my religious faith as I matured. I still look fondly back upon it. I still love the religious music of Bach. I still get this feeling of awe. In a cathedral, I can get a feeling of luminosity out of the numinous. When I'm on a mountain top, when I hear a dog howling, I still wake up some mornings and say, "I'm amazed that I exist. I'm amazed there is this world." But you can get that without being a Catholic. 

Does that experience of awe or the numinous feel religious?

Not in a traditional sense. I was raised to believe in God, the Trinity, and particularly the Resurrection. Unfortunately, I now know four words: "No brain, never mind." That's bad news. Once my brain dies, unless I can somehow upload it into the Cloud, I die with it. I wish it were otherwise, but I'm not going to believe something if it's opposed by all the facts. 

A few years ago, you and some other scientists spent a week with the Dalai Lama. Was that a meaningful experience?

Yes, it was. There were thousands of monks in the Drepung Monastery who were listening to our exchange. This particular Tibetan Buddhist tradition is quite fascinating. I'm not a scholar of it, but they view the mind primarily from an interior perspective. They've developed very sophisticated ways of analyzing it that are different from our way. We take the external way of Western science, which is independent of the observer. But ultimately, we're trying to approach the same thing. We're trying to approach this phenomenon of conscious experience. They have no trouble with the idea of evolution and other creatures being sentient. I found that very heartening—in particular the Dalai Lama's insistence on the primacy of science. I asked him, "What happens if science is in conflict with certain tenets of Buddhist faith?" He laughed and said, "Well, if this belief doesn't accord with what science ultimately discovers about the universe, then we have to throw it out." 

But the Dalai Lama believes in reincarnation.

We talked about that. In fact, I said, "Well, I'm really sorry, Your Holiness, but I think we just have to agree that Western science shows that if there's no physical carrier, you're not going to get a mind. You're not going to get memory because you need some mechanism to retain the memory." I asked him, "Were you not reincarnated from the previous Dalai Lama?" And he just laughed and said, "Well, I don't remember anything about that anymore." 

Has this scientific knowledge helped you sort out the deep existential questions about meaning, about why we're here?

My last book is titled Confessions of a Romantic Reductionist. I'm a reductionist because I do what scientists do. I take a complex phenomenon and try to pull it apart and reduce it to something at a lower level. I'm also romantic in the sense that I believe I can decipher the distant contrails of meanings. I find myself in a universe that seems to be conducive to life—the Anthropic Principle. And for reasons I don't understand, I also find myself in a universe that became conscious, ultimately reflecting upon itself. Who knows what might happen in the future if we continue to evolve without destroying ourselves? To what extent can we become conscious of the universe as a whole? 
I don't know who put all of this in motion. It's certainly not the almighty God I was raised with. It's a god that resides in this mystical notion of all-nothingness. I'm not a mystic. I'm a scientist. But this is a feeling I have. I find myself in a wonderful universe with a very positive and romantic outlook on life. If only we humans could make a better job of getting along with each other. 



Steve Paulson is the executive producer of Wisconsin Public Radio's nationally syndicated show To the Best of Our Knowledge. He's the author of Atoms and Eden: Conversations on Religion and Science. You can subscribe to TTBOOK's podcast here.

Lead image: Courtesy of Allen Institute













Biology | Health
What My Stroke Taught Me
The surprising, quiet nourishment of losing my internal monologue.
By Lauren MarksIllustrations by Jackie Ferrentino
♦ 

In my memories of the Scottish hospital, the sky is always blue, though I know that can't be completely accurate. Summer was waning, and as my friends and I had already experienced, Edinburgh was prone to unpredictable storms. Yet, I can't think of a single moment of rain in the two weeks I lay in bed. My morphine-soaked haze only allowed glimpses and fragments: the bracing air coming in from an open window, the rough comfort of my mother's fingers wiping my fever-moist brow, my father's tears. All of that must have been confusing to me, but when I think of this time, I remember more clarity than confusion. I remember the Quiet. 
This was not a Quiet I had known before. It was a placid current, a presence more than an absence. Everything I saw or touched or heard pulsed with a marvelous sense of order. I had a nothing mind, a flotsam mind. I was incredibly focused on the present, with very little awareness or interest in my past or future. My entire environment felt interconnected, like cells in a large, breathing organism. To experience this Quiet was to be it. 



However, this sense of serenity was not shared by those around me. After I had collapsed in an Edinburgh bar while singing karaoke, and the medics had taken me away in an ambulance, my friends called my parents in the United States. It was the middle of the night in Edinburgh, but early evening in Los Angeles, and no one was overly worried about my fall from the stage, since it appeared I was suffering a simple concussion. That all changed two hours after my hospital admission—when the results of my CT scan showed the actual crisis unfolding. An aneurysm had ruptured in my brain and the hemorrhage was spreading. A neuroradiologist explained to my parents how precarious my situation was—how often people died the instant an aneurysm ruptured, and even after treatment, only slightly more than half of these patients actually survive the next few days. With every second being critical, the doctor was preparing for an emergency operation. But my now-horrified parents were stranded in California. Their passports were in their safety deposit box, and the bank branch was closed for the night. My parents rattled on the windows of the bank the next morning, successfully convincing the bank to open early for them because there was no time to waste. My procedure was well under way when my parents boarded their flight the next morning, leaving my brother and grandmother behind at the house. The operation was already over when they got to Edinburgh. My parents and friends came together, relieved that I had survived the operation, but living with a keen awareness of how perilous my situation still was. 
It took a few days for me to wake up fully, under the influence of a combination of swollen brain tissue and heavy sedation. However, when I was more alert, the Quiet I found myself experiencing was much more interesting than my medical state. I had woken up to a new world, hushed and full of curiosities. 

This was the very moment I became aware I couldn't read anymore.

One of these moments of marvel took place during a move between the critical unit and the recovery ward. I was being transported in a mirrored elevator, and although there were no bandages on my face and my vision was clear, it was almost impossible for me to recognize my own reflection. Yet, somehow, this didn't disturb me. In fact, it made remarkable sense because I was quickly realizing that my reflection was not the only thing that was different. Transformation felt abundant. Once-fixed concepts, like "wall" and "window," weren't as easy to identify anymore, and the differences between "he" and "she" and "I" and "it" were becoming indistinguishable. I knew my parents were my parents and my friends were my friends, but I felt less like myself and more like everything around me. 
I was wheeled to a bed by a westerly-facing window, with three other women in the room. My suitemates were often in discussion with one another. Even through their brogues, I understood what they were saying, but I rarely took part in the conversations. I just enjoyed the way their voices plodded and pattered like footsteps. 
At this point I didn't know much about my brain injury at all. I wasn't in any pain, so my thoughts about my new condition were unfocused and fleeting. Instead of being occupied by questions about why I was in the hospital and what had happened to me, my mind was engrossed in an entirely different set of perceptions. The smallest of activities would enthrall me. Dressing myself, I was awed by the orbital distance between cloth and flesh. Brushing my teeth, I was enchanted by the stiffness of the bristles and the sponginess of my gums. I also spent an inordinate amount of time looking out the window. My view was mainly of the hospital's rooftop, with its gray and untextured panels, though I developed a lot of interest in a nearby tree. I could only make out the tops of the branches, but I'd watch this section of needles and boughs intently, fascinated by how the slightest wind would change the shape entirely. It was always and never the same tree. 
Very few things disturbed me during this period of time. But even in this formless daydream I remember the moment that most closely resembled real distress. Or, at least, when I became aware of an actual loss. 
It must have been midday because the sunlight was falling across my body, and that slat of light emphasized the white nightstand on my left. My parents had filled the shelves inside with clothing, and the nurses made sure there were plenty of liquids for me to drink in there, too. On this day, I noticed that there was a stack of magazines on the nightstand, as well as a book. I am not sure how long they had been there—for all I knew, they could have even predated my arrival—but this was the first time they piqued my interest. 
The high gloss of the magazine cover felt wet in my hands. And as I opened it up, I was instantly bombarded with photos of red carpet parades and illustrated makeup tips, a circus of color and distraction. I couldn't linger anywhere. It felt as if the magazine were shouting at me. Closing it was a relief. 
I turned to the book. It was a novel by Agatha Christie, something I had probably read many years earlier. I opened to Chapter One and flipped slowly and evenly through the first few pages, a motion that seemed to come naturally to me. But on the third page, I stopped. I returned to the first page and started again. Slower this time. Much slower. My eyes focused and refocused in the bright sunlight, but I continued to only see the black, blocked shapes where words used to be. 
Thinking about it now, I don't know how I could be so certain that it was an Agatha Christie novel, especially since this was the very moment I became aware I couldn't read anymore. With this simultaneously familiar and unfamiliar book in my hands, I first took in the actual loss of words. For my entire life, language had been at the forefront of every personal or professional achievement, and very few things had brought me as much joy and purpose. If I had ever been warned that I might be robbed of my ability to read, even for a limited amount of time, it would have been a devastation too cruel to bear. Or so I would have thought. But a day did come when I couldn't read the book in front of me, when paragraphs appeared to be nothing more than senseless jumbles, and the way I actually processed this massive loss was surprisingly mild. The knowledge of the failure was jarring, without a doubt, but was there any misery or angst? No. My reaction was much less sharp. A vague sense of disappointment swept through me, but then ... my inability to use words in this way just felt like transient information. Now that the ability was gone, I could no longer think of how or why it should have any influence on my life whatsoever. 
It's shocking to reflect on that moment, and think about how the loss of something so crucial washed past me with such a vague wisp of emotion. But I was living so deeply in the present—and in the comfort of the Quiet—I couldn't fully realize how my sense of identity had shifted. It would be several weeks before I detected how much of myself had gone missing, and how hard I'd have to fight to regain it. However, the unpleasant sensations that came with holding that book drifted away as soon as I closed it. And with no effort at all, my attention settled back on the impossible blue sky. 



A few days after the surgery and a battery of tests, Dr. Rustam Al-Shahi Salman, the consultant neurologist overseeing my case, made my parents aware of the short- and long-term issues at hand. Dr. Salman was slim and soft-spoken, his gestures and words thoughtful, and he was never rushed, a demeanor that fit nicely in the Quiet I now inhabited. He was also probably the first person who used the word "aphasia" with my family. However, he explained it in much more detail with my parents than with me. 
He told my parents that aphasia did not attack a person's cognitive abilities and most often left a person's intelligence completely intact. But this condition could manifest quite differently in different people, and aphasia is generally divided into two categories: receptive and expressive. Expressive aphasia (also called "non-fluent" or "Broca's" aphasia) is characterized by word-finding difficulties, while receptive aphasia (also called "fluent" or "Wernicke's" aphasia) affects language comprehension. The expressive issues were most pronounced in my case, but in the beginning, I struggled with receptive issues, too, unable to detect the missing or garbled parts of my own language. 
The speech and language therapist Dr. Salman appointed to me aimed to change that. 
Anne Rowe was near my mother's age, with faded red curls cut close to her head. For a while, it seemed to me that her only job was to hand me worksheets. Piles and piles of worksheets. One of the first worksheets she gave me had a panel of faces. Every day I was instructed to point at the bald man in the images to tell her how I was feeling. 
I feel fine, I said. Or thought I said. But Anne would insist on a more in-depth answer. 
Why don't you just try to point to the picture that feels most appropriate for you? she would ask. 



It didn't occur to me then that Anne was employing this image prompt not as an exercise but a necessity—because most of the time she couldn't understand my responses to her questions. While my expressive aphasia prevented me from speaking clearly, my receptive aphasia prevented me from knowing when my language was not clear. According to my parents, in the first two weeks I could only say 40 or 50 words. 


Anne's records from our initial sessions mention that creating the sounds for speech was often challenging for me too: "Lauren is able to use fully intact phrases at times without hesitation, but has clear difficulties with word finding and motor planning for speech." This meant I had trouble shaping my mouth to make the right sounds—a condition known as speech apraxia, which often accompanies an onset of aphasia. Children go through a similar process, stuttering into speech while parents ask them to repeat and refine what they are saying until they do it correctly. Anne's worksheets had the same goal. Pointing at a drawing of a mouth, she'd say: The tip of the tongue goes here. ... 
Then she would illustrate on her own face: T, T, T, Teh, is the tip of the tongue. Th, Th, Th is Thuh, the fat part of the tongue. 
I wasn't disturbed when Anne asked me to take part in these articulation exercises. They didn't indicate to me that there was something especially wrong. In fact, they strongly resembled the routine vocal warm-ups I had been doing—and enjoying—since theater school. Asking an actor to demonstrate the difference between a P sound and a B sound over and over was nothing out of the ordinary. When I was instructed to do so in the hospital, I assumed I was excelling at it, flexing my muscle memory, until Anne subtly indicated my failures and misfires with her feedback throughout our sessions. 
Very good, she'd say. Or: Not exactly, try again. 
At some point, I realized that Anne was saying "not exactly" a lot. And if we hit too many "try agains," Anne would suggest we move on to something else for a while. It was a major hint that something was amiss. I didn't know exactly what was wrong, but I would try to fix it because I preferred positive feedback to negative. 

Without language, I was paying attention to the world in a new way.

One week after the rupture, Anne administered the Western Aphasia Battery test on me. After the reading section, she made this note: "Testing was stopped as Lauren was becoming distressed. L. is very aware she could not do task." Though I have a hard time remembering this distress, I trust in Anne's reporting. My best guess is that my anxiety was only skin-deep and short-lived. I also believe my awareness was more limited than Anne might have assumed. I probably wasn't thinking about my inability to do this task and how that might affect my limitations on future tasks. At the time, I had very little concern for the past or future; but in the present, I simply didn't like to disappoint. That, more than anything, was probably the source of my distress. Lucky for me, though, it didn't last long. In the way I perceived the world, negative impressions could pass very quickly, as if I had never even had them. 
My trouble with spoken language was mirrored in my written language. I discovered as I progressed in my sessions with Anne that I had not completely forgotten the alphabet, but I had forgotten its order. If I isolated single letters at a time, I could still identify them on a page. It took a lot of guidance from Anne, but with her by my side, I could slowly sound out these letters, occasionally creating a very fragile word. Anne noted: "There are frequent errors reading aloud, especially words with irregular pronunciations, and Lauren finds it difficult to know if she is correct or not." So, while I had not lost my ability to read entirely, "reading" in this new iteration of my life involved a razor-sharp focus, accommodating only a word at a time. I also wasn't able to know my own accuracy without someone else's support. I would slowly sound out a word, but it took so long that when I went on to tackle the next one, I often would forget what I had just read. Perhaps that was what had happened with the Agatha Christie book I had attempted to read by myself. I had been expecting the language on the page to behave the way it used to, and when it didn't, the whole picture crumbled in front of me. Words could be approachable in small, isolated units. But a full sentence? That was beyond imagining. 



I realize now that Anne was trying to address a systematic failure in me: my newly acquired aphasia. I just couldn't think of it like that. I could flip-flop in our exchanges and not hear the mistakes. When I did, I would assume I was simply tired or that the disturbances were all minor and temporary. And as soon as our session would end, I would gently be redelivered to the happy stillness of the pervasive Quiet. 



My life had always been populated with big personalities, and I had created different approaches as a way to interact with each of them—as a daughter, as an older sister, as an actress, as a roommate, as a girlfriend. Before the stroke, my ability to appreciate the needs and desires of these complex characters around me came pretty easily. But after the stroke, my emotional sensitivity had dulled tremendously. It was hard to know what other people might be thinking, and I wasn't that interested in finding out. My general disinterest in interpersonal interactions was probably rooted in both emotional and anatomical aspects. 
The rupture had originated on the middle cerebral artery in the left hemisphere of my brain, bleeding into the Sylvian fissures and my left basal ganglia. This cerebral artery supplies the blood for the two language centers of the brain—Broca's area and Wernicke's area. The basal ganglia are usually associated with motor control, but they also affect habits, cognition, and emotion. Some basal injuries can blunt emotional awareness and slow "goal-directed" activity. With such a wide range of influences, the alterations to the basal ganglia were probably affecting me in many ways at the time, but after the rupture, it was my faltering language that was my most visible symptom. 
My aphasia had invisible effects, too, in ways that many people wouldn't even think about. It was not just my external language that was ailing. My inner monologue, my self-directed speech, had also gone almost completely mute. In its place was the radiant Quiet. The nourishing Quiet. The illuminating Quiet. 
The Quiet was not something I spoke to anyone about. While my parents were on alert for signs of a secondary stroke (vasospasms are common after a rupture), I was happy enough floating in this meditative state. It felt deeply unique to me, but I later learned of other people (who also sustained damage to the left hemisphere of the brain) who have reported similar phenomena. Clinical psychologist Scott Moss describes waking up in the hospital with his own aphasia. His account is included in Injured Brains of Medical Minds. He writes: 
I did comprehend somewhat vaguely what was said to me. ... I didn't have any difficulty focusing: It was simply that the words, individually or in combination, didn't have meaning, and even more amazing, I was only a trifle bothered by that. ... I had also lost the ability even to engage in self-talk. ... I simply existed. ... It was as if without words I could not be concerned about tomorrow. 
And Jill Bolte Taylor, a Harvard-trained neuroanatomist, who is well-known for being the author of the bestseller My Stroke of Insight, lost this inner monologue as well. She describes it as "brain chatter" that was "replaced by a pervasive and enticing inner peace." In addition, she writes that she "didn't think in the same way," partially because of the "dramatic silence that had taken residency" in her. Bolte Taylor specifically identifies her perceptual changes as related to a shift of attention between the two hemispheres of her brain. 
In The Master and His Emissary: The Divided Brain and the Making of the Western World, psychiatrist and writer Iain McGilchrist goes much further into detail about the differences between these hemispheres. The brain looks like a walnut split down the middle, and its two halves are called hemispheres. Each is a fully functional processing unit, like a PC and Mac side by side in the skull. Though they usually work together to create a seemingly uniform worldview, a human being can live with only one functional hemisphere, or one hemisphere can do the heavy lifting while the other is under repair (as is often the case for a person who has suffered a stroke). McGilchrist takes issue with the pseudoscience of people calling themselves "left-brained" or "right-brained," but that being said, the hemispheres do have different strengths, or as McGilchrist describes it, their differences deal with "competing needs" and "the types of attention they are required to bring to bear on the world." This bifurcated arrangement doesn't just exist in humans, but in most vertebrates, too. In a single moment, a bird, using its left hemisphere, must identify if an item is food or sand and using its right hemisphere, simultaneously be on guard for predators. McGilchrist mentions that these are "two quite different kinds of exercise, requiring not just that attention should be divided, but that it should be two distinct types [of attention] at once." 



These hemispheric differences are not so divergent in humans, only more sophisticated. Our left hemisphere is much more detail-focused, and since both language centers exist on this side of the brain, it is much more verbal. But the right hemisphere has a keen awareness, too, and it is more vigilant than the left, more receptive to new information. McGilchrist writes: 
The left hemisphere's "stickiness," its tendency to recur to what it is familiar with, tends to reinforce whatever it is already doing. There is a reflexivity to the process, as if trapped in a hall of mirrors: It only discovers more of what it already knows, and it only does more of what it already is doing. The right hemisphere by contrast [is] seeing more of the picture, and taking a broader perspective. 
This description resonates intensely with me. Without language, I was paying attention to the world in a new way. Without the talents and abilities I had once relied on—and used to identify myself—I was interacting with more ineffable senses. I had escaped from my old hall of mirrors, and with my language-dominant left hemisphere somewhat disabled, I was probably taking in a whole host of perceptions from the right hemisphere that were suddenly prioritized. 
I was experiencing a near-constant sensation of interconnectedness, but my observations often lacked specific categories and dimensions, and a sense of my own personal preference. My "self" didn't seem at all pertinent in this kind of processing. It was all happening to me and through me, but not necessarily because of me. 



THE quiet: After an aneurysm ruptured her brain, Lauren Marks lost her inner monologue. 
Brooks Girsch


I believe this temporary shift—changing the dominance from one hemisphere to the other and losing my inner voice for a while—was a huge part of what made the Quiet so quiet. The constant stream of language, which I had always assumed was thought, had stopped. It's hard to describe this voice exactly, and even harder to describe its lack. It is the internal monologue that turns on in the morning, when we instruct ourselves to "Get up" and "Make breakfast." It's a voice we use to monitor ourselves, to criticize or to doubt—and it can be pernicious this way. However, it can be an effective tool as well. We can motivate ourselves with it, understand our environment better, and sometimes modify our situations as well. My inner speech returned very slowly, not on a certain day, but in bits and bobs. In the hospital, though, I didn't realize that I no longer had access to it, only that something in me felt substantially ... different. 
However, I certainly was able to think after the aneurysm's rupture. In many ways, my thinking had never been clearer. I retained the capacity for complex thought, but it was not represented by words or phrases, and my ideas didn't cluster or activate one another the same way. It wasn't ignorance, but there was an element of innocence. 
And on the whole, this silence served me very well. With my internal monologue on mute, I was mainly spared from understanding my condition early on. Unable to ask myself: What is wrong with me? I could not, and did not, list the many things that were. 
I was no longer the narrator of my own life. 



Ten years later, after another major surgery and countless hours of formal and informal language therapy, I have regained much of my linguistic capacity. How much is lost forever, I'll never know. I cannot promise that I am much like the person I was five years ago, or 15 years ago, or that I will be the same person 50 seconds from now. But I know experiences like this are not limited to people who have had brain injuries. Anytime we talk about our childhood, or any other distant period of our lives, we have to accommodate multiple versions of ourselves—even though we don't sound, or speak, or even think, like these people anymore. My changes were more swift than many. But we all contain these kinds of multitudes. 
We are rarely prepared for the next stages in our lives, and we lurch forward into positions we are not equipped for, without the expertise we might sorely need. With that in mind, perfection can never be the goal. But fluidity might be. And sometimes without exactly realizing it, in the process of doing what we are doing, we become the people who are capable of doing it. 
Language was both my injury and the treatment of that injury, and in many ways, I have been writing my way back to fluency. I suspect I will continue to keep reaching out for language, even when it falls short. Speech, overt or covert, can be such a gift, but sometimes it is at its best when it isn't being used at all. 
How beautiful a word can be. Almost as beautiful as the silence that precedes it. 



Lauren Marks is an advocate for those who live with language disorders like aphasia. In 2011, she was an Emerging Voices Fellow for PEN Center USA. A Stitch of Time is her first book.
Copyright © 2017 by Lauren Marks. From the forthcoming book A Stitch of Time: The Year a Brain Injury Changed My Language and Life by Lauren Marks to be published by Simon & Schuster, Inc. Printed by permission.












Matter | Physics
Is Matter Conscious?
Why the central problem in neuroscience is mirrored in physics.
By Hedda Hassel Mørch
♦ 

The nature of consciousness seems to be unique among scientific puzzles. Not only do neuroscientists have no fundamental explanation for how it arises from physical states of the brain, we are not even sure whether we ever will. Astronomers wonder what dark matter is, geologists seek the origins of life, and biologists try to understand cancer—all difficult problems, of course, yet at least we have some idea of how to go about investigating them and rough conceptions of what their solutions could look like. Our first-person experience, on the other hand, lies beyond the traditional methods of science. Following the philosopher David Chalmers, we call it the hard problem of consciousness. 
But perhaps consciousness is not uniquely troublesome. Going back to Gottfried Leibniz and Immanuel Kant, philosophers of science have struggled with a lesser known, but equally hard, problem of matter. What is physical matter in and of itself, behind the mathematical structure described by physics? This problem, too, seems to lie beyond the traditional methods of science, because all we can observe is what matter does, not what it is in itself—the "software" of the universe but not its ultimate "hardware." On the surface, these problems seem entirely separate. But a closer look reveals that they might be deeply connected. 






Consciousness is a multifaceted phenomenon, but subjective experience is its most puzzling aspect. Our brains do not merely seem to gather and process information. They do not merely undergo biochemical processes. Rather, they create a vivid series of feelings and experiences, such as seeing red, feeling hungry, or being baffled about philosophy. There is something that it's like to be you, and no one else can ever know that as directly as you do. 
Our own consciousness involves a complex array of sensations, emotions, desires, and thoughts. But, in principle, conscious experiences may be very simple. An animal that feels an immediate pain or an instinctive urge or desire, even without reflecting on it, would also be conscious. Our own consciousness is also usually consciousness of something—it involves awareness or contemplation of things in the world, abstract ideas, or the self. But someone who is dreaming an incoherent dream or hallucinating wildly would still be conscious in the sense of having some kind of subjective experience, even though they are not conscious of anything in particular. 

Philosophers and neuroscientists often assume that consciousness is like software, whereas the brain is like hardware.

Where does consciousness—in this most general sense—come from? Modern science has given us good reason to believe that our consciousness is rooted in the physics and chemistry of the brain, as opposed to anything immaterial or transcendental. In order to get a conscious system, all we need is physical matter. Put it together in the right way, as in the brain, and consciousness will appear. But how and why can consciousness result merely from putting together non-conscious matter in certain complex ways? 
This problem is distinctively hard because its solution cannot be determined by means of experiment and observation alone. Through increasingly sophisticated experiments and advanced neuroimaging technology, neuroscience is giving us better and better maps of what kinds of conscious experiences depend on what kinds of physical brain states. Neuroscience might also eventually be able to tell us what all of our conscious brain states have in common: for example, that they have high levels of integrated information (per Giulio Tononi's Integrated Information Theory), that they broadcast a message in the brain (per Bernard Baars' Global Workspace Theory), or that they generate 40-hertz oscillations (per an early proposal by Francis Crick and Christof Koch). But in all these theories, the hard problem remains. How and why does a system that integrates information, broadcasts a message, or oscillates at 40 hertz feel pain or delight? The appearance of consciousness from mere physical complexity seems equally mysterious no matter what precise form the complexity takes. 
Nor would it seem to help to discover the concrete biochemical, and ultimately physical, details that underlie this complexity. No matter how precisely we could specify the mechanisms underlying, for example, the perception and recognition of tomatoes, we could still ask: Why is this process accompanied by the subjective experience of red, or any experience at all? Why couldn't we have just the physical process, but no consciousness? 
Other natural phenomena, from dark matter to life, as puzzling as they may be, don't seem nearly as intractable. In principle, we can see that understanding them is fundamentally a matter of gathering more physical detail: building better telescopes and other instruments, designing better experiments, or noticing new laws and patterns in the data we already have. If we were somehow granted knowledge of every physical detail and pattern in the universe, we would not expect these problems to persist. They would dissolve in the same way the problem of heritability dissolved upon the discovery of the physical details of DNA. But the hard problem of consciousness would seem to persist even given knowledge of every imaginable kind of physical detail. 



In this way, the deep nature of consciousness appears to lie beyond scientific reach. We take it for granted, however, that physics can in principle tell us everything there is to know about the nature of physical matter. Physics tells us that matter is made of particles and fields, which have properties such as mass, charge, and spin. Physics may not yet have discovered all the fundamental properties of matter, but it is getting closer. 
Yet there is reason to believe that there must be more to matter than what physics tells us. Broadly speaking, physics tells us what fundamental particles do or how they relate to other things, but nothing about how they are in themselves, independently of other things. 
Charge, for example, is the property of repelling other particles with the same charge and attracting particles with the opposite charge. In other words, charge is a way of relating to other particles. Similarly, mass is the property of responding to applied forces and of gravitationally attracting other particles with mass, which might in turn be described as curving spacetime or interacting with the Higgs field. These are also things that particles do or ways of relating to other particles and to spacetime. 

Conscious experiences are just the kind of things that physical structure could be the structure of.

In general, it seems all fundamental physical properties can be described mathematically. Galileo, the father of modern science, famously professed that the great book of nature is written in the language of mathematics. Yet mathematics is a language with distinct limitations. It can only describe abstract structures and relations. For example, all we know about numbers is how they relate to the other numbers and other mathematical objects—that is, what they "do," the rules they follow when added, multiplied, and so on. Similarly, all we know about a geometrical object such as a node in a graph is its relations to other nodes. In the same way, a purely mathematical physics can tell us only about the relations between physical entities or the rules that govern their behavior. 
One might wonder how physical particles are, independently of what they do or how they relate to other things. What are physical things like in themselves, or intrinsically? Some have argued that there is nothing more to particles than their relations, but intuition rebels at this claim. For there to be a relation, there must be two things being related. Otherwise, the relation is empty—a show that goes on without performers, or a castle constructed out of thin air. In other words, physical structure must be realized or implemented by some stuff or substance that is itself not purely structural. Otherwise, there would be no clear difference between physical and mere mathematical structure, or between the concrete universe and a mere abstraction. But what could this stuff that realizes or implements physical structure be, and what are the intrinsic, non-structural properties that characterize it? This problem is a close descendant of Kant's classic problem of knowledge of things-in-themselves. The philosopher Galen Strawson has called it the hard problem of matter. 
It is ironic, because we usually think of physics as describing the hardware of the universe—the real, concrete stuff. But in fact physical matter (at least the aspect that physics tells us about) is more like software: a logical and mathematical structure. According to the hard problem of matter, this software needs some hardware to implement it. Physicists have brilliantly reverse-engineered the algorithms—or the source code—of the universe, but left out their concrete implementation. 
The hard problem of matter is distinct from other problems of interpretation in physics. Current physics presents puzzles, such as: How can matter be both particle-like and wave-like? What is quantum wavefunction collapse? Are continuous fields or discrete individuals more fundamental? But these are all questions of how to properly conceive of the structure of reality. The hard problem of matter would arise even if we had answers to all such questions about structure. No matter what structure we are talking about, from the most bizarre and unusual to the perfectly intuitive, there will be a question of how it is non-structurally implemented. 
Indeed, the problem arises even for Newtonian physics, which describes the structure of reality in a way that makes perfect intuitive sense. Roughly speaking, Newtonian physics says that matter consists of solid particles that interact either by bumping into each other or by gravitationally attracting each other. But what is the intrinsic nature of the stuff that behaves in this simple and intuitive way? What is the hardware that implements the software of Newton's equations? One might think the answer is simple: It is implemented by solid particles. But solidity is just the behavior of resisting intrusion and spatial overlap by other particles—that is, another mere relation to other particles and space. The hard problem of matter arises for any structural description of reality no matter how clear and intuitive at the structural level. 
Like the hard problem of consciousness, the hard problem of matter cannot be solved by experiment and observation or by gathering more physical detail. This will only reveal more structure, at least as long as physics remains a discipline dedicated to capturing reality in mathematical terms. 



Might the hard problem of consciousness and the hard problem of matter be connected? There is already a tradition for connecting problems in physics with the problem of consciousness, namely in the area of quantum theories of consciousness. Such theories are sometimes disparaged as fallaciously inferring that because quantum physics and consciousness are both mysterious, together they will somehow be less so. The idea of a connection between the hard problem of consciousness and the hard problem of matter could be criticized on the same grounds. Yet a closer look reveals that these two problems are complementary in a much deeper and more determinate way. One of the first philosophers to notice the connection was Leibniz all the way back in the late 17th century, but the precise modern version of the idea is due to Bertrand Russell. Recently, contemporary philosophers including Chalmers and Strawson have rediscovered it. It goes like this. 
The hard problem of matter calls for non-structural properties, and consciousness is the one phenomenon we know that might meet this need. Consciousness is full of qualitative properties, from the redness of red and the discomfort of hunger to the phenomenology of thought. Such experiences, or "qualia," may have internal structure, but there is more to them than structure. We know something about what conscious experiences are like in and of themselves, not just how they function and relate to other properties. 
For example, think of someone who has never seen any red objects and has never been told that the color red exists. That person knows nothing about how redness relates to brain states, to physical objects such as tomatoes, or to wavelengths of light, nor how it relates to other colors (for example, that it's similar to orange but very different from green). One day, the person spontaneously hallucinates a big red patch. It seems this person will thereby learn what redness is like, even though he or she doesn't know any of its relations to other things. The knowledge he or she acquires will be non-relational knowledge of what redness is like in and of itself. 
This suggests that consciousness—of some primitive and rudimentary form—is the hardware that the software described by physics runs on. The physical world can be conceived of as a structure of conscious experiences. Our own richly textured experiences implement the physical relations that make up our brains. Some simple, elementary forms of experiences implement the relations that make up fundamental particles. Take an electron, for example. What an electron does is to attract, repel, and otherwise relate to other entities in accordance with fundamental physical equations. What performs this behavior, we might think, is simply a stream of tiny electron experiences. Electrons and other particles can be thought of as mental beings with physical powers; as streams of experience in physical relations to other streams of experience. 



Manuel Litran / Paris Match via Getty Images


This idea sounds strange, even mystical, but it comes out of a careful line of thought about the limitations of science. Leibniz and Russell were determined scientific rationalists—as evidenced by their own immortal contributions to physics, logic, and mathematics—but equally deeply committed to the reality and uniqueness of consciousness. They concluded that in order to give both phenomena their proper due, a radical change of thinking is required.
And a radical change it truly is. Philosophers and neuroscientists often assume that consciousness is like software, whereas the brain is like hardware. This suggestion turns this completely around. When we look at what physics tells us about the brain, we actually just find software—purely a set of relations—all the way down. And consciousness is in fact more like hardware, because of its distinctly qualitative, non-structural properties. For this reason, conscious experiences are just the kind of things that physical structure could be the structure of. 
Given this solution to the hard problem of matter, the hard problem of consciousness all but dissolves. There is no longer any question of how consciousness arises from non-conscious matter, because all matter is intrinsically conscious. There is no longer a question of how consciousness depends on matter, because it is matter that depends on consciousness—as relations depend on relata, structure depends on realizer, or software on hardware. 
One might object that this is plain anthropomorphism, an illegitimate projection of human qualities on nature. After all, why do we think that physical structure needs some intrinsic realizer? Is it not because our own brains have intrinsic, conscious properties, and we like to think of nature in familiar terms? But this objection does not hold. The idea that intrinsic properties are needed to distinguish real and concrete from mere abstract structure is entirely independent of consciousness. Moreover, the charge of anthropomorphism can be met by a countercharge of human exceptionalism. If the brain is indeed entirely material, why should it be so different from the rest of matter when it comes to intrinsic properties? 



This view, that consciousness constitutes the intrinsic aspect of physical reality, goes by many different names, but one of the most descriptive is "dual-aspect monism." Monism contrasts with dualism, the view that consciousness and matter are fundamentally different substances or kinds of stuff. Dualism is widely regarded as scientifically implausible, because science shows no evidence of any non-physical forces that influence the brain. 
Monism holds that all of reality is made of the same kind of stuff. It comes in several varieties. The most common monistic view is physicalism (also known as materialism), the view that everything is made of physical stuff, which only has one aspect, the one revealed by physics. This is the predominant view among philosophers and scientists today. According to physicalism, a complete, purely physical description of reality leaves nothing out. But according to the hard problem of consciousness, any purely physical description of a conscious system such as the brain at least appears to leave something out: It could never fully capture what it is like to be that system. That is to say, it captures the objective but not the subjective aspects of consciousness: the brain function, but not our inner mental life. 

In order to give both phenomena their proper due, a radical change of thinking is required.

Russell's dual-aspect monism tries to fill in this deficiency. It accepts that the brain is a material system that behaves in accordance with the laws of physics. But it adds another, intrinsic aspect to matter which is hidden from the extrinsic, third-person perspective of physics and which therefore cannot be captured by any purely physical description. But although this intrinsic aspect eludes our physical theories, it does not elude our inner observations. Our own consciousness constitutes the intrinsic aspect of the brain, and this is our clue to the intrinsic aspect of other physical things. To paraphrase Arthur Schopenhauer's succinct response to Kant: We can know the thing-in-itself because we are it. 
Dual-aspect monism comes in moderate and radical forms. Moderate versions take the intrinsic aspect of matter to consist of so-called protoconscious or "neutral" properties: properties that are unknown to science, but also different from consciousness. The nature of such neither-mental-nor-physical properties seems quite mysterious. Like the aforementioned quantum theories of consciousness, moderate dual-aspect monism can therefore be accused of merely adding one mystery to another and expecting them to cancel out. 
The most radical version of dual-aspect monism takes the intrinsic aspect of reality to consist of consciousness itself. This is decidedly not the same as subjective idealism, the view that the physical world is merely a structure within human consciousness, and that the external world is in some sense an illusion. According to dual-aspect monism, the external world exists entirely independently of human consciousness. But it would not exist independently of any kind of consciousness, because all physical things are associated with some form of consciousness of their own, as their own intrinsic realizer, or hardware. 



Manuel Litran / Paris Match via Getty Images





As a solution to the hard problem of consciousness, dual-aspect monism faces objections of its own. The most common objection is that it results in panpsychism, the view that all things are associated with some form of consciousness. To critics, it's just too implausible that fundamental particles are conscious. And indeed this idea takes some getting used to. But consider the alternatives. Dualism looks implausible on scientific grounds. Physicalism takes the objective, scientifically accessible aspect of reality to be the only reality, which arguably implies that the subjective aspect of consciousness is an illusion. Maybe so—but shouldn't we be more confident that we are conscious, in the full subjective sense, than that particles are not? 
A second important objection is the so-called combination problem. How and why does the complex, unified consciousness of our brains result from putting together particles with simple consciousness? This question looks suspiciously similar to the original hard problem. I and other defenders of panpsychism have argued that the combination problem is nevertheless not as hard as the original hard problem. In some ways, it is easier to see how to get one form of conscious matter (such as a conscious brain) from another form of conscious matter (such as a set of conscious particles) than how to get conscious matter from non-conscious matter. But many find this unconvincing. Perhaps it is just a matter of time, though. The original hard problem, in one form or another, has been pondered by philosophers for centuries. The combination problem has received much less attention, which gives more hope for a yet undiscovered solution. 
The possibility that consciousness is the real concrete stuff of reality, the fundamental hardware that implements the software of our physical theories, is a radical idea. It completely inverts our ordinary picture of reality in a way that can be difficult to fully grasp. But it may solve two of the hardest problems in science and philosophy at once. 




Hedda Hassel Mørch is a Norwegian philosopher and postdoctoral researcher hosted by the Center for Mind, Brain, and Consciousness at NYU. She works on the combination problem and other topics related to dual-aspect monism and panpsychism.













Biology | Neuroscience
Is There Awareness Behind Vegetative States?
The answer to a simple question may show if someone's really "home."
By Julie Sedivy
♦ 

Imagine that a loved one, let's say your brother, has suffered a serious brain injury. After languishing in a coma, he finally "emerges"—that is, he cycles between sleep and wakefulness, yanks his hand away when it's pricked, is startled by loud noises, and so on. But it's not clear that he's ever truly awake; his eyes are open, but they rove around aimlessly. He can't communicate or follow instructions, even simple ones like "Squeeze my hand" or "Blink if you can hear me." Does your brother still inhabit his body? 
Our notion of what it means to retain a self may boil down to Descartes' pithy "I think, therefore I am." Selfhood can withstand many assaults: paralysis, memory impairment, blindness, even loss of language. But the loss of awareness—the ability to be conscious of our experiences and to reflect on them—seems to cut away at something truly fundamental. 



agsandrew


A substantial number of people who come out of a coma remain, sometimes for decades, in a persistent vegetative state. These patients show no overt evidence of being conscious of their surroundings, of who they are, or what they perceive or feel. They appear to be reduced to a bundle of reflexes. Yet, family members sometimes claim that their loved ones are "in there." For example, Paul Tremblay, whose son Jeff has been deemed by doctors to have been vegetative for more than 16 years, developed a weekly ritual of loading him up in a wheelchair and taking him to the movies, believing that Jeff was able to follow and enjoy the plots of movies. Is this mere wishful thinking? 
By definition, the behavior of vegetative patients offers no signs of conscious mental activity. But what if we were to look very closely at the activity in their brains? Could evidence of consciousness be found there? Adrian Owen, one of the world's leading researchers on disorders of consciousness, described in a Scientific American article how stunned he was when he placed one such patient named "Kate" into a scanner and showed her photos of her friends and family; the activity in her brain was remarkably similar to that of intact, alert people looking at pictures of familiar faces. Here was a glimmer of evidence of awareness. 

The boundary between conscious and unconscious mental life is surprisingly murky.

But just a glimmer. The trick with using brain scans to infer conscious awareness is that it's not enough to find that the brain activity of unresponsive patients is similar to that of healthy, awake brains. As it turns out, much of the mental activity of healthy, awake people—even activity that we think of as highly intelligent—runs on automatic pilot, entirely bypassing conscious awareness. This makes it very easy to assume consciousness where there is none, whether in vegetative patients or, for that matter, in ourselves. 
Owen learned this lesson the hard way. He was initially intrigued to see activity in the speech perception areas of the brains of some vegetative patients in response to speech but not to non-speech sounds. But it soon became apparent that the same pattern of activity could be found in healthy subjects rendered unconscious under anesthesia—in fact, the activity was just as strong in the unconscious subjects. To truly probe for consciousness, scientists needed to find a cognitive task that couldn't be achieved without it. 
There is no clear scientific definition of consciousness, and the boundary between conscious and unconscious mental life is surprisingly murky. But for the most part, scientists agree that mental processes that are sustained, rather than fleeting, and that involve the goal-oriented control of attention, are especially likely to involve consciousness. Here's an example: I'm going to ask you a question, and if the answer is no, imagine playing tennis, and if the answer is yes, imagine walking around the rooms of your house. Now, is your name Mike? 
It's hard to see how a task like this could be performed on mere reflex. It requires you to understand the instructions, to correctly answer the question, and to conjure up specific imagery while remembering that tennis is associated with no and walking around your house with yes. And yet, Owen and his colleagues have found that about one out of every five vegetative patients they have tested is able to accurately answer questions like these. (This is evident because thinking about tennis activates different regions of the brain than thinking about navigating through a house.) Some patients can even use this task to communicate about their inner lives; in a particularly moving moment caught on camera by the BBC, Scott Routley was able to convey that he was not in pain. 
These tests appear to offer evidence of consciousness in a small but meaningful number of patients diagnosed as vegetative. But what of the majority whose brain scans on these tests fail to show evidence of awareness? It's hard to know what this means, because such patients could be aware, but simply missing some of the brain functions that are needed to perform the task—perhaps they no longer understand language, or their working memory is broken to the point that they can't retain the instructions long enough. 
Hardest of all is this question: If patients like Kate, Jeff, and Scott are in fact aware, what exactly are they experiencing? Is consciousness a unified phenomenon that underlies our experiences, binding them into a meaningful interpretation that can be observed by a higher self? If so, then the awareness of some vegetative patients is likely to be much like our own. But if the nature of consciousness is fragmented and ephemeral, then these patients may exist in a different state altogether, hovering in a liminal dream-like existence. To understand the inner lives of unresponsive patients, scientists will need to venture into one of the most dimly lit corners of the science of the mind. 
In the meantime, Paul Tremblay takes his son to the movies. Brain scans by Owen and his colleagues revealed that his moment-by-moment brain activity while watching a short Hitchcock film was organized and much like the brain activity of healthy subjects. To his father, Jeff is Jeff. In an interview with Maclean's magazine, he acknowledged "He's different but he's still, we've accepted, this is Jeff. We love him just the same." 



Julie Sedivy teaches at the University of Calgary. She is the author of Language in Mind: An Introduction to Psycholinguistics and the co-author of Sold on Language: How Advertisers Talk to You and What This Says About You. She is on Twitter as @soldonlanguage.

This article originally appeared on our blog, Facts So Romantic, in August 2015.


Lead image: irenetinta / Getty Images









Chapter Two
Morals











Biology | Animals
What the Rat Brain Tells Us About Yours
The evolution of animal models for neuroactive medicine.
By Alla Katsnelson
♦ 

A little more than a decade ago, Mike Mendl developed a new test for gauging a laboratory rat's level of happiness. Mendl, an animal welfare researcher in the veterinary school at the University of Bristol in England, was looking for an objective way to tell whether animals in captivity were suffering. Specifically, he wanted to be able to measure whether, and how much, disruptions in lab rats' routines—being placed in an unfamiliar cage, say, or experiencing a change in the light/dark cycle of the room in which they were housed—were bumming them out. 
He and his colleagues explicitly drew on an extensive literature in psychology that describes how people with mood disorders such as depression process information and make decisions: They tend to focus on and recall more negative events and to judge ambiguous things in a more negative way. You might say that they tend to see the proverbial glass as half-empty rather than half-full. "We thought that it's easier to measure cognitive things than emotional ones, so we devised a test that would give us some indication of how animals responded under ambiguity," Mendl says. "Then, we could use that as a proxy measure of the emotional state they were in." 
First, they trained rats to associate one tone with something positive (food, of course) and a different tone with something negative (hearing an unpleasant noise). They also trained them to press a lever upon hearing the good tone. Then, for the test, they'd play an intermediate tone and watch how the animals responded. Rats have great hearing, and the ones whose cage life wasn't disturbed were pretty good judges of where the new tone fell between the other two sounds. If it was closer to the positive tone they'd hit the lever, and if it was closer to the negative one they'd lay off. But the ones whose routine had been tweaked over the past two weeks judged this auditory information more negatively. Essentially, their negative responses bled into the positive half of the sound continuum. 
Since Mendl published his so-called judgment bias task in 2004, it's been shown to work in at least 15 other species, including dogs, sheep, bees, and even us humans. Some scientists—himself included—have begun to ask whether there's a role for it beyond animal welfare. Considering that it probes one of the core clinical measures of depression, could it be used to evaluate the efficacy of much-needed new medicines for that condition? 



RAT FUNK: For years the pharmaceutical industry depended on the "forced swim test" to validate antidepressants. It showed rats given the drug would paddle longer in water before giving up than rats not doped.
Frank Greenaway / Getty Images


Drug discovery in neuroscience has hit a wall, with just 1 in 10 drugs tested in the final stage of clinical trials reaching the finish line of approval. With very few exceptions, no new types of drugs for mind disorders have been approved for decades. You might think drugs fail because they're found to be toxic, but most die in clinical trials because they aren't shown to work. Trace that back to the root of the problem, and one big stumbling stone along the drug development pathway is the point where animal tests—and most are done in rodents—wrongly predicted they would.
"We have lots of experience with this—15 to 20 years of failure," says Ricardo Dolmetsch, the global head of neuroscience at the Novartis Institutes for Biomedical Research. "I can name 14 or 15 examples [of tested drugs] that were just fantastic in animals and did not do anything at all in humans." 
Even as these failures have accrued, neuroscientists armed with increasingly potent tools for pinpointing the genes that play a role in psychiatric disorders and the brain circuits those genes control are getting closer to understanding the pathologies of these illnesses. As drug companies—which had largely abandoned or strongly curtailed their efforts in neuroscience and mental health over the past several years—begin to dip their toes back into the water, it seems a fitting time to ask whether modeling aspects of the human mind in rodents is even possible. 



One word explains why testing neuropsychiatric drugs in animal models is hard, and that word is language. If we want people to tell us how they feel, we ask them. Animals, of course, have to show us—and it turns out some of our widely used methods for guiding them to do so haven't been that great. That's particularly true for depression. How do we know a rat is depressed? 
An experiment called the "forced swim test" or "Porsolt test," after its founder Roger Porsolt, has been widely used since the late 1970s, at least by pharmaceutical companies and drug regulators. 
It's a remarkable story. Before the mid 20th century, treatments for mental or psychiatric disorders consisted primarily of psychotherapy or interventions like sleep cures, insulin shock therapy, surgeries such as lobotomy, or electrical brain stimulation—most prominently, electroconvulsive therapy. Quite suddenly, spurred by the accidental discovery of an antipsychotic drug called chlorpromazine in 1952, these conditions were re-imagined as chemical imbalances that could be corrected with a well-designed pill. 
Initially, these new compounds had their first runs in institutionalized patients. Medicinal chemists had a synthesizing frenzy, riffing off compounds that had seemed effective in the hopes of adjusting potency and side effect profiles, or of further expanding the cornucopia of psychoactive drugs. Soon, companies began freely giving out early-stage compounds to academic researchers who were up for observing how animals that ingested these novel chemical entities behaved. 

"I can name 15 examples that were just fantastic in animals and did not do anything at all in humans."

By the 1970s, companies were deeply invested in conducting their own behavioral testing, primarily in rodents. Anti-anxiety medicines were big sellers, and there was a handful of ways to screen for them; for example, seeing whether experimental compounds could boost a rat's interest in exploring an unfamiliar environment, or its willingness to engage in a behavior that it had been conditioned to avoid. It's hard to say whether or how closely those tasks reflected anxiety as experienced by humans. Certainly, though, such drug testing drew on the relatively new fields of ethology and behaviorism, which generally assumed that behavioral principles gleaned from laboratory animals broadly applied to people. 
For depression, however, as well as for conditions like psychosis, the tests weren't very good because they relied too strongly on pharmacology. Give a rat a drug known to induce a state that seems to have features of the disorder, and then see if an experimental compound reverses the effect. The problem was, this system was inherently rigged to find drugs that worked by the same mechanisms that the inducing agents did. 
At the time, Porsolt was working at Synthelabo (later acquired by Sanofi), a French pharmaceutical company. While conducting a water maze experiment, he noticed that his rats had a propensity to just stop swimming in the middle of the task. He found it curious, and it reminded him of the work of another researcher, Martin Seligman. A few years earlier, Seligman had found that dogs trapped in adverse situations from which they couldn't escape eventually stopped trying—a phenomenon he termed "learned helplessness." What Porsolt observed with his rats looked similar. 
Porsolt soon designed the forced swim test, which made its debut in a two-page report in Nature in 1977. It's very easy to perform. Researchers place a mouse or rat in a beaker of water from which there is no exit. Invariably, after a few minutes, it stops struggling to escape and simply hangs in the water, immobile. Animals given antidepressant drugs before undergoing the procedure a second time, Porsolt reported, struggle longer before apparently succumbing to what he poetically called "behavioral despair." 
Pretty much right away, academic researchers studying depression and pharmaceutical companies developing new medicines began using it in full force. "There's not a single dossier of a newly introduced antidepressant in the last 20 years where they have not used the swimming test," Porsolt says now. "It's become the standard test." 
The assay gave what you'd call the correct answer for the early antidepressants available in the 1970s, on which Porsolt validated it—that is, the drugs that kept the animals afloat longer also relieved depression in people. And by most accounts it worked great in predicting efficacy for the first serotonin reuptake inhibitors, specifically Prozac (fluoxetine), which was approved in the United States in 1987. 
Porsolt concedes he made a hugely anthropomorphic leap in the reasoning that he attributed to the animals' experiences. But he doesn't see it as a problem. "You know, I'm a pragmatist," he says. "There's nothing wrong with engaging in anthropomorphism, provided you put it to the test." 
What convinced Porsolt his assay was the real deal is the antidepressants available then tended to make animals sluggish, but in the context of the test they had the opposite effect. "That was the first big surprise—that you give the classical antidepressants of the time—tricyclics like imipramine—at doses which otherwise are sedative, and the animals become active again," he says. Because inducing this state didn't require pre-administering drugs, like earlier tests had, he believed his behavioral assay could in theory identify antidepressant effects in any type of chemical compound. 



There is little to quibble with in the initial models. Neurobiology was in a larval state and the "animal assays were really smart," says Steven E. Hyman, director of the Stanley Center for Psychiatric Research at the Broad Institute of the Massachusetts Institute of Technology and Harvard, and director of the National Institutes of Mental Health from 1996 to 2001. But when psychiatric drugs went mass-market in the 1980s, companies doubled down on the strategy of relying on simple behavioral tests, like the forced swim test, to screen new compounds. 
For a while, Hyman says, the drugs improved in terms of safety and side effects. Their efficacy, however, generally didn't, and it soon became clear that behavioral tests didn't help identify new types of chemical compounds. Yet companies kept turning the same animal model crank. "They were the accepted models and they were quick and easy to do," says Mark Tricklebank, who founded and, until a few years ago, directed Eli Lilly's Centre for Cognitive Neuroscience, an industry and academic partnership to improve animal models of cognition. "Too much focus on results and deadlines tends to push people to worry only about collecting data and not its quality," he says. 
Today, 30 years after Prozac arrived on the market, it's remarkable how few novel types of antidepressants have been found. (Other psychiatric conditions, like schizophrenia, have fared no better.) The fact is, the forced swim test is a poor stand-in for depression. There's just no way to conclude why rats or mice cease swimming in the bucket. "It may be that those are actually the wise rodents, because they're conserving energy once they realize they're not drowning," says Hyman. "If you give them imipramine or a drug like it and they struggle longer, why is that better?" Emma Robinson, a psychopharmacologist at the University of Bristol, agrees. The Porsolt test may have been key to the development of Prozac and second-generation antidepressants, she says, but, "to be honest, I don't think we know what the forced swim test is measuring. It's given a lot of false data." 



A big part of the difficulty in judging what such behavioral tests in animals do and don't reveal about the human mind comes down to what we might call human errors of implementation. Take another ubiquitously used behavioral test, the Morris water maze, in which researchers release a rat or mouse into a pool of water, then time how long it takes to find a submerged platform to stand on. Normally, over several trials the animal gets quicker at putting something solid beneath its feet, revealing its use of spatial memory. 
But the test has also been adopted as a stand-in for clinically relevant memory loss, like the kind experienced in Alzheimer's disease—even though there is no evidence that it is applicable there. "It's a measure of fear-based, fear-motivated escape, which is of very little relevance to the disorders for which it's regularly used," says Joseph Garner, a neuroscientist at Stanford University who studies animal models. In fact, notes Garner, although the Morris water maze is one of the most widely used tests in behavioral research, a 2007 study found that an animal's performance correlates strongly with visual acuity, suggesting it is as much a test of vision as of memory. 
Behavioral tests used in pain research provide another good example. One standard measure for whether an analgesic is effective in a mouse is how quickly it withdraws its paw from a heat lamp. Reflexively withdrawing from heat, though, is very different from the debilitating pain that generally troubles people—which tends to be chronic rather than acute, and to come from within rather than from outside the body. If a drug can treat one type of pain in an animal, says Jeffrey Mogil, a pain researcher at McGill University in Montreal, that doesn't mean it will work for the other types in humans. "It's a mismatch of the symptoms in humans and the choice of symptoms in animals," Mogil says. This dissociation has beguiled the search for novel pain medicines, but he explains that we shouldn't be surprised. "We use that test because it's convenient for us, and reliable." 
With all these problems, a growing cadre of researchers says that the use of animal models in psychiatry needs a major rethink because the kinds of behavioral tests the field has relied on to probe rodent minds simply don't match up to the human mind. Behaviors in rodents and humans have been shaped by very different evolutionary trajectories, notes Hyman, and assuming they are supported by the same brain circuitry simply because they appear similar to us is "in the same intellectual ballpark as [classifying] insects, birds, and bats together because they all fly." 

"Too much focus on results and deadlines tends to push people to worry only about collecting data and not its quality."

Does that mean that it's impossible to come up with tasks that do allow researchers to compare an animal's mental state directly with a human's? Perhaps not. Over the past few decades, neuropsychiatrists have developed standardized batteries of human cognitive tasks for probing processes like attention and impulsivity, with the aim of better understanding the cluster of symptoms that come together in any given disease. Because a strong focus was placed on tasks that don't rely on language, the field was able to build on that work by "reverse-translating" them—basically, recreating them as closely as possible in animal models. 
Meanwhile, advances in neuroimaging—both in humans and in rodents—mean it's possible to make sure that the same brain regions and the same circuits are engaged in the human and the animal model. "If we see the same neural circuits involved in the rat as in the human, or if some particular task or drug strengths communication between different parts of the brain, then we know we have a translatable task," says Holly Moore, a neuroscientist at Columbia University who uses animal models to study the neural basis of schizophrenia. "We just now have the imaging chops to do that." 
A few years ago, a grant from Pfizer launched Robinson—the University of Bristol psychopharmacologist—on an effort to build on Mendl's rat happiness task and develop one that's more suitable for drug testing. Rather than having the animals judge the similarity between different tones, Robinson has them dig for food, a task more relevant to their lives. She trains them to associate a specific digging material—say, sawdust, or sand—with the food reward. When asked to choose between the two types of digging material later, their choice is colored by whether or not they were having a good day, by lab rat standards, when they trained in it. Her lab has already begun to explore how to compare these rats' measured mental states to those of people trained to do a human version of such a task. But Robinson admits there's a lot left to do in determining what it means to make this match-up. 
For now, nobody can say for sure whether all of this activity will produce novel medicines for people who need them. In schizophrenia, there have already been some positive effects for the field, Moore says. "I see the literature moving toward a more thoughtful approach to behavioral tasks and more widely questioning assumptions underlying both the human and animal research," she says. "I do know we won't be wasting as much time as we have been." 
It could be the way forward is not to abandon rodent behavioral tests, but to refine them. Garner argues that for researchers who are well-versed in rat or mouse behavior, there is no a priori impediment to designing studies in which rodent cognitive faculties are directly compared to human ones. Many behaviors are in fact evolutionarily conserved, he says, and brain imaging or other techniques can be used to ensure that the same neural circuits are engaged across species. Even if the approach works, however, it's unclear whether drug companies will follow that route, since such tests would most likely be more complicated and time-intensive—and more expensive. 
Novartis, for one, is taking a different route. The company plans to test new drugs in rodents. But rather than futz with behavioral tests that make assumptions about rodent minds and human diseases, they will use the animals just to determine that the drug hits the cells or brain regions it has been intended for. As for testing whether or not a drug treats some component of a psychiatric disorder, Novartis is going back to the future—that is, straight to humans. Dolmetsch guesses that other companies in psychiatry are doing the same. Some of the companies' leads will come through developing better versions of compounds like ketamine, serendipitously found to have psychiatric effects in humans. Others will come through dissecting neural circuits in people with rare mutations that point to some mechanism underlying brain and mind diseases. 
"I think studying animal behavior is still valuable for its own sake," says Dolmetsch. "It's just not necessarily the best way of modeling psychiatric diseases." 




Alla Katsnelson is a freelance science writer with a doctorate in neuroscience. She lives in Northampton, Massachusetts.













Ideas | Climate
To Fix the Climate, Tell Better Stories
The missing climate change narrative.
By Michael Segal
♦ 

Here are two sets of statements from far-distant opposites in the climate change debate.
The first is from Naomi Klein, who in her book This Changes Everything paints a bleak picture of a global socioeconomic system gone wrong: "There is a direct and compelling relationship between the dominance of the values that are intimately tied to triumphant capitalism and the presence of anti-environment views and behaviors." 
The second is from Larry Bell, professor of architecture and climate skeptic, whom Klein quotes in her book. He argues that climate change "has little to do with the state of the environment and much to do with shackling capitalism and transforming the American way of life ...". 
Let us put aside whether we agree or disagree with these statements or are offended by them. What concerns us is their scope: Both attach a breadth of narrative to climate change that far exceeds what is, at base, a relatively well-understood set of climate mechanics (human-produced carbon emissions are changing the composition of our atmosphere and warming the planet) and a well-developed set of solutions (renewable and possibly nuclear energy, efficiency improvements, consumer education, and the appropriate pricing of carbon). 
Each side of the climate debate accuses the other of exaggeration and suffers from its own. Skeptics ignore basic climate facts and perils, while those who point their finger at capitalism itself discard one of the best tools at their disposal. It is in part market forces, after all, that have produced a thousand-fold reduction in the cost of solar power over the past three decades (guided by policy). 



hollywood does dystopia: Of the ready-made narratives we have available to understand the climate, utopia and dystopia are two of the strongest.
Disney / Pixar


There is a swirl of other, orthogonal narratives, too. American conservatives worry about global agencies interfering in domestic affairs. Some Europeans mistakenly dismiss climate change denial as uniquely American: In December of 2015, Richard Branson told CNN that skepticism is not something he has to deal with in Europe, despite the fact that the percentage of people who believe climate change is caused by human emissions is higher in the United States than in the United Kingdom.1
The climate conversation can sometimes feel like a shouting match in a roomful of children wearing earplugs. Each narrative doesn't just oppose the next but is deeply incompatible with it. Partly this is a natural result of what is at stake. But it is also because something is missing. We have allowed our political, national, economic, and cultural narratives free play in the modern climate change debate. But where, in this shouting match, are the narratives from science itself? Where is the science teacher? 



Peter Sheridan Dodds has a nickname for us humans: Homo narrativus. Dodds, a professor at the University of Vermont, uses mathematics to study social networks. He has argued that people see the stories of heroes and villains, where there are really just networks and graphs. It's our desire for narrative, he says, that makes us believe that something like fame is the result of merit or destiny and not a network model quirk. 
That we love heroes is something we can all intuitively understand. Less obvious is that climate, too, has a considerable narrative weight and is something we understand through storytelling. "Climate cannot be experienced directly through our senses," writes Mike Hulme in his book Why We Disagree about Climate Change. "Unlike the wind which we feel on our face or a raindrop that wets our hair, climate is a constructed idea that takes these sensory encounters and builds them into something more abstract." That abstraction has a moral and a historical quality: from the portrayal of flood myths as part of our relationship with the divine, to the birth of fictional monsters like Frankenstein in the wake of climate events, to our association of storms and earthquakes with emotional states—climate has always been more than a mathematical average of weather. In fact, Hulme says, it is only recently, and primarily in the West, that the cultural and physical meanings of climate have become so separated. 

There's no doubt that climate change presents a real and severe danger.

That separation has contributed to a narrative vacuum—and, like nature itself, people abhor a vacuum. We fill it with the narratives we have at hand, even if they are powerfully at odds with each other. This goes some way to understanding the vitriol of the climate debate. "The ideological freightage we load onto interpretations of climate and our interactions with it," writes Hulme, "are an essential part of making sense of what is happening around us today in our climate change discourses." Stories about the virtues and evils of capitalism, the role of divine control, nationalist values, and so on, are not so much maliciously inserted into what could be a sober conversation but are an inevitable response to a story that is incomplete without them. 
Faced with an absence, we revert to old narratives, and there are few older than utopia and dystopia. The skeptic storyline of the rise of a dictatorial world government usurping American values must be considered not as a unique reply to climate change but as the latest instance of a well-established dystopic trope, stoked by the climate narrative vacuum. Something similar can be said for attacks on the capitalist enterprise from the left. The public, for its part, is served visions of an apocalyptic future, whether it's from politicians or from Hollywood—and, simultaneously, the utopianism of far-distant science fiction, which as a category is consumed in greater quantity than science journalism and which reflects and encourages what sociologists call "optimism bias" or "technosalvation." These utopian instincts are strengthened by a historical data point obvious to all: Our species has survived every obstacle we've encountered, and we are still here. 
Utopia and dystopia can reach even the highest levels. From the White House are echoes of dystopian claims that climate change is a hoax orchestrated by foreign powers. And Mattias Hjerpe and Björn-Ola Linnér, from the Centre for Climate Science and the Department of Water and Environmental Studies at Linköping University, in Sweden, point out that utopian elements can regularly be found in planning documents of the Intergovernmental Panel on Climate Change (IPCC), a United Nations body. The IPCC's special reports on emissions scenarios, for example, "all envision a radical narrowing of global income gaps between rich and poor countries. This vision is outright utopian thought."2 Not only has the per capita income gap between rich and poor countries grown over most of the past three decades, but the economic development required for a significant narrowing of the gap seems at odds with the IPCC's own sustainability goals. That, say Hjerpe and Linnér, "is utopian in the sense that it is not a projection based on current trends, but rather an extrapolation of current policy goals." 
Both dystopian and utopian narratives have their own rationales and evidentiary support, and there's no doubt that climate change presents a real and severe danger. But in the public realm, these types of narratives also have a tendency to be useless. They leave the public spectating a stalled debate between extremes and generate ample motivation to check out. 



This is not to say that the climate conversation is irreparably broken. It's true we can't take away those unhelpful narratives that have already been attached to it. But we can add new ones, and some narratives are more powerful than others. Scientific narratives, if they're done right, are some of the most powerful of all. They teach us more than facts, mechanisms, and procedures. They convey a worldview of skeptical empiricism and indefinite revision, show us how to negotiate the boundary between our rational and emotional selves, teach us to suspend judgment and consider all the possibilities, and remind us that a belief in objective truth is a deep kind of optimism with massive dividends. Perhaps most important of all, they situate us in the world. 
The successful assimilation of broad narratives from astronomy and genetics reminds us how powerful science narrative can be. We think of ourselves today as genetic machines, carrying around an adaptive program, which we inherit and pass on, doing so on this one habitable planet among countless others in a universe with a finite age. These facts have become intuitions and a part of our identity. The goal of climate change coverage should be a similar creation of intuition from fact. Intuition that our planet is a dynamic thing, that its environment is highly interconnected, that it has been remade many times by things living and dead. 
Are we getting that done? The media has communicated the basic facts behind climate change well enough: the famous line graph of rising carbon dioxide levels, the 300 parts per million line in the sand, the northward migration of adapting species, and the endangerment of those left behind. But the narrative around these facts is more obscure. In the words of social scientists Susanne Moser and Lisa Dilling, science communicators "often assume that a lack of information and understanding explains the lack of public concern and engagement, and that therefore more information and explanation is needed to move people to action."3 Many of these facts are, by now, either uncontested or unsurprising. It is the narratives around them that are missing. 



not so fast: National narratives can obscure climate debate, as when Richard Branson incorrectly suggested that climate change skepticism is less prevalent in the U.K. than in the U.S.
Wikimedia Commons


Kirk Johnson, director of the Smithsonian National Museum of Natural History, puts it this way: "If you look at how the media treats scientific discoveries, they'll go to the wonder. ... [They'll say] 'here's this thing that's been discovered,' not the process of how we figured it out. And I think that understanding of how we know what we know is so critical ... If you don't help people understand what those processes are, [if] you just say 'here's the answer,' now they can go onto the web and dial up an alternate answer. I think we're seeing an erosion of credibility of science to the public because of this huge flood of technology and information."
This erosion is essential to understanding the modern climate debate. In the words of the philosopher Richard Rorty, "We understand knowledge when we understand the social justification of belief, and thus have no need to view it as accuracy of representation."4 In the absence of social justification, the public ends up being called on to be the judge of accuracy of representation—in other words, of scientific content. Sure enough, quasi-scientific arguments based on misinterpreted data fragments abound in the skeptic community.5 Why did temperatures stay flat during World War II, despite an emissions increase? Was there an 18-year hiatus in temperature rise? The only reasonable answers to these questions lie with the scientific community, but they will be ignored if that community hasn't earned an authoritative public voice. That is especially true when the answer is, "We're not sure yet." 

Faced with an absence, we revert to old narratives, and there are few older than utopia and dystopia.

The question of authority is complicated further by the multidisciplinary nature of climate change. Authority within the sciences revolves tightly around narrow silos of expertise. As the academics Simon Shackley and Brian Wynne put it, "A common response by scientists to challenges to their authority is to demarcate the realm within which their expertise is autonomous."6 In other words, there is a retreat to the silo. But at the policy level, climate change involves atmospheric chemistry, plant and ocean biology, solar physics, geochemistry, soil science, and glaciology, among other disciplines. Building authority in climate science is therefore not well served by the tendency of the scientist to retreat to home turf. Here, too, narratives can help. 
Even scientists need to lure each other with narratives. The philosopher Rom Harré offers up that pillar of modern professional science, the scientific paper, as exhibit A. He argues that the three-part structure of the typical paper (hypothesis, results, and inductive support) is a post facto interpretation: "Anyone who has ever done any actual scientific research knows that this is a tale, a piece of fiction. The real-life unfolding of a piece of scientific research bears little resemblance to this bit of theatre."7 Speaking as both a former scientist and a former academic editor, I can attest to the truth of this statement. From the lab to the publisher's desk, narrative is constantly helping to organize, sell, and drive science. As Harré puts it, "Science must present a smiling face both to itself and to the world." If narrative is necessary for one scientist to convince another of his or her result, it's certainly necessary to engage and convince the public. 
The narrative questions around climate change are broad. What does it mean for there to be a scientific consensus? How is the scientific method properly applied to a system that resists experimentation? What does a complex system look like? What is the nature of risk and probability? Each has a direct bearing on the climate change conversation without necessarily being about climate change. They, and others like them, constitute a suprascientific narrative that is necessary for science to become culture. In a way, every good science story is a story about all of science and helps us understand every other science story. 
So let's tell more of them. 



Michael Segal is the editor in chief of Nautilus. 
This article is adapted from one originally published in the January 2017 issue of The South Atlantic Quarterly. 




References

1. Sir Richard Branson on climate change. www.cnn.com. (2015). Retrieved from http://www.cnn.com/videos/world/2015/12/13/climate-change-branson-harlow-nrcnn-intv.cnn 
2. Hjerpe, M. & Linnér, B. Utopian and dystopian thought in climate change science and policy. Futures41, 234-45 (2009). 
3. Moser, S.C. & Dilling, L. Communicating climate change: Closing the science-action gap. In Dryzek, J.S., Norgaard, R.B., & Schlosberg (Eds) The Oxford Handbook of Climate Change and Society Oxford University Press, New York, NY(2011). 
4. Rorty, R. Philosophy and the Mirror of Nature Princeton University Press, Princeton, NJ (1980). 
5. Meredith, C. 100 reasons why climate change is natural. www.express.co.uk (2012). 
6. Shackley, S. & Wynne, B. Representing uncertainty in global climate change science and policy: Boundary-Ordering devices and authority. Science Technology and Human Values21, 275-302 (1996). 
7. Harré, R. Some narrative conventions of scientific discourse. In Nash, C. (Ed.) Narrative in Culture: The Uses of Storytelling in the Sciences, Philosophy, and Literature New York: Routledge, New York, NY (1990). 












Ideas | Aliens
Do Aliens Have Inalienable Rights?
What ET teaches us about our moral obligations.
By Peter Singer
♦ 

Last January I was walking with my granddaughter along a beach near Melbourne when we noticed several people gathered around a rock pool, peering into the water. Wondering what had attracted their attention, we went over and saw that it was an octopus. If the spectators were interested in it, it also seemed interested in them. It came to the edge of the pool, one of its eyes directed at the people above, and stretched a tentacle out of the water, as if offering to shake hands. No one took up the offer but at least no one tried to capture the animal or turn it into calamari. That was pleasing because, as Peter Godfrey-Smith says in his recent book Other Minds: The Octopus, the Sea, and the Deep Origins of Consciousness, an octopus is "probably the closest we will come to meeting an intelligent alien." 
If we do ever meet an intelligent alien, even a tasty one, I hope we have sufficient ethical awareness to think of more than pleasing our palates or filling our stomachs. My view that this would be the wrong way to respond to such an encounter, however, leads to a deeper question: What moral status would extra-terrestrials have? Would we have obligations to them? Would they have rights? And would our answers depend on their intelligence? 
These questions bring to mind Steven Spielberg's celebrated 1982 movie E.T. the Extra-Terrestrial. In case you haven't seen the film, it features a friendly extraterrestrial who lands on Earth with some colleagues on a botanical research expedition and is accidentally left behind. E.T. is befriended by Elliott, a 10-year-old boy, and soon shows that he has a full range of human-like feelings, including homesickness. He also has greater compassion for other species than most humans do. In one memorable scene, Elliott, moved by feelings that come from E.T., liberates the frogs in his biology class. 



I use E.T. as a thought-experiment to challenge students to reconsider their speciesism—the still widely held assumption that the boundary of our species is also the boundary of beings with rights, or with interests that we ought to take seriously. So far, the only nonhumans we have encountered are animals, plants, fungi, and microscopic living things such as protozoans, bacteria, algae, and spirochetes. When we consider the moral status of these organisms, our thinking is likely to be biased by our own interests in using them, especially as sources of food, or in avoiding being made ill by them.


This is clearest when we think about how we ought to treat nonhuman animals. We have deeply embedded customs of killing and eating animals and using their skins for fur and leather. Many people can hardly imagine a meal without meat or other animal products. Religious and philosophical thinkers are as susceptible to bias as other people, and so most of them have justified this practice. In doing so, these thinkers have dug a broad gulf in our minds between ourselves and nonhuman animals. Even the term "nonhuman animals" sounds odd, because it implies that we are animals. It should not sound odd at all, because we have known, ever since Darwin, that we are animals. Yet we persist in thinking that we are a separate creation, that we alone are made in the image of God, or that we alone have an immortal soul.

It might be difficult to tell whether extraterrestrial life forms are capable of suffering or experiencing happiness.

E.T. challenges the moral significance of the species boundary both because we recognize in him a being with feelings very like ours, and because we have no prejudice against him based on a history of eating his kind, putting them in circuses for our amusement, or using them as beasts of burden. So if I ask my students, "Would it have been ethically permissible for scientists to kill E.T. and dissect him for the purposes of what would surely be extremely interesting scientific research?" they unanimously reject that idea. Some things that we could do to harm aliens, they concede, are wrong. If they accept that, then they must also accept that the sphere of proper ethical concern is not limited to members of the species Homo sapiens. 
Accepting that it would be wrong to kill and dissect E.T. is a first step in exploring our ethical obligations to extraterrestrial life, but it does not take us very far. Perhaps we have ethical obligations only to beings who have a high level of intelligence, self-awareness, or communicative ability, and if we ever discover extraterrestrial life lacking in these qualities, we will have no obligations to them. 
Once the species-barrier has been breached, however, it is not so easy to fall back on the requirement that a being pass some threshold for cognitive abilities in order to have rights. For then we have to consider human beings who fail that test—as both human infants, and humans with profound intellectual disability, do. Surely they have interests that need to be considered, whether or not they possess, or have the potential to develop, higher cognitive capacities. 
Most of us now accept that we have at least some obligations to avoid inflicting suffering on nonhuman animals, and the same would surely hold for any extraterrestrial beings who we discover to be capable of suffering. In my view, pain and suffering are equally bad, no matter what the species, or planetary origins, of the being suffering. The only thing that matters is the intensity and duration of the suffering. I would make a similar claim about pleasure and happiness. We can think of this as a form of equality—equal consideration for similar amounts of suffering or happiness. 



 SELF-AWARE SEA LIFE?: Peter Godfrey-Smith has said that an octopus is "probably the closest we will come to meeting an intelligent alien."
Sylke Rohrlach / Wikipedia


It might, however, be difficult to tell whether extraterrestrial life forms are capable of suffering or experiencing happiness. We recognize the suffering of other beings by analogy with our own. Where there is a nervous system sufficiently like ours, and behavior similar to our responses to pain, it is reasonable to assume that a nonhuman animal is experiencing pain. This reasoning is strengthened by our knowledge that we and nonhuman animals have a common evolutionary origin, because it is plausible that the mechanisms that we share with other animals work in similar ways. 
The further back our evolutionary divergence from another being was, the more difficult it is to establish whether consciousness is present. That is why an encounter with an octopus is so fascinating. The behavioral evidence, not only for consciousness but also for intelligence is very strong— just go to YouTube and search for "octopus intelligence" and you will find many videos of octopuses solving novel problems as well as learning how to do things by observing how another, more experienced, octopus goes about it. Yet our last common ancestor with the octopus was a worm-like creature living 600 million years ago, long before there were any minds at all on this planet. So mind has evolved twice—at least—on this planet. Perhaps if we do encounter extraterrestrial organisms that might have minds, we can use behavioral tests, as we do with the octopus and other cephalopods, to determine whether it is probable that they really are conscious. 
If there are reasonable grounds for believing that a being may be capable of suffering, we should, wherever possible, give that being the benefit of the doubt. That means applying the principle of equal consideration, but with a discount for uncertainty. 
Albert Schweitzer famously advocated an ethic of "reverence for life" and some deep ecologists hold that rivers and mountains have intrinsic value. We don't need to go to those lengths in order to see that the existence of another mind—another center of consciousness—places moral demands on us. If there is something that it is like to be another being, then we have a moral responsibility to avoid harming that conscious being, and, in so far as it is within our power and a reasonable use of our resources, to make that being's life go as well as possible. 



Peter Singer is a professor of bioethics at Princeton University and Laureate Professor at the University of Melbourne. His books include Animal Liberation, The Life You Can Save, The Most Good You Can Do and, most recently, Ethics in the Real World. 












Biology | Psychology
What Do Animals See in a Mirror?
A controversial test for self-awareness is dividing the animal kingdom.
By Chelsea WaldIllustration by Emmanuel Polanco
♦ 

The idea for a tool to probe the basis of consciousness came to Gordon G. Gallup, Jr. while shaving. "It just occurred to me," he says, "wouldn't it be interesting to see if other creatures could recognize themselves in mirrors?" 
Showing chimpanzees their reflections seemed like a fascinating little experiment when he first tried it in the summer of 1969. He didn't imagine that this would become one of the most influential—and most controversial—tests in comparative psychology, ushering the mind into the realm of experimental science and foreshadowing questions on the depth of animal suffering. "It's not the ability to recognize yourself in a mirror that is important," he would come to believe. "It's what that says about your ability to conceive of yourself in the first place." 
Gallup was a new professor at Tulane University in Louisiana, where he had access to the chimps and gorillas at what would later be known as the Tulane National Primate Research Center. The chimpanzees there had been caught as youngsters in Africa and shipped to America, where they were used mainly in biomedical research. By comparison, his experiment was far less invasive. He isolated two chimps in cages, and placed a mirror in each cage for eight hours at a time over 10 days. Through a hole in the wall, Gallup witnessed a shift in the chimps' behavior. First they treated the reflection like it was another chimp, with a combination of social, sexual, and aggressive gestures. But over time, they started using it to explore their own bodies. "They'd use the mirror to look at the inside of their mouths, to make faces at the mirror, to inspect their genitals, to remove mucous from the corner of their eyes," Gallup says. 
Gallup was sure that the chimps had learned to recognize themselves in the mirror, but he didn't trust that other researchers would be convinced by his descriptions. So he moved on to phase two of the experiment. He anesthetized the chimps, then painted one eyebrow ridge and the opposite ear tip with a red dye that the chimps wouldn't be able to feel or smell. If they truly recognized themselves, he thought he knew what would happen: "It seemed pretty obvious that if I saw myself in a mirror with marks on my face, that I'd reach up and inspect those marks." 
That's exactly what the chimps did. As far as Gallup was concerned, that was proof: "the first experimental demonstration of a self-concept in a subhuman form," he wrote in the resulting 1970 report in Science. "It was just clear as day," he remembers. "It didn't require any statistics. There it was. Bingo." 
But the result that really blew Gallup's mind came when he tested monkeys, and discovered that they did not do the same. The ability to recognize one's reflection seemed not to be a matter of learning abilities, with some species being slower than others. It was an issue of higher intellectual capacity. Gallup had obtained the first good evidence that our closest relatives share with us a kind of self-awareness or even consciousness, to the exclusion of other animals. Here, finally, was an experimental handle on a topic that had been the subject of speculation for millennia: What is the nature of human consciousness? 

Sidebar: DIY Mirror Test


Explore the self-awareness of your kid, puppy, or parakeet at home. Reactions vary according to your subject. Place a big, colorful sticker in a baby's hair away from a mirror, making sure the baby doesn't notice or feel it. Then take the baby to a mirror. The baby may point at the sticker, but they don't realize that it's on their own face staring back at them. When you remove it and show it to them, they'll be surprised. 

By the time the baby is 2 years old, however, it will undergo a cognitive shift in self-awareness, and easily use the mirror to locate the sticker. Puppies and kittens aren't so impressed with stickers, but often play with the image in the mirror that they do not recognize as their own reflection. Older dogs and cats tend to ignore their reflections, possibly because they have lost interest. 
Moshe Blank


Gallup wasn't the first to come up with the notion that it might be significant if a person or animal recognizes itself in the mirror. He would only later learn that Charles Darwin had shown mirrors to orangutans, but they didn't figure the mirror out, at least while he was watching. Darwin had also noted that, for their first few years, his children couldn't recognize themselves in their reflections. In 1889, German researcher Wilhelm Preyer became the first to posit a connection between mirror self-recognition and an inner sense of self in people. 
More than 50 years later, French psychoanalyst Jacques Lacan conceived of a childhood "mirror stage," in which mirrors contribute to the formation of the ego. By 1972, developmental psychologists started using mark tests similar to Gallup's to pin down the age at which children begin to recognize themselves in the mirror: 18 to 24 months.
Meanwhile Gallup, who moved to the University at Albany-SUNY, became interested in whether any non-primates could pass. In the early 1990s, he encouraged one of his Ph.D. students, Lori Marino, to explore the question. Working with Diana Reiss at Marine World Africa USA in California, Marino exposed two bottlenose dolphins at an aquarium to a mirror. Like the chimpanzees, the dolphins learned to use the mirror in a variety of ways, even "having sex in front of the mirror with each other, which we call our dolphin porno tapes," Marino says. The three researchers published the results, saying they were "suggestive" of mirror self-recognition. 
Still, they were missing the crucial mark test for another decade. The biggest hurdle was anatomical: The dolphins didn't have hands to touch a mark. But Reiss and Marino, by then at the New York Aquarium, designed a modified test. When marked with black ink on various parts of their bodies, the dolphins flipped and wriggled in an attempt to see it, convincing the researchers and many others that they recognized themselves.



For Reiss and Marino, the dolphin study was not only convincing, it was a call to action. They and others argue that passing the mirror test indicates a level of self-awareness that makes it unethical to keep a species in captivity. "These animals have at least some level of self-awareness, and if they do, they know where they are, they can be aware of the limitations of their physical environment," Marino says. She is now the science director for the Nonhuman Rights Project, which is attempting to gain legal rights for animals with higher-order cognitive abilities by getting courts to recognize them as "legal persons," and Reiss advocates for dolphin protection. Key to their arguments is the scientific evidence that chimps, elephants, cetaceans, and other animals are self-aware like humans. Not only can they suffer, but they can think to themselves, I am suffering.
Gallup, now in his 70s, mainly stays away from advocacy work but he likes to philosophize about what exactly mirror self-recognition shows, and why that capability might have evolved. Clearly, it has little to do with mirrors since aside from the occasional still pond, our distant ancestors would never have encountered their reflections. He's come to the conclusion that a pass of the mirror test indicates a profound level of consciousness that includes animals' ability to contemplate their own thoughts and experiences as well as to imagine what others could be thinking and experiencing. This ability is called "theory of mind." 
For support, he points to the fact that children start to demonstrate theory of mind at roughly around the same time that they start to recognize themselves in the mirror. "You have to be aware of yourself in the first place in order to begin to take into account what other people may know, want, or intend to do," he says. He notes that people with schizophrenia often cannot recognize themselves in the mirror, and they struggle with theory of mind as well. For example, compared to controls, schizophrenic individuals were less likely to understand a request hidden in a husband's statement to his wife, "I want to wear that blue shirt, but it's very creased." 

Povinelli calls this reasoning "folk psychology"—unscientific inferences made based on our own human experiences.

Gallup suggests that a powerful sense of self may have evolved because it helped great apes deal with complex social situations. "Intellectual prowess supplanted physical prowess as a means of achieving dominance," he says. And, he suggests that strong self-awareness may also entail death-awareness. "The next step, it seems to me logically, is to confront and eventually grapple with the inevitability of your own individual demise," he says. 
As for why dolphins and other non-primates recognize themselves in mirrors, Gallup isn't yet convinced they do. He suggests an alternative explanation for why his former student's dolphins wriggled in the mirror: to see marks on what they perceived as another dolphin peering back at them. And he requires replication of recent studies finding that elephants use their trunks to touch white crosses on their foreheads, and magpies dislodge stickers on their chests with their beaks. 
Then there are researchers who discount whether the mirror test says anything about theory of mind in any animal, including humans. Most notably, Gallup's mentee, Daniel Povinelli. Like a son who witnesses his father's foibles and decides to become his opposite, Povinelli, now at the University of Louisiana-Lafayette, has become one Gallup's most outspoken critics, even as they remain close on a personal level. He's come to believe that a chimp doesn't need to have an integrated sense of self in order to pass the mirror test. Instead, it needs only to notice that the body in the mirror looks and moves the same as its own body, and then make the connection that if there's a spot on the body in the mirror, there could also be a spot on its own body. That ability would still be pretty sophisticated, Povinelli adds, and it might reflect a keen awareness of the position of body parts that would likely be very helpful for swinging through trees. Indeed, he speculates that this high-level physical self-awareness may have developed when our tree-dwelling ancestors increased in size and faced more challenges while navigating their branchy, leafy world. 
Povinelli's concerns stretch to other landmark studies on theory of mind in chimps, such as those that document how a subordinate chimp refrained from hidden food when she watched a dominant chimp see researchers hide the food. The authors of this study argued that this was because the subordinate chimp reasoned about what the dominant chimp had seen and what it would do. Combined with results from other experiments, they concluded that chimps can "understand both the goals and intentions of others as well as the perception and knowledge of others," and they can predict the action that will result. 
But Povinelli calls this reasoning "folk psychology"—unscientific inferences made based on our own human experiences. The subordinate chimp doesn't have to know the dominant's mind, he says, all she has to know is to avoid interfering with the dominant chimp. 
To apply Povinelli's logic to humans, we may think deep, reflective thoughts when using a mirror to brush our teeth, but that doesn't mean that the part of the brain that's using the mirror to direct our toothbrush is the same part of the brain that's contemplating the self. Those two abilities may develop at the same time in children, but that does not mean that they're related, much less one and the same. 
Povinelli's critiques aside, most comparative psychologists say there's something to mirror recognition, not least because it's only been observed in intellectually superior animals. Neuroscientists are now trying to shed light on the matter by searching for a physical basis for the ability in the brain. Although they haven't found a clear signal yet, Gallup remains undeterred. After nearly 45 years of fending off challengers, he is not likely to wake up in the morning, look in the mirror, and change his mind. 



Chelsea Wald is a freelance science writer who contributes to Science and Nature. She lives with her reflection in Vienna, Austria.

This article was originally published in our "Symmetry" issue in May, 2014.








Chapter Three
Origins











Culture | Linguistics
The Kekulé Problem
Where did language come from? 
By Cormac McCarthyIllustrations by Don Kilpatrick III
♦ 

Cormac McCarthy is best known to the world as a writer of novels. These include Blood Meridian, All the Pretty Horses, No Country for Old Men, and The Road. At the Santa Fe Institute (SFI) he is a research colleague and thought of in complementary terms. An aficionado on subjects ranging from the history of mathematics, philosophical arguments relating to the status of quantum mechanics as a causal theory, comparative evidence bearing on non-human intelligence, and the nature of the conscious and unconscious mind. At SFI we have been searching for the expression of these scientific interests in his novels and we maintain a furtive tally of their covert manifestations and demonstrations in his prose.

Over the last two decades Cormac and I have been discussing the puzzles and paradoxes of the unconscious mind. Foremost among them, the fact that the very recent and "uniquely" human capability of near infinite expressive power arising through a combinatorial grammar is built on the foundations of a far more ancient animal brain. How have these two evolutionary systems become reconciled? Cormac expresses this tension as the deep suspicion, perhaps even contempt, that the primeval unconscious feels toward the upstart, conscious language. In this article Cormac explores this idea through processes of dream and infection. It is a discerning and wide-ranging exploration of ideas and challenges that our research community has only recently dared to start addressing through complexity science.


—David Krakauer President and William H. Miller Professor of Complex Systems, Santa Fe Institute




I call it the Kekulé Problem because among the myriad instances of scientific problems solved in the sleep of the inquirer Kekulé's is probably the best known. He was trying to arrive at the configuration of the benzene molecule and not making much progress when he fell asleep in front of the fire and had his famous dream of a snake coiled in a hoop with its tail in its mouth—the ouroboros of mythology—and woke exclaiming to himself: "It's a ring. The molecule is in the form of a ring." Well. The problem of course—not Kekulé's but ours—is that since the unconscious understands language perfectly well or it would not understand the problem in the first place, why doesnt it simply answer Kekulé's question with something like: "Kekulé, it's a bloody ring." To which our scientist might respond: "Okay. Got it. Thanks." 
Why the snake? That is, why is the unconscious so loathe to speak to us? Why the images, metaphors, pictures? Why the dreams, for that matter. 
A logical place to begin would be to define what the unconscious is in the first place. To do this we have to set aside the jargon of modern psychology and get back to biology. The unconscious is a biological system before it is anything else. To put it as pithily as possibly—and as accurately—the unconscious is a machine for operating an animal. 
All animals have an unconscious. If they didnt they would be plants. We may sometimes credit ours with duties it doesnt actually perform. Systems at a certain level of necessity may require their own mechanics of governance. Breathing, for instance, is not controlled by the unconscious but by the pons and the medulla oblongata, two systems located in the brainstem. Except of course in the case of cetaceans, who have to breathe when they come up for air. An autonomous system wouldnt work here. The first dolphin anesthetized on an operating table simply died. (How do they sleep? With half of their brain alternately.) But the duties of the unconscious are beyond counting. Everything from scratching an itch to solving math problems. 

Did language meet some need? No. The other five thousand plus mammals among us do fine without it.

Problems in general are often well posed in terms of language and language remains a handy tool for explaining them. But the actual process of thinking—in any discipline—is largely an unconscious affair. Language can be used to sum up some point at which one has arrived—a sort of milepost—so as to gain a fresh starting point. But if you believe that you actually use language in the solving of problems I wish that you would write to me and tell me how you go about it. 
I've pointed out to some of my mathematical friends that the unconscious appears to be better at math than they are. My friend George Zweig calls this the Night Shift. Bear in mind that the unconscious has no pencil or notepad and certainly no eraser. That it does solve problems in mathematics is indisputable. How does it go about it? When I've suggested to my friends that it may well do it without using numbers, most of them thought—after a while—that this was a possibility. How, we dont know. Just as we dont know how it is that we manage to talk. If I am talking to you then I can hardly be crafting at the same time the sentences that are to follow what I am now saying. I am totally occupied in talking to you. Nor can some part of my mind be assembling these sentences and then saying them to me so that I can repeat them. Aside from the fact that I am busy this would be to evoke an endless regress. The truth is that there is a process here to which we have no access. It is a mystery opaque to total blackness. 
There are influential persons among us—of whom a bit more a bit later—who claim to believe that language is a totally evolutionary process. That it has somehow appeared in the brain in a primitive form and then grown to usefulness. Somewhat like vision, perhaps. But vision we now know is traceable to perhaps as many as a dozen quite independent evolutionary histories. Tempting material for the teleologists. These stories apparently begin with a crude organ capable of perceiving light where any occlusion could well suggest a predator. Which actually makes it an excellent scenario for Darwinian selection. It may be that the influential persons imagine all mammals waiting for language to appear. I dont know. But all indications are that language has appeared only once and in one species only. Among whom it then spread with considerable speed. 
There are a number of examples of signaling in the animal world that might be taken for a proto-language. Chipmunks—among other species—have one alarm-call for aerial predators and another for those on the ground. Hawks as distinct from foxes or cats. Very useful. But what is missing here is the central idea of language—that one thing can be another thing. It is the idea that Helen Keller suddenly understood at the well. That the sign for water was not simply what you did to get a glass of water. It was the glass of water. It was in fact the water in the glass. This in the play The Miracle Worker. Not a dry eye in the house. 
The invention of language was understood at once to be incredibly useful. Again, it seems to have spread through the species almost instantaneously. The immediate problem would seem to have been that there were more things to name than sounds to name them with. Language appears to have originated in southwestern Africa and it may even be that the clicks in the Khoisan languages—to include Sandawe and Hadza—are an atavistic remnant of addressing this need for a greater variety of sounds. The vocal problems were eventually handled evolutionarily—and apparently in fairly short order—by turning our throat over largely to the manufacture of speech. Not without cost, as it turns out. The larynx has moved down in the throat in such a way as to make us as a species highly vulnerable to choking on our food—a not uncommon cause of death. It's also left us as the only mammal incapable of swallowing and vocalizing at the same time. 
The sort of isolation that gave us tall and short and light and dark and other variations in our species was no protection against the advance of language. It crossed mountains and oceans as if they werent there. Did it meet some need? No. The other five thousand plus mammals among us do fine without it. But useful? Oh yes. We might further point out that when it arrived it had no place to go. The brain was not expecting it and had made no plans for its arrival. It simply invaded those areas of the brain that were the least dedicated. I suggested once in conversation at the Santa Fe Institute that language had acted very much like a parasitic invasion and David Krakauer—our president—said that the same idea had occurred to him. Which pleased me a good deal because David is very smart. This is not to say of course that the human brain was not in any way structured for the reception of language. Where else would it go? If nothing else we have the evidence of history. The difference between the history of a virus and that of language is that the virus has arrived by way of Darwinian selection and language has not. The virus comes nicely machined. Offer it up. Turn it slightly. Push it in. Click. Nice fit. But the scrap heap will be found to contain any number of viruses that did not fit. 



ON THE ORIGIN OF LANGUAGE: "So what are we saying here?" writes Cormac McCarthy. "That some unknown thinker sat up one night in his cave and said: Wow. One thing can be another thing." (Above, a reproduction of a fresco in the Chauvet Cave, site of evocative prehistoric paintings.)
JEFF PACHOUD/AFP/Getty Images


There is no selection at work in the evolution of language because language is not a biological system and because there is only one of them. The ur-language of linguistic origin out of which all languages have evolved. 
Influential persons will by now of course have smiled to themselves at the ill-concealed Lamarckianism lurking here. We might think to evade it by various strategies or redefinitions but probably without much success. Darwin of course was dismissive of the idea of inherited "mutilations"—the issue of cutting off the tails of dogs for instance. But the inheritance of ideas remains something of a sticky issue. It is difficult to see them as anything other than acquired. How the unconscious goes about its work is not so much poorly understood as not understood at all. It is an area pretty much ignored by the artificial intelligence studies, which seem mostly devoted to analytics and to the question of whether the brain is like a computer. They have decided that it's not, but that is not altogether true. 
Of the known characteristics of the unconscious its persistence is among the most notable. Everyone is familiar with repetitive dreams. Here the unconscious may well be imagined to have more than one voice: He's not getting it, is he? No. He's pretty thick. What do you want to do? I dont know. Do you want to try using his mother? His mother is dead. What difference does that make? 

To put it as pithily as possibly—and as accurately—the unconscious is a machine for operating an animal.

What is at work here? And how does the unconscious know we're not getting it? What doesnt it know? It's hard to escape the conclusion that the unconscious is laboring under a moral compulsion to educate us. (Moral compulsion? Is he serious?) 
The evolution of language would begin with the names of things. After that would come descriptions of these things and descriptions of what they do. The growth of languages into their present shape and form—their syntax and grammar—has a universality that suggests a common rule. The rule is that languages have followed their own requirements. The rule is that they are charged with describing the world. There is nothing else to describe. 
All very quickly. There are no languages whose form is in a state of development. And their forms are all basically the same. 
We dont know what the unconscious is or where it is or how it got there—wherever there might be. Recent animal brain studies showing outsized cerebellums in some pretty smart species are suggestive. That facts about the world are in themselves capable of shaping the brain is slowly becoming accepted. Does the unconscious only get these facts from us, or does it have the same access to our sensorium that we have? You can do whatever you like with the us and the our and the we. I did. At some point the mind must grammaticize facts and convert them to narratives. The facts of the world do not for the most part come in narrative form. We have to do that. 

So what are we saying here? That some unknown thinker sat up one night in his cave and said: Wow. One thing can be another thing. Yes. Of course that's what we are saying. Except that he didnt say it because there was no language for him to say it in. For the time being he had to settle for just thinking it. And when did this take place? Our influential persons claim to have no idea. Of course they dont think that it took place at all. But aside from that. One hundred thousand years ago? Half a million? Longer? Actually a hundred thousand would be a pretty good guess. It dates the earliest known graphics—found in the Blombos Cave in South Africa. These scratchings have everything to do with our chap waking up in his cave. For while it is fairly certain that art preceded language it probably didnt precede it by much. Some influential persons have actually claimed that language could be up to a million years old. They havent explained what we have been doing with it all this time. What we do know—pretty much without question—is that once you have language everything else follows pretty quickly. The simple understanding that one thing can be another thing is at the root of all things of our doing. From using colored pebbles for the trading of goats to art and language and on to using symbolic marks to represent pieces of the world too small to see. 
One hundred thousand years is pretty much an eyeblink. But two million years is not. This is, rather loosely, the length of time in which our unconscious has been organizing and directing our lives. And without language you will note. At least for all but that recent blink. How does it tell us where and when to scratch? We dont know. We just know that it's good at it. But the fact that the unconscious prefers avoiding verbal instructions pretty much altogether—even where they would appear to be quite useful—suggests rather strongly that it doesnt much like language and even that it doesnt trust it. And why is that? How about for the good and sufficient reason that it has been getting along quite well without it for a couple of million years? 



Apart from its great antiquity the picture-story mode of presentation favored by the unconscious has the appeal of its simple utility. A picture can be recalled in its entirety whereas an essay cannot. Unless one is an Asperger's case. In which event memories, while correct, suffer from their own literalness. The log of knowledge or information contained in the brain of the average citizen is enormous. But the form in which it resides is largely unknown. You may have read a thousand books and be able to discuss any one of them without remembering a word of the text. 
When you pause to reflect and say: "Let me see. How can I put this," your aim is to resurrect an idea from this pool of we-know-not-what and give it a linguistic form so that it can be expressed. It is the this that one wishes to put that is representative of this pool of knowledge whose form is so amorphous. If you explain this to someone and they say that they dont understand you may well seize your chin and think some more and come up with another way to "put" it. Or you may not. When the physicist Dirac was complained to by students that they didnt understand what he'd said Dirac would simply repeat it verbatim. 
The picture-story lends itself to parable. To the tale whose meaning gives one pause. The unconscious is concerned with rules but these rules will require your cooperation. The unconscious wants to give guidance to your life in general but it doesnt care what toothpaste you use. And while the path which it suggests for you may be broad it doesnt include going over a cliff. We can see this in dreams. Those disturbing dreams which wake us from sleep are purely graphic. No one speaks. These are very old dreams and often troubling. Sometimes a friend can see their meaning where we cannot. The unconscious intends that they be difficult to unravel because it wants us to think about them. To remember them. It doesnt say that you cant ask for help. Parables of course often want to resolve themselves into the pictorial. When you first heard of Plato's cave you set about reconstructing it. 
To repeat. The unconscious is a biological operative and language is not. Or not yet. You have to be careful about inviting Descartes to the table. Aside from inheritability probably the best guide as to whether a category is of our own devising is to ask if we see it in other creatures. The case for language is pretty clear. In the facility with which young children learn its complex and difficult rules we see the slow incorporation of the acquired. 
I'd been thinking about the Kekulé problem off and on for a couple of years without making much progress. Then one morning after George Zweig and I had had one of our ten hour lunches I came down in the morning with the wastebasket from my bedroom and as I was emptying it into the kitchen trash I suddenly knew the answer. Or I knew that I knew the answer. It took me a minute or so to put it together. I reflected that while George and I had spent the first couple of hours at cognition and neuroscience we had not talked about Kekulé and the problem. But something in our conversation might very well have triggered our reflections—mine and those of the Night Shift—on this issue. The answer of course is simple once you know it. The unconscious is just not used to giving verbal instructions and is not happy doing so. Habits of two million years duration are hard to break. When later I told George what I'd come up with he mulled it over for a minute or so and then nodded and said: "That sounds about right." Which pleased me a good deal because George is very smart. 
The unconscious seems to know a great deal. What does it know about itself? Does it know that it's going to die? What does it think about that? It appears to represent a gathering of talents rather than just one. It seems unlikely that the itch department is also in charge of math. Can it work on a number of problems at once? Does it only know what we tell it? Or—more plausibly—has it direct access to the outer world? Some of the dreams which it is at pains to assemble for us are no doubt deeply reflective and yet some are quite frivolous. And the fact that it appears to be less than insistent upon our remembering every dream suggests that sometimes it may be working on itself. And is it really so good at solving problems or is it just that it keeps its own counsel about the failures? How does it have this understanding which we might well envy? How might we make inquiries of it? Are you sure? 




Cormac McCarthy is a board member and senior fellow of the Santa Fe Institute.













Culture | Psychology
How Nostalgia Made America Great Again
When the present looks bleak, we reach for a rose-tinted past.
By Mike Mariani
♦ 

Make America great again. Clearly the message resonated. In 2016, prior to the presidential election, the Public Religion Research Institute, a nonpartisan group, published its annual American Values Survey. It revealed 51 percent of the population felt the American way of life had changed for the worse since the 1950s. Further, 7 in 10 likely Donald Trump voters said American society has gotten worse since that romanticized decade. 
Of course America today has its problems, but many indices of standards of living show the general population is better off now than it was 60 years ago. We live on average 10 years longer, the education rate is higher, as is homeownership. When it comes to crime, The Atlantic reported last year, "By virtually any metric, Americans now live in one of the least violent times in the nation's history." 
So why do so many people see the past as better than today? For many of them, it may well have been. Middle- and working-class Americans seduced by appeals to earlier eras may have had higher-paying jobs with better benefits, greater financial security, and a more defined place in the community. Perhaps they were happier. For some, cultural changes since the Saturday night sock-hop may have only strengthened their beliefs that American values have frayed. But an innate psychological trait may also explain why people tend to view the past as better than today: nostalgia. 
Most everybody knows the term nostalgia, if not its origin. It was coined by Swiss physician Johannes Hofer in 1688: a portmanteau of nostos and algos, Greek words for homecoming and pain or distress, respectively. And most have an understanding that nostalgia means finding pleasure in remembering or reliving a past experience—hearing a favorite old song, for instance, or remembering a stirring love affair. 



THOSE WERE THE DAYS: A 2016 poll revealed that 51 percent of the population felt the American way of life had been tarnished since the 1950s.
Illustration by Len Small


Recent science, though, makes good on the etymology of the term. It reveals nostalgia is not just a wistful glow associated with pleasurable events and experiences. It is an innate response to pain or distress, and, in some sense, a coming home. What's more, cognitive scientists say, a defining trait of nostalgia is its capacity to distort the past. 
In the process of looking back, people tend to filter out negative or painful experiences. Memories themselves are often not what they seem. They are not hardwired in our brains, a factual representation of our autobiographical pasts. Rather, memory is fluid, and we're constantly reframing our personal histories to fit into a greater life arc. In many cases, the past looks as halcyon as it does because rosy hindsight molds it to appear that way to help us maintain mental health. Our past is constantly shifting to accommodate our present. 



When Clay Routledge, a professor of psychology at North Dakota State University, and author of Nostalgia: A Psychological Resource, began researching nostalgia, he was interested in how this universally shared feeling might help us better deal with the future. "I started out with this very specific hypothesis of nostalgia as a coping resource," he says. "We can reflect back in time on experiences that we find personally meaningful. Might we use such past-oriented experiences as a way to cope with future-oriented anxieties?" 
Pursuing this hunch, Routledge and a team of researchers at the University of Southampton began looking at the potential benefits of "nostalgizing," as they termed it: How it might help us maintain equilibrium in times of crisis, recall loving relationships, and generally lean on the bright spots in our pasts. 

Nostalgic memories are a central part of our identity, of who we think we are.

In one particular experiment conducted by Tim Wildschut at the University of Southampton, a hub for contemporary nostalgia research, psychologists induced nostalgia in study participants by having them write about a "nostalgic event" from their lives. A control group was instructed to recount in writing a merely "ordinary event." Researchers found that the group that wrote about a nostalgic event reported more positive affect, higher levels of self-regard, and a stronger sense of social bonds compared with those participants who wrote about an ordinary event. 
Subsequent studies further contextualized nostalgia's utility, showing that it's frequently triggered by low moods, loneliness, and even a sense of meaninglessness. These triggers suggested that nostalgia might be a kind of defense mechanism, a way to maintain resiliency during periods of anxiety, despair, and existential distress. "What seems to be the case is that nostalgia can be an adaptive tool to deal with a lot of psychological threats," says Wijnand van Tilburg, a social cognition researcher at King's College London. 
Something less immediately apparent from this new research espousing nostalgia as a mental panacea was how, exactly, it was having such beneficial effects on our mental health and sense of well-being. 
Nostalgia, it turns out, helps cultivate what psychologists call "self-continuity." The concept refers to our ability to maintain our identity and sense of self through the vicissitudes of our lifespans, from the death of a loved one to a career change to devastating illness. 
"Self-continuity means the sense that I have this stable, continuous sense of self," Routledge says. Self-continuity gives coherence to our lives, the impression that there is a permanent, unchanging self underneath the random events and crises that transform our circumstances over the years. 
Even before a connection was established with nostalgia, psychologists understood self-continuity as a critical component to a person's sense of identity, mental well-being, and ability to adapt to shifting conditions. In most cases, self-continuity was a source of psychological equanimity and a way to negotiate existential threats. 
Major changes or periods of upheaval can cause us to question ourselves, and feel estranged from our pasts. Psychologists refer to this as "self-discontinuity" or "disjuncture." Past research has found self-discontinuity—this sense of estrangement from past selves—to be a maladaptive trait, something that causes psychological distress. But a strong sense of self-continuity can help combat those episodes of disjuncture in our lives, when radical change threatens our fundamental sense of who we are. 
"I might get divorced, I might move to a new country, I might even become a born-again Christian," says Susan Bluck, a professor of psychology at the University of Florida, who studies autobiographical memory. "How do we have the sense that we're the same person over time? That's what self-continuity is." People with a strong sense of self-continuity also report better mental and physical health, and develop more constructive coping styles. 
So how does nostalgia enhance one's self-continuity? A 2014 study coauthored by Routledge sought to find out. Routledge and his co-authors rated participants on two scales. The first, the Social Readjustment Rating Scale, looks at major life events that may disrupt self-continuity and elicit self-discontinuity. The second was the Southampton Nostalgia Scale, a rating system for nostalgia-proneness. They found a positive correlation between experiences that induce self-discontinuity and an individual's nostalgia proneness. 



HAT TRICK: Nostalgia can bring comfort, but during politically charged times it can also reveal a dark side, shading people from reality.


The researchers believe these findings indicate that nostalgia is a natural response to self-discontinuity, a psychological tool that mitigates damage to our sense of self. "Nostalgic memories are a central part of our identity, our sense of self, of who we think we are," Routledge says. 
In the study, "nostalgic memories" were simply memories that specifically induced nostalgia, as opposed to, say, ordinary memories that elicited minimal emotional response. Routledge explains that the various phenomena associated with autobiographical memory—chiefly psychological biases called fading affect and rosy retrospection—are ideally suited to induce nostalgia. "The way these memories works, works perfectly for nostalgia," he says. In other words, the memories inducing nostalgia have been burnished and idealized over time, shorn of their negative aspects. But these recollections serve an important adaptive purpose. "When people are experiencing situations that challenge that sense of self and make them feel uncertain about life, they naturally recruit nostalgia as a way to restore that self-continuity," Routledge says. 
In a separate experiment published in the same study, the authors found that inducing nostalgia raised levels of self-continuity among participants. Not only, then, is nostalgia used to counteract episodes of self-discontinuity, but it also strengthens our sense of an overarching self over a lifetime. 
"Nostalgia is really important to help us connect across time the aspects of who we are that are unchanging," says Krystine Batcho, a professor of psychology at Le Moyne College and one of the first academics to begin reevaluating nostalgia in the 1990s. 
Batcho gives the example of long-term relationships. "When you say 'I love you,' the assumption underlying that very statement is that there's something unchanging about the essence of who we are," she says. "If you didn't have those unchanging aspects, I would argue it would be extremely difficult to have serious long-lasting human relationships." 
These social psychologists present nostalgia as more than just ephemeral reveries that come and go with smells or songs bound to certain times and places. These memories are valuable tools for resiliency and psychological coping. "When you feel uncertain about what's going on in your life, you can reassure yourself: I've had these important memories that define me," Routledge says. 



But self-continuity still doesn't fully explain how nostalgia casts its spell. What enables us to focus only on certain events and episodes, or cut through ambivalence and zero in on the happier aspects of our relationships? As it turns out, our memories are partial. Biased. They are not objective representations of our pasts. Instead, they often magnify our positive experiences, while gradually diminishing negative ones. This is referred to as the fading affect bias. 
Since at least the 1930s, psychologists have been researching the phenomenon of the human tendency to forget negative emotions associated with their memories faster than positive emotions. "The impact of negative emotions fade faster than positive emotions," Routledge says. 
In a study by Colorado State University psychologist Richard Walker in 1997, participants recorded and rated events based on how pleasant they were. They then reevaluated those events, respectively, three months, one-and-a-half years, and four-and-a-half years later, again rating them based on pleasantness. Researchers found that while participants rated most experiences as less extreme over time—either less pleasant or less unpleasant—negative emotions faded faster. Walker's findings have been corroborated many times over, including a study in 2014 that showed fading affect bias prevalent worldwide, in countries ranging from Australia to Germany to Ghana. 
"People seem pretty good at getting over things," Routledge says. "We have this psychological immune system that helps reduce the impact of negative experiences. They fade faster." 

People who feel nostalgic for a past era perceive the current situation as unacceptable.

Not all negative memories and unpleasant experiences fade quickly or even at all, of course. Bluck is quick to draw the distinction between the negative affect associated with certain memories that we all experience, and the trauma that is anything but fading. "We're not talking about highly traumatic life events," Bluck says. "For example, PTSD can have uncontrolled, involuntary rumination over really traumatic events. That mechanism holds for certain kinds of very difficult events. At the same time, for everyone, this general positive affect bias is going on in everyday life." 
Multiple social psychologists say nostalgia is associated with fading affect bias, although the connection is oblique. "The rose-tinted glasses phenomenon is just naturally how personal memories work," Routledge says. 
One possible explanation for fading affect bias is it's an evolutionary adaptation—over time, individuals who reflected on their pasts more fondly and were able to dismiss the painful parts proliferated through natural selection. "That's very possible," Batcho says. Studies have found that nostalgia can help individuals survive through the most adverse circumstances. A 2012 study coauthored by Xinyue Zhou and Tim Wildschut found that nostalgizing could actually make one report feeling warmer while gritting through frigid temperatures. As Wildschut told The New York Times, nostalgia "could contribute to survival by making you look for food and shelter for that much longer." Psychologists find the circumstantial evidence of nostalgia's evolutionary properties hard to deny. "In that sense, you could argue, it could very well be an adaptation," Batcho says. 
Last summer, in the heat of the presidential race, the Berggruen Institute, a Los Angeles-based think tank devoted to governance, published a set of articles on nostalgia. One, "Nostalgia for a Past That Never Quite Was," by Antonio Damasio, professor of neuroscience, psychology, and philosophy at the University of Southern California, and author of, most recently, Self Comes to Mind: Constructing the Conscious Brain, provided insight into why nostalgia is an adaptation. "There is no doubt that nostalgia is a valuable feeling or it would have long been dismissed from the human toolkit," Damasio wrote. What makes nostalgia valuable is that its emotional tenor, either sadness or joy, brings about a time of inaction, a time to reason, make decisions, work out solutions. 
However, Damasio explained, the comforts of nostalgia come with a warning. We "need to be alert," he wrote, "to one reason why remembrances can be comforting: Our memories can be selective; they are great film editors, capable of glorifying some facts and suppressing others when they are inconvenient. Also, there is a tendency, across a lifetime, to re-experience memorized facts and events with a positive slant, possibly part of an adaptive but non-conscious attempt to increase one's wellbeing." 
Neuroscientists have long shown the act of remembering colors our past experiences. "Memory, rather than being something that's static, and doesn't change, is really a fluid phenomenon," says Alan Hirsch, a neurologist who has studied nostalgia and memory since the early 1990s. That fluidity, Hirsch explains, allows us to mold memory to our advantage. He goes so far as to compare our propensity for reshaping our pasts with individuals suffering from Korsakoff Syndrome—typically chronic drinkers with severely impaired memories—who confabulate to compensate for lost or fuzzy memories. "It's almost like that, to a small degree," he says. "They're filling in the things in the past." 
To delve further into memory, Routledge is working on a study in which Britons who were children during World War II put together narratives of their experiences. While most of the narratives include many painful and even traumatic episodes—children being estranged from their parents or hiding underground during the Nazi bombing of Britain—their accounts of the war also invoke dimensions that may have taken decades to fully reveal themselves. Despite all the fear and upheaval, survivors recall spending time with extended family, reaching an intimacy with their cousins, aunts, and uncles that wouldn't have been possible outside of those extraordinary circumstances. "They've been able to re-construe, or learn some sort of life lesson from that experience, which at the time must have been terrifying." Routledge says. 
While the World War II survivors in Routledge's study may never look back on the war as a categorically positive experience, time allows them to reshuffle the deck, producing a more measured view that accentuates the period's redeeming qualities. "If you look at nostalgia memories, it's not the case that they're universally positive," Routledge says. "It's that the narrative arc of the memory is redemptive." 



Most of the psychologists I spoke to agreed that nostalgia—recalling past experiences, even if selective, positive ones, reshaped by time—had a salutary personal effect. They also, however, acknowledged the social consequences of nostalgia during politically charged periods—how it might influence the way people see issues and candidates, and subsequently how they vote. 
"There is a real concern about the potential dark side of nostalgia," Routledge says. "What I think people are doing is imagining the positive. They're plucking features that they think are better, like 'Oh, maybe life was simpler, or people could get better jobs.' I don't know what they're holding onto but it doesn't necessarily mean that it was true, first of all. And they're probably not thinking about who it negatively affected." 
Batcho, too, was careful to point out nostalgia should not be stereotyped. Not everybody who experiences and derives comfort from nostalgia does so in order to brighten his view of the present. "People who tend to be more historically nostalgic are people who tend to view the present less favorably than people who are more personally nostalgic," she says. "If you think about people who want to feel nostalgic for a past era, a past age in history, they're people who perceive the current situation as somehow unacceptable, they're unhappy with it." 
Still, the point remains that nostalgia, an innate, adaptive trait, is a necessary guide through the thickets of memory and experience. In the end, says Routledge, "Our brains are just trying to make sense of life." 



Mike Mariani is a writer based in Hoboken, New Jersey. His work has appeared in The Atlantic, Newsweek, Pacific Standard, The Guardian, and others. You can follow him @mikesmariani.












Culture | Psychology
Why Poverty Is Like a Disease
Emerging science is putting the lie to American meritocracy.
By Christian H. CooperPhotography by Nathan Cooper
♦ 

On paper alone you would never guess that I grew up poor and hungry. 
My most recent annual salary was over $700,000. I am a Truman National Security Fellow and a term member at the Council on Foreign Relations. My publisher has just released my latest book series on quantitative finance in worldwide distribution. 
None of it feels like enough. I feel as though I am wired for a permanent state of fight or flight, waiting for the other shoe to drop, or the metaphorical week when I don't eat. I've chosen not to have children, partly because—despite any success—I still don't feel I have a safety net. I have a huge minimum checking account balance in mind before I would ever consider having children. If you knew me personally, you might get glimpses of stress, self-doubt, anxiety, and depression. And you might hear about Tennessee. 
Meet anyone from Tennessee and they will never say they are from "just" Tennessee. They'll add a prefix: East, West, or Middle. My early life was in East Tennessee, in an Appalachian town called Rockwood. I was the eldest of four children with a household income that couldn't support one. Every Pentecostal church in the surrounding hillbilly heroin country smelled the same: a sweaty mix of cheap cleaner and even cheaper anointing oil, with just a hint of forsaken hope. One of those forsaken churches was effectively my childhood home, and my school. 



Schoolhouse: The Front St. Pentecostal Church in Rockwood, Tennessee. It was where I went to school, and the center of my daily life.


Class was a single room of 20 people running from kindergarten through twelfth grade, part of an unaccredited school practicing what's called Accelerated Christian Education. We were given booklets to read to ourselves, by ourselves. We scored our own homework. There were no lectures, and I did not have a teacher. Once in a while the preacher's wife would hand out a test. We weren't allowed to do anything. There were no movies, and no music. Years would pass with no distinguishing features, no events. There was barely any socializing. 
On top of it all, I spent a lot of my time pondering basic questions. Where will my next meal come from? Will I have electricity tomorrow? I became intimately acquainted with the embarrassment of my mom trying to hide our food stamps at the grocery store checkout. I remember panic setting in as early as age 8, at the prospect of a perpetual uncertainty about everything in life, from food to clothes to education. I knew that the life I was living couldn't be normal. Something was wrong with the tiny microcosm I was born into. I just wasn't sure what it was. 
As an adult I thought I'd figured that out. I'd always thought my upbringing had made me wary and cautious, in a "lessons learned" kind of way. Over the past decades, though, that narrative has evolved. We've learned that the stresses associated with poverty have the potential to change our biology in ways we hadn't imagined. It can reduce the surface area of your brain, shorten your telomeres and lifespan, increase your chances of obesity, and make you more likely to take outsized risks. 
Now, new evidence is emerging suggesting the changes can go even deeper—to how our bodies assemble themselves, shifting the types of cells that they are made from, and maybe even how our genetic code is expressed, playing with it like a Rubik's cube thrown into a running washing machine. If this science holds up, it means that poverty is more than just a socioeconomic condition. It is a collection of related symptoms that are preventable, treatable—and even inheritable. In other words, the effects of poverty begin to look very much like the symptoms of a disease. 
That word—disease—carries a stigma with it. By using it here, I don't mean that the poor are (that I am) inferior or compromised. I mean that the poor are afflicted, and told by the rest of the world that their condition is a necessary, temporary, and even positive part of modern capitalism. We tell the poor that they have the chance to escape if they just work hard enough; that we are all equally invested in a system that doles out rewards and punishments in equal measure. We point at the rare rags-to-riches stories like my own, which seem to play into the standard meritocracy template. 
But merit has little to do with how I got out. 



We may not remember 1834 as a banner year, but it was in the field of organic chemistry. It was then that chemists Jean-Baptiste Dumas and Eugène Péligot distilled and analyzed a clear liquid—what they called methylene, and what we'd call methanol today—from softly heated wood chips. At its heart was a methyl group, consisting of one carbon atom bound to three hydrogen atoms. As it would turn out 150 years later, methyl groups play a critical role in gene expression. 
In the fall of 1991, Aharon Razin and Howard Cedar published the extraordinary paper "DNA Methylation and Gene Expression," which showed that gene expression works much like a snake tightly coiled around the Rod of Asclepius.1 Perched atop the indissoluble warp and weft of our genetic code are methyl groups that control how tightly our genetic code wraps around special proteins, called histone proteins. The tighter a portion of code is wrapped, the less likely it is to have any effect (or in the jargon, the less likely it "gets expressed"). This, we now know, is one pillar of the mechanism of the epigenome: Who you are as a person is not just defined by your DNA, but by which parts of it your epigenome permits to be expressed. 
Six years later, Michael Meaney, a professor at McGill University specializing in the biology of stress, published a breakthrough result together with his colleagues: The quality of maternal care alters the epigenome in rats, affecting glucocorticoid stress receptors in the hippocampus as well as the response of the hypothalamic-pituitary-adrenal axis to stress.2 Similar effects were later found in zebra finches which, like humans, are socially monogamous and involve both parents in raising offspring. Messenger-RNA levels of glucocorticoid and mineralocorticoid receptors were reduced in maternally deprived birds, which made stress hormones remain elevated in adult finches for longer periods of time. The researchers wrote that epigenetic mechanisms could be responsible for the changes, but they did not prove them to be.3



Alternative housing: This home, consisting of a plywood attachment to a trailer, is made possible by equal doses of ingenuity and lax housing laws.


In human children, epigenetic changes in stress receptor gene expression that lead to heightened stress responses and mood disorders have been measured in response to childhood abuse.4 And last year, researchers at Duke University found that "lower socioeconomic status during adolescence is associated with an increase in methylation of the proximal promoter of the serotonin transporter gene," which primes the amygdala—the brain's center for emotion and fear—for "threat-related amygdala reactivity."5 While there may be some advantages to being primed to experience high levels of stress (learning under stress, for example, may be accelerated6), the basic message of these studies is consistent: Chronic stress and uncertainty during childhood makes stress more difficult to deal with as an adult. 
From one perspective, epigenetics offers a compelling narrative of life experiences feeding back directly onto the basic programming that makes us who we are. But the field also has some foundational controversies. In June of last year, a team of researchers from the Albert Einstein College of Medicine, Bristol University, and the European Bioinformatics Institute published a paper arguing that the field is plagued with misinterpreted results. The sources of misinterpretation included confusing cause and effect (diseases can produce epigenetic markers as well as the other way around); spurious and misinterpreted statistics; confounding variables which cause apparent correlations; and a large variability among the epigenomes of individual cells, which is usually not controlled for in experiments. 
John Greally, one of the study's co-authors, argues that some of the landmark results in the field, including Meaney's, have suffered from these problems. "At the time [of Meaney's study]," he explains, "the idea was that if I see something like a DNA methylation change, in cells of either the rats that didn't get licked by their mothers, or the kids from the lower socio-economic group, or whatever it might be, then I'm learning how we're reprogrammed as a response to that environmental condition." But the measurement of DNA methylation explains more than whether a cell has been reprogrammed or not. It is also related to the proportions of cell subtypes, each with different epigenomes, that are present in the subjects being compared. Greally and his co-authors call this the meta-epigenome. 
But Greally also points out that, even if the molecular mechanism is a shift in cell subtypes rather than cellular reprogramming through methylation, there is still an interesting conversation to be had. "Even if you find that there's a change in the proportion of say, cell subtype proportions in the peripheral blood, and it's associated with a condition like low socio-economic status or something like that, that's actually a pretty interesting finding," he says. "It kind of gets back to the issue of how you define epigenetics." It may be possible that shifts in cell subtypes are inheritable, even though they do not involve a reprogramming of a cell through methylation. Tim Spector of King's College in London, for example, has found DNA sequence variants associated with cell subtype variation. 
The science of the biological effects of the stresses of poverty is in its early stages. Still, it has presented us with multiple mechanisms through which such effects could happen, and many of these admit an inheritable component. If a pregnant woman, for example, is exposed to the stresses of poverty, her fetus and that fetus' gametes can both be affected, extending the effects of poverty to at least her grandchildren. And it could go further. 



Echo: Once a busy street and meeting place, Main St. in Rockwood suffered a deadly blow with the arrival of big-box retail.


Studies of mice and fruit flies have shown that epigenetic traits similar to the ones Meaney proposed can be passed down, and last for dozens of generations. The effects of things like diet and prenatal parental stress have been observed to be inherited, not just through histone modifications, but also through DNA methylation and non-coding RNAs.7 In one 2014 study, the offspring of a mouse trained to fear a particular smell were observed to also fear that smell, even with no previous exposure to it. The effect lasted for two generations.8 In humans, inheritable effects of stress have been observed through at least three generations from parents who survived mass starvation (Dutch Hunger Winter),9 a fluctuating food supply (the Överkalix cohort)10 and the Holocaust. The effects of early paternal smoking and paternal betel quid chewing have been observed to be transmitted to children in a sex-specific manner, supporting biological epigenetic transmission in humans.11 According to a 2014 survey of the field, "the few human observational studies to date suggest (male line) transgenerational effects exist that cannot easily be attributed to cultural and/or genetic inheritance."10
Even at this stage, then, we can take a few things away from the science. First, that the stresses of being poor have a biological effect that can last a lifetime. Second, that there is evidence suggesting that these effects may be inheritable, whether it is through impact on the fetus, epigenetic effects, cell subtype effects, or something else. 
This science challenges us to re-evaluate a cornerstone of American mythology, and of our social policies for the poor: the bootstrap. The story of the self-made, inspirational individual transcending his or her circumstances by sweat and hard work. A pillar of the framework of meritocracy, where rewards are supposedly justly distributed to those who deserve them most. 
What kind of a bootstrap or merit-based game can we be left with if poverty cripples the contestants? Especially if it has intergenerational effects? The uglier converse of the bootstrap hypothesis—that those who fail to transcend their circumstances deserve them—makes even less sense in the face of the grim biology of poverty. When the firing gun goes off, the poor are well behind the start line. Despite my success, I certainly was. 



So how did I get out? By chance. 
It's easy to attach a post-facto narrative of talent and hard work to my story, because that's what we're fed by everything from Hollywood to political stump speeches. But it's the wrong story. My escape was made up of a series of incredibly unlikely events, none of which I had real control over. 
At age 14, I'd had eight years of trying to teach myself using photocopied handouts, without textbooks, lesson plans, aids, or even a teacher. I was desperate to get out and terrified of winding up like the people I saw around me at the Christian compound. So, I picked up the phonebook and started dialing trade schools, colleges, anything and anyone that might give me a new option. Randomly, unexpectedly, I reached the president of the local community college, Sherry Hoppe. I was probably 12 years old the first time I met Hoppe and even at that age I could tell my story was not unique in her experience. 
At that same college, I met Bruce Cantrell, a professor who wound up being like a father figure to me while I was navigating being 15 and poor. He grew up poor too but ultimately did well. We never actually talked about it but we just clicked. A few years later he ran for office and made me his campaign manager. We won and I got a priceless education in the reality of Roane County bare-knuckle politics. I'll forever be grateful to Bruce and Sherry. With their help, I ultimately got my accredited college degree. 
Did I show initiative? Sure. And there have been many people who have interpreted my escape from poverty as a confirmation of some foundational meritocracy that justifies the whole system. But the fact is hillbilly country is full of people just as desperate to get out as me, and taking just as inventive a set of measures. Yes, I am the exception that proves the rule—but that rule is that escape from poverty is a matter of chance, and not a matter of merit. 
I have relatives and friends who are as bright and hard-working as I am, with roughly the same kind of educational path or better. But none of them made it out of poverty. One of them also got into community college, but not before he saw his drugged-up best friend kill himself. That proved to be a one-way ticket to a lifetime of emotional problems. Another was lucky enough to attend an accredited public school, learning far more there than I ever did in my Accelerated Christian Education. He ended up a heroin addict. They would not, as I did, find the path to graduation curiously free of obstacles. They would not become, as I did, head of a derivatives trading desk on Wall Street. They are not, as I am, writing about poverty. They are still living it. As of now, I can count around 20 friends and family who have checked out by handgun or heroin. I have no doubt I will add to that count this year. 



Salvaged: Glenn's Auto Junkyard hasn't experienced the decline of other local businesses.


Why do so few make it out of poverty? I can tell you from experience it is not because some have more merit than others. It is because being poor is a high-risk gamble. The asymmetry of outcomes for the poor is so enormous because it is so expensive to be poor. Imagine losing a job because your phone was cut off, or blowing off an exam because you spent the day in the ER dealing with something that preventative care would have avoided completely. Something as simple as that can spark a spiral of adversity almost impossible to recover from. The reality is that when you're poor, if you make one mistake, you're done. Everything becomes a sudden-death gamble.
Now imagine that, on top of that, your brain is wired to multiply the subjective experience of stress by 10. The result is a profound focus on short-term thinking. To those outsiders who, by fortune of birth, have never known the calculus of poverty, the poor seem to make sub-optimal decisions time and time again. But the choices made by the poor are supremely rational choices under the circumstances. Pondering optimal, long-term decisions is a liability when you have 48 hours of food left. Stress takes on a whole new meaning—and try as you might, it's hard to shake. 
The standard American myth of meritocracy misinterprets personal narratives like mine. The accumulated social capital of American institutions—stable transfer of power, rule of law, and entrepreneurship—certainly create economic miracles every day. But these institutions are far more suited to exponentially growing capital where it already exists, rather than creating new capital where society needs it. Stories such as mine are treated as the archetype, and we falsely believe they are the path to escape velocity for an entire segment of the population. In doing so, they leave that population behind. I am the face of the self-made rags-to-riches success story, and I'm here to say that story is a myth. The term "meritocracy" was coined in 1958 as a mockery of the very idea of evaluation by merit alone. We've forgotten to laugh, and the joke is on us. 



It's time for us to update our response to poverty to take into account the new science that describes it. 
Take education. One of the strongest voices connecting the dots from poverty to performance in the classroom and economic struggles later in life is Harvard's Roland G. Fryer. In their seminal work, "It May Not Take a Village: Increasing Achievement Among the Poor," Fryer and his colleagues focused on closing the achievement gap between rich and poor through a mosaic of strategies, primarily at school. 
But the standard bearer of the achievement gap—math performance—is a symptom and not a cause. When support from well-intended social programs that address things like test scores inevitably diminish or stop, their positive outcomes fail to persist and we grow skeptical about poverty alleviation as a whole. But academic achievement isn't the real problem—it's uncertainty and stress. When the 2011 National Assessment of Educational Progress finds no city in America where more than 25 percent of Black or Hispanic children in the eighth grade function at grade level in reading or math, do we blame our schools, or conclude that we lost the neurological arms race long before the children were tested? 



Uptick: Along with the junkyard and the convenience store, Peggy Ann's is one of the few local businesses that has survived the downturn.


We should leverage the lessons of the science of poverty rather than ignore them. Poverty alleviation programs like conditional cash transfers, for example, reward parents or caregivers with direct payment for taking actions, like ensuring school attendance or arranging for preventative care. They encourage stress alleviation and long-term planning that is far upstream of doing well on an exam—they provide exactly the kind of certainty that the poverty-stricken brain needs. In a paper released in June of 2009, Lia Fernald and Megan Gunnar showed that such programs lowered salivary cortisol levels and reduced lifetime risk for a range of mental and physical disorders.12 There should be more programs like these: For example so-called whole-child policies, which focus on the long-term development of children starting from birth while reducing uncertainty during the first three years of childhood development. 
Our new scientific understanding of the experience of poverty can also inform medical treatments later in life. In 2009, Michael Meaney, Gustavo Turecki, Moshe Szyf and colleagues took hippocampus samples from suicide victims with a history of childhood abuse and tested for DNA methylation controlling the expression of the gene NR3C1.4 They discovered an increased methylation around the NR3C1 promoter, which, in other studies, has been directly linked to a reduced expression of a protein called brain-derived neurotropic factor (BDNF). BDNF is among the most active neurotrophic factors, which drive the growth and development of new neurons even in adulthood. And the degree to which it is expressed may be inheritable. A 2015 study linked NR3C1 and reduced expression of BDNF in infants born to mothers who reported prenatal depressive symptoms.13
It may be that BDNF is your best friend if you are an adult and want to change your neurological wiring. It could open a pathway to change brain wiring in exactly those areas that are most damaged by early stress and poverty: the prefrontal cortex, hippocampus, and the entire chain of the hypothalamic-pituitary-adrenal system. Those areas of the brain govern long-term memory, emotional control, and delayed gratification; all markers of individuals that outperform in academic settings in youth and are higher earners in adulthood.14, 15 Low doses of ketamine have been shown to act as a rapid anti-depressant, and that impact is directly linked to increased levels of BDNF.16
I would consider trying this treatment myself. But that is not my primary interest in the science of poverty. My interest stems from something else: worrying about the future. 



We stand at the precipice if we don't re-evaluate our understanding of poverty and inequality. The narrative in the neo-liberal west is that if you work hard, things work out. If things don't work out, we have the tendency to blame the victim, leaving them without any choices. Brexit, Le Pen, and the defeat of Hillary Clinton are examples of the cracks that result from inequality and poverty, symptoms of my childhood experience writ large. The Piketty pitchforks are out, and the march to global disorder can only be arrested by adopting measures that begin to price in the stacked deck that I and anyone else born into deep poverty sees, and resents. 
I believe we will see the Italian Five Star Movement submit a referendum to leave the EU this year, and that Marine Le Pen has better than even odds of winning the French election. The EU is in danger of buckling under a globalist defeat and may exist in name only two years from now. 
These trends are being accelerated by the blind belief that the poor have failed to seize the opportunities that the market or globalization has created. This myth deserves to be taken off life support—and the emerging, empirical, and carefully observed science of poverty can help us do so if we pay it the attention it deserves. 





Christian H. Cooper is the former head of interest rate derivatives trading at an investment bank in New York City and is currently focused on raising a global macro fund. He is a Truman National Security Fellow and a Term Member at the Council on Foreign Relations.






References

1. Razin, A. & Cedar, H. DNA methylation and gene expression. Microbiological Reviews55, 451-458 (1991). 
2. Liu, D., et al. Maternal care, hippocampal glucocorticoid receptors, and hypothalamic-pituitary-adrenal responses to stress. Science277, 1659-1662 (1997). 
3. Banerjee, S.B., Arterbery, A.S., Fergus, D.J., & Adkins-Regan, E. Deprivation of maternal care has long-lasting consequences for the hypothalamic-pituitary-adrenal axis of zebra finches. Proceedings of the Royal Society B279, 759-766 (2012). 
4. McGowan, P.O., et al. Epigenetic regulation of the glucocorticoid receptor in human brain associates with childhood abuse. Nature Neuroscience12, 342-348 (2009). 
5. Swartz, J.R., Hariri, A.R., & Williamson, D.E. An epigenetic mechanism links socioeconomic status to changes in depression-related brain function in high-risk adolescents. Molecular Psychiatry22, 209-214 (2017). 
6. Champagne, D.L., et al. Maternal care and hippocampal plasticity: Evidence for experience-dependent structural plasticity, altered synaptic functioning, and differential responsiveness to glucocorticoids and stress. Journal of Neuroscience28, 6037-6045 (2008). 
7. Lim, J.P. & Brunet, A. Bridging the transgenerational gap with epigenetic memory. Trends in Genetics29, 176-186 (2013). 
8. Dias, B.G. & Ressler, K.J. Prenatal olfactory experience influences behavior and neural structure in subsequent generations. Nature Neuroscience17, 89-96 (2014). 
9. Heijmans, B.T., et al. Persistent epigenetic differences associated with prenatal exposure to famine in humans. Proceedings of the National Academy of Sciences105, 17046-17049 (2008). 
10. Pembrey, M., Saffery, R., Bygren, L.O., & Network in Epigenetic Epidemiology. Human transgenerational responses to early-life experience: Potential impact on development, health and biomedical research. Journal of Medical Genetics51, 563-572 (2014). 
11. Pembrey, M.E., Bygren, L.O., & Golding, J. The nature of human transgenerational responses. In Jirtle, R.L. & Tyson, F.L. (Eds.) Environmental Epigenetics in Health and Disease Springer Publishing, New York, NY (2013). 
12. Fernald, L.C.H. & Gunnar, M.R. Poverty-alleviation program participation and salivary cortisol in very low-income children. Social Science & Medicine68, 2180-2189 (2009). 
13. Braithwaite, E.C., Kundakovic, M., Ramchandani, P.G., Murphy, S.E., & Champagne, F.A. Maternal prenatal depressive symptoms predict infant NR3C1 1 F and BDNF IV DNA methylation. Epigenetics10, 408-417 (2015). 
14. Xu, X., et al. A significant association between BDNF promoter methylation and the risk of drug addiction. Gene584, 54-59 (2016). 
15. Kheirouri, S., Noorazar, S.G., Alizadeh, M., & Dana-Alamdari, L. Elevated brain-derived neurotrophic factor correlates negatively with severity and duration of major depressive episodes. Cognitive and Behavioral Neurology29, 24-31 (2016). 
16. Haile, C.N., et al. Plasma brain derived neurotrophic factor (BDNF) and response to ketamine in treatment-resistant depression. International Journal of Neuropsychopharmacology17, 331-336 (2014). 












Culture | Neuroscience
The Deep Space of Digital Reading
Why we shouldn't worry about leaving print behind.
By Paul La FargeIllustration by Irene Rinaldi
♦ 

In A History of Reading, the Canadian novelist and essayist Alberto Manguel describes a remarkable transformation of human consciousness, which took place around the 10th century A.D.: the advent of silent reading. Human beings have been reading for thousands of years, but in antiquity, the normal thing was to read aloud. When Augustine (the future St. Augustine) went to see his teacher, Ambrose, in Milan, in 384 A.D., he was stunned to see him looking at a book and not saying anything. With the advent of silent reading, Manguel writes, 
... the reader was at last able to establish an unrestricted relationship with the book and the words. The words no longer needed to occupy the time required to pronounce them. They could exist in interior space, rushing on or barely begun, fully deciphered or only half-said, while the reader's thoughts inspected them at leisure, drawing new notions from them, allowing comparisons from memory or from other books left open for simultaneous perusal. 
To read silently is to free your mind to reflect, to remember, to question and compare. The cognitive scientist Maryanne Wolf calls this freedom "the secret gift of time to think": When the reading brain becomes able to process written symbols automatically, the thinking brain, the I, has time to go beyond those symbols, to develop itself and the culture in which it lives. 



16th-CENTURY INTERNET: The "book wheel," invented in 1588, was a rotating reading desk that allowed readers to flit among texts by giving the wheel a quick spin.
Wikipedia


A thousand years later, critics fear that digital technology has put this gift in peril. The Internet's flood of information, together with the distractions of social media, threaten to overwhelm the interior space of reading, stranding us in what the journalist Nicholas Carr has called "the shallows," a frenzied flitting from one fact to the next. In Carr's view, the "endless, mesmerizing buzz" of the Internet imperils our very being: "One of the greatest dangers we face," he writes, "as we automate the work of our minds, as we cede control over the flow of our thoughts and memories to a powerful electronic system, is ... a slow erosion of our humanness and our humanity."
There's no question that digital technology presents challenges to the reading brain, but, seen from a historical perspective, these look like differences of degree, rather than of kind. To the extent that digital reading represents something new, its potential cuts both ways. Done badly (which is to say, done cynically), the Internet reduces us to mindless clickers, racing numbly to the bottom of a bottomless feed; but done well, it has the potential to expand and augment the very contemplative space that we have prized in ourselves ever since we learned to read without moving our lips. 

Critics like to say the Internet causes our minds to wander off, but we've been wandering off all along.

The fear of technology is not new. In the fifth century B.C., Socrates worried that writing would weaken human memory, and stifle judgment. In fact, as Wolf notes in her 2007 book Proust and the Squid: The Story and Science of the Reading Brain, the opposite happened: Faced with the written page, the reader's brain develops new capacities. The visual cortex forms networks of cells that are capable of recognizing letterforms almost instantaneously; increasingly efficient pathways connect these networks to the phonological and semantic areas of the cortex, freeing up other parts of the brain to put the words we read into sentences, stories, views of the world. We may not keep the Iliad in our heads any longer, but we're exquisitely capable of reflecting on it, comparing it to other stories we know, and forming conclusions about human beings ancient and modern. 
The Internet may cause our minds to wander off, and yet a quick look at the history of books suggests that we have been wandering off all along. When we read, the eye does not progress steadily along the line of text; it alternates between saccades—little jumps—and brief stops, not unlike the movement of the mouse's cursor across a screen of hypertext. From the invention of papyrus around 3000 B.C., until about 300 A.D., most written documents were scrolls, which had to be rolled up by one hand as they were unrolled by the other: a truly linear presentation. Since then, though, most reading has involved codices, bound books or pamphlets, a major advantage of which (at least compared to the scroll) is that you can jump around in them, from chapter to chapter (the table of contents had been around since roughly the first century B.C.); from text to marginal gloss, and, later, to footnote. 
In the age of print, nonlinear reading found its most elaborate support in the "book wheel," invented by the Italian engineer Agostino Ramelli in 1588: a "rotary reading desk" which allowed the reader to keep a great number of books at once, and to switch between them by giving the wheel a turn. The book wheel was— unfortunately!—a rarity in European libraries, but when you think about all the kinds of reading that print affords, the experience of starting a text at its beginning and reading all the way to the end, which we now associate with "deep" reading, looks less characteristic of print in general than of the novel in particular: the one kind of book in which, we feel, we might be depriving ourselves of something vital if we skipped or skimmed. 
The quality of digital media poses one kind of problem for the reading brain; the quantity of information available to the wired reader poses a different and more serious problem. But it's worth noting that readers have faced this problem before, too. Gutenberg printed his first Bible in 1455, and by 1500, some 27,000 titles had been published in Europe, in a total of around 10 million copies. The flood of printed matter created a reading public, and changed the way that people read. 




The German historian Rolf Engelsing argues that a "reading revolution" took place at the end of the 18th century: Before that point, the typical European reader had only a few books—the Bible, an almanac, maybe a work of devotional literature—and he read them over and over, so that they were deeply impressed on his consciousness. Afterward, Europeans read all kinds of material—novels, periodicals, newspapers—and they read each item only once before racing on to the next. Contemporary critics were doubtless appalled, but on the other hand, from that flood of printed matter, we got the Enlightenment, Romanticism, the American and French revolutions. 
It's true that studies have found that readers given text on a screen do worse on recall and comprehension tests than readers given the same text on paper. But a 2011 study by the cognitive scientists Rakefet Ackerman and Morris Goldsmith suggests that this may be a function less of the intrinsic nature of digital devices than of the expectations that readers bring to them. Ackerman and Goldsmith note that readers perceive paper as being better suited for "effortful learning," whereas the screen is perceived as being suited for "fast and shallow reading of short texts such as news, e-mails, and forum notes." They tested the hypothesis that our reading habits follow from this perception, and found it to be correct: Students asked to read a text on-screen thought they could do it faster than students asked to read the same text in print, and did a worse job of pacing themselves in a timed study period. Not surprisingly, the on-screen readers then scored worse on a reading comprehension test. 
If those same students expected on-screen reading to be as slow (and as effortful) as paper reading, would their comprehension of digital text improve? A 2015 study by the German educator Johannes Naumann suggests as much. Naumann gave a group of high-school students the job of tracking down certain pieces of information on websites; he found that the students who regularly did research online—in other words, the ones who expected Web pages to yield up useful facts—were better at this task (and at ignoring irrelevant information) than students who used the Internet mostly to send email, chat, and blog. 
Meanwhile, some writers are taking advantage of the formal possibilities of digital media to tell stories and communicate information in new ways. One of these new forms is what people in the 1990s called "hypertext": text divided into units called "lexia," which are connected by links, sometimes in a branching or tree-like structure, sometimes in webs or cats'-cradles or other tangled forms. (Technically, the Web is a hypertext, but the word often refers to single works with an internally linked structure.) 

The digital novel Pry is the opposite of a shallow work; its whole play is between the surface and the depths of the human mind. It is exhilarating.

The impact of hypertext on the reading brain has, as you'd expect, received a fair amount of scientific attention. In 2005, the psychologists Diana DeStefano and Jo-Anne LeFevre reviewed 38 studies of hypertext reading; their expectation was that hypertext would be found to impose a greater cognitive load on the reader than linear text, because of the effort involved in scanning a page for links, and deciding which link, if any, to follow. DeStefano and LeFevre further hypothesized that this increased cognitive load would cause readers' recall and comprehension to suffer. They concluded that this expectation was, generally speaking, correct, and Carr cited it in his 2011 book The Shallows, as evidence that the Internet is making us stupid. 
In fact, though, DeStefano and LeFevre's findings were equivocal. The cognitive load imposed by hypertext doesn't correspond in a straightforward way to the number of choices presented at a decision point, or to the total number of links in a hypertext. (Indeed, a 1996 study by Michael Wenger and David Payne found that hypertext did not impose a greater cognitive load on readers than linear text, a result that DeStefano and LeFevre note in passing.) In two studies, hypertext seemed to improve comprehension. One involved readers with little prior knowledge of a subject, who were able to use a highly structured hypertext (one whose structure mirrored the organization of its subject matter) to learn more effectively than similar readers of linear text. In the other study, academically gifted readers learned better from unstructured hypertext than from linear text. The author, Amy Shapiro, hypothesized that these readers were obliged to engage more actively with the hypertext, in order to figure out the relation between its parts; this engagement led to increased understanding, the way puzzling over a difficult poem yields more than reading quickly through an easy one. 
DeStefano and LeFevre also remark that "[f]ew of the studies that we reviewed considered affective factors such as engagement or enjoyment." This may seem like a small point, but it's not. In a 2008 study by the psychologists Tal Yarkoni, Nicole Speer, and Jeffrey Zacks, subjects were given two narratives to read, while their brain activity was monitored by a functional MRI scanner. One narrative was a fairly straightforward account of a day in a boy's life; the other was the same account with the sentences scrambled. Here's a bit of the latter: 
Mrs. Birch called in a pleasant tone, "Raymond, take a bath and then you can go to bed." Raymond noticed this immediately and asked curiously, "Am I four feet high?" He stood and went toward them in a slow, jogging run. 
Based on the fMRI data, Yarkoni, Speer, and Zacks concluded that the scrambled sentences forced the readers to keep remaking their "situation models," their mental representations of what was happening in the story. Situation models guide reading comprehension and memory; without them, we get lost, which explains, in neuropsychological terms, why the scrambled sentences were harder to remember. And yet, when I read the two experimental texts, I found myself thinking about how much more interesting the scrambled one was, and how much more fun it was to read. Maybe I'm just the kind of person who likes building situation models, but I don't think I'm alone in this. If there were no pleasure in reading things that don't make sense, who would read the Surrealists? Who would giggle at bad subtitles, or Mad Libs? 
Comprehension matters, but so does pleasure. In Proust and the Squid, Wolf, director of the Center for Reading and Language Research at Tufts University, observes that the brain's limbic system, the seat of our emotions, comes into play as we learn to read fluently; our feelings of pleasure, disgust, horror and excitement guide our attention to the stories we can't put down. Novelists have known this for a long time, and digital writers know it, too. It's no coincidence that many of the best early digital narratives took the form of games, in which the reader traverses an imaginary world while solving puzzles, sometimes fiendishly difficult ones. Considered in terms of cognitive load, these texts are head-bangingly difficult; considered in terms of pleasure, they're hard to beat. 



SHAPE OF READING TO COME: Reading's future is signaled by interactive novels like Pry, which, along with text, uses video clips to expose its protagonist's memories.
Samantha Gorman and Danny Cannizzaro


A new generation of digital writers is building on video games, incorporating their interactive features—and cognitive sparks—into novelistic narratives that embrace the capabilities of our screens and tablets. Samantha Gorman and Danny Cannizzaro's 2014 iPad novella, Pry, tells the story of a demolitions expert returned home from the first Gulf War, whose past and present collide, as his vision fails. The story is told in text, photographs, video clips, and audio. It uses an interface that allows you to follow the action and shift between levels of awareness. As you read text on the screen, describing characters and plot, you draw your fingers apart and see a photograph of the protagonist, his eyes opening on the world. Pinch your fingers shut and you visit his troubled unconscious; words and images race by, as if you are inside his memory. Pry is the opposite of a shallow work; its whole play is between the surface and the depths of the human mind. Reading it is exhilarating. 
There's no question when you read (or play) Pry that you're doing something your brain isn't quite wired for. The interface creates a feeling of simultaneity, and also of having to make choices in real time, that no book could reproduce. It asks you to use your fingers to do more than just turn the page. It communicates the experience of slipping in and out of a story, in and out of a dream, or nightmare. It uses the affordances of your phone or tablet to do what literature is always trying to do: give you new things to think about, to expand the world behind your eyes. It's stressful, at first. How are you supposed to know if you're reading it right? What if you miss something? But if you play (or read) it long enough, you can almost feel your brain begin to adapt. 
Most of the Web is not like Pry—not yet, anyway. But the history of reading suggests that what we're presently experiencing is probably not the end times of human thought. It's more like an interregnum, or the crouch before a leap. Wolf points out that when it comes to reading, what we get out is largely what we put in. "The reading brain circuit reflects the affordances of what it reads," she notes: affordances being the built-in opportunities for interaction. The more we skim, the more we're likely to keep skimming; on the other hand, the more we plunge into a text, the more we're likely to keep plunging. "We're in a digital culture," Wolf says. "It's not a question of making peace. We have to be discerning, vigilant, developmentally savvy." And of course we have to be surprised, delighted, puzzled, even disturbed. We have to enjoy ourselves. If we can do that, digital reading will expand the already vast interior space of our humanity. 



Paul La Farge is the author of three novels: The Artist of the Missing, Luminous Airplanes, and Haussmann, or the Distinction;and a book of imaginary dreams, The Facts of Winter. 




This article was originally published in our "Space" issue in January, 2016.









Chapter Four
Bugs & Bots











Culture | Literature
Why Doesn't Ancient Fiction Talk About Feelings?
Literature's evolution has reflected and spurred the growing complexity of society.
By Julie Sedivy
♦ 

Reading medieval literature, it's hard not to be impressed with how much the characters get done—as when we read about King Harold doing battle in one of the Sagas of the Icelanders, written in about 1230. The first sentence bristles with purposeful action: "King Harold proclaimed a general levy, and gathered a fleet, summoning his forces far and wide through the land." By the end of the third paragraph, the king has launched his fleet against a rebel army, fought numerous battles involving "much slaughter in either host," bound up the wounds of his men, dispensed rewards to the loyal, and "was supreme over all Norway." What the saga doesn't tell us is how Harold felt about any of this, whether his drive to conquer was fueled by a tyrannical father's barely concealed contempt, or whether his legacy ultimately surpassed or fell short of his deepest hopes. 
Jump ahead about 770 years in time, to the fiction of David Foster Wallace. In his short story "Forever Overhead," the 13-year-old protagonist takes 12 pages to walk across the deck of a public swimming pool, wait in line at the high diving board, climb the ladder, and prepare to jump. But over these 12 pages, we are taken into the burgeoning, buzzing mind of a boy just erupting into puberty—our attention is riveted to his newly focused attention on female bodies in swimsuits, we register his awareness that others are watching him as he hesitates on the diving board, we follow his undulating thoughts about whether it's best to do something scary without thinking about it or whether it's foolishly dangerous not to think about it. 
These examples illustrate Western literature's gradual progression from narratives that relate actions and events to stories that portray minds in all their meandering, many-layered, self-contradictory complexities. I'd often wondered, when reading older texts: Weren't people back then interested in what characters thought and felt? 



ACTION FIGURE: In ancient literature, emotions were predictable reactions to external actions or events. Here King Harold's death is depicted in the Bayeux Tapestry. The embroidery states, "Harold the King was killed."
Wikipedia


Perhaps people living in medieval societies were less preoccupied with the intricacies of other minds, simply because they didn't have to be. When people's choices were constrained and their actions could be predicted based on their social roles, there was less reason to be attuned to the mental states of others (or one's own, for that matter). The emergence of mind-focused literature may reflect the growing relevance of such attunement, as societies increasingly shed the rigid rules and roles that had imposed order on social interactions. 
But current psychological research hints at deeper implications. Literature certainly reflects the preoccupations of its time, but there is evidence that it may also reshape the minds of readers in unexpected ways. Stories that vault readers outside of their own lives and into characters' inner experiences may sharpen readers' general abilities to imagine the minds of others. If that's the case, the historical shift in literature from just-the-facts narration to the tracing of mental peregrinations may have had an unintended side effect: helping to train precisely the skills that people needed to function in societies that were becoming more socially complex and ambiguous. 



We humans owe our intensely social natures to biological evolution. We're genetically endowed with a social intelligence that extends far beyond the reach of our nearest primate relatives. Even toddlers understand that people's perspectives can differ from their own or that external actions are propelled by internal goals, and they are resistant to learning from adults whose knowledge appears dubious. But genes are only part of the story. We may come pre-equipped with a standard set of skills (a "start-up kit," in the words of researchers Cecilia Heyes and Chris Frith), but the ability to accurately grasp the thoughts and emotions of others, or mentalizing ability, varies quite a bit from person to person—and there's growing evidence that complex mentalizing skills are culturally transmitted through a slow learning process, much like reading or playing chess. For example, while babes-in-arms are sensitive to basic emotions such as happiness or sadness, the ability to recognize socially intricate emotions like embarrassment or guilt only emerges at age 7 or later, and continues to be polished up well into adulthood. 
The extent to which parents talk to their children about what others are thinking has been found to have profound effects on children's ability to discern the contents of other minds. A study by Rosie Ensor and her colleagues showed that the frequency with which mothers used words such as think, forget, wonder, learn, or pretend when their children were just 2 years old predicted their mentalizing skills at ages 3, 6, and even 10.1

Heavy readers of fiction showed the highest level of brain activity.

It's unlikely that these results arise from underlying genetic differences shared by parents and children—that is, that parents talk more about mental states because they themselves have better mentalizing abilities, which their children in turn are likely to inherit. Evidence for a direct role of language comes from psychologists Jennie Pyers and Ann Senghas, who studied deaf adults exposed to Nicaraguan Sign Language, a language that recently emerged when the Nicaraguan government began educating deaf children together in one national school.2 What began as a simple gesturing system has flowered into an elaborate and complex language, allowing researchers to study the birth and development of an entirely new language and its community. 
Pyers and Senghas compared some of the earliest signers, who had learned the language in its more rudimentary form, with a group of younger signers who had learned the language at a later, more complex stage. They found that the early signers used fewer verbs describing mental states than those who had learned the later version of the language; they also performed worse on a test that probed for their ability to discern others' beliefs. But when the researchers returned two years later, they found that the younger signers had graduated from school and begun to interact with the older signers, using the more complex version of Nicaraguan Sign Language. As a result, the older signers now used as many mental state verbs as their younger peers and performed just as well on the mentalizing test. Language had done for them what 25 years of less wordy social interaction had not. 
Nicaraguan Sign Language presents an elegant analogy: Just as it has incorporated, over several decades, new vocabulary for talking about mental states, Western literature has evolved, over several centuries, new literary techniques for expressing the mental states of characters. 
As noted by literary scholar Monika Fludernik, medieval authors represented characters' mental states mainly through their direct speech and gestures, which were used to convey intense emotions in a stereotypical way—lots of hand-wringing and tearing of hair, but few subtle gestures such as raised eyebrows or faint smiles flickering over lips. The direct reporting of emotion was fairly common, but mostly kept short and simple ("He was afraid"). Moreover, emotions were usually predictable reactions to external actions or events, revealing little about a character that was complex or surprising. 
Elizabeth Hart, a specialist in early literature, writes that in medieval or classical texts, "people are constantly planning, remembering, loving, fearing, but they somehow manage to do this without the author drawing attention to these mental states." This changed dramatically between 1500 and 1700, when it became common for characters to pause in the middle of the action, launching into monologues as they struggled with conflicting desires, contemplated the motives of others, or lost themselves in fantasy—as is familiar to anyone who's studied the psychologically rich soliloquies of Shakespeare's plays. Hart suggests that these innovations were spurred by the advent of print, and with it, an explosion in literacy across classes and genders. People could now read in private and at their own pace, re-reading and thinking about reading, deepening a new set of cognitive skills and an appetite for more complex and ambiguous texts. 



'TIS NOBLER IN THE MIND: Between 1500 and 1700, it became common for characters to pause in the middle of the action and launch into monologues of conflicting desires, familiar to anyone who's studied the rich psychology of Shakespeare's plays.
Danita Delimont


The emergence of the novel in the 18th and 19th centuries introduced omniscient narrators who could penetrate their characters' psyches, at times probing motives that were opaque to the characters themselves. And by the 20th century, many authors labored not just to describe, but to simulate the psychological experience of characters. In her literary manifesto "Modern Fiction," Virginia Woolf wrote, "Let us record the atoms as they fall upon the mind in the order in which they fall, however disconnected and incoherent in appearance, which each sight or incident scores upon the consciousness." 
This clarion call was taken up by Dorothy Parker, as in the following passage of "Sentiment," where she shapes sentences into obsessive, rhythmic loops of thought: "But I knew. I knew. I knew because he had been far away from me long before he went. He's gone away and he won't come back. He's gone away and he won't come back, he's gone away and he'll never come back. Listen to the wheels saying it, on and on and on." 
For Parker and many writers since, all facets of language—from sound to imagery to syntax—are tools for conveying mental states. 



If mentalizing skills can be burnished by language that draws attention to mental states, has literature's increasing use of such language improved readers' social intelligence over the centuries? Psychologists can't go back to the 1200s to administer batteries of tests to medieval denizens, but they can test and compare present-day humans whose reading habits differ. 
Such research shows a clear link between people's mentalizing skills and the books on their nightstands. In a study led by Raymond Mar, voracious readers of fiction were better than lighter consumers of fiction at making nuanced social judgments based on limited information—for example, deciphering complex emotions by looking at photographs of people's eyes, and using subtle cues in videos of social interactions (such as guessing who was the child of the two adults in the video based on body language, tone of voice, and other nonverbal information).3 Heavy readers of expository nonfiction showed the opposite pattern, performing worse than lighter readers of nonfiction. Other research, using similar tests, has found a specific advantage for reading literary fiction4 compared with popular genre fiction, or for romance fiction5 over science fiction. 
These studies don't prove that a particular literary diet nourishes social intelligence; it's hard to rule out the possibility that people who are more attuned to other minds are simply more interested in reading about them in the first place, in which case, reading habits would be one result of social intelligence. The ideal experiment would randomly assign people to different reading regimens over a sustained period and then compare the effects. 

A story that was strong enough to shift the needle of my moral compass was "93990" by George Saunders.

A slightly more practical (and modest) attempt to demonstrate causality was undertaken by David Kidd and Emanuele Castano; in their experiments, volunteers were randomly assigned to read a single text of either literary fiction, popular genre fiction, or nonfiction before taking a test of their ability to identify complex emotions based on photos that were tightly cropped around a subject's eyes.6 The results showed that those who had read the literary fiction text had higher scores than the others, suggesting that certain kinds of reading can stimulate mental processes that are relevant to identifying the emotions of others. Unlike formulaic popular fiction, which tends to rely on stereotypical characters and transparent motives, characters in literary fiction act in surprising and ambiguous ways that spill beyond the confines of familiar scripts. In a typical thriller (or medieval saga, for that matter) a character might respond to the murder of a wife with homicidal vengeance, a reaction that requires little analysis; but if he were to send the killer letters impersonating the dead wife—a literary novel just waiting to be written!—this would trigger deeper speculation about his motives and mental state. 
Kidd and Castano's study is controversial, in part because of its lack of clear criteria for determining the categories of "literary fiction" versus "popular fiction," and in part because several recent studies have failed to replicate7 its results (though similar findings8 have been reported for watching an award-winning TV drama versus a documentary). 
Nonetheless, additional research using brain imaging supports their general claims, showing that at least some of the time, reading can stimulate the same mental processes that are involved in deciphering other minds. Diana Tamir and her colleagues found that different patterns of brain activity were elicited by passages of text that were rich in social content compared with passages loaded with vivid spatial details.9 In reading the social passages, people fired up the same brain network that is active in performing various tests of mentalizing skills. Moreover, consistent with the hypothesis that such texts can train social intelligence, heavy readers of fiction showed the highest level of brain activity in the mentalizing network during reading. 
Another study, conducted in the Netherlands, also found heightened activity in the mentalizing network in response to literary passages that described characters' thoughts, desires, or beliefs.10 In contrast, action-focused passages provoked activity in a very different network involving the visual and motor cortices. Although this study didn't delve into the literary habits of the volunteers, individual brains differed in striking ways: Subjects seemed to divide up into those whose brains were most responsive to action sequences, versus those whose brains resonated to thinking about characters. 
Overall, there is mounting evidence for literature's potential to reshape the mind. But we still know little about which qualities of a text, or which literary techniques, best arouse the mentalizing network. And, as the brain imaging evidence suggests, the neural activity provoked by any given text may depend largely on the reader—not just on what is being read, but what has been read in the past, and how the reader is now approaching the text. 



Aside from psychological experiments with present-day subjects, we can also look closely at literature itself for clues about the mentalizing powers of readers throughout history. All authors make choices about how much to state explicitly and how much to leave implicit. These choices reveal authors' tacit assumptions about how large a gap between language and intention their readers will be able to leap over, how well their readers will be able to elaborate thoughts that are underspecified by language itself. 
Contemporary literature is full of broad gaps. The author Margaret Atwood notes that her own writing was influenced by Beatrix Potter, whom she describes as a master of oblique discourse.11 In The Tale of Mr. Tod, Benjamin Bunny and Peter Rabbit are in pursuit of Tommy Brock, a badger who has captured Benjamin's children in a bag and is headed home, where he will likely eat them. On the way, the two rabbits pass the house of Cottontail Bunny, and ask if her husband, a black rabbit, is home, presumably to ask for his help in confronting Tommy Brock. In response, Cottontail says nothing about her husband, but simply states, "Tommy Brock had rested twice while she watched him." As the two rabbits continue their pursuit, Peter says, "He was at home; I saw his black ears peeping out of the hole." Benjamin replies, "They live too near the rocks to quarrel with their neighbours ..." 
Atwood writes, "At the age of four, I quickly grasped that Cottontail had lied, but the 'rocks' remark took some thought. Finally, I got it: Tommy Brock has a shovel, and those that live in burrows too near the rocks are easy to catch by digging. Long-term craft lesson: no need to spell everything out because the reader is the co-creator of the story and can be depended on to pick up the dropped clues." 

Research shows a clear link between people's mentalizing skills and the books on their nightstands.

Atwood was undoubtedly a precocious 4-year-old, but there is evidence that average children can pick up such dropped clues, and that this process not only activates mentalizing networks in the brain, but that it hones these skills even more than the explicit labeling of mental states. In one study, kindergarten children heard stories such as Rosie's Walk, in which a hen walks through a barnyard, appearing to be blithely unaware that she is being followed by a fox.12 A second group of children heard the same stories, but with mental states clearly identified. "Rosie heard the loud BUMP but did she figure out that the hungry fox was behind her? No, she didn't turn around. She doesn't know that he's behind her." The first group, forced to read between the lines, later performed better than the second group on a test that required them to infer the beliefs of others. 
When an author expresses deep confidence in a reader and creates a space in which the reader can, from the depths of her own social imagination, lower her consciousness into the body and experiences of another, the effect can be transformational. I've read many pieces over the years that have addressed the ethical implications of experimentation with animals. But the only one that provoked an empathic response strong enough to shift the needle of my moral compass was a story by George Saunders titled "93990." Which is striking, because this story is written in the dispassionate form of a lab report documenting the effects of a toxic substance administered to a group of 20 monkeys. There are virtually no mental state verbs; no subjective interpretations or introspections; no incursions into anyone's consciousness. The story borrows the robes of academic language to describe, with utter detachment, the growing distress and eventual deaths of the test subjects. 
Of course, it's precisely because the sterile language of the story refuses to acknowledge the inner experiences of either the test subjects or the experimenters that the story had such an effect on me. Rather than drawing attention to the mental states of his characters, Saunders was inviting me to reflect on them by creating a chasm between the horrors of what the experimenter was observing, and the language in which it was being observed. 
The effect is deeply moving. And intimate: It is as if Saunders himself has beckoned me over and silently parted a curtain, inviting me to stand next to him and watch the events unfolding behind it. With no one's thoughts ever overtly expressed or described, I find myself thinking of the physical and mental agony of the animals and the conditioned indifference of the scientists, and thinking about the mind of Saunders thinking about the minds of the animals and the minds of the scientists we are watching together. 



Julie Sedivy has taught linguistics and psychology at Brown University and the University of Calgary, and is the author of Language in Mind: An Introduction to Psycholinguistics. She is currently writing a book about losing and reclaiming a native tongue.




References

1. Ensor, R., Devine, R.T., Marks, A., & Hughes, C. Mothers' cognitive references to 2-year-olds predict theory of mind at ages 6 and 10. Child Development85, 1222-1235 (2014). 
2. Pyers, J.E. & Shenghas, A. Language promotes false-belief understanding: Evidence from learners of a new sign language. Psychological Science20, 805-812 (2009). 
3. Mar, R.A., Oatley, K., Hirsh, J., dela Paz, J., & Peterson, J.B. Bookworms versus nerds: Exposure to fiction versus non-fiction, divergent associations with social ability, and eh simulation of fictional social worlds. Journal of Research in Personality40, 694-712 (2006). 
4. Kidd, D. & Castano, E. Different stories: How levels of familiarity with literacy and genre fiction relate to mentalizing. Psychology of Aesthetics, Creativity, and the Arts (2016). Retrieved from DOI: 10.1037/aca0000069 
5. Fong, K., Mullin, J.B., & Mar, R.A. What you read matters: The role of fiction genre in predicting interpersonal sensitivity. Psychology of Aesthetics, Creativity, and the Arts7, 370-376 (2013). 
6. Kidd, D.C. & Castano, E. Reading literary fiction improves theory of mind. Science342, 377-380 (2013). 
7. Panero, M.E., et al. Does reading a single passage of literary fiction really improve theory of mind? An attempt at replication. Journal of Personality and Social Psychology111, e46-e54 (2016). 
8. Black, J. & Barnes, J.L. Fiction and social cognition: The effect of viewing award-winning television dramas on theory of mind. Psychology of Aesthetics, Creativity, and the Arts9, 423-429 (2015). 
9. Tamir, D.I., Bricker, A.B., Dodell-Feder, D., & Mitchell, J.P. Reading fiction and reading minds: The role of simulation in the default network. Social Cognitive and Affective Neuroscience11, 215-224 (2016). 
10. Nijhof, A.D. & Willems, R.M. Simulating fiction: Individual differences in literature comprehension revealed with fMRI. PLOS One (2015). Retrieved from DOI:10.1371/journal.pone.0116492 
11. The Stories That Changed Margaret Atwood's Life www.cbc.ca (2013). 
12. Peskin, J. & Astington, J.W. The effects of adding metacognitive language to story texts. Cognitive Development19, 253-273 (2004). 




Lead photo collage credits: Fernando Cortes / gilotyna4 / Shutterstock













Numbers | Artificial Intelligence
We Need Conscious Robots
How introspection and imagination make robots better.
By Ryota Kanai
♦ 
People often ask me whether human-level artificial intelligence will eventually become conscious. My response is: Do you want it to be conscious? I think it is largely up to us whether our machines will wake up. 
That may sound presumptuous. The mechanisms of consciousness—the reasons we have a vivid and direct experience of the world and of the self—are an unsolved mystery in neuroscience, and some people think they always will be; it seems impossible to explain subjective experience using the objective methods of science. But in the 25 or so years that we've taken consciousness seriously as a target of scientific scrutiny, we have made significant progress. We have discovered neural activity that correlates with consciousness, and we have a better idea of what behavioral tasks require conscious awareness. Our brains perform many high-level cognitive tasks subconsciously. 
Consciousness, we can tentatively conclude, is not a necessary byproduct of our cognition. The same is presumably true of AIs. In many science-fiction stories, machines develop an inner mental life automatically, simply by virtue of their sophistication, but it is likelier that consciousness will have to be expressly designed into them. 



H. Armstrong Roberts / ClassicStock / Getty Images


And we have solid scientific and engineering reasons to try to do that. Our very ignorance about consciousness is one. The engineers of the 18th and 19th centuries did not wait until physicists had sorted out the laws of thermodynamics before they built steam engines. It worked the other way round: Inventions drove theory. So it is today. Debates on consciousness are often too philosophical and spin around in circles without producing tangible results. The small community of us who work on artificial consciousness aims to learn by doing. 
Furthermore, consciousness must have some important function for us, or else evolution wouldn't have endowed us with it. The same function would be of use to AIs. Here, too, science fiction might have misled us. For the AIs in books and TV shows, consciousness is a curse. They exhibit unpredictable, intentional behaviors, and things don't turn out well for the humans. But in the real world, dystopian scenarios seem unlikely. Whatever risks AIs may pose do not depend on their being conscious. To the contrary, conscious machines could help us manage the impact of AI technology. I would much rather share the world with them than with thoughtless automatons. 



When AlphaGo was playing against the human Go champion, Lee Sedol, many experts wondered why AlphaGo played the way it did. They wanted some explanation, some understanding of AlphaGo's motives and rationales. Such situations are common for modern AIs, because their decisions are not preprogrammed by humans, but are emergent properties of the learning algorithms and the data set they are trained on. Their inscrutability has created concerns about unfair and arbitrary decisions. Already there have been cases of discrimination by algorithms; for instance, a Propublica investigation last year found that an algorithm used by judges and parole officers in Florida flagged black defendants as more prone to recidivism than they actually were, and white defendants as less prone than they actually were. 
Beginning next year, the European Union will give its residents a legal "right to explanation." People will be able to demand an accounting of why an AI system made the decision it did. This new requirement is technologically demanding. At the moment, given the complexity of contemporary neural networks, we have trouble discerning how AIs produce decisions, much less translating the process into a language humans can make sense of. 

In the real world, dystopian scenarios seem unlikely.

If we can't figure out why AIs do what they do, why don't we ask them? We can endow them with metacognition—an introspective ability to report their internal mental states. Such an ability is one of the main functions of consciousness. It is what neuroscientists look for when they test whether humans or animals have conscious awareness. For instance, a basic form of metacognition, confidence, scales with the clarity of conscious experience. When our brain processes information without our noticing, we feel uncertain about that information, whereas when we are conscious of a stimulus, the experience is accompanied by high confidence: "I definitely saw red!" 
Any pocket calculator programmed with statistical formulas can provide an estimate of confidence, but no machine yet has our full range of metacognitive ability. Some philosophers and neuroscientists have sought to develop the idea that metacognition is the essence of consciousness. So-called higher-order theories of consciousness posit that conscious experience depends on secondary representations of the direct representation of sensory states. When we know something, we know that we know it. Conversely, when we lack this self-awareness, we are effectively unconscious; we are on autopilot, taking in sensory input and acting on it, but not registering it. 

These theories have the virtue of giving us some direction for building conscious AI. My colleagues and I are trying to implement metacognition in neural networks so that they can communicate their internal states. We call this project "machine phenomenology" by analogy with phenomenology in philosophy, which studies the structures of consciousness through systematic reflection on conscious experience. To avoid the additional difficulty of teaching AIs to express themselves in a human language, our project currently focuses on training them to develop their own language to share their introspective analyses with one another. These analyses consist of instructions for how an AI has performed a task; it is a step beyond what machines normally communicate—namely, the outcomes of tasks. We do not specify precisely how the machine encodes these instructions; the neural network itself develops a strategy through a training process that rewards success in conveying the instructions to another machine. We hope to extend our approach to establish human-AI communications, so that we can eventually demand explanations from AIs. 



Besides giving us some (imperfect) degree of self-understanding, consciousness helps us achieve what neuroscientist Endel Tulving has called "mental time travel." We are conscious when predicting the consequences of our actions or planning for the future. I can imagine what it would feel like if I waved my hand in front of my face even without actually performing the movement. I can also think about going to the kitchen to make coffee without actually standing up from the couch in the living room. 
In fact, even our sensation of the present moment is a construct of the conscious mind. We see evidence for this in various experiments and case studies. Patients with agnosia who have damage to object-recognition parts of the visual cortex can't name an object they see, but can grab it. If given an envelope, they know to orient their hand to insert it through a mail slot. But patients cannot perform the reaching task if experimenters introduce a time delay between showing the object and cueing the test subject to reach for it. Evidently, consciousness is related not to sophisticated information processing per se; as long as a stimulus immediately triggers an action, we don't need consciousness. It comes into play when we need to maintain sensory information over a few seconds. 



Sovfoto / Getty Images


The importance of consciousness in bridging a temporal gap is also indicated by a special kind of psychological conditioning experiment. In classical conditioning, made famous by Ivan Pavlov and his dogs, the experimenter pairs a stimulus, such as an air puff to the eyelid or an electric shock to a finger, with an unrelated stimulus, such as a pure tone. Test subjects learn the paired association automatically, without conscious effort. On hearing the tone, they involuntarily recoil in anticipation of the puff or shock, and when asked by the experimenter why they did that, they can offer no explanation. But this subconscious learning works only as long as the two stimuli overlap with each other in time. When the experimenter delays the second stimulus, participants learn the association only when they are consciously aware of the relationship—that is, when they are able to tell the experimenter that a tone means a puff coming. Awareness seems to be necessary for participants to retain the memory of the stimulus even after it stopped. 
These examples suggest that a function of consciousness is to broaden our temporal window on the world—to give the present moment an extended duration. Our field of conscious awareness maintains sensory information in a flexible, usable form over a period of time after the stimulus is no longer present. The brain keeps generating the sensory representation when there is no longer direct sensory input. The temporal element of consciousness can be tested empirically. Francis Crick and Christof Koch proposed that our brain uses only a fraction of our visual input for planning future actions. Only this input should be correlated with consciousness if planning is its key function. 



A common thread across these examples is counterfactual information generation. It's the ability to generate sensory representations that are not directly in front of us. We call it "counterfactual" because it involves memory of the past or predictions for unexecuted future actions, as opposed to what is happening in the external world. And we call it "generation" because it is not merely the processing of information, but an active process of hypothesis creation and testing. In the brain, sensory input is compressed to more abstract representations step by step as it flows from low-level brain regions to high-level ones—a one-way or "feedforward" process. But neurophysiological research suggests this feedforward sweep, however sophisticated, is not correlated with conscious experience. For that, you need feedback from the high-level to the low-level regions. 
Counterfactual information generation allows a conscious agent to detach itself from the environment and perform non-reflexive behavior, such as waiting for three seconds before acting. To generate counterfactual information, we need to have an internal model that has learned the statistical regularities of the world. Such models can be used for many purposes, such as reasoning, motor control, and mental simulation. 

If we can't figure out why AIs do what they do, why don't we ask them?

Our AIs already have sophisticated training models, but they rely on our giving them data to learn from. With counterfactual information generation, AIs would be able to generate their own data—to imagine possible futures they come up with on their own. That would enable them to adapt flexibly to new situations they haven't encountered before. It would also furnish AIs with curiosity. When they are not sure what would happen in a future they imagine, they would try to figure it out. 
My team has been working to implement this capability. Already, though, there have been moments when we felt that AI agents we created showed unexpected behaviors. In one experiment, we simulated agents that were capable of driving a truck through a landscape. If we wanted these agents to climb a hill, we normally had to set that as a goal, and the agents would find the best path to take. But agents endowed with curiosity identified the hill as a problem and figured out how to climb it even without being instructed to do so. We still need to do some more work to convince ourselves that something novel is going on. 
If we consider introspection and imagination as two of the ingredients of consciousness, perhaps even the main ones, it is inevitable that we eventually conjure up a conscious AI, because those functions are so clearly useful to any machine. We want our machines to explain how and why they do what they do. Building those machines will exercise our own imagination. It will be the ultimate test of the counterfactual power of consciousness. 




Ryota Kanai is a neuroscientist and AI researcher. He is the founder and CEO of Araya, a Tokyo-based startup aiming to understand the computational basis of consciousness and to create conscious AI. @Kanair


Lead image originally from Dano / Flickr












Culture | Linguistics
Ingenious: Julie Sedivy
Why language both enraptures and deceives us.
By Kevin BergerProduced by Yvonne Bang
♦ 

The purpose of language is to reveal the contents of our minds, says Julie Sedivy. It's a simple and profound insight. We are social animals and language is what springs us from our isolated selves and unites us with others. 
Sedivy has taught linguistics and psychology at Brown University and the University of Calgary. She specializes in psycholinguistics, the psychology of language, notably the psychological pressures that give birth to language and comprehension. 
More recently Sedivy has been writing about language in her own life. She was born in Czechoslovakia, spent time as a kid in Austria and Italy, and came of age in Canada. She speaks Czech, French, and English, and gets by in Spanish, Italian, and German. 
In "The Strange Persistence of First Languages," published in Nautilus we're proud to say, Sedivy explored how revisiting her first language, Czech, brought her closer to her late father and revived memories of her own past. "I've discovered that my native language has been sitting quietly in my soul's vault all this time," she wrote. 
This week in Nautilus Sedivy shines the beam of her language knowledge on literature. In her essay, "Why Doesn't Ancient Fiction Talk About Feelings?" she burrows into the evolution of literature and how its shift to the interior life of the mind has reflected the expanding complexities of society. 
In the following interview, from her home in Calgary, Sedivy explains how language both enraptures and deceives. She explains why Donald Trump's language shrinks the world into gross simplicities. Our discussion delves into what makes good fiction and bad writing. Throughout, in her words and insights, Sedivy reveals what makes her a true language expert. 




Interview Transcript


How did language shape human evolution?

That's entering a very speculative domain, but we can have some insights into that by looking, for example, at populations that exist now that maybe don't have access to language. For instance, many deaf people around the world are still raised in environments where they're not really given access to a language because their senses don't allow them to take in the ambient language around them. Unless they're put together with other speakers who use a signed modality of language, they might spend most or even all of their lives without access to language. 
There are some interesting studies that look at what happens when you take someone like this and you now add the experience of language. There's a very interesting study, for example, by Jennie Pyers and Annie Senghas that looked at Nicaraguan sign language signers as a result of their exposure to language. 
One of the things that language allows us to do, I mentioned that the purpose of language is largely to share the contents of our minds with other people. One way that we do that is, we often directly refer to contents of minds by using verbs like, "I think," "he knows," "she believes," "she's pretending." These kinds of verbs signal something about what's in other people's minds. 
It turns out that exposure to language and in particular to language that expresses the idea of peering into other people's minds seems to actually hone our ability to read minds, even in non-verbal domains. For example, we become better at deciphering people's complicated emotions based on their facial expressions as a result of this. 

Aren't we winging it when we "read" another's mind?

Yeah, and to some extent we're always winging it because we're operating from our own senses and our own perspectives; and to a large extent we often have to suppress that information when we try to project ourselves into the mind of another person. There's this complicated shifting out of your own body and out of your own mind that needs to happen and that turns out to be sometimes quite difficult to disengage from. 

Language is the medium with which we communicate with ourselves.

At the same time, I think we have extremely sophisticated abilities to have some good hypotheses about the contents of people's minds, even on the basis of very, very indirect information. One way we know this is by looking at the gap between the meaning that's communicated directly by language and the meaning that we have to infer from what the language gives us. This gap is absolutely pervasive through all of language. We don't ever go around fully specifying everything that's in our mind. We take shortcuts because we assume that the person we're talking to will be able to reconstruct a lot of that. 
Here's a very simple example. If I introduce you to someone as my biological mother, immediately you're thinking, "Oh. Oh, you have someone who gave birth to you and that person is distinct perhaps from the person who raised you." I haven't told you any of that. In fact, most people could truthfully describe their female parent by saying "my biological mother." But you have added a layer of meaning by virtue of your sensitivity to the fact that most of the time people strip away the details that this is the person who gave birth to you. If I am going to the trouble of giving you that detail, there must be some reason for it and immediately you start generating hypotheses about what's a likely reason for that? We do this kind of thing all the time, and much of the time very, very successfully. 

How does language help us understand ourselves?

Language is the medium with which we communicate with ourselves in a sense. It's the medium that we use to structure our own thoughts, to create our own narratives, to put order into impressions. I imagine just as labeling an object for a baby causes them to pay attention to certain aspects and then to its relationships to other objects, that doing that within ourselves in our own internal monologue might have the effect of creating certain categories within ourselves, allowing us to come to certain conclusions. I think we probably spend more time talking to ourselves than we do to anybody else and I suspect that a huge amount of shaping our thinking comes from this process. 

When does language fail us?

Language fails us in the ways that it's a very indirect mapping of the world around us. It strips information away, certain details of information away from the world, so it's a great simplifier. Again, you take a very simple concept like an apple and by applying a certain word to it, a symbolic word, we now have turned it into a schema and that takes away a lot of sensory detail that sometimes as writers, for example, people really struggle to recreate to put some of that sensory detail back in. 
I think the biggest way that language might fail us is in the way that it under-specifies reality. You can see that playing out in communication between people as well. Again, these gaps between the meaning that language is able to provide and what it is that we're trying to jam into language when we try to express ourselves, some of that gets lost and in many occasions the person that we're communicating with isn't picking up on all of that we're intending to jam into the language that we use. 

Samuel Beckett has called language a veil. Know what he means?

Yes. Yes, absolutely. In a sense, it is a mediator. A very, very simple example of this, is—someone asked me just the other day, "Why is it that words for animal sounds are different in different languages?" Pigs might oink in English, but they make a different sound in another language. The answer to that is we're basically trying to recreate this auditory stimulus by using the tools that language gives us, which are vowels and consonants now. Pigs don't oink or they don't make sounds in vowels and consonants, but that's what language gives us. 
We're trying to take this complex sound and pack it into a string of vowels and consonants, and that's going to be largely constrained by the particular language we speak because different languages have different solutions for how they string vowels and consonants together. That's just a very small example of the veil that language puts between the thing that it's describing and the way in which it's described. 

Can a powerful person use language to deceive?

Absolutely. I think the tools of language that a deceptor has at his or her disposal are huge. Certainly, directing attention, simplifying ... For example, here's maybe a less obvious way that you can deceive and this is something I think about a lot as someone who communicates science to a general audience. The notion of simplicity: What happens when you present complex information in simple language? To some extent you have to when you're dealing with an audience that maybe doesn't understand the complexity of the issues you're trying to get across. But if you strip away too much of the complexity, what you're communicating in part is that the ideas themselves, the issues are simple. 

The biggest way that language might fail us is in the way that it under-specifies reality.

There's some interesting research that has looked at the correlation between simple language and the tendency of U.S. presidents to behave in authoritarian ways. There is a predictive relationship that speeches that are expressed using very simple basic language tend to precede very authoritarian acts like the use of executive orders, for instance. Simplicity itself I think is something that can be deceptive. Certainly, for anyone who is in the field of journalism, science, politics, and who has an ethical preoccupation with deception—that just opens up a whole can of worms. 

What do you think of Trump's language?

Well, I think we have rarely had a president who uses such simple and simplifying language. That certainly plays out in the use of the heavy reliance on simple notions like amazing, sad, bad, unfair. These really strip away a lot of the complexities that are behind them. They reduce information into very gross impressions. The simplification of points of view, the simplification of the good and the bad, and even just the conveyance that, "We're going to make good deals," for example. "It's going to be great." That this is a simple problem just waiting for someone who has the right instincts to come along and solve this, is absolutely pervasive in Donald Trump's language. 

What's the downside of simplified language?

Well, I think the big downside is that it's false. The world is a complex place. It's not a simple environment. There are many interacting forces simultaneously that really elude simple explanations or simple solutions. One thing that I certainly have become very aware of through a couple of decades now of being a scientist is that for every simple, elegant explanation or theory we have come up with, we have discovered that the truth is actually not simple or elegant. It's messy, noisy, complex. 
That's a problem when we're trying to communicate it in simple language because we're essentially lying about the nature of the world. The great challenge is how do we communicate something about the complexity of the world in a way that's understandable, that's accessible, but that does its best to limit the amount of lying that we do? 

How has studying language shaped your own life?

The biggest effect is that it makes me walk around the social world in utter delight because I'm constantly noticing what's happening in social interactions or even in a cleverly constructed ad or a beautifully written passage. The training that I have as a linguist and as a psycholinguist allows me to just see, "Ah, I see what's going on there and why this works." There's this resonance of understanding that opens up for me. I think that's really been the greatest impact. 

For every simple, elegant explanation or theory we have come up with, we have discovered that the truth is actually not simple or elegant.

That's something that I try to impart to my students more than anything is the idea that you can apply this vast body of knowledge about how language works, the scientific method, and you can start to observe your daily interactions, your conversations, the choices of words that people make or don't make and the impacts that they have in a much more conscious and aware way and that just makes all of these interactions so much richer and more layered for me. 

Have your language studies influenced your appreciation of literature?

Hugely. I have always loved literature. I loved literature before I loved linguistics, before I knew linguistics even existed. I was always just resonating to the things that you could do with language. Again, now there's a sense of a double vision or a layered awareness that goes on when I read texts and also a sense of better understanding of what might go wrong when things go wrong. 
If I hit a clunky phrase, for instance, I often have a sense of, "Oh, well this is why it feels clunky because it's straining the memory system over here and if we made a small adjustment we can unravel that and make it much easier to process." I find that deeply pleasurable just to be able to have that heightened sensitivity to language. 
A thing that annoys me is that when I talk to visual artists or photographers, they have some access to scientific knowledge about visual perception that they very easily bring into their work and that they don't find problematic to discuss at all in combining that with their art. But many writers seem to me to be resistant to the idea of consciously deliberating about language or even learning a whole lot about how it's put together, how its mechanics work. I have never understood that because it seems to me that you just gain extra tools in having that awareness. Just like we have been talking about how labeling something with a word, labeling an apple with the word, draws your attention to certain aspects of it. I think having these scientific concepts and tools and vocabulary directs my attention to aspects of language that maybe it would be harder to see if I didn't have those tools. So this is a great frustration to me that we don't teach literature, we don't teach how to write from the perspective of an understanding of what is it that readers do when they read the text? What kinds of impact is that having on the human mind? 
I'm really hoping that that will change, but I'm not seeing evidence of it changing very quickly because it's 30 years ago now that I first took a linguistics course, having no idea what I was stepping into. I think it's still the case that many students who take a linguistics course now don't really understand what they're stepping into until they're there. That's a great shame to me. 

What makes bad writing?

What makes bad writing? Oh, there are so many ways you can write badly, so, so many ways! Oh, I think ultimately bad writing is when you have miscalculated what your reader is doing, whether you're thinking about that consciously or not. 
Kevin: What do you mean what the readers are doing? 
You're miscalculating how the reader is unpacking meaning from what you have given them in the form of language. If language is just a code, a shorthand to approximate meaning, you know what meaning you have in your mind and you're using language as the tool, the channel to try to get that across, but that relies on a theory that you have implicitly, most of the time, about how your reader is unpacking that and what resonances psychologically are going off in the mind of your reader as they do that. 
One failing you can make as a writer is to not give enough information. The reader gets lost, confused. You have under-specified too much. Another way you can fail your reader is to give too much information. You have underestimated their ability to recover meaning in ways that now irritate them or lead them to look for false meaning. 

Fiction allows us to step out of the constraints of reality to project ourselves into possibilities that don't exist and might exist.

If I introduce you to the only mother I have ever known as my biological mother, I have given you too much information because now you're searching around for some reason for that, for that redundancy, and you're coming to a false conclusion, one that doesn't actually capture the relationship to my mother, who happens to be biological and in fact and spiritual and in all the ways that a mother can be. Those are just two ways a writer can fail. 
They can also fail in, I think, in not understanding or ... Writing can fail to one reader and succeed to another, because the set of assumptions a writer has made about what kind of a frame of mind they're bringing to the reading is correct in the case of one reader but false in the case of another reader. For example, any English teacher I think will tell you that when they teach literature to high schoolers there's a huge range of ability to deal with under-specified meanings or ambiguous meanings or to make inferences on the basis of text. That's something that I think teachers are now more attuned to, to steering those students who don't naturally engage in inference-making to try to encourage them to do that because so much of literature involves making inferences. 
Again, I think as a writer you have an implicit idea of the kind of reader you're writing for and if that misfires for a particular reader or for all readers, that can be experienced as a failing of the writing. 

What good is fiction?

Fiction is assimilation of the world, to a large extent I think. Fiction allows us to step out of the constraints of actual reality to project ourselves into possibilities that don't exist, might exist, may not yet exist, might have existed in the past. It creates an incredible space to explore hypotheses and that makes it I think very, very powerful. It allows us to generate, to create in language, worlds that aren't in front of us. By virtue of having developed these tools to capture worlds that are in front of us we can now take those same tools to create new worlds and to explore the ramifications of those worlds, the what-ifs. 
I think that, in a sense, is really analogous to what we're doing in scientific exploration. When we develop theories about reality, we're making predictions based on those theories that may or may not map onto actual reality but it allows us a way of generating possibilities, of generating what-ifs and if we change this aspect of reality what would be the outcome? That's really a way of describing an experiment. I think fiction is very analogous to that. 

Why is it significant that literature evolved toward inner voices?

I think it's deeply significant because there's a lot of discussion nowadays both in the scientific literature and also in the popular press about the importance of social intelligence, the ability to interact with other people in ways that are successful; the ability to read communication that's implicit; to be flexible in the way that you can communicate with a variety of different people who might bring a variety of different assumptions to play. All of these are correlated with material success, success in relationships—generally, the ability to function in the world. 
What's interesting I think is there is reason to believe that if you look at people who have a high level of social intelligence, that it doesn't simply come from genetic luck. Certainly, there are some reasons to believe that people might be genetically predisposed to having a higher or lower level of social intelligence, but it appears that a significant amount of it is conveyed and culturally transmitted. It turns out that there is a very slow process of learning, for instance, how complex emotions work. While even very small children might be able to grasp very simple emotions like happiness, sadness, anger, fear, I have all kinds of complex emotions. Like, think of an emotion like embarrassment. Well, where does embarrassment come from? Embarrassment comes from a theory that you have about how other people are reacting to you and how other people are perceiving you and your own response to that theory that you have. These kinds of complexities of interactions and emotions are something that seem to be quite slowly acquired and that some people hone to a fine degree, other people to a much lesser degree. 

Do you think fiction like Virginia Woolf's has shaped culture?

Well, I do. I think it has pushed culture to be more attentive to the contents, to the intricate, complex contents of each other's minds, to maybe be more speculative about what other people are thinking and feeling because one of the things that fiction does well, especially modern fiction, is it creates these wildly implausible scenarios and disconnections between events in the external world and the way that people respond to them. 
I'm thinking, for example, of a recent film I saw, Elle, a French movie, where this woman who is the victim of a brutal rape has a really unusual response to it. She begins to seduce her attacker. This is a very unexpected response. If you look at literature dating back many hundreds of years, you wouldn't see these stories of great disconnections between external events, pushing people into these bizarre responses. That's something that's really emblematic of modern fiction and film. What does that do? It immediately makes you think, "Well, why is she doing this? What a strange response? What could be driving that?" and to start generating theories about that. 
I think to the extent that fiction has moved into this domain of exploring complex, ambiguous, unexpected responses to events, it seems to really prompt that kind of analysis. I suspect that it's a very plausible hypothesis that there is a feedback loop that's going on that the more complicated our societies are, the more dependent they are on these kinds of nuanced interactions and our understanding of them, the more we see that reflected in literature and in turn, the more we read that literature—or to some extent see those films—the more we become exercised or adept at thinking about why people behave the way that they do, what kinds of motives might they be driven by. 

What would you be if you weren't a scientist?

Well, that's easy. I'd be a writer, which is kind of what I am as well as a scientist. I think I'm at a point now where I'm seeing the scientific part of my life really corresponding to the writerly part of my life. It's just such a natural set of things to put together because I think the aesthetic love and appreciation of language just melds so beautifully with the scientific aspect. I guess I'm in the process of turning myself into more of a writer and hopefully not less of a scientist, although certainly my time is less and less directly immersed in the day-to-day details of designing experiments and collecting data and that sort of thing. 












Biology | Insects
If Bugs Are Sentient, Should We Eat Them?
The ethics of eating insects.
By Barbara J. King
♦ 

Oyamel Cocina Mexicana, in the Penn Quarter neighborhood of Washington, D.C., is a restaurant specializing in insects. Stepping inside on a cool June evening in 2014, my friend Stephen Wood and I were immersed in the colors and smells of Oaxaca, Mexico. Oyamel is the name of the fir tree native to central Mexico where monarch butterflies rest upon migrating from the United States and Canada, and the décor had a lepidopteran theme: The glass door at the entrance was studded with transparent red, yellow, and pink butterflies, and butterfly mobiles hung from the ceiling. 
But it wasn't butterflies that Stephen and I had come to sample. Our quest focused on chapulines, soft tacos stuffed with grasshoppers. Taking our order, the waitress noted our luck: The grasshoppers sometimes get held up coming through customs from Mexico, but that night they were readily available. Stephen and I ordered a number of small, tapas-like dishes, and when the chapulines arrived, I saw insect body parts right away. A delicate grasshopper leg tumbled onto the table when I raised the taco to my mouth. 
Oyamel is not alone. Fried wild-caught dragonflies and spider rolls featuring rose-haired tarantulas, katydid-and-grilled-cheese sandwiches and tacos stuffed with grasshoppers: The variety of foods laced with insects and spiders available in the United States and Europe today—when you go looking for them—is considerable. The venues in which they may be found are equally varied, ranging from upscale restaurants to street-side food carts and science-museum bug festivals. Entomophagy is on the rise and generating excitement. 



a crickety crunch: Many cultures around the world have long made a practice of eating insects. Now, some restaurants in the Western world are adding bugs to the menu.
©fitopardo.com


Of course, millions of people around the world have long sought out insects and regularly, intentionally consumed them. They do not pluck bugs from under the bed or the dusty attic, of course, but forage for sources of fresh protein and other nutrients in the wild or purchase prepared insects or insect flour at traditional markets. In fact, humans eat over 1,600 species of insects. "The Western abhorrence of eating insects is unusual on a global scale," note naturalist David Raubenheimer and anthropologist Jessica M. Rothman. Westerners may clamor for honey without fully recognizing that when ingesting it they are consuming regurgitated bee products, but people in many countries consciously embrace a wide variety of bugs as food.
As I dined at Oyamel, I pondered some questions not often addressed by fans of entomophagy: What do we know about insect intelligence, personality, and sentience? 

Fruit flies make decisions—and they take longer when the information presented is difficult to evaluate.

The first step in taking these questions seriously is seeing these tiny animals. Growing up in the New Jersey suburbs, I loved whiling away an hour watching the scurrying yet remarkably well- organized activity visible within an ant farm, peering into anthills in the yard to trace the stream of red ants as they flowed in and out, and watching fireflies on a summer's night as they blinked messages in code through the humid air. What I don't remember is thinking of any of these small creatures as individuals; they existed for me in an abstracted sort of aggregate, in a way that my pet cat Queen and dog Shadow, or even the elephants and monkeys I met on family trips to the Bronx Zoo, never did. 
Insects aren't primates like chimpanzees (or us), and they aren't, usually, our pets. What might happen, though, if we tap into our natural curiosity about insects and spiders and ask how they live on their own terms? We do these tiny animals a disservice if we fail to ask questions that only relatively recently in recorded history have humans begun to ask about chimpanzees (or cats and dogs). Do insects learn? How do they interact with their world in intelligent ways? Do they experience the world via distinct personalities? 



Wasps may strike us as buzzing, sometimes stinging, annoyances when we spend time outdoors. But there's another way to look at them: as animals with busy brains. Paper wasps, with a brain less than 0.01 percent the size of our own, recognize individuals who are important to them. Neurobiologist Elizabeth Tibbetts discovered this fact when she altered wasps' facial features by applying modeling paint to them. Nestmates of these suddenly different-looking wasps responded in an atypically aggressive way, while their behavior toward control wasps, who were daubed with paint but whose facial features remained unaltered, didn't change at all. The specificity of the hostile response indicated that the wasps recognized faces, using that recognition to determine who belonged in their community. Nestmates with painted faces were suddenly seen as strangers, and the reactions were not friendly. 
Queens of the species of wasp used by Tibbetts in this experiment (Polistes fuscatus) work together cooperatively within shared nests, but they also experience female-female competition. Facial recognition is adaptive in this context because queens need to distinguish potential rivals from potential allies. Tibbetts went on to train these wasps to distinguish pairs of images of various sorts. "Most strikingly," she and her co-author Adrian Dyer write, "simply removing the antennae from a wasp face image or rearranging the face components dramatically reduced their impressive face-learning capacity." This fact suggests to Tibbetts and Dyer that the wasps process faces holistically in specialized parts of the brain, as we humans do. 
Tibbetts then expanded her study to include a second species of wasp (Polistes metricus) in which solitary queens— instead of clusters—establish nests. In this case, paint-altered nestmates elicited no immediate facial-recognition response. But here's the fascinating part: In the training phase, these wasps did learn to discriminate faces. Presumably, this ability had not been directly selected for in this species over evolutionary time. The mental capacity is there but doesn't emerge under natural conditions. Reviewing studies of wasps and bees in general, Tibbetts and Dyer conclude that "there is ever so much more going on their teensy brains than we could have imagined possible." 
Does that conclusion apply to other insects? Yes, if we're talking about learning. In entomology, learning is defined as the ability to acquire, and represent in one's brain, new information. Historically, the working assumptions in entomology were all about instinct. The reigning equation "simple nervous systems = behaviors driven by hard-wired instinct" was straightforward enough—and also spectacularly wrong. 

When I ate those grasshoppers, I swallowed animals who had experienced the world in some individualistic ways.

One spring morning when I was writing this essay, my Twitter stream lit up with the news that fruit flies make decisions, and what's more, they take longer to do so when the information presented is difficult to evaluate. In an ingenious experiment, fruit fly subjects were first trained to avoid a certain strong smell, then offered a choice between two samples of that smell whose intensities varied by degrees. The insects took longer to make their choice when the difference in smell was subtle (or minimal) than when it was pronounced (or maximal). Neuroscientist Shamik DasGupta and his team concluded that the experimental outcome "bears the behavioral signature of evidence accumulation." In other words, these insects wait until they have gathered enough information to make a reasonable choice when presented with options that complicate decision-making. This weighing of variables according to context is linked in the fruit flies to one specific gene (FoxP) and about 0.1 percent of the flies' total neuron count—right around 200 neurons. 
Far more famous an example of insect learning is the honeybees' waggle dance. In this case, the acquiring of new information happens socially. Performing in the dark hive, the dancers, experienced forager bees, clue in younger, naïve bees about how far to fly, and in what direction, to find suitable flowers. Thanks to scientific experiments, we know that the dances do not operate like the GPS devices that send us, via detailed driving instructions, to a pinpoint location. Instead, they convey information that directs the observer bees to the right general region. There, the flowers themselves provide sight and smell cues; the bees zero in on these beacons and begin to forage. 
Decision-making fruit flies and information-sharing bees are joined by a host of other examples: Learning, both individual and social, is a robust phenomenon in the insect world. At the end of a 2008 review paper, Reuven Dukas concludes, "Learning is probably a universal property of insects, which rely on learning for all major life functions." No mindless drones, insects are intelligent in the sense that they evaluate information coming into their senses and their brains from their physical and social environment, and in some striking cases, they think about how to act on the information they have learned. 



In 2012, I was startled to come upon this provocative sentence at the top of a science-news post on the BBC Nature website: "The experiences of youth can change the adult personalities of crickets, a new study has found." The very fact that biologist Nicholas DiRienzo and his colleagues had hypothesized such a connection tells us something important—that established animal-behavior researchers by now expect indicators of personality to be found in some insect species. 
DiRienzo explains in a technical article published in Animal Behaviour that degree of boldness—an organism's willingness to expose itself to risk—is a trait expressed consistently in individual crickets at different ages and in different situations. Boldness tends to co-occur with aggression. "We consider aggressiveness to be a personality trait in this species," the researchers note, "particularly since aggressiveness and boldness are correlated and thus form a behavioral syndrome." 
The scientists experimentally manipulated the sounds experienced by young crickets (Gryllus integer, commonly found in the American West). They started with males too young to have yet developed an ear, called a tympanum and located in crickets on the front legs. As they were reared, the crickets were separated into two groups: Those in one group had a chorus of male calls played to them, mimicking what they would have heard in the wild; those in the other group experienced only silence. 

What happens when we view insects through the lens of "animals we eat," as we do for chickens or pigs?

The males reared without hearing the cricket chorus, referred to as "acoustic sexual signals" because the calls are uttered during male-male competition for females, were more aggressive and more likely to become dominant. I enjoyed reading the details of this experiment, imagining the researchers at work avidly watching cricket male-male grappling matches, the events through which they assessed aggression levels. But I couldn't work out on my own why males reared in silence should be better and more dominant grapplers. 
DiRienzio and his coauthors, it turns out, think that crickets use the sounds they hear—or don't hear—to figure out population density. The crickets who hear nothing assume that they will face little competition from other males in their forays to find females, and act accordingly—asserting greater dominance than they would if they had discerned evidence of greater competition around them. In other words, signals from the surrounding environment alter cricket personality. 
Now, measuring levels of boldness and aggressiveness in crickets admittedly affords a limited perspective on animal personality. Hanging out with crickets, we would not likely feel that we were in the presence of highly distinct individuals the way we would with chickens or chimpanzees. Some animals vary one from the other along more complex dimensions, not just bold/less bold or aggressive/less aggressive, but gregarious/socially shy, emotionally volatile/phlegmatic, spiteful/easygoing, and so on. 
Insects are not cookie-cutter copies of each other when it comes to their ways of being in the world. The chorusing-cricket experiment shows that personality is not merely a matter of inborn genetics, because the rearing environment plays a role. (For some scientists, animal personality is affected in part by the environment, whereas animal temperament stems from genetics.) In short, when I ate those grasshoppers, I swallowed animals who had experienced the world in some individualistic ways. 



"Tarantulas tend to taste somewhat like smoky lobster," reports Daniella Martin. While I am fascinated by insects' and arachnids' biology and evolution, and try to minimize harming these animals (with some exceptions, including mosquitoes), I need to repress shivers when I get close to them. My reaction is, once again, culture-bound. No documented death of a person has ever come about because of a tarantula bite. Yes, these are large animals for arachnids (the biggest known boasts a leg span of 12 inches and a weight of 5 ounces), and their hairy appearance can be startling. Negative reactions to tarantulas based on their size and physical traits may be exacerbated when there's no great fondness in the surrounding culture for these creatures. "Thanks in part to Jiminy Cricket," Martin notes, "[crickets] have pretty good PR in Western society." Tarantulas have anything but. 
Yet the tarantula's hairiness should lead to fascination rather than fear, because it's a lovely example of evolution at work: Tarantulas don't build webs but like other spiders they do sense their world largely through vibrations; the hairs help detect those and thus help the animal capture prey. I learned these and other cool tarantula facts from an online interview with Nicole Atteberry, curator of ectotherms at Zoo Miami. Atteberry went on to distinguish between shy and aggressive tarantulas, evoking tarantula personality. 



sentient spiders?: Some research suggests that spiders, like this Greenbottle Blue Tarantula, have distinct personalities.
davemhuntphotography/Shutterstock


The key here is to find a reasonable balance in how we think about insects' individuality. Samuel Marshall, an arachnologist who has studied wild tarantulas in French Guiana (considered by some the tarantula capital of the world) and has clocked countless hours with tarantulas in the lab, cautions that because of their rudimentary nervous systems, we shouldn't go too far down the road of thinking in cognitive or emotional terms about tarantulas. He doesn't believe, for example, that tarantulas can become anxious or depressed in the way many vertebrate creatures may. Talking with Discover Magazine in 2004, however, he embraced the word "personality" as applying, for example, to how different tarantulas from a single population of the same species respond to handling. These variable tendencies form part of a suite of potentially fairly complex behaviors. Two of Marshall's students, Melissa Varrecchia and Barbara Vasquez, discovered that Indian ornamental tarantulas prefer to associate with their siblings over other possible companions. "Long-lived, giant spiders," Marshall said at the time, "have a lot more going on than we have any idea of." 
In exploring the science of spider personality, I contacted Marshall, who, in the wonderful way of science networking, sent me on to Susan Riechert, a spider biologist at the University of Tennessee, Knoxville. "As spider behavior is highly repeatable," Riechert told me, "it has a very strong heritable component and thus I always refer to spider behavioral tendencies as temperament." Her comment conveyed that variability within a species' repertoire doesn't invariably stem from learned complexity. For example, Riechert's and Thomas Jones' paper on variation in spider social organization shows (in this specific case) an imperviousness to environmental influence. 

It's challenging to ascertain whether insects feel pleasure and pain.

Riechert and Jones study Anelosimus studiosus, a social spider found in North and South American forests. In this species, there's maternal care, which is atypical for spiders: The mothers guard their young offspring and offer them food via regurgitation. When the mother dies, a dominant daughter often assumes control of the nest and forces out her siblings. Working in the United States, the scientists identified two studiosus sites (each with many spider nests), accessible by water, at 2-degree intervals in latitude, from south Florida's Everglades (26 degrees) to east Tennessee (36 degrees). Solitary nests were, they discovered, the most frequent type at all latitudes. The presence of multi-female nests and a cooperative-female social structure was first found at 30 degrees and increased in frequency as latitude increased. 
With a laboratory phase added to the field research, the results get really interesting. Riechert and Jones collected nests from two cold-water and two warm-water sites, and raised the juveniles from those nests in the lab. Then they transplanted this second generation back into the wild at various latitudes. In this way, some of the juveniles from solitary nests were transplanted to latitudes where multi-female nests were common, and vice versa. All of these juveniles tended, in the scientists' words, "to express the social structure of the parental nest, regardless of the warm- or cold-water environment of the transplant site." When multi-female nests were transplanted, for instance, into the Tennessee habitat that favors single-female nests, new multi-female nests resulted. Even though social structure correlates with latitude, it's not the case that certain environments induce certain social structures. Social behavior in this spider species is resistant to environmental factors and doesn't demonstrate plasticity. Can there even be such a construct as personality under such conditions, given that personality is shaped in part by the environment? It would seem not, but there's definitely evidence for temperament in these spiders. 
The ecologist Jonathan Pruitt found that studiosus individuals can be categorized as more aggressive or more docile. He became a sort of arachnid matchmaker, creating in the lab 90 spider couples; some paired an aggressive male and an aggressive female, some a docile male and a docile female, and others one of each. The next generation's temperaments were consistently (but not completely) predictable: An aggressive pair's offspring were nearly all aggressive, and so on. Pruitt then moved the 90 nests out into the wild, shielding half of them from other invading spiders and allowing the other half to exist amid the inter-spider competitions that naturally develop in the wild. All of the spider colonies in the predator-managed areas did equally well. Among the homogeneous colonies transferred to natural conditions, the docile colonies did better initially, but over the longer term the aggressive ones survived and reproduced more, apparently because they were less frequently consumed as prey. As a write-up in Science Now about Pruitt's research noted, "It turns out nice guys do finish last, at least among arachnids." Pruitt observes, however, that when mixed colonies were introduced to the wild and aggressive studiosus individuals lived side by side with mellow ones, all the spiders did well, perhaps because spiders of different temperaments excel at different survival tasks. 



Insects learn, and may make thoughtful decisions as they learn. Though we often think of them in the aggregate, they may be distinct in their personalities (or their temperaments). It's challenging to ascertain whether they feel pleasure and pain, and to the all-important question of sentience in insects and spiders I can find no ready answer. Given their sophisticated abilities for learning, though, it seems clear that the possibility of sentience should not be ruled out. 
Entomophagy is poised on the verge of a major upswing, of spreading far beyond traditional contexts where it has always been popular. Meanwhile, scientists have begun in the last 15 or 20 years to ask deeper questions about insect intelligence and personality than ever before. It will be a fascinating thing to watch as the two trajectories intersect, perhaps even collide: a growing interest in eating bugs and an equally growing interest in understanding the complexities of their behavior. As enthusiasm for entomophagy builds in the United States and Europe, hard questions about insect consciousness should be kept front and center. 
Generalizing about an enormous taxonomic group of animals is risky. Nonetheless, writing in 2014, Oliver Sacks felt confident enough to offer a summary that resonates with the material reviewed in this chapter: "We often think of insects as tiny automata—robots with everything built-in and programmed. But it is increasingly evident that insects can remember, learn, think, and communicate in quite rich and unexpected ways. Much of this, doubtless, is built-in—but much, too, seems to depend on individual experience." It's precisely that unexpected angle that we need to keep our eye on. While it's far less easy to offer a definitive statement about sentience in insects than about intelligence or personality, insects are surprising us. 




Barbara J. King is a biological anthropologist and Chancellor Professor of Anthropology at The College of William and Mary.

Reprinted with permission from Personalities on the Plate: The Lives and Minds of Animals We Eat. © 2017 by Barbara J. King. Published by the University of Chicago Press. All rights reserved.
Lead image originally from Paris Street, Rainy Day by Gustave Caillebotte












Ideas | Philosophy
You Can't Upload Your "Self" Into Virtual Reality
Thomas Metzinger on the nature of subjective experience.
By Cody Delistraty
♦ 

In his 2003 book, Being No One, Thomas Metzinger contends there is no such thing as a "self." Rather, the self is a kind of transparent information-processing system. "You don't see it," he writes. "But you see with it." 
Metzinger has given a good amount of thought to the nature of our subjective experience—and how best to study it. A fellow at the Johannes Gutenberg University in Mainz, Germany, he directs the Neuroethics Section and the MIND Group, which he founded in 2003 to "cultivate," he says, "a new type of interdisciplinarity." To bridge what he calls the academic variant of the generation gap, the group is formed of philosophers and scientists—young and old—interested in psychology, cognitive science, and neuroscience. 

There's a long history of conscious self-models on this planet.

When I spoke to Metzinger recently, he explained that the self evolved as a biologically useful construct to "match sensory perceptions onto motor behavior in a meaningful way." Earlier this year, Metzinger made waves by publishing an article in Frontiers in Robotics and AI that argued that virtual reality technology—the ability to create illusions of embodiment—will "eventually change not only our general image of humanity, but also our understanding of deeply entrenched notions, such as 'conscious experience,' 'selfhood,' 'authenticity,' or 'realness.'" 
In our conversation, we discussed the origins of the self, intimations of mortality, what those who champion a singularity—an immortal union of brain and computer—are missing, and how virtual reality could perhaps push the self into entirely new modes of experience. 



Thomas Metzinger, in 2012
Thomas Metzinger






What do you mean when you say the self doesn't exist?

We know there is a robust experience of self-consciousness; I don't doubt this. The question is how could something like that emerge in evolution in an information-processing system like the human brain? Can we at all conceive of that being possible? Many philosophers would have said no, that's something irreducibly subjective. In Being No One I tried to show how the sense of self, the robust experience of being someone, could emerge in a natural way in the course of many millions of years of evolution. 
The question was how to arrive at a novel theory of self-consciousness, what a first-person perspective is, that, on the one hand, takes the self really seriously as a target phenomenon and, on the other hand, is empirically grounded. If we open skulls and brains we don't find any entity that could be the self. It seems there are no arguments that there should be a thing like a substance, a self, either in this world or outside of this world. 

What explains the evolution of a self?

I think even simple animals that can't have beliefs or higher cognitive states about themselves have a robust sense of selfhood. There's a long history of conscious self-models on this planet. They have been here long before human beings have arrived on the scene; they are a product of evolution with many biological functions. 
One, for instance, is to control the body—to match sensory perceptions onto motor behavior in a meaningful way. Another much deeper one is the unconscious forms of self-representation; for instance, the immune system that biological organisms have evolved. A million times every day our immune systems says, "this is me" or "this is not me," "kill or don't kill," "cancer cell" or "good tissue." If it would make a mistake in one of these selections, you would already have one malignant tumor cell every day. So we are grounded in very efficient mechanisms of defending the integrity of the organism, the life-process itself. 

What's most unique about the human self as opposed to similar mechanisms in other organisms?

In humans I think something very special has happened. Our self-models have opened the door from biological evolution into cultural evolution. They made living together in large societies possible, and there are, of course, long stories to be told there, because we can use our own self model to understandwhat another human being believes or desires, something we cannot perceive with our sensory organs. But if we have a self model of ourselves, our own internal model, we can use it to simulate mental states. 
Here we come into a very interesting and deeper principle. There is mortality denial. There is the theory of terror management, which says that many cultural achievements are actually attempts to manage the terror that comes along with insight into your own mortality. The way I have put this is that, as biological beings for millions of years we operate under biological imperatives and almost the highest one is you must not die, under no circumstances. 
Now, we human beings, we have a problem that no creature before us had. We have this brand new cognitive self-model and we have this insight that you will die—everybody dies—and that creates an enormous conflict in our self-model. Sometimes I call it a chasm or a rift, a deep existential wound that is given to us by this insight—all my emotional deep structure tells me there is something that must never happen, and my self-model tells me it is going to happen. 

How does a self help deal with the knowledge of death?

Animals self-deceive, and they motivate by self-deceiving. They have optimism bias; just like human beings, different cognitive biases emerge. So we have to efficiently self-deceive. The self becomes a platform for cultural forms of symbolic immortality, the different ways human beings tackle the fear of death. The most primitive and simple, down-to-the-ground way is they become religious, a Catholic Christian, for instance, and say, "It is just not true, I believe in something else," and form a community and socially reinforce self-deception. That gives you comfort; it makes you healthier; it is good at fighting against other groups of disbelievers. But as we see in the long run, it creates horrible military catastrophes, for instance. There are higher levels, like, for instance, trying to write a book that will survive you. 

How can virtual reality change the self? 

That is a very interesting question. What my colleagues and I have done is remote-control a robot directly via brain computer interface. With your motor imagery, you control a robot 4,000 kilometers away, through the Internet, while looking out of the robot's eyes with virtual reality goggles while you're in the brain scanner generating motor imagery. This is, of course, a new form of embodiment. It's not that your sense of self literally jumps over into an avatar or into the robot, but what you do is you create a complex causal loop by which an external tool—a robot, a second body—is being made directly controllable by you, with your own mind. 
Now, of course, this offers itself for mortality denial; there is a religion already in California. It's all these uploading freaks—the Singularity University, the techies. It promises immortality, but it doesn't have all the old-fashioned stuff with God. You find these people who say will we upload our self-models into virtual reality in 30 years. They get big investors by saying this. 

You don't buy it? Why couldn't we upload a self?

The problem—the technical problem—is that a large part of the human self-model is grounded in the body, in gut feelings, in inner organ perceptions, in the vestibular sense, and therefore you cannot really copy the human self-model out of the biological body unless you would at some point really cut it off, so to speak. And then you would maybe have a sense of self jumping into an avatar, but you would not have all that low-level embodiment, the gut feelings, the emotional self-model, the sense of weight and heaviness—all that would be gone. 
Maybe we could create very different forms of selfhood and offer them for augmentation, but for a number of reasons I think that the whole idea of actually "jumping" out of the biological brain and into virtual reality completely has probably insurmountable technical problems. It also has a philosophical problem because the deeper question is, of course, what would jump over into the avatars if there is no self? It's just like discussing reincarnation with Buddhists; what is it that would be reincarnated—your neuroses, your greed, your ugly childhood memories? If there is no substantial self in here right now, what is it that you would copy into an artificial medium? 
Nevertheless, I think we're going to see some dramatic changes in human self-awareness through these new technologies in the coming decades—no doubt about this. We may generate wildly different forms of self-experience. 

What's the biggest challenge of creating those different forms of experiences?

One key word is "embodiment." The bodies we have now and the way our conscious self-model is grounded in these bodies has been optimized for millions and millions of years, with our biological ancestors, with monkeys swinging through branches. What we have in this biological body is so optimized and so robust because it has incrementally evolved over millions of years by trial and error—literally millions of our ancestors dying for us—to have this fluid and context-sensitive body control right now. This dense form of embodiment in virtual reality may be far away. 

But could we one day be embodied in virtual reality?

There is another possibility: We might have another form of embodiment, a technological form. Maybe it is something without gut feelings; maybe it is something without the sense of weight, for instance. Maybe we will have different artificial self-models that we learn to control, and with this process have different forms of self-experience as well. Why should we just replicate what biology has created? Maybe we want to create something more interesting or more cool? 



Cody Delistraty is a writer and historian based in Paris. He writes on books, culture, and interesting humans for places like The New York Times, The New Yorker, The Paris Review, and Aeon. Follow him on Twitter @Delistraty.






This article originally appeared on our blog, Facts So Romantic, in October 2016.











Chapter Five
Equations











Biology | Neuroscience
Is Consciousness Fractal?
Our subconscious love for fractals may tell an evolutionary story.
By Jordana Cepelewicz
♦ 

In one way, Jackson Pollock's mathematics was ahead of its time. 
When the reclusive artist poured paint from cans onto vast canvases laid out across the floor of his barn in the late 1940s and early 1950s, he created splatters of paint that seemed completely random. Some interpretations saw them as a statement about the futility of World War II, others as a commentary on art as experience rather than representation. As Pollock refined his technique over the years, critics became increasingly receptive to his work, launching him into the public eye. "We have a deliberate disorder of hypothetical hidden orders," one critic wrote, "or 'multiple labyrinths.' " 
In 1999, Richard Taylor, a physicist at the University of Oregon, expressed the "hidden orders" of Pollock's work in a very different way. Taylor found that Pollock's patterns were not random after all. They were fractal—and the complexity of those fractals steadily increased as Pollock's technique matured. 
Now, Pollock would not have known what a fractal was, nor would anyone else have at the time. It wasn't until 1975 that the eminent mathematician Benoit Mandelbrot coined the term to describe patterns that are self-similar across different-sized scales, a "middle ground" between order and chaos. The "Nautilus" section of one famous fractal pattern named after Mandelbrot, for example, looks like a spiral, as does a magnified view of one of its sections, and so on. 



fractal splatters: Jackson Pollock's art concealed a fractal dimension that increased as he aged.
Namuth Hans / Getty Images


Fractals are characterized by their "fractal dimension," which is a non-integer number. Where the dimension of a straight line is one, and a rectangle is two, a fractal line drawn on a piece of paper will have a dimension between one and two. The greater the complexity of the line, the closer its dimension is to two. Similarly, a fractal area will have a dimension between a non-fractal surface (with dimension two), and a volume (with dimension three). 
Taylor calculated that the fractal dimensions of Pollock's work hovered close to 1 in the early days of his experimentation, in 1943, which means they were barely fractal at all. But over the next decade, they increased regularly, hitting just over 1.7 in 1952, 20-odd years before Mandelbrot's seminal work. Pollock seemed to be drawn to the patterns on a strictly intuitive basis. "If he spent 10 years refining his fractals," Taylor wondered, "then why?" 



On a warm September evening in 2002, two men attacked a middle-aged furniture salesman named Jason Padgett from behind as he left a karaoke bar, knocking him unconscious. When he came to, he found that the blows he'd sustained had left him with a severe concussion, post-traumatic stress disorder, and, quite literally, a new worldview. All around him, he claimed, familiar scenes now appeared as discrete geometric patterns—as shapes that under re-scaling maintained some semblance of themselves. He saw fractals everywhere: in trees and clouds, in drops of water, in the number pi. "Geometrical blueprints," as he called them, were superimposed over his vision. 
Padgett's astonishing new worldview drew the attention of a team of neuroscientists who scanned his brain to determine which regions were responsible for his newly acquired synesthesia. But in a sense, the transformation may have been revealing an underlying bias toward fractal visual processing in all of us. Taylor believes we have evolved to be efficient interpreters of the fractals that surround us in nature—from lightning and waterfalls to the spiral arms of the Milky Way. Our bodies exploit fractal networks to maximize surface areas and help distribute oxygen, cells, and signals. Blood vessels branch out like root systems; the brain houses folds within folds. According to Taylor, this fractal-rich environment means we don't simply enjoy looking at fractals—we are designed to process them effortlessly, and even have a need to be looking at them. 



NASA


In a 2015 experiment, Taylor and a team of researchers showed test subjects computer-generated fractals on a screen, and then gradually faded the images. At the faintest levels, subjects were best able to detect images whose fractal dimensions were most prevalent in nature. "That's also why you might see a face in a cloud," Taylor says, or a profile in a rock face. "You're so fluent at processing that information that your visual system gets a little trigger-happy and starts to see things in the image that aren't actually there." 
Our fractal fluency begins with the movement of our eyes. When we look at a fractal, our eyes trace a fractal trajectory with a dimension of around 1.4 —no matter what the fractal's dimension is. Nature's most prevalent fractals share this dimension, falling within a range of 1.3 to 1.5. "If we lived on a planet where 1.8 was prevalent, we would have ended up with an eye trajectory of 1.8," Taylor says. "Clearly what's happened is our visual system has evolved." 
And we feel good when we do what we've evolved to do. In another set of studies, Taylor used skin conductance and EEG measurements to measure test subject's reactions to viewing the mid-dimension fractals found most often in nature. He and his colleagues found the images reduced the mind and body's physiological stress by as much as 60 percent, "an enormous amount for a non-pharmacological approach." 

In the brain, as in the heart, "just right" means just fractal enough to walk the line between chaos and order.

That may be why, for instance, we tend to gaze out the window to refresh ourselves when we're tired or having trouble focusing at work. Or why patients recover more quickly when their hospital room has a natural view, and why art that takes nature as its subject helps lower anxiety and stress levels. Take a widely read study published in 1984 by Roger Ulrich, an architecture professor who focuses on healthcare design. He and his team examined the medical records of patients recovering from a type of gallbladder surgery in a hospital located in a Pennsylvania suburb. They found, after controlling for other influences, that patients in rooms with a window overlooking leafy trees recovered on average one day faster, suffered from fewer postsurgical complications, and took less pain medication than patients whose window opened up on a brick wall. 
"The parts of the brain that recognize a beautiful view are very rich in endorphins, a feel-good, anti-pain molecule," says Esther Sternberg, an immunologist and the founding director of the University of Arizona Institute on Place and Wellbeing. More specifically, the nerve cells in the pathway between the visual cortex and parahippocampal place area, where there is a high density of endorphin receptors, exhibit greater levels of activity when people view natural or beautiful scenes. "This helps reduce the stress response. So even if there's no direct evidence yet that looking at fractals helps you heal, you can make the link by adding up all these different kinds of studies." 
The fractal dimension of art is not always obvious. The bare-boned Zen meditation garden of Kyoto's 15th-century Ryoanji Temple, for example, solely of 15 rocks positioned across a rectangular swath of raked gravel. In 2002 a group of researchers decided to investigate the mathematical reason for its appeal to tourists and meditators. Using a technique called medial-axis transformation, they found that the axes of symmetry between the rock clusters formed the fractal contour of a tree. When the rocks were rearranged in computer simulations, that tree-like structure and its meditative effect were lost. "The people who built the temple didn't know about fractals," says Sternberg, who was not involved in the study. "But they understood at some unconscious level that placing the rocks in that way made people feel calm." 



Hidden depths: This bare 14th-century meditation garden has an underlying fractal nature, revealed by a mathematical technique called a medial-axis transformation.
2002 Nature Publishing Group


These responses are corroborated by quantitative EEG measurements of brain activity. Mid-dimension fractals produce a strong alpha-wave response (which corresponds to a wakefully relaxed state) and a strong beta-wave response (indicating a high ability to focus). Taylor is continuing to explore these effects now using fMRI techniques, which show that mid-dimension fractals stimulate the parahippocampal region, ventrolateral temporal cortex, and dorsolateral parietal cortex. The latter two regions are responsible for visual processing and spatial memory, and the first of the three regulates emotions—including emotional reactions to music. 
This isn't the only connection between music and fractals. Studies have found that both pitch fluctuation and rhythm in classical music, from Bach to Beethoven, have a fractal nature. Fractals are also to be found in literature. In February 2016, a group of researchers at the Institute of Nuclear Physics in Poland published a paper showing that the variation of sentence length in a collection of more than 100 texts in different languages obeyed a fractal pattern. Stream-of-consciousness compositions, like James Joyce's Finnegans Wake, were correlated with fractals of fractals, or multifractals. 
If so many of our deepest acts of expression have a fractal nature to them, could it be that our consciousness itself has a fractal character? 



In the mid-1980s, Harvard Medical School cardiologist Ary Goldberger discovered that the fluctuations in our heart rates that occur over the course of seconds correlate statistically to those that occur over minutes and hours. In other words, our heartbeats are fractal—and the more fractal they are, the healthier. 
A fractal system, Goldberger explains, strikes an optimal balance between variability and order. Shifting that balance in one direction or the other—toward greater randomness or greater order—can wreak havoc. "Older or sicker [systems] lose the correlations," Goldberger says, "or get pathologically correlated and lose complex variability." When the heartbeat loses its fractal correlations, for example, it becomes erratic, resulting in arrhythmias such as atrial fibrillation. On the other hand, a steadier, more predictable and periodic pulse rate may point to congestive heart failure or cancer. 
"Being fractal is a way for a system to be in touch with itself, talking to itself, but not locked in," Goldberger says. "You can't exist if you're fixed at one frequency, but if you're all over the place, that also doesn't fly. It's a compromise." 
Something similar is true of the brain. In patients with schizophrenia or depression, the brain's electrical activity (as measured by electroencephalograms) is often too complex; in subjects with epilepsy, it's not complex enough. In the brain, as in the heart, "just right" means just fractal enough to walk the line between chaos and order. 
This intersection between our experience and fractals may run even deeper than Taylor's evolutionary hypothesis. "Any act of creativity is an act of physiology," Goldberger says. "The extent that we are fractalized in our essence makes you think that maybe we would project that onto the world and see it back, recognize it as familiar. So when we look at and create art, and when we decide what to take as high art, are we in fact possibly looking back into ourselves? Is creation in part a re-creation?" "It wouldn't come as a shock to me if consciousness is fractal," Taylor says. "But I have no idea how that will manifest itself." 
One potential manifestation is a much-debated and controversial theory of consciousness proposed by physicist Roger Penrose and anesthesiologist Stuart Hameroff in the mid-1990s. About a decade earlier, Penrose suggested that consciousness results from quantum computation taking place in the brain. Hameroff followed up on this work with the suggestion that the brain's quantum processing happened not at the level of the neuron but in microtubules, tiny structures within neurons responsible for cell division and structural organization. Proteins inside the microtubules contain clouds of delocalized electrons whose quantum behavior can cause vibrations in the microtubules to "interfere, collapse, and resonate across scale, control neuronal firings, [and] generate consciousness." 
So where do fractals come into play? It is known that EEGs, signals correlated with conscious awareness—like Goldberger's heartbeats—exhibit fractal dynamics in the time domain. Hameroff argues that the fractal hierarchy of the brain also exists in the vibrations that resonate across the scales of the spatial domain, from the dynamics of networks of neurons, to the neurons themselves, to the dynamics of their microtubules. "Consciousness can move up and down the fractal hierarchy," writes Hameroff, "like music changing octaves," resonating across levels. 
Giuseppe Vitiello, a physicist at the National Institute for Nuclear Physics in Italy, takes a different approach to the application of quantum physics to brain dynamics (using quantum field theory instead)—but he, too, likens it to an ordering along fractal lines. Like a magnet, he says: disordered on the microscopic level until a trigger causes the magnetic "arrows" to all point in the same direction and result in an organized macroscopic system. Vitiello showed that the advent of this coherent structure—namely, of coherent quantum states—corresponds to the way fractals are represented mathematically. In other words, underlying the brain's fractal processes is quantum coherence. 
Philosopher Kerri Welch looks at consciousness in a more holistic way, through the lens of time and memory. "I think consciousness is a temporal fractal," she says. "We're taking in an infinite amount of data every moment. It's a jump in scale every time we compress that data." According to Welch, perceived time is not a linear progression but a "layering." A fractal. This "fractal-ness" changes as we do: Infants, for instance, live purely in the present, she says, not dividing time, surely not experiencing it the way we do now. That's why, for them, the delta-wave brain state—similar to what's seen in adults in deep sleep—dominates, according to Welch. "And then, as we grow into childhood, we start seeing faster brain waves, theta brain waves ... then alpha waves, and finally beta waves once we reach adolescence." This layered understanding of time, she says, corresponds to how we increasingly divide time into smaller and smaller pieces. 
And with it, "it's also our internal density increasing," Welch adds. "As we get older, we switch, taking in the complexity that surrounds us and recreating it inside. Our internal fractal dimension—that internal density—is increasing." 
In this view, it was quite natural for Pollock's drip paintings to become more and more fractal as he grew older. They may simply have been mirroring the increasingly fractal nature of his own self. As he said himself, "Painting is self-discovery. Every good artist paints what he is." 



Jordana Cepelewicz is a science journalist. 

Lead photo collage: Callista Images / Imgorthand / Getty Images














Matter | Physics
A Theory of Consciousness Can Help Build a Theory of Everything
Neuroscience is weighing in on physics' biggest questions.
By George Musser
♦ 

For an empirical science, physics can be remarkably dismissive of some of our most basic observations. We see objects existing in definite locations, but the wave nature of matter washes that away. We perceive time to flow, but how could it, really? We feel ourselves to be free agents, and that's just quaint. Physicists like nothing better than to expose our view of the universe as parochial. Which is great. But when asked why our impressions are so off, they mumble some excuse and slip out the side door of the party. 
Physicists, in other words, face the same hard problem of consciousness as neuroscientists do: the problem of bridging objective description and subjective experience. To relate fundamental theory to what we actually observe in the world, they must explain what it means "to observe"—to become conscious of. And they tend to be slapdash about it. They divide the world into "system" and "observer," study the former intensely, and take the latter for granted—or, worse, for a fool. 

A purely atomic explanation of behavior may be just that: an explanation of what atoms do. It would say nothing about brains, much less minds.

In their ambitions to create a full naturalistic explanation of the world, physicists have some clues, such as the paradoxes of black holes and the arbitrariness of the Standard Model of particles. These are our era's version of the paradoxes of atoms and light that drove Einstein and others to develop quantum mechanics and relativity theory. The mysteries of the mind seldom come up. And they should. Understanding the mind is difficult and may be downright impossible in our current scientific framework. As philosopher David Chalmers told a Foundational Questions Institute conference last summer, "We won't have a theory of everything without a theory of consciousness." Having cracked open protons and scoured the skies for things that current theories can't explain, physicists are humbled to learn the biggest exception of all may lie in our skulls. 



Solving these deep problems will be a multigenerational project, but we are seeing the early stages of a convergence. It has become a thing for theoretical physicists to weigh in on consciousness and, returning the favor, for neuroscientists to weigh in on physics. Neuroscientists have been developing theories that are comprehensive in scope, built on basic principles, open to experimental testing, and mathematically meaty—in a word, physics-y. 
Foremost among those theories is Integrated Information Theory, developed by neuroscientist Giulio Tononi at the University of Wisconsin-Madison. It models a conscious system, be it brain, bot, or Borg, as a network of neurons or equivalent components. The theory says the system is conscious to the extent that its parts act together in harmony. The underlying premise is that conscious experience is psychologically unified—we feel our selves to be indivisible, and our sensations form a seamless whole—so the brain function that generates it should be unified, too. 
These components are on-off devices wired together and governed by a master clock. When the clock ticks, each device switches on or off depending on the status of the devices to which it is connected. The system could be as simple as two components and a rule for how each affects the other—a light switch and a light bulb, say. At any moment, that system could be in one of four states, and moment by moment, it will flit from one state to another. These transitions could be probabilistic: A state might give rise to one of several new states, each with some probability. 



FROM ATOM TO STRATUM: Much of physics is a navigation among the layers of the world, from the base level (perhaps vibrating strings) to particles, atoms, and molecules, up to the whole universe and beyond. Each level is built on the one below, yet autonomous of it. The higher-level dynamics bring out structure that is obscured in the fundamental description.
Tupungato / Shutterstock


To quantify the cohesion of a system and its claim to being conscious, the theory lays out a procedure to calculate the amount of collective information in the system—information that is smeared out over the whole network rather than localized in any individual piece. Skeptics raise sundry objections, not least that consciousness could never be reduced to a single number, let alone the measure that Tononi proposes. But you don't need to buy the theory as a full description of consciousness in order to find it a useful tool. 
For starters, the theory could help with the puzzles of emergence that arise in physics. One of the most striking features of the world is its hierarchical structure, the way that ginormous numbers of molecules obey simple rules such as the ideal-gas law or the equations of fluid flow, or that a crazed hornet's nest of quarks and gluons looks from the outside like a placid proton. Entire branches of physics, such as statistical mechanics and renormalization theory, are devoted to relating processes on different scales. But as useful as higher-level descriptions can be, physicists conventionally assume they are mere approximations. All the real action of the world occurs at the bottom level. 
For many, though, that is perplexing. If only the microscopic scale is real, why does the world admit of a higher-level description? Why isn't it just some undifferentiated goop of particles, as indeed it once was? And why do higher-level descriptions tend to be independent of lower-level details—doesn't that suggest the higher levels aren't just parasitic? The success of those descriptions would be a miracle if there were no real action at higher levels. And so the debate rages between those who think higher levels are just a repackaging of subatomic physics that we humans happen to find convenient, and those who think they represent something genuinely new. 
Tononi and his colleagues had to wrestle with the very same issues in creating Integrated Information Theory. "Scale is an immediate rejoinder to IIT," says Erik Hoel, Tononi's former student and now a postdoc at Columbia University. "You're saying the information is between interacting elements, but I'm composed of atoms, so shouldn't the information be over those? ... You can't just arbitrarily say, 'I choose neurons.' " 
Any network is a hierarchy of subnetworks, sub-subnetworks, all the way down to the individual components. Which of these nested networks is conscious? Our nervous system stretches from head to toe, and its neurons and other components are complex little creatures in their own right. Yet our conscious experiences arise in specific areas of the cerebral cortex. Those are the places that light up in a brain scan when you're performing tasks that you're aware you're performing. You could lose much of the rest of the brain, and although you might not be happy about it, at least you'd know you weren't happy about it. Three years ago, a 24-year-old woman checked into a Chinese hospital complaining of dizziness and nausea. The doctors did a CAT scan and found a big hole in her brain where the cerebellum should have been. Though deprived of three-quarters of her neurons, she showed every sign of being as conscious as any of us are. 

Physicists face the same hard problem as neuroscientists do: the problem of bridging objective description and subjective experience.

What's so special about the cortex? Why isn't your arm conscious, like an octopus'? Why aren't we vast societies of willful neurons, or of enzymes within neurons, or of atoms and subatomic particles? To explain why consciousness resides where it does, Tononi, Hoel, and colleagues Larissa Albantakis and William Marshall had to come up with a way to analyze hierarchical systems: to look at activity on all scales, from the whole organism down to its smallest building blocks, and predict where the mind should reside. They ascribe consciousness to the scale where the collective information is maximized, on the assumption that the dynamics of this scale will preempt the others. 
Though inspired by the nervous system, Integrated Information Theory is not limited to it. The network could be any of the multilayered systems that physicists study. You can leave aside the question of where consciousness resides and study how the hierarchy works more generally. Hoel has set out to develop a standalone theory of hierarchical causation, which he discussed recently in an entry for this year's Foundational Questions Institute essay contest and a paper last week in the journal Entropy. 
An approach based on Integrated Information Theory allows for the possibility that causation occurs on more than one level. Using a quantitative measure of causation, researchers can calculate how much each level contributes to the functioning of a system, rather than presume an answer at the outset. "If you don't have a good measure or concept of causation," Hoel says, "then how are you sure about your claims that the microscale is necessarily doing all the causal work?" 



As a physicist, your goal is to create the maximally informative description of whatever you are studying, and the original specification of the system isn't always optimal. If it contains latent structure, you'll do better by lumping the building blocks together and tweaking their linkages to create a higher-level description. For example, if the components in the two-component system always change in lockstep, you might as well treat them as a single unit. You gain nothing by tracking them independently and, worse, will fail to capture something important about the system. 
Hoel focuses on three ways that the higher level can improve on the lower level. First, it can hide randomness. Air molecules endlessly reshuffle to no lasting effect; their myriad arrangements are all basically the same—a difference that doesn't make a difference. Second, the higher level can eliminate spoilers. Sometimes the only role of a component is to gum up the other linkages in the system, and it might as well be removed. Its effect can be captured more simply by introducing some randomness into the behavior of the remaining components. Third, the higher level can prune out redundancy. Over time, the system might settle into only one of a few states; the others are irrelevant, and higher levels gain in explanatory value by dropping them. This kind of attractor dynamics is common in physical systems. "The higher scale is not just a compressed description," Hoel says. "Rather, it's that by getting rid of noise, either in the case of increasing determinism or by reducing redundancy, you get a more informative description." Given the new description, you can repeat the process, looking for additional structure and moving to an even higher scale. 



How to look for structure—well, that's something of an art. The connection between levels can be extremely non-obvious. Physicists commonly construct a higher level by taking an average, but Integrated Information Theory encourages them to approach it more like a biologist or software engineer: by chunking components into groups that perform some specific function, like an organ of the body or a computer subroutine. Such functional relations can be lost if all you do is take the average. "Molecules inside the neurons have their particular functions and just averaging over them will not generally increase cause-effect power, but rather make a mess," Albantakis says. 
If the base level is deterministic and free of redundancy, it already provides an optimal description by the standards that Hoel sets out, and there is no emergence. Then physicists' usual intuition—that any higher level is good only as an approximation—holds. Arguably, the very concept of causation breaks down for such a system, because the system is fully reversible; there is no way to say that this causes that, because that could well cause this. Conversely, the base level could be totally uninformative while higher levels show strong regularities. This recalls speculations in fundamental physics, which go back to mathematician Henri Poincaré and physicist John Wheeler, that the root level of nature is completely lawless and all the laws of nature emerge only in the aggregate. 



The higher level does lose something; by definition, it doesn't capture the system in every detail. But the tradeoff is usually worth it. The proof, Hoel says, lies in theorems from communication theory. You can think of the current state of the system as a transmitter, the subsequent state as a receiver, and the relationship between the two as a wire. "Each state is a message that causal structure is sending into the future," he says. Randomness and redundancy are like noise on the line: They corrupt the message. 
When a communications line is noisy, you can often shove data down it faster by using an error-correcting code. For instance, you might transmit in triplicate, dispersing the copies to make them likelier to get through. On the face of it, that cuts the data rate to a third, but if it lets you fix errors, you can come out ahead. In a precise mathematical sense, a higher-level description is equivalent to such a code. It squelches noise that drowns out the essential dynamics of the system. Even if you lose detail, you have a net gain in explanatory traction. "Higher scales offer error correction by acting in a similar manner to codes, which means there is room for the higher scales to do extra work and be more informative," Hoel says. 
For instance, in the two-component, four-state system, suppose that one of the states always gives rise to itself, while the other three cycle among one another at random. Knowing what state the system is in gives you, on average, 0.8 bit of information of what comes next. But suppose you lump those three states together and use them to store one state in triplicate. The system is now smaller, just two states, but fully deterministic. Knowing its present state gives you 1 bit of information about the successor. The extra 0.2 bit reflects structure that the original description concealed. You could say that 80 percent of the causal oomph of the system lies at its base level and 20 percent at the higher level. 



STATES ISSUE: At the microscopic scale, this dead-simple network can exist in one of four states and change state at every tick of the clock. But three of the states randomly cycle among one another, so you might well lump them together to create a higher-level description with only two states. This situation is analogous to a gas. Its molecules reshuffle, and by neglecting their motions you can bring out the overall behavior of the gas.
Hoel, E.P. When the map Is better than the territory. Entropy 19, 188 (2017).


Joseph Halpern, a computer scientist at Cornell who studies causation, thinks Hoel is on to something. "The work has some interesting observations on how looking at things at the macro level can give more information than looking at them at the micro level," he says. But he worries that Hoel's measure of information doesn't distinguish between causation and correlation; as any statistician will tell you, one is not the other. In recent years, computer scientist Judea Pearl of the University of California, Los Angeles, has developed an entire mathematical framework to capture causation. Hoel incorporates some of this work, but Halpern says it would be interesting to apply Pearl's ideas more thoroughly. 
What's especially interesting about the communications analogy is that a similar analogy comes up in quantum-gravity theories for the origins of space. Error-correcting codes provide some lateral thinking on an idea known as the holographic principle. In a simple example, our three-dimensional space might be generated by a 2-D system (the "film" of the hologram). The contents of 3-D space are smeared over the 2-D system much as a bit of data might be stored in triplicate and dispersed. By looking for the patterns in the 2-D system, you can construct a third dimension of space. Put simply, you could start from the fundamental description of a system, look for structure, and derive a notion of scale and distance, without presuming it at the outset. 



SPOILER ALERT: At the microscopic level (left), this network consists of six components and can exist in one of 64 states. One of them, F, is a spoiler. It interacts with each of the others and interferes with their transitions. It is basically a noise source, so can be treated as such to create a higher-level description (right). At the microlevel, knowledge of a state gives you, on average, 3.56 bits about the next state. At the macro, it gives you 4.5 bits. Therefore the macro description is the more informative one.
Hoel, E.P. When the map Is better than the territory. Entropy 19, 188 (2017).


The line of thinking based on Integrated Information Theory might also loop back on the mind-body problems that inspired it. Physics and psychology have been at odds since the days of the ancient Greek Atomists. In a world governed by physical law, there seems to be little room for human agency. If you wrestle with whether to eat a cookie, torn between its yumminess and your latest cholesterol report, psychology speaks of conflicting desires, whereas physics accounts for your decision as a chain of atomic motions and collisions. Hoel argues that the real action occurs at the psychological level. Although you could trace the atomic antecedents of all your behavior, a purely atomic explanation would be just that: an explanation of what atoms do. It would say nothing about brains, much less minds. The atomic description has the brain within it, in some highly scrambled form, but to find it, you must strip away the extraneous activity, and that requires some additional understanding that the atomic description lacks. Atoms may flow in and out of your brain cells, their causal relations forming and breaking, yet the mind endures. Its causal relations drive the system. 



Emergence is not the only knotty physics subject that Integrated Information Theory might help to untangle. Another is quantum measurement. Quantum theory says an object can exist in a superposition of possibilities. A particle can be both here and there at the same time. Yet we only ever see particles either here or there. If you didn't know better, you might think the theory had been falsified. What turns the word "and" into "or"? The textbook explanation, or Copenhagen Interpretation, is that the superposition "collapses" when we go to observe the particle. The interpretation draws a line—the so-called Heisenberg cut—between systems that obey quantum laws and observers that follow classical physics. The latter are immune to superposition. In gazing upon a particle that is both here and there, an observer forces it to choose between here or there. 
The greatest mystery of quantum mechanics is why anyone ever took Copenhagen seriously. Its proponents never explained what exactly an observation is or how the act of making one would cause a particle to choose among the multiple options open to it. These failings led other physicists and philosophers to seek alternative interpretations that do away with collapse, such as the quantum multiverse. But for the sake of leaving no stone unturned, suppose that Copenhagen is basically right and just needs to be fixed up. Over the years, people have tried to be more explicit about the Heisenberg cut. 
Perhaps size defines the cut. Systems that are big enough, contain enough particles, or have enough gravitational energy may cease to be quantum. Because a piece of measuring apparatus satisfies all these criteria, it will collapse any superposition you point it at. How that would happen is still rather mysterious, but at least the idea is specific enough to be tested. Experimenters have been looking for such a threshold and found none so far, but have not completely ruled it out. 

Emergence is not the only knotty physics subject that Integrated Information Theory might help to untangle.

But a more direct reading of Copenhagen is that consciousness is the deciding factor, an idea taken up by physicists Fritz London and Edmond Bauer in the 1930s and Eugene Wigner in the '60s. Because conscious experience, by its very nature, is internally coherent—you always perceive yourself to be in some definite state—the mind doesn't seem capable of entering into a superposition. It will therefore collapse whatever it apprehends. This conjecture didn't get very far because no one knew how to quantify the mind, but Integrated Information Theory now provides a way. In 2015 Kobi Kremnizer, a mathematician at Oxford, and André Ranchin, who was a grad student at Imperial College London and has since left academia, explored this possibility in a short paper. Chalmers, at New York University, and Kelvin McQueen, a philosopher of physics at Chapman University, have also taken it up. 
In turning to consciousness theory to understand the quantum, these scholars invert the approach taken by physicists such as Roger Penrose who look to quantum theory to understand consciousness. Their proposition is that a brain or brain-like network can enter into a superposition—a state of doublethink in which you perceive the same particle both here and there—but only temporarily. The more interconnected the network is, the faster it will collapse. Experimenters could test this idea by adapting their existing searches for a size threshold. For instance, they might compare two objects of the same size and mass but different degrees of internal interconnectedness—say, a dust mote and a bacterium. If information integration is the key factor, the former can be put into a superposition, but the latter will resist. "Ideally, it would be nice to experiment on nanocomputers which we could program with very large amounts" of integrated information, McQueen says. A bacterium or its cyborg counterpart may not have much of a mind, but it may have enough to qualify as an observer that stands outside the quantum realm. 



Integrated Information Theory might also fix another nagging problem with the concept of collapse. It's one thing to tell you a superposition will collapse, quite another to say what it collapses to. What is the menu of options that a particle will choose from? Is it "here" and "there," "slow" and "fast," "mostly here but a little bit there" and "mostly there but a little bit here," or what? Quantum theory doesn't say. It treats all possible categories on an equal basis. 
The Copenhagen Interpretation holds that the menu is set by your choice of measuring apparatus. If you measure position, the particle will collapse to some position: here or there. If you measure momentum, it will collapse to some momentum: slow or fast. If you measure some quantity too weird to have a name, the particle will oblige. Collapse theories seek to explain how this might work. All measuring instruments are made of the same types of particles, differing only in how they are assembled. So, at the particle level, the collapse process must be the same for all—the menu is fixed at that level. The theories usually assume this fundamental menu is one of positions. Measurements of momentum and other quantities ultimately translate into a measurement of a position, such as where a needle points on a dial. There is no deep reason that position plays this privileged role. The theories assume it just to ensure that they reproduce our observations of a world that consists of spatially localized objects. 

The greatest mystery of quantum mechanics is why anyone ever took Copenhagen seriously.

But perhaps some process actively sets the menu. For instance, if collapse is driven by the gravitational field, then it will depend on the positions of masses, which would tend to single out position as the relevant variable. Likewise, if collapse is driven by consciousness, the nature of the mind might dictate the menu. Cosmologist Max Tegmark of the Massachusetts Institute of Technology has speculated that the categories might be determined by the structure of thought. The world may be built of separate but interacting parts because our minds, according to Integrated Information Theory, are made in the same way. 
Collapse is not just a major unsolved problem in its own right, but a possible window into a level of reality underlying quantum theory. Physicists have suggested that a size threshold could arise from fluctuations of the primitive ingredients out of which space and time emerge. Likewise, if information integration is the culprit, that would presumably reveal something deep. It might connect collapse to the same missing principles that scientists need to understand consciousness. "The immediate goal is self-consistent description," McQueen says. "But the process of reaching such a goal can often lead unexpectedly to new kinds of explanation." 
The physicists and philosophers I asked to comment on collapse driven by information integration are broadly sympathetic, if only because the other options for explaining (or explaining away) collapse have their own failings. But they worry that Integrated Information Theory is poorly suited to the task. Angelo Bassi, a physicist at the University of Trieste who studies the foundations of quantum mechanics, says that information integration is too abstract a concept. Quantum mechanics deals in the gritty details of where particles are and how fast they're moving. Relating the two is harder than you might think. Bassi says that Ranchin and Kremnizer use a formula that predicts absurdities such as the instantaneous propagation of signals. "I find it feasible to link the collapse ... to consciousness, but in order to do it in a convincing way, I think one needs a definition of consciousness which boils down to configurations of particles in the brain," he says. In that case, the collapse would be triggered not by consciousness or information integration per se, but by more primitive dynamics that integrated systems are somehow more sensitive to. 
Which brings us back to emergence and the biggest emergence problem of all: how the quantity of particles makes the quality of mind. Integrated Information Theory may not solve it—the scientific study of consciousness is young, and it would be surprising if neuroscientists had hit on the right answer so soon. Consciousness is such a deep and pervasive problem that it makes an odd couple of neuroscience and physics. Even if the answer does not lie in the interconnectedness of networks, it surely demands the interconnectedness of disciplines. 



George Musser is a writer on physics and cosmology and the author of Spooky Action at a Distance and The Complete Idiot's Guide to String Theory. He is a contributing editor at Nautilus, and was previously a senior editor at Scientific American for 14 years. He has won the American Institute of Physics Science Writing Award, among others.












Matter | Physics
Roger Penrose On Why Consciousness Does Not Compute
The emperor of physics defends his controversial theory of mind.
By Steve Paulson
♦ 

Once you start poking around in the muck of consciousness studies, you will soon encounter the specter of Sir Roger Penrose, the renowned Oxford physicist with an audacious—and quite possibly crackpot—theory about the quantum origins of consciousness. He believes we must go beyond neuroscience and into the mysterious world of quantum mechanics to explain our rich mental life. No one quite knows what to make of this theory, developed with the American anesthesiologist Stuart Hameroff, but conventional wisdom goes something like this: Their theory is almost certainly wrong, but since Penrose is so brilliant ("One of the very few people I've met in my life who, without reservation, I call a genius," physicist Lee Smolin has said), we'd be foolish to dismiss their theory out of hand. 
Penrose, 85, is a mathematical physicist who made his name decades ago with groundbreaking work in general relativity and then, working with Stephen Hawking, helped conceptualize black holes and gravitational singularities, a point of infinite density out of which the universe may have formed. He also invented "twistor theory," a new way to connect quantum mechanics with the structure of spacetime. His discovery of certain geometric forms known as "Penrose tiles"—an ingenious design of non-repeating patterns—led to new directions of study in mathematics and crystallography. 
The breadth of Penrose's interests is extraordinary, which is evident in his recent book Fashion, Faith and Fantasy in the New Physics of the Universe—a dense 500-page tome that challenges some of the trendiest but still unproven theories in physics, from the multiple dimensions of string theory to cosmic inflation in the first moment of the Big Bang. He considers these theories to be fanciful and implausible. 



EMERGENT BEAUTY: Roger Penrose has always been in search of deep structures of the universe, reflected in the tiling he created, where basic shapes—in this case the rhombus—give rise to extraordinary patterns.


Penrose doesn't seem to mind being branded a maverick, though he disputes the label in regard to his work in physics. But his theory of consciousness pushes the edges of what's considered plausible science and has left critics wondering why he embraces a theory based on so little evidence. 
Most scientists regard quantum mechanics as irrelevant to our understanding of how the brain works. Still, it's not hard to see why Penrose's theory has gained attention. Artificial intelligence experts have been predicting some sort of computer brain for decades, with little to show so far. And for all the recent advances in neurobiology, we seem no closer to solving the mind-brain problem than we were a century ago. Even if the human brain's neurons, synapses and neurotransmitters could be completely mapped—which would be one of the great triumphs in the history of science—it's not clear that we'd be any closer to explaining how this 3-pound mass of wet tissue generates the immaterial world of our thoughts and feelings. Something seems to be missing in current theories of consciousness. The philosopher David Chalmers has speculated that consciousness may be a fundamental property of nature existing outside the known laws of physics. Others—often branded "mysterians"—claim that subjective experience is simply beyond the capacity of science to explain. 

Conventional wisdom goes something like this: The theory is almost certainly wrong, but Penrose is brilliant.

Penrose's theory promises a deeper level of explanation. He starts with the premise that consciousness is not computational, and it's beyond anything that neuroscience, biology, or physics can now explain. "We need a major revolution in our understanding of the physical world in order to accommodate consciousness," Penrose told me in a recent interview. "The most likely place, if we're not going to go outside physics altogether, is in this big unknown—namely, making sense of quantum mechanics." 
He draws on the basic properties of quantum computing, in which bits (qubits) of information can be in multiple states—for instance, in the "on" or "off" position—at the same time. These quantum states exist simultaneously—the "superposition"—before coalescing into a single, almost instantaneous, calculation. Quantum coherence occurs when a huge number of things—say, a whole system of electrons—act together in one quantum state. 
It was Hameroff's idea that quantum coherence happens in microtubules, protein structures inside the brain's neurons. And what are microtubules, you ask? They are tubular structures inside eukaryotic cells (part of the cytoskeleton) that play a role in determining the cell's shape, as well as its movements, which includes cell division—separation of chromosomes during mitosis. Hameroff suggests that microtubules are the quantum device that Penrose had been looking for in his theory. In neurons, microtubules help control the strength of synaptic connections, and their tube-like shape might protect them from the surrounding noise of the larger neuron. The microtubules' symmetry and lattice structure are of particular interest to Penrose. He believes "this reeks of something quantum mechanical." 
Still, you'd need more than just a continuous flood of random moments of quantum coherence to have any impact on consciousness. The process would need to be structured, or orchestrated, in some way so we can make conscious choices. In the Penrose-Hameroff theory of Orchestrated Objective Reduction, known as Orch-OR, these moments of conscious awareness are orchestrated by the microtubules in our brains, which—they believe—have the capacity to store and process information and memory. 
"Objective Reduction" refers to Penrose's ideas about quantum gravity—how superposition applies to different spacetime geometries—which he regards as a still-undiscovered theory in physics. All of this is an impossibly ambitious theory that draws on Penrose's thinking about the deep structure of the universe, from quantum mechanics to relativity. As Smolin has said, "All Roger's thoughts are connected ... twistor theory, his philosophical thinking, his ideas about quantum mechanics, his ideas about the brain and the mind." 
This is a heady brew, but unconvincing to critics. Most scientists believe the brain is too warm and wet for quantum states to have any influence on neuronal activity because quantum coherence only seems possible in highly protected and frigid environments. The most damning critique has come from Max Tegmark, a professor of physics at the Massachusetts Institute of Technology, who calculated that any quantum effects within microtubules would break down after 100 quadrillionths of a second. "For my thoughts to correspond to a quantum computation, they'd need to finish before decoherence kicked in, so I'd need to be able to think fast enough to have 10,000,000,000,000 thoughts each second," Tegmark writes in his 2014 book Our Mathematical Universe: My Quest for the Ultimate Nature of Reality. "Perhaps Roger Penrose can think that fast, but I sure can't." Even Penrose's old collaborator Stephen Hawking is dubious. "I get uneasy when people, especially theoretical physicists, talk about consciousness," he's written. "His argument seemed to be that consciousness is a mystery and quantum gravity is another mystery so they must be related." Penrose dismisses Hawking's criticism, saying their disagreement is really about the nature of quantum mechanics. 



Last year I saw Penrose in action at a one-day conference on consciousness in Lucerne, Switzerland. It was an intriguing assortment of speakers, including the neuroscientist Christof Koch, Buddhist monk Matthieu Ricard, The Tao of Physics author Fritjof Capra, and even an ayahuasca expert. Then there was Penrose, who played the part of the unworldly Oxford don—slightly rumpled in appearance, with an impish sense of humor. He had set up two overhead projectors on stage, and then darted back and forth between these machines, laying down a series of transparencies filled with his own handwritten notes and drawings of neurons and microtubules, the Leaning Tower of Pisa, a floating astronaut, and—as I recall—the Little Mermaid, all in an effort to explain the Orch-OR theory of consciousness. Modern science may be a high tech game, but this was a dazzling piece of performance art, and the overflowing audience loved it. 
Hameroff was also at the conference, and it turned out their hotel rooms were just down the hall from mine. In my brief interactions with them, I got the sense that Hameroff plays the role of willing accomplice—not only touting the genius of Sir Roger, but also looking after Penrose when it came to travel arrangements and even getting to the conference site. Hameroff can also be the pugnacious bulldog defending their theory (which, in the panel discussion, he did by needling Koch about various details of brain activity). 

Marvin Minsky once told me that the study of consciousness is "what people wasted their time on in the 20th century."

This past March, when I called Penrose in Oxford, he explained that his interest in consciousness goes back to his discovery of Gödel's incompleteness theorem while he was a graduate student at Cambridge. Gödel's theorem, you may recall, shows that certain claims in mathematics are true but cannot be proven. "This, to me, was an absolutely stunning revelation," he said. "It told me that whatever is going on in our understanding is not computational." 
He was also jolted by a series of lectures on quantum mechanics by the great physicist Paul Dirac. Like many others, Penrose struggled with the weirdness of quantum theory. "As Schrödinger clearly pointed out with his poor cat, which was dead and alive at the same time, he made this point deliberately to show why his own equation can't be the whole truth. He was more or less saying, 'That's nonsense.' " To Penrose, the takeaway was that something didn't add up in quantum theory: "Schrödinger was very upset by this, as were Dirac and Einstein. Some of the major figures in quantum mechanics were probably more upset than I was." 
But what, I asked, does any of this have to do with consciousness? "You see, my argument is very roundabout. I think this is why people don't tend to follow me. They'll pick up on it later, or they reject it later, but they don't follow argument." Penrose then launched into his critique of why computers, for all their brute calculating power, lack any understanding of what they're doing. "What I'm saying—and this is my leap of imagination which people boggle at—I'm saying what's going on in the brain must be taking advantage not just of quantum mechanics, but where it goes wrong," he said. "It's where quantum mechanics needs to be superseded." So we need a new science that doesn't yet exist? "That's right. Exactly."



Vanessa Penrose / Caroline Davis2010 / Flickr


After we'd talked for 20 minutes, I pointed out that he still hadn't mentioned biology or the widely held belief that consciousness is an emergent property of the brain. "I know, I know," he chuckled, and then told me why he felt compelled to write his first book on consciousness, The Emperor's New Mind, published in 1989. It was after he heard a BBC interview with Marvin Minsky, a founding father of artificial intelligence, who had famously pronounced that the human brain is "just a computer made of meat." Minsky's claims compelled Penrose to write The Emperor's New Mind, arguing that human thinking will never be emulated by a machine. The book had the feel of an extended thought experiment on the non-algorithmic nature of consciousness and why it can only be understood in relation to Gödel's theorem and quantum physics. 
Minsky, who died last year, represents a striking contrast to Penrose's quest to uncover the roots of consciousness. "I can understand exactly how a computer works, although I'm very fuzzy on how the transistors work," Minsky told me during an interview years ago. Minsky called consciousness a "suitcase word" that lacks the rigor of a scientific concept. "We have to replace it by 'reflection' and 'decisions' and about a dozen other things," he said. "So instead of talking about the mystery of consciousness, let's talk about the 20 or 30 really important mental processes that are involved. And when you're all done, somebody says, 'Well, what about consciousness?' and you say, 'Oh, that's what people wasted their time on in the 20th century.' " 
But the study of consciousness has not gone the way Minsky had hoped. It's now a cottage industry in neuroscience labs and a staple of big-think conferences around the world. Hameroff is one of the driving forces behind this current enthusiasm. For years he and Chalmers have run the biennial "Toward a Science of Consciousness" conference that features dozens of speakers, ranging from hardcore scientists to New Age guru Deepak Chopra and lucid dream expert Stephen LaBerge. Hameroff's connection to Penrose also goes back decades. He first contacted Penrose after reading The Emperor's New Mind, suggesting he might have the missing biological component that would complement Penrose's ideas about the physics of consciousness. 

The science of consciousness feels stuck, and here's a theory—however speculative—that suggests a possible way forward.

"I finished the book without really knowing what I was doing," Penrose recalled. "Stuart wrote me a good old-fashioned letter in which he said, 'It seems that you don't know about microtubules.' " When they met in Oxford, Penrose realized that microtubules had the best chance of anything he'd seen that could mediate large-scale quantum coherence within the brain. And ever since, Penrose and Hameroff have been peddling their theory. Then in 2013, scientists in Japan announced that they had detected vibrations in microtubules, which, according to Penrose and Hameroff, seemed to show that the brain is not too warm and noisy for delicate quantum activity, and launched a new round of debate about the Orch-OR theory. 
In some ways, Penrose and Hameroff are the odd couple of science. Hameroff is upfront about his spiritual views, talking openly about the possibility of the soul existing after death. Penrose is an atheist who calls himself "a very materialistic and physicalist kind of person," and he's bothered by New Agers who've latched onto quantum theories about non-locality and entanglement to prop up their paranormal beliefs. 
I asked what he thought of Hameroff's far-flung ideas about disembodied consciousness. "Well, I have to allow him his freedom," he said. "It does worry me a bit. I mean, he goes a lot further than I would be prepared to." Still, he acknowledges that consciousness is a huge mystery. "I'm not even sure what materialistic means, quite honestly. Quantum mechanics behaves in ways that one thinks are certainly at odds with the view we used to have." 
As we probed the deeper implications of Penrose's theory about consciousness, it wasn't always clear where to draw the line between the scientific and philosophical dimensions of his thinking. Consider, for example, superposition in quantum theory. How could Schrödinger's cat be both dead and alive before we open the box? "An element of proto-consciousness takes place whenever a decision is made in the universe," he said. "I'm not talking about the brain. I'm talking about an object which is put into a superposition of two places. Say it's a speck of dust that you put into two locations at once. Now, in a small fraction of a second, it will become one or the other. Which does it become? Well, that's a choice. Is it a choice made by the universe? Does the speck of dust make this choice? Maybe it's a free choice. I have no idea." 
I wondered if Penrose's theory has any bearing on the long-running philosophical argument between free will and determinism. Many neuroscientists believe decisions are caused by neural processes that aren't ruled by conscious thought, rendering the whole idea of free will obsolete. But the indeterminacy that's intrinsic to quantum theory would suggest that causal connections break down in the conscious brain. Is Penrose making the case for free will? 
"Not quite, though at this stage, it looks like it," he said. "It does look like these choices would be random. But free will, is that random?" Like much of his thinking, there's a "yes, but" here. His claims are provocative, but they're often provisional. And so it is with his ideas about free will. "I've certainly grown up thinking the universe is deterministic. Then I evolved into saying, 'Well, maybe it's deterministic but it's not computable.' But is it something more subtle than that? Is it several layers deeper? If it's something we use for our conscious understanding, it's going to be a lot deeper than even straightforward, non-computable deterministic physics. It's a kind of delicate borderline between completely deterministic behavior and something which is completely free." 
It's hard to know what to make of these pronouncements. Even if you're skeptical of Penrose's argument about consciousness, it's tempting to root for him. The science of consciousness feels stuck, and here's a theory—however speculative—that suggests a possible way forward. The fact that Penrose is asking so much of us—not just to accept quantum coherence in microtubules but also his contention that consciousness can only be explained by still-undiscovered laws of physics—may simply be too far-reaching to ground a new scientific theory. And there's another problem as well. Suppose 20 or 200 years from now the broad outlines of Orch-OR are confirmed. Have we explained consciousness—or just pushed the mind-brain problem into a deeper mystery, the quantum mind-body problem? Can we ever bridge the gap between the physical and immaterial worlds? 
As I wondered why Penrose keeps hammering away at his theory on consciousness after all these years, I asked him if he thinks there's any inherent meaning in the universe. His answer surprised me. "Somehow, our consciousness is the reason the universe is here." So does he think there's intelligent life—or consciousness—somewhere else in the cosmos? "Yes, but it may be extremely rare." But if consciousness is the point of this whole shebang, wouldn't you expect to find some evidence of it beyond Earth? "Well, I'm not so sure our own universe is that favorably disposed toward consciousness," he said. "You could imagine a universe with a lot more consciousness that's peppered all over the place. Why aren't we in one of those rather than this one where it seems to be a rather uncommon activity? 
"So, yes, we want to see the purpose of it. I don't know. Maybe it's attributing the wrong word. Purpose—what does that mean?" He chuckled. 



Steve Paulson is the executive producer of Wisconsin Public Radio's nationally syndicated show To the Best of Our Knowledge. He's the author of Atoms and Eden: Conversations on Religion and Science. You can subscribe to TTBOOK's podcast here.

Lead photo collage: Vanessa Penrose / Caroline Davis2010 / Flickr














Ideas | Psychology
Pre-Conscious Humans May Have Been Like the Borg
Does an alien race from Star Trek tell the story of human consciousness?
By Jacob Lopata
♦ 


Captain Picard: "How do we reason with them, let them know that we are not a threat?"
Guinan: "You don't. At least, I've never known anyone who did."

With this brief, ominous exchange, the heroes of Star Trek: The Next Generation are introduced to one of their most formidable enemies: the Borg, a race of cyborgs whose minds are linked to a collective "hive mind" through sophisticated technology. The collective expands their civilization through a process of mental and physical "assimilation": They find new intelligent beings, like humans, implant them with Borg technology, and integrate them into the hive mind, erasing their previous identities. 



awestruck: With human-type consciousness, an adolescent Borg named Hugh gains human-type wonder, after he is separated from the collective consciousness of the Borg.
CBS


Individual Borg are not conscious in the way humans are, and they have no sense of individuality. The hive mind is a dictator, an unquestioned voice that commands each individual. The Borg nature is split in two, an executive called the collective and a follower called the drone. 
For the humans living in the Star Trek universe, the prospect of assimilation is terrifying. When asked why humans resist assimilation, Chief Engineer Geordi La Forge says, "For somebody like me, losing that sense of individuality is almost worse than dying." 
For many humans living in the real world, the fictional Borg are similarly unsettling. But why? What is it exactly about the Borg that irks us so? Could it be that somewhere in the recesses of our minds we sense something unpleasant about our ourselves when we view the Borg? What if they reflect a different kind of human mentality, one that was actually Borg-like? 

The internal voices that commanded bicameral humans eventually fell silent, and humanity was forever changed.

An intriguing, albeit highly controversial, idea very much like this was actually proposed by Julian Jaynes, an American psychologist who taught at Princeton University. In his 1976 book, The Origin of Consciousness in the Breakdown of the Bicameral Mind, Jaynes theorizes that human consciousness—by which he means the ability and tendency to think about ourselves as individuals—emerged suddenly, and relatively recently in history, around 3,000 years ago. That would mean that anatomically modern humans were alive for hundreds of thousands of years before becoming conscious. 
Jaynes argues that before this recent emergence of consciousness, humanity experienced the world in a manner similar to the Borg. There was not a holistic self with free will, but rather a two-part psyche, or "bicameral mind," in which one part gave "orders" to a second part that acted on those orders. For bicameral humans, "volition came as a voice that was in the nature of a neurological command, in which the command and the action were not separated, in which to hear was to obey." Jaynes says these commands were often perceived as coming from gods, and that they live on today as the internally hallucinated voices heard by some schizophrenics. 



assimilation: The Borg capture Captain Jean-Luc Picard and turn him into Locutus, all but erasing his previous identity.
CBS


In this era, humans did not have an internal self that allowed for introspection or reflection. As such, the Borg are an excellent example of Jaynes' description of the bicameral mind of early humans. The primary difference is that bicameral humans, unlike the Borg, were not technologically linked together in a single collective mind. Without collective thought, bicameral humans would have had trouble solving and managing complex problems. As ancient communities became more literate, urban, and complicated, Jaynes says the bicameral mind "broke down," a change that is reflected in the increasing consciousness of literary characters created soon after that time. After the advent of writing, the internal voices that commanded bicameral humans eventually fell silent, and humanity was forever changed. 
But what about the Borg? Are they destined to remain nothing more than unconscious automatons for all Star Trek eternity? In a case of art imitating life (if in fact Jaynes' theory is correct), it turns out it is possible for the Borg to undergo a transformation similar to the one undergone by bicameral humans. In the episode "I, Borg," a single injured cyborg is captured and disconnected from the collective by the Enterprise crew. With the Borg executive silenced, this drone eventually becomes conscious and develops individuality, a close analog for Jaynes' theory of the breakdown of the bicameral mind in humans. Before this transition, the isolated drone is unable to function, but afterward, it becomes endowed with a key feature of consciousness: the concept of "I." In essence, this Borg—now named Hugh—attains human consciousness. 
So maybe what we really fear is not the behavior of a fictional enemy, but a dark remnant of our historical selves. If Jaynes is correct, the transformation from internally commanded, unconscious beings to thinking, reflecting people would have to be considered the most significant and far-reaching adaptation in the history of our species. It was a change that gave us that which we are most loath to lose: our individuality. 




Jacob Lopata is an entrepreneur, aerospace engineer, and commercial pilot based in Chicago.


This article originally appeared on our blog, Facts So Romantic, in November 2014.

















