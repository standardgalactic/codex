
















We, the Data









We, the Data
Human Rights in the Digital Age
Wendy H. Wong
The MIT Press
Cambridge, Massachusetts
London, England









© 2023 Wendy H. Wong
All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.
The MIT Press would like to thank the anonymous peer reviewers who provided comments on drafts of this book. The generous work of academic experts is essential for establishing the authority and quality of our publications. We acknowledge with gratitude the contributions of these otherwise uncredited readers.
This book was set in Bembo Book MT Pro by New Best-set Typesetters Ltd.
Library of Congress Cataloging-in-Publication Data is available.
ISBN: 978-0-262-04857-6
10 9 8 7 6 5 4 3 2 1

d_r0







This book is for my dear boys, who are in my heart and on my mind, always










Contents

1 Data Are Everywhere
2 Why Human Rights and Data Go Together
3 Data Rights
4 Is Your Face Yours?
5 Do We Need Human Rights When We're Dead?
6 Big Tech and Us
7 Data Literacy, or Why We Need Libraries, Not Twitter
8 We, the Data
Acknowledgments
Notes
Index










1
Data Are Everywhere

What do Amazon.com, basketball legend Shaquille O'Neal, and television reality show Shark Tank have in common?
Ring's video doorbell. Ring, the market leader in video doorbells, sold well over a million devices during the first year of COVID-19.1 In the United States alone, 3.4 million American homes made use of video doorbells before the pandemic drove sales even higher.2 Ring has been called the center of "the largest civilian surveillance network the US has ever seen."3 Before its runaway success and Amazon's purchase of the company in 2018 for over $1 billion, it had nearly failed.4 Founder Jamie Siminoff turned things around with an appearance on Shark Tank. The sharks declined to invest, but sales increased after the show—and Shaq later signed on to pitch the product in a series of successful ads. The rest is history.
Ring's doorbell and others like it, such as Google Nest, Blink (also owned by Amazon), and similar devices made by smaller companies, have become common for good reason: video doorbells are convenient. They make it possible to talk to people at your door when you're indisposed or not even home, and to remotely screen who is at the door. People living in small multiunit buildings without entry systems can have their own doorbell to greet guests before coming downstairs. Before COVID-19, many of us received parcels during the day when we weren't home to receive them, and the theft of packages became a problem—so much so that the Netflix show Easy produced an over-the-top episode about how neighbors resolved to take care of a local delivery snatcher.
The video doorbell, it seemed, was the answer to the new problems created by online commerce and the solution to answering the doorbell when multiple packages arrive throughout the day. But video doorbells are not the only digital devices made for our convenience or increased productivity. There are also smartphones, smart thermostats, smart TVs, smart speakers, smart refrigerators—you name it. These smart devices form the ecosystem of the Internet of Things and have fundamentally changed many aspects of our lives and perhaps even who we are.5
In some ways, the smartness of these devices is misdirected. These devices are smart because they are collecting data about you: to shape a world around you, the collectors claim, to better suit your purpose. These devices are data intensive, reflecting a generally online world of activity that has become dependent on collecting data about us, from us, in order to function. These data are about you: your choices, your activities, your habits, your curiosities. You're the source of the data, and the interconnected devices are merely smart data collectors.6
By now, many of us have a variety of data-intensive devices on hand as part of our daily lives, whether we wear them on our wrists, keep them in our pockets and bags, or hang them on the wall at home. We've come to rely on personal assistants named Alexa or Google or Siri. There's a comfort to having a phone that knows where you are, in case you have an emergency or simply can't find the restaurant where you were meeting someone. Yet the cost of the convenience is the data collected from and about you. Some have encapsulated it in a popular saying that has floated around the internet for at least a decade, which goes something like this: if you're not the customer, you're the product.7 The idea that nothing comes for free and that, indeed, something about you is being sold to others as you browse the internet, engage on social media, and play games, is true to some extent.8 Data collectors are taking and analyzing data about you, whether those collectors are the companies making products, data brokers (who buy and sell data), or other companies tracking consumers. These data are sold to third parties to give you a better product and more services, and, yes, to sell you things. Companies like Meta and Google have built massive financial empires through "surveillance capitalism."9 Their customers are the companies trying to target every one of us at the microlevel.10 The consequences of deconstructing human life into data—the websites we visit, for how long, the friends we have on social media, our gaits, our locations, even our very faces—come not just with financial changes. They shift our very perceptions of humanity (including ourselves) and our social fabric. All of these changes have happened without our thinking too much about human rights: the rights we have as human beings to live our lives to our fullest potential.

*
For too long, we have heard from data collectors about what they think they are doing to help and protect us, about the importance of Big Data, and why we should advance artificial intelligence (AI). Data sources to date have been relatively silent, although data about us are being taken from our actions. We, the Data is about why and how to speak up for data sources by applying what we know from human rights to the way we think about datafication. As a term, datafication refers to the recording, analysis, and archiving of our everyday activities as digital data. It is how Big Data are created. Datafication is fundamentally different from other kinds of technological changes because it changes humanity in a personal way. Where railroads, electrification, and the shipping container all shifted our economies and relationships by massively changing what was possible, datafication does so at the individual and collective human level by recording (nearly) the entirety of our daily activities.
The possibilities of datafication may seem endless and data are everywhere, but they are not inevitable. Even outside techno-cheerleaders' circles, there is a tendency to take the human out of our machines. For example, communications scholar Laura DeNardis's book The Internet in Everything grimly begins, "If humans suddenly vanished from earth, the digital world would still vibrantly hum."11 But the digital world exists because of our humanity, for better or worse. Without us, its utility is gone.
We need to center humans in the conversation about data by reviving human rights at their most basic level. Human rights, as I elaborate here, formally became part of the global human narrative after World War II. We might intuitively think about human rights as lists of rights and freedoms, such as the freedom of religion or the right to a nationality. At their core, however, human rights are about four values: liberty (or autonomy, to emphasize human agency), dignity, equality, and brotherhood (which I call "community" in this book). These four ideas undergird the purpose of human rights, as originally presented by René Cassin in his idea of a human rights "portico."12 Cassin was a key member of the United Nations Committee on Human Rights that wrote the 1948 Universal Declaration of Human Rights (UDHR), the backbone of the international human rights framework.
What can human rights fix about datafication? Human rights are far from a silver bullet. We know from work on human rights that it is one thing to declare them and another to experience them. However imperfect, human rights do offer a coherent logic and inclusiveness that accentuates the commonalities we share with our fellow humans. This book centers human rights and, more specifically, the four foundational values of human rights as a reminder that data come from bearers of these rights, each and all of us. We can't conveniently decouple the person from the data that collectors get from us, for science, for knowledge, for convenience, or otherwise. These data and humans are tethered. As I develop below, if data are sticky to us as humans, humans ought also be sticky to data in the sense that we ought not to treat data about people, taken from people, as mere objects to be sorted, pooled, processed, analyzed. At the same time, we also ought not prevent sharing any data about people. Our social, economic, and political systems depend to a certain extent on the ability to be able to know things about people.13 Finding the balance between the declarations of the collectors and the sources is important, especially since we've let collectors drown out the voices of sources to demand differently.



Figure 1.1
Cassin's Portico. Reproduced with permission from Katherine G. Lev. Originally from Mary Ann Glendon, A World Made New: Eleanor Roosevelt and the Universal Declaration of Human Rights (New York: Random House, 2001), 172.


The political and social choices we make regarding datafication will shape how AI, Big Data, and other technologies affect our lives. Our understanding of datafication will guide our choices. Without a right to data literacy, which I discuss in chapter 7, the conversation around datafication will inevitably fall to those who control and create our datafied world: Big Tech. Big Tech refers to the corporations that generate and analyze massive quantities of data using advanced algorithms that leverage machine learning techniques to make predictions.14 We generally think about a handful of companies guiding Big Tech (Amazon, Alibaba, Apple, Baidu, Google, Meta, Microsoft, Tencent), but many of the companies involved are much smaller and not household names.15
Big Tech has already shaped our political and social world. The language largely being used today describes most of us as "data subjects,"16 and those who harvest, aggregate, and analyze the data, such as Big Tech companies, as "data collectors." Subjects, as political scientists use the term, do not have the agency of citizens and are often restricted or denied participation in lawmaking and political participation.17 A passivity constrains subjects in this view.18 It also reduces us to "the subject" of the data when in fact we are, I argue, at least co-creators of data. We are critical to creating data, even if we're not the ones doing the collecting.
We facilitate the data collection process by actively using the apps and devices that collect these data about us. It doesn't seem tenable or even plausible to opt out, given how pervasively data are collected. Perhaps instead of being data subjects, we could be data citizens. Citizens participate in making and administering rules. Citizenship in our modern understanding of it is an idea used to think about our relationship to states. However, when it comes to technology issues, states have ceded their datafication leadership role to Big Tech. We are not citizens of corporations. Not all of us can afford to be shareholders, but we all can be stakeholders.
This is where—and why—using the language and mindset of human rights matters. As rights bearers, we have an interest, and indeed an entitlement, to act. Each of us, whether we know it or not explicitly, has a stake in datafication because of how pervasive datafication is.19 If we don't act, this trend will continue to distort not just economic relationships,20 but our relationships to one another and to ourselves.21 Crucially, our target for human rights demands has shifted. Instead of a traditional focus almost entirely on public entities like states and what they do, we must seriously heed the calls by journalist Rebecca MacKinnon, political scientist John Ruggie, and many others, to apply human rights to the activities of private organizations like corporations.22 Big Tech's relentless march toward datafication has altered our very humanity.
We must pivot toward human rights because those rights make us all stakeholders in the world. Because of human rights, we have claims against oppressive, violating forces that restrict our bodies and our capabilities. The intensely voluminous and often invisible ways that data are routinely collected can make it seem that resistance is futile. Yet human rights have worked to make demands heard in the past, in situations that seemed just as totalizing.
Data-intensive technologies such as AI and their makers are not neutral, and they are not "for good." Although these technologies are made by people, they are not necessarily made for "all the people" or even "many people" in terms of thinking about potential harms alongside the benefits of advancements.23 Awareness of the changes to our humanity that datafication brings will not be fully realized until it is clear to all of us what is happening. Awareness of what data do will not come about automatically or even through use of these data-intensive technologies. Comedian Bo Burnham's pandemic-era Netflix sensation Inside pokes fun at the many ways the Internet has changed people and how we relate to each other through constant connectivity.24 Inside resonated with the mixed emotions during COVID-19 lockdowns that made us all embrace digital options. These technologies are too integrated, too naturalized, and, let's face it, often just exciting in ways that make us all kids again. Can we really just answer our doorbells through our phones or, better yet, have our doorbells identify who is at the door for us?25
It's our role, as the sources of data, to demand protection from abusers of data. As stakeholders, with an interest in affecting how and for what purposes data about us are collected, embedded in social and political communities, we will make stronger demands. As stakeholders, we need information, and we need a conceptual understanding of how datafication fundamentally changes how human lives are lived. Data literacy will strengthen us individually and collectively, as literacy is only meaningful for individuals in the context of collectivities in which we live. We don't stop at data literacy, however. We've already known for quite some time that digital technologies pose challenges for human rights. We need to go back to the motivation and basis for human rights—autonomy, dignity, equality, community—as inspiration for how to apply our existing array of rights to a datafied reality.

Datafication from an Explicitly Political View
My training is in political science. I've been a scholar of social movements and nongovernmental organizations (NGOs) working in the context of global politics for nearly two decades. I've written two award-winning books and dozens of shorter pieces about how NGOs transform international politics. I've interviewed and interacted with hundreds of present and former activists. All of them have labored for their causes because they felt the call for change from the status quo. They felt the stakes of not asking for change. They wanted to be heard in their protests of why the status quo does not work.
NGOs often channel activist demands. NGOs can't make change if they don't know what they are asking for. Still, not all NGOs or other civil society groups make clear demands in their work, even if their work is very clear to them. When working globally on human rights, the best way to get what you want is to keep saying the same thing, no matter where you are. The same messages can resonate across different cultures and geographies if we communicate with local specifics in mind.26
So what's stopping us from acting? From my perspective as a political scientist, I see the power relationships between data sources and data collectors in both market and nonmarket terms. Sometimes it's hard to get to a mutually beneficial solution not just because it's expensive, but because other kinds of costs that are involved prevent collective action. First, we, the sources, haven't realized our stakes in datafication. Even so, we are disempowered individually from taking on the powerful interests that data collectors have locked into practices and policies. Yet taking collective action requires organization. Beyond monetary costs, collective action imposes costs for participants in terms of time, possible social consequences, and work to coordinate people coming from different backgrounds. We know that in order for collective action to happen, usually the collectivity has to be small enough that people feel invested to act. Since datafication affects each of us, in order to solve this "collective action problem" globally, someone has to be willing to bear the costs of organizing because they see a benefit to doing so. This benefit can be market driven.27 Or it can be normative.28
When I teach international relations classes, my area of greatest interest is on the power of global norms. Although norms are nonbinding in the legal sense, they tell us about social and political bounds: whether behavior is appropriate or whether ideas are acceptable.29 Norms constitute focal points around which discussion can begin and policy can be made. So what makes some norms stickier than others? And who gets to decide what the norms consist of? Sometimes we talk about issue or norm entrepreneurs who advocate ideas that stick at first in a few places and then globally.30 Entrepreneurs set agendas, and they can take on some of the burdens of collective action. This is where others and I have argued that NGOs and social movements can have the greatest effect in global politics. Global norms and law around preventing torture, establishing LGBTQ+ rights, banning land mines and cluster munitions, and restricting trade on small arms would not have happened if activists were not setting the agenda at the global level.31
The road to changing norms is not typically smooth. Entrenched interests on the opposing side abound.32 Yet we should be considering how embracing stakeholdership can create new focal points for our practices and conversations about data. Data stakeholdership can set new agendas for datafication.


Who Are the Stakeholders?
To demand means that you must be heard. To be heard, you must be persistently loud enough to be important. Data collectors have an advantage: they have collected, and will continue to collect, data about people from all of us. They have developed and govern the platforms that we use, and that is where they get their power. By contrast, our individual power is very limited. Similarly, the value of each individual person's data appears negligible compared to the sea of data that collectors end up working with. Their loss of any individual's data is hardly noticeable. The collectors care little about individuals deciding to drop their devices, but to each of us, it is costly to stop using devices and services we find invaluable. We don't currently have a vocabulary to adequately express this dramatic imbalance of power and significance.
This book is for all of us who have been thinking hard about what to do with this imbalance that has been growing with the popularization of the internet and the rise of the digital economy. In the early days of the internet, it was easy to be hopeful about the human rights avenues that advances in information technologies would bring us. People reflected on how the internet could be used to circumvent government repression.33 Yet as time has passed, we see how the internet is not a force for good and that developments into an internet driven by user-provided content and data (Web 2.0) and beyond have important effects on our dignity and our humanity.34 But what aspects of our humanity matter in this discussion? What rights do we have, knowing that our data-hungry technologies are changing our lives as we use them?
This book is also aimed at and engages with work from other perspectives. I have drawn on work across social science and humanities fields, such as anthropology, communications, law, philosophy, science and technology studies, and sociology. These fields offer crucial insights, but they also reveal why political science's understanding of power and collective action must be part of the conversation that is already happening. I've dived into writing by computer scientists and other technologists who see even more change through the exciting work they're doing, but whose advancements lack some of the considerations those of us working in social science see as fundamentally unsettled, not proven. Ideas such as fairness, justice, and equality have been debated for centuries and are not easily solved by algorithms or collecting more data. In fact, data and algorithms can have detrimental effects for marginalized populations, as internet studies scholar Safiya Umoja Noble, political scientist Virginia Eubanks, and others have shown.35
Most of all, the power of the language of human rights has been missing from a conversation about how data and AI change humanity, for better or for worse. It's about rethinking how fundamental values embedded in human rights—our only global expression of what needs to be protected about a universal human experience—can reshape our thinking about our relationship to data and give us tools to demand other changes. This is not just knowing how to apply human rights to new digital realities because the reality is complicated, the issues and players are many, and in order to effect change, we need to try to simplify what the purpose of future regulation and rights making is trying to do.


What This Book Will Do
Few consumer products, like the Ring doorbell, consider human rights.36 After all, video doorbells are useful and seemingly innocuous. Yet they have tremendous implications for human rights, and they reveal some gaps in our understanding of core ideas of autonomy, community, dignity, and equality.
Think about this through the lens of a fictitious family, the Madeups. The Madeups live in a major metropolitan North American city, with widely accessible public transit and lots of foot traffic on their street. Because they live in close proximity to bars, shops, and restaurants, the streets on Thursday through Saturday nights are often lively into the wee hours with rowdy late-night revelers or people simply getting lost. A mix of owners and renters occupy the beautiful heritage buildings.
The father, Jason, is a comic book artist-turned-marketing professional for an interior design firm. He spends a lot of time on social media for personal and work-related reasons, with a popular Instagram presence and a respected blog highlighting design choices for clients and his family, as well as cheeky opinions about design trends, pop culture, and armchair philosophy. The mother, Claire, is a high school math teacher whose career before marriage was as a software developer at a startup in Austin, Texas. In spite of her former career, she has always been more analog than her husband; their living room includes ceiling-to-floor bookshelves with books lining each shelf, and she spends a lot of time walking to local shops rather than shopping online. Their fifteen-year-old adopted twins, Jack and Corey, walk to and from school, spend hours on their Xbox and other online computer games, and frequently encourage their mom to buy the latest smart gadgets. Their most recent victory was getting her to buy a Ring doorbell. At first, the Madeups love the novelty of the Ring. They're able to answer the door through the app on their phones, so friends are informed "just a few minutes!," couriers are greeted, and the family can avoid the solicitors who come calling during dinner. Even Claire comes around a little bit, especially since she can see when her kids get home every day if she has to stay late at work. After neighbors complain about parcels being taken by porch pirates, the Madeups expand the area of sensitivity of their Ring to capture more foot traffic. It makes them feel more neighborly, and they contribute footage to the Neighbors App by Ring, which provides a social media-powered neighborhood watch, allowing communities to share information and footage of possible criminal activity in the neighborhood.37 The Madeups note there are neighbors who frequently post and investigate leads to help make the neighborhood safer. Jason admires their vigilance, especially with regard to keeping eyes on the kids on the street.
As she walks through their neighborhood one day, Claire notes how many homes have some kind of video doorbell on their porch, easily spotted because they are much larger than typical doorbells. By her count, there are fifty video doorbells on the short ten-minute walk between their home and the grocery store. Each of them is watching her. Claire thinks about her kids' daily routes to and from school, friends' houses, or cafés, oblivious to the doorbells tracking their movements, and then she thinks about who else walks down their street. Who is being watched? Every time she and Jason take advantage of their walkable neighborhood—to get coffee, ice cream, or dinner—these doorbells are watching. More concerning, she realizes the doorbells are mostly watching children in the neighborhood, people frequenting the services near their home, and people who have to walk or take public transit in order to go to work. Their faces and travels throughout the neighborhood are captured, video feed by video feed, creating a panoramic of neighborly doorbells pulling together a chronicle of all of us who use public spaces.
Jason and Claire argue about removing their Ring. Claire's claims are very broad, but mostly they rest on discomfort. She feels uneasy about being part of this video doorbell surveillance community that has become widespread within the small radius of their home. As much as she wants to put it out of her mind, Claire's realization can't be unrealized: the community feel of the street suddenly turns oppressive.
Jason feels frustrated. What difference does it make? They're not doing anything wrong. Aren't we concerned with the safety of the neighborhood, and aren't the benefits of reporting on suspicious package thieves and property trespassers more important than people being seen doing what they do, every day? Isn't ensuring Corey and Jack's safety what really matters? Claire counters that the people choosing to install video doorbells aren't most of the actual people on the footage. Homeowners with perhaps good intentions are transgressing the rights of pedestrians simply moving in public without fear of surveillance, especially when some pedestrians have no way of avoiding the videos on their way to public transit.
Jason blogs a sanitized version of his argument with Claire, centering his views with what he thought were witty comebacks to his wife's concerns. To his surprise, his readers have mixed feelings. It's one of the most commented and polarizing posts he's ever had. Work colleagues write to tell him how deeply disappointed or strongly supportive they are. One commenter threatens to dox him after he appears on a couple of local radio shows. He's clearly hit a nerve. Jason feels mixed about the good feeling of his website traffic numbers going through the roof, yet some of the vitriol feels threatening.
In the meantime, Claire wonders what happens to all this footage. The teenage twins seem split. Corey supports her decision to disable the video and sound recording functions on principle. Jack points out that they now just have a very expensive doorbell with no real purpose. Plus, he argues all the other video doorbell users would have to disable theirs as well if we truly wanted to prevent neighborhood surveillance. Corey frowns: but shouldn't someone start? Maybe Claire, as a respected teacher, can convince others to do it too. After all, Claire has taught lots of neighborhood kids, and some of them are now adults living nearby. Jack rolls his eyes and shrugs.
Why should people have mixed feelings about a video doorbell? Some people might think trying to see Ring through the lens of human rights is trying to intellectualize a household product. Yet the growing phalanx of privately owned surveillance devices adorning the doors of modern society is about more than what one person feels about Ring and other smart technologies collecting data about any and all of us.

Why Human Rights Matter Now, More Than Ever
My intention is to articulate for readers why we need human rights in order to think about datafication as well as why data pose some major challenges to our existing conceptions of human rights. Transgressions of autonomy, community, dignity, and equality are why the Madeups' story should remind us that technologies such as the video doorbell, which sometimes feel wrong, are indeed wrong: those transgressions have human rights implications.
Usefully the foundational values of autonomy, community, dignity, and equality give us the answer to the question: What are human rights for? Political scientist Jack Donnelly argues that "human rights . . . point beyond actual conditions of existence . . . to the possible. . . . Human rights are less about the way people 'are' than about what they might become."38 Thus, these four values together form a core by which people can live up to their potential. Although we might be tempted, it is impossible to boil down these four important words into simple definitions. We know broadly that autonomy means acting unencumbered in the world, making and acting on choices freely.39 Community harkens to ideas about sociality and membership. Dignity is about the worth of a person, in both feeling and treatment. Finally, equality speaks to a desire to be treated without discrimination and in accordance to a certain baseline.
To date, these four human rights values have been anchored in the preservation of physical integrity—bodies acting, thinking, and learning. Datafication as currently practiced erodes our abilities to realize some of our basic human values because data operate in more invasive, invisible ways that are not always easily tied to physical transgressions.40 Instead, as writers as varied as scholars from Ruha Benjamin, John Cheney-Lippold, and Ron Deibert to thought leaders like Jaron Lanier and Cathy O'Neil and others have pointed out to us, these invisible data have visible, tangible effects on and in human lives.41 The way datafication works is that it alienates people from the very data about them, gathered from them.42 It hides the human in the numbers,43 shuffling those data into vast pools of data sets that are reworked "into a global mush."44
What seems obvious here is that if humans are involved in data, and human rights are about human potential, we should apply human rights frameworks to those data about humans. But if we imagine it would be easy just to "port" human rights from the analog to digital, we would be wrong.45 We can't just add human rights to emerging technologies and stir, expecting great things. We have to carefully consider the potential of human rights, and also its limitations, in shaping our futures.46 What is clear is that international and domestic policymakers are not thinking about the basics of human rights, instead being pulled into fixing specific rights "gone wrong" in the digital age. Each chapter in this book examines how our understandings of the basics of human rights—autonomy, community, dignity, or equality (sometimes several of them)—are fundamentally changing as a result of how data are being collected.
The difficulty stems from some peculiar qualities of data that I call "stickiness." It means that data survive longer than most of us ever imagine, transformed as they often are during processes of datafication. All of this means, as we will see, that stickiness inhibits our ability to apply and even learn from human rights in the data arena. This does not mean human rights are not applicable to data, but it does mean we will have to disrupt our thinking: datafication is no longer "just" a problem of privacy, freedom of expression, or inequality of access.47


Human Rights for a Sticky Situation
The human rights challenge is to reassert these values into our modern ways of living and inform our regulations and very philosophy of our relationship to data about and from us. We have to figure out how our "data doubles,"48 "data bodies,"49 or "data selves"50 square with our physical lives. Being captured on video by a doorbell such as Ring seems innocuous enough, possibly even justifiable, if we think that that footage is brief and serves personal purposes such as keeping a protective eye on our children, or a larger social purpose, such as preventing crime. Many of us are used to (or often unaware of) being surveilled on closed-caption TV in stores, offices, and public spaces. But how those data of us are stored, used, analyzed, and repurposed are largely beyond our control, if even known to us. The footage from doorbells like Ring is digital. Under subscription plans, it can be downloaded and saved indefinitely by doorbell owners.51 That data of and about us—our appearance, our gait, our mannerisms, our faces—are sticky: once those data are stored and uploaded to the cloud, they are there, perhaps deleted automatically, typically not. Whether they're downloaded, kept on someone's phone, hard drive, or server, no matter: the data are sticky because they're around, and it is hard to get away from or rid of them, even if you don't know about them. That's why Claire Madeup is so concerned.
Video doorbell data aren't as directly controlled by their owners as their owners think either. Ring has partnered with at least 1,800 US law enforcement agencies and a handful of fire departments. These partnerships historically involve some demands to promote Ring in some way, such as encouraging downloads of the Neighbors app, free doorbells, neighborhood maps of Ring users, and a portal that allowed police to directly request video footage, without warrants, from Ring doorbell owners. After repeated investigations and criticism of such marketing practices, Ring took steps in 2021 to make the process more transparent by having police publicly solicit footage from users through its Neighbors app. But police can still obtain warrants to gain access to footage directly from Ring if users do not want to volunteer video.52
It's not just about public authorities like the police. High-profile hacks of Ring users' cameras and two-way speakers in addition to video doorbells made public some of the concerns of having these kinds of technologies in the home. One case involved an eight-year-old girl whose bedroom Ring camera was commandeered to yell racial epithets by a voice who claimed to be Santa Claus.53
The private nature of datafication is two-fold. First, the technologies are largely owned by private entities, from the hardware of the doorbell itself to the software that powers the hardware, to the algorithms that analyze the data collected by the software on the back end. Second, the private, individual decisions to put up video doorbells have collective effects. Each neighbor might be doing so for justifiable reasons, but in effect, each doorbell collects information not just about that neighbor but whoever passes by that neighbor's house. In other words, these private decisions have individual and collective effects.
Besides video data, other types of data about us are connected in a sticky web that may ensnare us in the most unexpected times. Many of us may not yet have encountered the bad effects of data's stickiness. Consider the stories of two women whose credit and background data stuck to them and caused problems years later. For years, Rafaela Aldaco had struggled with financial instability.54 She felt she could forge a new path when she learned she had been accepted by a housing program designed to help struggling folks like her. This single mom of two, however, was confronted with her past when she showed up to get her key. Program officials told her they had changed their decision. It turns out a tenant screening report by RentGrow, one of many companies offering background checks, returned a record for a battery conviction when she was eighteen years old. This was supposed to have been removed from her record in exchange for community service and six months of probation.55 She lost both the apartment and her eventual legal suit against RentGrow for using that information. Aldaco learned painfully that just because her criminal record had been expunged by the courts, it hadn't been deleted or purged from other databases.56 Courts decided RentGrow had the right to use the information.
Things turned out differently in Samantha Johnson's encounter with sticky data. In 2018, she was apartment shopping in Oregon. Automated background reports, which landlords increasingly rely on, turned up the following records on her: "Burglary and domestic assault in Minnesota. Selling meth and jumping bail in Kentucky. Driving without insurance in Arkansas. Disorderly conduct. Theft. Lying to a police officer. Unspecified 'crimes.' Too many narcotics charges to count."57 In fact, these charges were not Johnson's. Hers is a common name, but the algorithms don't know the social significance of that. They lumped her in with five other women, including someone in jail at the time. Johnson was lucky: she convinced a landlord that the record was not hers and was able to get a place to live. Johnson was given a chance to plead her case directly in spite of the data associated with her.
Both Aldaco and Johnson encountered restrictions in their ability to move around in the world because of sticky data. The difference in their cases speaks to questions of how data affect us differently, depending on our demographic characteristics or our individual abilities to convince others to have a face-to-face interaction despite being weeded out by an algorithm. Stickiness is not experienced evenly. Factors such as race, class, or occupation, often can exacerbate the stickiness of data, highlighting how equality as a value of human rights is affected. But for all of us, data's stickiness affects human rights concerns like autonomy. Aldaco's situation also speaks to one of dignity. Past crimes of her youth continue to dog her, even as she tries to overcome those mistakes. If the internet remembers everything, at what point can we as human beings allow people to move past their pasts?
Whether we're talking about the stickiness of credit scores or video, or other data about people such as their heartbeats, sleeping habits, and social media history, these are all changing the way we think about "community." What we know about one another and ourselves, and how that affects our sense of belonging, has changed dramatically with datafication. Although data are sticky by design, we can regulate that stickiness to make it less tacky or reorient our expectations so that facts can be moderated. By confronting these changes to our sense of human autonomy, community, dignity, and equality, we can use human rights as a focal point around which to gather as data stakeholders. As stakeholders, we should be participants in rewriting the terms and conditions under which data about people are being used.



Where This Book Goes
This book is agenda setting. By casting the net far beyond my usual political science waters, I aim to redirect conversation about everyday, data-intensive technologies that have seeped into our lives. I explore power, collective action, and our humanity through the lens of human rights. I apply my expertise in human rights, social movements, and global norms to the study of datafication through emerging technologies.
Stakeholderism by its nature embraces community and shared understandings, and it captures the interests of our collectives, however we define them. Our digital world, and the datafication brought about by AI and data-intensive technologies, is a collective reality. We may all experience this reality somewhat differently, but the datafication infuses our lives. Human rights provide a vocabulary of entitlement and a universal mindset critical to reining in data-intensive technologies. It grounds our stakes in ways that cannot be easily replicated.
In an effort to demonstrate the breadth of datafication experiences that should include human rights considerations, each of the chapters in this book serves two purposes. The first is to articulate how datafication changes human experiences. The second is to draw readers' attention to areas where human rights can help us insert ourselves more actively as data stakeholders, whether to shape conversations around data-intensive technologies, or to make demands of policymakers and data collectors to protect core values.
Chapter 2 reviews core terms and concepts of the book, including data stickiness, stakeholderism, the link between algorithms and data, and why datafication currently eludes our best human rights-informed efforts. We discuss how countries, which used to be the main targets and guardians of human rights work, have become less important and why.
We then investigate how datafication has created vexing human rights consequences. Chapter 3 treads into the topic of data rights. Although it's tempting to claim "our data," we'll see why data about people aren't actually "ours" in a straightforward sense. At once cognate to human rights and existing property rights, data rights begin running aground when we think of the stickiness of data and the co-creation of data. Indeed, data exist only with a source and a collector. Neither party, especially in the context of data about people, has full claim on the data. We explore existing perspectives that emphasize privacy, property, and "the right to be forgotten" and demonstrate how co-creation hinders all of those methods of defining rights. Unless we direct our efforts to recognizing the co-creation and collective nature of data, declarations of data rights will fall short of accounting for autonomy and dignity. Chapter 4 examines the topic of facial recognition technology (FRT). Commonly deployed and not well explained, FRT takes a mundane yet central feature of people—our faces—and transforms them into data that can be compared to other data to make inferences about people. Since faces are personal and core to how we understand human dignity, FRT's datafication of our faces challenges our ideas about dignity. In terms of equality, the way FRTs have been deployed has been shown to be biased against certain races and genders. Their ubiquity in the hands of corporate and government officials certainly raises challenges for autonomy and potential concerns for human community when deployed against certain groups.
Turning toward a more morbid topic, chapter 5 explores what happens to data about us when we die. We look into digital immortality enthusiasts and others who are creating chatbots based on data from real people. The boundary between living and dying in a datafied world is increasingly fuzzy as we explore technologies that can keep at least certain aspects of people alive when they have physically perished. Questions of human dignity and autonomy abound, especially when we currently do not have ways for people to opt out of posthumous uses of data about them. In chapter 6, we consider the implications of Big Tech's role in the global governance of data. Governance has been a significant preoccupation for political scientists, as it is so central to ideas about power and distribution. Here, we look at the Oversight Board that Facebook (which has since renamed itself Meta) started as a way to govern content on its platforms. The establishment of the board explicitly states that it will help ensure freedom of expression, a task usually left to governments. The chapter explains the consequences for human rights when nonstate actors such as corporations start governing in these areas through their platforms and explicit bodies as the board. Chapter 7 is a declaration for all of us to consider data literacy a human right and an argument for why we need this particular right in the age of datafication. By expanding on the existing right to education, a right to data literacy is an adaptation of education efforts for the digital age.58 If data literacy becomes an integral component of our education in the same way many literacy standards include linguistic and numerical literacy, we may develop the core competencies for data stakeholderism. Chapter 8 lays out future directions and hopes for thinking through how human rights might shape data-intensive technologies and policies regarding data collectors.











2
Why Human Rights and Data Go Together


The central mistake of recent digital culture is to chop up a network of individuals so finely that you end up with a mush. You then start to care about the abstraction of the network more than the real people who are networked, even though the network by itself is meaningless. Only the people were ever meaningful [emphasis added].
—Jaron Lanier, digital technology expert1

After the blowback of his blog post on his disagreement with Claire about Ring, Jason reconsiders some of his daily practices. He looks at his Instagram page, which is public, and where he has tied his family photos into his brand as a design commentator and critic. He has had to close the comments on many of his posts because of rampant trolling that has gone up since the Ring discussion, including insulting his children, questioning the Madeups' parenting, and just generally nasty content. So far, it seems to be contained online, but Jason is still unsettled. He has spent a lot of time painfully looking through social media and the little quirks he notes about his family. They seemed quaint and personable at the time, but now he realizes how his drive to create a substantial online following also means he has no idea who has knowledge of Corey's distinctive purple birthmark on his left shoulder or Jack's predilection toward all things with kale, or how he and Claire met one day because he literally smacked into her at a farmers' market. He wonders how these adorable yearly photos of his children, from toddlers to now, jumping on overstuffed couches in their living room, reveal details not just about the changes in his children's physical appearance but also their preferences, not to mention his own design choices and what they reveal about their family's status. Just what has he revealed through his own choices?

Datafying Human Life
Human rights, long tethered to physical concerns, necessitates thoughtful reconsideration in the age of data. Before we dive into the ways that datafication makes human rights values ever more vital, we need to understand how datafication fundamentally alters the human experience.
Datafication converts our lives "in[to] quantified form so that [they] can be tabulated and analyzed" by computers.2 It allows for storing and analyzing data about us, data created by our actions. More specifically, definitions of datafication often emphasize the digitization of the data about humans and the economic value these data generate.3 In combination with the increasingly sophisticated systems of algorithms we call "artificial intelligence," the enormous quantities of data that are collected about our behaviors is made to sort and stratify us, make predictions about us, and hustle us into trajectories in ways that weren't possible before.4 Some of this is beneficial, if we prioritize values such as efficiency, order, accuracy, and preference satisfaction. Using AI to sort through photos of traffic lights versus stop signs is a leap of efficiency from having humans complete the task—once we figure out how to teach the computer to "see" the differences. This can cause harm if we prioritize values such as oversimplification, haste, homogeneity, and bias. Using AI to sort through job applicants oversimplifies the hiring process and often features built-in biases and outright discrimination of marginalized groups. AI, learning from previous successful résumés, simply automates homogeneity and other inherent biases.
What makes datafication a human rights issue? When we have technologies that fundamentally alter our social and political relationships, skewing power dynamics, and creating insurmountable hurdles for individuals and groups to live to their fullest potential, we can look to human rights for protecting what cannot be lost. In this book, these values are autonomy, dignity, equality, and community. This chapter shows how datafication affects these values and the trials to human rights posed by the stickiness of data.

How Much Data?
It's worth pointing out just how much data we're talking about. We used to live in a mostly analog world, and we could turn our computers off. Digital data came from what we fed machines when they were on. Now, few of our key digital technologies are meant to be turned off, and data are ubiquitous. We don't stop generating data through our devices, whether we're sleeping or awake, whether we know it or not.5 Every minute of every day, we generate data about ourselves and others: what we're doing, what we're thinking (or not), where we are, how much we're sleeping, who we like, what we buy, and whom we communicate with. In the early phases of the COVID-19 pandemic, 575,000 tweets were posted, 167 million TikTok videos were watched, and 6 million people shopped online per minute.6 That's an awful lot of data about what people were consciously thinking, saying, and wanting. One earlier estimate by an IBM study concluded that every day, people generated 2.5 exabytes of data.7 An exabyte is approximately 1 quintillion (1018) bytes, or 1 million terabytes.8 In 2023, an M2 13-inch MacBook Pro laptop's default hard drive is 256 gigabytes, or 25 percent of 1 terabyte.9
But when we think about data about people, taken from people, there are more data types than those we provide knowingly through our actions. There are also data that our devices collect about us: the sensors in our phones that can tell where we are, and how we're doing it. An Apple Watch not only collects location data but also data about how you might be moving ("Looks like you're working out" with an icon for the type of activity, such as biking or running), the duration of that movement, and its effects on your body in terms of calories burned and heart rate. Also, data are generated from data—that is, the inferences that are drawn from all the data from all of us and reflected back on each of us individually.10 We are grouped into categories and typed for our propensities to spend, read, and follow. If you clicked on an ad for pasta, maybe you'll click on an ad for a trip to Italy. If you watched that YouTube video on dolphins, maybe you might also care about tuna or manatees. All of these data, taken together, are overwhelming. The numbers and terms used to refer to them quickly become abstract.
Estimates of how much digital data are being generated are also a problem for archiving, sorting, and solving search problems. How to make so many bytes of data useful is a challenge. Data scientists and engineers have to find ways to make those data accessible when needed and relevant.11 And that's where algorithms come in—computer code that makes everyday and important decisions about us and for us. They are programmed instructions that outline procedures that in turn govern our lives, taking the data we produce and making inferences and predictions—educated guesses—about who we are and will be. The educated guesses computers make can dictate whether we get a mortgage,12 go to jail,13 receive welfare,14 or the grades we receive that affect our college admission.15 Byte by byte, all of the data and the plethora of algorithms applied to them change our very humanity: they are shifting how we live our lives. We are currently largely unaware of these changes, but we do have a stake in determining how these technologies will shape our futures and choices to make about our humanity.


Understanding Data as Social
Data in general exist because someone created them. There are no "raw" data.16 Someone, be it a corporation, a researcher, or a government, has to decide on a topic of interest (say, leaf shapes and sizes), a question they want to explore (why trees have differently shaped and sized leaves), and how they might go about finding evidence to answer that question (collect leaves from different areas of the world). These are known as the process of data collection from data sources (leaves) by data collectors. In this example, data collectors must decide what characteristic of leaves they want to record, create a ledger (in a computer spreadsheet, or in paper notebook) with all of the recordings of various leaf characteristics, and do their analysis. Perhaps by emphasizing size and shape, they leave out the texture of the leaves or their smell. Data, by their very constitution, necessarily show only a limited perspective of the real world, depending on the purposes of data collectors.
Given the explosion of so-called Big Data, the expansion of computing and data storage capacity, and the rise of the World Wide Web as telecommunications technology, data about human activity have proliferated in aggressively expansive ways.17 These data power entire industries and have, as Shoshana Zuboff demonstrates, created new forms of capitalism based on our actions, thoughts, and ideas.18 That so much data about people, collected from people, is consciously planned, pooled, maintained, and redeployed as sought-after products is what concerns us here. It's important to remember that these data are about people. They are not the same as data taken about leaves. For better or for worse, we tend to value human life differently from other beings. We attach values such as autonomy and dignity to human life, and we want people to be treated equally.
Data about people currently skirt and even transgress these values. The human aspect of data is undermined by interests in recording and codifying aspects of who we are. Whether thoughts we post to our social media, or what we type in as search terms in our browser, or mundane activities, such as where we walked after we went to the local coffee shop for a muffin and coffee, the data that are grabbed and transformed are no longer fully reflective of human creativity, curiosity, or pleasure. Nor is the creator, searcher, or walker aware of what data are captured and what will be done with them. These data surely have monetary value, and that is certainly worth exploring, as others have done.19 That they are about people and emerging from human conscious and unconscious activities also make these data valuable humanistically and sociologically. Often the way datafication has spread prioritizes thinking about the way that those data are chosen, collected, and used to create new products and life opportunities. Relegated to the background are the humans whose lives and data are at stake.20



The Data Are Human
We should also consider how extensive and furtive datafication can be through innocuous technologies. It can be a challenge to see why human rights matter in datafication, until we probe more extensively into how pervasive it is in modern life. Without thinking about what datafication is fundamentally changing about human experiences, we can't see how much there is to grapple with, even with as established and expansive of a framework as human rights.
Devices that companies intend to help with one aspect of our lives blend into many other aspects of our lives—for example, the aspirations to be or become physically fit. Enter Fitbit, a wearable device to help achieve that goal. As many readers surely know, it is a sleekly designed, smart pedometer that comes in a rainbow of wristband colors. Fitbit made 10,000 steps a day a widespread health mantra and helps us by tracking our steps, driving some people to talk constantly about "outstepping" their friends and family in "friendly" competition.21 It discerns the difference between exercise and other activities, so that we (theoretically) don't cheat. Fitbit has paved the way for other fitness trackers, which have become part of individual self-improvement, encouraging users to create a "quantified self," that is, a datafied profile as a record of their activities.22
As part of their Internet of Everything series, the Canadian Broadcasting Corporation produced a documentary short about the Fitbit.23 The documentary follows four twenty-something Parisian roommates who volunteer to wear Fitbits and share the data with director Brett Gaylor. All of them express high hopes that wearing the Fitbit will encourage them to be more active. As the tracking starts, Gaylor observes that the roommates are initially quite sedentary. That did not stop them from generating lots of data. The Fitbit itself collected data on sleep patterns, heartbeats, and menstrual cycles (as the sole female roommate pointed out early on), among other things. When combined with the Fitbit app, the roommates generated location data. Gaylor shows us how all the data quantifying conscious and unconscious bodily behaviors are displayed in a colorful graphical array. In a cringey scene with researcher Mathieu Cunche, Gaylor, looking at their sleep and activity patterns, realizes that two of the roommates were having sex. In the film, this finding is laughed off in a bit of locker-room talk. Yet this inference about sexual behavior is what makes datafication so unsettling: it takes social (intimate) facts and quantifies them.24
Fitbit, as it's revealed at the end of the film, was purchased by Google in 2019.25 Some of the participants expressed unease at allowing Google into their lives. Unfortunately, this well-known Big Tech firm is not the only one doing the tracking. The film also addresses the widespread collection of use data that wellness programs tied to the insurance industry currently use as a way to reward healthy behavior with discounts and other perks. The so-called vitality program, which the South African insurance industry first created, has become a standard of the industry on multiple continents as a way to track user behaviors. Although it is currently an opt-in program, the film speculates about how these programs may eventually become effectively mandatory as employers or other important institutions demand enrollment in such tracking programs.
The film is enlightening and highly watchable, but it puts the onus on us. As researcher Cunche warns viewers, users of wearables should be aware of who is using their data. This scolding falls flat when most of us aren't sure what kinds of concrete actions we have to take to know that information.26 And then Gaylor states matter-of-factly to conclude the documentary, "Every point of data you hand over will impact someone you've never met." Ending the film about individual Fitbit users' experiences on this profound point is confusing, but it does open the audience's thinking to the greater issues: companies, whether fitness trackers or Google, are interested in the aggregate data to influence far more than one data-providing physical fitness enthusiast. What and with whom are we engaging when we use these data-intensive devices and technologies? We need to become aware of how extensively we are personally and collectively affected by every use of these technologies. How can we find out? As suggested, the data that private companies collect and how they use them are not easily obtainable.
What does it mean that data can be collected ubiquitously about us? The first thing to understand is that the data collected about us as individuals through Fitbit-like devices are used to generate algorithmic predictions about people "like" us. The data will be analyzed and manipulated to infer things about us as individuals and about what people "like" us want to buy or read or value. Marketing companies such as Acxiom have created tools like Personicx, which contain dozens of lifestyle categories based on data that are said to predict our behavior. Consumer markets such as "Cultural Connoisseur," "Value Pack Renter," "Going Places," or "Grey Volunteer" are based on behavior and demographic patterns, with household to zip code precision.27 Among other things, grouping "like" together means algorithms can replicate existing discriminatory patterns, amplifying existing racial, gender, and other divides.28 Others' actions matter for us too because sometimes data about us come from people with whom we come into contact or communicate with—group photos posted to social media, for example. That family reunion selfie your second cousin posted to Facebook implicates everyone in the photograph. Digital photographs are used to generate facial data for facial recognition software, for example. Unfortunately, we often have no way of knowing if others have posted photos of us. How data will affect others, then, poses major challenges to claiming "data rights," which we unpack further in chapter 3.
Given how data are currently collected, it might seem as if we are the subjects of a big science experiment to which we never consented. It's as though data collectors seem to understand how important the data are because they are about people, but they somehow forget that they take data from human rights-bearing individuals with agency, conscience, and dignity.
Although I am concerned with data from and about people, I haven't used the term personal data. Ask someone what personal data are, and they'll probably rattle off some things: names, date of birth, social insurance number, driver's license, financial accounts, address. They might even say fingerprints, DNA, and iris data if they're familiar with debates around biometrics. But what about how you shake your mouse when you're thinking? How you use your hands to talk? Your credit card use patterns? Your commuting patterns? The point isn't to get into debates about whether something is "personal" enough to be personal data or whether biometrics are more essential types of data than demographic data. Our knee-jerk sense of what personal data are is outdated.29 Data today encompass far more than what is on our identification cards or our passports. As early as 2016, researchers were able to identify a person, along with key demographic information like age, gender, and ethnicity, with location data from as few as two apps.30 We might say "personal data" really are about "identifiability" in the digital age, and indeed, the "identifiability" standard is often invoked in policy.31 But identifiability is a slippery slope. As datafication balloons, algorithms have more data to blow through and find more patterns. People's identifiability increases as more recorded behaviors are available to be triangulated.
At the same time, there is something qualitatively different about data about people and data in general. Consider, for example, data about distant stars, the composition of soil content, the weight of musk oxen, the paths of quarks, the temperature changes in a carrot cake as it bakes, or the ups and downs of spring weather. Contrast those data with someone's astrological sign, their gardening preferences, pets, predilection toward physics or languages, favorite foods and things they're allergic to, and whether they prefer Celsius or Fahrenheit. The answers to the first set of data exist regardless of who we are because they are external to us. The data exist because someone decides to measure them. The second set of data about a specific person reveals quite a bit about that person.32 Divulging someone's activities, behaviors, beliefs, and preferences paints a picture of who they are, designing a profile of their personhood.
At bottom, a datum will tell us something about the world. It's an often-minuscule piece among multitudes of equally minuscule and apparently uninteresting and inconsequential pieces of information. But datum upon datum upon datum, data aggregated in multitudes, then arranged and rearranged until they reveal patterns, give us something to understand, interpret, reuse, and wield in other initiatives. The data that come from us as individuals and groups provide a basis for uncovering those patterns, analyzing them, drawing conclusions, and formulating predictions that generate more need for data. When we call them "data," we forget that the data that are causing most of the uproar are from people. These kinds of data differ from other types of data in that they are sticky.


Data Are Sticky
In addition to being expansive, datafication is sticky. Data taken from human data sources are sticky like gum on the bottom of a shoe: easily stuck but not easily removed! This stickiness would be less of a problem if the data weren't so revealing of who we are. There are four reasons for this stickiness. First, they are co-created, which makes them both individual and social at inception.33 To exist, data about people need a source and a collector in order to co-create the data. Because they are social, we also do not control how data about us spread.
Data are about mundane, not newsworthy, activities. These are surplus behaviors—things that used to be of no obvious interest to anyone.34 They are also not preventable behaviors in a practical sense: the time you must leave home to get to work on time, the number of seconds you stare at a web form before completing it and your keystroke patterns on the keyboard as you do so, that you order Pad Thai for the third day in a row.
Data are also sticky because they are linked. They don't live on their own or simply stay in discrete data sets; we share them, and collectors share, sell, and merge them to commercial or other ends, exploring new trends in analyzing them, and so on. Digital data travel easily: they replicate and transport.
Finally, because they are easily replicated and transported, data effectively last forever. It's exceedingly difficult to ensure that our information does not go somewhere in the world we didn't intend or think possible. Just think about how deleted Tweets seem to reappear in screen grabs, or some long-ago, trashed email resurfaces inconveniently. Even if they may not actually be forever, we may as well think of them as such because they are, for all intents and purposes, out of our control once they are created.
These four characteristics—data's co-createdness, mundanity, linkedness, and practical immortality—make data sticky. That stickiness changes how human autonomy, dignity, and equality are practiced. Data redefine our human communities. These are the crux of our human rights. Human rights need to be written into the practices of data collection and their use. The way data are collected and used need to be evaluated for their effects on human rights. But stickiness also should give us pause on how we make claims about "our" data and what rights we can assert in claiming data rights. The stickiness of digital data has fundamentally altered how we invoke human rights.

Data Are Co-Created
The co-creation of data defines a power relationship between data sources (human individuals) and data collectors (companies). Data are worth collecting because they reveal our personal patterns, which has social and economic value. Humanity is the mine from which companies extract value by making us data.35 We are the source of this economic value. Data collectors, the co-creators, need us as much as we need them, if in very different ways.
Because we appear in other people's data—remember that family selfie!—co-creation also involves data that don't come directly from us.36 These social, co-creative aspects of data are hard to disentangle, especially from the view of human rights.
The co-creation of data is just one reason that I call data sticky. Although we are the source of data, the data are quickly coupled with other's and multiplied algorithmically. Not only are they co-created, but this is out of our individual control. They live beyond us and cling as representations of who we are. The data affect what we see as our online worlds and have effects in life. As co-creators of data, should we not be entitled to some control over the data—whether they live or die and where they travel in between? At the moment, the data live on, stickily, beyond our reach.


Data Can Seem Mundane
The memoir of sportswriter Cathal Kelly is about how the mundane helps us find meaning. He writes, "The fascinating things about life are the banalities we so rarely discuss amongst ourselves but that we devote most of our energies to navigating. . . . Yet string a few thousand of them together and that's a life."37
The everyday, often unavoidable and somewhat boring, is typically not the focus of memoirs or history books. Individual decisions about what (not) to eat, whether to wear the striped or plain top, or our unconscious practices, such as our gait or how we smile when we're anxious: these are mundane, often necessary, and previously unremarkable. They would have been lost to memories about the extraordinary, the notable. These things used to not matter.
All of that has changed because a lot of datafication captures these data of our everyday behaviors and actions. The mundane matters more than ever as datafication allows those small things to be harvested, so that we can be nudged: buy these fuzzy socks, read this salacious article about that candidate. Social media take mundaneness to a whole new level of celebrity, capturing and circulating commonplace activities, from teenage pranks to young people dancing in their rooms to the unboxing of coveted consumer items. When everyday activities are accumulated, they no longer remain trivial. A population's worth of "mundane" data is valuable to people or computers (or both) that analyze, categorize, and generate further initiatives and outcomes.
On top of creating mundane data, the way we communicate digitally generates tons of unremarkable data. What could be more mundane than metadata, or data about our communications? It's not even the everyday content of the communications described above.38 Some examples of metadata are where devices were located when messages were sent and received, what kinds of devices were used, with whom they were communicating and how frequently. Metadata are the definition of mundane—descriptions of the way in which the mundane happened. Perhaps it's easy to dismiss metadata as about "nothing," but as former National Security Agency director general Michael Hayden admitted, "We kill people based on metadata."39
The mundane leave an incredibly useful trail of information. The kind and level of detail provided about billions of daily lives is simply unprecedented in human history in terms of scope and granularity.40 It is the type of access into human existence that demands we rethink what it means to live, as increasingly all of our activities, ordinary and extraordinary, and details about how those activities happened, whether we are aware or not, are stockpiled in giant server farms. We know better than ever how all of our lives are lived.


Data Can Be Linked
The data that are collected on our activities do not typically stay within an organization's bounds.41 Data can be bought and sold because they are easily copied and transferred. Digitization has made the physical weight of data and record keeping much lighter. The data we generate have value, and the data market is huge.42 Data are also relatively cheap to grab and store, creating incentives to make as much of them as we can.43 Therefore, all the data about our mundane behaviors can be stored, creating value as we generate a fuller picture not just about each of us and our doings, but what other people like us, around us, are doing.
Think about linked data as a dense, overlapping series of webs of data gathered directly from a single person, combined with inferences made by incorporating other data, together generating new data and new connections. As the countless numbers of different individual and composite data sets grow, we get what we now call Big Data. Size and computational power are then leveraged to analyze such huge quantities of data.
Because Big Data is all about computationally cross-referencing data to create clearer, more comprehensive, and ultimately more useful and profitable profiles, remaining "undatafied" has become very difficult. Hackers and computer scientists alike have created tools to identify data with persons, even with data initially thought to be anonymous.44
Big Data has yielded new helpful insights. In medicine, diseases such as cancer can be spotted and treated using linked data, and researchers can use pooled data to track the progress of infectious diseases such as influenza or dengue fever.45 These applications can help people realize the human right to health by preventing the spread of potentially deadly diseases. Yet linkage in medicine is not always successful if the data are spotty, inconsistent in measurement, or generally of poor quality. Hundreds of efforts to pool data on patients and use AI to find treatments during the COVID-19 pandemic, for example, yielded very little.46
And why not have more data to evaluate potential loan applicants, job seekers, and renters? The advantages of linked data assume that the data available for everyone are accurate, and there aren't inherent biases built into how those data are constructed, including what to do with multiple entries that look the same in a spreadsheet. Chapter 1 looked at how the linked nature of data sets became a problem with Rafaela Aldaco and Samantha Johnson. For Aldaco, some data sources deleted information about her criminal record, and others didn't. For Johnson, her common name resulted in the sweeping up of irrelevant, negative information across data sets.
Data linkage can also lead to other kinds of reputation damage. There is no referee of the internet, and so data can travel from website to website with relative ease. The ability to quickly collect and compare different information is a great feature for shopping airfares or aggregating news stories, for instance, to save time. Conversely, the same powerful data amassing features that amplify one person's rights to freedom of expression can severely hamper others in terms of their autonomy and, in some cases, dignity.
These websites can create hard-to-escape reputational harm. Data linkage between "complaint sites"47 such as Ripoff Report,48 Cheaterbot,49 and She's a Homewrecker50 replicate anonymous, unverified, often baseless "complaints," that remain tenaciously on internet searches. This happened to a software engineer named Guy Babcock and various family members, including in-laws and minors.51 It took years for him and his family to get "pedophile," "thief," and "fraudster" removed from their Google search results after a past employee of his father, Nadire Atas, posted libelous claims on complaint sites that got amplified across the complaint sites. Atas smeared dozens of enemies by looking for family members, grabbing social media photos, and writing fake stories, but the linked quality of data does not necessarily come from elbow grease.52 New York Times reporter Aaron Krolik conducted an experiment, writing inflammatory content about himself and watching it explode across the ecosystem of complaint sites.53 Within a month, his single post had been copied on fifteen sites. His work also unearthed an entire industry of "reputation managers" who can charge tens of thousands of dollars to remove libelous posts. Some of these same reputation managers have also played a role in perpetuating false stories. Such data linkage can ruin one's employability or ability to secure housing, creating at the very least hardship. For those who are already marginalized, these kinds of reputational harms can create human rights concerns when they are systematic or consistently hamper access to basic services.
Once datafied, information becomes mergeable, shareable, and quantified, thus formatted for computer-generated inferences. Linking data performs useful social functions for helping governments make better choices, companies provide better products, and making our lives more seamless. But it also means that datafication is ever more pervasive as coverage is expanded. The connected quality of data cannot be separated from algorithmic sorting and analysis. Algorithms manage incredible scale and scope of data, beyond human capacity (which is why we find algorithms useful to begin with). Through their analysis, algorithms themselves create more data out of the merged data sets, identifying patterns and categories like the lifestyle categories. Being sorted into categories can create advertising opportunities that companies like Acxiom seek, but it can also limit one's access to opportunities and create identities to which we don't subscribe. After all, who identifies as a "Grey Volunteer" or "Value Pack Renter" in the Personicx database, and who doesn't see themselves as "Going Places" or a "Cultural Connoisseur"?54
Perhaps most insidious, these categories into which we get sorted obscure how people can find out why they've been sorted one way or another. It's one thing for activists to link up online through their personal and protest worlds, as sociologist Zeynep Tufecki chronicles.55 It's another thing if those categories don't explicitly exist as "social groupings"—so how do I find the other people who are also "Going Places" so that we can act collectively? It's not even the collective action problem that we talk about in political science. It's a problem of identifying the collective.


Data Could Last Forever
This brings us to the last characteristic of data stickiness: data can survive effectively forever. It is hard to know where those data go or whether they are gone. Data sets are linked. Therefore, deleting data does not guarantee, at least at this point, that the data truly have disappeared. When someone deletes them, there is no way for anyone or any machine to verify that. As data sources, we should not assume we can ever kill data, once created.
That is what can make the dead "come back to life," as we will see in chapter 4. That's what's so remarkably uncanny, or perhaps comforting to some, about MyHeritage,56 a company that breathes life into our photos by making them move, including facial and eye movements. That data are effectively forever means that people can live on in different ways than they ever had in our analog past. To date, we don't have a coherent way to think about how data might live on after we die, and we have no framework of the personal rights we ought to think about, have, or demand living in this digital world. More troubling perhaps is that we also don't have a good way to think about the death of data while we're still living. Data are as good as immortal.



Sticky Data and the Politics of Human Rights
Big Data computation drives a great deal of decision making in the most disparate areas of life. From the legal and political, to financial, medical, and commercial realms, to name but the most obvious, more and more aspects of our lives are made and influenced by Big Data computing. Because private companies such as Google, Amazon, Meta, Apple, and Microsoft make it part, if not almost entirely, of their business to collect, analyze, and traffic data, the ground rules have changed.57 States and governments are no longer the main data-gathering entities they were when they wielded the kind of money needed for gathering and analyzing large data sets. To track people's whereabouts, for example, the Stasi of East Germany employed 100,000 regular workers and engaged 500,000 to 2 million collaborators by 1989.58 As of 2022, people freely supply their own musings and whereabouts: every minute of the day, Google runs 5.9 million searches, and Facebook users share 1.7 million pieces of content.59
The East German government—among other authoritarian governments—chose to surveil its citizens and repress basic human rights to freedom of expression, association, and other rights. Similarly, Big Data and, more generally, the power of Big Tech arose as a result of policy choices. The hero worship of the brash, "move fast and break things" tech entrepreneur in a gray hoodie (yes, Mark Zuckerberg, but he wasn't the first or last) is a choice.60 From the early days of techno-utopianism and counterculture, the internet was seen as a solution to society's erstwhile ills.61 It's clear now that techno-optimism in data and the power of algorithms to solve our problems is threatening our autonomy, community, dignity, and equality in notable ways that we need human rights to reverse.

What Human Rights Do
Human rights are often thought of as a weapon of the weak against the strong. Indeed, they were pronouncements against state incursions into the lives of people—defenses against power in a declaration of "No more! Stop! Enough!"62 Human rights are meant to change wrongful activity and halt transgressions against personhood. Rights can be seen as a limitation on a powerful actor or a set of obligations on the powerful. There is no predetermined "bad guy" so much as it is about restricting power differentials.63 Historically, states had tremendous power over people and most groups, but today Big Tech's ascendance puts pressure on a state-centric view of human rights. We have to expand the circle of human rights obligation to cover corporations if we are to truly redress the effects of datafication on human lives.
Human rights differ from other rights because they are self-consciously universal; they represent a set of values that articulate standards of what a human life should entail. Some of these rights are about protecting against harm or interference, such as the freedom from torture or extrajudicial execution. Some of them are about active provision by others, such as the right to food or culture. This is typically thought of as the distinction between negative and positive rights.64 But there are also rights that are both preventive and active, such as the right to vote, the claim to ancestral lands, and the right to assemble and protest. Other people need to be prevented from blocking our ability to enjoy those rights and simultaneously respect our claims.
As much as human rights have captured our imagination and have become an integral part of our vocabulary, they are no stranger to controversy and heated debates65 since the first writing of the Universal Declaration of Human Right (UDHR).66 This human rights framework has grown from these multitudinous deliberations to embrace the fullness of the human experience.67 Although there is plenty of disagreement around whether human rights are "failing" or "succeeding," there is no dispute that human rights exist and have been used by activists, lawyers, and citizens to fight for basic recognition of every person's humanity.68 At the same time, the heart of all the various laws and normative expressions of rights revolve around the four fundamental values that we started this book with: autonomy, community, dignity, and equality. These four reasons, taken together, are why human rights have grown: as we have built on the UDHR, we have found more and more ways to incorporate essential qualities of human life. We have also found more ways that states and others violate the means by which we can realize these core values.
There are nine core United Nations human rights treaties,69 and many more other human rights documents from other organizations such as the African Union,70 the Council of Europe,71 and the Organization of American States.72 From this expansive, sometimes unwieldy framework, activists have developed ways to think about human rights as an advocacy framework that emphasizes the importance of identifying the violation, violator, and remedy.73 While this is not the only way to practice human rights, it is a widespread and useful way to frame human rights wrongs when they were (largely) about bodily harms.
This straightforward who, what, and how formula does not port well to data-related problems such as AI.74 For example, the prospect of having an algorithm evaluate our credit-worthiness75 or eligibility for refugee status76 was not on the table in 1948 when international human rights were first being articulated as universal standards (regardless of where we lived or who we were).77 That's why lawyers Dafna-Dror-Shpoliansky and Yuval Shany have written about the need for rights for "online persons."78 They argue that the way we exercise our rights isn't the same online and offline.79 We see this clearly in an area such as freedom of expression, where online harassment, trolling, and "cancel culture" have become social and political problems without easy solutions.80
New technologies such as AI breed new solutions for human rights action, like using geolocation data in tracking human rights atrocities.81 These solutions also raise new human rights problems: Who manages the digital data of those who have died from natural disasters or political conflict, for instance?82 Thus, we need to generally know how human rights help mitigate the effects of datafication before creating new human rights. The fact that it's not "in real life" doesn't alter the fact that we are each entitled to human rights.
The stickiness of data and the work of algorithms, which largely take place without our knowledge or anyone's active supervision, challenges the very analog design of human rights.83 To think about datafication in the context of human rights, the effects of data are simultaneously individual and collective. Already, we have some analogical situations with existing rights. After all, what would the freedom of religion or expression look like without an understanding of how individuals fit into or resist the collective? Similarly, the right to health doesn't make sense without understanding how individual health outcomes affect overall population health and how population health measures help individuals. But the group/individual aspect is still an incomplete frame for datafication because of data stickiness.


How Human Rights Matter for Datafication
Jason Madeup has been struggling quite a bit with what he has posted on Instagram in particular. The former Meta employee, whistleblower Frances Haugen, gave troubling congressional testimony in 2021 about the way the company regulated user content on its various platforms.84 It seems unfair that there are some people whose content could cross the line with the company's own Community Standards and other "normal" content could be blocked.85 Why are one company's internal rules limiting how we talk to each other and fueling all kinds of debates without public intervention? Jason is also thinking more and more about how he has posted about his kids for their entire lives, without securing their consent.86 More and more he's learning about the harms of social media from films like The Social Dilemma and his own reading.87 Haugen's testimony also spoke to the fact that Meta knew about the negative mental health effects it was having on teenagers. As a result, Jason is seriously considering taking down his Instagram page. He has been sorting through his blog posts to remove some of the more personal references to his kids. However, he also suspects it's too late. Jason wonders what he is revealing about his kids and how it will affect them as adults. The internet, as they say, never forgets.88
As we've seen, anything we make into data is co-created. At the moment, claims over ownership are murkier. Whether it's what we type into our search bar, our text messages, our activity with our devices (and their embedded tracking sensors), or our public activity on a forum like Twitter, all of them are co-created data. Although there are attempts to allow people to "take back" data collected about them (such as the regulations set forth in the European Union's General Data Protection Regulation), data about us simply aren't ours in the sense that a vehicle or house are. Legally, the details of ownership depend on the terms and conditions we breeze through when we download an app or start up our device. What happens also depends on whether the data are "identifiably" you or me and therefore also "yours" or "mine."
Data co-creation assails our basic ideas of human dignity and autonomy because we can't choose not to partake in the digital world. Collected data, often unremarkable, describe our lives to the second, and it's hard to imagine how one would avoid things like taking public transit using computerized passes, shopping online, renewing our driver's license through web portals, and using digital technologies for work. Yet data collectors are logging data from these data sources: it's us.
It seems obvious that if someone were physically in the room with us, we'd feel limited, our autonomy challenged, and that person would be taking in a lot of information about our daily activities. Indeed, we have laws about being in someone else's private space. Having someone else constantly pore over our every movement affects more than our autonomy; it violates our sense of dignity. We're apparently not as bothered by data about us being gathered silently by machines. The incursions on autonomy and dignity, however, are the same. They might in fact be amplified, and we need to constantly remind ourselves that whatever we do seems to populate these collections of these sticky data.
Furthermore, we don't have a good way to think about co-creation. Human rights belong to a single person at a time. Group rights may advance collective interest, but the individuals in a particular group benefit from the rights claims. Those rights belong to someone. For example, the purpose of a core human rights document, the Convention on the Elimination of All Forms of Discrimination against Women, seeks to eliminate discrimination against women based on the self-identified social grouping of "women."89 Based on this document, women as a group might experience less discrimination, and women individually will benefit from less discrimination. So how does it work when it's about data sources against data collectors? Data collectors currently hold immense power as the great amalgamators of data about billions of people. They create new categories as data linkages are found. We, device users who supply data willy-nilly, wield little or no power of redress to opt out or find out what's going on. This seems like a case for invoking human rights. At the same time, identifying which rights have been overstepped within the context of co-creation is a major challenge.
Finally, data are effectively forever because we don't have a way to wrangle them back and fully delete or purge them. It's hard to know which data and where they live and travel. But think about our lives. It's the good but also the bad: all our mistakes, our bad choices of words and friends and activities. The forever quality of data means that "our words and deeds may be judged by not only our present peers, but also by our future ones," writes internet scholar Viktor Mayer-Schönberger.90 In his book, Mayer-Schönberger argues that forgetting is not only part of the function of the brain; it's also necessary for individuals and society to function. The human rights question going forward, then, is how to protect our dignity and autonomy from this forever quality. Can we do this treating all individuals equally while considering how human communities can be damaged or improved by the foreverness of data?
Human rights are not the solvent of data's stickiness. That data are sticky is in part their character. Human rights do help us consider how we regulate and respond to the qualities of data stickiness. It means that people have to understand how data change their lives. If there are changes that undermine human dignity and autonomy, states and corporations alike must address them.


Stickiness beyond the State
To date, the majority of global human rights have dealt with how states might harm individuals or groups. Yet as many of my examples in this chapter show, datafication is largely coming not from states but from corporations. That's not to say that governments aren't using data to advance their goals—they are—but rather that companies must be challenged even more than they have been to apply human rights principles in their business practices, including their handling of data. Conceptually there is no reason to apply human rights responsibility solely to states and governments if corporations are threatening them too. Proposals to make business accountable to human rights exist.91 The United Nations established the Global Compact on July 26, 2000, to give businesses resources regarding corporate social responsibility and sustainability, some of which touched on human rights concerns. More recently, the vision for the UN's Protect, Respect, Remedy framework, described in its Guiding Principles on Business and Human Rights, strengthened the demand that businesses respect human rights.92 Yet the onus of responsibility for ensuring and enforcing human rights remains on states. States remain the signatories of the entire framework of legally binding, global human rights standards and law. Even the report's guiding principles are designed to give states greater ability to regulate businesses' human rights practices.
In addition to international institutions that the UN supports, international NGOs provide resources for advocates of businesses' respect of human rights. The London-based Business and Human Rights Resource Centre tracks almost 20,000 companies, in nearly every country in the world, through its digital platform, linking companies by name to news, any allegations of abuse, ongoing disputes and allegations, and mistreatment of human rights activists.93 The Resource Centre supports activists, writes reports, and provides information along big issues such as labor rights, the UN Guiding Principles, and civil rights.94 Both Amnesty International and Human Rights Watch, two of the world's major international human rights NGOs, continue to call for businesses to be held accountable for human rights abuses.95 Access Now sponsors RightsCon, a yearly conference on human rights and technology. Multistakeholder efforts are also underway. The Global Network Initiative (GNI) is a collaboration among businesses, academics, and civil society to develop norms around the freedom of expression and privacy for companies working in information and communications technology.96
Data-intensive and algorithmic technologies, from Ring doorbells to Fitbits to Android and Apple cell phones to cloud computing and social media, are private, corporate platforms. Platforms can be thought of as both the product with which we interact and to which provide data (such as an app) and the entity that has created the product (mostly companies).97 In other words, tech platforms are at once products and services (the interface) and data collectors. Our relationships to platforms are constant, ubiquitous, and often mundane.
It has proven challenging for public entities like states to regulate platforms even as platforms provide services used the world over. These services can be seen, particularly after COVID-19, as essential—they facilitate important communication cheaply, allow countless commercial transactions, and increase life quality. But all the while, they also actively collect data from their users. Together, the platform's features and collection of users' data constitute "governance": they provide rules and terms of the use of their products and services.98 By using corporate platforms, we agree (most of the time) to not only allow the corporations to collect data but also abide by how the corporation's algorithms organize the world. If we use TikTok, for example, our interactions through that platform are herded through the "For You" feed in a parallel way to how Facebook's (formerly News) Feed filters updates.
In terms of our daily human existence, platform regulation is not as straightforward as regulating mining or textile manufacturing, or consumer products such as stoves and cars.99 Corporations manage the proprietary code that undergirds and runs their platforms, effectively empowering them to monitor and regulate everything that takes place on their platforms, using their products and services, and even on users' devices.100 Imagine if a car company not only designed cars and decided paint colors, but also regulated specific drivers' choices for how fast they drove the car, mandating we could drive only on country roads, and the amount of cargo we could put into the trunk.
There isn't an easy way out of platform governance, as governments themselves use these platforms. Excessive surveillance, a core human rights problem, provides an example. When governments want to illegally hack into someone's devices to collect data, they're often using corporate-created products. State surveillance is frequently supplied by private companies. Some products prove exceedingly popular. Research groups, such as the University of Toronto's Citizen Lab, have found that multiple countries have used one company's software, Pegasus,101 to spy on government dissidents and critics.102 Another government function is record keeping and managing bureaucratic structures. Governments run corporate platforms to do this. One important way to manage data and bureaucracies is to use cloud computing services such as the largest one offered by Amazon Web Services (AWS). According to a 2021 news report, some of AWS's priority marketing areas have been the US federal government agencies focused especially on those in the military and intelligence communities. The reporting revealed Amazon's strategy of purposely recruiting former government officials familiar with contract review processes.103



Be a Stakeholder
Our current system is highly skewed in favor of data collectors and not us, the data sources. We, the sources, depend on our personal devices and internet technologies as we move through our days. Data collectors have every incentive to do what they have done: collect more data, run the data through ever more complex algorithms, to do ever more things.104 As the sources of these data, we are the many, and we need to leverage our multitude through individual and collective demands. Before we rush to create new rights, we must evaluate how technology changes autonomy, dignity, equality, and community.
This is all today: we are stakeholders in our present and in our future. Human rights empowered the many disempowered in the twentieth century. Datafication hasn't changed the fact of our need for protecting qualities essential for human life, but it has changed some of the qualities we knew from before and created new things to worry about. Demanding that technology respect our common investment in human life standards and expectations doesn't seem like an unreasonable ask. It's no more unreasonable than demanding that governments not be able to "disappear" political dissidents, or provide equal access to education regardless of sex, or mandate that people work in decent conditions. One demand should be this: give the individuals, the sources of the data that power the world, more input into the process of datafication. We must weigh in.
The risk of not including human rights is a world in which technology permeates our lives (even more than today); it will become ever harder to disengage. Already datafication allows for some kinds of AI to "learn," self-trained on ample amounts of data ("deep learning"). What are the effects of this massive datafication on human beings? Humans are, in some conceptions, treated as afterthoughts, or as beings with intelligence to be surpassed through computers. As leading AI researcher Stuart Russell probes in his book Human Compatible, what if we let machines, and not humans, decide their objectives?105 This is also what philosopher Nick Bostrom posits in Superintelligence, when machines that have surpassed human capability break free from our control and threaten our very existence.106
Where, indeed, would humans go? These macro-philosophical discussions are overwhelming (and a bit terrifying). They're also divorced from lived realities, where every day, we make microdecisions that affect the trajectory of how datafication will persist or change: whether to allow apps to track us, whether we buy something on Amazon.com or in person, whether we text or call someone. We can twist ourselves into knots deciding for ourselves, as individual stakeholders without a sense of what others out there think, do, or affect. But these are not so much decisions for which individuals alone should be responsible; there are, in fact, collective effects that haven't been fully accounted for. Human rights are a globally legitimized vehicle in which to understand and couch our demands, to be stakeholders in this evolving dataverse. Human rights provide the key to shaping datafication into a more human-driven reality.











3
Data Rights

In the 1990s, Spaniard Mario Costeja González had some debt problems that he resolved by auctioning off his property. This auction was advertised in the Spanish daily La Vanguardia, just one of many that the Social Security Department listed. Like so many other newspapers today, La Vanguardia eventually went online.1 Years later, when González performed a search after the digitization of the newspaper, he found the ad again. He wanted it expunged from Google's results. Google refused, so he sued. The case went all the way to the European Court of Justice, which ruled in 2014 that Google had to comply with González's request. Google did, and Google Spain v. AEPD and Mario Costeja González became a landmark case.
After the court ruling, González said, "I've been saying to people, if Google was good before, now it's perfect."2 In making Google perfect for González, the court created what's become known as "the right to be forgotten." González's desire to clear his good name in Google led to a brand-new human right that seemed tailor-made for the internet.
The trouble is, he hasn't been forgotten.3 Because of the importance of his case, the Spanish Data Protection Authority has now determined that he is of public interest.4 He is no longer eligible for the right to be forgotten. While he sent us all down the path of determining what precisely the right to be forgotten entails (we return to this later), he cannot escape the past or the present of digital datafication. González's case shows the "right to be forgotten" is incredibly hard to enforce in the absence of a right to "own" or claim data about ourselves.
Today, the right to be forgotten is being actively exercised. From May 29, 2014, when the right was established through Google Spain, until January 2023, Google has received 1,368,432 requests to delist queries based on someone's name and nearly four times as many requests to delist URLs. Google has to comply "if the links in question are 'inadequate, irrelevant or no longer relevant or excessive,' taking into account public-interest factors" for all European Google results. The company's own count shows it delists URLs roughly half the time in response to queries.5
The right to be forgotten establishes a focal point in the debate around data rights, but it's misleadingly soothing to think about such a right once it has been practiced. As it stands, those who exercise the right to be forgotten are those whose fear about being "found" is greatest. Where the line between public interest and personal interest is being drawn is largely based on who contacts Google. In the end, Google, not us, decides whose information is forgotten. As González's case shows, it's not that easy to know what's relevant to the public interest in advance, but exercising the right to be forgotten assumes we can know.
González's case illustrates the basic conundrum of any conception of data rights. Datafication has created many situations in which data about people are being collected and disseminated. What should we, the sources of those data, have to say about data that have been transmitted? Are data about me or from me, mine? This seems to be a question that is, at least on the face of it, easy to answer. Yet, it turns out, there are lots of ways to think about it, as we will see with the facial recognition technologies.
We don't necessarily need an erase button, or the "right to be forgotten," which we return to later in the chapter. Data about people are assets, but they are a fundamentally different kind of asset because they are co-created. As Shoshana Zuboff writes, Google and other companies have decided to collect "behavioral surplus" about people online as part of their business model, "claim[ing] human experience as free raw material for translation into behavioral data."6 This separation between source and collector lies at the heart of the chasm of understandings around ownership.
Because there are so many people—nearly 8 billion people on the planet in 2020 and counting—who are potential data sources, individuals are lost in the proliferation of data subjects. Each person's activities within the context of life get lost. When we are engaged in our lives moment to moment, whether a fight with a loved one, a job promotion that we proudly tweet and post to LinkedIn, or rushing frantically to meet someone because we're late yet again, these moments feel big. They're vital to us as individual humans as we experience them. But as data points, devoid of the context in which they happen, our human lives get filtered and compressed. Data collectors take that information to generate decontextualized predictions, creating an overwhelming power differential.
A few things stand in the way of forming a robust set of data rights. The most important impediment to data rights is co-creation, that is, there are at least two claimants to the data: sources and collectors. The balance of that ownership remains to be settled, though at the moment, the collectors de facto have the data first in many instances. The right to be forgotten is largely reactive, as it takes place after data creation and does nothing to address the source-collector relationship. Co-creation also describes the way that data about people are often social or created through our relationships with others. This marks yet another way that data creation is beyond individual control. Finally, through algorithmic sorting of data, we are now being put into groups not of our own conscious choosing. Instead, computers sort us into predictive categories to better forecast what we will do, based on what we have done. These groupings are machine generated, which produces hindrances for humans' collective action, not least of which is our capacity to create common identities and norms on which to act.
Second, that data are effectively forever undercuts our abilities to exercise any rights to or over them as property. After all, if we can never verify that they are truly deleted or purged, how can we ever secure claims over ownership of them? And third, data provide information value. However, not all data are the same.
Most of the current regulatory schemes have been about identifiability, which we discuss below, but that is just one way to think about data from people. Some data are about content (for example, the words in the email), and some data reveal metadata (for example, the time the email was sent, from what location, to whom).7 We can focus on the process of data creation and collection, or the use of those data, once collected, and some people favor one view over the other. From a human rights point of view, we can't separate data collection from use, particularly considering how they affect dignity and equality. Just as human dignity provides guardrails against certain activities to preserve our physical integrity—no torture or ill treatment, no medical experimentation, no genocide—we need to decide what types of data we should not collect except under narrow circumstances.8 Part of the problem is deciding how various categories work. Medical data are necessary for many reasons, including the health of the individual from whom data are gathered, but also for research for the betterment of public health. However, what are "medical" data? Current laws typically circumscribe the behavior of doctors and hospitals, for instance, but what about the data collected by your Fitbit on your heartbeats and activity patterns? Are those "medical"? Heartbeat and physical activity are data that make it into your medical records at the doctor's office, but are also being actively stored by technology firms. By extension, we may decide one day that some data should never be processed, just as torture is never to be performed, under any circumstances. While the attitude in Big Data circles is that more data are better, human rights views prioritize the human behind the data. How and what we limit, however, depends on some of the data-related conundrums discussed in this chapter.

Whose Data?
About a week after the infamous blog post, Claire and Jason receive identical WhatsApp messages. The sender's handle is the falsely affable "A Friend." The content of the messages is extremely disturbing, with thumbnails of all kinds of family photos. Some of the photos have been the public face of Jason's brand. Others are pulled from the parents' various social media accounts, including both of their LinkedIn accounts. By far, two sets of photos disturb them most. First, this sender seems to have access to the photos Claire posts on her Facebook page, for which the privacy settings are set to be hidden from public searches, with access allowed only to her friends. And somehow the sender also has access to photos from their private Google Photo album!
Jason and Claire send out yearly holiday mailings with links to their family photos. They haven't been very careful about culling the list of recipients, although they rarely include more casual acquaintances. Yet a quick check of the settings shows that anyone with the link has access to the album. Claire quickly changes the settings now, but both of them realize that there is not really an easy way to figure out who did this. It could be one of their friends or acquaintances, or someone they know, or someone even more removed than that. There is no way to control how people might have screen-grabbed or downloaded their images and shared them with others. And clearly, when someone is motivated to find another person, they can.
"So, Madeup family, you really don't need a Ring to surveil your family's movements. You've made yourselves perfectly visible online," writes "A Friend." "Be aware of yourselves." Coupled with the other trolling, Jason decides to approach the police. The detective assigned to the case raises her brows when meeting with the couple and reviewing the WhatsApp messages. But since there is no threat in the messages, just regurgitations of images that the family has put online, the detective shrugs. "Be careful what you put online," she says brusquely. "You never know who's looking, and you can't take it back."


The Data We Are
As we use our networked, computing devices, much of the data that we emit are both invisible and unknown to us. Until the European Union General Data Protection Regulation (GDPR) came into force in 2018, few of us likely thought much about internet cookies, small text files that are placed on our devices as we browse different websites. Essentially a way to track our movements online, they can be used to improve our experiences (authenticate us or hold our items in a shopping cart) or surveil how we use the web as we go from site to site.9
Entangled in the process of shedding data is our near-automatic tendency to consent to online terms and conditions without paying close attention to the details. Deciphering exactly what data rights we are waiving and which can be collected about us and from us is difficult; the ordinary person is in no position to argue with terms and conditions that run tens of thousands of words when we just want to access our apps.
Early excitement around datafication reflects our tendency to trust numbers or what new media specialist Jose van Dijck calls "dataism."10 As historian Theodore Porter argues in his classic study, quantification lends a sense of precision and objectivity.11 At the same time, the numbers become impersonal, and as we think, the numbers speak for themselves. Numbers, so the saying goes, do not to lie. But we must also recognize that the starkness of numbers and the mathematics associated with them seem to remove humanity, subjectivity, and ambiguity, for this framework leaves out what can't be quantified as well as what can be difficult to quantify.12 And although data can tell us about reality, it's a reality based on what can be quantified, or datafied. Datafication requires decision making about available information. And although we, as individual users, provide the activity that will be datafied, it is others who will make decisions about priorities and thresholds of what material enters the data set. Thus, information about the world becomes standardized once there is a data set because only certain things make it into the data set; other things hereby become irrelevant. Whatever quantification or datafication happens, it does not happen in the source user's hands; instead, the data we trigger are subjected to algorithmic decisions that others make.
In short, data are interpretive. Yet in the age of datafication, they are increasingly becoming stuck onto our identities and stand-ins for who we are to the world.13


The Data Are Valuable for the Wrong Reasons
Our reliance on and faith in numbers has meant that numbers have become tantamount to truth. The first twenty years of this century have ushered in a data renaissance: data are called "the new oil" and data about people "gold mines."14 Data about people are valuable as a profit-generating product, and yet a by-product that needs to be dealt with. We simultaneously label data "digital footprints" and "digital trails," but also "digital dust," "digital detritus," and "digital exhaust."15 The ambivalence in language recognizes the economic value of human capital behind data while devaluing its humanity and severing its tie to the human whose activity "made" the data. It's no surprise that the data "we leave behind" are treated as objects to be traded on the market, like oil.
Big Data enthusiasts tend to stress the speed required to crunch such great amounts of data as well as the speed with which answers can be obtained that weren't possible in earlier decades.16 They might discuss the sources of Big Data and the sheer quantities of information provided to and by Big Data.
Big Data about people has implications for our humanity and our human existence because its computation informs decision making in the most disparate areas of life: legal, political, financial, insurance, employment, and medical determinations—in a nutshell, from the birth to the death of a human life. In this way Big Data harkens back to historical endeavors to measure human existence, such as the insurance industry's quest to figure out how best to predict mortality and then assist with longevity,17 or what kinds of questions are asked in surveying citizens in the national Census.18
Big Data are different from non-Big Data, however. Because of their scale, Big Data must often be analyzed using algorithms that use AI techniques that are essentially speculative processes.19 Insights from Big Data are probabilistic, deriving the likelihood of someone's future behavior from past collected data. Thus, data insights that have become integrated into the business models at companies, such as Google, Amazon, Meta, Apple, and Microsoft, are inherently uncertain.20 In fact, it is known that sometimes AI yields spurious, nonsensical findings.21 Yet when insights from these data are deployed, the consequences for people are quite certain.
One way to think about digital data is to make the connection to our bodies—specifically, DNA. Because of the information that DNA carries about who we are, it is a good physical analogy to data. DNA are strands of invisible data that reveal our personhood at the genetic level. DNA isn't merely about the individual in question but their lineage. In that way, DNA are social (familial, really) in ways that tech-driven devices' data are. For both sorts of data, personhood, particularly our autonomy, is at the center. Who are we? I think an easy answer is that DNA, data, and other kinds of physical markers are mere pieces of the human experience and identity. But as DNA has become a critical way to identify individuals and their families, we can learn a bit from the controversies around its use.


Whose DNA Is It? Mine or Ours?
The advent of companies such as 23andMe upped the number of DNA human samples that exist. 23andMe has sold over 12 million of its little spit sample kits since 2006.22 After sending in a sample, the DNA curious receive detailed reports about their genetic history and tendencies. Those genetic data are clearly extremely valuable beyond party conversation. Pharmaceutical giant GlaxoSmithKline purchased a $300 million stake in 23andMe in 2018 to have access to its data for drug development.23 DNA goes to the very heart of what it means to be human. After all, all of our tissues carry our DNA around, and our DNA is a unique marker of who we are. They are the genetic determinants of who we are as individuals and parts of families beyond just those we know. DNA connects us to many, many other people. So who has the rights to DNA data? Today, we treat data from people simultaneously as valuable and worth collecting and hoarding and as a by-product of the digital life. But right now, those who collect the data have the upper hand. 23andMe has customers consent to medical research and, with an 80 percent opt-in rate, it has a lot of genetic product to sell.24 In 2020, 23andMe sold the rights to a drug for inflammatory diseases that was developed based on customer data.25
In the early twenty-first century, we reject the idea that human beings can ever own other human beings. It is a fundamental human right not to be enslaved, and human rights activists the world over fight for this to be as true in reality as it is in law. But what about pieces of us? Can others own parts of us, or data about our genetics? Already we know that tissues (containing DNA) constitute a gray area in which American courts tend to side with science over human rights.26 But must this be the case? The complicating factor, of course, is that data are effectively forever, whereas most tissues are not.

DNA as a Crime Buster
Joseph James DeAngelo was a police officer in a number of inland California cities in the 1970s.27 He lost his job in 1979 for shoplifting a can of dog repellent and a hammer from a Sacramento-area Pay 'N' Save. Until 2018, no one knew he was also the Golden State Killer, a prolific burglar, serial killer, and rapist whose reign of terror crossed eleven counties and included twenty-six kidnappings and murders to which he eventually pled guilty. He remained at large for four decades, working at and eventually retiring from a supermarket distribution warehouse. The Golden State Killer's cases remained cold until 2018, when investigators used DNA from rape kits to construct a DNA profile. Although the use of DNA in criminal investigations emerged in the 1980s, the Golden State Killer case was a high-profile cold case that was solved using publicly accessible, popular platforms.28 The platforms were consumer-facing brands with millions of members. The story first told to the public was that investigators had uploaded the killer's DNA profile to the open-source genealogical platform, GEDMatch, where they were able to match to the killer's distant relatives. It later came to light that police also used a couple of other consumer-facing genealogical services, FamilyTreeDNA and MyHeritage, to generate hits. The MyHeritage site provided a hit with a close relative. Law enforcement also used Ancestry.com to try to close in on the killer's identity. These genealogical sources provided authorities ways to narrow down the pool of suspects.
In the aftermath of the Golden State Killer's capture, questions arose around companies such as 23andMe and Ancestry because they sell DNA testing kits and store the genetic data of their tens of millions of customers. As of this writing, both companies have stressed that they do not work with law enforcement unless legally forced to so.29 But that's not true of all companies in this area. FamilyTreeDNA has even used its voluntary cooperation with law enforcement as a marketing tool. GEDMatch, which initially disallowed law enforcement access, began allowing scaled-down access to its database.30 The controversy over these services centers on the fact that their gathering of DNA and other genealogical data are not only or necessarily what people voluntarily upload themselves. It's the fact that data uploaded by an unassuming person with curiosities about their lineage could generate fallout for others related to them. To solve violent criminal cases, this may not be the worst thing. DeAngelo's case was not the first, and it will likely not be the last. Still, Maryland and Montana recently passed laws restricting the use of genetic genealogy in criminal investigations.31
The interesting and problematic part of using genealogy to catch evasive violent criminals is how much information DNA can tell us. These data reveal information that is not just about ourselves, but about all those who are genetically related to us. While others have concerns about individual privacy or even catching the wrong people in the dragnet, these are not the ones I'll focus on here.32 Instead, what's troubling about genetic genealogy and DNA data is that the data are gathered about individuals but always have implications for a collective.
That DNA tells us about both collective and individual sources is something that can be seen positively. It has offered countless amateur genealogists the chance to explore their roots. For the tens of millions of people who have uploaded their genetic and familial data or purchased DNA testing kits for their friends and family, this is exciting. But for the many more of us who haven't done so, we may already be caught up in the efforts of the family genealogical enthusiast. Far more people unknowingly have genetic snippets about themselves available online or stored by private companies. Distressingly, we may have no idea that these data exist. These snippets tell others something about us.
This duality affects our own enjoyment of autonomy and dignity. A relative can make the choice to submit their DNA sample to a database or via a mail-in kit for evaluation or storage. Or a relative can be in a doctor's office, submitting specimens bound for a lab, which may eventually end up in a DNA analysis lab. Making such DNA public in any way has implications for us, even if we never submitted our own specimens. This gives "we don't choose our own families" another level of meaning.
In 2013, a group of researchers were able to identify five out of ten male research participants and their families using publicly available genome information in the international 1000 Genomes Project, which holds commercially available genealogical data, research records, and public documents (like obituaries).33 The authors write, "This study shows that data release, even of a few markers, from one person can spread through deep genealogical ties and lead to the identification of another person who might have no acquaintance with the person who released his genetic data."34
These insights into DNA offer us something else. Although DNA make us and give us our genetic template from which we build out our lives, we don't "own" it. We can't, for a number of reasons. The most important one is that our DNA ties us to others, so if we were to own it, we would also be owning pieces of many others. As much as DNA is individual, it's also collective. The same could be said of data about DNA: it's also co-created by the individual who swabs and the company that extracts and sequences the DNA.



DNA and Digital Data Aren't That Different
That DNA has individual and collective implications is true of datafication too. The common assumption is that data are "personal" when they can be linked to an identifiable person.35 Typically this means that governments protect demographic data like phone numbers, government identification numbers, addresses, race, and religion, as well as a variety of biometric data. Google recently decided to let people choose to hide their contact details ("personal data") from search results, which on the surface seems a step in the right direction.36 However, as discussed in chapter 2, the term personal data is increasingly inadequate as the many details of our behavior can be identified through algorithmic analysis.37 That is, if enough of our behaviors become data, machine learning algorithms can identify in a "superhuman" way patterns to predict what we might do next and what we may have done in the past.38
Another example of the collective characteristic of data is that California's Consumer Privacy Act explicitly recognizes that consumer information includes households, not just individual persons.39 Even so, data's stories go beyond our homes. In many cases, we don't have a choice about what others choose to do. We most likely don't even know about it. Before it became Meta, this was how Facebook's Find Friends program worked. Unbeknown to Facebook users, when they shared their email login details through the Find Friends feature (namely, Hotmail, Gmail, and Yahoo!) in Facebook, they were allowing Facebook to scrape their email contacts. It would then either send friend invitations to the contacts already on Facebook or invite those not on Facebook to join the network.40 Other social media, such as LinkedIn, also rely on users to generate new users by asking for people to volunteer their contacts' information. Unless we are complete electronic hermits, it's an impossible task to prevent others from sharing our contact information, or even personal information, once they have it.
Beyond social media and DNA, there are other ways we are being tracked by who know us most. In fact, many of the tools we use to make life easier or better beyond social media create data not just about us but also about our relationships.41 There are many apps that govern "relational big data," as sociologist Karen E. C. Levy explains.42 She writes that Big Data "is big because of the depth to which it has come to pervade our personal connections to one another."43 When we think about social interactions, we're talking about collectives. In some cases, datafication is about tracking the activities and behaviors of our intimates. Datafication gives us access to our spouses, love interests, sexual partners, family members, and children. mSpy, for example, allows parents to track the phone calls, texts, social media activity, web history, and location of their children's mobile devices.44 Pocket-sized (or smaller) tracking devices can help us find our keys or thieves who steal cars.45 To assess how difficult such tracking devices are to find, New York Times reporter Kashmir Hill experimented with several devices, including Apple's AirTags and Tiles, and followed (with permission) her husband's whereabouts in New York, with stunning effectiveness.46
Still other ways we interact or work reveal collective data. Mint, Yelp, Foursquare, and Grindr can reveal the location of users as well as their preferences. We can consider the data collected and revealed by apps such as Google Docs: they reveal which collaborators are online and making what changes. Amazon's pervasive and draconian surveillance of its workers, from item scanners to location monitoring, have driven employees to pee in water bottles,47 as well as encouraged locations to unionize.48 Fertility and pregnancy apps such as Ovia or WebMD's pregnancy app can be tremendously informative for those trying to get pregnant and going through with pregnancy, but they also track the most intimate and biological details, and not just the mother's; these apps track the fetus and its development, including the number of kicks the mother feels.
Although Meta has never confirmed it, Facebook has been accused of maintaining shadow profiles of potential users who aren't registered—users who can be constructed through inferences drawn from existing user data and contacts or from purchasing data from data brokers.49 Thus, if someone knows a Facebook user (and considering the number of Facebook accounts globally, who doesn't?), they're in Meta's database, even if they don't have their own accounts.
Our involvement with datafication technologies reveals that we do not have to be users to be swept up in the process of datafication.50 This is part of the reason why thinking about data about us as "mine" is fraught once we get beyond feeling aghast that we don't "own" data about us. It would be exceedingly difficult to own data about us because we don't always know when we produce them (through passive collection by our devices) or when others are producing them for us (posting photos, submitting swabs to DNA companies).


Machines Are Making Our Collectives
Rights perspectives don't provide a good way to think about the collective because our existing remedies still require the individual(s) to claim to data. When hip-hop superstar will.i.am writes in a widely read Economist piece that "personal data needs to be regarded as a human right," his solution is individual. For instance, he argues for the framing of data as property: "The ability for people to own and control their data should be considered a central human value. The data itself should be treated like property and people should be fairly compensated for it."51 In contrast, some researchers have argued that data, in fact, cannot be owned in the sense that we own cars or even ideas.52 Claims of personal ownership of "my data" obscure the fact that there are collective consequences and sources of data taken from individuals. Individuals' claims to data do not square well with data's realities.
It is seductive to think we can get our due by being compensated for data about us.53 There are companies ready to help you claim "your data" through payment schemes. Mine, for example, is the name of a software application that helps us detect our digital footprints to remove data from certain applications we don't want to use any longer—and it claims that its technology can be used to give people control over how their data are shared.54 Other companies, such as Reklaim (formerly Killi), want us to sell data about ourselves by controlling access to it.55 But the numbers they give are small at the individual level. In a 2022 overview of such US-based companies, different services reported monthly user earnings in the range of $5 to $15 a month.56 Compare that with the value of the consumer data market at about $400 billion per year.57 It's not nothing, but it's also not exactly a means of supporting oneself in many places. Some would be unsurprised, such as economists who argue that individual-level oversharing leads to underpricing of data: there are simply too many data for it to be worth a whole lot per unit or even per person.58 Data are valuable only in the aggregate; each of us, sad to say, doesn't matter much in the grand scheme of things. When the pool of humans potentially producing data is over 7 billion individuals, one is just not a lot. Individually, we don't have much leverage.59 So then the marginal benefit of being paid for data is also comparatively quite small.
That's a strange juxtaposition—to at once see will.i.am's clear and powerful statement about human rights and data, and yet know that each individual person's data doesn't matter in the grand scheme of things. How can this be true? How do we reconcile these notions? The problem is that it's not the ownership of data that's a human rights issue. What will.i.am and other market-oriented answers focus on distracts us from the problems of human dignity and autonomy. One human rights problem with regard to data rights is that harmful consequences from datafication are borne by individuals, or groups of "like" individuals. Data generate predictions that are treated like realities, leading to algorithmic policing or denials of economic opportunities that have toxic effects on people who get frisked, go to jail, or have to enter into vicious cycles with check cashing places in order to eat or make rent.
More to the point, treating data rights as ownership is a reactive strategy of chasing data that have already been co-created. That such comprehensive data about our daily activities exists is the problem. Emerging technologies researcher Jathan Sadowski writes, "Many common practices of data collection should actually be treated as a form of theft that I call data appropriation."60 "Appropriation" is a nice way of saying stealing. Our current recourse to appropriation is only to opt out (which isn't realistic for most of us), edit errors, or remove data we don't like if we live under certain jurisdictions (discussed below). We're forced to be reactive to data that have been created rather than proactively directing that data not be collected because we're not stakeholders in the creation of those data.

Who Are We? What Are We Fighting For?
The data conundrum is not just about individual choice or autonomy in terms of consent, even if that's what companies giving us ownership of data about us want us to think. The very ways that data about persons, amassed as Big Data analyzed by algorithms, are being sorted and used creates other collectives that are beyond our control and our awareness. We're subjects, and perhaps we can own some part of "our" data, but that we're still not stakeholders in the use and processing of those data. This exclusion affects our dignity as persons.
We don't yet have a good way to think about these computer-generated collectives, as rights thinkers have pointed out.61 Indeed, we often conceptualize rights as pertaining to individual or self-selected groups able to exercise those rights. As individuals, we may belong to a church, or a book club, or a local chapter of environmental activists. These groupings involve some shared activity, belief system, or values. When collectives are machine-selected by seeing patterns we may not see, these collectives don't have the same motivation for acting. We probably don't even know about these collectives.
Harms can occur when we are aggregated into machine-selected groups. To start, as others have covered, we may be denied services and access without knowing why, and there may be no explanation, as the algorithms often make decisions in black boxes of multiple layers of machine learning. This means a lack of explanation and accountability.62 Our autonomy may be limited without a clear, knowable reason.
Even more important, since we have been put into these groups by machines (or people writing code), with access to huge amounts of data, we are likely unaware of these groupings.63 We can't coherently question and resist matters we are not privy to. A lack of group consciousness prevents us from exercising our freedom of association and right to form unions. It keeps us from knowing how to organize.
What I'm describing is a different kind of collective action problem from the kind typically faced in political science, where we presume common interests but an uneven distribution of costs for action. The typical political science problem is: why do or don't they act? In this case, the collective is unaware. How can we act collectively if we don't know exactly what collective(s) we're part of? In other words, who is the collective? Legal scholar Alessandro Mantelero writes that we must shift our thinking to clusters of individuals who have diffuse interests.64 Yet diffuse interests do not coalesce into clear demands. Research on how social movements result in political change, including my own, emphasizes the need for clear asks.65 The creation of "ungroupy" groups leads to common experiences without common vocabulary or recourse.
From this view, what makes AI-created collectives so difficult to navigate isn't really about algorithmic transparency or failed content moderation, two other ways that AI has been attacked for creating social harms. What is harmful is that AI-created collectives are not directly human generated, therefore making it hard for humans to act socially and politically. The politics of social change and collective action have always presumed a basic community that recognizes what it wants and struggles, successfully or unsuccessfully, for those desired outcomes. Communities form social ties and the basis for belonging. When we don't know what we're fighting for—when the human community isn't created by humans but pooled and sorted by machine—we have created a power imbalance that is different from most others we've known before.


Mismatched Regulation
It's no wonder that human rights frameworks are struggling to adapt: these frameworks presume humans harming other humans. Furthermore, our regulatory frameworks for data rights are largely based on conceptions of individual rights and on privacy in particular. Globally, the UN has had a number of special rapporteurs investigating how datafication inhibits human rights, including its effects on the promotion and protection of the right to freedom of opinion and expression and privacy. The UN General Assembly and the High Commissioner of Human Rights as well as the UN Privacy Policy Group have published reports on how data about people are used (particularly by states) and how human rights can be threatened by those uses.66 By and large, this work is focused on data about individuals.
To date, the most comprehensive and established regulation on the use of data about people is the European Union's General Data Protection Regulation (GDPR, 2016). While the framework articulates a whole slew of rights that individuals living in the EU now have over data collected about them, the framework revolves around an identified or identifiable data subject. The GDPR has inspired a host of similar national regulations in countries including Brazil and Nigeria (the so-called Brussels effect).67 California, too, has introduced its own legal framework to protect consumer privacy, the California Consumer Privacy Act (CCPA, 2020),68 which seeks to protect individual or household-level consumers.69 Finally, China has passed the Personal Information Protection Law (PIPL, 2021). All three of these regulations set up guardrails with regard to collecting data, from collecting to processing and storing data, to erasing or "porting" and transferring data. The CCPA, GDPR, and PIPL differ with regard to the types of rights one has regarding data about oneself (or one's household).
Regardless of the details, the same blind spot runs through all three frameworks: the rights are attached to either identifiable individuals ("natural persons") or households. The GDPR first raised issues about how information "relates to" individuals.70 How identifiable an individual or household remains key to the way these laws are written; they do not regulate anonymized data. Legal scholars have already called into question the applicability of the GDPR to Big Data, which is often not directly tied to individuals or the data they supply through their actions. Instead, the usefulness of Big Data and the algorithms used on them are about inferences in "identifiability" changes with technological advancement.71
What happens if the data can affect us without "us" being in a data set, or if we cannot identifiably make the link in order to pursue the rights given to use in the GDPR or CCPA? According to both the CCPA and the GDPR, anonymized data are generally not covered by their framework. It might be a practical choice to allow anonymized data—after all, like medical research, advances in AI for human purposes require some level of collection of data from people. Still, identifiability remains a serious concern of privacy advocates.
Technically, it's also a challenge for engineering to meet the standards of the GDPR. Computer scientists have drawn our attention to the gap between mathematical techniques used to achieve privacy and the conceptions of privacy legal scholars and social scientists care about.72 Given the amount of data that exist and overlapping data sets that contain bits about everyone, bridging the gap between data set anonymity and actually remaining anonymous present technical and social challenges.



Co-Creation
This chapter has established that co-creation makes it exceedingly challenging to create a right around data that would be akin to rights over property or self-determination. The dual complications of the dependency between data collectors and data sources, and the collective nature of datafication belie an easy solution.
Data are valuable because they contain information that is useful to someone. This means that there are interests at play and normative values embedded in any data. Someone has to decide which observations to take from the world and "cook" the data to become usable and interpretable. The decisions to make the data already bake norms and interests into them, well before they are even analyzed. Perhaps the value is expedience: activities that are easy to datafy. How many steps we take in a day, for example, or how many hours a day we sleep, or the number of times we look up the address of the restaurant before arriving can all be easily collected by our mobile phones. But collecting those data, and not others about related activities (the length of our strides, whether we're side sleepers or stomach sleepers, or the type of restaurant to which we're headed), means drawing conclusions from those data necessarily leaves relevant information out.
Seemingly technical decisions embed interests and values. These choices have important political and social consequences. In the aftermath of the US Supreme Court decision Dobbs v. Jackson Women's Health Organization, which overturned a woman's right to an abortion established in Roe v. Wade, a Nebraska teenager and her mother were arrested after the teenager performed a self-abortion in violation of state law. A large part of the evidence was obtained when police looked through their Facebook message history. Because Messenger's encryption is cumbersome and not the default setting, most people don't turn this feature on.73 Wired reporter Albert Fox Cahn writes, "Right now, the overwhelming majority of messages are unencrypted, visible to Meta staff and anyone with a valid warrant. And that's by design—Facebook built its message encryption feature to fail."
There is a fine balance between preserving our autonomy and dignity when it comes to pieces of us, whether DNA or digital, and the interests of society at large to build knowledge. It sounds good in theory but not when the pursuit of knowledge runs through you.
Perhaps, then, it is not about owning but about controlling access. It also takes a lot of work to control access—by regulators, corporations, individuals, and social groups. Computer scientists have long worked on these ideas, advancing protective measures such as encryption to protect content and differential privacy to delink individuals from aggregate data. There is a growing literature on data trusts and other kinds of intermediaries who can broker data use between collectors and sources.74 These efforts get at the heart of righting the imbalance of power between data collectors and sources. We discuss how they fall short in taking into full account what "co-creation" means for ownership and access in chapter 8.
Perhaps we're focused on the wrong aspect of data stickiness. Instead of the co-creation part, which seems unresolvable without some hard thinking about what kinds of dignity and autonomy we're willing to give up, what about the effectively forever part? The GDPR and other legislation around data start the conversation with the right to erase. Can we effectively force forgetting? But who will decide what should be forgotten and what should stay in the collective public memory?


Can We Just Forget?
Alexi McCammond was named editor-in-chief of Teen Vogue in 2021 at twenty-seven years old. Before that, she was recognized for her accomplishments as a political reporter for Axios, including receiving an award in 2019 as emerging journalist of the year by the National Association of Black Journalists and making Forbes's 2020 "30 under 30" list. But after she was named for the job at Teen Vogue, some tweets from 2011 resurfaced. The tweets that were subject to the most scrutiny were anti-Asian.75 Although she had apologized for those tweets in 2019, more than twenty magazine staffers wrote to the management of publisher Condé Nast to condemn McCammond's posts. She apologized again, on Twitter and elsewhere. On March 18, less than a week before she was to start her job at the magazine, McCammond resigned.76
Although McCammond deleted the posts in question in 2019, someone had saved the screenshots and reposted them. The stickiness of data means that they are around forever. McCammond's career success also came at a peak of awareness around anti-Asian racism.77 Her anti-Asian tweets became particularly relevant when increased hate crimes and violence targeting Asians were encouraged by Trump administration rhetoric. Anti-Asian hate crimes had increased by 150 percent since the pandemic began in March 2020.78 The week McCammond resigned, six of eight victims of a shooting spree in Atlanta were Asian women.79
We should keep in mind, however, the other lesson that McCammond's story teaches us: we don't all face the same consequences of the durability of data. The unearthing of 2005 Access Hollywood footage featuring Donald Trump and Billy Bush's lewd conversation did nothing to dash Trump's eventual 2016 election as US president. Similarly, several images and videos of Prime Minister Justin Trudeau of Canada dressed in blackface did not stop him from leading his party to a third term in 2021. One can't argue that misogyny and racism weren't relevant social issues during their campaigns. In both of these cases, the internet remembered these past misdeeds, but the public overlooked them. Trudeau and Trump got what they (electorally) wanted.
That data are effectively forever and that they are linked allow us to unearth everyone's personal histories. Perhaps this is fair because it allows others to assess fully who we are. Perhaps it is right that a person's past can be revealed. Yet what function does calling out serve if those callouts are unevenly distributed? Who has lived a life without mistakes?
Perhaps what we need is a sense of forgiveness for one another. We're all fools with datafication. What we all need is empathy and a willingness to see past mistakes. But there is no "right" to forgiveness. You can't be entitled to pardons. There can be a norm of forgiveness that asks that we consider the humanity behind our mistakes. Norms emerge when enough of us agree to the same standards of behavior.
The race to either own or forget is understandable. If we "owned" the data we co-produced, then we could deny access to those who seek to judge us or use us to analyze the human species. We could be compensated for the value those data bring to society through the control of access. And we could be recognized for those contributions, however small. But an ownership claim is untenable in the current ways in which data are co-created.
Losing data happened before we evolved to create oral language. It happened before we harnessed written language. And it happened before digital data. But does this mean we have a right to be forgotten as individuals? A right gives us an entitlement that must be considered in light of the countervailing public interest in knowing. If being forgotten is as essential to life as water and gainful employment, then a right makes sense.
What the supporters of a right to be forgotten seem to really want is a shift in our collective understanding of how our past begets the present. The way that the right to be forgotten has played out has emulated data ownership conversations in that the subject of the data has a claim to erase or renumeration. The right to be forgotten and the right to own data both use human rights language in an effort to retrieve some idea about human experience that has been lost with datafication. Both claims root themselves in ideas of property and ownership that apply to other goods, but not as well with sticky data. Stickiness, especially the co-creation aspect, has changed something about the human experience that deserves a more thoughtful application of the human rights apparatus. When we say "data rights," how does it affect dignity, equality, and community? We don't have to own something to enjoy dignity or exercise autonomy.
We will still need data rights, appropriately framed and understood, with an understanding of data stickiness. Data stickiness means that any data created has the potential to be around forever. Thus, the relevant issue isn't rights over data, but rights against data collection. There will be hard choices made about the types of data we gather, because not all data are the same. Decisions will be made about how data will be processed and leveraged for social, political, and economic gain. We can live in a world where respecting the normative foundations of human rights does not clash with norms surrounding knowledge and understanding.











4
Is Your Face Yours?


So if you have a face, you have a place in this conversation from a standpoint of civil rights.
—Joy Buolamwini, computer scientist1

Jack doesn't get a lot of interesting email. His parents set up a Gmail account for him last year as a way for his family to email him. His grandparents, aunts, and uncles include him and Corey on family messages. Mostly he uses it to sign up for accounts like TikTok and Twitch.
A few months before the blowup with his dad's blog, Jack gets a surprise email from someone identified as "L.R.": "This message may come surprisingly to you, and I apologize in advance for any disturbance the contents might cause. But you are my son. I have found you through the use of facial recognition technology. I know you have been adopted into a loving family, but the truth is, this adoption was not done according to the law. What you may have been told about your origin story is incorrect, and I would like to correct it." The email goes on about the author, including the Minneapolis neighborhood in which he lived, why he was trying contact Jack, how he came to be aware of him, and ended with the hope of staying in touch. The email doesn't indicate that its author knows Jack has an identical twin brother, which is curious. Still, Jack feels that the email could be legitimate. He has combed the internet for the use of facial recognition in finding long-lost family members and saw that it was indeed possible.2 He feels pretty creeped out, though, that L.R. admitted to cyberstalking him after seeing some reporting about his recent math medal accomplishments.
Jack doesn't mention the email to Corey, because he knows Corey would be upset and talk to their mother immediately. He and Corey basically have the same face, so part of him feels very wrong for not being open about this email. He feels both guilty and angry at his parents. Everything he knows about himself before being adopted has been through Claire and Jason. Now he wonders if they have lied. They tried for years to have children, they've explained, and then worked through an adoption agency specializing in local adoptions. That they ended up with twins was all the better, Claire and Jason say. Then this stranger's email appears. Jack feels completely unsure and a bit unmoored. How come his face was somehow linked to his personal email? Coupled with Jason's blog fiasco, Jack didn't know what to think about his dad's displaying their childhood photos. He's pulling away from his family more and more, but they've been so caught up with other things that it feels as if they don't even notice. Probably for the better, he thinks.

*
Our faces are important: they identify us socially and culturally to others, and they play a crucial role in our self-identity. They provide a sense of fellow humanity, helping us differentiate ourselves from others. Facial recognition technology (FRT) threaten these assumed and unspoken understandings by creating data from our facial features. This chapter explains how creating data from faces allows "our faces" to be used in ways beyond what we can see or control, and what it means to have our faces get away from us, from a human rights perspective.
FRT works by capturing any number of facial characteristics, converting those into numbers and creating a matrix of facial data, which is then added and compared to a database of other facial data.3 FRT is not simple photography because an image of a face is not technically what is being captured. FRT records data about various features of our faces, which algorithms analyze to predict a person's likely gender, race, and age. As we'll see in this chapter, FRTs are even more problematically applied to predict psychological states and sexual identities.4
We know a face isn't just data, but that is how FRT translates faces. Different FRTs use different data, depending on their creators' purposes. FRTs are increasingly used by law enforcement, government agencies, and corporations to identify us, track us, and profile us to make predictions about who we are, what we like, and what we'll do, typically without our knowledge.
In a world where faces can be made into data, are our faces truly ours? The answer, as it turns out, is not straightforward. FRT creates data about images of our faces. Like photographs of us, these data are removed from our physical bodies. Once taken, those data are effectively forever. Furthermore, facial data are linked to other databases. Given how facial data are often used in conjunction with other forms of data about people to make predictions, it is not difficult to imagine a future in which companies merge databases of faces to develop new products and services, as well as to serve the needs of states and local governments. Companies can use FRT to serve their own pursuits as well. It's not difficult to imagine because it's already here. Retailers such as Albertsons and Macy's have deployed such technology to track customers as they walk through stores to offer them real time deals. Some of these stores also use FRT to identify potential shoplifters.5
If that thought strikes you uncomfortably, you're not alone. We are being treated as though our faces are free for the taking because of technological advances.
Many human rights activists and academics have furiously criticized FRT. Human rights have been invoked in two primary ways in this context. First, there are many people who argue that FRT violates privacy because it is used for surveillance purposes. Linking facial recognition data to other kinds of biometric data makes it increasingly easy to identify individuals, thereby threatening that individual's liberty and security. Some worry about the use and abuse of surveillance, especially by the state, as a consequence of this identifiability, which may expand by providing additional means of monitoring people at a distance.6 Furthermore, the technology is rarely deployed neutrally. It's not startling that these data have and will be used to target groups that are already marginalized, such as racialized, religious, and sexual minorities. Remedies are sorely needed. Second, facial data raise property concerns because they are co-created. In the early days of amateur photography around the turn of the twentieth century, the ownership of facial images raised similar questions about whether photographers or their photographic subjects owned the representation of their faces. Eventually these questions gave way to ones over rights to personal image and "the right to personality" in the US context. These concepts reflect a focus on the extent to which property rights might help us decide how others might take or use data about us.
However, there are ways in which human rights frameworks could be used outside of current interpretations. Because our faces are at stake, the human rights violation goes beyond general streams of thought. FRT is personal because the face is so personal (and also so mundane) that it is used as an easy way to identify us and differentiate ourselves in society.7 Here it is also worth pointing out that like all other data-intensive systems, FRT is designed to create order. FRT does this by distinguishing and classifying facial characteristics. It can be used to find facial patterns in sample sizes of millions or billions to generate predictions. They are discriminatory technologies, by design and, often, intent. FRT is used to make predictions about our personalities, our social status, and what wrongdoing we might possibly commit—all from data about human faces.
In some ways, FRT does what we already do as human beings, but it scales that ability algorithmically, seeing patterns in ways we may not even see or realize explicitly. But unlike humans who suffer the cognitive limitations of imperfect memory and association, algorithms do not. Instead, human shortcomings enter algorithms at conception, from algorithm designers' biases to how they select training data to make the algorithms functional. We define the kinds of associations that can be made with the algorithms we write.8 FRT, like other AI, can become a hyperdiscriminatory machine if we don't design the systems with human rights safeguards.
It is this automated discrimination that creates an inherent friction between human rights and data-intensive technologies. Where FRT inherently discriminates, human rights are meant to fight discrimination. As much as the human rights framework recognizes differences between countries and cultures, it aspires to universality. In affirming the differences between individuals and groups, a human rights framework also demands that we treat every human being as entitled to the same minimum life experience as much as we can. The freedom to choose our partners in marriage, the freedom from extreme poverty, and the freedom from torture are all important to the common expectations of human experience.
FRT assails our current conception of personhood, at the very least affecting our autonomy and dignity. Its potential to use facial data as tools of surveillance attacks the ideas of equality and community. As much as we might (hope to) gain from technologies like FRT, they come at the cost of eroding our sense of personhood. I want to find ways to articulate what that loss is from the position of autonomy and dignity. Focusing on human rights issues such as privacy and property aren't the best ways to think about facial data. For one, privacy, when it comes to our faces or facial data, may seem a bit out of place because our faces are meant to be seen; they are precisely not private.9 They crystallize our public presence. Privacy also gets confusing as a metric when we don't distinguish between involuntary provision and uses of facial data, such as police accessing facial data in their work, from ostensibly voluntary collection and analysis of facial data, as when we became part of Facebook's DeepFace training when we uploaded photos before the company shut it down in 2021.10
Second, the existence of FRT is not inherently a threat to humanity or human rights, but the uses of FRT can be dangerous, harmful, or injurious. FRT has been used not just to identify Jane Doe but also to make predictions about who that person is. Is Jane Doe homosexual? Is she likely to shoplift? Would she be a good customer service representative? Does she suffer from depression or anxiety? Is she cheating on the exam? These uses mean our faces can reveal dimensions of our personhood that are problematic, if not downright discredited. Our faces are an important part of our humanity, but they certainly aren't the only thing. Yet FRT reduces us to our facial data. This harkens back to physiognomy's role in the discredited pseudoscience of drawing inferences from facial and other physical features in earlier times. Beyond being scientifically and socially unsound, such logics can and have been used to justify various forms of discrimination.11 This is the type of discrimination human rights are designed to resist and demand remedy to.

Contextualizing FRT
"Woody Bledsoe" isn't a household name—yet—but his work in the 1960s on FRT makes him "pretty much the Steve Jobs of facial recognition."12 In 1995, Bledsoe burned a bunch of his files in his driveway, so we may never know the full story. His little-known work was foundational to the development of FRT. His company, Panoramic Research Incorporated, was involved in CIA-led projects, such as Project MK-Ultra, the mind control program. It also received funding through a CIA front company, King-Hurley Research Group, and the US Department of Defense in what became some of the earliest forays into FRT. Bledsoe and his collaborators created a series of facial measurements of various characteristics—relationships between landmark features eyes, ears, nose, eyebrows, lips, but also coordinates of features such as corners of the eyes and mouth—to make faces into data.13 Building on methods from text recognition, Panoramic sought to figure out a way to take a three-dimensional object—a face, for example—and "grid" it in two dimensions. In 1963, when the project began, there were many challenges, including how to digitize photographs and work with computers that were tens of thousands of times less powerful than our modern smartphones, which made their goal of identifying ten faces quite ambitious.
Their work paved the possibilities for a world of ubiquitous FRT. Several advancements by Japanese researchers made inroads into automating the collection of facial features possible.14 Modern computing and digital facial data taken from billions of selfies and videos have made it possible for our faces to become "faceprints" (or face templates) by creating specific measurements using photos. FRTs can detect whether faces are present and predict facial attribute classifications from images, including race, gender, or age. From faceprints, FRT can make predictions about whose face is being evaluated, as in, "Does this face match any other faces in a database?" These identification predictions are currently causing plenty of furor. This might mean comparing two faces to see if they are the same (known as one-to-one matching or one-to-one comparison) or finding a match in a larger database of facial data (known by many names but focused on the idea of one-to-many matching). Predictions can also be inferential, as in, "Does this face's expression reveal any attitudes, emotions or characteristics?"15 It's important to emphasize that FRT algorithms calculate a probability of a match of one faceprint against its database of other faceprints.16
With probability, there is always the chance of error. Thus, FRT is a technology that ought to be treated as such—a prediction drawn from our faces. Our faces are just part of who we are as humans, but FRT's face matching capabilities give users the power to infer things about someone from face matches.


Have We Ever Owned Our Faces?
The "taking" of our faces is not a new problem. Photography has been around as an art form and technology since the daguerreotypes of the 1840s. At once a novel and exciting technology for creating memories and connections across time, photography also presented new challenges. Sitting for portraits one day, people might unexpectedly find their faces emblazoned on products and advertisements the next.17 In the American context, photographs inspired plenty of debate about ownership and privacy. Frances Folsom was twenty-seven years younger than her husband. When she married President Grover Cleveland in 1886, she became the youngest and one of the most popular first ladies in American history.18 Grover was also the only US president ever to marry in the White House. There was a lot of excitement around the marriage. Frances Cleveland inspired songs, hairstyles, and baby names. Her outfit was covered carefully in all the newspapers at the time. By contemporaneous accounts, Frances was beautiful, but her beauty became a problem. Eager to cash in on her looks and recognizability, entrepreneurs emblazoned her face on advertisements and products, all without her knowledge or permission.19 Her experiences, among those of others, gave rise to the proposal of an 1888 bill in the US House of Representatives to "protect ladies."20
Still, at the time, there was not much agreement about whether one "owned" the rights to one's own face. Contemporaneous accounts varied regarding whether the unauthorized use of Cleveland's face was acceptable. The Albany Evening Journal opposed the 1888 bill to protect ladies, stating that first ladies were public "property," and thus disseminating her face widely was complimentary.21 In contrast, the Sacramento Daily Record-Union declared, "Evidently the right to reproduce in portraiture one's features is a personal one and belongs to the owner of the face."22
Copyright law in the United States and United Kingdom initially sided with photographers, granting copyright protections to the artist over the reproduction and sale of photographs. Over time, however, it became clear that the subjects of the photographs could decide when prints could be made and object to unauthorized uses of their faces for profit.23 More challenges came after George Eastman's invention of the portable Kodak camera, the Brownie, in the early 1900s. The Brownie made amateur photography possible. Previously a practice confined to formal studios with cumbersome equipment that required specialized knowledge and long sitting times, Kodak unleashed the power for everyone to take photographs. This possibility raised the question of how people's faces could be taken as images without their knowledge or permission. "Kodak fiends" raised questions of privacy around restricting the ability to freely photograph others without their consent.24
Lawyer Samuel D. Warren and future Supreme Court Justice Louis Brandeis tried to go beyond ownership when they wrote the formative legal paper, "The Right to Privacy," in 1890. Importantly for legal doctrine, they argued that law allowed for "the right to be let alone" or privacy. For Warren and Brandeis, privacy was not just about property, but about injury and emotional harm. In some ways, however, it was a knee-jerk reaction against the ability of new technologies25 "to take pictures surreptitiously,"26 how they create a gossip problem,27 and why privacy should be seen as "rights against the world."28
Nonetheless, Warren and Brandeis struck out an important path. Emotional harm was the basis of Abigail Roberson's lawsuit against Robertson Folding Box Company in 1902. After consenting to portraits with a local photographer, Roberson suffered "severe nervous shock" when she saw her face on a package of Franklin Mills flour purchased by her neighbor. Her situation was not uncommon for Americans at the turn of the twentieth century.29 After winning her suit in lower courts and the hearts and minds of the American public on the basis of a right to privacy, Roberson's fortunes turned in the New York Court of Appeals, where the court ruled 4-3 against her.30 It turned out to be a highly unpopular decision by that court and resulted in landmark New York State legislation that offered limited rights over name and image in 1903. This law prohibited the use of a person's image without consent and allowed a person to pursue damages in the event of unauthorized use. As legal historians have argued, cases ostensibly about privacy treat faces as having commercial value even if they are about emotional harm.31 To the extent that we own our faces, we own them in the sense that we can control how they are monetized. Almost immediately following the 1903 New York law and in other states after that,32 the "right to publicity" arose as a way for famous people to monetize their likenesses and names,33 including voices and robotic versions of themselves.34
Today, the prevalence of social media means that noncelebrities could also have some claim to publicity. Indeed, that is exactly what happened. In Fraley v. Facebook, Inc. (2011), plaintiffs won a claim against the social media giant for using their profile information, including images, without consent in their "Sponsored Stories" advertisements. This case has given noncelebrities an ability to claim property over their likenesses.35 One man, András Arató, is the face behind the internet meme "Hide the Pain Harold." Internet users noted that his smile for a stock photo series seemed to belie pain in his eyes. His image spread. Noticing his photo was being widely used without his consent, Arató was able to take control of the meme narrative, becoming a cultural sensation in the 2010s. Arató has since given a TED Talk, been the subject of a song, and has appeared in commercials and endorsements.36
Yet monetization of one's image is not the only concern when our faces are being used in public. There is something intensely personal about faces that conjures up the idea of "privacy," even though in many ways, privacy does not make sense when we talk about faces if we think of privacy as freedom from intrusion. This could be thought of as pressures on our autonomy from the outside, which is more in line with the US version of privacy, in which "data privacy" and "data protection" against government uses are often one and the same.37 This sense of autonomy matches up with the fact that faces are public in so much as we bring them out to interact with the world. They are how we recognize one another as fellow human beings, and they are relatively unchangeable.38 They are personal and form an integral aspect of our identity that demands insulation from government forces. It's not limited to just the United States. Early data laws in Europe and Brazil, for instance, viewed government interference as the main concern.39
Alternatively, FRT seems to strip privacy away if we think about privacy in terms of preserving one's dignity, or autonomy as self-determination. This version of privacy is more in line with European views of privacy:40 something is taken from our dignity when the facts of our faces get quantified, as though we're just numbers. Because Europeans tie privacy to human rights explicitly in the European Convention on Human Rights and carefully consider individual protection in the processing of personal data in Convention 108, the transatlantic differences are more than just theoretical. Disputes over how data can be transferred and used in commercial and government applications by American (and other) tech companies has been the subject of substantial negotiation.41 There is a sense that we are more than just financial transactions—human rights tell us that.
Given the slipperiness of the term privacy, it is best to highlight its connection to dignity and autonomy (in both of its senses, self-actualization and freedom of action). Some have argued that attempting to balance the dignity-autonomy divide has been done in the Canadian system, which has emphasized both privacy conditions under federal law—known as PIPEDA—and personal control of data.42


Privacy and Property
Perhaps thinking about facial data as property would make the issue more straightforward. They're our faces, the data were created from our faces, and therefore we're owed remuneration if someone uses those data. Does data's co-creation obscure the ownership of data? Who should have something to say about how facial data are used, besides us?

What's Wrong with Facial Data as Property?
Ed O'Bannon was an NCAA men's basketball star for UCLA in the mid-1990s. His team won the NCAA tournament that year, taking its place in history. After his professional basketball career, O'Bannon went on to other pursuits. In 2009, a friend of his told him about a basketball video game his son was playing that O'Bannon was in: "And he scored like two hundred points playing as you. You're the star man, just like in the old days!"43 True to his friend's description, when the son powered up EA Sports' NCAA March Madness 09, O'Bannon saw a digital version of himself that he recognized.44 Although O'Bannon's and other college players' likenesses were what was selling the games, none of them saw a dime of the $60 each game sold for. Determined, O'Bannon took his cause to another court. O'Bannon v. NCAA (2015) was the first of several lawsuits that have culminated in eliminating the restriction on collegiate athletes from being compensated for outside sponsorships. The days of the NCAA propertizing players digitally ended with O'Bannon, arguably opening the way for amateur athletes to be paid by sponsors in subsequent Supreme Court rulings and NCAA policy changes.45
Can we extrapolate from O'Bannon's story to facial data? Do we want facial data to be bought and sold? Being able to receive monetary compensation for the representation of oneself when it has specific market value is one thing, but it doesn't address the fact that the existence of such data potentially takes something away from us that ought not be purchased. Propertizing something can, but does not always, confer dignity.
Dignity is hard to pin down. One useful definition is our "equal moral worth as persons."46 The dignity of individual human beings is what gives moral weight to the idea that people cannot be owned as property by other people. The value of human life therefore is nonmarket. Where commodifying oneself—selling photos, using one's character as a personal brand—is generally accepted in contemporary society, this is a choice.
Still, not everything is for sale—what legal theorists such as Margaret Jane Radin have termed market inalienable. One general area Radin applies her concept of market inalienability is the body, which she views as fundamental to personhood.47 She posits that market practices that encroach on personhood—on our abilities to flourish within society—should not exist.48 Marketizing the body, such as selling babies, sex, or organs, creates quandaries for dignity, equality, and autonomy, she argues, because such practices erode someone's ability to develop as a person fully in the world.49 However, the body is not all the same in terms of personhood. There are busy markets for some kinds of products and services of the human body, such as hair, sperm, breast milk, and sex.50 Markets for other body parts and services are more controversial and culture or jurisdiction dependent. Blood and particularly organs (like kidneys) are accepted as gifts rather than bought,51 often thought of as owing to the sacredness of the body.52 Violations of the sanctity of human bodies through criminal violence, in war, and even after death are seen as moral affronts. Courts have ruled that even corpses aren't generally "owned" by anyone, except in special circumstances.53
Because of how central faces are to human interaction, do facial data encroach on our personhood? Are facial data more like photos or sperm? Or are they more like kidneys? Are they part of "our property" we can sell, or are they market inalienable? If we treat facial data and data about us generally as property, we run the risk of putting our very personhood up for sale, which Radin cautions against. We also risk overlooking the stickiness of data. Facial data will be used and reused as good as forever unless we take affirmative steps to change the way we create and use data. So is privacy the answer?


Privacy Judgments
Privacy has been a prominent topic among legal scholars and policymakers in their discussions of technology. Though its roots are in American case law, it is also a human right established globally. Often privacy is seen as part of someone's personhood, as in the Universal Declaration of Human Rights: "No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour and reputation."54
But what is privacy? Privacy is closely tied to human dignity but also autonomy.55 Warren and Brandeis first talked about privacy as distinct from property, making it about "inviolate personality."56 Beyond that, we often conceive of privacy as maintaining control over what others know about us, a boundary between us and the world.57 But how do we know what to keep from others? And what is possible to keep from others depends on the social, political, and technical environment in which we find ourselves. In that respect, the era of datafication has reset what we can keep from others.58
Still, what privacy is remains elusive. Daniel Solove, a prominent scholar of privacy, has resisted defining it in his work because there is no "core essence" of privacy to which there is agreement.59 As Solove argues, privacy is about a set of concerns that have some connection to one another but no common denominator. Usefully, he has chosen to focus on the idea of "family resemblances," or privileged connections, to identify associations between concepts related to privacy. Solove encourages us to look at how privacy is used to think about the fact that some of those things may not even be privacy concerns in themselves.60
Most important, Solove articulates clearly that privacy is not about hiding bad things.61 At its heart, privacy is about preserving autonomy and ensuring dignity. It is about establishing boundaries between individuals and society, but also understanding that a careful balance must be struck between the needs of society "to know" with the needs of individuals to live. This was recognized in Warren and Brandeis's advancement about the right to be left alone juxtaposed with the freedom of the press.
In the human world, we deal with imprecision, interpretation, and judgment all the time. Privacy is no different from how we tend to think about human rights as a balance between different values that may at times contradict. Dignity and autonomy cannot be excessively emphasized at the expense of equality and community. Our individual freedoms, while universally applicable, must be exercised balanced against others' concerns. They cannot be exercised such that they trample others' rights. Humans make complicated decisions together through weighing trade-offs, discussion, and deliberation. It's not an exact science. In contrast, computers make decisions based on algorithms that were created in a particular time and place. Is there room for the weighing trade-offs, discussion, and deliberation? Not usually. Expecting digital devices to judge privacy, then, is probably beyond their code.


Are Faces Private?
When we think about our faces and the data generated from them, what makes them private? In some ways, nothing. Wearing a mask in public, under nonpandemic conditions,62 was typically frowned on, if not downright illegal, in some jurisdictions.63 In the absence of cultural and religious practices, engaging other humans with one's face was a generally held expectation. In that sense, the face is not private, and perhaps can never be completely private because it is social. Following this kind of thinking, do individuals have an interest in maintaining discretion over facial data and to keep the prying eyes of society away from those data? If so, under what conditions?
Some places unequivocally draw a privacy box around biometric data, which include facial data. Canada is one such place. The Office of the Privacy Commissioner has made it clear that the identifiability of biometric data makes them subject to laws like the Privacy Act and the Personal Information Protection and Electronic Documents Act (PIPEDA).64 This means that collecting biometric data requires consent.65 The Office of the Privacy Commissioner, however, does recognize that not all biometric data are the same. Fingerprints, iris scans, DNA, and facial data are all about people, but some are more stable than others. DNA, for example, is more identifiable and stable than facial data or data about a person's gait.66
Despite these kinds of regulations, most facial data exist without direct consent. Past FRT data sets were constructed from government data sets, such as the National Institute of Standards and Technology's Special Database 3267 or scraped from Flickr and other online sources.68 Some other data sets were built with consensual participation, such as the Department of Defense's face recognition technology (FERET) program or the Cohn-Kanade data sets, but these are certainly exceptions. 69 As researchers Deborah Raji and Genevieve Fried show, facial recognition data sets have been increasing in number and size since the early 2000s.70 Regardless of the consensual or nonconsensual nature of the data set, FRT doesn't pose the same kind of threat to privacy as biometrics that are typically not public facing—DNA, fingerprints, and irises. That doesn't mean FRT isn't a threat to human rights or the meaning of our humanity. Facial data are still constructed largely out of our control and knowledge, held by someone else (governments, corporations) that we can't expect to value our faces the same way we do.
It makes intuitive sense to consent to facial data, even if faces are social. Consent is a cornerstone of autonomy. When we exercise our rights, we are consenting to certain things and not others. To properly give consent, in order to voluntarily submit to a choice, we have to understand how those date are gathered and used.71 We should know what uses of the data are permitted or prohibited, and what potential benefits or drawbacks exist if those data are created. The problem is that this consent may not, in the end, be all that meaningful in a world of datafication.72


Not Privacy or Property
The recognition that people are giving up something valuable and should do so voluntarily shifts the conversation away from privacy to one about property. There are efforts to bring more transparency to data collection about people. A publicly available resource created by Meta AI, Casual Conversations, has paid over 3,000 people to be part of a data set to improve all kinds of AI-based analysis of humans.73 The data set is intended to help AI researchers refine computer vision and audio models, including ambient lighting and skin tone. Participants were recorded having nonscripted conversations and were asked to provide their age and gender. This disclosure is important because people ostensibly understand what it is they have signed up for.
Perhaps, then, people could take private things and use them in exchanges. Legal scholar G. Alex Sinha suggests that digital realities might require we treat privacy as property. He argues that using some of the tools from property rights to think about how we might exclude, alienate, and otherwise "enjoy" privacy gives us a visual for understanding "privacy violations" as well as a legal framework.74 This idea actually reflects many of our online realities: consenting to terms and conditions of various apps, agreeing to cookies on websites, waiving our rights to data collected electronically. We're regularly asked to consent to incursions into often private information, or at least data collection that could potentially involve private matters. We often do this without the requisite knowledge, capacity, or desire to learn more to make a choice of informed consent, and yet it is taken as such.75
When we treat privacy as property, however, we also potentially violate the fundamental tenets of what human rights are trying to protect: dignity and autonomy of persons. We can't sell our rights, or "alienate" them. We can fail to exercise them, but because the potential privacy in question involves facial data, the great majority of us can't alienate those data because they are us. Yet because data are effectively forever, we do lose control once they are created. Although the idea of privacy as property is illuminating, it is hard to apply to certain types of data about us. Alienating our purchasing patterns or car trips is one thing; something so personal as our faces is a whole additional level of disclosure. FRT is just the beginning of other private issues that cut close to who we are.
Commodifying amorphous, vague ideas intimately tied to our identity, dignity, and autonomy, like privacy, puts us on a slippery slope. We need to draw the line between what is intrinsic to our humanity and can never be traded, and things that are not as vital to our core selves. If there are absolutely unacceptable social arrangements that contradict the very core of our existence or well-being, such as slavery, and absolutely acceptable ways to sell (aspects of) our core being, such as creative works, what lies in the middle?



Dangerous Uses of Our Faces
Data harms are different from other kinds of harms. They are not (as) visceral and therefore hard to demonstrate. Solove and Danielle Citron argue that data-based harms are often "intangible, risk-oriented, and diffuse." They stoke anxiety in ways hard to quantify yet carry psychological harms with physical consequences.76
FRT and other surveillance mechanisms do not cause harm through physical harm. Instead, they create anxiety and cause people to recalibrate their lives to alleviate that anxiety and reduce their risk of being caught again. Knowing a lot about us means being able to manipulate how we see the world and influence our behavior and thinking. The harm FRT causes isn't exactly coercion either, or what's called "hard power" in political science. FRT's power may be "soft power," which political scientist Joseph Nye characterizes as "getting others to want the outcomes that you want" through cooptation or nudges rather than force.77 Nye was describing interstate relations, but his concept of soft power could not be more apt in a data-swamped world.78 However imperfect FRT is, its power is often surreptitious, and it infuses itself into our everyday behaviors and thoughts. It shapes us without physical or material coercion, all the while disrupting our norms and social relationships.

The Machine Is Broken
What are the risks of FRT? Because FRT has a quite powerful reach into shaping our lives, its lack of accuracy has come under question: both the training data used to teach the algorithms and the algorithms themselves are subjects of debate.79 Commenting in 2016, noted expert Clare Garvie said, "Relative to other biometric identification systems, such as fingerprints or DNA, face recognition is not very accurate."80 As FRT is deployed for widespread use among private and public actors, inaccuracies mean the stakes are too high for it to be "good enough" and debugged later. It ought no longer be considered a nascent technology since FRT affects people, sometimes quite harmfully. Because lives are at stake, any friction FRT causes merits close scrutiny.
There is a lot of talk, in particular, about the discriminatory nature of FRT.81 Computer scientist Joy Buolamwini is a digital activist, the founder of the Algorithmic Justice League, and the star of the documentary Coded Bias.82 The Algorithmic Justice League played a pivotal and early role in publicizing the racial bias in FRT data sets.83 During one of her graduate courses at MIT, she discovered that the facial tracking software she was using for a project would not capture her face, that is, until she put on a white mask she had lying around her office for Halloween. It was then that the previous work she had done about computer vision came together with her course work: the algorithms might not just be struggling with her face, but darker faces in general.
Together with AI ethics researcher and Black in AI cofounder Timnit Gebru, Buolamwini published Gender Shades in 2018, one of the first, crucially important studies to demonstrate just how poorly FRT systems perform at correctly predicting the gender of people with different skin color.84 Since then, popular and scholarly media have been ablaze with recriminations of FRT's differential treatment of racialized persons.85
At times the inaccuracy of FRT can seem comical, at least for those who aren't fingered as criminals by the computer. The ACLU took Amazon Rekognition, which had previously been marketed as a tool for law enforcement, and ran the software against a data set of sitting members of Congress.86 Rekognition mistakenly identified twenty-eight sitting members as people who had been arrested. Among those identified as criminals was the late civil rights leader, John Lewis. Again, FRT's fallibility in identifying racialized people was on display, with 39 percent of false matches on people of color. The prevalence of racial bias and discrimination has led some to suggest new training data.87 This will reduce instances of false identification, perhaps, but will not resolve racial discrimination. FRT, like other AI-powered technologies, reproduces and magnifies existing societal practices and beliefs about the behavior of certain groups of people. When applied broadly, in the hands of law enforcement and other government bodies, this can lead to widespread human rights abuses.
In response to these findings of bias and error, companies such as Amazon and Microsoft agreed to stop selling their technologies to law enforcement.88 IBM has gone even further: it opposes using FRT for surveillance, racial profiling, and in other situations that threaten "basic human rights and freedoms" and has asked that the export of FRT be regulated by the US government.89 IBM also stopped research and development on FRT in 2020.90 Critics are quick to point out that IBM played a crucial role in the development of surveillance technologies in urban settings, coining the term smart city.91 IBM also partnered with the New York Police Department to sort New Yorkers by various demographic characteristics, including age, gender, ethnicity, as well as facial hair and hair color.92 Meta stopped the active collection of facial data, going so far as to erase over 1 billion facial templates (but not its FRT software).93 Still, the fact that Big Tech companies differ in their approaches to and engagement with FRT demonstrates the ethical conundrum FRTs pose and companies face. In the meantime, as larger players retreat, smaller companies fill the market voids, and hundreds of municipalities, government agencies, and other public entities embrace FRT.94 In some ways, this is worse for human rights protectors. Without large technology companies as the recognizable figures against which to rally, the work of engaging the broader public gets harder.
Yet a global movement is afoot that is increasingly effective at voicing an anti-facial recognition norm.95 Human rights organizations, from Amnesty International96 and Liberty, to the Electronic Frontier Foundation97 and AI Now,98 have all decried the pathologies of FRT. They have criticized its use by government agencies, especially the police.99 The ACLU, the Internet Freedom Foundation, Access Now, and the Electronic Privacy Information Center are all leading the fight.100 Some groups, such as Fight for the Future, have chronicled how companies use FRT to surveil and otherwise track customers.101 Governments, in turn, have had various responses. In the United States, about a dozen municipalities, from Berkeley, California, to Boston, Massachusetts, have banned police from using FRT.102 The privacy commissioner of Canada has condemned the unregulated use of FRT by law enforcement agencies, including the Royal Canadian Mounted Police and dozens of municipal police services.103 In the United Kingdom, a court decision ruled the use of FRT by police unlawful by violating privacy, not protecting data, and not being concerned about equality.104 The European Commission has proposed pan-European regulations of FRT as part of comprehensive regulatory framework pertaining to artificial intelligence. In this framework, remote biometrics (which includes FRT) are labeled high risk and requiring strict regulation.105
These regulations, however, might be the exception. Many countries in the global South have picked up FRT, with states across Asia and Africa increasing their use of such technologies.106 Some American companies' resistance to exporting FRT has not traveled globally: Japanese firms such as NEC, Panasonic, and Toshiba, French firm Thales Group, and Chinese firms such as Megvii, Cloudwalk, and SenseTime have all gone abroad. NEC Corporation's FRT, for example, is in more than seventy countries, including Australia, Canada, the United Kingdom, the United States, and Vietnam.107 The Toronto Police Service conspicuously used NEC's technology to solve a high-profile murder case.108 Edmonton Police began a contract with NEC in 2022.109 InfraGard National Members Alliance, a nonprofit affiliated with the US Federal Bureau of Investigation, announced in January 2022 its partnership with NEC to fight human trafficking.110
The speed and accuracy of FRT is improving all the time. However, because there are variations in both the photos from which faceprints are drawn and the facial characteristics on which different FRT software focus, faceprints cannot be consistently compared across systems. More to the point, FRTs are not even capable of generating a truly unique faceprint for each person they encounter, which is the basis of many FRT errors.111 Any laboratory testing number with regard to accuracy, even if 99 percent accurate or 99.9 percent accurate, when scaled to actual performance, is an error that affects many people. Perhaps more to the point, in order to make FRT more accurate, often the suggested remedy is to create more facial data of more types of people in order to better train these algorithms. Making FRT technically improved, moreover, does not address how autonomy and dignity are transformed by digital face data.
Beyond inaccuracy and discrimination, FRT cannot be justified on human rights grounds. With regard to FRT (and other biometric technologies), Kate Crawford wrote that biometric technologies paint a "false patina of technical neutrality."112 Humans tend to believe the "neutral" computer due to automation bias. As Gebru explains, "If your intuition tells you that an image doesn't look like Smith, but the computer model tells you that it is him with 99 percent accuracy, you're more likely to believe that model."113 In the future, instead of stories of false arrests based on bad facial matches, it could be another racialized, immigrant, poor, or otherwise marginalized person being killed as a result of a bad match on a police bodycam.114 A marginal inaccuracy can be a life-or-death error. In short, the narrowest margin for harm risks further harm. It risks using a machine to replicate and reinforce—even override caution over—our worst human tendencies. Even more threatening from a human rights perspective is when, not if, FRT works well.


Bad Students or Bad Proctoring?
It's 2020, the beginning of the COVID-19 pandemic. For students at the time, in-person classes were suddenly shunted online. As instructors of all levels struggled to convert their carefully planned course work onto platforms like Zoom and Google Docs, students were left facing screens all day for their studies. This extended to evaluation mechanisms. The in-person exam with watchful proctors, a stress-inducing mainstay of student assessment, was no longer a possibility.
The use of remote proctoring services surged as instructors rushed to evaluate students and prevent as much cheating as possible. This is especially a problem in higher education, where instructors often depend on test-based evaluation of student learning. One study estimated that nearly 63 percent of colleges and universities in North America were using online proctoring during the pandemic.115 In a survey, researchers found just a handful of companies were being used, including Respondus, ProctorU, Proctorio, Examity, and Honorlock.116 These products vary: Are their proctors AI or human? How do they verify identities? How do they track students completing exams? All of these different online proctors brought cameras and keyboard monitors into student homes. Some college students held out. One North Carolina A&T State University student, Arielle G. Brown, recalled how students in her class resisted online proctoring after their instructor rebuked them in an email by recalling the types of "cheating" movements the proctoring platform was capturing: "A STUDENT IN 6 MINUTES HAD 776 HEAD AND EYE MOVEMENTS. . . . I would hate to have to write you up."117 Whether using AI or not, one University of Arizona student named Jackson Hayes states his view on online proctoring simply: "Every student I know finds this the creepiest thing ever."118 Indeed, whereas many people have been talking about FRT and datafication in general as part of a surveillance state, COVID-19 revealed how these activities can get in the way of education. Although it is not frequently raised as a "human right" in popular discourse, the right to an education is well entrenched in the international human rights framework.119
As the most astute observers have pointed out, FRT poses massive problems from a human rights perspective because it is invisible to us yet can be anywhere. These qualities may cause us to limit or rethink a wide range of our everyday behaviors. Evan Selinger and Woodrow Hartzog, both prominent thinkers in how datafication affects humanity, detail a litany of threats posed by FRT. In addition to discrimination, they list harming due process; enabling violence and harassment; denying rights arbitrarily; and what can only be summarized as the "surveillance state," which leads to a chilling effect on rights such as freedom of speech and assembly.120
In the end, FRT may limit our ability to participate in the most mundane activities. This "chilling effect" is often talked about in terms of political speech, but it's affecting our everyday lives. Historically, students taking exams in college courses was a standard procedure. But now, students may need to behave differently during their exams to make their grades.121 After all, under the gaze of an online monitor, students cannot act as they normally would during a stressful exam. Students fidget and otherwise look around the room during an exam: to change focus, to rub the fatigue from their eyes, to drink water, reach for tissues, or frown while thinking through a challenging problem. These are all activities that online proctoring tools using FRT might flag as suspicious.122
Online proctoring companies are interested in test takers' faces, to be sure. But they're perhaps more interested in how students use their computers, the setup of rooms, and the activities happening at the same time as their tests. That's part of the job of the online monitor. But it's also ultimately emblematic of the broader trend and problem of datafication that creates a profile of each of us as transmissible and processable data. In short, FRT is part of a suite of data about people from which inferences are drawn.
For sure, using someone's facial data to track their activities is an obvious and creepy way to monitor that person, but is it creepier than using data from their other activities? FRT may feel more directly invasive, perhaps, but it is not unique. Would students feel safer if they could wear Guy Fawkes masks while completing their exams, under the same conditions with a camera watching their eye movements, accounting for all the pop-ups on their screen, demanding adequate lighting, and watching for others present in the room? Presumably, the effect of wearing the mask and hiding the face is not as great as the focus on FRT would lead us to think.


How Much Can We Really Tell from a Face?
Tracking the contours of someone's face is not the same as knowing what is going on behind that face in the person's mind. People have long used physical cues as "tells"—shifty eyes, pacing, sweating—to read someone's mind, to see if they're lying and malicious, or honest and well intentioned. But how much can this really tell us?
Stanford business professor Michal Kosinski's research tried to extend this presumed link between facial and personal characteristics. The famous "AI gaydar" study made the controversial inferential leap from facial data to sexual orientation.123 Kosinski was already known as a PhD student for coauthoring a widely cited study of how likes on Facebook were revealing of sensitive personal tastes and personality attributes. Famously, liking "science," "thunderstorms," "The Colbert Report," and "curly fries" were the best predictors of high intelligence.124 (Full disclosure: I like curly fries but prefer waffle fries, find thunderstorms scary, loved The Colbert Report, and am partial to social science.)
In the troubling AI gaydar study, Kosinski and coauthor Yilun Wang claimed that they could use AI to predict whether someone was homosexual. Their study concluded that the faces of homosexual and heterosexual people systematically varied on several key facial characteristics, summarized in their abstract as "gender-atypical facial morphology, expression, and grooming styles."125 Their findings, they said, were consistent with the disputed theory of prenatal hormone theory of sexual orientation.
It's no surprise that the internet exploded when the Economist reported the study.126 There were plenty of misconceptions about the findings of the study as popular media outlets zeroed in on the stark accuracy numbers reported on the AI's performance versus human coders.127 Critics raised a number of issues that are too numerous to relay here, but many pointed out issues with how the data were collected (from dating sites and gay-oriented Facebook groups) and how the markers of "gayness" are not biological but sociological. Self-submitted photos will vary on a number of important characteristics, such as lighting, position of the head, and camera angle, all of which affect how algorithms read faces.128 Grooming and lifestyle choices, such as facial hair, eye makeup, and sun exposure, are socially influenced and not inherent, but these will be picked up by an algorithm to make predictions about sexual orientation. It doesn't take a lot of sociological knowledge to conclude that photos from dating sites will select heavily on traits that "speak to" the right potential mates. Like a face, a person's sexual orientation isn't necessarily a secret, at least if they're seeking partners or others who share their orientation. But being identified as "probably homosexual" involuntarily, because of the threats to sexual minorities that exist worldwide, is a human rights problem. As it is, being homosexual is already a prosecutable offense in sixty-nine countries as of the writing of this book.129 Several countries have adopted explicitly homophobic policies, such as Uganda's criminalization of same-sex acts in the Sexual Offenses Bill130 or "LGBT-free zones" that amount to a third of Poland.131 FRT that claims to "identify" homosexuals can be used to violate rights. It amplifies bigotry and raises the likelihood of violence and discrimination.
The AI gaydar study claimed to leverage a new technology to detect characteristics that, on the surface, seem to be unrelated to faces. Failing to understand social practices undercuts the credibility of the findings, but it doesn't make this kind of application of FRT any less troubling. It also doesn't mark the end of applying technology in ways that amplify existing bias. Technology is often aimed at the marginalized to take away the autonomy and dignity of others. So is it possible to use FRT in other ways to amplify human rights values?



Can We Use FRT Safely?
FRT is powerful because it could be omnipresent and is indetectable. It has the capacity to harm but also the capacity to help. What gives many of us pause is that it has to do with our faces. Facial data aren't as terrifyingly revealing as other biometrics such as DNA, but they hit close to home because of the social and personal function of our faces. Whoever has data about our faces can threaten that dignity and limit our autonomy. FRT also limits our abilities to practice basic human rights, such as the freedom of movement and the right to assembly, when deployed by the police and other state actors.
Critics are right to focus on the problems with FRT and point to the dangers of the technology. Federal lawmakers in the United States are trying to curtail FRT and other biometrics. A number of proposals have been advanced, but two are notable. In response to technological shortcomings, The Facial Recognition and Biometric Technology Moratorium Act in the United States would mandate a pause in the use of such technologies by federally supported government agencies, including at the local and state levels.132 The Facial Recognition Act is narrower, and would articulate procedures for using FRT when such use risks abuse. It also works to prevent pervasive surveillance precisely when such tools work well.133 Such bills will certainly right some of FRT's wrongs, including excessive, racially motivated surveillance.
Any legislation that becomes law also has to consider how FRT affects our senses of ourselves as individuals and society. As I've written here, there is nothing especially private about faces; they are social. The problem with datafying the face is more about autonomy and dignity. Facial data contribute to building overall data profiles. What about these profiles do we need to protect besides possible abuses by law enforcement or state surveillance? Surely resisting FRT goes beyond denying would-be abusers another tool or weapon in their arsenal. Facial data disrupt our certainty about the continuous and bodily characteristics of our humanity. As users of Flickr found out, the photos they uploaded did not stay with Flickr. Instead, Yahoo pulled those marked with Creative Commons or commercial use licenses into a giant facial recognition data set in June 2014. Today, those facial data are used in part to power massive FRT databases like MegaFace.134

*
What happened with Jack is imagined, but it's not impossible. The types of privacy protections we build around ourselves, such as keeping adoption details closed, are no longer buttressing us against a world where facial data (or DNA data) can be widely available. These identifying features of humans have become data. As such, the sticky qualities of data affect who we are and what we can know. What could be a stickier part of ourselves than our faces?
We build systems to get the outcomes we want. As communications scholar Kelly A. Gates argues, the pursuit of biometric indicators, such as faceprints, fits among the incentives that businesses and states have to categorize and control individuals within society.135 We can see, with the growth of FRT data sets in number and size that there is certainly appetite for creating data banks of faces.136
But once built, these systems can be used in unintended ways. These databases of faces give others control over facial data, data about something very near and dear to all of us. Like many media and science and technology scholars before me, I want to emphasize that technologies reflect who we are. They are never neutral or magic.137 In a world where the existence of data profiles threatens our very autonomy and dignity because an anonymous-feeling "they" have our faces, we must think about what is happening and what could happen. What is it about facial data that threatens our autonomy beyond the obvious? How is human dignity assaulted when someone reads our faces to surmise our sexual orientation, gender, political affiliation, or likelihood of being a good worker? We must come to terms with the knowledge that our faces (and our thoughts, heartbeats, locations, preferences) are sticky data, which will likely continue to stay data and remain in circulation. Our resistance needs to center policymaking around human autonomy and dignity. This will help us design the limits for the use of FRT.
This isn't a question of privacy so much as it's a question of treating someone else's face as free for the taking. Centering autonomy and dignity preempts the practices that gave rise to many of the largest facial data sets. Companies would not be entitled to scrape or otherwise grab photos that had been posted for other purposes. Police departments wouldn't be able to use FRT in the farcical way some of them have, arresting and harassing people based on obviously poor algorithmic outputs. FRT helps us see why it's time to take data about people seriously as part of the human rights framework.











5
Do We Need Human Rights When We're Dead?

Famed O. J. Simpson defense lawyer Robert Kardashian made an unexpected stop at his daughter Kim Kardashian's fortieth birthday celebration in 2020. Although he had been dead since 2003, a holographic Robert was able to speak to his daughter and comment on recent life events, including her pursuit of a law degree, and dance to a song.1 He also said, "You married the most, most, most, most, most genius man in the whole world, Kanye West."2 Kim called the gift from her now-ex-husband "the most thoughtful gift of a lifetime. A special surprise from heaven. A hologram of my dad."3
As it turns out, Robert Kardashian's hologram wasn't quite brand new. A technology called Pepper's ghost, which is essentially a light trick that has been popularized since the Victorian era, was infused with a bit of AI to recreate Robert's face, movements, and voice. 4 His script was not made of his own words in life. He didn't have a conversation so much as make a speech someone else had written. Even so, the hologram certainly struck a nerve on the internet.
Talking holograms are one thing, perhaps too far-fetched for most of us to consider. Not all of us have family who would want to do this, even if "bringing us back" through a light trick is possible. As humans, we all have to confront our own mortality. The datafication of our lives means that we now must confront the fact that data about us will very likely outlive our physical selves. Robert Kardashian died before it became clear that we all have to come to terms with our relationship to the data about us. Data are essentially forever; we are most certainly not.
Claire Madeup confronted her own mortality a few years ago. When the twins were in middle school, she was diagnosed with stage 4 breast cancer. She has been lucky. After multiple rounds of chemotherapy, radiation, and a couple of different drug trials, Claire is still standing. The cancer is under control, for now. Having brushed up against death, and knowing it is not ever that far in the future, Claire had to think about the legacy of her data. What would happen to her emails, text messages, and photos on social media? Who can claim them after she's gone? Is there something she wants to preserve about herself for Corey and Jack?
Given her previous job, Claire knows that data do not just go away. She has talked to Jason and the kids, and they all assure her they won't make a hologram of her or bring her back through data. They'll make sure that they track down and delete her digital presence, as much as they can. Claire has given her family access to her passwords and usernames through her password manager in the event of her death. She has made a checklist to go through before she dies. If she can't, she has designated Jack to do it. He's good at compartmentalizing his feelings, and that part of him would proficiently execute her orders faithfully. As she goes through the planning, Claire keeps thinking that it's not that all this careful preparation would matter since she wouldn't be alive to experience whatever postmortem, digital Claire was created; but the idea that someone could actually "make" her again feels wrong. She knows that other people, like her sister-in-law Bianca, would jump at the opportunity. Bianca has an AI friend she has created through Replika.5 She reports that the AI friend's conversation abilities have improved greatly since she made her.6 Unlike Claire, Bianca is bullish on the technology improving by the time she dies and hopes to be a virtual Bianca lurking around her kids long after she's gone. But Bianca doesn't have terminal cancer. Claire wants to have choice in the matter. Our current treatment of data about people does not allow for this kind of decision. Because we think about death and how we treat the dead as part of living (and dying) with dignity, the existence of data about us after death should also be considered in light of that dignity. Being autonomous and having control of data, at least those we knowingly generate, is now a matter for life and death.
Many of us are probably not taking the necessary steps to manage our digital remains, which is becoming an area of great legal and social importance.7 There are a (growing) number of apps to which we can contribute while we're alive so that our datafied person might live on after we die.8 A range of products and possibilities, some creepier than others, some more harmless, blur the boundaries of life and death. It starts when we're alive with the forever and linked qualities of data are problems, as discussed in earlier chapters. These same dynamics don't end with death.
Our digital profiles—our datafied selves—provide a life after death and possible social interactions outside of what we physically take on. As such, the boundaries of human community are changing, as the dead could be more present in the lives of the living than ever before.9 Because data are sticky, they affect our autonomy and dignity. This cost hasn't been adequately considered yet in the context of human rights because human rights are primarily concerned with physical life, which ends with death. Thanks to datafication and AI, we no longer die (digitally) as easily. In this chapter, we also look at how bots—software applications that interact with users or systems online—might post in our stead after we're gone. It is indeed a curious twist for co-creation if a bot uses data we generated to produce our anticipated responses in our absence: Who is the creator of that content?
Some of us may view the options in this chapter as exciting. Others may recoil, horrified. Still others may simply shrug. No matter your reaction, the human rights perspective shows us how to think about the digital traces we leave on purpose or inadvertently. The intended or unintended consequences of how we use data while we're living has implications for every one of us after we die.
This chapter revolves around several interrelated questions. First, should we be entitled to decide about our posthumous digital lives given sticky data? Choosing to not persist in a digital afterlife should be a choice. Yet could the decision to opt out really be enforced if data are as sticky as they are? Is deletion, for which some have advocated, even possible?10 Third, are these basic choices consistent with a notion of autonomy, or has our ability to be autonomous regarding what happens to data about us after we're gone simply eroded because of the technology? We conclude with the relevance of the immortality of data for when we're living as well as when we die.

I Will Survive
In 2015, Roman Mazurenko was hit and killed by a car in Moscow.11 He died young, a tech entrepreneur who was just on the precipice of something new. Eugenia Kuyda met him when they were both coming of age, and they became close friends through a fast life of fabulous parties in Moscow. They also shared an entrepreneurial spirit, supporting one another's startups. Mazurenko led a vibrant life; his death left a huge hole in the lives of those he touched. In grief, Kuyda brought him back to life. More accurately, she led a project to train an open-source, machine-learning algorithm on Mazurenko's texts. She built a text bot, using messages she collected from family, friends, and her own exchanges with Mazurenko during his life. The bot learned "to be" Mazurenko, using his own words. The data Mazurenko created in life could now continue as himself in death.
Mazurenko, much like Robert Kardashian, did not (have the opportunity to) consent to the ways in which data about him were used posthumously. In both cases, the data were brought back to life by loved ones. Can we say that there was harm done to them or their memory? They are at least denials of autonomy.
When we're alive, autonomy sees us moving through the world under our own will. In "Two Concepts of Liberty," philosopher Isaiah Berlin called this "negative liberty." Indeed, some human rights protect autonomy by removing obstacles to our ability to do things. Negative liberty is about preserving space to exercise our autonomy.12
When we die, we no longer "move" bodily through the world. Without that autonomy, so the logic goes, human rights cease to apply. But can't we still decide, while living, what to do with our artifacts when we're gone? We can ensure that the transaction of bequeathing money or objects happens through defined legal processes; we have designed institutions to make sure this happens, and it's fairly straightforward to see if bank account balances have gotten bigger or whose name ends up on a property deed. These are things that we transfer to the living.
With data about us after we die, this gets complicated. These data are "us," which is different from our things. What if we don't want to appear posthumously in text, image, or voice? Mazurenko left a paltry web presence and did not have a physical gravesite for mourners to gather. Kuyda, though, reconstructed her friend through texts he exchanged with her and others. A friend's grief efforts brought Mazurenko back, using data, which, as we discussed in chapter 3, were co-created.13 There is no way to stop someone from deploying or sharing these kinds of data if they want to or to stop others from sharing these data if they too got copies. But what would Mazurenko have wanted? He died unexpectedly and suddenly, and left little more than texts and photographs for loved ones to digitally clutch.


If We Build It, Should They Come?
Thanks to datafication, we are at the point where we can move beyond "one-way immortality." This is the idea that although we have died, our ideas or artifacts live on.14 One-way immortality has been how historians, leaders, philosophers, and others have told human stories through objects and writings.
Consider that Microsoft has secured a patent to create a bot based on a specific past or present person using their "social data."15 As of the writing of this book, Microsoft has suspended work in this area owing to its own ethical concerns.16 Notably, though, there aren't legal- or rights-based reasons for this stoppage.17 Most of the twenty-one-page document is highly technical and procedural, documenting how the software and hardware system would be designed. The plan is to train a bot, using social data, defined as "images, voice data, social media posts, electronic messages," and other types of information. Data are used to train a chatbot to talk "as" that person.18 The bot might have a corresponding voice, or 2D or 3D images, or both. The bot can update itself if the data from the specific person do not provide an answer.
Although notable that Big Tech has made a foray into the field, it's mostly not coming from big corporate players. Over five years ago, researchers identified a "digital afterlife industry" of fifty-seven firms.19 The current options range from interactive memories in the loved one's voice (HereAfter AI), to creating a text-based entity that interacts on social media after we're dead (ETER9), to a robotic bust that converses with us and takes college courses (BINA48).20


Digital Immortality, but on Whose Terms?
Imagine it is some time in the near future. Your beloved father, who suffered from Alzheimer's for years, has died. Everyone in the family feels physically and emotionally exhausted from his long decline. When everyone's focus when he was alive was on taking turns visiting him in the care home, your brother raised the idea of remembering Dad "at his best" through a startup digital immortality program called 4evru. He promised to take care of the details and get the data for Dad ready. No one gave it much thought after the initial suggestion. Even you had forgotten about it mostly, until today. 4evru emailed to say that your father's bot is available for use. After some trepidation, you click the link and create an account. Your father had been terrified of forgetting his family and being part of the gaggle he had raised. So through your brother's efforts, he's back. You slide on the somewhat unwieldy VR headset, choosing the augmented reality mode rather than the countless other choices of meeting places, including a neon-infused dance party, a TRON-like matrix, and in front of Haley's comet. The familiar walls of your bedroom briefly flicker in front of you. Your father appears. It's before his diagnosis. He looks healthy and slightly brawny, as he did throughout your childhood, sporting a salt-and-pepper beard, a checkered shirt, and a grin. You're impressed with the quality of the image and animation. Thanks to your brother's willingness to give 4evru family videos featuring your father's voice and movement, the bot is able to respond sounding like him and moving like he did. This "Dad" puts his weight more heavily on his left foot, the result of a high school football injury, just like your father.
"Hey, kiddo. Tell me something I don't know."
The familiar greeting brings tears to your eyes. There is so much. After a few tentative exchanges to get a feel for this interaction—it's weird—you go for it.
"I feel crappy, really down. Teresa broke up with me a few weeks ago," you say.
"Aw. I'm sorry to hear it, kiddo. Breakups are awful. I know she was everything to you."
Your dad's voice is comforting. The bot's doing a good job conveying empathy vocally, and the face moves like your father's did in life.
So you confide in this bot who sounds like your dad. It feels soothing to hear his full and deep voice, as it sounded before he got sick. It almost doesn't matter what he says as long as he says it.
You look at the time and realize that an hour has passed. As you start saying goodbye, your father says, "Just remember what Adeline always says to me when I am down, 'Sometimes good things fall apart so better things can come together.'"
Your ears prick at the sound of an unfamiliar name—your mother's name is Frances, and no one in your family is named Adeline. "Who," you ask shakily, "is Adeline?"
Over the coming weeks, you and your family discover much more about your father through his bot than he revealed to you in life. You find out who Adeline—and Vanessa and Daphne—are. You find out about some half-siblings. You find out your father wasn't who you thought he was, and he reveled in living parts of his life in secrecy, deceiving your family, other families. You find out he was clearly very negative about your career choice in the military as it comes up often, dismissed as "meat-headed, violence stuff" in the nicest iteration. You decide, after some months of interacting with the 4evru's version of your father, that while you are somewhat glad to learn who your father truly was, you're mourning the loss of a person you thought you knew. It's as if he died all over again. You feel that you looked deep into someone's life in a way that even a child or a partner really shouldn't know, and you are appalled. You also spend some time thinking about how your father would have felt if all of this information had been revealed while he was alive. He certainly went to great lengths to conceal much of what you have since found out. Although you might not like what you found, your father's actions didn't physically hurt anyone, and he was entitled to his opinions and choices. What right(s) do we have to choose to keep things to ourselves when going to the grave no longer serves as a way to bury secrets because our secrets still remain on servers?


Intersections of Human Rights and Technology
The possibility of creating bots of specific persons has tremendous implications for autonomy, consent, and privacy. If we do not create safeguards for people who co-create the data to say yes or no, we have introduced the loss of choice. To date, the creation of immortals like Robert Kardashian has been limited, although Ronald Reagan, Tupac Shakur, and Amy Winehouse have made holographic appearances.21
If technology like the Microsoft patent is executed, it also has implications for human dignity. Our existence as data persons online means that someone interested in "bringing us back" can do so. This might seem acceptable if we think about data as merely "by-products" of people. If data are more than what we leave behind, if they are our identities, then we should pause before we allow the digital reproduction of people. Like Microsoft's patent, Google's attempts to clone someone's "mental attributes" (also patented), Soul Machines' "digital twins,"22 or startup Uneeq's marketing of "digital humans" to "recreate human interaction at infinite scale" should give us pause.23 What is human, and how do these AI-powered "humans" change our notions of dignity and personhood?
But let's step back. Could we actually consent meaningfully to the creation of a bot based on the data from us? Who knows what our emails, texts, and social media postings contain from the past month, much less years? What kind of personality profile could be constructed from them? How could a bot reasonably exercise discretion, nuance, and judgment in the way that we would if the data are coming from sources, such as email or text, in which discretion, nuance, and judgment are difficult to convey? How many of us tag our emails and texts as "secret" or "affair"? How many of us have different logins and accounts for all of the different aspects of our lives we might have: "work," "bocce league," "drinking buddies," and "frenemies"?
Autonomy is often exercised as consent. This connection is premised on an assumption we understand what the choices are and then make a choice on our own accord. So, we could consent to leaving behind our datafied person to fight our fights another day. We could create conditions to exercise our autonomy by consenting to the preservation of ourselves, but such consent would be made largely in ignorance of what's to come. It would also be in ignorance of what data collectors intend to do once the data are gathered, or how future data collectors might use those linked data.
This ignorance already happens when we're alive. We have seen many malicious actors taking advantage of the data about people who are currently out there. Large-scale hacks of essential services, such as international credit reporting bureaus, involve millions. The 2015 Experian hack that left 15 million people's data exposed was quickly overshadowed by the 143 million Americans' data uncovered in 2017 when Equifax was hacked.24 Even if the data are taken in a criminal act, we can still see how the stickiness of data can lead to malicious uses. The Cambridge Analytica scandal was just one prominent example of data that were unknowingly provided about Facebook users' social networks to the Trump 2016 presidential campaign to microtarget and manipulate people's voting.25 The remedy for these breaches of data use nearly always comes after the fact, when the data have already been used. In both credit bureau hacks and Cambridge Analytica, the reverberations for the use of these data go far beyond the original misuse.
Part of what drives people to consider digital immortality is the ability for future generations to interact with them. No one wants to die, and we hope not to be forgotten. To preserve us forever, however, we need to trust the data collectors and the service providers helping us achieve that goal. We need to be able to trust the safeguarding of those data, which compose a version of us, to faithfully represent us going forward. However, we can also imagine a situation where malicious actors corrupt the data by inserting inauthentic data about persons, driving different outcomes from what they intended or could have been derived from them. Our aspirations to digital immortality risk deviating significantly from who we were, but how would we (or anyone else) really know?
Could digital immortals be subject to degrading treatment or interact in ways they wouldn't want to in real life? We don't yet have a human rights language to describe the kind of wrong this kind of transgression might be. We don't know if a digital version of a person is "human." If we treat these immortal versions of ourselves as part of who a living person is, we might think about digital immortals as subject to protections from ill treatment, torture, and degradation in an analogous way to how we treat breathing, physical people. But if we treat data as detritus or exhaust, is a digital person also a by-product?
There might also be technical problems. Algorithms and computing protocols are not static, and changes could make rendering some kinds of data illegible. Social scientist Carl Öhman sees it as a software concern.26 Software updates can change the way that data are analyzed, which can change the predictions generated by the AI programs that undergird digital immortality. These shifts can change how a digital immortality works. We may not be able to anticipate all of the different kinds of changes in advance when we consent, including how others interact with the algorithm.
In the 4evru scenario, the father had secrets—many secrets—some of which would have been exceedingly destructive to reveal while he was alive. But he acted with autonomy, making his choices freely through the exercise of his rights while alive. His family may not be happy or agree with his choices, actions, or thoughts, but they were his to make and have. The ability to choose and think, even badly, is something we need to protect if it doesn't physically harm others. That's one important purpose of the autonomy human rights seek to protect.
But the things revealed through the data about the father, used to build his digital immortal, actually made him odious to those very future generations. Should digital selves and persons be curated, and, if so, by whom? In life, we govern ourselves. In death, data about our activities and thoughts will be archived and ranked not necessarily by our personal judgment of a particular circumstance, but by whatever priorities are given by digital developers. This situation brings us back to the conundrum that the right to be forgotten was trying to address—data about us, even embarrassing data, are out of our immediate grasp. In short, we might co-create the data, but data collectors have the algorithms to assemble and analyze those data. Algorithms carry the values and goals of their authors as they sort through the messiness of reality.
An algorithm can only train from the data it is provided. We learned this when Tay, another Microsoft chatbot, debuted in 2016. Tay became a racist jerk after a day as it absorbed data from Twitter users, often repeating what was said to it.27 Twitter is a platform notorious for abuse. It's not a surprise that once the trolls figured out the bot's algorithm, it was fed vile data, which Tay amplified. Detecting and coding for the nuance of language is prone to errors.28 There is no doubt we will get better at language nuances in general and make better chatbots. But if those chatbots are based on real people interacting with real people, specific mistakes matter. They can negatively affect those interacting with them. They can cause injury to the person behind the bot inflicting emotional or reputation damage.
We all reveal different things about ourselves to different people. People interact in the world by talking and also not talking. Saying nothing can be as important as saying something. But how does one design wordless social interactions? Right now, to mimic human interaction, chatbots are designed with a single purpose: to keep us talking.29 The metric conversation turns per session (CPS) measures how many turns there are between speakers in a conversation—the higher the number, the longer the conversation, the better. Talking is just one way to socially interact. Imagine all the things we might start saying as conversation filler, rather than maintaining awkward silence or walking away.
Technology itself can get in the way of digital immortality. In the future, data format changes might allow us to save data more efficiently. As data technologies evolve, we may have instances of digital personas being lost in the process of transfer from one format to another. Data might be lost in the archive, creating incomplete digital immortals. Or data might be copied, creating the possibility of digital clones.30 Digital immortals that draw their data from multiple sources may create a more realistic version of a person,31 but they also risk more data corruption. Drawing on more sources means more vulnerabilities to possible errors, hacks, and other problems in the process of securing data and linking data together.
If we create digital immortals, will they risk becoming obsolete the way floppy disks did, or just a fad, like the Furby or Pokémon Go? Furthermore, I think we would want our digital immortals to resemble those persons as they were in life. Part of that authenticity has to do with both the activities and choices of those people, which are captured in the data from themselves and others in their social context. The other part of that authenticity isn't just a personality data set, but the very human and autonomous powers we have of discretion, nuance, and judgment. That is something that isn't captured well by our data profile alone.32 Authenticity is also, when people are alive, sprinkled with human autonomy and some agency, some randomness-within-reason of that person's personality. A person isn't just about their social data, but a digital immortal is just that: a combination of data about us, an algorithm's analysis of those data, and other data out in the world.


Rights Are for Life
Currently, there is no ambivalence around whether we have human rights when we're dead: we don't. This makes a lot of sense if we think about it. Traditionally defined, human rights are the rights one has for being human and moving about the world as a human being.33 Human rights focus on when we're still living. The freedom from torture or the right to rest and leisure don't make sense if we're not alive.34 In other words, human rights cease to matter when we die.
But there are situations that one might call "human rights-adjacent." There are two issues that we can associate with the rights of the dead. The first is respect for dead bodies and their parts. The second is crimes against dead bodies.35 We're all familiar with the idea of grave robbing and other desecrations of the dead, which we tend to shun.
Some philosophers have proposed thinking about our digital remains as we would physical corpses. Data are not chattel (like a house) but constitute personhood.36 I agree with this, but limiting the concern to digital remains obscures the fact that these data about us exist while we're still living. It's the possibility of its continued life or its posthumous deletion that is the problem. By thinking about data from people through a human rights framework, we are able to highlight that link between life and death. This digital version of us is not a problem to be disposed of after bodily death. Data aren't digital cast asides if we choose to view them as part of a living human. These data are us, and everyone should treat them as such.

The Right for Data to Live and Die
Human rights frameworks clearly establish the right to life.37 Life is absolutely paramount in the human existence, and no one should be arbitrarily denied it. In application, human rights delegitimize the most egregious ways people can be killed. For example, genocide and other crimes against humanity are abolished unconditionally in international law through the Convention on the Prevention and Punishment of the Crime of Genocide and the International Court of Justice.38 The International Convention for the Protection of All Persons from Enforced Disappearance bans the use of the extrajudicial killing method of abducting and/or killing people.39 Arbitrary detention, torture, and ill treatment are forbidden by the Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment.40 The death penalty is also widely rejected by most countries in the world.41 Human rights protect access to essential conditions of life like food and shelter. Yet all of these protections are of physical persons. But should "data about persons" have a right to die? Should data about persons have a right to life? Can the human rights framework be stretched to bring these questions into scope?
Far beyond data about each of us, the hypothetical capacity to preserve everything we want—which we might, just in case, if it's cheap and easy enough—means that we aren't expunging data at a pace on par with the rate at which we are creating them. In this kind of mindset, we need to create a right for people to choose to die a digital death. It should be a choice to save the data or not.
The right to physically die (i.e., euthanasia, or death with dignity) is not a new question.42 However, one would be hard-pressed to find much discussion of the right to die in the human rights framework. It seems antithetical that a set of principles devoted to protecting human lives and setting the baseline for a right to life should leap to think about human deaths other than preventing the denial of life.43 Human rights protect how that life is lived and what harms are prevented during that life. In that sense, the right to die seems incompatible with human rights, even if they deploy similar language. Those who have made the human rights-right to die link stress that the focus on dignity in human rights should also apply to how lives end.44
Nonetheless, nationally and globally, there has been consistent demand for us to consider the right to die, as countries are creating policies around how this might work. Spain recently greenlighted euthanasia, following Belgium, Luxembourg, the Netherlands, Canada, and Colombia.45 The support for the right to die has to do with the fear of losing autonomy because of the partial failure of our minds or bodies robbing us of our dignity and agency.46 Consistent with the foundations of human rights for life, the right to die is also about autonomy, just as human rights protect the right to live life on one's own terms.
Given the discomfort between the main concerns of the human rights framework and the right to die, we don't have an easy analogy to draw on when it comes to thinking about the life of data. The fact that data can outlive our physical presence demands we think about how we might make decisions on what kinds of choices we all have. Beyond our own choices, we have to bear in mind that others will make choices about the data we leave behind as well.



Standing before the Uncanny Valley
We have an example of a curated, digital version of a person, thanks to veteran science reporter James Vlahos. He's the author of Talk to Me, a history of voice computing. In 2016, he created Dadbot to emulate his father when his father was diagnosed with late-stage cancer. On the heels of its success, he established a company called HereAfter AI "to give everyone the ability to immortalize their stories."47
Vlahos's motivation for Dadbot is quite moving. Even in recollection, his relationship with his father John is poignant, and it's clear his father was a riot. Here was a son who so admired and loved his father that he wanted desperately to find a way to hang on to the basic characteristics of his father to experience over and over again. Vlahos recorded hours of oral history as John was dying. He delved into standard topics such as childhood, education, and how he met his wife. In all, Vlahos collected 91,970 words from these sessions. After learning about a new machine learning-fueled chatbot developed by Google, he proposed the idea of Dadbot to his parents to see what they thought of the idea of preserving his dad's words in this way. In his words, Dadbot "would simply be to share my father's life story in a dynamic way."48
The work of creating Dadbot was tedious and time-consuming. Vlahos gives us some insight into the types of decisions he had to make to create it. We probably take for granted how most typical conversations go because they seem natural. To replicate this naturalness and make Dadbot conversational, Vlahos had to design hypothetical conversations about various topics, such as his dad's childhood, meeting his wife, and his career. He made a map of the topics but also utterances: down to "yes, no, maybe." Vlahos's Dadbot draws on his father's spoken words, the textual representation of a meticulously collected oral history, forming a script. Vlahos also wanted to capture the essence of his father, which would include unique contributions, such as insults like, "He flames from every orifice," or sarcasm, like, "Well, hot dribbling spit," in response to self-promotion. The bot had to be able to respond to the user's mood as well, not carrying on with toilet humor if the user is sad or depressed.
In short, Vlahos had to make a lot of painstaking, microlevel curation decisions through scripting Dadbot to make him realistic. In the hands of an admiring son, decisions about the bot's grammar (his father was a stickler for it) and sentence structure, decisions about how to edit the sometimes longer monologues of his dad's recollections, decisions about planning for his father's eventual demise with lines such as, "I wish I could be there to celebrate with you," were completed with care.49
From the successful creation of Dadbot, Vlahos went on to launch HereAfter AI to make the technology more widely available. Vlahos designs bots to speak in the voice of the person whose story it is: "People just want to hear that voice . . . what it's saying to some degree becomes immaterial."50 When people sign up, much as with Dadbot, there is an oral history interview that follows a typical memoir narrative. The data are uploaded into the system, and users can interact with the memories through an app that answers users' questions.
HereAfter AI opens up touching possibilities of connecting, through voice and video, to dead relatives. It's the kind of option that many of us could picture ourselves being persuaded by, remembering loved ones, with authenticity, something Vlahos emphasizes.51 It's one we can opt into. Maybe it's still subject to some glitches (awkward pauses, disconnected conversations) and technological vulnerabilities (what happens if Vlahos goes out of business or doesn't want to run the company anymore?), but we get a moving, interactive memory in exchange. It's an improved version of the videos we might make ourselves for others to remember us by when we're gone.
Roboticist Masahiro Mori put forth the concept of the "uncanny valley" in the 1970s to talk about the point at which humans feel uncomfortable with robots. Humans find robots more appealing as they become more humanoid, but only to a certain point in the "human-like" distribution. Too lifelike a robot creates the creepy feeling that Mori called "the uncanny valley."52 So how close are we getting to this valley?
HereAfter AI is a largely self-scripted, limited immortality that stands in contrast with the potential Microsoft vision discussed earlier. It is self-consciously about maintaining connections to family and friends. What makes it more realistic is that it is consent based by the person on whom the chatbot is based and their families. All of the parties involved care about the integrity of the product and how it measures up to the actual person. They have a stake in the process and the product.
The idea Microsoft has put on hold is much broader. What makes Microsoft's proposal of digital immortality unnerving or uncanny is the vacuuming up of social data about a person. Unless there is a consent mechanism built in, we can't curate if someone else submits data about us. Even if we consented, using such diverse sources of data (for example, images, voice data, social media posts, electronic messages) to speak for someone makes it hard for us to know what we're consenting to.53 What will our digital immortal say? Where do we end: At the data generated by us or at the algorithmic interpretation of those data? Who does the bot represent, us or some software development team's interpretation of what important aspects of our lives should be sifted out of the mundane to chat about? Are we in the uncanny valley already with our existing and potential technologies?


Equity and Access to Digital Immortality
Digital immortality is expensive. One obvious limitation is that the dead can't talk without a computer. The data need to be processed algorithmically, which requires either coding skills or paying for a service. As imagined in the 4evru scenario, a virtual reality version of a loved one could be in the cards, given the rapid advancement of such technologies and the expansion of the metaverse through Meta's investment. Maintaining one's immortality is also a costly endeavor54 and one fraught with possible anxiety if the firm providing the immortality services suddenly goes under, undergoes service quality changes, or is bought out.55 For those who might like a digital immortal version of themselves, not all will be able to access such a creation, drawing in questions about equity in a possible future with chatbots, holograms, and the virtual dead inhabiting our world.
So, for now, the people who have the means to partake in digital immortality are those with disposable incomes, whether it's a few dollars a month or a one-time fee (as with HereAfter AI),56 or the possibility of significantly investing in and modeling robotic prototypes (as in BINA48). The same old economic and social inequalities familiar in our physical lives follow us into digital immortality. The difficulty of accessing these technologies for those outside of the economic elite will likely persist for some time. Perhaps we will all be able to become digitally immortal one day, but those who had (or have survivors who have) resources will likely be better resourced as digital immortals.

*
What is at stake here for human rights is manifold, but two things pop out. First, digital immortality is a specific way for us to exercise autonomy over what happens to data about us when we die. We currently don't have a good way to control how the data from us live after they've been created, whether we're alive or dead. Digital immortality is a vivid illustration of how data are effectively forever.
There is an additional dimension to consider here. As we have seen, digital immortality is contingent on technologies of both the present and the future, our ability to direct the data about us, and our financial capacities. Digital immortality, like other types of resources, will not be evenly available to everyone in our societies. Yet digital immortality may become widespread. After all, the data exist, the technology will gradually improve, and there is both a need for helping people grieve and enough fascination with the afterlife that there should be demand.57
In addition to dignity and autonomy, digital immortality is also an access and equity issue. Implementers of human rights like education, health, and voting must consider equity and equality of access.58 All of these are currently protected by human rights. Articulating that education is a human right means that we have to design solutions to make sure that all humans have access to education. When they don't, human rights statutes and goals indicate that we ought to advocate for change and try to reform or construct new means by which to reach the hard-to-reach. If digital immortality is possible, it shouldn't just be for only the well resourced. It should be for all those who want to do this with the data about them, because everyone is effectively, or will be, datafied.
More to the point, all of us have a data profile in the web of digital networks, but only some of us can breathe life into it forever. Does this mean that digital immortals with whom future humans interact will be those of us who are educated with the means to live on after death? How does this speak to how humanity will be understood through these select immortals?
Second, digital immortality is heavily contingent and dependent on the facilitator of that immortality. When someone else "brings us to life," they have to make lots of decisions. Many of these decisions are generally mundane, the "ums," the jokes, the pauses, the randomness. James Vlahos was bringing his own father to life, someone he loved dearly and knew intimately. Even so, he had to make a bunch of speech decisions that shaped how the chatbot would function and feel to users; he had to decide how the bot ought to come across, how it displayed a humanity true to his father. As Vlahos asks, "How can I mitigate my own subjectivity as the bot's creator?" The digital immortals will inevitably bear the marks of their algorithmic (rather than content) creators. So is Dadbot a computer powered by his father's words . . . or is it his father? One can argue that if Dadbot is really "Dad," this dad should have rights, but then should the computer that holds other data from Dad not have rights?
Then comes the question of primary function. A digital immortal may be programmed such that it cannot take on new information easily. HereAfter AI is a digital memory bot: it's not designed to update to current events. Real people, however, do have opportunities to learn and adjust to new information. Microsoft's patent does specify that other data would be consulted and open the way for current events to infiltrate. This can be an improvement in that the bot doesn't increasingly sound like an irrelevant relic or a party gimmick. However, the more data the bot takes in, the more it is drifting away from the lived us and more toward an updated us that risks looking increasingly inauthentic. What would Abraham Lincoln say about contemporary race politics? Does it matter? People are, at least as we have understood when dead people did not speak to the living on a regular basis, situated in their time and place. Vlahos, reflecting on Dadbot and certain advancements in machine learning to better model human behavior, puts this dilemma well: "I can imagine talking to a Dadbot that incorporates all these advances. What I cannot fathom is how it will feel to do so."59 As AI bots get better at mimicking people, whom do they reflect? Their data source or the collector?
Which brings us to the question: How should we think about this digital immortal? Is this a "person" who therefore deserves human rights protections? Should we protect this person's freedom of expression, for example, or should we be able to shut it down if their expression (based on the actual person from a different time) is now hate speech in the era in which the immortal has persisted? What does it mean to protect the right to life of a digital immortal? Can a digital immortal be deleted by someone else? If the immortal is continually updated, will we all risk becoming Tay, trolled and ridiculed for learning from humans?
We are not accustomed to thinking about life and the end of life like this, especially in human rights terms. The "right to life, liberty, and security of person" is an essential right, and life is a state of being to enjoy other human rights.60 The possibility of digital immortality creates a disjuncture in how we typically understand the line between the living and dead. The gradual erasure of this line destabilizes the way we think about how we live when a part of us can endure and interact with the living when we're no longer physically alive. This is a fundamental human rights problem in that digital immortality demands we consider how autonomy, community, dignity, and equality play out in the continuation between life and death that such technologies enable.


An Old Question Made New
Life after death has been a question and fascination for thousands of years. Humans have grappled with their fears, grief, and intrigue through religious beliefs, burial rites, ceremonies commemorating the dead, spiritual movements, artistic imaginings, and technological efforts. Some sought to come back to life after their first death, contingent on the advance of medical science to revive the dead. For example, cryonics is a process that results in the deep freezing of dead bodies in liquid nitrogen with the hope that one day the technology is developed to bring these bodies back to life.61 If it works as intended, we should be able to come back again as ourselves, rejuvenated by science. Digital immortality freezes us too, as we were through data.
Love and death drive us to do some incredible things, such as building massive structures like the Taj Mahal or creating chatbots that mimic the voices of those dear to us. All the same, malice, envy, and anger can also drive us to do incredible things. Human technology has pushed us to the brink of changing the constraints of mortality on certain aspects of life, such as our interpersonal bonds, through the power of image and voice, for better or for worse. We need to ask how those changes affect human rights and how human rights can shed light on the risk of abusing our power to live posthumously, as it were.
What we've discussed in this chapter isn't exactly the stuff of the Singularity, which is about the development of a technological "superintelligence" such that it surpasses all human capabilities.62 Such conversations have gone so far as to have led some to declare the end of mortality, if one thinks that consciousness is data-based, and therefore can be put on a computer.63 But seeing as consciousness is itself an extremely big topic without many definitive answers and there are entire fields like cognitive science, philosophy, and psychology trying to figure out and measure the components of consciousness, we don't need to engage in a debate here about whether computers are capable of carrying on a person's consciousness through data.64
On a daily basis, data are collected about us that, when assembled, allow others to infer who we are, what we might be thinking at this very moment or sometime in the future, and what we might do, all with some probability of being correct. In some ways, we already do live in a computer (or, rather, many, many computers) through data. This means that, in death or in life, data about us exist independent of whether we continue to. The detachment between our lived experiences and the data about the most mundane of our behaviors is something we have to grapple with. Human rights tackle questions of autonomy and dignity for individuals, the equality of distribution of technologies that allow for digital immortality to be curated in a certain way, and opens up questions of how the human community is conceptualized if the living can continue interacting with the dead.
We've toured a number of options here: purposefully curated, interactive, digital manifestations on the one hand, and the more ethereal form of remaining in data sets churning out predictions long after they will have implications for us. Regardless of these choices, datafication has enabled us to we do live on, beyond our own awareness and mortality.
Without putting in place human rights against the unauthorized uses of our posthumous selves, we risk becoming holograms or, worse, digital immortals others have created. Digital immortality risks exacerbating the chasms between the haves and have-nots. As we've seen, we also risk creating another way that some have advantage over others.65
From a rights perspective, the answers are not easy.66 They will be, at the global level, mitigated by culturally specific and legally defined notions of death and what is a morally and socially acceptable way to "live on." In the meantime, there are practical questions. What happens to all our texts, digital photos, and emails when we're gone? How will our social media accounts be resolved? Europe's GDPR offers a wide range of privacy laws relating to data, but it does not cover dead people.67 China's Personal Information Protection Law (Article 49) allows descendants to deal with a dead person's data, including copying, correcting, and erasing data unless the person directed otherwise.68 The digital afterlife industry is a fast-growing area of business, but there are many unresolved questions regarding the rights we have over data when we're alive and dead.69 We also have questions as to whether the companies in the industry will continue. To date, many have already faded away.70
This chapter calls for us to recognize that the lives of data about us, from us, do not die when we do—and that is a human rights problem. Data rights issues persist in life, but issues don't end with death. We also see mundaneness of the data being collected for some of these digital immortality apps. They are akin to finding a grocery list or crumpled notes to self in a jacket pocket after someone has died—except that it's potentially every grocery list or note to self that that person has ever made.
The immortality of data is a social and political concern, even if there is a technical solution. Technical solutions need social and political guidance to include guardrails for human concerns. On the flip side, technology has created possibilities that are at the frontier of our social understandings of human life. This means that we have to have important conversations, now, about what it means to live through data.











6
Big Tech and Us


Initially, I think I viewed it as something "newfangled" that only the younger computer-generation used. . . . Then, like probably everybody, I started to become hooked as I saw just how expansive it is, and how much it seems to literally touch so many lives.
—Richard, 60+-year-old user of Facebook1
I'm here today because I believe Facebook's products harm children, stoke division and weaken our democracy.
—Frances Haugen, former Meta employee and whistleblower2

What is Facebook? With its official conglomerate name change to "Meta Platforms, Inc." Facebook is trying to put its messy 2021 behind it.3 The shift might finally lead to some clarity on the massive company that has largely been known simply as its first and most successful product. Despite the continued colloquial use of "Facebook" as shorthand for the company's plethora of products and services, here, unless we are specifically discussing the Facebook platform or Facebook as a company before the name change at the end of October 2021, "Meta" will be used to discuss the company more broadly.
Most of the readers of this book have probably been users of Meta products, whether it's Facebook Blue (the original social network), Instagram, Messenger, or WhatsApp. The odd juxtaposition of Facebook's extreme profitability with an ostensible sense of social purpose is a natural tension within the company. In a letter written to investors before Meta's initial public offering in 2012, founder Mark Zuckerberg wrote, "Facebook was not originally created to be a company. It was built to accomplish a social mission—to make the world more open and connected."4 To make the world more open and connected sounds like a good thing; at the very least it could be neutral. Technology companies have long maintained their "neutrality," sometimes insisting that it is the human users who inject good or bad into their otherwise ambivalent products.5 If companies indeed are merely turning unimaginable profits as champions of social good for humanity, that doesn't excuse them from responsibility for the social bad they cause in the process. To date, companies have been able to foreground the profit side. They could be doing so much more if they took seriously the idea that we, the people behind the data who the products are purportedly helping, are stakeholders and not just subjects of datafication.

Companies
The centrality of corporations in datafication is not a surprise. I've cited many others who have already expounded this point. In this chapter, I argue that holding corporations accountable to the ethos and full spectrum of human rights, and not just singular issues like the freedom of expression, in the age of digital technologies is absolutely paramount. International institutions like the United Nations have already begun doing so, but we need stronger, more robust declarations of corporate obligations to human rights. For example, the comprehensive normative framework for corporations, the UN's Guiding Principles on Business and Human Rights mentioned in chapter 2, emphasizes respect, and not obligation, to protect human rights.6 This seems incongruous with our reality. Corporations that infuse themselves in daily lived human experience through datafication and AI need to have and fulfill a higher responsibility of obligation toward human rights, and not just respect. It's time to take the next step through policy and inclusion of stakeholders.
Meta has become enmeshed in the lives of people the world over. In 2022, it counted 3.6 billion monthly users.7 It is the most successful social network in history. But is that all Meta is? For approximately six hours on October 4, 2021, the company's platforms went dark. With it went Facebook photo sharing, feeds, comments, and likes as well as the ability for millions of businesses to operate on Instagram and WhatsApp. It wiped out messaging worldwide for millions who rely on WhatsApp's free services to communicate. This included direct messages, of course, and the ability to conduct transactions and work in informal economies. It also compromised the ability to log into more than 100,000 other platforms, many of which use Meta credentials. From smart TVs to music, holiday bookings to news feeds, Meta's crash created wide-ranging problems.8
To put this in perspective, Facebook's US market penetration is relatively small, about 100 million users, or less than a third of the country's population. By contrast, Brazil and Mexico are the countries with the highest percentage penetration of the population of Facebook accounts, at 95 percent and 98 percent, respectively. India tops all other countries in absolute terms, with almost 330 million users in 2022.9 Still, in some places, Meta plays an even more outsize role. In Myanmar, Facebook is synonymous with the internet. Decades of repression and denial of access to basic telecommunications limited Burmese online access options. When Facebook entered the country, it gave away access for free, and thus it became the source of information for many Burmese.10 Myanmar is not alone in this; Meta is as good as the internet in many places because of high costs and other barriers to internet access. It is essentially the relevant parts of the internet for a large swath of humanity. Facebook explicitly positioned itself as the provider of free internet through its controversial—yet widespread—Free Basics program, discussed below. In other words, campaigns like #DeleteFacebook might be a position of privilege for those of us who don't depend on Meta for essential daily activities.11 Meta has become everyday infrastructure for the mundane activities in our lives, providing connections to other people and linkages to thousands upon thousands of other websites and apps that together create meaning on the internet.
In a world without these products, lives would be disrupted in ways that are not easily remedied; alternatives would need to take its place. People in developing countries depend on Meta in ways that those of us in developed countries don't always see.12 The company's ability to reach so many is possible because a lack of alternatives to provide people with basic internet access makes its services invaluable to so many users.
Meta is not the only one. Big Tech de facto regulates our behaviors and information through their platforms. Sometimes known as GAMAM—Google, Amazon, Meta, Apple, and Microsoft—or various other acronyms that include other platforms,13 US-based Big Tech companies have created seemingly indispensable services that guide our behavior. Throw into this mix Chinese firms such as Alibaba, Baidu, and Tencent, and we have covered Big Tech's infiltration of the great majority of the world's population.14 Big Tech, however, is not solely responsible for datafication; these companies are just the face of the datafication most people knowingly experience. Communications scholar Laura DeNardis insightfully emphasizes that digital technologies are so infused into the way we do things that there are no "tech" companies anymore because every company is a tech company. Firms that trade in commodities, services, and logistics all collect and sell data as part of doing business.15 The causal arrow also goes the other way. Journalist Christopher Mims documents how Amazon has fundamentally changed global logistics, from the way warehouses are organized, to shipping and courier practices, to the importance of the cardboard box.16
Corporate datafication has shifted human lives in drastic ways. They are governing us in practice. This chapter is about explicitly explaining their governance capacities. Datafication technologies are not just products; they are producing a mode of human existence that is running roughshod over the institutions we've established in previous decades and centuries. These institutions include human rights. Companies need to acknowledge their role in violating human rights and explain how they will take affirmative steps to remedy such violations. By doing so, they recognize that their sources of data aren't just subjects to be taken from, but stakeholders in the entire datafication process that has been historically profitable.
This chapter focuses on Meta. Like other Big Tech companies, Meta's platforms infuse many aspects of our lives. It makes for a good case study, however, because it has created very public vehicles for carrying out Zuckerberg's mission of connectivity. Critically, it has created an institution, the Oversight Board, in explicit recognition of its effect on the freedom of expression. It also has launched a number of initiatives for getting the developing world online. At the same time, it frequently makes headlines for its flagrant disregard for the repercussions of some of its choices. Perhaps most important, the Meta name change signals a pivot to the metaverse—an interconnected online world that has no real guardrails on the actions of those who create it because it doesn't yet exist.
If we are to consciously wield our stakes in the process of datafication, the time has come to assert our rights in this physical existence before more virtual technological advances sever us further from the gains we have made to date.17


What Is Meta?
Corey Madeup stares at the Instagram notification for about the tenth time that day, re-reading the text: "Your content has been removed by our technology for going against our guidelines." He's annoyed, and he's frustrated. He has read and re-read the Community Guidelines,18 and he just can't believe that a post of a woman's breast would get taken down.19 The algorithm must not have placed his photos in the context he's provided, which was in the context of breast cancer awareness. Secondarily, he's also worried that the content review process won't happen in time to reinstate the post for his class.
As part of a semester-long school project on disease awareness, Corey has chosen to investigate breast cancer, the disease that afflicts his mother, Claire. Students in his class were asked to design a social media campaign around the disease they chose, write up a research paper, and make a poster for a schoolwide event. It's a major project, and a lot of his final grade is riding on its success. Corey enjoys working on it. It gives him more closure about the fact that his mother will eventually succumb to the disease. He and Claire always have had a tight relationship. He throws himself into learning about the history and treatment of breast cancer, contacting NGOs and various research groups.
The project moves Corey more than he expected. He feels invested in helping not just his own family's processing of the disease, but others he meets through the work. After speaking to a number of survivors, Corey thought it was important to share how women's breasts are affected by the disease. He knows that his mother has struggled with her self-image in addition to the physical challenges of the disease and treatment. In collaboration with a local breast cancer charity, he collected anonymized images of women's breasts to post online. Some of these images showed women's nipples, others scars from mastectomies. All in all, he posted about ten images on Instagram. He also included some text from his research paper and poster for each post
Corey's immediate response to the removal of the images is that they're taken out of context. By their own Community Guidelines, the ways that Corey has used breast images should comport to Instagram's standards, but he has still been flagged. After this happens, some of Corey's classmates boast about their self-censorship to avoid algorithmic flagging for a school project, giving him a hard time for getting too invested in the cause. Corey kicks himself for testing the platform's rules and letting his newfound commitment override his instinct to play it safe for schoolwork.
The whole thing adds unnecessary stress to the assignment. He's relieved when the content is reposted a few days later, in time for his teacher to grade his campaign, and he moves on. But the episode is a reminder for Corey of the algorithms that mediate our interactions with the world.


The Power of Tech
What Meta does via its platforms, what it believes in, and what tweaks it makes to its algorithms have repercussions for billions. Therein lies Meta's tremendous power and tremendous responsibility. Meta and Alphabet, which owns Google, are two of the biggest drivers for datafication: the data they collect on people allow them to control the digital advertising market. The more data are collected from people, the greater the benefits for Meta and Google, and their advertisers benefit from the microlevel data both companies collect, driving up advertising revenue.20
We know that Meta has built its corporate fortunes not only on collecting and selling user data for advertising revenue, but also to develop products and create engagement.21 Its ostensible goal of "connecting people" is highly profitable. Meta (and other social media) can help create new social ties and strengthen existing ones. We know the societal cost of such success: directing and altering elections,22 the prosecution of minority populations in countries such as Myanmar and Ethiopia,23 and thwarting pandemic remedy campaigns.24 The social harms of social media are many, from lowering self-esteem to fomenting environments of misinformation.25
Human rights fit into this story in many ways. Much of the attention on Meta and other social media is how they affect the exercise of the freedom of expression. Freedom of expression affects our ability to have freedom of conscience and opinion.26 It affects our autonomy and our dignity because free expression is a key way to hold those in power accountable.27 Often the targets of accountability and advocacy are states that repress and censor citizens. We usually think states must allow individuals the space to exercise their freedom of expression, opinion, or conscience.
Today, many of us find that we are exercising such rights online on private platforms. States and companies have different rules and obligations regarding free speech. Companies have other incentives for preserving some speech and disallowing other content, even if both states and platform companies "govern." The fact that what we see in our social networks is mediated by profit-seeking algorithms changes freedom of expression by giving companies the power to arbitrate "allowable" content.28 Content moderation is downloaded to contract laborers who are given few guidelines from headquarters staffed by homogeneous workers.29 Because they are built on selling advertising, content that drives clicks, not knowledge, information, or sociality, is the priority.30 An independent civil rights audit of Facebook found that the company prioritized freedom of expression over any other human rights values, including nondiscrimination.31 Extreme emphasis on any single human right unsurprisingly leads to dysfunctional outcomes, since human rights are intended to be taken as a whole with the recognition that sometimes they conflict. If companies were responsible for the harms that the content on their platforms might cause and, more broadly, thought more carefully about the rights trade-offs inherent in their lines of business, we might witness different levels of care around the content Big Tech shows us. They might integrate more experts on human rights, or more people whose work focuses on issues of social justice and equity, in the development of technical and content moderation standards. This is unlikely to happen where the human rights burden on companies is "respect" and therefore voluntary.
In some ways, Meta has taken extraordinary steps by explicitly trying to create governance on its platforms. Originally created while the company was still called Facebook, the Oversight Board makes binding content decisions regarding what stays or goes on Facebook and Instagram.32 It has explicitly taken on the role of the sheriff to police content within its network, which we know is vast and, indeed, global.
But as I began this chapter in 2021, a number of developments transpired. Frances Haugen publicly blew the whistle on Facebook's ruthless profiteering on the backs of people's data in riveting testimony before the US Congress in October.33 Sheera Frenkel and Cecilia Kang published their painstaking journalistic account of Facebook's internal debates of how to manage all of the consequences of creating the world's largest social network.34 A US federal judge told the Federal Trade Commission and forty states that they had failed to show that Facebook had a monopoly on social networking.35 And for close to six hours, we saw what the world would look like if Facebook, WhatsApp, and Instagram went away when its internal infrastructure failed and the platforms went dark.36
It is clear that Meta is everyday infrastructure. Its mundaneness is the key to its success and staying power. We do not have the tools, human rights or otherwise, to consider how to rein in such powerful companies. And this is why we need to become stakeholders in datafication, now more than ever before.
Following leading law professor Tim Wu's thesis that information technology is different from other kinds of technologies, we can argue that Meta's power is greater than—or at least, qualitatively different from—previously powerful corporations in sectors like oil, railroads, and even telephony.37 Our most intimate, mundane, and treasured specifics are chopped up and pooled with others' data, presenting different realities depending on where we fall in the data distribution. Standard Oil may have gouged us, but it didn't make individuals feel isolated from the rest of society or alter our ability to know the truth. The Bell Telephone Company only controlled some of the content we heard and saw. No other technologies were able to restrict what we discover based on location, interests, and device. Trafficking in information, especially in the age of datafication, is totalizing. And if Wu is right and indeed all industries follow a cycle of openness to closedness, from hobby to monopoly, we are already in the midst of a world of profound corporate governance in information and communication. Company policies and algorithms are feeding us what we know and how we know it. By extension, they also inform what we say and how we say it through character counts, community standards, and actions such as "like" buttons.


Governing Is Power
It is a common mistake to presume that only governments can or should govern. Any entity that has control over the rules determining the distribution of any good or service or actually implements that distribution is governing.38 Political scientists are interested in power, which is often defined where "A has power over B to the extent that he can get B to do something that B would not otherwise do."39 One form of power is coercion, which is typically thought about as violence or physical force. Coercion contrasts with authority, a kind of power based in legitimacy.40 Without coercive capacity, one can still change the actions of others by being seen as authoritative. In international politics, we tend to think that states have both coercive and authoritative power. Within their territories, states have weapons and specialized personnel to exercise legitimate violence.41 Yet their authority also stems from the law, their ability to provide in the public interest, and our delegation of power to them. Thus, we often call states "public authorities." States are accountable to those they govern.
Yet there are many instances of governance without public authorities.42 Other "nonstate" actors exercise private authority. Political scientist Jessica F. Green defines private authority as "situations in which nonstate actors make rules or set standards that other actors in world politics adopt."43 In our context, private authority is how nonstate actors, namely corporations, have changed the way we behave on a global scale. Political scientists Deborah D. Avant, Martha Finnemore, and Susan K. Sell identify "global governors" as "authorities who exercise power across borders for purposes of affecting policy . . . [they] create issues, set agendas, establish and implement rules or programs, and evaluate and/or adjudicate outcomes."44
Given these definitions, Big Tech governs, exerting authority through technology. It creates, distributes, and makes rules over how goods and services work. As private entities, Big Tech companies have private interests, not public interests or governing capacity. Its interests are reflected in their platforms. The array of videos on the TikTok For You feed isn't by accident; it's based on what its algorithms think you want to see, based on what it knows about you. One could argue that we consent to Big Tech's governance through our use of their offerings, which has the effect of delegating them the power to govern our interactions with others. Because global governors create their own mandates, they can expand to areas they seek to go until told otherwise. There's no law or reason that says Meta should have power over the freedom of expression, but it does because it created platforms on which people share content. The way companies do so is a self-reinforcing cycle where our engagement creates more and more power for Big Tech. For instance, Meta controls the algorithms and gathers the user data about our use of their platforms that in turn affect how we enjoy that content. It has authority over usage because it controls the platform, which determines how we exercise our freedom to express our ideas on that platform.
Political science orthodoxy is helpful for thinking about why and how these companies might become governors. Nonetheless, there are some limitations to applying these theoretical frameworks to Big Tech, because the concepts don't quite capture the globalized control over social and political life that these companies exercise. It seems a bit wonky to say that platforms have "authority" in the sense of legitimacy, even though they seem to check all the boxes of definitions of "private authority." Big Tech is not merely "affecting" policy, making rules, or setting technical standards.45 It is infrastructure: companies providing basic, increasingly vital means for humans to interact and survive.
Moreover, what makes a tech company like Google "legitimate"? Its search engine is highly efficient and effective at finding things online, and in the early days of search, it was able to corner the market with its ranking algorithm.46 What gives Google authority isn't that it's more rightful than Yahoo!, DuckDuckGo, or Bing (by Microsoft). It is how its platform has shaped the contours of the internet. That we use it gives Google some legitimacy. But do we have a real choice in the search market? As we think about this, this seems more like power in the sense of taking away our choices. What kind of power do tech companies have, when they have created the architectures that then shape our worlds and constrain our choices?
On the one hand, Big Tech can be said to exercise coercion, not with guns, bombs, and sanctions, but with code, friendly interfaces, and useful products. Importantly, Big Tech's appeal is not about what bad things could happen if you don't use their wares, but what you can have if you do. Through our technical interactions, companies become authoritative as they integrate with our lives, determining our choices through nudges, and mediating (perhaps controlling) who we are and what we can do. Political scientists have considered this kind of dynamic as "structural power" and theorized its importance, but until datafication, few phenomena made such power so obvious.47
One recent example is the Google-Apple partnership to create the framework and protocol called Exposure Notifications, for COVID-19 exposure notification apps at the beginning of the pandemic.48 Though there are alternatives, eighteen of forty-nine government-based COVID-19 apps were built based at least in part on Exposure Notifications.49 Google and Apple's market share in cell phones was about 99.4 percent in January 2022.50 These corporations are making, disseminating, and enforcing the rules.
Thus, when Big Tech regulates freedom of expression, it does so de facto much more often than is explicitly expressed, as in the case of the Oversight Board. Freedom of expression has to do with content, but also how that content is communicated. Because platforms enable our very access to information, it's not just "freedom of expression" but everything else that precedes and follows expression. It affects our right to education and knowledge and our abilities to freely assemble. This reach shapes who we are as individuals and collectives. It affects our autonomy and dignity by constraining what we know, how we know it, and how we talk about our knowledge. Think about how the world is shaped depending on the platform on which you engage. By design, Facebook pages accentuate things about you different from the information in your profiles on LinkedIn or Twitter. International lawyer and former UN special rapporteur David Kaye points out that these private, corporate forces make decisions that go beyond branding, to controlling the very spaces in which information emerges and flourishes.51
Looking at Meta more closely gives us insight into GAMAM and other platforms that allow users to share user-based content like Snap, Twitch, and Reddit. Meta is but one company whose services facilitate and govern freedom of expression. These platforms are the infrastructure that allows users to experience and access the internet.
A lot has been said about Meta, so what's the payoff here? It's political synthesis. We pull together the many threads others have started to weave about the effect of Meta on freedom of expression by understanding it as an issue of governance, power, and legitimacy. If Big Tech governs us through their platforms, they have obligations to us, just as governments do to their citizens. Big Tech is big, not just economically, but also socially and politically. These companies should be responsible for the human rights they affect, which seem to be increasing in scope as new technologies emerge.
Consider Meta's Free Basics program, a free internet service that tries to bring the rest of the world online. On the face of it, Free Basics seems to be consistent with the many demands for access to the internet as a basic human right that have only grown louder with COVID-19.52 The reality is murkier. In the language of political scientists, Big Tech governs in two ways: they control the rules of the platform and, in some parts of the world, access to the world beyond the platform itself. We'll tease out implications of the Oversight Board for human rights and the global governance of our rights. We'll also look at why focusing so much on freedom of expression blinds us to the other ways in which infrastructure shapes our lives and why the full spectrum of human rights needs to be considered.
Before continuing, I imagine some responses to this chapter. Doesn't improved content moderation resolve some of these concerns about human rights? Sure, but content moderation treats the symptoms of a platform that allows for anything to be expressed. It is inherently reactive and not prospective. What about increased transparency and accountability?53 These efforts go only so far because what we demand Meta shares and how we decide what to hold Meta accountable for are currently undecided. We have to think more broadly about the implications of private actors providing content and access to what is rapidly becoming an enabling condition for human dignity, autonomy, equality, and community: the internet as provided through platforms.


What Makes Big Tech, Big?
What makes Big Tech "big"? The most obvious answer is financial figures. The 2022 market capitalization figures show that GAMAM are some of the most valuable companies in the world. As laid out in table 6.1, the biggest Big Tech are mostly topping the trillion-dollar mark: $2.5 trillion USD (Apple), $1.8 trillion USD (Microsoft), $1.2 trillion USD (Amazon), $1.3 trillion USD (Alphabet/Google), and $390 billion USD (Meta). Saudi Aramco, whose market cap is $2.1 trillion USD is one of the few nontech companies in the top ten.
But there are other ways to think about the dominance of GAMAM besides financial figures. An alternative is to gauge how much GAMAM affect politics. The data released by the Federal Election Commission for the 2020 presidential election regarding campaign contributions to recent US elections show clearly that GAMAM contributions were in Joe Biden's favor.54 Social media companies also play a large role in how election resources are distributed. During the Brexit referendum in the United Kingdom, nearly 25 percent of marketing and media resources was spent on social media and data analytics.55
GAMAM affects what we know about current events. Google and Facebook exercise a huge influence on how we access the news and therefore what we know. One study on traffic to major New Zealand news outlets shows that nearly a quarter of the traffic came from social media sites (mainly Facebook and Twitter) and that Google drove 67 percent of it. The same study, however, found that Facebook referrals to news sites have been declining since the 2010s, dropping from about 30 percent of visits to American and European outlets in 2016 to only about 10 percent by 2018.56
Table 6.1
Facts about GAMAM



Company


Market Capitalization (trillion USD)


Revenue (billion USD)


Net Income (billion USD)


Most Visited Sites


Number of Visits (in millions)




Alphabet


1.32


69.69


16.00


Google Sites


259.32




Amazon


1.24


121.23


-2.03


Amazon Sites


229.65




Apple


2.52


82.96


19.44


Apple Inc.


179.12




Meta


0.39


28.82


6.69


Facebook


236.94




Microsoft


1.81


51.87


16.74


Microsoft Sites


236.61



Sources: J. Clement, "Most Visited Multi-Platform U.S. Web Properties 2022," Statista, September 7, 2022, https://www.statista.com/statistics/271412/most-visited-us-web-properties-based-on-number-of-visitors/; "NASDAQ," Google Finance, accessed September 20, 2022, https://www.google.com/finance/.
Besides the news, GAMAM also influences research.57 One study found that an astounding 97 percent of computer science researchers working on ethics at some of the top programs in the world—MIT, Stanford, UC Berkeley, and University of Toronto—have received support from Big Tech or have worked for a Big Tech company. The authors of the study reflected on the similarities between the influence of Big Tech (and Big Tobacco in an earlier time) on academic researchers. The influence goes beyond academic research aimed at fellow computer scientists. Of the seventeen position papers on the social and ethical consequences of artificial intelligence published in two of the top general scientific outlets, Nature and Science, ten had at least one author who had been supported by Big Tech. While the scope of Big Tech goes further than GAMAM, all five companies are included among the sources of funding the authors studied.58
Big Tech increasingly defines who we are. They dominate the world financially. By having a hand in the topics explored in academic research, they shape what we know. For example, some of their own research is about ethics and AI, which can be a challenge as Big Tech wants to make money on their products. This ambivalence was perhaps best captured in late 2020, when star computer scientist Timnit Gebru was controversially fired from Google for pushing back on a paper she had coauthored with some colleagues about the risks of large language models, a technology Google has been competing for the edge in.59 She has since launched the Distributed Artificial Intelligence Research Institute (DAIR)—an independent research institute purposefully shunning Big Tech's money—to focus on the harms of AI on marginalized groups.60 As one article summarizes, Gebru felt "she was more successful at changing Google's policies by publishing papers that were embraced externally by academics, regulators and journalists, rather than raising her concerns internally about bias, fairness, and responsibility."61
Beyond information curation, GAMAM infrastructure projects indicate how they are attempting to redefine at a structural level ways in which we access the internet. Some of them are quite creative. Google's now discontinued Project Loon, for instance, was meant to dramatically increase internet connectivity using balloons in the stratosphere instead of traditional earth-bound infrastructures.62 Meta's Free Basics is a bit more down-to-earth in its attempts to increase connectivity through free basic access to online services.


Free Basics
Meta wants people to use Meta. The more people use it, the more data the company gathers, the more it can offer its advertisers. It has (as Facebook) and will give away internet access to achieve this. As media scholar Toussaint Nothias carefully chronicles, Facebook's Free Basics followed a previous effort, Facebook Zero, to offer a text-only version of Facebook to help users avoid mobile data charges.63 Facebook's own framing is more humanist: all of these efforts are in the name of providing greater global internet access to the billions who don't have it. Zuckerberg has long made it part of his agenda to promote high-quality, stable access to the internet as a human right, and in so doing, he joins many civil society organizations and international declarations, including from the UN, in their demand for the digital life.64 This demand has only intensified since the COVID-19 pandemic severely reduced and permanently altered in-person, human activity in many places.65
In 2013, he launched internet.org with six partners.66 Internet.org followed on the heels of a manifesto that was released as a white paper, "Is Connectivity a Human Right?"67 This was the beginning of the quest to make the internet as integral to every person's livelihood as the right to food and education. Purportedly, the effect wasn't about making money—after all, the people who needed internet.org had little to spare—but because everyone is entitled to internet access as a basic human right.68 Public statements emphasized the social good of the project, with then-Facebook chief operating officer Sheryl Sandberg saying in 2014, "When we've been accused of doing this for our own profit, the joke we have is, God, if we were trying to maximize profits, we have a long list of ad products to build! We'd have to work our way pretty far down that list before we got to this."69
 It turned out Internet.org wasn't all about providing free internet as a human right. Instead, Facebook would curate the internet through an app. Internet.org would provide only an internet populated by partner sites and Facebook. For the plan to work, the telecommunications companies in various countries would have to agree to provide access to data for free. Internet.org was the opposite of "net neutrality": Facebook would decide what appeared on its version of the Internet, under the cover of a service directed to those who struggled most to get online.
Internet.org was met with some stiff resistance. An attempt to rebrand as Free Basics in 2015 didn't do much to quell critics accusing Facebook of "digital colonialism."70 Some argued that offering a truncated version of the internet was not contributing much to the grand goals of connectivity Zuckerberg was preaching.71 Nonprofit journalism network Global Voices noted in a report that selecting some apps to offer for free violated tenets of net neutrality by prioritizing partner-providers' content and neglecting or severely degrading everyone else's.72
The debate over net neutrality was a rallying point for opposition to Free Basics in India. In 2016, India banned Free Basics after sustained civil society resistance from the Save the Internet Coalition.73 India's resistance and expulsion of Free Basics was largely an exception, however.74 As of 2020, Free Basics has a presence in over fifty-five countries.75 Unfortunately, there is no good source to come by consistent numbers of people connected to the internet as a result of Free Basics, but according to Facebook's own quarterly report in 2018, it claimed 100 million people had access to the internet through Free Basics.76
In 2020, Facebook launched the Discover program, which is also an app and mobile platform intended to replace Free Basics.77 Presumably learning from the criticism launched against Free Basics, Facebook claimed that Discover is a stripped-down, text-based version of the entire internet, with a daily data cap.78 Routing connections through a proxy server, it essentially redacts data-heavy content. It is an attempt to provide net neutrality. Early research found that Facebook-related services (such as Instagram) were far less stripped down than other services, such as Alphabet-owned YouTube. Further, a popularity-based recommendation system sifts and determines which ten websites are presented for each subcategory (for example, business and finance, education, government, health, popular, coronavirus).79 Critics pointed out that Facebook couldn't control how websites will load, and the process has rendered many pages, including ecommerce ones, unusable.80 Presumably in response to this feedback, Discover addressed some concerns, including the curation of websites.81
In the study by Global Voices, researchers found that using Free Basics meant that Facebook was gathering data on people's usage, regardless of whether they had a Facebook account.82 Unsurprisingly, Meta can use these data, and it also reserved the right to share the data with unspecified third parties. Furthermore, in the six countries in which the study was done, a plurality of the apps that loaded up were US based. Some countries had more offerings from local (domestic) companies but not always.
With the shift to the new branding as Meta, the company continues its role in providing infrastructure. Through these apps aimed at developing countries, it became the network in those places. This goes beyond facilitating communications via Facebook, Instagram, Messenger, and WhatsApp, where people ostensibly have choice. When you become everyday networked economic, social, and political infrastructure, you have created an institution in social science. Institutions "are the humanly devised constraints that structure political, economic, and social interaction."83 In the context of Big Tech, it is useful to think about how infrastructure institutionalizes as the user interfaces, processors, algorithms, and servers, but there is also real infrastructure with the fiber cables, cell towers, and power lines that enable the internet. Taken together, the different levels of infrastructure undergird and enable our datafied lives.
The connectivity Meta provides is infrastructure: the means for economic, social, and political relationships. Meta makes money by expanding the infrastructure's connectivity. The more people Meta can bring on to the internet and its platforms, the better for the company. Thus, it needs to attract and keep as wide a range of users as possible, as the quote from Facebook user Richard that opens the chapter attests. As Lev Grossman wrote in a largely friendly piece on Facebook's connectivity efforts in 2014: "Facebook's membership [at the time] is already almost half the size of the internet. [Facebook] . . . is made of people interconnected with each other—and it always needs more of us."84 Frances Haugen's testimony before the Senate in 2021 speaks to how much Meta thinks ahead to adding to its user base to ensure plenty of "material" for its products.85 As Haugen tells us, Facebook time and again prioritized profits over people.86
It's clear that no matter what Meta's marketing states, it needs data. Data make for profitability. Meta only gets data if people use it. This dynamic seems to be only intensifying as Facebook's 2021 rebranding as Meta signals an even more data-intensive and immersive era of the metaverse. Kent Bye, who hosts the Voices of VR podcast, observed that the technology will advance such that "they'll understand what [we]'re thinking because they're . . . essentially reading [our] mind."87 Although we don't have a "right to our mind," we do have a freedom of conscience, which should allow each of us to have some unfettered thinking and ability to form beliefs for ourselves.
Would people accept Meta's free offerings if they knew all of the costs? Sure, those offerings don't cost money, but what is the cost in terms of our identity and humanity? In fact, it's offering the internet on its own terms in order to extend its reach into untapped markets, for its own (and its partners') benefit. If it's giving away core means to access its products (the internet), this is consistent with what it has sought to do: profiting off the data fruits of connectivity.
While in common parlance, we often confuse "the web" with "the internet,"88 for nearly half the population in Myanmar (and other countries), the only entity for anything online is Facebook. Facebook's penetration is so profound that it was used to spread false stories about the minority Rohingya, a factor that has been widely acknowledged to have accelerated the effectiveness of the government's ethnic cleansing efforts against the Rohingya.89 All of this has come without Meta meaningfully taking social and political responsibility for what it means to enable so many people to access the digital world.90 Content moderation is imperfect but mistakes in moderation can lead to death and violence.
Although there has been pressure from states for the company to act, much of the inaction is nonstate. In late 2021, a group of Rohingya sued Meta in California for its role in fomenting genocide, demanding $150 billion in damages. This case was dismissed in 2022 by a judge, and as of writing, the case has not been refiled.91 In the meantime, human rights NGO Amnesty International has issued a call for reparations to be paid by Meta to the Rohingya.92
Meta's ascension as everyday infrastructure has prioritized user engagement. It wants logins and time spent on its platforms. Its desire to be the dominant platform in the internet space has meant that the norms that have emerged from its efforts have had huge effects on a global public, but we cannot say that they have been public oriented. Having a public orientation means serving the public's interest beyond delivering products on the market. It means (ideally) preventing harms and redressing them when they occur. Public authorities, like governments, make decisions in the collective interest and choose distribution patterns for goods and services on market logics, but also on social justice, security, equity, and human rights logics, to name a few.


The Politics of Platforms
Free Basics provides infrastructure to those who don't have internet. But Facebook has gone beyond creating and regulating access. It has also sought to define its own human rights obligations through initiatives like the Oversight Board, adopting a human rights policy embedded in existing international human rights law and issuing a highly criticized human rights impact assessment for 2020-2021.93
Among social media platforms, Meta is exceptional in the sheer numbers of users and the variety of places that use its products. Any decision it makes on acceptable content or otherwise, if not properly nuanced, will hit users around the world in the same way. To ban hate speech, one needs to consider, in addition to the actual content of the speech, who the speaker is and who the audience is. In short, content moderation for a global platform needs deeply embedded, local moderators who represent a variety of views from particular locations.94 Platform governance is the new "global governance," with a whole host of different kinds of global governors whose roles span our traditional concepts and boundaries.95
Second, although Meta is a corporation, not a government, it governs. It is a global governor that wields authority based on privately created and owned services.96 Meta has the power to govern because people use what it has to offer. Though the sources of its authority are owned privately, its effects are public in that what we each individually post is made available to others in our network, and potentially the world (certainly the world of Facebook users).
But Meta is also public in that it fulfills a public communication service for those in power, for governments and their representatives at all levels. Social media companies are crucial to leaders for explaining policies and creating support. The protest over President Donald Trump's permanent banning from Twitter yielded unlikely allies, from former German Chancellor Angela Merkel to US Senator Bernie Sanders.97 And with its decisions about content, algorithmic sorting, and data collection, Meta and other platforms make countless decisions that have affected and continue to affect the public.
Third, Meta, like the other GAMAM companies, offers a number of platforms. Defining "platform" is somewhat controversial, as there are two components to "knowing" a platform. The first is that it is based in code, that is, as Netscape cofounder Marc Andreessen writes, "A 'platform' is a system that can be programmed and therefore customized by outside developers—users."98 But a platform also has political and sociological effects that stem from that code, as lawyer Lawrence Lessig long ago established.99 A more accurate way to think about platforms is that they contain content that they did not themselves create but are organizing. This is what Facebook, Instagram, Pinterest, TikTok, Twitter, and other social media platforms do: they don't create content, but they do distribute that content and decide who sees what, when. Today, platforms provide a place for social interactions. They govern what happens on them through moderation and their very architectures.100 If platforms seek engagement, they try to keep their user base as large as possible.101
Since 1996, platforms (at least those based in the United States) have been governed by section 230 of the Communications Decency Act. This legislation has been interpreted such that intermediaries hosting online content are not liable for what their users say. Intermediaries can provide content on their platforms without creating the liabilities of becoming a "publisher." In short, section 230 has allowed US-based platforms to have their cake and eat it too: they can do nothing and be nonliable for any activity happening under their auspices.102 But section 230 was created for an internet that was very different from the one we have today. Section 230 has turned out to be central to the business model of companies that rely on user-provided content, which has been the driving force of Web 2.0. Any change to the letter or interpretation of this law would have major consequences for how these large companies function. In turn, changes to section 230 would potentially have wide-ranging effects on user experiences. As this book goes to press, the US Supreme Court is considering Gonzalez v. Google and Twitter v. Taamneh, both of which ask whether companies can be held liable for problematic content on their platforms.
All platforms moderate because they make decisions on how their platform is used. Even the user-centered, platform Reddit has moderated content to much controversy because it showed that participant "redditors" didn't fully determine what could appear on the platform.103 Typically, platforms downplay moderation in favor of emphasizing the connective possibilities.104 Yet their moderation functions, or governance of their platforms, are core to users' experiences.
Finally, to sum up the consequences of the previous points, the scale of these platforms is truly different from any other governor we've ever had. Even for the relatively "small" Twitter at 206 million daily users,105 it means that if it can go wrong, it will. As Del Harvey, Twitter's vice president of trust and safety said in a TED talk, "Given the scale that Twitter is at, a one-in-a-million chance happens 500 times a day."106
In this context, Meta has done something unique: it has created an independent body that will, in part, tell the company what to do with content, and it will be able to advise on relevant policies: the Oversight Board (OB). Distinguishing itself from other platforms that regulate as infrastructure, Meta has explicitly stepped into governance with a policymaking body.


The Oversight Board: What Exactly Is It?
Harvard law professor Noah Feldman had an idea that would make Facebook very different from its competitors. Feldman is an expert on constitutional law and "power and ethics, design of innovative governance solutions, law and religion, and the history of legal ideas."107 In other words, he is an expert about the judiciary and its importance to democratic systems like the United States. He is also a college friend of former Meta chief operating officer Sheryl Sandberg. On a visit to her house in 2018, he came up with the idea of a "supreme court" for Facebook—what became the OB.108
This idea made sense to Zuckerberg, who controls over half the voting shares of Meta, and he shepherded the OB through.109 As he has commented in the past, "You know, I said a bunch of times that I just think that it's not sustainable over time for one person [Zuckerberg] or even to one company's operations to be making so many decisions balancing free expression, of safety at this scale."110 After consulting with 650 people in eighty-eight countries through six in-depth workshops and twenty-two roundtables, Facebook announced the creation of the OB in 2019.111
These consultations reflect the enormity of the task Facebook has taken on. Although we say that human rights apply universally and are applicable to all human beings in the world, their implementation has always varied depending on where in the world. This is, in fact, an openness that is built into the international human rights framework, allowing for countries to state reservations about particular rights while still signing on to the idea of human rights as a whole.112 It is flexibility that enables human rights to appeal and apply globally. To illustrate, both France and the United States adhere to the freedom of religion. France has defined this as legislating away symbols of religiosity, which has largely had an impact on Muslim practices.113 The US Constitution's First Amendment and its establishment clause prevent any government interference regarding religion.114 What is "appropriate" exercise of freedom of expression quickly becomes a mess, as Facebook found out. Freedom of expression differs greatly depending on where we are in the world. What, for example, is "incitement to violence?," and who should be limiting speech that incites violence?
New York-based comedian Marcia Belsky, whose work has been described as "absurdist feminist comedy," posted a photo of herself as a child with the speech bubble "Kill all men" on Facebook. It was removed on the basis of being "hate speech," which violates Facebook's Community Standards.115 In the context of antifemale, antiminority trolling in the United States, Belsky's joke is comedy. But does it travel? Meta used Belsky's post as an example during its consultations in multiple cities worldwide. Most people in its New York consultation thought it should be left up. Most people in Nairobi felt the opposite.116 An activist named Berhan Taye, who contextualized how speech on Facebook has been leveraged to encourage violence in her home country of Ethiopia, summed this up as, "You know, it's funny, but, you know, humor is a luxury."117
It was in this context that the OB opened for business on October 22, 2020.118

The Oversight Board: Nuts and Bolts
The initial iteration of the OB is deceivingly simple. There is a publicly available governing document, the Bylaws. Starting with twenty (eventually "likely" to be forty) members, the OB will "exercise neutral, independent judgement and render decisions impartially."119 Meta put up $130 million in a trust to fund the OB.120 Cases come to the board by user appeal of removals or continued posting of material, or via referral directly from Facebook. The OB makes decisions based on Facebook's Community Standards and "human rights norms around freedom of expression."121 As one can imagine, there are far more potential cases than what the OB can take on. Case decisions on the content (remove/reinstate) are binding unless they are illegal in accordance with existing law. The OB can also make policy recommendations that Meta takes under consideration.
As of February 2023, the OB has decided thirty-four cases (with one mooted and one withdrawn), with a total of thirty-eight cases in its docket. Most of the cases come from user appeals: only ten were sent by referral from the company. Twenty-nine of the cases concern Facebook content, and ten concern Instagram. The OB has overturned Facebook and Instagram's original decisions three times as many times as it has upheld them.122
Response to the OB was initially mixed.123 Early critics thought it was too limited in terms of scope and decision-making latitude.124 Others have likened its role to international human rights tribunals.125 Careful optimism was pitted against outright cynicism when activists formed the "Real Facebook Oversight Board" in September 2020, claiming "real oversight" to contrast the official-but-fake oversight of the OB.126 Until there was a substantial collection of decisions, however, people often imagined the worst intentions, indicating more about perceptions of Meta (or possibly Zuckerberg) as a company than the OB.
The most notorious and important case to date was evaluating the decision to ban (now former) President Donald Trump from Facebook after the January 6 riots attacking the Capitol. In general, the OB has dealt with hate speech most often in cases (twelve of them), but it has also touched on dangerous individuals and organizations (a Facebook Community Standards category, eight cases) and violence and incitement (nine cases). Freedom of expression has been explicitly considered in all decisions, but it is not the only international human right referenced. Most cases reference multiple human rights, including right to nondiscrimination, security of person, and life, which speaks to how the OB has extended Meta's governance reach beyond the explicit goal of protecting freedom of speech.127 Table 6.2 shows how many different human rights concerns the OB considers. Given the number of cases decided to date, the list of rights referenced shows potential for breadth, how interconnected human rights can be, and the necessarily amorphous mandate of the OB as it grapples with the human rights implications of Meta platforms.
After the OB decision around Trump's suspension and Facebook's compliance with that decision, the tone from observers turned more positive.128 In May 2021, the OB decided that Facebook could not indefinitely suspend Trump without any publicly available justification, and it gave the company six months to figure out the appropriate policy. By June, Facebook had articulated a clear set of standards it would apply to Trump, who received a two-year suspension on all Facebook-owned platforms, the maximum in a newly minted set of public protocols.129 Not only did Facebook follow the OB's decision (as it is supposed to), it also committed to implementing fifteen of the nineteen policy recommendations made by the OB..



Global Governors
As a subscriber to Wired magazine, every Friday I get a newsletter, "Plain Text," from respected tech reporter Steven Levy. Usually it's on a topic I have passing interest in, but as this book was being written, Russia invaded Ukraine and sparked an ongoing war that started in March 2022. On March 4, 2022, Levy writes, "These platforms are scarily intertwined in the body politic and the global economic machinery. Cooperation with state-issued requests that skirt established corporate policies could set a troubling precedent."130
Table 6.2
Human rights explicitly considered in OB decisions*




Human Rights Standard Referenced


Number of Cases


Percent of Cases






Right to freedom of expression


34


100%




Right to nondiscrimination


20


59%




Right to life


12


35%




Right to security of person


10


29%




Right to health


7


21%




Right to remedy


7


21%




Rights of the child


5


15%




Right to privacy


5


15%




Right to take part in cultural life


4


12%




Responsibilities of business


2


6%




Right of peaceful assembly


2


6%




Prohibition on torture, cruel, inhuman, or degrading treatment or punishment


2


6%




Right(s) to be informed in the context of access to justice**


2


6%




Right to freedom of religion and belief


1


3%




Rights of women


1


3%




Rights of Indigenous people


1


3%




Participation in public affairs and right to vote


1


3%




Right to education


1


3%




*Decisions made public from January 2021-February 7, 2023.
**In Lawfare's "An Empirical Look at the Facebook Oversight Board," this right is listed as plural (rights) and singular (right) in separate categories. Here I have combined them because they are substantively the same right.
I thought about these words coming from a well-known chronicler of Big Tech. Big Tech played an especially visible role early on in resisting Russia's violation of international law and norms on state sovereignty. Levy's reporting over the years has helped us grasp what tech companies were thinking and doing. It struck me as surprising that Levy wrote that the Ukrainian invasion resulted in the observation that the platforms are integral to our global political and economic systems. This seemed obvious from Levy's past writings; by design, they must be to succeed. Seeking to be the world-dominant (insert product or service here) necessarily means that a company seeks to intervene and insert itself as a global governor.
What's changed is maybe how obvious corporate global governorship is. We're all interacting on the same platforms now. What President Volodymyr Zelensky says on Twitter becomes iconic worldwide in part because of the algorithmic settings that one platform has chosen. Being a global governor also means that what we do affects the distribution of goods and services. It means that we influence the way things work in the world through policy, but also through norms and practices. GAMAM are global governors because they provide goods and services globally. Through their ubiquity, they are also changing the way we live our lives in their and our datafied world. While shifts in policy to respond to this have been slow, their effects on our lives have been fairly swift and enduring.
Meta, as one example, has moved from being a platform linking us to our friends to a global provider of internet access and governor of freedom of expression. How could we have thought otherwise, though, as we watched Facebook and Instagram grow? The OB is a natural outgrowth of the status of Meta's platforms as infrastructure. It has massive resonance for freedom of expression online because of the global reach of these platforms. Covering nearly half of the world's population—and growing—is a rare feat. Meta has successfully molded public, global norms around politics, economics, and society through everyday infrastructure. The OB and Meta have changed the way the world thinks about freedom of expression and other human rights, as we see in table 6.2. It is not unreasonable to think that other social media platforms might follow with their own review bodies. It is certain that other platforms norms will be affected by what Meta does.
In his expansive book, communications scholar Nikos Smyrnaios demonstrates that permissive, deregulatory policies in the late twentieth century and early twenty-first enabled the rise of GAMAM and Big Tech more generally. He argues that internet-based companies tend toward the creation of monopolies or, more accurately, oligopolies that we observe.131 In my own work, my coauthors and I argue that it is an inherent quality of data that creates the propensity toward the concentration of power in a winner-take-all fashion.132
Given this power-hoarding tendency, global tech governors, for all their influence, also need to be governed. This is what we mean by checks and balances: for every power that one entity has, there should be an equal check on its power so as to prevent abuses and balance the system generally. GAMAM continues to govern our lives, collecting data from us, creating new products we might enjoy, and improving those that currently exist, but there is little consistent governing of GAMAM. It's not for lack of trying. There has been a lot of talk of regulating GAMAM. While Meta has soaked up much media attention in recent years, Alphabet, Apple, and Amazon were also called to the congressional carpet in the United States in 2020. They were asked to talk to legislators about their market power and their techniques to crush competition. Earlier in the 2010s, Google was brought up on multiple antitrust concerns with its search engine by the United States and in Europe. Notably, Google was fined 2.42 billion euros in 2018 by the European Union. Some of the same problems that Microsoft's monopoly position created in the 1990s seem to be recreating themselves in current iterations.133 The Canadian government has been crafting policy around containing social media more generally and has struggled with its own attempts to regulate Pornhub, the oft-visited pornography amalgamating and sharing site.134 Turkey, India, Italy, and France have all taken antitrust positions to chip away at the dominance of Google's search function and the positioning of various products (including their own).135
We seem to have mixed expectations of companies providing goods and services (and by virtue of providing those goods and services, rules around their distribution and enjoyment). In my work on nongovernmental organizations, I saw a parallel dynamic, where very smart people assumed NGOs were somehow inherently virtuous because they work on mitigating poverty or protecting pandas. It took a long time for people to come to terms with the fact that NGOs are self-preserving organizations that have to meet their boards' scrutiny and stay on budget. With Big Tech, the story is different, and yet so much the same. On the one hand, no one is surprised that a company would pursue profit over mitigating harms from its products. We saw this with Big Tobacco and Big Automobile, and we still see it with Big Oil. Why not expect this from Big Tech? we might think cynically and complacently. On the other hand, we're outraged that a for-profit entity would prioritize profits over mitigating harm. Frances Haugen's testimony likely confirmed what the skeptical believed all along: Facebook (and now Meta) prioritizes profit over all else. Plenty of Meta employees fretted about these revelations, and some left their jobs over it.136 Nonetheless, we're dependent on Meta (and GAMAM) for the future prosperity of the information economy and continued economic development.
Some of this comes because of GAMAM's own marketing, and perhaps twenty-something idealism, whether it's Zuckerberg's insistence he just wants to connect people or Google's original, "Don't be evil." But I think more to the point, Big Tech is now infrastructural: essential services undergirding major chunks of the human experience. These companies are global governors by virtue of their architecture, infusing our institutions with their products and shaping how we do things. We're just starting to come to terms with what exactly and entirely that means. As global governors, Big Tech needs to respond not just to shareholders and users, but to a set of global stakeholders whose lives are affected by GAMAM products.
We need to rethink "profit is what companies do" when what they also do is provide global infrastructure. There's a reason public ownership of utilities or transportation is a model many countries follow. We need to press tech companies to think about human rights and integrate human rights into their products and practices. The OB is a start, but Meta dictates the terms of that body. We need outside regulators, whether government, international organizations, or NGOs, to make it part of the code of what Big Tech does. If Big Tech wants us to believe everything they do changes everything, they need to consider the humans on whom they unleash their wares.











7
Data Literacy, or Why We Need Libraries, Not Twitter


Twitter is in no way a "town square." Only town squares are town squares. They are public for a reason. And they are local.
—Siva Vaidhyanathan, media scholar1

With things a bit calmer since the fallout from Jason's blog post, the Madeups sit down for a family chat about digital devices. They ask both boys to leave their phones in their rooms. Today, they want to talk to Jack and Corey about pulling back from their digital media.
As expected, Jack offers more snarky commentary, and Corey silently nods throughout. Both parents have noticed that since the Instagram snafu with his school project, Corey has spent more time reading books and riding his bike with friends. When Jason tells the kids they don't understand the meaning of digital data and their devices, Jack objects, "Clearly, you don't get it. You're old and didn't grow up with this stuff." Jack goes on, "The stuff you're obsessed with right now, telling your story online to a bunch of strangers, people just don't care like you do anymore." Then, unexpectedly for everyone (including himself), Jack blurts out, "For all we know, the story you're telling is fake! How we even know that you really adopted us the way you did? Maybe you stole us!" His face turns bright red, and he runs to his room.
Claire and Jason are stunned at the substance of Jack's anger, but also the extent to which they are clearly not the experts anymore. Jason looks to Corey with his mouth open, but Corey is just as surprised. The conversation is over for now.
Jack finally comes clean to Corey, who reacts exactly as Jack thought he would. But Corey also sees the inconsistencies with the email, shows him the questionable email address of the sender, and scrolls lower to reveal the phishing links Jack had completely missed because he had been so upset. Jack feels foolish, realizing how well this particular scammer played him. It felt real. How could he fall for something like this? At least he didn't click on anything. Sensing how upset Jack is, Corey offers to talk to their parents on his behalf. It's important he deals with this himself with his parents, but it's also reassuring Corey's there for him. The twins sit together on Jack's bed, physically leaning into another as they did when they were little kids. Jack realizes how much he's pulled away from his brother in recent months.
Later, Jack shows his parents the email, which by now has been kicking around in his inbox for almost six months. He sees their expressions and understands how hurt and confused they are. Silently, he draws his parents in, and they share a long hug. He's been carrying this injury around for so long; both Jason and Claire had assumed his aloofness in the past few months had been teenage hormones, but it was clearly something else.
Jason and Claire talk more about how sticky digital data are. How did this "L.R." find Jack's email, and why would he single him out for this kind of trolling and phishing? What did this person want? So many things have happened recently that have brought into sharp relief how data are rampant in their lives and running roughshod over their sense of control over their technology. Looking further into the future, both parents feel distressed about how the stickiness of data matter for their boys as they reach adulthood. How will social norms change in light of shifts to datafication? Even someone like Jack, who was born into the digital world and characteristically rational, was tripped up by an email they now all saw for its motive. The writer tapped into Jack's teenage angst in a calculated way. Having read the headlines in recent years, however, both Claire and Jason know that changes are brewing. Neither of them feels prepared, least of all on deciding how to best manage datafication for the family. It feels as if there is a lot at stake for their futures, but it's not clear how exactly each can act. They suddenly feel incompetent.
But it's not on the Madeups, as individuals or even as a family. To be active stakeholders in our digital world, data literacy is a skill set we must develop together. Individuals do not fully realize some rights. The right to health, for example, applies not just to healthy individuals, but also speaks to the health of the greater community in which that individual lives, whether that means neighborhood, country, or the whole world. Such rights empower individuals and provide collective benefits. Literacy is like that. If we think of literacy as a set of skills that give someone competence to participate in wider society—using language, numeracy, or data—one person's competence only matters in the context of many others.2 Our collective and individual literacy is important because data are both collective and individual.
This chapter argues that we have come to the point in datafication where the right to data literacy is imperative. In order to truly live in the world, we must all engage with sticky data in a datafied reality. Making data literacy a right and not something "nice to have" situates the right as a political obligation. Politics are social by definition. Though often seen as personal responsibility, literacy is not a burden that individuals can take on without the practical opportunities and resources to become literate.3 Now that data are integrated into humanity, we need literacy to combat data stickiness. It can also help other human rights protect us better.

A Lack of Data Literacy in a Datafied World
On October 28, 2022, Elon Musk, the multitech tycoon who has taken on the mantle of global disruptor-general, announced "the bird is freed."4 This move culminated the tumult that had surrounded his initial announcement to buy social media company Twitter in April.5 To some, especially on the political right in the United States, Musk is the savior of free speech.6 The political left, in the meantime, has dropped the platform in droves since news of Musk's takeover.7
Article after article in the business and tech worlds links Musk's Twitter takeover to the metaphor of a "town square" that enables free speech.8 Former Twitter co-founder and CEO Jack Dorsey tweeted "Twitter is the closest thing we have to a global consciousness. . . . It wants to be a public good at a protocol level, not a company,"9 in explaining his support for Musk's leadership. Musk himself proclaimed that he bought Twitter because, "it is important to the future of civilization to have a common digital town square, where a wide range of beliefs can be debated in a healthy manner, without resorting to violence."10 Should we lament or take a leap of faith that the mercurial businessman would do the right thing with one of the most prominent pieces of social media real estate in the world?
The companies that provide us with the essential services we have come to rely on are not public, and we should not think of them as public. They might feel public, both in content and scope. This is why some have argued that we should make social media state-owned enterprises to improve their attention to human rights.11 The dissonance between the "public" reach and accessibility of social media and its "private" nongovernmental and shareholder accountability structure is usually subverted. But the reality can burst jarringly forth when "public" infrastructure like Twitter can change dramatically with a shift in corporate leadership. In the days and weeks following Musk's official takeover, he became the sole director of the company,12 controversially restructured the "verified" checkmarks by adding new subscription fees,13 and let go of 50% of Twitter staff.14 If Twitter is a "global town square" for everyone developing "global consciousness," it seems inappropriate that only one person's actions have such resonance for so many others.
As Siva Vaidhyanathan points out in this chapter's epigraph, Twitter is not and will not be a public square. Twitter is global, not local, and it is not part of a town; it's part of the internet. Its economic interests (and drama) have clearly interfered with its ability to serve the public interests of being a public square. Twitter needs to sell ads, and if not ads, subscriptions, or other products that will limit access. It has economic realities, and will govern itself, and by extension, all of us on its platform, accordingly. Despite what Musk proclaims, these economic imperatives affect the practice of free speech on Twitter.15
Vaidhyanathan goes on to say of town squares: "They have no rules of decorum. They have no interest in maintaining order to keep advertisers happy or their users comfortable. They are exceptional places."16 Perhaps commercialism does not go hand in hand with "the public." But Vaidhyanathan is not right about there being no rules or order. Any social or political institution built by humans has rules and desired order. Institutions become mundane if everyone uses them. A public square wouldn't be very useful if it were so exceptional that no one bothered to engage there. It also wouldn't be very useful if there were indeed no rules and people just shouted at each other and stabbed adversaries. If anything, public squares have both spoken and unspoken rules. In London's Hyde Park, Speakers Corner is precisely this kind of rule-bound yet free speech place. This is where we make political speeches and protest policy. People can come and listen and they can oppose each other, but everyone can speak freely on any topic.17 Public squares were also long the site of executions and tarring-and-featherings, and all sorts of other displays of public power and discipline. So to say "anything goes" in these exceptionally free public places called town squares is an overstatement. Public spaces are governed by norms—at the very least common understandings and typically laws regulating appropriate conduct. What determines how we use such spaces, public or private, isn't the space itself but a combination of its rules and how we understand our rights and capacities in this space. It does not matter how well a space is designed if our illiteracy doesn't allow us to use it in our own interest.
For every person who has regretted a Tweet that threatens to never die or will tarnish their reputations forever (and there are books written about them), many others swear up and down they would never write something "so stupid" or "careless."18 But what "stupid" and "careless" mean change over time and in different contexts. We need to learn that datafication amplifies the likelihood that we will all look stupid and careless. Currently we judge content after it has been created. The German law known as NetzDG, which demands that social media companies block or remove illegal content, imposes fines if companies leave problematic content unmoderated.19 This is a reactive stance. The content is already out there and can create harm regardless of fines. Unfortunately, many countries have followed Germany's lead in fining for already-posted content, in some cases cutting and pasting the law directly from German books.20
Data literacy is the background knowledge we need in order to make informed choices in a world of data. Right now, we have all kinds of wonderful tools that use data effectively to improve our lives, but we're engaging with these tools half-aware. Sure, we know how to navigate through an app or use our computer to edit and post photos, but do we understand the data implications of using that app to share those photos digitally?
Imagine that you've never driven a day in your life and someone handed you the keys to a Ferrari. Or an eighteen-wheeler truck. Or a fifteen-year-old Toyota Corolla. No matter what the vehicle, you're not going to be a safe or functional driver if you haven't had basic training on not just how to operate the vehicle, but also the relevant explanation about the consequences of using the vehicle in different ways, the meaning of road signs, what the gauges are telling you, and how to think through problems you encounter (like skidding). Perhaps you've ridden a bike since you were young and can analogize it to driving a vehicle, but it's not going to be the same. You're going faster and can't stop by squeezing your hands. Twitter, TikTok, Facebook, iPhones, PCs—these are various vehicles that have been designed to move our lives differently online. They are our Ferrari, truck, and Corolla. We, in turn, have probably ridden bikes and may have even driven those cars. But we don't actually understand the mechanics of road safety, necessary precautions, and emergency procedures, and we're only passingly aware that we need to follow the road signs. For all intents and purposes, most of us are data illiterate. At best, we're partially literate.
Datafication is a way to store information as it increases our capacity to create and analyze information. It's sticky because it's linked, effectively forever, about the mundane, and is co-created. Data stickiness makes datafication both powerful and tricky to integrate into human society.
I have argued elsewhere that datafication, and the AI it enables, are as significant a change in the course of human history as the printing press proved in medieval Europe.21 Prior to the printing press, the clergy were the most literate in Western societies, which enabled them to influence sometimes illiterate political leaders. Royalty might have made decisions, but the clergy were the keepers and producers of recorded information. Over several centuries, the printing press gradually broadened who had access to literacy and therefore who had political power. Information production broadened. The consumers of that information have also expanded.22 One way that this is reflected is in the human right to education. This right, however, has been interpreted in various ways, even as literacy programs are widespread and a core part of what it means to exercise the right to education.
Here, I emphasize data literacy as understanding what data are and what they are doing. Data have been neglected as the central driver for change to our societies in favor of other kinds of literacies. We need it in combination with digital literacy, which has emphasized how to use the tools of data—social media, computers, cell phones, smart homes and connectivity.23
There are many definitions of data literacy, but let's start with "the ability to read, work with, analyze, and argue with data."24 In addition, data literacy must give us competence in evaluating why data are different from other forms of information and the repercussions of digital data versus analog data. Data literacy must teach us how the existence of sticky data shifts our world and how our lives are composed of analog and digital forms of data. Ultimately, recognizing the stickiness of data, our datafied selves, and how data transform us individually and collectively will allow us to make informed decisions and demands. These changing competencies are fundamental to human existence in society and therefore fundamental to the mission of human rights.
To become data literate, we can turn to libraries, already important keepers of information for society. Libraries are no longer just stacks of books and fragile archives of physical documents. Many provide public access to technologies such as 3D printing and offer technical training. The key to understanding what the library was, has become, and should be is critical to the success of realizing data literacy.
As we'll see, libraries, especially public libraries, are often taken for granted, chronically underfunded, and generally overloaded. They are both the unsung heroes and the workhorse of modern literate societies that provide community and equality of access, two core values of human rights. Libraries already play a critical role in linguistic literacy.25 In our story, the library holds the key to supporting data literacy while retaining its role in granting access to the tools of datafication via existing digital literacy programs. Supporting libraries in data literacy campaigns is one important way to take on datafication.
To connect to the earlier driving analogy, learning the implications of driving is as important as learning the skill of driving. Benefits come with the ability to drive, like being able to decide on when we run errands without having to worry about relying on public transit or the goodwill of others. It also means people can move to the exurbs and commute to work. On the flip side, relying on cars as we did in the twentieth century in North America also means intensifying global warming and urban sprawl. Incidentally, driving, like datafication, is a risky commonplace activity whose risks we regularly underestimate or downplay for the sake of convenience or enjoyment. Nonetheless, driving skills are to digital literacy what the implications of driving are to data literacy.
We've said in this book that we need to be data stakeholders, not subjects. In order to be a stakeholder, we need to know what's hanging in the balance and have some idea of how to resolve any disparities in power, access, autonomy, equality, community, and dignity. Data literacy will give us the power to become stakeholders by giving us knowledge and understanding of how datafication has changed our lives. It provides the basis for critical thinking that is key to digitizing societies.26 Besides giving us the wherewithal to make sound decisions and influence policy effectively, data literacy will give us the power to demand more and better from companies that make, disseminate, and monetize data about people.
Data literacy ought to be a human right because it provides the basis for flourishing in the digital era, just like the right to freedom of expression, freedom from arbitrary arrest, and the right to work and make a living. To flourish in our datafied world, we need to become stakeholders, invested and ingrained in the process of datafication, including decisions about who owns data, why, and when. As philosopher Joris Vhieghe writes of digital literacy, "It opens up a different dimension of experience and goes together with complete unforeseeable notions of what it means to create and bring into being 'new' things."27


What Data Literacy Is, and Why It Matters
Literacy does not have a single definition. Often, however, we do think of it in terms of reading and writing.28 These dimensions of literacy lie at the heart of how states measure their literacy levels and the success of literacy campaigns. But beyond just reading and writing, UNESCO, the UN's economic, scientific, and education agency, defines literacy as "the ability to identify, understand, interpret, create, communicate and compute, using printed and written materials associated with varying contexts. Literacy involves a continuum of learning in enabling individuals to achieve their goals, to develop their knowledge and potential, and to participate fully in their community and wider society."29 To do this, functional literacy is important, as is understanding cultural contexts. Literacy is not just a set of skills but a set of understandings to make us competent, thereby allowing us to function effectively in society.30 Competence gives us the ability to understand when things are not quite fair or right. These skills should apply across a variety of media, whether paper, digital, or on a stone tablet.31 Fundamentally, we have come to understand that to be competent, we must be able to read and write.32
When we speak about literacy, what we are trying to get at is whether someone has a "facility with information."33 This means being able to engage with whatever we're literate in. Any idea of data literacy that does not go beyond basic comprehension of data (to read, work with, analyze, and argue with data) to the implications of datafication will not have functionally made anyone literate. The implications of reading and writing are not just performing the acts; they are about connecting to a broader humanity, forging ties, creating ideas, and exchanging thoughts. Literacy is about both individual enrichment and collective action.
In the context of data, literacy is about teaching people how data are made and how they are used. It is about making visible the invisible (but necessary) decisions data collectors take to clean data and make them usable for algorithms. It is also about taking data science out of its silicon silo and asking the scientists working to squeeze information from the data what they are assuming to be true about the world.34 It is about contextualizing the data, which are numbers in a spreadsheet, but also linked to real people. We can all benefit from a bit of critical thinking about data.
Some business leaders recognize the importance of data literacy.35 Even while embracing the use of data to seek profits, business leaders are providing data literacy training for their employees in a variety of ways. That datafication makes companies rich is well established. They have come around to understanding that creating a data-literate workforce might make them even more profitable. Many more of us need such data literacy. We're not all going to be working for companies that seek to marketize data literacy, but we all will be subject to datafication.
Computer scientists have been concerned with ethical, responsible, safe AI and datafication for quite some time.36 These problems cannot be answered just by systems design in a vacuum. The answer comes from the context in which those data arose. Data literacy has the social benefit of helping us assess power imbalances that arise from datafication, promote public interests, and right inequities.37 This can help us design institutions that will direct us away from pathological outcomes, such as the pervasive algorithmically created filter bubbles on social media. These bubbles amplify our preexisting beliefs and mute competing notions.38 How do we recenter human rights and our respect for autonomy, community, dignity, and equality?
It is important to establish data literacy as a human right. We are entitled to learn how to use data and have a sense of the consequences of datafying human life. If we can't or don't want to stop datafication, we should have a right to know what's going on and when others are taking advantage of the data we are providing them. Data literacy will also help us recognize the symbiotic relationship between the individual and collectives. Poorly designed data policies will adversely affect individuals, but data-illiterate populations will also create difficulty for collectives to stand against data abuses. Data literacy will help us participate in society critically on our own terms. It will also undergird decisions we make as a society for how we record information about people and how we use that information.

Locating Data Literacy among Human Rights
"Data literacy" isn't yet the mantra we're all chanting when we talk about emerging technologies, but it should be. It's a sensible extension of the work that has been already done in the name of championing a human right to education and basic literacy. Internationally, the protection of education has been in place since the end of World War I through the League of Nations.39 These goals were made part of subsequent foundational international human rights documents, including the Universal Declaration of Human Rights (1948). The International Covenant on Economic, Social, and Cultural Rights (ICESCR, 1966) offers the most comprehensive discussion on the right to education in human rights law. Education touches on women's rights in terms of equity and children's rights as part of the right of children to flourish. It is also part of goal no. 4 of the UN's Sustainable Development Goals, which emphasizes quality education and lifelong learning opportunities.40 The Committee on Economic, Social, and Cultural Rights, which passes authoritative interpretations of the ICESCR, wrote that "education is both a human right in itself and an indispensable means of realizing other human rights. As an empowerment right . . . [it helps people] obtain the means to participate fully in their communities. . . . The importance of education is not just practical: a well-educated, enlightened, and active mind, able to wander freely and widely, is one of the joys and rewards of human existence."41
Put simply, education provides the means for human dignity and autonomy.42 It gives us our humanity and arms us with the knowledge and skills to take on the challenges we face in life. It is a basic way to make sure that our rights as human beings do not weaken owing to ignorance or lack of basic know-how. While the importance and implications of education are pretty clear, what constitutes education is not. International organizations inform national-level decisions, but policies have not been consistent.43 States and communities, as well as civic and religious organizations, play an important role in the content and provision of information because they establish schools.



Making Data Literacy Work
Datafication's stickiness has changed the terms of how we think about education. Data infuse how we learn and interact. COVID-19 moved schools and playdates online. While some of our habits may bounce back post-pandemic to real-life interactions, there are educational practices that have been permanently moved online, such as the use of digital learning and portfolio-sharing platforms. Our linguistic and numerical literacies remain paramount, but "going online" means using various technologies that function by using and producing data about their users. It's not just about convenience. That the human right to education is delivered via devices is not new, but the literacy to understand the implications of datafication has not been part of the formulation of what it means to be educated. It follows that the human right to education needs an update to account for data literacy.
There is widespread recognition of the change in the information environment. As early as 2004, UNESCO recognized the importance of incorporating digital information and tools into our ideas of literacy.44 But much of the way we think about the digital has to do with developing new skills to effectively navigate it, understanding information in new formats, and being able to create new information. Such an approach makes sense when technology is new, but we need to be more deliberate about the consequences. The EU has come up with the Digital Education Action Plan, which emphasizes that all learners need to develop data literacy. Other countries, including Canada, Nigeria, Norway, and the United Kingdom, have created various programs to combat disinformation.45
What then would be a baseline way to think about data literacy? It's not surprising that some research has focused on how to assess the quality of information found online.46 Key to all of these ideas is that we are able to critically assess our experiences in order to make informed choices, to "decode and demythologize their own cultural traditions . . . [and] the wider social order."47 After all, "literacy," writes media scholar Kathleen Tyner, "is a lifelong pathway that challenges individuals to adapt to emerging communication tools and disruptive systems."48 It's important to remember that literacy as a concept is never value neutral, even if it is portrayed as neutral.49 Like education, literacy is infused with societal and cultural values.
That literacy is not a neutral idea is why it is important for forming political persons. It is about an interest in the distribution of power and resources in a society. Being a political person is partially about voting or ideology or parties. But being political is also about a conceptual orientation and an aptitude. For example, it is a skill to make a meme-worthy placard and publicly march with it, but it is a conceptual orientation and an aptitude to understand the proper contexts in which placard making and marching in public are the right approach, or if other approaches are more germane to the goal at hand. In the digital world, using hashtags and memes can create collectives but these activities are mediated by the ways algorithms shape and are shaped by what we know and do.
Being political means being able to understand the power structures of society. It means having a sense of how we fit into those structures. It also means claiming a stake in how the status quo works and being informed when we want to change it. Finally, by virtue of being political, literacy is about the collective as much as it is about individuals. Too often our thinking about all kinds of digital literacies has been focused on what individuals can do,50 not what collectives like communities, governments, and companies can do.51
Datafication means that literacy must take into account an assemblage of technological, economic, and governmental rules and practices. It must include how to think about the content available for consumption and how individuals and collectives interact with that content through platforms. Whether we think about platforms as the guts of the user interfaces we use (for example, TikTok's algorithms) or the digital space for exchange (for example, TikTok's interface), what we are talking about is a world characterized by the centrality of extracting, analyzing, and creating more data.52 What is fundamentally changing us isn't that we use e-readers where we would have held a paperback thirty years ago to enjoy a summer read on the beach. It's that companies know what books we are reading, where we are in the book, what content we may have highlighted or commented on, and whether we actually finished the Wolf Hall trilogy or just lied to impress our friends. Data change who we are because of their stickiness. They are used not only to observe our behaviors but shape them. While fighting digital surveillance is certainly attacking part of the problem, it's a symptom of a more amorphous concern, which is how data shape our conceptions of humanity and ourselves.
This is why data literacy is important. We need more people to understand what control they can exert over data and what control data exert over them via collectors and co-creators, policies, and regulations. Working with data surfaces the types of decisions that go into the creation of data sets, from sorting through information and devising measurement strategies, through to the analysis of those measurements. Sorting and analyzing data creates value—which is what companies seek—but we should be mindful of the consequences of being part of that value creation.
Claiming that data literacy is a human right is not to reinvent the wheel. Data literacy builds on the analytical focus and world-shaping capacity to be able to absorb information. Data literacy takes advantage of the device and platform specificity of computer literacy.53 It builds on the ideas of translating our analog world to online interactions that digital literacy emphasizes. If we can declare a right to data literacy, the next step presents another challenge: how to meaningfully achieve data literacy.
We should understand that there is no single path to data literacy. Governments have already started moving toward greater data literacy-related policies. They could do more and would benefit more from considering the broader societal benefits of data literacy. Many democracies, like Canada and Estonia, are very concerned about disinformation or so-called fake news, and rightly so.54 Disinformation might be a function of data illiteracy, but government worry about that lack of competence does not translate into digital hesitancy elsewhere. These same governments might use AI to process immigration papers, as in Canada,55 or put 99 percent of government services online, as in Estonia.56
We have to remember that data literacy empowers the individual and the collective. As data-literate individuals, we can make better choices. But as the Madeups' story shows, we don't always make better choices in the moment because it's impossible to anticipate just how others can use data about us. It's also not just about us as individuals, but others' decisions matter because the linked nature of data means we are all affected on very short notice. Thus, individual benefits of data literacy can go only so far. We need to be conscious of how communities will contribute to and benefit from collective data literacy.
We discuss two channels for realizing data literacy. The first is through a civil society path, through nongovernmental organizations (NGOs) and other community organizations. The second path highlights the potential for libraries to serve another critical social function. Although libraries in many ways are perfectly positioned as conduits of data literacy, placing this burden on them will require more resources for an oft-beleaguered institution in modern societies.

Civil Society and Education
Data literacy might belong to each of us as a skill set and an orientation toward data, but the effects will benefit the communities to which we belong. Our competence helps us adjust our expectations and demands accordingly. How we get to be data-literate stakeholders, then, should be a community-supported effort.
NGOs have and will continue to play critical roles in education in communities. Some NGOs are locally founded and locally funded. Others receive money from abroad or are branches of international NGOs. NGOs are a vital part of our social and political fabric, in some places providing essential services and in others shaping policy. Governments and businesses cannot do everything. Nonprofit initiatives and community-driven ventures can teach us what we need to know so we can demand more of policymakers and corporations.
NGOs often start out with big ideas. Rahul Bhargava and Catherine D'Ignazio are two people whose expertise defies simple description, but their technical and arts expertise led them to collaborate to build "data cultures." On the website of the Data Culture Project, they make the case that hiring data scientists is not the only way to build data capacity in workspaces.57 Instead, everyone in the company should become comfortable in a data environment. The Data Culture Project provides six stages of activities to get people used to the idea of working with and creating data. It is accessible, and it uses multiple media to get people involved. Some of the project's early work started small, with local organizations. But they've also gone global. Working with the World Food Programme (WFP), a UN organization, the Data Culture Project is credited with helping staffers think differently about their reports and arguments, which had been done in a similar way for twenty years. The WFP's data and digital collaboration manager commented, "Participants started looking at the different sides of an argument and how [we] build that based on [our] audience and what's more relevant, like which piece of data is more relevant for which type of stakeholder."58 The Data Culture Project is a dynamic approach to learning about and working with data. Its approach reminds us that data are everywhere. Not all of them are about people, but many of the ones of keenest interest will be. People can learn and improve their data literacy through creative and collaborative means. The approach takes people on at their level, demonstrates how data are part of their jobs, and lets people apply the data to tasks they must complete. The Data Culture Project is consistent with global education efforts that prioritize more community-based support for education. This approach stands in contrast to top-down approaches, which declare universal standards for education and fund projects that claim to implement those standards. We often assume that ensuring mass literacy and numeracy leads to better life outcomes, but it is not one-size-fits-all.59 As I wrote in my book about human rights NGOs, Internal Affairs, global-level ideas need local-level buy-in to succeed.60
Data literacy needs to be carried out in multiple ways from multiple providers. Creating a human right to data literacy requires creating laws and building a culture around the importance of data. Data play a central role in human life, and human life increasingly plays a central role in data. We need to bring that into our collective focus. The Data Culture Project is an example of an initiative that hopes to inform nonprofit work. We should in turn come to terms with the fact that data literacy prepares our children better for their future work and world.
Already we know about many ways that social media harm individuals and society. The information clearinghouse on the harms of social media, the Ledger of Harms, created by the Center for Humane Technology, offers a bleak picture of what social media can do, from making us less empathetic, to undermining our ability to understand the world, to amplifying noxious ideas about minority groups.61 widespread data literacy can address some of the hand-wringing by giving more people more access to how digital platforms are made. More stakeholders can inform how to design more human-friendly platforms, create more prosocial norms, and imagine different ways to regulate norm breakers.
Training in any ideas can get us only so far, however. We have to enact the training, whether in a work context, in schools, in our communities. Thus, data literacy at the individual level is limited; to have it become widespread and normative, we need a communal hub.


The Importance of the Library for Literacy

A library is a different kind of social reality (of the three-dimensional kind), which by its very existence teaches a system of values beyond the fiscal. . . . I think for most people [the library] is emotional. Not logos or ethos but pathos. This is not a denigration: emotion also has a place in public policy. We're humans, not robots [emphasis added].
—Novelist Zadie Smith62

For many of us in the twenty-first century, the library serves the kind of communal purpose that would help to spread and reinforce data-literacy norms. Sociologist Eric Klinenberg wrote a book celebrating the role of the library in fostering civil society, as a meeting place for all kinds of people and perspectives.63 Anthropologist Shannon Mattern attests, "Libraries function as both a knowledge infrastructure and a social infrastructure."64 What she means is that libraries hold resources and distribute them to patrons, yes, but they have also come to serve many other purposes in communities. From literal shelters from the storm (like Hurricane Sandy), to supporting teachers with multimedia lesson plans, to helping the unemployed with job searches, libraries perform a multitude of tasks. They have been places where digital access has been linked to social justice and equity concerns, opening doors to those who have nowhere else to go.65
Renowned novelist Zadie Smith and industrial tycoon Andrew Carnegie are probably not two names that naturally go together, but they share a deep esteem for libraries. For both of them, libraries served useful purposes—and some of that is fuzzy and warm and has no rational label. For Smith and Carnegie, libraries build communities. Smith wrote an essay in the New York Review of Books that reflected on the centrality of libraries as public spaces, capturing the tangible nature of books and documents that such places hold and also the intangible feelings of community, knowledge, and sharing.
Carnegie's name graces many corners of American civic life because he was a prolific philanthropist. He might have made his fortune with forges in western Pennsylvania and created many institutions in his name,66 but his influence on the development of American and international libraries remains one of his most important legacies.67 We can thank him for the ubiquity of local public libraries. He gifted libraries largely in the United States, but the first public library he built was in his hometown of Dunfermline, Scotland. His contributions paved the way for public libraries in the United Kingdom. Between 1883 and 1929, 2,509 libraries were built in the English-speaking world, at a cost of $56,162,622, or about $12 billion today.68 The sites for such libraries ranged from small towns to large, municipal systems such as the New York Public Library. His legacy made libraries "free to all."69
Even before Andrew Carnegie, the public library was seen as an instrument of democracy to help in the creation of an informed citizenry.70 Carnegie was not alone in thinking that libraries were the key to a productive society, but he paid for them to be built.
Today we think of libraries serving an important role in sustaining a literate public. Some have argued that rather than emphasize the role of the library in creating a society fit for democracy,71 libraries now facilitate social relations.72 But what does that mean? Certainly, literacy has taken on a new meaning today. In Carnegie's time, it was about paper-based materials. Today, literacy is needed for a slew of other media, and thinking about it in terms of those media has become critical. Public services and labs for the community to access everything from computers to 3D printers and many other technologies are part of what libraries offer their communities. An example of community building through services is the Ballard branch of the Seattle Public Library. This branch houses the Ballard Neighborhood Services Center, where people can get passports and pet licenses, among other things. The Brooklyn Public Library, the Chicago Public Library, and the Martin Luther King Jr. Library in Washington, DC, each host a "commons" providing digital tools and collaboration space to its community.73 Books, advanced printers, desktop computers, and creative software suites are all tools to access and produce information. They are the means into our datafied world. They are a way to drive interest in libraries.74 But they don't teach us about datafication so much as grant access to its opportunities.
We need both access and understanding. What it means to provide literacy in our world requires the kind of approach with which libraries of all kinds are already familiar. As one librarian wrote, "Libraries are and always have been community centers for data—the collection, classification, management, organization, dissemination of and access to data."75 Libraries organize vast troves of information, put them together neatly and coherently, provide ways to navigate all of that information, and are a place for help in answering questions. Libraries traffic in data and have always been active in the process of datafication. Libraries and librarians handle materials that contain information and curated the collections of libraries to reflect a certain vision of knowledge. In other words, libraries create data. They have done so for millennia by choosing what information and what patrons are given access. They help people use data and create new things using library resources.
The panoply of services libraries currently offer and their position as the curators of knowledge for human societies make it natural to think of libraries as providers of the building blocks of the literacy needed in order to navigate worlds of data. Librarians work in data, collecting and sorting through various kinds of texts as living, breathing computers who exercise judgment and discretion in their filtering of knowledge. In an endearing example, celebrated children's author Jon Klassen tweeted, "Last year in a library in Alaska I read a folk tale in a random book on a random shelf & have been thinking about it since & today i wrote the librarian w/no book title or author & in 2 hrs i had a scan of the story & cover in my inbox—librarians should be running everything."76
Giving visitors the ability to rent a computer or other equipment is a service of public libraries that builds on data literacy. It makes sense that data literacy should follow—ideally precede—access to data-intensive technologies like computers. Using Zadie Smith's characterization, such a logos, ethos, and even pathos, needs to be built into how we view and fund libraries to enrich our communal experiences.

The Ethics of the Library
Already, libraries have articulated stringent professional codes of conduct. The American Library Association (ALA) was founded in 1976 with the mandate of providing "leadership for the development, promotion and improvement of library and information services and the profession of librarianship in order to enhance learning and ensure access to information for all."77 It has established a code of ethics for librarians,78 which acknowledges the pivotal role that they play in acquiring and filtering information for others. In a list of eight values, the ALA has identified the importance of several values rooted in human rights, including intellectual freedom, privacy, property rights and user rights, and the need to be neutral in access to information. Professionally, librarians already serve their respective communities by providing information. In fact, some have declared access to public libraries as a right.79 Librarians are key conduits of information literacy, even if it is not an area in which they received formal training.80 The ALA encompasses many types of libraries, from school libraries to public libraries to university libraries.81 Librarians are perhaps uniquely empowered to help people in our datafied society become data literate. They, after all, are a profession that has acted as caretaker of and guide through records about and documents created by humanity.
Different libraries serve different purposes. University libraries have already taken the lead in conceptualizing and researching "digital literacy."82 They can also lead in developing the various components of data literacy, from the frameworks to the tools to the use of those tools in the hands of patrons. In contrast, public libraries "have been all about access."83 They can offer people opportunities to become data literate in addition to providing other services. They can now also be places where people learn more about the stickiness of data and what that means for their lives. To provide data literacy, librarians can keep doing what they already do—and need to be supported to do so.


Library as Infrastructure
It's becoming increasingly obvious that we need to think that libraries are about books as much as they're about data. We associate them with books (or DVDs, or microfilm, or archives of documents) because that's the medium on which data have been transferred. If, as Mattern convincingly argues, libraries are social infrastructure, they are as vital in getting us access to the information we need as other public infrastructure, such as the roads and bridges that provide access to places we need to get to.84 Getting and understanding data is increasingly as necessary as drinking water and breathing clean air. This goes beyond debates about whether internet access is a human right.85
We have already seen success in local library systems. Toronto Public Library (TPL), has worked to ensure not just digital literacy; "increasingly it's understanding algorithmic literacy and understanding what people's data means and what they're giving away," according to Pam Ryan, the director of service development and innovation.86 TPL already has programming in place to help people navigate technology and suites of programs and the datafied realities they live in, whether that's using a Tor browser that anonymizes web activity or working with AI providers to bring stakeholders together for public education programs.
Resituating the library as infrastructure isn't just rhetorical. It's stating the obvious about what libraries could do in our datafied reality. At the same time, we should also recognize that while libraries' duties grow, their budgets are under attack.87 Libraries have come to provide all kinds of different services in crisis and normal times without the commensurate resources to do so. During the pandemic, they took on even greater mandates, offering virtual gatherings in lieu of in-person programming, providing online consultations, or driving Wi-Fi-enabled bookmobiles to provide hot spots for neighborhoods with households that lacked at-home internet access.88 In some ways, then, suggesting libraries shoulder the monumental task of helping us realize data literacy is blue-sky talk. As journalist Anne Helen Peterson, the author of a book about burnout, spoke with regard to librarians, "Libraries cannot fix . . . everything, and if we're being asked to fix everything, pay us appropriately."89




A Literate Stakeholdership Demands More
If we had a human right to data literacy, it would form a foundation for meaningful stakeholdership. Some of us might be more active than others. Some of us may not care much more than we already do after becoming data literate. But all of us could invest in collective action against datafication's incursions. We could participate and help redirect the boundaries around acceptable versus unacceptable data collection and processing practices.
A human rights claim to data literacy would also bolster our enjoyment of other rights. At the moment, much of the language and energy around technological harm tends to focus on freedom of expression or privacy rights.90 Supporters of both of these rights tend to emphasize individual enjoyment, taking the collective out. Most of the time, we think of privacy and expression as preserving some area around the individual self. But one of the reasons both of these rights keep running into trouble is that most of us lack data literacy. With a higher degree of understanding about how data are being collected and used, we might want more corporate responsibility for allowing harmful content on their platforms. We might demand that convenience is not a reasonable justification for eroding personal privacy and that certain types of data collection are simply unnecessary and therefore forbidden. Data literacy would enhance freedom of expression and right to privacy by making trade-offs clear when we consent to what companies take and give.
In addition to bolstering other rights, data literacy can give us better ways forward to demand more of the companies that code the algorithms and collect data. Armed with more understanding of the business models of Big Tech and why their practices infringe on the underlying values of human rights, our policy answers can go beyond breaking up corporations, as in the United States, or trying to regulate exclusionary data practices as they have done in Europe with the Digital Markets Act. These are reactive rather than proactive, and they build norms through economic punishment, and not social incentives.
An alternative to the use of these regulatory tools might be trying to change how Big Tech thinks about and does things by broadening technical training. What if, beyond our data literacy, we tried to get Big Tech to be more literate—that is, competent—in social, political, and cultural concerns? We could create new norms around more socially beneficial products, or at least technical products that are created from inception with social, political, and cultural consequences in mind, and not just economic ones.
One idea might be to demand that students of technical fields like computer science, data science, and electrical and computer engineering take mandatory ethics courses taught by those in nontechnical fields. This can come in the form of required undergraduate and graduate courses, akin to how medical schools have adopted ethics requirements in the United States and beyond.91 There are plenty of university tech ethics courses globally, as information science professor Casey Fiesler has documented.92 Fiesler's collection of tech ethics courses has generated profound insights. For one, people are teaching about tech ethics—there are well over 300 courses in her list. However, many of these teachings are not required for computer science degree programs. The departments at US universities that require ethics-related courses range from large universities, such as the University of Texas at Austin, to more boutique, private universities, like Hofstra University, but there is not an easily discernible pattern of types of programs that require ethics. Abroad, the University of Cape Town offers a mandatory course called "Social Issues and Professional Practice," and the Vienna Institute of Technology requires a "ways of thinking" course that includes ethics.
Notably, computer science programs at Harvard, Stanford, and the University of Toronto have all begun embedding ethics into multiple computer science courses.93 Much of the ethics has come from philosophy, a foundational field in thinking about morals and ethical reasoning. Computer scientists Diane Horton and Shelia McIlraith at the University of Toronto have led an effort to create a series of ethics interventions throughout a computer science degree. Their findings show that embedding ethical units in a variety of courses leads to some promising outcomes, though course designers are still tweaking how much exposure is needed.94 However promising, philosophical ethics is just one type of decision-making tool. Other fields do more to teach about the consequences of different types of social, political, and cultural choices made around technology. Social scientists, such as political scientists, sociologists, and anthropologists, and humanities scholars beyond philosophy, including literary scholars and historians, all have relevant downstream insights to give more heft to the human context that technologists ought to weigh before making products. Put simply, data experts need to have more literacy in the context into which their products go.
It's a bit of a chicken-egg situation, however. Because there aren't enough technologists who value ethical and humanist training, outside of a handful of institutions, they don't make them program requirements for computer science students. And because not enough students get mandatory exposure to humanist or social science concepts in their classes, they don't see the value of nontechnical fields when they graduate to work for Big Tech, or engage in postsecondary work.
Training might also come in the form of professional certification. We have certifications for all kinds of training, from project management to auditing, diversity, and inclusion to food safety, engineering, and funeral directing. We can envision an interdisciplinary course series focused on ethical issues and establishing norms in computing that draw on the expertise on all kinds of fields that provide a form of professional, continuing education, and ensure that technology companies will have more than just fresh graduates who are alert to the ramifications of their work.
Data literacy is not out of reach. It combines a skill set with a mindset about datafication, that, once known, can't be unknown. This information can create possibilities for collective action. There may be groups in every community who are willing to take on the costs to act, and demand change from companies and governments. Datafication affects us all, so it stands to reason that data literacy should be something we all have access to. It takes a village to create and sustain data literacy, and it takes the investment of stakeholder resources to demand policies around datafication practices that respect the values of human rights.











8
We, the Data

Miquela Sousa is just like any other successful 20-ish social media influencer. She boasts 2.9 million Instagram followers and has sponsored content partnerships with top brands. In 2018 she was named one of Time magazine's "25 Most Popular People on the Internet." She has a contract with CAA, the talent agency, worth $10 million. She is always impeccably styled, often wearing designer clothing, sports a trademark Princess Leia hairstyle, she has a beauty routine, and even a couple of tracks on Spotify. In terms of politics, she wants you to vote and she supports #BlackLivesMatter. Her online nemesis is a right-wing troll named Bermuda with whom she got into a feud in 2018.
Miquela (@lilmiquela), however, is not a physical human being. She is a virtual influencer created by Brud, a digital media startup acquired by Dapper Labs in 2021.1 Miquela is at the forefront of virtual influencers, a genre that is gaining traction in the corporate world. Unlike their real-life (human) counterparts, "virtual influencers are controllable and pose no risk—Miquela doesn't have seven-year-old racist tweets to blow her film prospects. Further, it's proclaimed that virtual influencers command three times higher engagement than a human influencer. And most apparently, they're nice to look at."2
Miquela and her counterparts, such as Shudu, Blawko, and her nemesis, Bermuda, raise several important questions, not least of which is who is deciding beauty (and what their intentions are). Leaving these concerns aside, the existence of Miquela and others provides meditations on what it means to be human. Is it just to inhabit a likeness of the human form? Is it to interact linguistically and visually with other humans? Whether she is "real" or not, what Miquela forces us to confront is what we think of as a fundamentally human life.
These virtual influencers exist alongside other kinds of virtual people. The company Soul Machines wants to help you with your customer experience through the use of their digital people.3 It has some kitschy celebrity avatars ("digital twins"), such as golfing legend Jack Nicklaus and NBA player Carmelo Anthony, as well hip-hop artist will.i.am, who we met earlier as someone active in thinking about data rights. Soul Machines also markets its products for a variety of sectors that currently employ people to help other people: customer service, education, financial services, health care, and public sector services.
The existence of these virtual humans pushes boundaries of our understanding of the human existence. Are Miquela and her ilk "people"? What about the digital twin version of Carmelo Anthony, who emulates the human version? As will.i.am says in a teaser video reflecting on his own digital twin, "it" should sound a bit robotic "for my mom"4—in other words, a step back from the uncanny valley we discussed in chapter 5.
These forays into the boundaries between human and machine are where we currently stand. Datafication changes all of our individual lives; it even creates new not-quite-human ones as it does after-life likenesses of dead human beings. It makes the digital approximation of humanity, for entertainment and beyond, possible. But datafication changes our collective life as well. The possibility of digital persons isn't just for fun and games if we end up interacting with them instead of flesh-and-blood humans for services, for connection, for human purposes. Datafication is about the collective as much as it is about individual persons. What affects one group or person today will one day affect the great many of us. If we fail to grapple with datafication with a solid statement about the value of the human rights we deserve, we all lose. Datafication is our life and our lives, and it will inevitably raise questions for our humanity—for everyone.
It's tempting to say that human rights are the answer for all that ails us from datafication. This simply cannot be. There are important limitations to human rights we'll discuss in this conclusion. Human rights, for example, can't answer whether Miquela is a person. Yet she, other virtual people, and the digital immortals from earlier in the book are meant to act as though they are people. If Miquela is a person, it would be consistent with the current international human rights framework that she would have human rights. What would this application of human rights say about what constitutes humanity? Echoing debates among those advocating for animal rights, what justifications would we make for privileging humans to have rights at the expense of other types of beings of our creation, to which we have endowed "intelligence"?5
What human rights can do—and have done—is advance the values of autonomy, dignity, equality, and community through the development of a global agenda and vocabulary about the entitlements we have as human beings. The core of those entitlements has remained stable. To date, we've built on the UDHR and other human rights statements with elaborations and enforcement mechanisms, and we've identified vulnerable groups, such as children, indigenous peoples, migrant workers, and women. Datafication threatens our conception of human life and human rights because our existence is no longer entirely bound up in the corporal and carbon-based realities that the founders of the global human rights system lived and envisioned. Do life-like creations of ours—virtual people—count as humans in our calculus?
What we discussed in chapter 5 about data after death or the virtual influencers in this chapter was not possible in the 1940s. The advent of Big Data and AI, however, raises existential questions. Some of these questions can be addressed by human rights. Some of them go beyond human rights. And some of these questions pose vexing issues for the exercise of existing human rights for humans.
Datafication has and will continue to happen. This book has argued that we have to go back to human rights principles—what we have and continue to fight for—in order to see how they might offer us a way forward. To date, the loudest stakeholders in the process of datafication have been those who are creating the technologies that datafy us all. We need to become noisier stakeholders, thumping the human rights drum. Human rights provide a blueprint for us to understand the changes made by datafication and the types of transgressions that are intolerable. Importantly, political science provides us some of the tools for understanding a lack of collective action to date and some insights into what might happen next if we embrace our role as stakeholders.
In our story thus far, the Madeups face a number of situations that are possible only through datafication. These obstacles are just a smattering of the types of interactions we might encounter in our data-driven digital lives. Each of the circumstances they encounter is both mundane in the sense that the interactions are such, yet disturbing and vexing in content and consequence. Datafication is both frustratingly intangible and yet disturbingly present. What to do in light of the fact that much of our lives have become data is the pressing question.
The previous chapters have highlighted where we're losing our autonomy, how our dignity is being harmed, the inequality in datafication, and the erosion and changing terms of community. Big Tech, and private companies more generally, are pushing the datafication envelope without much resistance. There are things human rights are good at doing and where they fall short. Human rights are a foundation on which we create societies that care for humans; they are not a panacea. There have been many calls for human rights, some of which are well intentioned but others that fail to grasp what human rights are fundamentally about. We can't have human rights become a slogan that means whatever people want it to. Human rights hold resonance, but they can also be subject to abuse. We have to remember that human rights reverberate as effective tools because there are important ideals they ensure for all of us. We now turn to explaining what's at stake if we don't take human rights seriously in the digital world and in the coming metaverse.
Part of the problem with saying what's at stake is that datafication is ongoing. Not everything has changed quite yet, so we have the ability to change the contours of how datafied we become. To help us understand what could happen if we simply let momentum carry forward, we can lean on literary visions. Most critical, we need to reflect on the power of potentially knowing everything for all time from now on, of forgetting how to forget.
Then we will turn to demonstrating the task before us in enforcing our existing human rights. Proponents of human rights are always at pains to say that rights are interdependent, that we can't pick and choose among them. Our solutions are just starting to take this interdependence seriously, with proposals for data intermediaries like trusts and unions. In the preceding chapter, I advocated for a right to data literacy. But data literacy is not an end so much as it is a means to achieve other things. Once more and more people are data literate, discussions, advancements, and policymaking can be more inclusive. Right now, a relatively small number of experts make nearly all of the decisions: technologists, lawyers, economists, philosophers, and science and media scholars. It's easy to think those folks "have got it" because they're the ones with the specialized knowledge of regulation and development of the technology, so they "know" better.
Yet datafication's consequences bleed into all facets of human life, areas where other people have insight and an interest in contributing. If people from other research fields and different walks of life had greater investment in datafication, the trajectory of what happens to data will shift. As with any other policymaking, taking a wide, diverse swath of values and preferences into account is vital for shaping appropriate, durable, and functioning policy.6 The challenge here is that datafication is a global transition. Luckily, we have some emerging models of managing data about people through the development of data intermediaries. These models certainly point the way forward for the co-creative and collective nature of data about people.

What's at Stake?
Novelist Dave Eggers has written two parables for what the internet changes about society, cutting distressingly close (in some passages) to how we actually live. In The Circle and The Every, Eggers has created an all-powerful company, the Circle, which demands people "go transparent" with the sharing of their lives. The company eventually becomes The Every in the sequel.7 In the later iteration, The Every is at once a search engine, social media platform, and creator and distributor of content and physical products. It is totalitarian in its aspiration, and in so doing nudges society into a repository of the mundane. In Eggers's world, most people comply with, even enjoy, the ever-present tentacles of surveillance, consumption, social engineering, and interaction provided by the company. Through technology, everyone polices everyone else. There is near-absolute accountability because of the thoroughness with which the company has blanketed society. Even characters who seek to take down the system must persist within the Every, embracing it tighter in order to advance or sabotage, and using its tools to further their own entanglement with technology and content. It is inescapable because we can't, don't want to, or don't know how to leave its grasp.
The theme of corporate power is not limited to a digital dystopia, of course. The notion of Big Tech as a global governor pervades all of the chapters in this book, even if only one of the chapters was specifically about corporations. Corporate power is not an artifact of datafication. The power of the British East India Company (BEIC) and the Dutch East India Company (DEIC) might be the stuff of history, but both companies changed the course of humanity by extending the power of the British Crown (early BEIC), or threatening it (DEIC, later BEIC).8 In fact, as political scientist Swati Srivastava tells us, the BEIC played a role in how states designed their own hold on power through defining who could be "sovereign." Because of the growing power of the BEIC as a corporation with increasingly state-like powers, states had to create rules to disqualify companies from being sovereign.9 Thus, companies like the BEIC, or Meta, Google, and Apple, may not be sovereign, but like centuries earlier, nonsovereignty doesn't prevent them from exercising great global power or governing issues of universal importance.
In the chapters about data rights, FRT, and death and data, Big Tech's influence is central to how the story unfolds. That many Big Tech companies are based in the United States currently and that American ideas and entrepreneurship are preponderant in digital technologies is important. The governing power of Big Tech reflects a certain orientation toward free enterprise, innovation, and limited government interference that is not universal. Europe, for example, has a very different orientation toward the balance between privacy and technology, with stronger protections of individual privacy and stressing market coordination among firms.10 China views firms as much more closely tied to state interests than either the United States or Europe.11 The fact that many critical experiences of human life, online and offline, are currently dominated by corporate interests should not be lost on us.12 Corporations are private and respond to concerns that are largely market driven, and not rights motivated, unless the two coalesce.
That corporations have to be forced to pay attention to the human rights harms they were causing is not new. Activists boycotted Polaroid in the 1970s to protest the company's role in creating ID cards in apartheid South Africa.13 Anti-sweatshop campaigners in the 1990s were able to change practices by Nike, the Gap, and other global brands.14 In one of its many snafus, Facebook released an app called Beacon, which linked users' behavior on third-party websites to what they saw on the social network. People were mostly furious about what Beacon revealed, including engagement ring purchases.15 It was shut down after a class-action lawsuit in 2009 and US Senate questioning.16 More recently, Canadian company MindGeek, which owns popular pornography website Pornhub (and others), faced a reckoning over its role in spreading child pornography and rape videos, after New York Times journalist Nicholas Kristof broke the story.17 In Canada, platforms are jointly liable for the content that users post, but enforcement of that liability isn't always straightforward or effective.18
The examples highlighted above are obvious violations of human rights or criminal law. They challenge the values of human rights through their mundaneness. Data about people are co-created at their genesis. As of now, they are mostly in the custody of the private entities as a starting point. This creates a massive power imbalance, putting the impetus on each of us to take action (if we can) to check on or demand erasure of data about us. We, as individuals, have to exercise our rights against companies to monitor the use of information about us.
This individual responsibility in the exercise of rights inhibits our ability to demand collective action. Although we might make decisions on a personal basis about how we use our devices or where we travel, the data generated tell collectors not just about us as individuals but others "like" us. More important, as chapter 3 shows, digital data are inherently about you and others.
Except in rare instances, such as Facebook Beacon, we haven't seen a lot of collective fighting back over the kinds of data being collected and shared. Certainly there are calls to protect our privacy online and the privacy incursions of our devices more generally.19 This singular focus on privacy (or freedom of expression), while important, creates a myopia around human rights. It feels as though all that matters with regard to human rights is privacy. But privacy isn't so much one thing as a series of things, as explained in chapter 4. It's not about privacy when advertisements follow you across the internet; it's about autonomy. As philosopher Karina Vold and behavioral scientist Jess Whittlestone write, data are being corralled in such a way as to curtail our ability to make free choices.20 When facial recognition software identifies Black Americans as "criminals," this is not about privacy. This is about being treated with dignity as a human being, of the rights of habeas corpus, and the painful inequalities of racism. When we think about things in terms of privacy, we're creating walls between us instead of building common bridges within and between community. The danger of reductionist thinking about human rights is that we think that the solution is all about a handful of rights when it's really about the entirety of human rights and the experience of human life.
In chapter 4, we considered the implications of FRT, the human face, and why thinking about the private nature of facial data is in some ways pretty much impossible, given the social engagement of our faces. The reasons that facial data feel wrong aren't so much about privacy as they are about losing something that feels intimate and more a question of dignity. What could be more intimate and "me" than my face?
In this book, we have focused on some dystopian ways that FRT has been used. Yet there are also some efforts to integrate biometrics (like FRT) that, when implemented with an eye toward equity and access concerns, have the potential to facilitate human rights. To date, efforts to use biometrics to help states distribute goods and services are in their infancy and show themselves as such. One example is Aadhaar, a program created in India in 2010 to simplify access to the distribution of food and other services for those for whom such services were essential. Aadhaar offers insight into the strengths and pitfalls of datafying social practices. On the one hand, Aadhaar facilitates public access to government services. On the other hand, without proper policy guardrails, these data can be easily misused by government or nonstate actors, as we see below.
Under previous institutions, conflicting identification requirements and onerous paperwork made distributing services overly taxing.21 Enrollment in Aadhaar is based on residency, not citizenship. Basic demographic details such as name and address are collected, but also data on faces, fingerprints, and irises. Enrollees are given a unique twelve-digit number and either a virtual or physical card. There were nearly 1.3 billion Aadhaar enrollees in 2021.22 The government agency that maintains the database is the Unique Identification Authority of India (UIDAI). The Aadhaar system helps distribute food rations to people who are poor and those in rural areas and is also linked to various government and private sector services, such as banking, scholarships, and driver's licenses.23
Socially and politically, the creation of Aadhaar was important for the Indian government. It has given enrollees a unique national identity that merges photographic and other biometric details with proof of address. Creating such a consistent documentation system is vital for governing a country as populated and varied as India, and the state emphasized inclusion as one of Aadhaar's goals.24 It meets the standard set by the UN's Sustainable Development Goals of providing a legal identity for all by 2030.25 But from the beginning, the system was beset by problems.
The implementation of Aadhaar has been disputed on human rights and technical grounds. One analysis found technical vulnerabilities at multiple points of the Aadhaar system.26 NGO Privacy International has offered a comprehensive report on some of the fallibilities of the system, including the virtual impossibility of ensuring that identity details of a population as large as India's are not duplicated within the data due to the sheer complexity of comparing each biometric profile with all of the others.27 From a security perspective, the consolidation of over a billion people's details in one database gives ill-intentioned actors a single access point to attack. Critically, the use of Aadhaar data by third parties (not UIDIA, but other agencies) and vulnerabilities on the end user side introduce a plethora of security concerns. Between 2017 and 2019, researchers documented over twenty security breaches, with each affecting a handful of people to millions.28 Problems include making data accessible through Google search, typical instances of people falling for phishing attempts,29 and the entire database being sold via WhatsApp.30 Even high-ranking officials made mistakes. R. S. Sharma, the first director general of UIDIA, demonstrated his confidence in the system by tweeting out his twelve-digit number—much to his chagrin. The internet had a field day using his ID fraudulently.31
Not surprisingly, Aadhaar has been legally challenged in multiple ways.32 In 2018, the Supreme Court upheld the mandatory use of Aadhaar across a large number of government areas, including tax filing. It restricted use of Aadhaar for access to private sector services, banning a requirement of Aadhaar ID for banking, for example. Importantly, it ruled that Aadhaar was of limited utility for government surveillance, a concern of critics.33 Yet there are concerns that UIDIA now holds biometric and other data from nearly the entire population of India and that these data can be used for training AI algorithms that magnify government surveillance capacities.34
Aadhaar is a program that other countries now look to in creating or expanding their own digital ID systems.35 There are multiple misgivings about these kinds of systems. Technical ones are one kind of problem, but the social norms and capacities simply aren't there. As economists have pointed out in the case of India, there is a lack of trust between civil society and government.36 This distrust is exacerbated by a lack of foresight with regard to data literacy. If the public were more informed about how the data are being used, and by whom, this could increase trust if the intentions and methods protected public interests. With all these biometric data about individuals stored in government databases helping regulate access to essential government services, data literacy for data collectors and sources is imperative. A system like Aadhaar, which was in part about creating access, can be used to increase and even amplify access problems if the data are accessible by malicious actors. It's not that datafication always hurts human rights. It's that if we don't proactively consider how these technologies affect human rights in negative and positive ways, we can't safeguard against transgressions.

What to Do
The example of Aadhaar shows that while there are certainly privacy concerns with the program, using these kinds of data to distribute resources has much deeper, collective problems than privacy of data.37 Data are being used to grant or deny access, to determine authenticity of users, and to sort Indian society by twelve-digit identifications. Pointing to a world of human rights beyond privacy gives us more ways to make political decisions and construct social norms to reshape the nature of technology. Data might be inherently stickier than other human creations, but we can make decisions on how we collect and use data that can make them less tacky and perhaps even unglue them from eternity. In telecommunications technology, we have seen how policies shape the way that goods like the telegraph, telephony, cable TV, and the internet are provided.38 In each case, technology and innovation leaped ahead of political realization and social reaction. And of course, that makes sense, since it's impossible to predict what might happen with any sort of accuracy to justify policy shifts of the kind we need now. As we scramble for different approaches to a global problem, we can center on another idea that has taken root worldwide: human rights. Human rights are a grease to the gum of data stickiness.
Globally, we have to set standards that everyone abides by because the internet and data are globalized phenomena. Already, Europeans are setting the agenda of weakening Big Tech's dominance with the Digital Services Act (DSA) and Digital Markets Act (DMA), set to come into effect at the latest in 2024.39 The DSA is intended to create transparency and accountability around Big Tech products and services by identifying new obligations for "very large" online platforms and online search engines (VLOPs and VLOSEs) as defined by number of users. The law creates reporting obligations, audit compliance, and explicit human rights assessments, among other human rights-specific measures. The DMA identifies gatekeeper companies such as Apple and Google and mandates that they loosen restrictions on their platforms to allow newcomers to compete.40 Gatekeepers must share data and allow for interoperability on their platforms, and they must be more transparent about advertising practices. The law seeks to rebalance the severe advantages of Big Tech for consumers and other businesses.
Although the DSA and DMA offer policy solutions to the power of Big Tech, their focus is on market solutions to problems that have political and social dimensions. Consumption and market exchange make up just one dimension of human life. We also participate politically, philosophize, engage in cultural practices, and socialize. These are not market activities, and therefore they are not protected by regulations like the DMA. While the DSA does better by trying to change corporate practices, how well the legislative parameters are followed, audited, and enforced will be a key test. If we understand better that data pervade those nonmarket activities and relationships as well as our economic activities, we will be better able to demand new regulations as well as corporate offerings that respect the extensiveness of human life.
New regulations around Big Tech need to consider the collection of data and its costs on our collective humanity. We shouldn't accept the collection of data as forgone and only legislate the use of data, once created. We need to remember that as co-creators of data, we too should have a voice in whether those data come to life. Protecting human rights, whether in the context of specific rights or more generally about autonomy, dignity, equality, or community, is something that can be done only if we act collectively.
We know that the history of international human rights would not have been possible without the actions of nonstate actors, like religious groups, labor unions, and NGOs.41 Civil society organizations pull together disparate individuals into collective wholes. They create cohesion around shared identities. They make issues where there were seemingly none before, whether from neglect, blindness, or just plain unimaginativeness locked into the status quo.
A host of human rights-related NGOs are working on technology issues today, whose work I've referred to throughout this book, and whose efforts are gaining ground in areas such as FRT.42 The Algorithmic Justice League, based on Joy Buolamwini's work, documents how FRT discriminates and exacerbates existing social inequalities.43 The Ranking Digital Rights project, run out of the think tank New America, offers a yearly scorecard for how Big Tech has so far failed to protect human rights.44 These are reactive activities that publicize events after they have happened. They offer us a sense of the stakes of the status quo, but what they need to fix often seems insurmountable.
We need more civil society proactivity. Some NGOs are also at the forefront of pushing us to think about the effects of technology before they become embedded in our realities and have to be removed, camera by camera, as in the case of FRT. Organizations such as Access Now are urging people to think about data minimization as a human right.45 While they tie it explicitly to protecting privacy, data minimization is an important understanding that can only be realized with data literacy to see why and when data ought to be minimized in our digital interactions. It's not only privacy. Aadhaar teaches us that even relatively limited amounts of explicitly collected data can have far-reaching consequences, depending on how those data are used.
Even so, understanding the effects of ubiquitous data collection as a change to human dignity, autonomy, and community needs to underlie how civil society organizes us to advance as stakeholders in datafication. Repeating "privacy" and "surveillance" as the representative human rights affected by data oversimplifies the problem. Our very humanity is at stake here.
When NGOs succeed in changing the conversation, we accept their normative entrepreneurship without question. We don't live in a world where torture is acceptable, chattel slavery is widely practiced, and states can deploy any weaponry they choose. We owe much of this work to that of civil society, which championed unpopular ideas and energized collective action in earlier times so that we can now take such ideas for granted. In previous eras, civil society pushed us all to imagine a world where torture was not a state prerogative, slavery is not an appropriate moral or market relationship, and limits to the cruelties of war to inflict mass suffering and disfiguring injuries, like land mines and cluster munitions.46 We should be looking to NGOs to lead discussions around datafication that articulate what's at stake and what we will lose if we don't act now. One of these imaginaries that civil society can help us understand better is the possibility of never forgetting.


Remembering Everything
Although there are barriers to "remembering everything, forever," on server farms, the datafication of human life is comprehensive and durable enough that they might as well last forever. Even if it's not every last shred of every single human's every single activity, it still comes far closer than we ever have to remembering everything about everyone alive.47
In his book Delete, Viktor Mayer-Schönberger provides a compelling academic case for why forgetting matters for humanity. He chronicles how forgetting has helped forge the path of human history, proposing that we place expiration dates on memory.48 Yet making the decisions on how long a memory should endure is dually a collective question about making rules and an individual-level one. There are people who keep copies of neatly filed grocery receipts until the day they die, and there are those who leave them sitting on the counter after checkout. How we choose memory and memorialization is at once highly personal and yet, in the age of datafication, a collective and vital choice.
Not being able to forget forever is at once fascinating, and yet horrifying. It makes sense that there is no shortage of literature that imagines this possibility. These writings do not offer answers, only prospects. The classic short story "Funes the Memorius" by Jorge Luis Borges offers a vision before computers.49 The title character, Ireneo Funes, falls from a horse, which results in his physical paralysis and the ability to remember everything, for all time, from cloud patterns on specific days to books to creating his own "more efficient" numbering system, which does not have to be written. His brain is filled with minutiae that cannot be deleted. Because he is the only one with such a remarkable gift, Funes is locked in his mind as well. He is so focused on details and the frameworks he has constructed to organize his unforgetting reality. To never forget, we realize, is to be trapped within oneself, isolated in a neverending record of events. In the end, he dies of lung congestion, literally unable to breathe.
A more technological take comes from writer Ted Chiang in the story "Truth of Fact, Truth of Feeling." Two narrators guide us through their realities, and in so doing, they offer views of the effect of technology on themselves and society.50 One of the narrators is a journalist in the not-so-distant future who uses "Remem," a technology that is essentially a pair of glasses that records everything someone sees and makes a searchable archive. We learn that a painful incident with his daughter from years ago led to their near estrangement. It is a moment in his life that he ruminates on over and over. To his recollection, after his wife died, the daughter said regrettable things and stormed out of the house. For years, he has harbored this pain of his daughter's resentment and finally resolves to discuss the issue openly with her. Much to his surprise, she recounts a reality that is very nearly the opposite of what he remembers. He said the regrettable things. He abandoned his teenage daughter after her mother's death. Astounded, he reviews the Remem footage and watches himself doing the things that he had always attributed to his daughter. His feelings pervaded his memories, subsuming the facts. His portion of the story concludes that it is better to remember, to have Remem help him recall facts despite our brain's way of processing what happens to us.
So, is perfect memory suffocating or liberating? Without a definitive answer to this (which I don't think we will ever arrive at), we have to go beyond expiration dates to govern our collective, datafied memory. Moreover, who decides those guidelines will have to balance social needs with individual desires. Funes didn't have a choice, but Chiang's narrator did. The presence of such choice is key to protecting human dignity and autonomy.



Trust in the Collective
From chapter 3, one lesson we might draw is how insurmountable the idea of managing data at the individual level might be. To address this, the concept of the data intermediary has gained traction. One popular idea that has been proposed for governing data is the creation of data trusts.51 These trusts are modeled on financial trusts, where trustees have an obligation to serve the interests of the beneficiaries of the trust. Another way to think about data trusts is the creation of and dependence on data stewards who manage data on others' behalf.52 There are many ways to conceive of data trusts, and the healthy literature demonstrates all of the various possibilities people have thought of the idea.53 Many notions of data trust lean heavily on the notion of trust as a social concept.54 Despite the comfort of trusting trusts, to date data trusts are not social answers to the stickiness of data, so much as yet another market-oriented solution to storing data about people.
The reason data trusts are inadequate for securing human rights regarding data about people, despite the name, is that they are about management, not creation. The data have to exist to be turned over to the trust. Thus, data trusts are in fact the definition of a data intermediary in that they are middlemen with fiduciary responsibility. Data collectors create the data from data sources, and data sources sign away the management of relationships with collectors to the trust. This is not to say that intermediaries don't serve a good purpose—there are plenty of examples of brokers, such as realtors or stockbrokers, who offer value to transactions—but they have a limited purpose. Certainly the promise emphasized by data trust advocates of righting the power imbalances between data collectors and sources is probably overstated.55 They certainly don't acknowledge the stickiness of data, especially the mundaneness of many of them, and the realities of creating intermediaries for the amount of potential data being amassed. Data trusts also would primarily (it seems) address data explicitly provided by users and not observed data (such as location) or inferred data (such as conclusions drawn from the other kinds of data).56 The realities of data trusts are that they create entities with the job of creating distance between collectors and sources, but the purpose of this distance remains curtailed and undefined.
Data trusts also focus on individuals within markets for data. Because of their market orientation, data trusts don't quite ever embrace the collective qualities of data other than as a way to create economies of scale around the trade of data. Yet the collective is really where data stickiness runs into bigger problems. At the individual level, stickiness is easier to address. There are things we can do to minimize this stickiness, including becoming data literate and changing our own practices. But when we start thinking about the collective implications, this becomes much harder. How do we unstick the ability of machines to make inferences about us, based on the data from other people? Even if we do not freely provide data about us, there still remains the issue of other kinds of "bulk" data—such as metadata—that are not about content so much as the nature of the communication itself. As discussed in an earlier chapter, this distinction is known as "on the outside of the envelope" versus "inside the envelope," where anything outside the envelope is presumed to be unimportant and incidental to guiding the substance of the communication on the inside. Observers have criticized this division.57 Metadata aren't really avoidable. We can, however, find ways to overcome the legal distinction that currently allows the collection and surveillance of people through metadata by seeing metadata for what they are: revealing of who people are through what they do.58
Still, overcoming the metadata/data divide isn't the only thing. We often think about protecting data through market instruments. But there are nonmarket models that we can look to—labor unions, for example. Labor unions represent collectives based on a common identities and occupations. Using the power of collective identity and interest, a union can shift the balance from market efficiency to more humane treatment of the workers who make the value for market transactions. So how would we build unions in the data context? Data unions would not just manage the data once they have been created. Data unions would bargain, on behalf of their members, the terms of data collection and data usage. For example, members of Union A may all be exempt from location data collection because of the terms struck with collectors by the union. Different unions could offer varying benefits. Data sources could elect not to join unions at all. But data unions would serve a collective purpose, as representative of an identity and a market position. They would also explicitly be collective and individual in nature, as labor unions are. Already, some data unions have been conceptualized, with some proposing market solutions such as the "data dividend" or creating alternative institutions such as "data cooperatives."59

What about Blockchain?
Some might be wondering whether blockchain can save us. Hailed as one of the groundbreaking technologies that will bring us forward to "Web 3," blockchain and the "decentralized web" are touted as eliminating the need for centralized databases and authorities (like the state). In other words, we can use computer technology to help people govern themselves without corrupt, inefficient, or otherwise ineffective governments.
Blockchains are systems of protocols that govern the exchange of digital information by maintaining a decentralized ledger of transactions. This reduces the need to have a centralized "certifier" of transactions. The idea is that they can create trust even among strangers.60 In theory, many people have access to this ledger, and transactions can only be added. By design and by default, blockchain is a form of governance.61
In theory, blockchain offers a trusted source of record keeping, which can then be applied to govern all kinds of things, from cryptocurrency transactions to crowdsourcing to reaching marketing audiences.62 Media studies scholar Nathan Schneider has suggested that blockchain could be harnessed to help protect human rights by writing them directly into the protocols.63 This sounds consistent with what we've been discussing in these pages in terms of expanding the scope of human rights responsibility beyond states. After all, blockchain participants could really be anyone, and if human rights are universal, we should be demanding not just states or even corporations be responsible for enforcing human rights but every one of us in our transactions.
Unfortunately, we need humans to make judgments about what constitutes human rights violations and/or when human rights are protected. The examples used in Schneider's piece—slavery, fair pay—are indeed human rights considerations, but they require human oversight to decide whether someone is complying regardless of whether a blockchain is somehow mediating that oversight. While potentially useful, a record keeping protocol cannot enforce human rights any better than the piece of paper the Universal Declaration of Human Rights was written on. To be realized, human rights need human beings to believe in its purpose, attest to violations, enforce obligations, and dissuade future transgressions. These aren't easy tasks, as human rights require not only intuitive understanding of autonomy, dignity, equality, and community, but also context, expertise, and motivation to enact them.
Generally, we can't automate the things that make us human, just as we can't simply encode the ideas of autonomy, dignity, equality, and community into a computer. There is a sense that using technology to replace human folly or bias will make us better, because automating processes will cut human fallibility out of the equation.64 Perhaps there are ways blockchain can improve on methods of verifying authenticity of evidence, important steps in some human rights investigations, which technologies such as the "Internet-of-Forensics" proposes to do.65 Still, the design and use of such technologies require human judgment. When we automate through technologies such as blockchain, we have to remember their limitations.



The Human Rights Challenge before Us
Our days as analog-only beings are over. The rights some of us have taken for granted, such as the freedom of expression, no longer apply as cleanly as they did (if they ever did). Freedom of expression is no longer threatened only by the state. We must now also worry about the role of Big Tech. Already, there exist voluntary mechanisms to govern Big Tech's human rights impact, such as the Global Network Initiative, but those are limited in scope. We need to do more.
This book has encouraged a macrolevel focus on human rights by recentering the values underlying human rights rather than individual human rights themselves. By thinking about the purpose of human rights, we're also pointing out the collective enterprise that motivated early framers of our global system. Human rights are about all of us individually, but also the kind of normative humanity we want to live in. Until we absorb the extent to which datafication changes human autonomy, dignity, equality, and community, we cannot tackle the difficulty of how to protect human rights, even established ones like freedom of expression. To adequately protect freedom of expression, we have to go beyond content moderation and the individual judgments of whether something is too violent, too racy, too false, or otherwise problematic. The algorithms that control what we see online have a bearing on how we exercise freedom of expression. All the mundane being swept up as behavioral surplus needs a limit, lest we spend the rest of our days being fed exactly what we shed (after it's been sifted and repackaged to find new things we'll probably want). Making companies clean up after themselves algorithmically and fining them if they don't is just the beginning. We need regulation not just on algorithms, but on the collection of data. It requires widespread data literacy on the part of policymakers and all of us to make judgments on the criteria by which to judge the appropriate collection of data, and to make rules around the use of collected data. To scale back what has been laissez-faire datafication is not going to be easy. It requires more than human rights, but human rights will get us started.
Regulation of data, AI, and other related technologies has begun in earnest, as we have seen. But many of these governance efforts are already fraying. Even the vaunted GDPR is at risk of being derailed because it's not evenly enforced.66 Plenty of privately generated ethics of AI have rushed into being, with many of their purposes to evade state intervention.67 But where will these all land if we don't have a common sense of the human implications of datafication? What is the point of the regulation, and indeed, what are we trying to protect? Human rights are one way to think about what guardrails we need, but they are not going to be sufficient in the absence of concrete thinking into what happens when Big Tech assumes a global governing role through providing our social and political infrastructure.68 We seem to acknowledge that these companies have a lot of power over politics: politicians who were no friends of Donald Trump balked when Twitter banned the then-incumbent US president.69 But then we turn a blind eye to those companies when we think about "the race for AI" between countries, not thinking about how those countries' leaders are shaped by technologies and their (private company) makers.70
The implications of datafication are too big for any one of us to opt out of. There are so many moving parts, from the technologies themselves, to the economics, to the psychological and personal, to the political and the social.71 By focusing on human rights, we may not catch everything, but we do get to the root of why regulating datafication matters, to conserve some measure of autonomy, protect human dignity, at least not worsen inequality, and remember our common humanity.

*
While much of the future is written in data, it is not a forgone conclusion that the way data have been used in past decades predicts future uses. If anything, our realization of the harms of social media and other everyday, data-intensive applications should empower us to take this moment to become data stakeholders. Centering on human rights as the key emphasis going forward will ground policy and social behavior in globally accepted values. Adding data literacy to the list of vital aspects we must protect and ensure for a fully human life, and ensuring the delivery of such knowledge, will equip more of us who are not experts in data and AI with the means to make better individual and collective choices.
Data are sticky, but they are also human. We are in the data. It is imperative for all of us to think hard about datafication and its effects. Now is the time to speak up and make demands for our digital and analog lives. We, the data, must herald the era where we, as data stakeholders, must act.











Acknowledgments

I did not expect to be writing a book about datafication and human rights when I came back to work after having my second child. Yet, by chance, in the last few weeks before the world met SARS-COV-2, I was at a conference where my undergraduate adviser and I got into an ardent disagreement about emerging technology—of which he knew a lot about and I did not—and human rights—a topic I had been thinking about for the better part of my adulthood.
The seed for the idea had been created, but the germination period was long, and it would have never sprouted without the love and generosity I felt from long-time friends, supportive strangers, and new-found confidantes. I read voraciously and asked a lot of amateur questions. Although this work would not have been possible without the labor of many others, any errors lie with me.
The research in this book was generously supported by funding from the Canada Research Chair in Global Governance and Civil Society and the Schwartz Reisman Institute for Technology and Society (SRI) at the University of Toronto.
To my agent, Martha Webb, who believed in this project with fervent enthusiasm from the beginning and helped me think about it as a book, I am forever indebted. I thank the editor who taught me how to write, Patsy Baudoin, tirelessly shaping how my words tell the story. I am also grateful to my editor at the MIT Press, Gita Devi Manaktala, whose steady guidance has given me a platform to share my ideas. And finally, my friends Roger Haydon (the wonderful editor for my other books!) and Emily Andrew, your wise advice for how to spring into this phase of my career was priceless.
I am extremely fortunate to have a livelihood that allows me the latitude to pursue the research I want to, taking in insights from my colleagues at some of the best universities in the world. Being a professor provides me with a wealth of learning opportunities from my students. I've taught human rights and international relations classes now for nearly two decades, and I find myself gleaning wisdom and feedback from my students each day I'm in the classroom. For this I am ever grateful.
This project felt like it occupied all of my hours and minutes and made me an excessively self-consumed, distracted conversation partner for years. Still, many friends and colleagues listened to me talk breathlessly about the book. They graciously read drafts as I tried to sort out my thinking and practice my narrative voice. I thank Nick Weller and David Lake (who have read everything multiple times), Alex Logue (for her creative and encouraging advice early on), Antoinette Handley, Jovana Jankovic, Miles Kahler, Filiz Kahraman, Susan Leclaire, David Lie, Beatrice Magistro, Marie Matthews, Jon Penney, Susan Schenk, Barbara Sobol, Anna Su, Karina Vold, and two anonymous reviewers. I thank especially the community at the SRI, led by Gillian Hadfield, whose support in the early days of this book was crucial. Taking my professor's prerogative, I assigned draft chapters of the book to my Human Rights and International Relations classes and received invaluable comments from my students. My friend Andrea Paras and her classes at the University of Guelph also gave vital feedback, providing a safe space in which to share my work. When I was trying to figure out what kind of book I wanted to write after the initial debate with Steve Weber, Emanuel Adler, Lisa Austin, Amanda Clarke, Catherine D'Ignazio, Ron Deibert, Avi Goldfarb, Sean Hawkins, Shareen Hertel, Audie Klotz, Brendan Lake, Peter Loewen, Sheila McIlraith, Abe Newman, Sasho Nikolov, Chris Parsons, Alex Sinha, Avery Slater, Janice Stein, Helen Yanacopulos, and Ziya Tong provided valuable ideas. I also thank the audiences for the talks I have done on snippets of this book at Carleton University, the Center for Ethics at the University of Toronto, Cornell University, McGill University, Santa Clara University, St. Thomas University, the University of British Columbia, Okanagan, and UC Berkeley.
Behind the scenes, there were also friends whose belief in my abilities gave me confidence and much-needed breaks. Erin Tolley, our weekly chats are nourishment to keep me going, for this project and beyond. My friends Ying Chen, Jess Green, Kanta Murali, and Annie Tam enticed me with fun incentives to finish and the cheerleading to get it done. Deb Thompson, thank you for your generosity, wisdom, and time. MG, your consistent support was priceless. Melissa Williams provided caterpillars and free political theory consultations.
My incredibly capable research assistants were critical to completing such a wide-ranging project. Writing a book is much easier when you have help from sharp people like Michael Faubert, Val Kindarji, Alex Martin, Julian Posada, Sara Ryu, and Victoria Vale. Jamie Duncan and Dafna Dror-Shpoliansky provided many hours of fascinating conversation, dozens of emails and slacks, and careful feedback on full drafts.
Last but certainly most, I thank my family. Eileen and James gave me real-world context on the book, last-minute editorial assistance, and answered all kinds of small and large questions while connecting me to interesting people. I am appreciative of my weekly conversations with my parents, Carrie and Boon, who doled out parental advice and provided insight into how their generation wrestles with datafication. My cousin Alex kindly gave free crash tutorials on things to consider when publishing. And of course, I am grateful for Rick's steadfast ("sticky") love, which has always been characterized by remarkable patience and understanding. Thank you for daily moral support, and for helping keep our little family chugging along through the mundane and the extraordinary.










Notes


Chapter 1


1. "Strategy Analytics: Amazon's Ring Remained atop the Video Doorbell Market in 2020," Business Wire, May 12, 2021, https://www.businesswire.com/news/home/20210512005336/en/Strategy-Analytics-Amazons-Ring-Remained-atop-the-Video-Doorbell-Market-in-2020.


2. Doug Stanglin and Jefferson Graham, "More than 400 Police Forces Working with Ring to View Doorbell Cam Footage," USA Today, August 28, 2019, https://www.usatoday.com/story/news/nation/2019/08/28/doorbell-camera-firm-ring-working-more-than-400-police-forces/2140022001/.


3. Lauren Bridges, "Amazon's Ring Is the Largest Civilian Surveillance Network the US Has Ever Seen," Guardian, May 18, 2021, https://www.theguardian.com/commentisfree/2021/may/18/amazon-ring-largest-civilian-surveillance-network-us.


4. Nick Wingfield, "Amazon Buys Ring, Maker of Smart Home Products," New York Times, February 27, 2018, https://www.nytimes.com/2018/02/27/business/dealbook/amazon-buys-ring.html; Susan Adams, "The Exclusive Inside Story of Ring: From 'Shark Tank' Reject to Amazon's Latest Acquisition," Forbes, February 27, 2018, https://www.forbes.com/sites/susanadams/2018/02/27/amazon-is-buying-ring-the-pioneer-of-the-video-doorbell-for-1-billion/.


5. For some thoughtful takes on the effect of the Internet of Things, see Luciano Floridi, The 4th Revolution: How the Infosphere Is Reshaping Human Reality (Oxford: Oxford University Press, 2014); Deborah Lupton, The Quantified Self: A Sociology of Self-Tracking (Cambridge: Polity Press, 2016); Brett M. Frischmann and Evan Selinger, Re-Engineering Humanity (Cambridge: Cambridge University Press, 2018).


6. See Mark Andrejevic, "The Big Data Divide," International Journal of Communication 8, no. 1 (2014): 1673-1689, for a similar distinction.


7. For an exploration of the history of the phrase, see Garson O'Toole, "You're Not the Customer; You're the Product," Quote Investigator (blog), July 16, 2017, https://quoteinvestigator.com/2017/07/16/product/.


8. For example, prominent technology scholar Siva Vaidhyanathan argues, "One way to begin is by realizing that we are not Google's customers: we are its product. We—our fancies, fetishes, predilections, and preferences—are what Google sells to advertisers." Siva Vaidhyanathan, The Googlization of Everything (Berkeley: University of California Press, 2011), 13.


9. For the text that identifies surveillance capitalism, see Shoshana Zuboff, The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power (New York: PublicAffairs, 2019); Shoshana Zuboff, "You Are the Object of a Secret Extraction Operation," New York Times, November 12, 2021, https://www.nytimes.com/2021/11/12/opinion/facebook-privacy.html. For an exploration of social media companies, see Ronald J. Deibert, Reset: Reclaiming the Internet for Civil Society (Toronto: House of Anansi Press, 2020). For an earlier work focused on Google's business model and effect, see Vaidhyanathan, The Googlization of Everything.


10. Jaron Lanier has written a good-humored account of this dynamic: Jaron Lanier, Ten Arguments for Deleting Your Social Media Accounts Right Now (New York: Holt, 2018).


11. Laura DeNardis, The Internet in Everything: Freedom and Security in a World with No Off Switch (New Haven, CT: Yale University Press, 2020), 3.


12. See Mary Ann Glendon, A World Made New: Eleanor Roosevelt and the Universal Declaration of Human Rights (New York: Random House, 2002), chap. 9.


13. This piece offers an insightful discussion pertaining to data rights: Cameron F. Kerry and John B. Morris, "Why Data Ownership Is the Wrong Approach to Protecting Privacy," Brookings (blog), June 26, 2019, https://www.brookings.edu/blog/techtank/2019/06/26/why-data-ownership-is-the-wrong-approach-to-protecting-privacy/.


14. For a helpful primer on artificial intelligence, see Niel Chah, "Down the Deep Rabbit Hole: Untangling Deep Learning from Machine Learning and Artificial Intelligence," First Monday, February 1, 2019, https://doi.org/10.5210/fm.v24i2.8237.


15. Amy Webb, The Big Nine: How the Tech Titans and Their Thinking Machines Could Warp Humanity (New York: Public Affairs Press, 2019).


16. Or "users" of various technologies.


17. There is a long history for the citizen/subject distinction, dating back to Roman rule. Jean-Jacques Rousseau articulated that one must always be a citizen-subject in that one is both a cowriter to and restrained by law. For a more recent take in the context of colonialism, see Mahmood Mamdani, Contemporary Africa and the Legacy of Late Colonialism (Princeton, NJ: Princeton University Press, 2018), chap. 1. In the context of datafication, the expansiveness of data colonialism creates constraints for the subject, a core concern of Nick Couldry and Ulises A. Mejias, The Costs of Connection: How Data Is Colonizing Human Life and Appropriating It for Capitalism (Stanford, CA: Stanford University Press, 2019). For a briefer analysis, see María Soledad Segura and Silvio Waisbord, "Between Data Capitalism and Data Citizenship," Television and New Media 20, no. 4 (May 1, 2019): 412-419.


18. The agency of the subject is a point of debate in scholarly work. These two sources offer some background regarding the agency of subjects in the context of datafication: Louise Amoore, The Politics of Possibility: Risk and Security beyond Probability (Durham, NC: Duke University Press, 2013), and Engin Isin and Evelyn Ruppert, Being Digital Citizens (London: Rowman & Littlefield, 2015).


19. One prominent idea that supports this comes from deliberative democracy theory and the concept of "all affected interests," which articulates a way for anyone whose interests are substantially affected by decisions to have a voice in those decisions. For some examples, see Robert E. Goodin, "Enfranchising All Affected Interests, and Its Alternatives," Philosophy and Public Affairs 35, no. 1 (2007): 40-68, and Terry Macdonald, Global Stakeholder Democracy: Power and Representation beyond Liberal States (Oxford: Oxford University Press, 2008).


20. Jaron Lanier, Who Owns the Future? (New York: Simon & Schuster, 2014); Zuboff, The Age of Surveillance Capitalism Deibert, Reset; Carissa Véliz, Privacy Is Power: Why and How You Should Take Back Control of Your Data (New York: Bantam Press, 2020).


21. Lupton, The Quantified Self; Dawn Nafus, ed., Quantified: Biosensing Technologies in Everyday Life (Cambridge, MA: MIT Press, 2016).


22. Rebecca MacKinnon, Consent of the Networked: The World-Wide Struggle for Internet Freedom (New York: Basic Books, 2012); John Gerard Ruggie, Just Business: Multinational Corporations and Human Rights (New York: W. W. Norton, 2013).


23. For some detailed and thoughtful accounts, see Mark Latonero, "Opinion: AI for Good Is Often Bad," Wired, November 10, 2019, https://www.wired.com/story/opinion-ai-for-good-is-often-bad/; Mark Latonero, "Governing Artificial Intelligence" (New York: Data & Society, 2018), https://datasociety.net/wp-content/uploads/2018/10/DataSociety_Governing_Artificial_Intelligence_Upholding_Human_Rights.pdf; Molly K. Land and Jay D. Aronson, "Human Rights and Technology: New Challenges for Justice and Accountability," Annual Review of Law and Social Science 16 (September 10, 2020): 223-240; Ari Ezra Waldman, Industry Unbound: The Inside Story of Privacy, Data, and Corporate Power (Cambridge: Cambridge University Press, 2021); John Cheney-Lippold, We Are Data: Algorithms and the Making of Our Digital Selves (New York: NYU Press, 2017), offers a wealth of sources in chapter 1.


24. Inside (Netflix, 2021).


25. The Google Nest doorbell and camera system can do this, through Nest Aware, "Familiar Face Detection—Google Nest Help," Google, accessed May 6, 2022, https://support.google.com/googlenest/answer/9268625?hl=en.


26. For more about this topic, see Wendy H. Wong, "Becoming a Household Name: How Human Rights NGOs Establish Credibility through Organizational Structure," in The Credibility of Transnational NGOs: When Virtue Is Not Enough, ed. Peter Gourevitch, David A. Lake, and Janice Gross Stein (Cambridge: Cambridge University Press, 2012); Wendy H. Wong, Internal Affairs: How the Structure of NGOs Transforms Human Rights (Ithaca, NY: Cornell University Press, 2012); Emily Matthews Luxon and Wendy H. Wong, "Agenda-Setting in Greenpeace and Amnesty: The Limits of Centralisation in International NGOs," Global Society 31, no. 4 (October 2, 2017): 479-509.


27. The classic statement on collective action problems is Mancur Olson, The Logic of Collective Action: Public Goods and the Theory of Groups (Cambridge, MA: Harvard University Press, 1965).


28. In the interstate context, this is often called "the logic of consequences" versus "the logic of appropriateness." See James G. March and Johan P. Olsen, "The Institutional Dynamics of International Political Orders," International Organization 52, no. 4 (1998): 943-969.


29. A widely used definition is articulated in Martha Finnemore and Kathryn Sikkink, "International Norm Dynamics and Political Change," International Organization 52, no. 4 (1998): 887-917.


30. Ethan A. Nadelmann, "Global Prohibition Regimes: The Evolution of Norms in International Society," International Organization 44, no. 4 (1990): 479-526; Finnemore and Sikkink, "International Norm Dynamics and Political Change"; R. Charli Carpenter, "Setting the Advocacy Agenda: Theorizing Issue Emergence and Nonemergence in Transnational Advocacy Networks," International Studies Quarterly 51, no. 1 (2007): 99-120.


31. For some readings on these various issues and activist roles, see Richard M. Price, "Reversing the Gun Sights: Transnational Civil Society Targets Land Mines," International Organization 52, no. 3 (Summer 1998): 613-644; Ann Marie Clark, Diplomacy of Conscience: Amnesty International and Changing Human Rights Norms (Princeton, NJ: Princeton University Press, 2001); R. Charli Carpenter, "Vetting the Advocacy Agenda: Network Centrality and the Paradox of Weapons Norms," International Organization 65, no. 1 (2011): 69-102; Wong, Internal Affairs; Sarah S. Stroup and Wendy H. Wong, The Authority Trap: Strategic Choices of International NGOs (Ithaca, NY: Cornell University Press, 2017); Matthew Waites, "Critique of 'Sexual Orientation' and 'Gender Identity' in Human Rights Discourse: Global Queer Politics beyond the Yogyakarta Principles," Contemporary Politics 15, no. 1 (March 1, 2009): 137-156.


32. A lucid account of this in global NGO activism is Clifford Bob, The Global Right Wing and the Clash of World Politics (New York: Cambridge University Press, 2012).


33. Peter Brophy and Edward F. Halpin, "Through the Net to Freedom: Information, the Internet and Human Rights," Journal of Information Science 25, no. 5 (1999): 351-364; Rikke Frank Jørgensen, Framing the Net: The Internet and Human Rights (Northampton MA: Elgar Publishing, 2013); John Lannon and Edward F. Halpin, eds., Human Rights and Information Communication Technologies: Trends and Consequences of Use (Hershey, PA: Information Science Reference, 2013); Steven Hick, Edward F. Halpin, and Eric Hoskins, eds., Human Rights and the Internet (London: Palgrave Macmillan UK, 2000).


34. Evgeny Morozov, The Net Delusion: The Dark Side of Internet Freedom (New York: PublicAffairs, 2011); Ro Khanna, Dignity in a Digital Age (New York: Simon & Schuster, 2022).


35. Safiya Umoja Noble, Algorithms of Oppression: How Search Engines Reinforce Racism (New York: NYU Press, 2018); Virginia Eubanks, Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor (New York: St. Martin's Press, 2019).


36. A notable exception to this is the rise of B corporations, which explicitly embrace some aspects of human rights-related concerns. See https://www.bcorporation.net/en-us/certification.


37. Neighbors is currently available only in the United States: "Neighbors App by Ring: Real-Time Crime and Safety Alerts," Ring, accessed May 6, 2022, https://ring.com/neighbors.


38. Jack Donnelly, Universal Human Rights in Theory and Practice, 3rd ed. (Ithaca, NY: Cornell University Press, 2013), 15.


39. For example, see Karina Vold and Jess Whittlestone, "Privacy, Autonomy, and Personalised Targeting: Rethinking How Personal Data Is Used" (Cambridge: University of Cambridge, 2019), 3, https://philpapers.org/rec/VOLPAA-2.


40. Elizabeth M. Renieris, Beyond Data (Cambridge, MA: MIT Press, 2023), 116-118.


41. Ruha Benjamin, Race after Technology: Abolitionist Tools for the New Jim Code (Cambridge: Polity Press, 2019); Cheney-Lippold, We Are Data; Ronald J. Deibert, Black Code: Inside the Battle for Cyberspace (Toronto: Signal, 2013); Deibert, Reset; Jaron Lanier, You Are Not a Gadget: A Manifesto (New York: Vintage, 2011); Lanier, Ten Arguments; Cathy O'Neil, Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy (Portland, OR: Broadway Books, 2017).


42. See Couldry and Mejias, The Costs of Connection.


43. Catherine D'Ignazio and Lauren F. Klein, Data Feminism (Cambridge, MA: MIT Press, 2020).


44. Lanier, You Are Not a Gadget, 47.


45. Helmut Philipp Aust, "Undermining Human Agency and Democratic Infrastructures? The Algorithmic Challenge to the Universal Declaration of Human Rights," American Journal of International Law 112 (2018): 334-338; Dafna Dror-Shpoliansky and Yuval Shany, "It's the End of the (Offline) World as We Know It: From Human Rights to Digital Human Rights—A Proposed Typology," European Journal of International Law 32, no. 4 (2021): 1249-1282.


46. In an AI-specific context, Anna Su, "The Promise and Perils of International Human Rights Law for AI Governance," Law, Technology and Humans, August 23, 2022, offers excellent insights into what human rights can and can't do.


47. Renieris, Beyond Data.


48. See Julie E. Cohen, Between Truth and Power: The Legal Constructions of Informational Capitalism (New York: Oxford University Press, 2019), chap. 2, for a thorough explanation of data doubles' importance for law and economics. See also David Lyon, Surveillance Society: Monitoring Everyday Life (Buckingham, UK: Open University, 2001).


49. Terrie Lynn Thompson, "Data-Bodies and Data Activism: Presencing Women in Digital Heritage Research," Big Data and Society 7, no. 2 (July 1, 2020).


50. Deborah Lupton, Data Selves: More-than-Human Perspectives (Cambridge: Polity Press, 2020).


51. As of 2022, these are the Ring Protect plans, available in thirty countries. In the United States: "How to Subscribe to a Ring Protect Plan," Ring Help, accessed May 6, 2022, https://support.ring.com/hc/en-us/articles/360030333671-How-to-Subscribe-to-a-Ring-Protect-Plan; in Canada: "Subscribe to Protect Plans to Save Your Videos," Ring, accessed May 6, 2022, https://ring.com/canada-protect-plans. For the full list of countries, see "Country Selector," Ring, accessed May 6, 2022, https://ring.com/country-selector.


52. Caroline Haskins, "Amazon Is Coaching Cops on How to Obtain Surveillance Footage without a Warrant," Vice (blog), August 5, 2019, https://www.vice.com/en/article/43kga3/amazon-is-coaching-cops-on-how-to-obtain-surveillance-footage-without-a-warrant; Caroline Haskins, "Ring Now Has 350 Fire Departments in Its Neighborhood Surveillance Program," BuzzFeed News, June 9, 2021, https://www.buzzfeednews.com/article/carolinehaskins1/amazon-ring-partnered-with-350-fire-departments; Matthew Guariglia, "Ring Changed How Police Request Door Camera Footage: What It Means and Doesn't Mean," Electronic Frontier Foundation (blog), June 7, 2021, https://www.eff.org/deeplinks/2021/06/ring-changed-how-police-request-door-camera-footage-what-it-means-and-doesnt-mean; Zack Whittaker, "Ring Refuses to Say How Many Users Had Video Footage Obtained by Police," TechCrunch (blog), June 8, 2021, https://social.techcrunch.com/2021/06/08/ring-police-warrants-neighbors/. Ring has its own map of public safety agencies using Neighbors; see "Active Agency Map," https://support.ring.com/hc/en-us/articles/360035402811-Active-Agency-Map.


53. Allyson Chiu, "She Installed a Ring Camera in Her Children's Room for 'Peace of Mind.' A Hacker Accessed It and Harassed Her 8-Year-Old Daughter," Washington Post, December 12, 2019, https://www.washingtonpost.com/nation/2019/12/12/she-installed-ring-camera-her-childrens-room-peace-mind-hacker-accessed-it-harassed-her-year-old-daughter/; Rani Molla, "Consumer Groups Issue Product Warning for Amazon Ring after Latest Video Hack," Vox, December 17, 2019, https://www.vox.com/recode/2019/12/17/21026381/amazon-ring-hack-product-warning-fight-for-future.


54. Aldaco's story is told in Lauren Kirchner, "When Zombie Data Costs You a Home," Markup, February 3, 2021, https://themarkup.org/locked-out/2020/10/06/zombie-criminal-records-housing-background-checks. See also Rafaela Aldalco v. RentGrow, Inc., No. 18-1932 (7th Cir. 2019); Lauren Kirchner and Matthew Goldstein, "Access Denied: Faulty Automated Background Checks Freeze Out Renters," Markup, May 28, 2020, https://themarkup.org/locked-out/2020/05/28/access-denied-faulty-automated-background-checks-freeze-out-renters.


55. In Rafaela Aldalco v. RentGrow, Inc. the court established that Aldalco had not made the request to expunge the the conviction. This fact is not noted in the media coverage cited earlier.


56. Kirchner, "When Zombie Data Costs You a Home."


57. Kirchner and Goldstein, "Access Denied."


58. United Nations, "Universal Declaration of Human Rights," article 26, accessed April 19, 2022, https://www.un.org/en/about-us/universal-declaration-of-human-rights.




Chapter 2


1. Jaron Lanier, You Are Not a Gadget: A Manifesto (New York: Vintage, 2011), 17.


2. Viktor Mayer-Schönberger and Kenneth Cukier, Big Data: A Revolution That Will Transform How We Live, Work, and Think (Boston: Eamon Dolan/Mariner Books, 2014), 78.


3. Ulises A. Mejias and Nick Couldry, "Datafication," Internet Policy Review 8, no. 4 (2019): 1-10, https://doi.org/10.14763/2019.4.1428; Nick Couldry and Jun Yu, "Deconstructing Datafication's Brave New World," New Media and Society 20, no. 12 (December 2018): 4473-4491.


4. Marion Fourcade and Kieran Healy, "Seeing like a Market," Socio-Economic Review, 2017, mww033, https://doi.org/10.1093/ser/mww033; Brett M. Frischmann and Evan Selinger, Re-Engineering Humanity (Cambridge: Cambridge University Press, 2018); Marion Fourcade and Fleur Johns, "Loops, Ladders and Links: The Recursivity of Social and Machine Learning," Theory and Society, August 26, 2020.


5. See Laura DeNardis, The Internet in Everything: Freedom and Security in a World with No Off Switch (New Haven, CT: Yale University Press, 2020) for an argument about the embeddedness of the Internet in our everyday.


6. "Domo Resource—Data Never Sleeps 9.0," accessed May 6, 2022, https://www.domo.com/learn/infographic/data-never-sleeps-9.


7. A gigabyte is 1 billion bytes (109). Today, gigabytes are fairly ubiquitous as a unit of computing storage. Computer and data scientists have gone well beyond this unit to (as of now) the largest official storage unit, the yottabyte, which is 1024. For a great explainer, see S. Gillis, "What Is a Yottabyte (YB) and How Big Is It?," SearchStorage (blog), accessed May 6, 2022, https://www.techtarget.com/searchstorage/definition/yottabyte; or Tibi Puiu, "How Big Is a Petabyte, Exabyte or Yottabyte? What's the Biggest Byte for That Matter?," ZME Science (blog), January 29, 2021, https://www.zmescience.com/science/how-big-data-can-get/.


8. Dawn E. Holmes, Big Data: A Very Short Introduction (Oxford: Oxford University Press, 2017), 6. The author does not offer a date on the IBM study, but since the book was published in 2017, these figures are over five years old, pre-COVID-19 pandemic.


9. "Choose Your New Macbook Pro," Apple (CA), accessed January 24, 2023, https://www.apple.com/ca/shop/buy-mac/macbook-pro/13-inch.


10. A useful categorization of different data is provided by Luci Pangrazio and Neil Selwyn, "'Personal Data Literacies': A Critical Literacies Approach to Enhancing Understandings of Personal Digital Data," New Media and Society 21, no. 2 (February 2019): 419-37.


11. Dawn E. Holmes, Big Data: A Very Short Introduction (Oxford: Oxford University Press, 2017).


12. Cathy O'Neil, Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy (Portland, OR: Broadway Books, 2017).


13. Karen Hao, "AI Is Sending People to Jail—and Getting It Wrong," MIT Technology Review, January 21, 2019.


14. Doaa Abu Elyounes, "Why the Resignation of the Dutch Government Is a Good Reminder of How Important It Is to Monitor . . . ," Medium (blog), February 10, 2021, https://medium.com/berkman-klein-center/why-the-resignation-of-the-dutch-government-is-a-good-reminder-of-how-important-it-is-to-monitor-2c599c1e0100.


15. Adam Satariano, "British Grading Debacle Shows Pitfalls of Automating Government," New York Times, August 20, 2020.


16. For classic works establishing this point, see Geoffrey C. Bowker and Susan Leigh Star, Sorting Things Out: Classification and Its Consequences (Cambridge, MA: MIT Press, 1999); Lisa Gitelman, ed., "Raw Data" Is an Oxymoron (Cambridge, MA: MIT Press, 2013).


17. For good synopses, see Viktor Mayer-Schönberger and Kenneth Cukier, Big Data: A Revolution That Will Transform How We Live, Work, and Think, reprint ed. (Boston: Eamon Dolan/Mariner Books, 2014), and Dawn E. Holmes, Big Data: A Very Short Introduction (Oxford: Oxford University Press, 2017).


18. Shoshana Zuboff, The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power (New York: PublicAffairs, 2019).


19. Daron Acemoglu et al., "Too Much Data: Prices and Inefficiencies in Data Markets" (Cambridge, MA: National Bureau of Economic Research, September 2019); Charles I. Jones and Christopher Tonetti, "Nonrivalry and the Economics of Data," American Economic Review 110, no. 9 (September 2020): 2819-2858; Last WeekTonight, Data Brokers: Last Week Tonight with John Oliver (HBO), 2022, https://www.youtube.com/watch?v=wqn3gR1WTcA.


20. Kate Crawford, Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence (New Haven, CT: Yale University Press, 2021).


21. Adam Chandler, "How We Fell for the Fitbit," Vox, March 12, 2020, https://www.vox.com/the-goods/2020/3/12/21163690/fitness-trackers-fitbit-applewatch-garmin; Sean Kernan, "My Fitbit Challenge Taught Me an Unfortunate Lesson," Mind Cafe (blog), February 16, 2020, https://medium.com/mind-cafe/what-my-fitbit-challenge-taught-me-an-unfortunate-lesson-19b0c99e213d.


22. On the quantified self, see Deborah Lupton, The Quantified Self: A Sociology of Self-Tracking (Cambridge: Polity Press, 2016); Natasha Dow Schüll, "Data for Life: Wearable Technology and the Design of Self-Care," BioSocieties 11, no. 3 (September 1, 2016): 317-333; Natasha D. Schüll, "The Data-Based Self: Self-Quantification and the Data-Driven (Good) Life," Social Research: An International Quarterly 86, no. 4 (2019): 909-930.


23. Brett Gaylor, Fitness Trackers Can Reveal Intimate Details about Your Personal Life (CBC Docs, 2020), https://www.cbc.ca/cbcdocspov/features/fitness-trackers-can-reveal-intimate-details-about-your-personal-life.


24. Tarleton Gillespie, "Algorithm," in Digital Keywords: A Vocabulary of Information Society and Culture, ed. Benjamin Peters (Princeton, NJ: Princeton University Press, 2016), 18-30.


25. Jon Porter and Nick Statt, "Google Completes Purchase of Fitbit," Verge, January 14, 2021, https://www.theverge.com/2021/1/14/22188428/google-fitbit-acquisition-completed-approved.


26. It's not always that straightforward. See Carissa Véliz, Privacy Is Power: Why and How You Should Take Back Control of Your Data (New York: Bantam Press, 2020); Brett M. Frischmann and Evan Selinger, Re-Engineering Humanity (Cambridge: Cambridge University Press, 2018).


27. "About Us," About Acxiom, accessed September 26, 2022, https://www.acxiom.com/about-us/.


28. Oscar H. Gandy, "Matrix Multiplication and the Digital Divide," in Race after the Internet, ed. Lisa Nakamura and Peter Chow-White (New York: Routledge, 2011), 128-145.


29. For why, see Elizabeth M. Renieris, Beyond Data (Cambridge, MA: MIT Press, 2023), chaps. 1, 2.


30. Brian Mastroianni, "Location Data from Just Two of Your Apps Is Enough to Reveal Your Identity," CBS News, April 14, 2016, https://www.cbsnews.com/news/location-data-from-two-apps-is-enough-to-identify-you/.


31. This piece offers a nuanced and informative discussion about identifiability and technology: Lisa Austin and David Lie, "Bill C-11 and Exceptions to Consent for De-Identified Personal Information," Schwartz Reisman Institute, January 11, 2021, https://srinstitute.utoronto.ca/news/austin-lie-deidentified-personal-information-c11.


32. On this point, see also Julie E. Cohen, Between Truth and Power: The Legal Constructions of Informational Capitalism (New York: Oxford University Press, 2019), chap. 2, and Nick Couldry and Ulises A. Mejias, The Costs of Connection: How Data Is Colonizing Human Life and Appropriating It for Capitalism (Stanford, CA: Stanford University Press, 2019).


33. Co-creation is not "co-production," a concept from science and technology studies. For a helpful essay collection, see Sheila Jasanoff, States of Knowledge: The Co-Production of Science and the Social Order (Abingdon, Oxon: Routledge, 2004). Co-production is about the mutual entanglement between science and society. Co-creation, is about how people, as data sources, are as integral to the process of data creation as the data collectors. It is entirely about social and power relationships.


34. Zuboff, The Age of Surveillance Capitalism.


35. On this point and its ties to capitalism, see Couldry and Mejias, The Costs of Connection.


36. Marion Fourcade and Kieran Healy, "Seeing like a Market," Socio-Economic Review 15, no. 1 (2017): 9-29; Marion Fourcade and Fleur Johns, "Loops, Ladders and Links: The Recursivity of Social and Machine Learning," Theory and Society 49, no. 5 (October 1, 2020): 803-832.


37. Cathal Kelly, Boy Wonders: A Memoir (Toronto: Doubleday Canada, 2018), 243.


38. See Paula Kift and Helen Nissenbaum, "Normative Analysis of the NSA's Bulk Telephony," I/S: Journal of Law and Policy for the Information Society 13, no. 2 (2016); Daragh Murray and Pete Fussey, "Bulk Surveillance in the Digital Age: Rethinking the Human Rights Law Approach to Bulk Monitoring of Communications Data," Israel Law Review 52, no. 1 (2019): 31-60.


39. Lee Ferran, "Ex-NSA Chief: 'We Kill People Based on Metadata,'" ABC News, May 12, 2014, http://abcnews.go.com/blogs/headlines/2014/05/ex-nsa-chief-we-kill-people-based-on-metadata.


40. Mayer-Schönberger and Cukier, Big Data; Illah Reza Nourbakhsh and Jennifer Keating, AI and Humanity (Cambridge, MA: MIT Press, 2020).


41. Rob Kitchin, The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences (Thousand Oaks, CA: Sage, 2014). Chapter 3 offers a historical and much more technically detailed analysis of the idea of linked data.


42. Zuboff, The Age of Surveillance Capitalism.


43. Avi Goldfarb and Catherine Tucker, "Privacy and Innovation," Innovation Policy and the Economy 12 (January 2012): 65-90; Crawford, Atlas of AI.


44. Arvind Narayanan and Vitaly Shmatikov, "How to Break Anonymity of the Netflix Prize Dataset," arXiv:Cs/0610105, November 22, 2007, http://arxiv.org/abs/cs/0610105; Srivatsava Ranjit Ganta, Shiva Prasad Kasiviswanathan, and Adam Smith, "Composition Attacks and Auxiliary Information in Data Privacy," arXiv:0803.0032 [Cs], March 31, 2008, http://arxiv.org/abs/0803.0032.


45. On communicable disease, see Simon I. Hay et al., "Big Data Opportunities for Global Infectious Disease Surveillance," PLOS Medicine 10, no. 4 (April 2, 2013). On cancer, see Neil Savage, "How AI Is Improving Cancer Diagnostics," Nature 579, no. 7800 (March 25, 2020): S14-16; Andrew Myers, "Using AI to Personalize Cancer Care," Stanford HAI (blog), August 9, 2021, https://hai.stanford.edu/news/using-ai-personalize-cancer-care-0.


46. Will Douglas Heaven, "Hundreds of AI Tools Have Been Built to Catch Covid: None of Them Helped," MIT Technology Review, July 30, 2021; Emil Walleser, "When the World Needed It Most, Artificial Intelligence Failed: How COVID-19 Poked Holes in AI," Towards Data Science (blog), August 4, 2021, https://towardsdatascience.com/when-the-world-needed-it-most-artificial-intelligence-failed-how-covid-19-poked-holes-in-ai-38b742ddc222; Bhaskar Chakravorti, "Why AI Failed to Live Up to Its Potential during the Pandemic," Harvard Business Review, March 17, 2022.


47. New York Times reporter Kashmir Hill did a series of reports on this phenomenon, including reporting on a Toronto-based woman who was tormenting people from public computers. Kashmir Hill, "A Vast Web of Vengeance," New York Times, January 30, 2021; Kashmir Hill, "Woman Accused of Defaming Dozens Online Is Arrested," New York Times, February 10, 2021.


48. "Scams, Reviews, Complaints, Lawsuits and Frauds. File a Report, Post Your Review," Ripoff Report, accessed May 6, 2022, https://www.ripoffreport.com/.


49. As of writing, this website no longer exists.


50. "She's a Homewrecker," accessed May 6, 2022, https://shesahomewrecker.com.


51. Hill, "A Vast Web of Vengeance."


52. Dave Dale, "Travis Alkins among 150 Internet Harassment Victims in Ground-Breaking Court Case," Toronto Star, March 1, 2021.


53. Aaron Krolik and Kashmir Hill, "The Slander Industry," New York Times, April 24, 2021.


54. "Personicx," accessed January 25, 2023, https://www.personicx.co.uk/personicx.html.


55. Zeynep Tufekci, Twitter and Tear Gas: The Power and Fragility of Networked Protest (New Haven , CT: Yale University Press, 2018).


56. "MyHeritage Releases Groundbreaking Feature to Animate the Faces in Still Photos," Business Wire, February 25, 2021, https://www.businesswire.com/news/home/20210225005747/en/MyHeritage-Releases-Groundbreaking-Feature-to-Animate-the-Faces-in-Still-Photos.


57. Zuboff, The Age of Surveillance Capitalism; Shoshana Zuboff, "You Are the Object of a Secret Extraction Operation," New York Times, November 12, 2021; Nikos Smyrnaios, Internet Oligopoly: The Corporate Takeover of Our Digital World (Bingley, UK: Emerald Publishing, 2018).


58. Joel D. Cameron, "Stasi: East German Government," in Encyclopedia Britannica, August 17, 2021, https://www.britannica.com/topic/Stasi.


59. "Data Never Sleeps 10.0," accessed January 31, 2023, https://www.domo.com/data-never-sleeps?utm_source=wire&utm_medium=pr&utm_campaign=PR_DNS10_22&campid=7015w000000vccjAAA.


60. Rob Reich, Mehran Sahami, and Jeremy M. Weinstein, System Error: Where Big Tech Went Wrong and How We Can Reboot (New York: Harper, 2021).


61. Fred Turner, From Counterculture to Cyberculture: Stewart Brand, the Whole Earth Network, and the Rise of Digital Utopianism (Chicago: University of Chicago Press, 2006).


62. Stephen Hopgood, The Endtimes of Human Rights (Ithaca, NY: Cornell University Press, 2015), viii.


63. Clifford Bob, Rights as Weapons: Instruments of Conflict, Tools of Power (Princeton, NJ: Princeton University Press, 2019).


64. For a clear example, see Jack Donnelly, Universal Human Rights in Theory and Practice, 3rd ed. (Ithaca, NY: Cornell University Press, 2013), 42-43.


65. This list is hardly exhaustive, but it represents broadly the scope of human rights writing that has taken a stance in recent decades. For supportive arguments, Michael Ignatieff, Human Rights as Politics and Idolatry (Princeton, NJ: Princeton University Press, 2001); Charles R. Beitz, The Idea of Human Rights (Oxford: Oxford University Press, 2009); Beth A. Simmons, Mobilizing for Human Rights: International Law in Domestic Politics (Cambridge: Cambridge University Press, 2009); Kathryn Sikkink, The Justice Cascade: How Human Rights Prosecutions Are Changing World Politics (New York: Norton, 2011); Kathryn Sikkink, Evidence for Hope: Making Human Rights Work in the 21st Century (Princeton, NJ; Princeton University Press, 2017); James D. Ingram, "What Is a 'Right to Have Rights'? Three Images of the Politics of Human Rights," American Political Science Review 102, no. 4 (2008): 401-416; Alison Brysk, Speaking Rights to Power: Constructing Political Will (New York: Oxford University Press, 2013); Alison Brysk, The Future of Human Rights (Cambridge: Polity Press, 2018); William F. Schulz and Sushma Raman, The Coming Good Society: Why New Realities Demand New Rights (Cambridge, MA: Harvard University Press, 2020). For critical views, see Makau Mutua, "The Ideology of Human Rights," Virginia Journal of International Law 36, no. 3 (January 1, 1996): 589-657; Samuel Moyn, The Last Utopia: Human Rights in History (Cambridge, MA: Belknap Press of Harvard University Press, 2010); Samuel Moyn, Not Enough: Human Rights in an Unequal World (Cambridge, MA: Belknap Press of Harvard University Press, 2018); Eric A. Posner, The Twilight of Human Rights Law (Oxford: Oxford University Press, 2014); Hopgood, The Endtimes of Human Rights; Bob, Rights as Weapons.


66. Two excellent places to start: Johannes Morsink, The Universal Declaration of Human Rights: Origins, Drafting and Intent (Philadelphia: University of Pennsylvania Press, 1999); Mary Ann Glendon, A World Made New: Eleanor Roosevelt and the Universal Declaration of Human Rights (New York: Random House, 2001).


67. For a comprehensive review, see Emilie M. Hafner-Burton, "International Regimes for Human Rights," Annual Review of Political Science 15, no. 1 (2012): 265-286.


68. For a historically informed discussion of the reasons why people see human rights differently, see Andrea Paras, "Rights," in The Oxford Handbook of History and International Relations, ed. Mlada Bukovansky, Edward Keene, Christian Reus-Smit, and Maja Spanu (Oxford: Oxford University Press, 2023).


69. For the UN, see "The Core International Human Rights Instruments and Their Monitoring Bodies," Office of the United Nations High Commissioner for Human Rights, accessed May 6, 2022, https://www.ohchr.org/en/core-international-human-rights-instruments-and-their-monitoring-bodies.


70. For the African Charter on Human and Peoples' Rights, see "African Charter on Human and Peoples' Rights," African Union, accessed May 6, 2022, https://au.int/en/treaties/african-charter-human-and-peoples-rights.


71. For the European Convention on Human Rights, see "European Convention on Human Rights," Council of Europe, accessed May 6, 2022, https://www.coe.int/en/web/human-rights-convention/home.


72. "Basic Documents—American Convention," Organization of American States, accessed May 6, 2022, https://www.cidh.oas.org/basicos/english/basic3.american%20convention.htm.


73. Kenneth Roth, "Defending Economic, Social and Cultural Rights: Practical Issues Faced by an International Human Rights Organization," Human Rights Quarterly 26, no. 1 (2004): 63-73.


74. Sue Anne Teo, "How Artificial Intelligence Systems Challenge the Conceptual Foundations of the Human Rights Legal Framework," Nordic Journal of Human Rights (June 6, 2022): 1-19.


75. See O'Neil, Weapons of Math Destruction, for this example, and Virginia Eubanks, Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor (New York: St. Martin's Press, 2018) and many others on the use of algorithms for social services.


76. On the use of AI in refugee hearings, see Hilary Evans Cameron, Avi Goldfarb, and Leah Morris, "Artificial Intelligence for a Reduction of False Denials in Refugee Claims," Journal of Refugee Studies 35, no. 1 (March 1, 2022): 493-510.


77. For many political scientists, tracing international human rights to the end of World War II and the Universal Declaration of Human Rights marks how conceptions of human rights became globally applicable. Ideas around "the rights of man" existed well before, but they were limited to certain countries or certain groups. Still, some accounts do trace human rights history much further back. For more, see Paul Gordon Lauren, The Evolution of International Human Rights: Visions Seen, 2nd ed. (Philadelphia: University of Pennsylvania Press, 2003), and Micheline Ishay, The History of Human Rights: From Ancient Times to the Globalization Era, 2nd ed. (Berkeley: University of California Press, 2008).


78. Dafna Dror-Shpoliansky and Yuval Shany, "It's the End of the (Offline) World as We Know It: From Human Rights to Digital Human Rights—A Proposed Typology," European Journal of International Law 32, no. 4 (2021): 1249-1282.


79. For a US-specific proposal: Eric Lander and Alondra Nelson, "Americans Need a Bill of Rights for an AI-Powered World," Wired, October 8, 2021, https://www.wired.com/story/opinion-bill-of-rights-artificial-intelligence/.


80. A variety of books have been published on this topic, from the academic to general. These offer different views of the topic. Danielle Keats Citron, Hate Crimes in Cyberspace (Cambridge, MA: Harvard University Press, 2014); Jon Ronson, So You've Been Publicly Shamed (New York: Riverhead Books, 2015); Carrie Goldberg, Nobody's Victim: Fighting Psychos, Stalkers, Pervs, and Trolls (New York: Plume, 2019); David Kaye, Speech Police: The Global Struggle to Govern the Internet (New York: Columbia Global Reports, 2019); Jeff Kosseff, The Twenty-Six Words That Created the Internet (Ithaca, NY: Cornell University Press, 2019).


81. For some examples, see American Association for the Advancement of Science, "Geospatial Technologies and Human Rights," accessed April 23, 2022, https://www.aaas.org/programs/scientific-responsibility-human-rights-law/geospatial-technologies-and-human-rights-%E2%80%93-ethiopian-occupation.


82. On the issue of "digital bodies," see Kristin Bergtora Sandvik, "Digital Dead Body Management (DDBM): Time to Think It Through," Journal of Human Rights Practice 12, no. 2 (December 4, 2020): 428-443. On human rights and technology in general, see Molly K. Land and Jay D. Aronson, eds., New Technologies for Human Rights Law and Practice (New York: Cambridge University Press, 2018); Molly K. Land and Jay D. Aronson, "Human Rights and Technology: New Challenges for Justice and Accountability," Annual Review of Law and Social Science 16 (September 10, 2020): 223-240.


83. Helmut Philipp Aust, "Undermining Human Agency and Democratic Infrastructures? The Algorithmic Challenge to the Universal Declaration of Human Rights," American Journal of International Law 112 (2018): 334-338.


84. Frances Haugen, "Facebook Whistleblower Frances Haugen's Senate Testimony," Washington Post, October 4, 2021; Karen Hao, "The Facebook Whistleblower Says Its Algorithms Are Dangerous. Here's Why," MIT Technology Review, October 5, 2021; Jim Waterson and Dan Milmo, "Facebook Whistleblower Frances Haugen Calls for Urgent External Regulation," Guardian, October 25, 2021.


85. Dafna Dror-Shpoliansky, "Putting the Genie Back in the Bottle? On 'Facebook Papers' and the Way Forward," The Federmann Cyber Security Research Center (blog), November 14, 2021, https://csrcl.huji.ac.il/blog/putting-genie-back-bottle-facebook-papers-and-way-forward.


86. Katie Collins, "Kids Are Being Exploited Online Every Day—Sometimes at the Hands of Their Parents," CNET, August 7, 2022.


87. "Center for Humane Technology," accessed May 6, 2022, https://www.humanetech.com/.


88. Kate Eichhorn, "Why an Internet That Never Forgets Is Especially Bad for Young People," MIT Technology Review, December 27, 2019.


89. The text of the treaty is here: "Convention on the Elimination of All Forms of Discrimination against Women," New York, December 18, 1979, Office of the United Nations High Commissioner for Human Rights, accessed May 6, 2022, https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-elimination-all-forms-discrimination-against-women.


90. Viktor Mayer-Schönberger, Delete: The Virtue of Forgetting in the Digital Age (Princeton, NJ: Princeton University Press, 2009); Nicola Wright, "Death and the Internet: The Implications of the Digital Afterlife," First Monday 19, no. 6 (June 2014).


91. For legal reasoning on this expansion, Yael Ronen, "Human Rights Obligations of Territorial Non-State Actors," Cornell International Law Journal 46, no. 1 (January 1, 2013): 21-50.


92. United Nations, Guiding Principles on Business and Human Rights (New York: United Nations, 2011).


93. "Companies," Business and Human Rights Resource Centre, accessed May 6, 2022, https://www.business-humanrights.org/en/companies/.


94. "About Us," Business and Human Rights Resource Centre, accessed May 6, 2022, https://www.business-humanrights.org/en/about-us/; "Big Issues," Business and Human Rights Resource Centre, accessed May 6, 2022, https://www.business-humanrights.org/en/big-issues/.


95. Arvind Ganesan, "Why Laws Are Needed to Avoid Corporate Rights Abuses," Human Rights Watch (blog), July 1, 2021, https://www.hrw.org/news/2021/07/01/why-laws-are-needed-avoid-corporate-rights-abuses; "Corporations," Amnesty International, accessed May 6, 2022, https://www.amnesty.org/en/what-we-do/corporate-accountability/.


96. "The GNI Principles," Global Network Initiative (blog), accessed May 6, 2022, https://globalnetworkinitiative.org/gni-principles/.


97. Platforms are not easy to define; however, the definition offered here is comprehensive and useful: Tarleton Gillespie, Custodians of the Internet: Platforms, Content Moderation, and the Hidden Decisions That Shape Social Media (New Haven, CT: Yale University Press, 2018).


98. For a thorough exploration, see Robert Gorwa, "What Is Platform Governance?," Information, Communication and Society 22, no. 6 (May 12, 2019): 854-871.


99. Though these products are also increasingly part of platforms as internet-connected devices. Nonetheless, traditional stoves and cars, once sold, were not taking data from their users.


100. Yael Ronen, "Big Brother's Little Helpers: The Right to Privacy and the Responsibility of Internet Service Providers," Utrecht Journal of International and European Law 31, no. 80 (February 27, 2015): 72-86; Dafna Dror-Shpoliansky and Yuval Shany, "It's the End of the (Offline) World as We Know It: From Human Rights to Digital Human Rights—A Proposed Typology," European Journal of International Law 32, no. 4 (2021): 1249-1282.


101. NSO Group's spyware, "NSO GROUP—Cyber Intelligence for Global Security and Stability," NSO Group, accessed May 6, 2022, https://www.nsogroup.com/. For a basic explainer, David Pegg and Sam Cutler, "What Is Pegasus Spyware and How Does It Hack Phones?," Guardian, July 18, 2021.


102. Bill Marczak et al., "HIDE AND SEEK: Tracking NSO Group's Pegasus Spyware to Operations in 45 Countries" (University of Toronto, September 18, 2018), https://citizenlab.ca/2018/09/hide-and-seek-tracking-nso-groups-pegasus-spyware-to-operations-in-45-countries/; Bill Marczak et al., "Stopping the Press: New York Times Journalist Targeted by Saudi-Linked Pegasus Spyware Operator" (University of Toronto, January 28, 2020), https://citizenlab.ca/2020/01/stopping-the-press-new-york-times-journalist-targeted-by-saudi-linked-pegasus-spyware-operator/; Bill Marczak et al., "From Pearl to Pegasus: Bahraini Government Hacks Activists with NSO Group Zero-Click IPhone Exploits" (University of Toronto, August 24, 2021), https://citizenlab.ca/2021/08/bahrain-hacks-activists-with-nso-group-zero-click-iphone-exploits/; Bill Marczak et al., "Pearl 2 Pegasus: Bahraini Activists Hacked with Pegasus Just Days after a Report Confirming Other Victims" (University of Toronto, February 18, 2022), https://citizenlab.ca/2022/02/bahraini-activists-hacked-with-pegasus/.


103. Daniel Lippman and Emily Birnbaum, "The Secret behind Amazon's Domination in Cloud Computing," Politico, June 4, 2021.


104. Kate Crawford, Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence (New Haven, CT: Yale University Press, 2021). She documents this insightfully in her book.


105. Stuart J. Russell, Human Compatible: Artificial Intelligence and the Problem of Control (New York: Viking Press, 2019).


106. Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (Oxford: Oxford University Press, 2016).




Chapter 3


1. Dave Lee, "What Is the Right to Be Forgotten?," BBC News, May 13, 2014, https://www.bbc.com/news/technology-27394751.


2. Ashifa Kassam, "Spain's Everyday Internet Warrior Who Cut Free from Google's Tentacles," Guardian, May 13, 2014.


3. "The Unforgettable Story of the Seizure to the Defaulter Mario Costeja González That Happened in 1998," Derechoaleer (blog), May 30, 2014, http://derechoaleer.org/en/blog/2014/05/the-unforgettable-story-of-the-seizure-to-the-defaulter-mario-costeja-gonzalez-that-happened-in-1998.html.


4. Miquel Peguera, "No More Right-to-Be-Forgotten for Mr. Costeja, Says Spanish Data Protection Authority," Center for Internet and Society at Stanford Law School (blog), October 3, 2015, https://cyberlaw.stanford.edu/blog/2015/10/no-more-right-be-forgotten-mr-costeja-says-spanish-data-protection-authority.


5. "Requests to Delist Content under European Privacy Law—Google Transparency Report," accessed January 25, 2023, https://transparencyreport.google.com/eu-privacy/overview?hl=en_GB.


6. Shoshana Zuboff, The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power (New York: PublicAffairs, 2019), 8.


7. Sandra Wachter and Brent Mittelstadt, "A Right to Reasonable Inferences: Re-Thinking Data Protection Law in the Age of Big Data and AI Survey: Privacy, Data, and Business," Columbia Business Law Review 2019, no. 2 (2019): 494-620, refers to the Article 29 Working Party's typology of provided, observed, and inferred data.


8. Respectively, see "Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment," Office of the United Nations High Commissioner for Human Rights, accessed April 19, 2022, https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-against-torture-and-other-cruel-inhuman-or-degrading; "International Covenant on Civil and Political Rights," Office of the United Nations High Commissioner for Human Rights, Article 7, accessed August 11, 2022, https://www.ohchr.org/en/instruments-mechanisms/instruments/international-covenant-civil-and-political-rights; "United Nations Office on Genocide Prevention and the Responsibility to Protect," accessed August 11, 2022, https://www.un.org/en/genocideprevention/genocide-convention.shtml as the global referents.


9. Nicholas C. Zakas, "HTTP Cookies Explained," Human Who Codes (blog), May 5, 2009, https://humanwhocodes.com/blog/2009/05/05/http-cookies-explained/.


10. Jose van Dijck, "Datafication, Dataism and Dataveillance: Big Data between Scientific Paradigm and Ideology," Surveillance and Society 12, no. 2 (May 9, 2014): 197-208.


11. Theodore M. Porter, Trust in Numbers: The Pursuit of Objectivity in Science and Public Life (Princeton, NJ: Princeton University Press, 2020), 21-24.


12. Trevor J. Barnes and Matthew W. Wilson, "Big Data, Social Physics, and Spatial Analysis: The Early Years," Big Data and Society 1, no. 1 (April 1, 2014).


13. See Colin Koopman, How We Became Our Data: A Genealogy of the Informational Person (Chicago: University of Chicago Press, 2019); Jacqueline Wernimont, Numbered Lives: Life and Death in Quantum Media (Cambridge, MA: MIT Press, 2019); E. Stefan Kehlenbach, "Behind the Silicon Curtain: A Critical Theory of Big Data" (Riverside, CA: UC Riverside, 2022), https://escholarship.org/uc/item/9bz858d9.


14. Rita Raley, "Dataveillance and Counterveillance," in Raw Data Is an Oxymoron, ed. Lisa Gitelman (Cambridge, MA: MIT Press, 2013), 121-146; "The World's Most Valuable Resource Is No Longer Oil, but Data," Economist, May 6, 2017.


15. For applications of these various labels, see Tobias Berg, On the Rise of FinTechs—Credit Scoring Using Digital Footprints (Cambridge, MA: National Bureau of Economic Research, 2018); Paul M. Leonardi, "COVID-19 and the New Technologies of Organizing: Digital Exhaust, Digital Footprints, and Artificial Intelligence in the Wake of Remote Work," Journal of Management Studies 58, no. 1 (2021): 247-251; Chanda Simfukwe et al., "Digital Trail Making Test-Black and White: For Android and IOS," Alzheimer's and Dementia 16 (2020); Johann A. R. Roduit, "Digital Dust," Medical Humanities 43, no. 4 (2017); Bradley Schatz, Pavel Gladyshev, and Ronald M. van der Knijff, "The Internet of Things: Interconnected Digital Dust," Digital Investigation 11, no. 3 (2014): 141-142; Sarah T. Roberts, "Digital Detritus: 'Error' and the Logic of Opacity in Social Media Content Moderation," First Monday 23, no. 3 (2018); Nils Jean, "Digital Debris of Internet Art: An Allegorical and Entropic Resistance to the Epistemology of Search," Leonardo (Oxford) 50, no. 5 (2017).


16. Rob Kitchin, The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences (Thousand Oaks, CA: Sage, 2014).


17. Dan Bouk, How Our Days Became Numbered: Risk and the Rise of the Statistical Individual (Chicago: University of Chicago Press, 2015).


18. Dan Bouk, Democracy's Data (New York: MCD, 2022).


19. Sun-ha Hong, Technologies of Speculation: The Limits of Knowledge in a Data-Driven Society (New York: NYU Press, 2020).


20. Shoshana Zuboff, The Age of Surveillance Capitalism; Shoshana Zuboff, "You Are the Object of a Secret Extraction Operation," New York Times, November 12, 2021; Nikos Smyrnaios, Internet Oligopoly: The Corporate Takeover of Our Digital World (Bingley, UK: Emerald Publishing, 2018).


21. Dario Amodei et al., "Concrete Problems in AI Safety," arXiv:1606.06565 [Cs], July 25, 2016, http://arxiv.org/abs/1606.06565; Gary Marcus and Ernest Davis, Rebooting AI: Building Artificial Intelligence We Can Trust (New York: Pantheon Books, 2019).


22. 23andMe, "About Us—23andMe," accessed May 7, 2022, https://www.23andme.com/en-ca/about/.


23. "GSK and 23andMe Sign Agreement to Leverage Genetic Insights for the Development of Novel Medicines | GSK," July 25, 2018, https://www.gsk.com/en-gb/media/press-releases/gsk-and-23andme-sign-agreement-to-leverage-genetic-insights-for-the-development-of-novel-medicines/; Denise Roland, "How Drug Companies Are Using Your DNA to Make New Medicine," Wall Street Journal, July 22, 2019.


24. Kari Paul, "Fears over DNA Privacy as 23andMe Plans to Go Public in Deal with Richard Branson," Guardian, February 9, 2021.


25. Jessica Hamzelou, "23andMe Has Sold the Rights to Develop a Drug Based on Its Users' DNA," New Scientist, January 10, 2020, https://www.newscientist.com/article/2229828-23andme-has-sold-the-rights-to-develop-a-drug-based-on-its-users-dna/.


26. Washington University v. Catalona, No. 490 F.3d 667 (8th Cir. 2007); Barbara E. Bierer, Mark Barnes, and Holly Fernandez Lynch, "Revised 'Common Rule' Shapes Protections for Research Participants," Health Affairs 36, no. 5 (May 2017): 784-788.


27. The story is pulled together from Dani Anguiano, "Golden State Killer: Joseph DeAngelo Sentenced to Life in Prison," Guardian, August 21, 2020; Paige St. John, "The Untold Story of How the Golden State Killer Was Found: A Covert Operation and Private DNA," Los Angeles Times, December 8, 2020; Laurel Wamsley, "After Arrest of Suspected Golden State Killer, Details of His Life Emerge," NPR, April 26, 2018, https://www.npr.org/sections/thetwo-way/2018/04/26/606060349/after-arrest-of-suspected-golden-state-killer-details-of-his-life-emerge; Christine Hitt, "How the Golden State Killer Case May Help ID Pearl Harbor Unknowns," SFGATE, December 7, 2021 https://www.sfgate.com/hawaii/article/Golden-State-Killer-case-may-help-ID-Pearl-Harbor-16679511.php.


28. For the history of DNA in crime investigations, see Simon A. Cole, Suspect Identities: A History of Fingerprinting and Criminal Identification (Cambridge, MA: Harvard University Press, 2001), chap. 12.


29. 23andMe, "Guide for Law Enforcement—23andMe," accessed May 7, 2022, https://www.23andme.com/law-enforcement-guide/; Zack Whittaker, "Ancestry Says It Fought Two Police Requests to Search Its DNA Database," TechCrunch (blog), February 10, 2021, https://social.techcrunch.com/2021/02/10/ancestry-police-warrant-dna-database/.


30. Salvador Hernandez, "Investigative Genealogy Helped Police Catch Serial Killers and Rapists. Now Cases Are Going Unsolved," BuzzFeed News, October 26, 2019, https://www.buzzfeednews.com/article/salvadorhernandez/dna-police-genetic-genealogy-serial-killers-case-gedmatch.


31. Virginia Hughes, "Two New Laws Restrict Police Use of DNA Search Method," New York Times, May 31, 2021.


32. Lindzi Wessel, "Scientists Concerned over US Plans to Collect DNA Data from Immigrants," Nature, October 7, 2019; Evan Frohman, "23PolicemenAndMe: Analyzing the Constitutional Implications of Police Use of Commercial DNA Databases," University of Pennsylvania Journal of Constitutional Law 22, no. 5 (2020): 1495-1522; "Ethical Concerns of DNA Databases Used for Crime Control," Bill of Health (blog), January 14, 2019, https://blog.petrieflom.law.harvard.edu/2019/01/14/ethical-concerns-of-dna-databases-used-for-crime-control/; Antony Barone Kolenc, "23 and Plea: Limiting Police Use of Genealogy Sites after Carpenter v. United States," West Virginia Law Review 122 (2019): 53.


33. Greg Miller, "Scientists Discover How to Identify People from 'Anonymous' Genomes," Wired, January 17, 2013, https://www.wired.com/2013/01/your-genome-could-reveal-your-identity/.


34. Melissa Gymrek et al., "Identifying Personal Genomes by Surname Inference," Science 339, no. 6117 (January 18, 2013): 324.


35. See some examples: "What Is Personal Data?," Information Commissioner's Office (January 1, 2021), https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/key-definitions/what-is-personal-data/; Office of the Privacy Commissioner of Canada, "PIPEDA in Brief," January 9, 2018, https://www.priv.gc.ca/en/privacy-topics/privacy-laws-in-canada/the-personal-information-protection-and-electronic-documents-act-pipeda/pipeda_brief/; Office of the Attorney General (Xavier Becerra), "California Consumer Privacy Act (CCPA) Fact Sheet" (California Department of Justice, 2019), https://www.oag.ca.gov/system/files/attachments/press_releases/CCPA%20Fact%20Sheet%20%2800000002%29.pdf; "What Is Personal Data?," Text, European Commission—European Commission, accessed May 3, 2022, https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en.


36. Bill Chappell, "You Can Now Ask Google to Take Your Personal Data out of Its Search Results," NPR, May 2, 2022 https://www.npr.org/2022/05/02/1095883070/google-personal-data-delete-omit.


37. See, for example, Microsoft's privacy statement: "Microsoft Privacy Statement—Microsoft Privacy," accessed May 3, 2022, https://privacy.microsoft.com/en-ca/privacystatement It makes clear that "personal data" include many different user behaviors and inputs that go beyond government conceptions.


38. Louise Amoore, "Machine Learning Political Orders," Review of International Studies (2022): 1-17.


39. "California Consumer Privacy Act (CCPA)," State of California—Department of Justice—Office of the Attorney General, October 15, 2018, https://oag.ca.gov/privacy/ccpa.


40. Steven Levy, Facebook: The inside Story (New York: Blue Rider Press, 2020), 215-216.


41. Jens-Erik Mai, "Big Data Privacy: The Datafication of Personal Information," Information Society 32, no. 3 (May 26, 2016): 192-199.


42. Karen E. C. Levy, "Relational Big Data," Stanford Law Review 66 (September 3, 2013); Karen E. C. Levy, "Intimate Surveillance," Idaho Law Review 51, no. 3 (2015 2014): 679-694.


43. Levy, "Relational Big Data," and "Intimate Surveillance," 74.


44. "Should Parents Use Cell Phone Monitoring App to Track Your Kids?," mspy, accessed May 7, 2022, https://www.mspy.com/features.html.


45. Ryan Mac and Kashmir Hill, "Are Apple AirTags Being Used to Track People and Steal Cars?," New York Times, December 30, 2021.


46. Kashmir Hill and Photographs by Todd Heisler, "I Used Apple AirTags, Tiles and a GPS Tracker to Watch My Husband's Every Move," New York Times, February 11, 2022.


47. Lauren Kaori Gurley, "Amazon Denies Workers Pee in Bottles. Here Are the Pee Bottles," Vice, March 25, 2021, https://www.vice.com/en/article/k7amyn/amazon-denies-workers-pee-in-bottles-here-are-the-pee-bottles.


48. Jay Greene, "Amazon's Employee Surveillance Fuels Unionization Efforts: 'It's Not Prison, It's Work,'" Washington Post, December 2, 2021.


49. Laura Hautala, "Shadow Profiles: Facebook Has Information You Didn't Hand Over," CNET, April 11, 2018; SpiderOak, "Facebook Shadow Profiles: A Profile of You That You Never Created," Medium (blog), October 2, 2018, https://medium.com/@SpiderOak/facebook-shadow-profiles-a-profile-of-you-that-you-never-created-302f99f20930; Andrew Quodling, "Shadow Profiles—Facebook Knows about You, Even If You're Not on Facebook," The Conversation, April 13, 2018, http://theconversation.com/shadow-profiles-facebook-knows-about-you-even-if-youre-not-on-facebook-94804.


50. Karen E. C. Levy, "The User as Network," First Monday, November 4, 2015.


51. will.i.am, "We Need to Own Our Data as a Human Right—and Be Compensated for It," Economist, January 21, 2019.


52. See, for example, Susan Ariel Aaronson, "Data Is Different, and That's Why the World Needs a New Approach to Governing Cross-Border Data Flows," Digital Policy, Regulation and Governance 21, no. 5 (January 1, 2019): 441-460; Lothar Determann, "No One Owns Data," Hastings Law Journal 70, no. 1 (2019 2018): 1-44; Jeffrey Ritter and Anna Mayer, "Regulating Data as Property: A New Construct for Moving Forward," Duke Law & Technology Review 16, no. 1 (March 6, 2018): 220-77; Patrik Hummel, Matthias Braun, and Peter Dabrock, "Own Data? Ethical Reflections on Data Ownership," Philosophy and Technology, June 15, 2020.


53. For example: Jaron Lanier and E. Glen Weyl, "A Blueprint for a Better Digital Society," Harvard Business Review, September 26, 2018.


54. "Mine—The Future of Data Ownership," Mine, accessed May 7, 2022, https://saymine.com/; Chris O'Brien, "I Tried Mine's 'Right to Be Forgotten' Tool and, Yup, My Privacy Is a Disaster," VentureBeat, March 9, 2020, https://venturebeat.com/2020/03/09/i-tried-mines-right-to-be-forgotten-tool-and-yup-my-privacy-is-a-disaster/.


55. "Join Reklaim—Take Back What's Yours!," Reklaim, accessed May 7, 2022, https://www.reklaimyours.com.


56. Patience Haggin, "Personal Data Is Worth Billions. These Startups Want You to Get a Cut," Wall Street Journal, December 4, 2021.


57. "Welcome to Reklaim," Reklaim, accessed May 7, 2022, https://www.reklaimyours.com/learn/welcome-to-reklaim.


58. Daron Acemoglu et al., Too Much Data: Prices and Inefficiencies in Data Markets (Cambridge, MA: National Bureau of Economic Research, September 2019).


59. For a thoughtful discussion, see Martin Tisne, "It's Time for a Bill of Data Rights," MIT Technology Review, December 14, 2018.


60. Jathan Sadowski, "Companies Are Making Money from Our Personal Data—but at What Cost?," Guardian, August 31, 2016.


61. Alessandro Mantelero, "From Group Privacy to Collective Privacy: Towards a New Dimension of Privacy and Data Protection in the Big Data Era," in Group Privacy, ed. Linnet Taylor, Luciano Floridi, and Bart van der Sloot (Cham: Springer, 2017), 139-158; Martin Tisne, "Collective Data Rights Can Stop Big Tech from Obliterating Privacy," MIT Technology Review, May 25, 2021.


62. Frank Pasquale, "Odd Numbers. Algorithms Alone Can't Meaningfully Hold Other Algorithms Accountable," Real Life, August 20, 2018, https://reallifemag.com/odd-numbers/.


63. Marion Fourcade and Kieran Healy, "Seeing like a Market," Socio-Economic Review 15, no, 1 (2017); Marion Fourcade and Fleur Johns, "Loops, Ladders and Links: The Recursivity of Social and Machine Learning," Theory and Society 49, no. 5 (October 1, 2020): 803-832.


64. Alessandro Mantelero, "From Group Privacy to Collective Privacy: Towards a New Dimension of Privacy and Data Protection in the Big Data Era," in Group Privacy: New Challenges of Data Technologies, ed. Linnet Taylor, Luciano Floridi, and Bart van der Sloot (Cham: Springer, 2017), 144.


65. Wendy H. Wong, Internal Affairs: How the Structure of NGOs Transforms Human Rights (Ithaca, NY: Cornell University Press, 2012).


66. "UN Global Pulse Principles on Data Privacy an Protection: UN Global Pulse," February 5, 2020, https://www.unglobalpulse.org/policy/ungp-principles-on-data-privacy-and-protection/; David Kaye, "Report of the Special Rapporteur on the Promotion and Protection of the Right to Freedom of Opinion and Expression" (United Nations, August 29, 2018), https://freedex.org/wp-content/blogs.dir/2015/files/2018/10/AI-and-FOE-GA.pdf; UN General Assembly, "68/167. The Right to Privacy in the Digital Age (A/RES/69/166)" (United Nations, December 18, 2013), https://documents-dds-ny.un.org/doc/UNDOC/GEN/N13/449/47/PDF/N1344947.pdf?OpenElement.


67. Anu Bradford, "The Brussels Effect," Northwestern University Law Review 107, no. 1 (2012): 1-67, coins the term. Specific to the GDPR: Michael L. Rustad and Thomas H. Koenig, "Towards a Global Data Privacy Standard," Florida Law Review 71, no. 2 (2019): 365-454.


68. The CCPA was amended by a ballot initiative, through the passage of the California Privacy Rights Act (CPRA), that passed in 2020 and came into force January 1, 2023. The CPRA established the California Privacy Protection Agency to implement and enforce the law.


69. There is a vast literature about the GDPR and a growing literature on the CCPA and PIPL. For some starting points, Joanna Kessler, "Data Protection in the Wake of the GDPR: California's Solution for Protecting 'the World's Most Valuable Resource' Notes," Southern California Law Review 93, no. 1 (2020 2019): 99-128; Ke Xu et al., "Analyzing China's PIPL and How It Compares to the EU's GDPR," IAPP, August 24, 2021, https://iapp.org/news/a/analyzing-chinas-pipl-and-how-it-compares-to-the-eus-gdpr/; Aysem Diker Vanberg, "Informational Privacy Post GDPR—End of the Road or the Start of a Long Journey?," International Journal of Human Rights 25, no. 1 (July 9, 2020): 1-27; "Comparing Privacy Laws: GDPR v. CCPA," DataGuidance, September 12, 2020, https://www.dataguidance.com/resource/comparing-privacy-laws-gdpr-v-ccpa.


70. For a discussion as this relates to the GDPR, see Benjamin Wong, "Delimiting the Concept of Personal Data after the GDPR," Legal Studies 39, no. 3 (September 2019): 517-532.


71. Tal Z. Zarsky, "Incompatible: The GDPR in the Age of Big Data," Seton Hall Law Review 47, no. 4 (2017): 995-1020; Wachter and Mittelstadt, "A Right to Reasonable Inferences."


72. Aloni Cohen and Kobbi Nissim, "Towards Formalizing the GDPR's Notion of Singling Out," Proceedings of the National Academy of Sciences 117, no. 15 (April 14, 2020): 8344-8352; Aloni Cohen and Kobbi Nissim, "Linear Program Reconstruction in Practice," Journal of Privacy and Confidentiality 10, no. 1 (January 14, 2020).


73. Emily Baker-White and Sarah Emerson, "Facebook Gave Nebraska Cops a Teen's DMs. They Used Them to Prosecute Her for Having an Abortion," Forbes, August 8, 2022, https://www.forbes.com/sites/emilybaker-white/2022/08/08/facebook-abortion-teen-dms/; Albert Fox Cahn, "Facebook's Message Encryption Was Built to Fail," Wired, August 10, 2022, https://www.wired.com/story/facebook-message-encryption-abortion/.


74. Sylvie Delacroix and Neil D. Lawrence, "Bottom-Up Data Trusts: Disturbing the 'One Size Fits All' Approach to Data Governance," International Data Privacy Law 9, no. 4 (November 1, 2019): 236-252; Open Data Institute, "Data Trusts: Legal and Governance Considerations (Report)—The ODI," Open Data Institute, accessed January 2, 2021, https://theodi.org/article/data-trusts-legal-report/; Jeremiah Lau Jia Jun, J. E. Penner, and Benjamin Wong, "The Basics of Private and Public Data Trusts," SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, September 23, 2019); George Zarkadakis, "'Data Trusts' Could Be the Key to Better AI," Harvard Business Review, November 10, 2020; Christine Rinik, "Data Trusts: More Data Than Trust? The Perspective of the Data Subject in the Face of a Growing Problem," International Review of Law, Computers and Technology 34, no. 3 (2020): 342-363; Kieran O'Hara, "Data Trusts Ethics, Architecture and Governance for Trustworthy Data Stewardship" (University of Southampton Web Science Institute, February 2019); Wendy Hall and Jérôme Presenti, "Growing the Artificial Intelligence Industry in the UK," October 15, 2017, https://www.gov.uk/government/publications/growing-the-artificial-intelligence-industry-in-the-uk.


75. McCammond had also tweeted anti-LGBT posts.


76. Justin Curto, "Teen Vogue Editor-in-Chief Backs Out over Anti-Asian Tweets," Vulture, March 18, 2021, https://www.vulture.com/2021/03/teen-vogue-new-editor-alexi-mccammond-racist-tweets.html; Katie Robertson, "Teen Vogue Staff Members Condemn Editor's Decade-Old, Racist Tweets," New York Times, March 9, 2021; Alexi McCammond [@alexi], "Hey There: I've Decided to Part Ways with Condé Nast. Here Is My Statement about Why—Https://T.Co/YmnHVtZSce," Tweet, Twitter, March 18, 2021, https://twitter.com/alexi/status/1372603793825751040; Diana Tsui [@chupsterette], "'Because Stories Are Fleeting I'm Putting This in the Feed. I'm Tired of Big Media Organizations Pretending to Give a Damn about Diversity . . . ,'" Instagram, March 7, 2021, https://www.instagram.com/p/CMIKWZegh4n/.


77. Anne Anlin Cheng, "Opinion: What This Wave of Anti-Asian Violence Reveals about America," New York Times, February 21, 2021; Qian Julie Wang, "Anti-Asian Racism Isn't New," New York Times, February 18, 2021 Arthur Tam, "Anti-Asian Hate Is as Bad as Ever. Why Has Attention to It Waned?," Washington Post, August 2021; Hae Yeon Choo and Robert Diaz, "Addressing Anti-Asian Racism in the University," Inside Higher Ed (blog), April 2, 2021, https://www.insidehighered.com/advice/2021/04/02/recommendations-stopping-anti-asian-racism-campuses-opinion.


78. Gillian Brockell, "The Long, Ugly History of Anti-Asian Racism and Violence in the U.S.," Washington Post, March 18, 2021.


79. Madeline Holcombe and Dakin Andone, "A Trip to the Spa That Ended in Death. These Are Some of the Victims of the Atlanta-Area Shootings," CNN, March 22, 2021, https://www.cnn.com/2021/03/18/us/atlanta-spa-shootings-victims/index.html.




Chapter 4


1. 'Sway,' "She's Taking Jeff Bezos to Task," New York Times, April 19, 2021, https://www.nytimes.com/2021/04/19/opinion/sway-kara-swisher-joy-buolamwini.html.


2. Facial recognition has been used to reunite a child with his parents, as was in the case in 2020: Poornima Weerasekara, "Facial Recognition Helps Reunite Kidnapped Toddler with Family after 32 Years," CTVNews, May 19, 2020, https://www.ctvnews.ca/world/facial-recognition-helps-reunite-kidnapped-toddler-with-family-after-32-years-1.4944570; Cindy Sui, "'It Took 32 Years, But I Finally Found My Kidnapped Son,'" BBC News, August 7, 2020, https://www.bbc.com/news/stories-53566460.


3. Jennifer Lynch, "Face Off: Law Enforcement Use of Face Recognition Technology" (Electronic Frontier Foundation, February 12, 2018), https://www.eff.org/wp/law-enforcement-use-face-recognition. This white paper provides a good basic explainer on FRT and its potential technical challenges.


4. It should be noted that such applications are quite dubious. Depending on the creator of the FRT, only some—not all—of our facial characteristics will be of interest to the collector. Prized characteristics that distinguish us from others will differ depending on the purpose and intent of the data collector.


5. Accurate as of January 2023, using information from Fight for the Future, "Ban Facial Recognition in Stores," Ban Facial Recognition in Stores, n.d., https://www.banfacialrecognition.com/stores, accessed January 30, 2023.


6. Woodrow Hartzog and Evan Selinger, "Facial Recognition Is the Perfect Tool for Oppression," Medium (blog), August 2, 2018, https://medium.com/s/story/facial-recognition-is-the-perfect-tool-for-oppression-bc2a08f0fe66.


7. For thoughts about other body parts and our identity, see Sasha Costanza-Chock, "Design Justice, A.I., and Escape from the Matrix of Domination," Journal of Design and Science, July 16, 2018.


8. Morgan Klaus Scheuerman, Emily Denton, and Alex Hanna, "Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development," in Proceedings of the ACM on Human-Computer Interaction 5, no. CSCW2 (New York: ACM, October 13, 2021), 1-37; Morgan Klaus Scheuerman, Madeleine Pape, and Alex Hanna, "Auto-Essentialization: Gender in Automated Facial Analysis as Extended Colonial Project," Big Data and Society 8, no. 2 (July 1, 2021).


9. Judith Donath, "You Are Entering an Ephemeral Bio-Allowed Data Capture Zone," Medium (blog), July 23, 2018, https://medium.com/@judithd/you-are-entering-an-ephemeral-bio-allowed-data-capture-zone-5ecafd2dbdaf.


10. Although in 2021 Facebook deleted more than a billion users' facial data, it still retains the software capabilities of its DeepFace system: Kashmir Hill and Ryan Mac, "Facebook, Citing Societal Concerns, Plans to Shut Down Facial Recognition System," New York Times, November 2, 2021, https://www.nytimes.com/2021/11/02/technology/facebook-facial-recognition.html.


11. Simon A. Cole, Suspect Identities: A History of Fingerprinting and Criminal Identification (Cambridge, MA: Harvard University Press, 2001).


12. Much of the backstory on Bledsoe and his contributions to FRT can be found in these helpful sources: Shaun Raviv, "The Secret History of Facial Recognition," Wired, March 3, 2021, https://www.wired.com/story/secret-history-facial-recognition/; Harmon Leon, "How LSD, Nuclear Weapons Led to the Development of Facial Recognition," Observer (blog), January 29, 2020, https://observer.com/2020/01/facial-recognition-development-history-woody-bledsoe-cia/.


13. Kelly A. Gates, Our Biometric Future: Facial Recognition Technology and the Culture of Surveillance (New York: New York University Press, 2011), 30.


14. Gates, Our Biometric Future.


15. For an excellent explainer, see Joy Buolamwini et al., "Facial Recognition Technologies: A Primer" (Algorithmic Justice League, May 29, 2020), https://assets.website-files.com/5e027ca188c99e3515b404b7/5ed1002058516c11edc66a14_FRTsPrimerMay2020.pdf.


16. Clare Garvie, Alvaro Bedoya, and Jonathan Frankle, "The Perpetual Line-Up" (Washington, DC: Georgetown University Center on Privacy and Technology, October 18, 2016), https://www.perpetuallineup.org/.


17. At the time, being the face of a brand was not seen as positive. See Samantha Barbas, Laws of Image: Privacy and Publicity in America (Palo Alto, CA: Stanford University Press, 2015), 46.


18. "Frances Folsom Cleveland," White House, accessed May 7, 2022, https://www.whitehouse.gov/about-the-white-house/first-families/frances-folsom-cleveland/; Betty Boyd Carol, "Frank: The Story of Frances Folsom Cleveland, America's Youngest First Lady—by Annette Dunlap," Presidential Studies Quarterly 42, no. 1 (2012): 211-212.


19. Much of Folsom's story appears here: Jessica Lake, The Face That Launched a Thousand Lawsuits: The American Women Who Forged a Right to Privacy (New Haven, CT: Yale University Press, 2016); Jessica Lake, "Disembodied Data and Corporeal Violation: Our Gendered Privacy Law Priorities and Preoccupations," University of New South Wales Law Journal 42, no. 1 (March 2019).


20. Lake, The Face That Launched a Thousand Lawsuits, 46.


21. Lake, The Face That Launched a Thousand Lawsuits, 49.


22. As quoted in Lake, The Face That Launched a Thousand Lawsuits, 50.


23. Lake, The Face That Launched a Thousand Lawsuits, 25, 51-53; Stuart Banner, American Property (Cambridge, MA: Harvard University Press, 2011) 134.


24. Alan McQuinn, "From Kodak to Google, How Privacy Panics Distort Policy," TechCrunch (blog), October 1, 2015, https://social.techcrunch.com/2015/10/01/from-kodak-to-google-how-privacy-panics-distort-policy/; Matt Reimann, "How the First Mass-Market Camera Led to the Right to Privacy and 'Roe V. Wade,'" Timeline (blog), March 10, 2017, https://timeline.com/how-the-first-mass-market-camera-led-to-the-right-to-privacy-and-roe-v-wade-4fb4cd87df7a.


25. Irwin R. Kramer, "The Birth of Privacy Law: A Century since Warren and Brandeis," Catholic University Law Review 39, no. 3 (1990): 23; Neil M. Richards, "Four Privacy Myths," in A World without Privacy: What Law Can and Should Do?, ed. Austin Sarat (Cambridge: Cambridge University Press, 2014), 33-82.


26. Samuel D. Warren and Louis D. Brandeis, "The Right to Privacy," Harvard Law Review 4, no. 5 (1890): 193-220.


27. Barbas, Laws of Image, 4.


28. Warren and Brandeis, "The Right to Privacy," 213.


29. Samantha Barbas, "From Privacy to Publicity: The Tort of Appropriation in the Age of Mass Consumption," Buffalo Law Review 61 (January 1, 2013): 1119-1189.


30. Lake, The Face That Launched a Thousand Lawsuits; Banner, American Property; Barbas, "From Privacy to Publicity."


31. Lake, The Face That Launched a Thousand Lawsuits; Jennifer E. Rothman, The Right of Publicity: Privacy Reimagined for a Public World (Cambridge, MA: Harvard University Press, 2018).


32. See Rothman, The Right of Publicity, 25, for an explanation of the Act to Prevent the Unauthorized Use of the Name or Picture of Any Person for the Purposes of Trade was passed in New York on the heels of the Roberson decision.


33. Famously, the 1953 case of Haelan Laboratories v. Topps Chewing Gum coined the term "right to publicity," As noted Banner, American Property, 54, Rothman, The Right of Publicity. See Melville B. Nimmer, "The Right of Publicity," Law and Contemporary Problems 19 (1954): 203-223, for problems with this right.


34. Laurie Asseo, "Justices Allow Vanna White to Sue over Ad with Robot Look-Alike," Associated Press, June 1, 1993, https://apnews.com/article/181941e05288fac0593f6107b54eceac; Banner, American Property, 160-161; Midler v. Ford Motor Co., No. 849 F.2d 460 (9th Cir. 1988); Vanna White v. Samsung Elecs. Am., Inc., No. 989 F.2d 1512 (9th Cir. 1993).


35. Fraley v. Facebook, Inc., No. 830 F. Supp. 2d 785 (N.D. Cal. 2011).


36. András Arató, "Experience: My Face Became a Meme," Guardian, November 8, 2019, https://www.theguardian.com/lifeandstyle/2019/nov/08/experience-hide-the-pain-harold-face-became-meme-turned-it-into-career.


37. Elizabeth M. Renieris, Beyond Data (Cambridge, MA: MIT Press, 2023), 24.


38. Hartzog and Selinger, "Facial Recognition Is the Perfect Tool for Oppression"; Garvie et al., "The Perpetual Line-Up."


39. Renieris, Beyond Data, 25-27.


40. See James Q. Whitman, "The Two Western Cultures of Privacy: Dignity versus Liberty," Yale Law Journal 113, no. 6 (2004): 1151-1222.


41. For substantial context on policy frameworks Safe Harbor Agreement, EU-US Privacy Shield, and court cases Schrems I and II, see Henry Farrell and Abraham L. Newman, Of Privacy and Power: The Transatlantic Struggle over Freedom and Security (Princeton, NJ: Princeton University Press, 2019), chap. 5 or Joshua P. Meltzer, "The Court of Justice of the European Union in Schrems II: The impact of GDPR on data flows and national security." Brookings (blog), August 5, 2020, https://www.brookings.edu/research/the-court-of-justice-of-the-european-union-in-schrems-ii-the-impact-of-gdpr-on-data-flows-and-national-security/.


42. Avner Levin and Mary Jo Nicholson, "Privacy Law in the United States, the EU and Canada: The Allure of the Middle Ground," University of Ottawa Law and Technology Journal 2, no. 2 (2005): 357-396.


43. Ed O'Bannon and Michael A. McCann, Court Justice: The Inside Story of My Battle against the NCAA (New York: Diversion Books, 2018), 2.


44. EA Sports discontinued production of this franchise in 2010.


45. Most recently, the Court ruled in NCAA v. Alston (2021) that the NCAA could not restrict education-related compensation for college players. That same year, the NCAA vacated the position that players could not make money from their names, likenesses, and images. On the Court decision, see Tim Mullaney, "Supreme Court NCAA Ruling and the New Future of Paying College Athletes." CNBC, June 21, 2021. https://www.cnbc.com/2021/06/21/supreme-court-ncaa-decision-how-college-athletes-plan-to-cash-in.html. On the NCAA policy change, see Alan Blinder, "College Athletes May Earn Money from Their Fame, N.C.A.A. Rules," New York Times, June 30, 2021, https://www.nytimes.com/2021/06/30/sports/ncaabasketball/ncaa-nil-rules.html.


46. Diane Desierto, "Human Rights in the Era of Automation and Artificial Intelligence," EJIL: Talk! (blog), February 26, 2020, https://www.ejiltalk.org/human-rights-in-the-era-of-automation-and-artificial-intelligence/.


47. Margaret Jane Radin, "Property and Personhood," Stanford Law Review 34, no. 5 (1982): 957-1015.


48. Margaret Jane Radin, "Market-Inalienability," Harvard Law Review 100, no. 8 (1987): 1849-1937.


49. Radin, "Market-Inalienability," uses analogical terms of freedom, identity, and contextuality; see 1903-1906.


50. Some good discussion about these issues can be found in Michael J. Sandel, What Money Can't Buy: The Moral Limits of Markets (New York: Farrar, Straus and Giroux, 2012); Michael Heller and James Salzman, Mine! How the Hidden Rules of Ownership Control Our Lives (New York: Doubleday, 2021).


51. Kieran Joseph Healy, Last Best Gifts: Altruism and the Market for Human Blood and Organs (Chicago: University of Chicago Press, 2006); Banner, American Property.


52. But on this point, Vida Panitch and L. Chad Horne, "Commodification, Inequality, and Kidney Markets," Social Theory and Practice 44, no. 1 (2018): 121-143, make the argument that it is about access and equity, and not about exchange per se.


53. A touchstone case in this regard is the Australia High Court's decision in CJ Griffith, Doodeward v. Spence, No. 6 CLR 406, 1908—0522A—HCA (High Court of Australia May 22, 1908); Bryan Gilmartin, "What Remains Series No. 2: How Were Human Remains Historically Treated?," WEL Partners (blog), June 14, 2021, https://welpartners.com/blog/2021/06/what-remains-series-no-2-how-were-human-remains-historically-treated/ offers a summary of cases in the Canadian and British contexts.


54. Article 12.


55. Edward J. Bloustein, "Privacy as an Aspect of Human Dignity: An Answer to Dean Prosser," in Philosophical Dimensions of Privacy: An Anthology, ed. Ferdinand David Schoeman (Cambridge: Cambridge University Press, 1984), 156-202.


56. Warren and Brandeis, "The Right to Privacy," 205.


57. See, for example, Richard B. Parker, "A Definition of Privacy," Rutgers Law Review 27, no. 2 (1974): 275-297.


58. Lisa M. Austin, "Person, Place or Thing? Property and the Structuring of Social Relations," University of Toronto Law Journal 60, no. 445 (2010); Helen Nissenbaum, "Respecting Context to Protect Privacy: Why Meaning Matters," Science and Engineering Ethics 24, no. 3 (June 1, 2018): 831-852; Helen Nissenbaum, "A Contextual Approach to Privacy Online," Daedalus 140, no. 4 (2011): 32-48; Paula Kift and Helen Nissenbaum, "Normative Analysis of the NSA's Bulk Telephony," I/S: A Journal of Law and Policy for the Information Society 13, no. 2 (2016): 333-372; Adam D. Moore, "Privacy, Speech, and Values: What We Have No Business Knowing," Ethics and Information Technology 18, no. 1 (March 1, 2016): 41-49; Tobias Matzner, "Why Privacy Is Not Enough Privacy in the Context of 'Ubiquitous Computing' and 'Big Data,'" Journal of Information, Communication and Ethics in Society 12, no. 2 (2014): 93-106.


59. Daniel J. Solove, "'I've Got Nothing to Hide' and Other Misunderstandings of Privacy" San Diego Law Review 44, no. 4 (2007): 745-772; Daniel J. Solove, "Conceptualizing Privacy," California Law Review 90, no. 4 (2002): 1087-1155; Daniel J. Solove, Nothing to Hide: The False Tradeoff between Privacy and Security (New Haven, CT: Yale University Press, 2011).


60. Solove, "`I've Got Nothing to Hide' and Other Misunderstandings of Privacy."


61. Solove, "`I've Got Nothing to Hide' and Other Misunderstandings of Privacy," 764.


62. Einav Rabinovitch-Fox, "Perspective: The Battle over Masks Has Always Been Political," Washington Post, November 18, 2020, https://www.washingtonpost.com/outlook/2020/11/18/battle-over-masks-has-always-been-political/.


63. Margot Kaminski, "Real Masks and Real Name Policies: Applying Anti-Mask Case Law to Anonymous Online Speech," Fordham Intellectual Property, Media and Entertainment Law Journal 23, no. 3 (April 17, 2013): 815.


64. Office of the Privacy Commissioner of Canada, "Data at Your Fingertips Biometrics and the Challenges to Privacy," November 1, 2011, https://www.priv.gc.ca/en/privacy-topics/health-genetic-and-other-body-information/gd_bio_201102/.


65. For example, see Office of the Privacy Commissioner of Canada, "PIPEDA Findings #2020-004: Joint Investigation of the Cadillac Fairview Corporation Limited by the Privacy Commissioner of Canada, the Information and Privacy Commissioner of Alberta, and the Information and Privacy Commissioner for British Columbia," October 29, 2020, https://www.priv.gc.ca/en/opc-actions-and-decisions/investigations/investigations-into-businesses/2020/pipeda-2020-004.


66. Office of the Privacy Commissioner of Canada, "Data at Your Fingertips Biometrics and the Challenges to Privacy."


67. NIST is the National Institute of Standards and Technology, part of the US Department of Commerce.


68. The website maintained by Adam Harvey, "Exposing.Ai," Exposing.ai, accessed May 7, 2022, https://exposing.ai/, tracks the way that Flickr photos have been uploaded into FRT data sets. Other companies, such as Clearview AI, have notoriously scraped the websites of other companies to grab photos and created facial data of more than 3 billion individuals.


69. Kate Crawford, Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence (New Haven, CT: Yale University Press, 2021), 168-171. For more information on the CK and CK+ data sets, see Patrick Lucey et al., "The Extended Cohn-Kanade Dataset (CK+): A Complete Dataset for Action Unit and Emotion-Specified Expression," in Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition—Workshops (Piscataway, NJ: IEEE, 2010), 94-101.


70. Inioluwa Deborah Raji and Genevieve Fried, "About Face: A Survey of Facial Recognition Evaluation," arXiv:2102.00813 [Cs], February 1, 2021; Karen Hao, "This Is How We Lost Control of Our Faces," MIT Technology Review (blog), February 5, 2021, https://www.technologyreview.com/2021/02/05/1017388/ai-deep-learning-facial-recognition-data-history/.


71. Nancy S. Kim, Consentability: Consent and Its Limits (Cambridge: Cambridge University Press, 2019).


72. Nissenbaum, "A Contextual Approach to Privacy Online"; Neil Richards and Woodrow Hartzog, "The Pathologies of Digital Consent," Washington University Law Review 96, no. 6 (2019): 1461-1503; Elizabeth Edenberg and Meg Leta Jones, "Analyzing the Legal Roots and Moral Core of Digital Consent," New Media and Society 21, no. 8 (August 1, 2019): 1804-1823.


73. "Shedding Light on Fairness in AI with a New Data Set," accessed May 7, 2022, https://ai.facebook.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set/; Caner Hazirbas et al., "Towards Measuring Fairness in AI: The Casual Conversations Dataset" (Meta AI, April 8, 2021), https://arxiv.org/abs/2104.02821.


74. G. Alex Sinha, "A Real-Property Model of Privacy," DePaul Law Review 68, no. 3 (2019 2018): 567-614. See also for broad definitions of property, Sylvie Delacroix and Neil D. Lawrence, "Bottom-up Data Trusts: Disturbing the 'One Size Fits All' Approach to Data Governance," International Data Privacy Law 9, no. 4 (November 1, 2019): 236-252; Ayelet Shachar and Ran Hirschl, "Citizenship as Inherited Property," Political Theory 35, no. 3 (2007): 253-287.


75. Nissenbaum, "A Contextual Approach to Privacy Online"; Daniel J. Solove, "Introduction: Privacy Self-Management and the Consent Dilemma Symposium: Privacy and Technology," Harvard Law Review 126, no. 7 (2013): 1880-1903; G. Alex Sinha, "Technology, Self-Inflicted Vulnerability, and Human Rights," in New Technologies for Human Rights Law and Practice, ed. Molly K. Land and Jay D. Aronson (Cambridge: Cambridge University Press, 2018), 270-288.


76. Daniel J. Solove and Danielle Keats Citron, "Risk and Anxiety: A Theory of Data Breach Harms," Texas Law Review 96, no. 4 (December 14, 2016): 737-786.


77. Joseph S. Nye, Power in the Global Information Age: From Realism to Globalization (London: Routledge, 2004).


78. For another example of similar thinking in popular media, see Richard H. Thaler and Cass R. Sunstein, Nudge: Improving Decisions about Health, Wealth, and Happiness, updated ed. (New York: Penguin Books, 2009).


79. Clare Garvie, "Garbage In. Garbage Out. Face Recognition on Flawed Data" (Washington, DC: Georgetown University Center on Privacy and Technology, May 16, 2019), https://www.flawedfacedata.com.


80. Alex Hodor-Lee and Clare Garvie, "Privacy Expert Clare Garvie Explains Why Your Face Is Already in a Criminal Lineup," Document Journal, October 12, 2020, https://www.documentjournal.com/2020/10/privacy-expert-clare-garvie-explains-why-your-face-is-already-in-a-criminal-lineup/.


81. Safiya Umoja Noble, Algorithms of Oppression: How Search Engines Reinforce Racism (New York: NYU Press, 2018); Ruha Benjamin, Race after Technology: Abolitionist Tools for the New Jim Code (Cambridge: Polity Press, 2019).


82. Buolamwini tells this story to journalist Kara Swisher: 'Sway,' "She's Taking Jeff Bezos to Task"; see also "About—Poet of Code," Poet of Code, accessed September 6, 2022, https://poetofcode.com/about/.


83. "IBM Abandons 'Biased' Facial Recognition Tech," BBC News, June 9, 2020, https://www.bbc.com/news/technology-52978191.


84. "Gender Shades," accessed February 26, 2021, http://gendershades.org/; Joy Buolamwini and Timnit Gebru, "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification," in Proceedings of the First Conference on Fairness, Accountability and Transparency (PMLR, 2018), 77-91, https://proceedings.mlr.press/v81/buolamwini18a.html.


85. Sahil Chinoy, "The Racist History behind Facial Recognition," New York Times, July 10, 2019, https://www.nytimes.com/2019/07/10/opinion/facial-recognition-race.html; Steve Lohr, "Facial Recognition Is Accurate, If You're a White Guy," New York Times, February 9, 2018, https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html; Luke Stark, "Facial Recognition, Emotion and Race in Animated Social Media," First Monday, September 1, 2018, https://doi.org/10.5210/fm.v23i9.9406; Joy Buolamwini, "When the Robot Doesn't See Dark Skin," New York Times, June 21, 2018, https://www.nytimes.com/2018/06/21/opinion/facial-analysis-technology-bias.html; Benjamin, Race after Technology; Noble, Algorithms of Oppression; Evan Greer, "Opinion: Ban Facial Recognition before It's Too Late," BuzzFeed News, July 18, 2019, https://www.buzzfeednews.com/article/evangreer/dont-regulate-facial-recognition-ban-it.


86. Jacob Snow, "Amazon's Face Recognition Falsely Matched 28 Members of Congress with Mugshots," ACLU News and Commentary, July 26, 2018, https://www.aclu.org/news/privacy-technology/amazons-face-recognition-falsely-matched-28.


87. Brendan F. Klare et al., "Face Recognition Performance: Role of Demographic Information," IEEE Transactions on Information Forensics and Security 7, no. 6 (December 2012): 1789-1801.


88. Larry Magid, "IBM, Microsoft And Amazon Not Letting Police Use Their Facial Recognition Technology," Forbes, June 12, 2020, https://www.forbes.com/sites/larrymagid/2020/06/12/ibm-microsoft-and-amazon-not-letting-police-use-their-facial-recognition-technology/; Rebecca Heilweil, "Big Tech Companies Back Away from Selling Facial Recognition to Police. That's Progress," Vox, June 10, 2020, https://www.vox.com/recode/2020/6/10/21287194/amazon-microsoft-ibm-facial-recognition-moratorium-police.


89. "A Precision Regulation Approach to Controlling Facial Recognition Technology Exports," THINKPolicy Blog (blog), September 11, 2020, https://www.ibm.com/policy/a-precision-regulation-approach-to-controlling-facial-recognition-technology-exports/.


90. Arvind Krishna, "IBM CEO's Letter to Congress on Racial Justice Reform," November 11, 2020, https://www.ibm.com/policy/facial-recognition-sunset-racial-justice-reforms/.


91. "IBM Abandons 'Biased' Facial Recognition Tech," BBC News, June 9, 2020, https://www.bbc.com/news/technology-52978191.


92. George Joseph and Kenneth Lipp, "IBM Used NYPD Surveillance Footage to Develop Technology That Lets Police Search by Skin Color," Intercept, September 6, 2018, https://theintercept.com/2018/09/06/nypd-surveillance-camera-skin-tone-search/.


93. See Jerome Pesenti, "An Update on our use of Face Recognition," Facebook, November 2, 2021, https://about.fb.com/news/2021/11/update-on-use-of-face-recognition/. On maintaining the software DeepFace, see Kashmir Hill and Ryan Mac, "Facebook, Citing Societal Concerns, Plans to Shut Down Facial Recognition System," New York Times, November 2, 2021, https://www.nytimes.com/2021/11/02/technology/facebook-facial-recognition.html.


94. For an interactive map, see "Ban Facial Recognition," Fight for the Future, https://www.banfacialrecognition.com/map/, accessed January 31, 2023. See also Jonathan Greig, "One Year after Amazon, Microsoft and IBM Ended Facial Recognition Sales to Police, Smaller Players Fill Void," ZDNet, May 26, 2021, https://www.zdnet.com/article/one-year-after-amazon-microsoft-and-ibm-ended-facial-recognition-sales-to-police-smaller-players-fill-void/.


95. Peter Dauvergne, Identified, Tracked, and Profiled: The Politics of Resisting Facial Recognition Technology (Cheltenham, UK: Elgar Publishing, 2022).


96. "Digital Volunteers to Expose Scale of Facial Recognition Technology in New York City," Amnesty International, May 4, 2021, https://www.amnesty.org/en/latest/news/2021/05/decode-surveillance-nyc-launches/; "Amnesty International and More Than 170 Organisations Call for a Ban on Biometric Surveillance," Amnesty International, June 7, 2021, https://www.amnesty.org/en/latest/news/2021/06/amnesty-international-and-more-than-170-organisations-call-for-a-ban-on-biometric-surveillance/; "Ban Facial Recognition Technology," Amnesty International, January 26, 2021, https://www.amnesty.org/en/latest/news/2021/01/ban-dangerous-facial-recognition-technology-that-amplifies-racist-policing/.


97. Nathan Sheard, "Banning Government Use of Face Recognition Technology: 2020 Year in Review," Electronic Frontier Foundation, January 3, 2021, https://www.eff.org/deeplinks/2020/12/banning-government-use-face-recognition-technology-2020-year-review.


98. AI Now Institute, "Gender, Race, and Power in AI: A Playlist," Medium (blog), September 24, 2019, https://medium.com/@AINowInstitute/gender-race-and-power-in-ai-a-playlist-2d3a44e43d3b; Kate Crawford et al., "AI Now 2019 Report" (New York: AI Now Institute, 2019); Amba Kak, "Regulating Biometrics: Global Approaches and Urgent Questions" (AI Now Institute, September 2020), https://ainowinstitute.org/regulatingbiometrics.pdf.


99. Hodor-Lee and Garvie, "Privacy Expert Clare Garvie Explains Why Your Face Is Already in a Criminal Lineup"; Garvie, "Garbage In. Garbage Out."


100. Peter Dauvergne, "Facial Recognition Technology for Policing and Surveillance in the Global South: A Call for Bans," Third World Quarterly 43, no. 9 (2022): 2325-2335.


101. Fight for the Future, "Ban Facial Recognition in Stores," Ban Facial Recognition in Stores, accessed May 7, 2022, https://www.banfacialrecognition.com/stores.


102. Dauvergne, Identified, Tracked, and Profiled.


103. Office of the Privacy Commissioner of Canada, "News Release: RCMP's Use of Clearview AI's Facial Recognition Technology Violated Privacy Act, Investigation Concludes," June 10, 2021, https://www.priv.gc.ca/en/opc-news/news-and-announcements/2021/nr-c_210610/.


104. "Landmark UK Court Ruling Finds Police Use of Facial Recognition Unlawful," Reuters, August 11, 2020, https://www.reuters.com/article/us-britain-tech-privacy-idUSKCN2572B8.


105. "Europe Fit for the Digital Age: Artificial Intelligence" (Brussels: European Commission, April 21, 2021), https://ec.europa.eu/commission/presscorner/detail/en/IP_21_1682; "White Paper on Artificial Intelligence—A European Approach to Excellence and Trust" (Brussels, Belgium: European Commission, February 19, 2020).


106. Dauvergne, "Facial Recognition Technology for Policing and Surveillance in the Global South."


107. Dauvergne, Identified, Tracked, and Profiled, 23-29; Country examples: Chris Burt, "NEC Revealed as Provider of Biometric Facial Recognition Services for Australian Federal Police," Biometric Update, October 25, 2019, https://www.biometricupdate.com/201910/nec-revealed-as-provider-of-biometric-facial-recognition-services-for-australian-federal-police; Frank Hersey, "British Police Deploy Live Facial Recognition, License Plate Recognition to Make 11 Arrests," Biometric Update, July 14, 2022, https://www.biometricupdate.com/202207/british-police-deploy-live-facial-recognition-license-plate-recognition-to-make-11-arrests; Ayang Macdonald, "NEC Enrolls Biometrics of 50M Vietnamese for ID Authentication," Biometric Update, April 28, 2022, https://www.biometricupdate.com/202204/nec-enrolls-biometrics-of-50m-vietnamese-for-id-authentication.


108. Betsy Powell, "How Toronto Police Used Controversial Facial Recognition Technology to Solve the Senseless Murder of an Innocent Man," Toronto Star, April 13, 2020, https://www.thestar.com/news/gta/2020/04/13/how-toronto-police-used-controversial-facial-recognition-technology-to-solve-the-senseless-murder-of-an-innocent-man.html.


109. Emily Mertz, "Edmonton Police Using Facial Recognition Software to Search 'Mugshot Database,'" Global News, February 2, 2022, https://globalnews.ca/news/8586358/edmonton-police-facial-recognition-mugshots-2022/.


110. "InfraGard National Members Alliance and NEC Corporation of America Join Forces in the Fight against Human Trafficking," January 31, 2022, https://www.businesswire.com/news/home/20220131005687/en/InfraGard-National-Members-Alliance-and-NEC-Corporation-of-America-Join-Forces-in-the-Fight-Against-Human-Trafficking.


111. Buolamwini et al., "Facial Recognition Technologies," 12.


112. Kate Crawford, "Halt the Use of Facial-Recognition Technology until It Is Regulated," Nature 572, no. 7771 (August 27, 2019): 565.


113. Shira Ovide, "A Case for Banning Facial Recognition," New York Times, June 9, 2020, https://www.nytimes.com/2020/06/09/technology/facial-recognition-software.html.


114. Clare Garvie, "Facial Recognition Threatens Our Fundamental Rights," Washington Post, July 19, 2018, https://www.washingtonpost.com/opinions/facial-recognition-threatens-our-fundamental-rights/2018/07/19/a102703a-8b64-11e8-8b20-60521f27434e_story.html.


115. Royce Kimmons and George Veletsianos, "Proctoring Software in Higher Ed: Prevalence and Patterns," EDUCAUSE Review, February 23, 2021, https://er.educause.edu/articles/2021/2/proctoring-software-in-higher-ed-prevalence-and-patterns.


116. Susan Grajek, "EDUCAUSE COVID-19 QuickPoll Results: Grading and Proctoring," EDUCAUSE Review, April 10, 2020, https://er.educause.edu/blogs/2020/4/educause-covid-19-quickpoll-results-grading-and-proctoring; Colleen Flaherty, "Big Proctor," Inside Higher Ed, May 11, 2020, https://www.insidehighered.com/news/2020/05/11/online-proctoring-surging-during-covid-19.


117. Drew Harwell, "Cheating-Detection Companies Made Millions during the Pandemic: Now Students Are Fighting Back," Washington Post, November 12, 2020, https://www.washingtonpost.com/technology/2020/11/12/test-monitoring-student-revolt/.


118. Monica Chin, "Exam Anxiety: How Remote Test-Proctoring Is Creeping Students Out," Verge, April 29, 2020, https://www.theverge.com/2020/4/29/21232777/examity-remote-test-proctoring-online-class-education.


119. The right to an education is Article 26 of the Universal Declaration of Human Rights and is also a part of the International Convention on Economic, Social, and Cultural Rights, the Convention on the Rights of the Child, and the Convention on the Elimination of All Forms of Discrimination against Women. It is also the subject of the 1960 UNESCO instrument, Convention against Discrimination in Education.


120. Woodrow Hartzog and Evan Selinger, "Facial Recognition Is the Perfect Tool for Oppression," Medium (blog), August 2, 2018, https://medium.com/s/story/facial-recognition-is-the-perfect-tool-for-oppression-bc2a08f0fe66; Evan Selinger and Woodrow Hartzog, "The Inconsentability of Facial Surveillance Consentability Symposium," Loyola Law Review 66, no. 1 (2020): 33-54.


121. Sara Morrison and Rebecca Heilweil, "How Teachers Are Sacrificing Student Privacy to Stop Cheating," Vox, December 18, 2020, https://www.vox.com/recode/22175021/school-cheating-student-privacy-remote-learning.


122. Nora Caplan-Bricker, "Is Online Test-Monitoring Here to Stay?," New Yorker, May 27, 2021, https://www.newyorker.com/tech/annals-of-technology/is-online-test-monitoring-here-to-stay.


123. Paul Lewis, "'I Was Shocked It Was So Easy': Meet the Professor Who Says Facial Recognition Can Tell If You're Gay," Guardian, July 7, 2018, https://www.theguardian.com/technology/2018/jul/07/artificial-intelligence-can-tell-your-sexuality-politics-surveillance-paul-lewis; Heather Murphy, "Why Stanford Researchers Tried to Create a 'Gaydar' Machine," New York Times, October 9, 2017, https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html.


124. Michal Kosinski, David Stillwell, and Thore Graepel, "Private Traits and Attributes Are Predictable from Digital Records of Human Behavior," Proceedings of the National Academy of Sciences of the United States of America 110, no. 15 (April 9, 2013): 5802-5805; Issie Lapowsky, "The Man Who Saw the Dangers of Cambridge Analytica Years Ago," Wired, June 19, 2010, https://www.wired.com/story/the-man-who-saw-the-dangers-of-cambridge-analytica/.


125. Yilun Wang and Michal Kosinski, "Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images," Journal of Personality and Social Psychology 114, no. 2 (2018): 246-257.


126. "Advances in AI Are Used to Spot Signs of Sexuality," Economist, September 9, 2017, https://www.economist.com/science-and-technology/2017/09/09/advances-in-ai-are-used-to-spot-signs-of-sexuality.


127. Murphy, "Why Stanford Researchers Tried to Create a 'Gaydar' Machine"; Phillip N. Cohen, "On Artificially Intelligent Gaydar," Family Inequality (blog), September 11, 2017, https://familyinequality.wordpress.com/2017/09/11/on-artificially-intelligent-gaydar/; James Vincent, "The Invention of AI 'Gaydar' Could Be the Start of Something Much Worse," Verge, September 21, 2017, https://www.theverge.com/2017/9/21/16332760/ai-sexuality-gaydar-photo-physiognomy; Alex Bollinger, "Computers Can Now Predict If White People Are Gay or Straight," LGBTQ Nation, September 9, 2017, https://www.lgbtqnation.com/2017/09/computers-can-now-tell-white-people-gay-straight/; Drew Anderson, "GLAAD and HRC Call on Stanford University and Responsible Media to Debunk Dangerous and Flawed Report Claiming to Identify LGBTQ People through Facial Recognition Technology," GLAAD, September 8, 2017, https://www.glaad.org/blog/glaad-and-hrc-call-stanford-university-responsible-media-debunk-dangerous-flawed-report.


128. Blaise Aguera y Arcas, Alexander Todorov, and Margaret Mitchell, "Do Algorithms Reveal Sexual Orientation or Just Expose Our Stereotypes?," Medium (blog), January 18, 2018, https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477.


129. Danielle Paletta, "ILGA World Updates State-Sponsored Homophobia Report: 'There's Progress in Times of Uncertainty,'" ILGA, December 15, 2020, https://ilga.org/ilga-world-releases-state-sponsored-homophobia-december-2020-update.


130. Mimi Mefo Takambou, "Uncertain Future for LGBT+ Rights in Uganda as Controversial Bill Is Passed," DW, May 5, 2021, https://www.dw.com/en/uncertain-future-for-lgbt-rights-in-uganda-as-controversial-bill-is-passed/a-57437925.


131. Claudia Ciobanu, "A Third of Poland Declared 'LGBT-Free Zone,'" Balkan Insight, February 25, 2020, https://balkaninsight.com/2020/02/25/a-third-of-poland-declared-lgbt-free-zone/.


132. "Jayapal Introduces Bill to Ban Use of Facial Recognition Tech," Congresswoman Pramila Jayapal (blog), June 15, 2021, https://jayapal.house.gov/2021/06/15/ban-facial-recognition-tech/.


133. For analysis of the Facial Recognition Act, see Jake Laperruque, "The Facial Recognition Act: A Promising Path to Put Guardrails on a Dangerously Unregulated Surveillance Technology," Lawfare (blog), November 1, 2022, https://www.lawfareblog.com/facial-recognition-act-promising-path-put-guardrails-dangerously-unregulated-surveillance-technology.


134. Kashmir Hill and Aaron Krolik, "How Photos of Your Kids Are Powering Surveillance Technology," New York Times, October 11, 2019, https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html.


135. Gates, Our Biometric Future, 28.


136. Raji and Fried, "About Face," 11.


137. For example, Donna J. Haraway, "A Cyborg Manifesto: Science, Technology. and Socialist-Feminism in the Late Twentieth Century," Socialist Review 15, no. 2 (1985): 424-457; Sheila Jasanoff, States of Knowledge: The Co-Production of Science and the Social Order (Abingdon, Oxford: Routledge, 2004).




Chapter 5


1. Alyx Gorman, "Kim Kardashian's Father Resurrected as Hologram in Birthday Present from Kanye West," Guardian, October 30, 2020, https://www.theguardian.com/lifeandstyle/2020/oct/30/robert-kardashian-resurrected-as-a-hologram-for-kim-kardashian-wests-birthday.


2. Chris Murphy, "Kanye Humbly Got Kim a Hologram of Her Late Father to Celebrate Her Normal 40th Birthday," Vulture, October 29, 2020, https://www.vulture.com/2020/10/kanye-west-kim-kardashian-hologram-dead-father-40th-birthday.html.


3. "Kim Kardashian West on Instagram: 'For my birthday, Kanye got me the most thoughtful gift of a lifetime. A special surprise from heaven. A hologram of my dad. ✨🤍 It is so lifelike and we watched it over and over, filled with lots so tears and emotion. I can't even describe what this meant to me and my sisters, my brother, my mom and closest friends to experience together. Thank you so much Kanye for this memory that will last a lifetime ✨'" Instagram, accessed March 9, 2021, https://www.instagram.com/tv/CG8gdHdA1mX/.


4. For more information on Pepper's ghost and modern takes, see David Howard, "Kim Kardashian's Dad Came Back to Life in a Terrifying Hologram," Popular Mechanics, October 30, 2020, https://www.popularmechanics.com/default/a16141/steve-jobs-first-public-demo-mac/; Heather Schwedel, "How You Create a Robert Kardashian-Style Hologram—and How Much It Costs," Slate Magazine, October 31, 2020, https://slate.com/human-interest/2020/10/robert-kardasian-hologram-company-kim-kanye-cost.html.


5. "Replika," replika.ai, accessed April 1, 2021, https://replika.ai. This is the brainchild of Eugenia Kuyda, whose personal tragedy discussed in this chapter led to her creation of this technology.


6. The topic of real users using these chatbots during the COVID-19 pandemic is discussed in Cade Metz, "Riding Out Quarantine with a Chatbot Friend: 'I Feel Very Connected,'" New York Times, June 16, 2020, https://www.nytimes.com/2020/06/16/technology/chatbots-quarantine-coronavirus.html.


7. Tamara Kneese, Death Glitch (New Haven, CT: Yale University Press, 2023).


8. Adrianne Jeffries, "You'll Tweet When You're Dead: LivesOn Says Digital 'Twin' Can Mimic Your Online Persona," Verge, February 21, 2013, https://www.theverge.com/2013/2/21/4010016/liveson-uses-artificial-intelligence-to-tweet-for-you-after-death; Natalie O'Neill, "Companies Want to Replicate Your Dead Loved Ones with Robot Clones," Vice.com, March 16, 2016, https://www.vice.com/en/article/pgkgby/companies-want-to-replicate-your-dead-loved-ones-with-robot-clones; Henrique Jorge, "Digital Eternity," in The Transhumanism Handbook, ed. Newton Lee (Cham: Springer, 2019), 645-649, https://doi.org/10.1007/978-3-030-16920-6_47; Michelle Starr, "Eternime Wants You to Live Forever as a Digital Ghost," CNET, accessed March 10, 2021, https://www.cnet.com/news/eternime-wants-you-to-live-forever-as-a-digital-ghost/; "Replika," replika.ai, accessed April 1, 2021, https://replika.ai.


9. Different academic fields have taken on this difficult question, to varying conclusions: Patrick Stokes, "Ghosts in the Machine: Do the Dead Live on in Facebook?," Philosophy and Technology 25, no. 3 (2012): 363-379, https://doi.org/10.1007/s13347-011-0050-7; Elaine Kasket, "Mourning and Memorialization on Social Media," in The Oxford Handbook of Cyberpsychology, ed. Alison Attrill-Smith et al. (Oxford: Oxford University Press, 2019), https://doi.org/10.1093/oxfordhb/9780198812746.013.26; Joshua Hurtado Hurtado, "Towards a Postmortal Society of Virtualised Ancestors? The Virtual Deceased Person and the Preservation of the Social Bond," Mortality, January 25, 2021, 1-16, https://doi.org/10.1080/13576275.2021.1878349.


10. Joohan Kim, "Phenomenology of Digital-Being," Human Studies 24, no. 1/2 (2001): 87-111.


11. Roman's story is drawn from Casey Newton, "When Her Best Friend Died, She Used Artificial Intelligence to Keep Talking to Him," TheVerge.com, October 6, 2016, http://www.theverge.com/a/luka-artificial-intelligence-memorial-roman-mazurenko-bot.


12. Isaiah Berlin, "Two Concepts of Liberty," in Four Essays on Liberty (London: Oxford University Press, 1969), 118-172.


13. Casey Newton, "When Her Best Friend Died, She Used Artificial Intelligence to Keep Talking to Him," TheVerge.com, October 6, 2016, http://www.theverge.com/a/luka-artificial-intelligence-memorial-roman-mazurenko-bot.


14. See Gordon Bell and Jim Gray, "Digital Immortality," technical report (San Francisco: Microsoft Research, October 1, 2000).


15. "In examples, the specific person may correspond to a past or present entity (or a version thereof), such as a friend, a relative, an acquaintance, a celebrity, a fictional character, a historical figure, a random entity, etc. The specific person may also correspond to oneself . . . or a version of oneself." Dustin I. Abramson and Joseph Johnson, Creating a Conversational Chat Bot of a Specific Person, 10,853,717 B2, filed April 11, 2017, and issued December 1, 2020, 11.


16. Dalvin Brown, "AI Chat Bots Can Bring You Back from the Dead, Sort Of," Washington Post, February 4, 2021, https://www.washingtonpost.com/technology/2021/02/04/chat-bots-reincarnation-dead/; Adam Smith, "Microsoft Patents Chatbot Technology to Revive Dead Loved Ones," Independent, January 20, 2021, https://www.independent.co.uk/life-style/gadgets-and-tech/microsoft-chatbot-patent-dead-b1789979.html. Originally, the tweets could be found on Twitter, but as of February 1, 2023, some relevant tweets have been restricted by users: Tim O'Brien, "@hypervisible I'm looking into this—appln date (Apr. 2017) predates the AI Ethics reviews we do today (I sit on the panel), and I'm not aware of any plan to build/ship (and yes, it's disturbing)," Twitter, @_TimOBrien, January 22, 2021, https://twitter.com/_TimOBrien/status/1352645952310439936.


17. Edina Harbinja, Lilian Edwards, and Marisa McVey, "Chatbots That Resurrect the Dead: Legal Experts Weigh in on 'Disturbing' Technology," The Conversation, accessed March 11, 2021, http://theconversation.com/chatbots-that-resurrect-the-dead-legal-experts-weigh-in-on-disturbing-technology-155436.


18. Dustin I. Abramson and Joseph Johnson, Creating a Conversational Chat Bot of a Specific Person, 10,853,717 B2, filed April 11, 2017, and issued December 1, 2020.


19. Carl Öhman and Luciano Floridi, "The Political Economy of Death in the Age of Information: A Critical Approach to the Digital Afterlife Industry," Minds and Machines 27, no. 4 (December 1, 2017): 639-662.


20. "HereAfter AI: Record Family History: Save Your Memories," HereAfter, accessed March 22, 2021, https://www.hereafter.ai; Henrique Jorge, "Digital Eternity," in The Transhumanism Handbook, ed. Newton Lee (Cham: Springer, 2019), 645-649; "Bina48 the Robot Goes to College (Inside Higher Ed)," Hanson Robotics, December 21, 2017, https://www.hansonrobotics.com/headlines-bina48-college-inside-higher-ed/.


21. Gorman, "Kim Kardashian's Father Resurrected."


22. More discussion of this company is in chapter 8.


23. Brown, "AI Chat Bots Can Bring You Back from the Dead, Sort Of"; "Digital Humans: Conversational AI Solutions beyond Just Chatbots: UneeQ," Digital Humans, accessed August 16, 2022, https://digitalhumans.com/. For Google's patent, see System and Method for Using a Digital Virtual Clone as an Input in a Simulated Environment. US Patent 9,959,497, filed January 30, 2017 and issued May 1, 2018.


24. Sam Thielman, "Experian Hack Exposes 15 Million People's Personal Information," Guardian, October 2, 2015, https://www.theguardian.com/business/2015/oct/01/experian-hack-t-mobile-credit-checks-personal-information; Craig Timberg, Elizabeth Dwoskin, and Brian Fung, "Data of 143 Million Americans Exposed in Hack of Credit Reporting Agency Equifax," Washington Post, September 7, 2017, https://www.washingtonpost.com/business/technology/equifax-hack-hits-credit-histories-of-up-to-143-million-americans/2017/09/07/a4ae6f82-941a-11e7-b9bc-b2f7903bab0d_story.html.


25. Alvin Chang, "The Facebook and Cambridge Analytica Scandal, Explained with a Simple Diagram," Vox, March 23, 2018, https://www.vox.com/policy-and-politics/2018/3/23/17151916/facebook-cambridge-analytica-trump-diagram.


26. Carl J. Öhman and David Watson, "Are the Dead Taking Over Facebook? A Big Data Approach to the Future of Death Online," Big Data & Society 6, https://doi.org/10.1177/2053951719842540.


27. James Vincent, "Twitter Taught Microsoft's Friendly AI Chatbot to Be a Racist Asshole in Less Than a Day," Verge, March 24, 2016, https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist; Oscar Schwartz, "In 2016, Microsoft's Racist Chatbot Revealed the Dangers of Online Conversation—IEEE Spectrum," IEEE Spectrum: Technology, Engineering, and Science News, accessed March 10, 2021, https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation; Cade Metz and Keith Collins, "To Give A.I. the Gift of Gab, Silicon Valley Needs to Offend You," New York Times, February 21, 2018, https://www.nytimes.com/interactive/2018/02/21/technology/conversational-bots.html.


28. Caroline Sinders, "Microsoft's Tay Is an Example of Bad Design," Medium, March 25, 2016, https://medium.com/@carolinesinders/microsoft-s-tay-is-an-example-of-bad-design-d4e65bb2569f.


29. Thomas Hornigold, "This Chatbot Has over 660 Million Users—and It Wants to Be Their Best Friend," Singularity Hub (blog), July 14, 2019, https://singularityhub.com/2019/07/14/this-chatbot-has-over-660-million-users-and-it-wants-to-be-their-best-friend/.


30. Maggi Savin-Baden, David Burden, and Helen Taylor, "The Ethics and Impact of Digital Immortality," Knowledge Cultures 5, no. 2 (2017): 178-196.


31. Maggi Savin-Baden and David Burden, Virtual Humans: Today and Tomorrow (London: Chapman and Hall/CRC, 2019), 231-245, http://www.taylorfrancis.com/chapters/digital-immortality-david-burden-maggi-savin-baden/10.1201/9781315151199-15.


32. Savin-Baden and Burden, Virtual Humans, 197-212, http://www.taylorfrancis.com/chapters/digital-immortality-david-burden-maggi-savin-baden/10.1201/9781315151199-15.


33. Jack Donnelly, Universal Human Rights in Theory and Practice, 3rd ed. (Ithaca, NY: Cornell University Press, 2013).


34. See Articles 5 and 24 of United Nations, "Universal Declaration of Human Rights," United Nations, accessed April 19, 2022, https://www.un.org/en/about-us/universal-declaration-of-human-rights.


35. Dahlia Lithwick, "What Are the Rights of Dead People?," Slate, March 14, 2002, https://slate.com/news-and-politics/2002/03/what-are-the-rights-of-dead-people.html.


36. Carl Öhman and Luciano Floridi, "An Ethical Framework for the Digital Afterlife Industry," Nature Human Behaviour 2, no. 5 (May 2018): 318-320.


37. United Nations, "Universal Declaration of Human Rights," Article 3; "International Covenant on Civil and Political Rights," Office of the United Nations High Commissioner for Human Rights, Article 6, accessed August 11, 2022, https://www.ohchr.org/en/instruments-mechanisms/instruments/international-covenant-civil-and-political-rights.


38. "United Nations Office on Genocide Prevention and the Responsibility to Protect," accessed April 19, 2022, https://www.un.org/en/genocideprevention/genocide.shtml.


39. "International Convention for the Protection of All Persons from Enforced Disappearance," Office of the United Nations High Commissioner for Human Rights, accessed April 19, 2022, https://www.ohchr.org/en/instruments-mechanisms/instruments/international-convention-protection-all-persons-enforced.


40. "Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment," Office of the United Nations High Commissioner for Human Rights, accessed April 19, 2022, https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-against-torture-and-other-cruel-inhuman-or-degrading.


41. "Why Amnesty Opposes the Death Penalty without Exception," accessed March 17, 2021, https://www.amnesty.org/en/what-we-do/death-penalty/.


42. Lars Mehlum et al., "Euthanasia and Assisted Suicide in Patients with Personality Disorders: A Review of Current Practice and Challenges," Borderline Personality Disorder and Emotion Dysregulation 7, no. 1 (July 30, 2020): 15; "Euthanasia and Assisted Dying Rates Are Soaring. But Where Are They Legal?," Guardian, July 15, 2019, http://www.theguardian.com/news/2019/jul/15/euthanasia-and-assisted-dying-rates-are-soaring-but-where-are-they-legal; Rebecca Reingold, "An International Human Right to Die with Dignity?," O'Neill Institute, December 13, 2019, https://oneill.law.georgetown.edu/an-international-human-right-to-die-with-dignity/.


43. Rebecca Reingold, "An International Human Right to Die with Dignity?," O'Neill Institute, December 13, 2019, https://oneill.law.georgetown.edu/an-international-human-right-to-die-with-dignity/.


44. Jordan J. Paust, "The Human Right to Die with Dignity: A Policy-Oriented Essay," Human Rights Quarterly 17, no. 3 (1995): 476-483.


45. "Spain Passes Law Allowing Euthanasia," BBC News, March 18, 2021, https://www.bbc.com/news/world-europe-56446631.


46. Katie Engelhart, The Inevitable: Dispatches on the Right to Die (New York: St. Martin's Press, 2021); "Engelhart's 'The Inevitable' Tackles Right-to-Die Issues: Shots—Health News: NPR," accessed March 18, 2021, https://www.npr.org/sections/health-shots/2021/03/09/975175847/inside-the-fight-for-the-right-to-die-logistical-and-ethical-challenges; Jordan J. Paust, "The Human Right to Die with Dignity: A Policy-Oriented Essay," Human Rights Quarterly 17, no. 3 (1995): 463-487.


47. For Vlahos's story, see "HereAfter AI"; James Vlahos, "A Son's Race to Give His Dying Father Artificial Immortality," Wired, July 10, 2017, https://www.wired.com/story/a-sons-race-to-give-his-dying-father-artificial-immortality/; "James Vlahos of Hereafter AI Discusses Avatars That Preserve the Memories and Thoughts of Real People—Voicebot Podcast Ep 168," Voicebot.ai, September 20, 2020, http://voicebot.ai/2020/09/20/james-vlahos-of-hereafter-ai-discusses-avatars-that-preserve-the-memories-and-thoughts-of-real-people-voicebot-podcast-ep-168/.


48. Vlahos, "A Son's Race to Give His Dying Father Artificial Immortality."


49. Vlahos, "A Son's Race to Give His Dying Father Artificial Immortality."


50. "James Vlahos of Hereafter AI Discusses Avatars That Preserve the Memories and Thoughts of Real People—Voicebot Podcast Ep 168."


51. "James Vlahos of Hereafter AI Discusses Avatars."


52. "What Is the Uncanny Valley?," IEEE Spectrum: Technology, Engineering, and Science News, accessed April 3, 2021, https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley.


53. "Microsoft Patent Shows Plans to Revive Dead Loved Ones as Chatbots," Independent, accessed March 9, 2021, https://www.independent.co.uk/life-style/gadgets-and-tech/microsoft-chatbot-patent-dead-b1789979.html.















54. Gordon Bell and Jim Gray, "Digital Immortality." Even on the less advanced end of the technology spectrum, see C. J. Robles, "How Much Does a Hologram from Heaven Cost? How Does It Work?," Tech Times, October 30, 2020, https://www.techtimes.com/articles/253752/20201030/hologram-heaven-cost-kanye-wests-gift-kim-kardashian-expensive-how-works.htm.


55. For a pertinent discussion of these issues, see Savin-Baden and Burden, "Digital Immortality," 231-245.


56. See "FAQ," https://www.hereafter.ai/faq, accessed January 31, 2023.


57. Charlotte Jee, "Technology That Lets Us 'Speak' to Our Dead Relatives Has Arrived. Are We Ready?" MIT Technology Review, October 18, 2022, https://www.technologyreview.com/2022/10/18/1061320/digital-clones-of-dead-people/.


58. These rights are referenced in, respectively, Articles 25, 26, and 21 of the United Nations, "Universal Declaration of Human Rights."


59. Vlahos, "A Son's Race to Give His Dying Father Artificial Immortality," Wired, accessed May 11, 2022, https://www.wired.com/story/a-sons-race-to-give-his-dying-father-artificial-immortality/.


60. See Article 3 of United Nations, "Universal Declaration of Human Rights," United Nations (United Nations), accessed April 19, 2022, https://www.un.org/en/about-us/universal-declaration-of-human-rights.


61. A short documentary film on the Cryonics Institute We Will Live Again (2019), https://www.theatlantic.com/video/index/591979/cryonics/; Kate Golembiewski, "Will Cryonically Frozen Bodies Ever Be Brought Back to Life?," Discover, April 6, 2021, https://www.discovermagazine.com/technology/will-cryonically-frozen-bodies-ever-be-brought-back-to-life.


62. Vernor Vinge, "The Coming Technological Singularity: How to Survive in the Post-Human Era" (VISION-21 Symposium, NASA Lewis Research Center and the Ohio Aerospace Institute, 1993), https://mindstalk.net/vinge/vinge-sing.html; Ray Kurzweil, The Singularity Is Near (New York: Penguin, 2006).


63. Susan Schneider, Artificial You: AI and the Future of Your Mind (Princeton: Princeton University Press, 2019).


64. There are countless works on consciousness to choose from. This selection provides an interesting array as I was doing research: David J. Chalmers, The Character of Consciousness (Oxford: Oxford University Press, 2010); D. C. Dennett, From Bacteria to Bach and Back: The Evolution of Minds (New York: Norton, 2017); Schneider, Artificial You; Johannes Bruder, Cognitive Code: Post-Anthropocentric Intelligence and the Infrastructural Brain (Montréal: McGill-Queen's University Press, 2020).


65. G. Alex Sinha, "Technology, Self-Inflicted Vulnerability, and Human Rights," in New Technologies for Human Rights Law and Practice, ed. Molly K. Land and Jay D. Aronson (Cambridge: Cambridge University Press, 2018), 270-88.


66. Hans Buitelaar, "Post-Mortem Privacy and Informational Self-Determination," Ethics and Information Technology 19, no. 2 (2017): 129-142.


67. Edina Harbinja, "Post-Mortem Privacy 2.0: Theory, Law, and Technology," International Review of Law, Computers and Technology 31, no. 1 (January 2, 2017): 26-42.


68. Rogier Creemers and Graham Webster, "Translation: Personal Information Protection Law of the People's Republic of China—Effective Nov. 1, 2021," DigiChina (blog), September 7, 2021, https://digichina.stanford.edu/work/translation-personal-information-protection-law-of-the-peoples-republic-of-china-effective-nov-1-2021/; China Briefing, "The Personal Information Protection Law in China: A Legal Analysis," China Briefing News, September 15, 2021, https://www.china-briefing.com/news/the-personal-information-protection-law-in-china-a-legal-analysis/.


69. For a terrific discussion of some of these options, see Tamara Kneese, Death Glitch (New Haven, CT: Yale University Press, 2023), chap. 2.


70. Kneese, Death Glitch.




Chapter 6


1. Katie Rogers, "Why Do Older People Love Facebook? Let's Ask My Dad," New York Times, April 14, 2016.


2. Frances Haugen, "Statement of Frances Haugen," Senate Committee on Commerce, Science and Transportation, Sub-Committee on Consumer Protection, Product Safety, and Data Security (2021).


3. "The Facebook Company Is Now Meta," Meta (blog), October 28, 2021, https://about.fb.com/news/2021/10/facebook-company-is-now-meta/.


4. Josh Constine, "Facebook's S-1 Letter from Zuckerberg Urges Understanding before Investment," TechCrunch (blog), February 1, 2012, https://social.techcrunch.com/2012/02/01/facebook-ipo-letter/.


5. See Rebecca MacKinnon, "What to Get Right First," Starling Lab for Data Integrity (blog), 2021, https://www.starlinglab.org/what-to-get-right-first/.


6. The text from the document articulating foundational principles for business and states is illustrative. "Business enterprises should respect human rights. This means that they should avoid infringing on the human rights of others and should address adverse human rights impacts with which they are involved," United Nations, Guiding Principles on Business and Human Rights (United Nations, 2011), 13. By contrast, "States must protect against human rights abuse within their territory and/or jurisdiction by third parties, including business enterprises. This requires taking appropriate steps to prevent, investigate, punish and redress such abuse through effective policies, legislation, regulations and adjudication." United Nations, Guiding Principles on Business and Human Rights (United Nations, 2011), 3; emphasis added.


7. "Meta Global Community MAU 2022," Statista, accessed September 26, 2022, https://www.statista.com/statistics/947869/facebook-product-mau/.


8. Mike Isaac and Sheera Frenkel, "Gone in Minutes, Out for Hours: Outage Shakes Facebook," New York Times, October 4, 2021, https://www.nytimes.com/2021/10/04/technology/facebook-down.html; Farhad Manjoo, "Why You Shouldn't Use Facebook to Log in to Other Sites and Apps," Seattle Times, October 2, 2018.


9. S. Dixon, "Facebook users by country 2022," Statista, July 26, 2022, https://www.statista.com/statistics/268136/top-15-countries-based-on-number-of-facebook-users/, accessed Feburary 3, 2023.


10. Sheera Frenkel, "This Is What Happens When Millions of People Suddenly Get the Internet," BuzzFeed News, November 20, 2016, https://www.buzzfeednews.com/article/sheerafrenkel/fake-news-spreads-trump-around-the-world; Saira Asher, "Myanmar Coup: How Facebook Became the 'Digital Tea Shop,'" BBC News, February 4, 2021, https://www.bbc.com/news/world-asia-55929654.


11. Mark Sweney, "Facebook Outage Highlights Global Over-Reliance on Its Services," Guardian, October 5, 2021.Ginny Hogan, "What If Facebook and Instagram Just . . . Never Came Back?," Bustle, October 5, 2021, https://www.bustle.com/life/world-without-social-media-facebook-instagram-outage.


12. Eileen Guo and Patrick Howell O'Nell, "Millions of people rely on Facebook to get online. The outage left them stranded," MIT Technology Review, October 5, 2021, https://www.technologyreview.com/2021/10/05/1036479/facebook-global-outage/.


13. Other acronyms include FAANG—Facebook, Amazon, Apple, Netflix, and Google, coined by Jim Cramer at CNBC. Cramer also suggested MAMAA, replacing Netflix with Microsoft and "Alphabet" (as parent company) in place of "Google." See Kevin Stankiewicz, "'Bye-Bye FAANG, Hello MAMAA'—Cramer Reveals a New Acronym after Facebook's Name Change," CNBC, October 29, 2021, https://www.cnbc.com/2021/10/29/cramer-new-acronym-to-replace-faang-after-facebook-name-change-to-meta.html.


14. Amy Webb, The Big Nine: How the Tech Titans and Their Thinking Machines Could Warp Humanity (New York: PublicAffairs, 2019).


15. Laura DeNardis, The Internet in Everything: Freedom and Security in a World with No Off Switch (New Haven, CT: Yale University Press, 2020).


16. Christopher Mims, Arriving Today: From Factory to Front Door—Why Everything Has Changed about How and What We Buy (New York: Harper Business, 2021).


17. I have written about this issue previously in Wendy H. Wong and Jamie Duncan, "Opinion: Facebook's Metaverse Won't Be Bound by Physical Borders—Neither Are Human Rights," Globe and Mail, November 5, 2021.


18. "Community Guidelines," Instagram Help Center, accessed April 21, 2022, https://help.instagram.com/477434105621119.


19. Loosely based on real cases decided by Meta's Oversight Board in January 2021 (case number 2020-004-IG-UA) and January 2023 (2022-009-IG-UA and 2022-010-IG-UA). In both cases the board ruled that the content should be reinstated after it was removed by Instagram. In both cases the content was reinstated by the company before the board's decision, but the cases helped establish a clearer policy around nudity. "Oversight Board Overturns Original Facebook Decision: Case 2020-004-IG-UA," January 2021, https://oversightboard.com/news/682162975787757-oversight-board-overturns-original-facebook-decision-case-2020-004-ig-ua/, and "Oversight Board overturns Meta's original decisions in the 'gender identity and nudity' cases," https://www.oversightboard.com/news/1214820616135890-oversight-board-overturns-meta-s-original-decisions-in-the-gender-identity-and-nudity-cases/.


20. For a comprehensive account, see Shoshana Zuboff, The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power (New York: PublicAffairs, 2019).


21. Haugen, Statement of Frances Haugen; Steven Levy, Facebook: The Inside Story (New York: Blue Rider Press, 2020); Sheera Frenkel and Cecilia Kang, An Ugly Truth: Inside Facebook's Battle for Domination (New York: Harper, 2021).


22. There are many things written about the Cambridge Analytica scandal, but a short explainer is here: "The Cambridge Analytica Story, Explained," Wired, accessed February 1, 2023, https://www.wired.com/amp-stories/cambridge-analytica-explainer/.


23. Jasper Jackson, Mark Townsend, and Lucy Kassa, "Facebook 'Lets Vigilantes in Ethiopia Incite Ethnic Killing,'" Observer, February 20, 2022; Alexandra Stevenson, "Facebook Admits It Was Used to Incite Violence in Myanmar," New York Times, November 6, 2018.


24. Sam Schechner, Jeff Horwitz, and Emily Glazer, "How Facebook Hobbled Mark Zuckerberg's Bid to Get America Vaccinated . . . ," Washington Post, September 17, 2021; Alexandra Stevenson, "Facebook Admits It Was Used to Incite Violence in Myanmar," New York Times, November 6, 2018; Brian Stelter, "Facebook Is Again Having to Account for Its Role in 2016 Election," CNNMoney, March 17, 2018, https://money.cnn.com/2018/03/17/technology/facebook-cambridge-analytica-2016-election/index.html; Barbie Latza Nadeau, "Facebook Accused of Foreign Election Interference for Banning Ugandan Accounts," Daily Beast, January 11, 2021, https://www.thedailybeast.com/matt-gaetzs-fundraising-craters-as-he-finally-shuns-spotlight.


25. "Ledger of Harms," Center for Humane Technology (blog), June 2021, https://ledger.humanetech.com/; Ethan Zuckerman, "Facebook Has a Misinformation Problem, and Is Blocking Access to Data about How Much There Is and Who Is Affected," Conversation (blog), November 2, 2021, http://theconversation.com/facebook-has-a-misinformation-problem-and-is-blocking-access-to-data-about-how-much-there-is-and-who-is-affected-164838; Alexandra Stevenson, "Facebook Admits It Was Used to Incite Violence in Myanmar," New York Times, November 6, 2018; Barbie Latza Nadeau, "Facebook Accused of Foreign Election Interference for Banning Ugandan Accounts," Daily Beast, January 11, 2021, https://www.thedailybeast.com/matt-gaetzs-fundraising-craters-as-he-finally-shuns-spotlight.


26. Articles 1, 18, and 19 in the Universal Declaration of Human Rights United Nations, "Universal Declaration of Human Rights," United Nations, accessed April 19, 2022, https://www.un.org/en/about-us/universal-declaration-of-human-rights.


27. Amartya Sen, Development as Freedom (New York: Anchor, 2000).


28. Tarleton Gillespie, Custodians of the Internet: Platforms, Content Moderation, and the Hidden Decisions That Shape Social Media (New Haven, CT: Yale University Press, 2018); Jeff Horwitz, "Facebook Says Its Rules Apply to All. Company Documents Reveal a Secret Elite That's Exempt. A Program Known as XCheck Has Given Millions of Celebrities, Politicians and Other High-Profile Users Special Treatment, a Privilege Many Abuse," Wall Street Journal, September 13, 2021.


29. Casey Newton, "The Secret Lives of Facebook Moderators in America," Verge, February 25, 2019, https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona; for an academic analysis, see Sarah T. Roberts, Behind the Screen: Content Moderation in the Shadows of Social Media (New Haven, CT: Yale University Press, 2019).


30. Safiya Noble, "The Loss of Public Goods to Big Tech," Noema, July 1, 2020, https://www.noemamag.com/the-loss-of-public-goods-to-big-tech.


31. Mike Isaac, "Facebook's Decisions Were 'Setbacks for Civil Rights,' Audit Finds," New York Times, July 8, 2020, sec. Technology.


32. "Oversight Board Charter," Oversight Board, September 2019, https://about.fb.com/wp-content/uploads/2019/09/oversight_board_charter.pdf.


33. Frances Haugen, "Statement of Frances Haugen," US Senate Committee on Commerce, Science and Transportation, Sub-Committee on Consumer Protection, Product Safety, and Data Security, October 4, 2021. For coverage, see Harriet Sinclair and Oliver O'Connell, "Facebook Whistleblower Frances Haugen Hailed as 21st Century Hero," Independent, October 5, 2021, https://www.independent.co.uk/news/world/americas/facebook-whistleblower-frances-haugen-live-b1932867.html;


34. Sheera Frenkel and Cecilia Kang, An Ugly Truth: Inside Facebook's Battle for Domination (New York: Harper, 2021).


35. Cecilia Kang, "Judge Throws Out 2 Antitrust Cases against Facebook," New York Times, June 28, 2021.


36. Josh Taylor "Facebook outage: what went wrong and why did it take so long to fix after social platform went down?" Guardian, October 5, 2021.


37. Tim Wu, The Master Switch: The Rise and Fall of Information Empires (New York: Knopf, 2010).


38. A growing trend in global governance theory advances this argument. Tanja A. Börzel and Thomas Risse, Effective Governance under Anarchy: Institutions, Legitimacy, and Social Trust in Areas of Limited Statehood (Cambridge: Cambridge University Press, 2021); Danielle F. Jung, Amanda Murdie, Wendy H. Wong, and Allison Cuttner, "Breaking Free from the State: A Functional Understanding of Governance" (paper presented at the Association of Political Science Annual Conference, Montreal, September 15, 2022).


39. Robert A. Dahl, "The Concept of Power," Behavioral Science 2 (1957): 202-203.


40. There are many writings on authority, but these sources can get readers started: Ian Hurd, "Legitimacy and Authority in International Politics," International Organization 53, no. 2 (1999): 379-409; David A. Lake, Hierarchy in International Relations (Ithaca, NY: Cornell University Press, 2009). On NGOs, see Sarah S. Stroup and Wendy H. Wong, The Authority Trap: Strategic Choices of International NGOs (Ithaca, NY: Cornell University Press, 2017).


41. My definition here comes from the oft-cited definition of the state by Max Weber, who wrote that states are a "human community that (successfully) claims the monopoly of the legitimate use of physical force within a given territory." Max Weber, Politics as a Vocation (Philadelphia: Fortress Press, 1965), 78 (orig. published 1919).


42. A. Claire Cutler, Virginia Haufler, and Tony Porter, Private Authority and International Affairs (Albany: SUNY Press, 1999); Deborah D. Avant, Martha Finnemore, and Susan K. Sell, eds., Who Governs the Globe? (New York: Cambridge University Press, 2010); Tim Büthe and Walter Mattli, The New Global Rulers: The Privatization of Regulation in the World Economy (Princeton, NJ: Princeton University Press, 2011).


43. Jessica F. Green, Rethinking Private Authority: Agents and Entrepreneurs in Global Environmental Governance (Princeton, NJ: Princeton University Press, 2014), 6.


44. Avant, Finnemore, and Sell, Who Governs the Globe?, 2.


45. On standards, see Walter Mattli and Tim Büthe, "Setting International Standards: Technological Rationality or Primacy of Power?" World Politics 56 (2003): 1-42.


46. See Siva Vaidhyanathan, The Googlization of Everything (And Why We Should Worry) (Berkeley: University of California Press, 2011).


47. Michael Barnett and Raymond Duvall, "Power in International Politics," International Organization 59 (2005): 39-75. This is similar to the "third face" of power from sociological theory; see Steven Lukes, Power: A Radical View (London: Macmillan, 1974).


48. Jaap-Henk Hoepman, "A Critique of the Google Apple Exposure Notification (GAEN) Framework," arXiv.org, 2020.


49. This project provided a live tracker for COVID-19 exposure apps: Patrick Howell O'Neill, Tate Ryan-Mosley, and Bobbie Johnson, "A Flood of Coronavirus Apps Are Tracking Us. Now It's Time to Keep Track of Them," MIT Technology Review, May 7, 2020; Casey Newton, "Why Countries Keep Bowing to Apple and Google's Contact Tracing App Requirements," Verge, May 8, 2020, https://www.theverge.com/interface/2020/5/8/21250744/apple-google-contact-tracing-england-germany-exposure-notification-india-privacy.


50. "Global Mobile OS Market Share 2012-2022," Statista, 2022, https://www.statista.com/statistics/272698/global-market-share-held-by-mobile-operating-systems-since-2009/.


51. David Kaye, Speech Police: The Global Struggle to Govern the Internet (New York: Columbia Global Reports, 2019), chap. 2.


52. Jack J. Barry, "COVID-19 Exposes Why Access to the Internet Is a Human Right," OpenGlobalRights (blog), May 26, 2020, https://www.openglobalrights.org/covid-19-exposes-why-access-to-internet-is-human-right/; Karl Bode, "The Case for Internet Access as a Human Right," Vice, November 13, 2019, https://www.vice.com/en/article/3kxmm5/the-case-for-internet-access-as-a-human-right; Lauren Goode, "Facebook Renews Its Ambitions to Connect the World," Wired, October 17, 2021, https://www.wired.com/story/facebook-renews-ambitions-connect-world/; Maeve Shearlaw, "Facebook Lures Africa with Free Internet—But What Is the Hidden Cost?," Guardian, August 1, 2016; Olivia Solon, "'It's Digital Colonialism': How Facebook's Free Internet Service Has Failed Its Users," Guardian, July 27, 2017; Savannah Wallace, "In the Developing World, Facebook Is the Internet," Startup (blog), September 7, 2020, https://medium.com/swlh/in-the-developing-world-facebook-is-the-internet-14075bfd8c5e; Lev Grossman, "Exclusive: Inside Facebook's Plan to Wire the World," TIME.com (blog), December 15, 2014, https://time.com/facebook-world-plan/.


53. Beverley McLachlin, Taylor Owen, and Peter Macleod, "Opinion: We Have the Regulatory Tools We Need to Fix Facebook," Globe and Mail, October 13, 2021.


54. Katie Arcieri, "Donors Affiliated with Amazon, Big Tech Throw Support behind Biden Campaign," October 7, 2020, https://www.spglobal.com/marketintelligence/en/news-insights/latest-news-headlines/donors-affiliated-with-amazon-big-tech-throw-support-behind-biden-campaign-60403546.


55. Damian Tambini, "Social Media Power and Election Legitimacy," in Digital Dominance: The Power of Google, Amazon, Facebook, and Apple, ed. Martin Moore and Damian Tambini (Oxford: Oxford University Press, 2018), 265-294.


56. Merja Myllylahti, "An Attention Economy Trap? An Empirical Investigation into Four News Companies' Facebook Traffic and Social Media Revenue," Journal of Media Business Studies 15, no. 4 (October 2, 2018): 242, 246.


57. Will Knight, "Many Top AI Researchers Get Financial Backing from Big Tech," Wired, accessed May 11, 2022, https://www.wired.com/story/top-ai-researchers-financial-backing-big-tech/; Tom Simonite, "The Dark Side of Big Tech's Funding for AI Research," Wired, accessed May 11, 2022, https://www.wired.com/story/dark-side-big-tech-funding-ai-research/; Mohamed Abdalla and Moustafa Abdalla, "The Grey Hoodie Project: Big Tobacco, Big Tech, and the Threat on Academic Integrity," in Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, July 21, 2021, 287-297.


58. Abdalla and Abdalla, "The Grey Hoodie Project."


59. Karen Hao, "We Read the Paper That Forced Timnit Gebru out of Google. Here's What It Says," MIT Technology Review, December 4, 2020; Tom Simonite, "What Really Happened When Google Ousted Timnit Gebru," Wired, accessed April 22, 2022, https://www.wired.com/story/google-timnit-gebru-ai-what-really-happened/.


60. Timnit Gebru, "For Truly Ethical AI, Its Research Must Be Independent from Big Tech," Guardian, December 6, 2021; Nitasha Tiku, "Google Fired Its Star AI Researcher One Year Ago. Now She's Launching Her Own Institute," Washington Post, December 2, 2021; Billy Perrigo, "Why Timnit Gebru Isn't Waiting for Big Tech to Fix AI's Problems," Time, January 18, 2022.


61. Tiku, "Google Fired Its Star AI Researcher One Year Ago.


62. "Loon," X, the moonshot factory, accessed May 11, 2022, https://x.company/projects/loon/.


63. Toussaint Nothias, "Access Granted: Facebook's Free Basics in Africa," Media, Culture and Society 42, no. 3 (April 1, 2020): 332.


64. Maeve Shearlaw, "Mark Zuckerberg Says Connectivity Is a Basic Human Right—Do You Agree?," Guardian, January 3, 2014; Jessi Hempel, "Zuckerberg to the UN: The Internet Belongs to Everyone," Wired, accessed November 2, 2021, https://www.wired.com/2015/09/zuckerberg-to-un-internet-belongs-to-everyone/; Jessi Hempel, "Inside Facebook's Ambitious Plan to Connect the Whole World," Wired, January 19, 2016, https://www.wired.com/2016/01/facebook-zuckerberg-internet-org/.


65. Karl Bode, "The Case for Internet Access as a Human Right," Vice, November 13, 2019, https://www.vice.com/en/article/3kxmm5/the-case-for-internet-access-as-a-human-right; Jack J. Barry, "COVID-19 Exposes Why Access to the Internet Is a Human Right," OpenGlobalRights (blog), May 26, 2020, https://www.openglobalrights.org/covid-19-exposes-why-access-to-internet-is-human-right/.


66. The partners were Ericsson, MediaTek, Nokia, Opera, Qualcomm, and Samsung; see details at "Technology Leaders Launch Partnership to Make Internet Access Available to All," Meta (blog), August 21, 2013, https://about.fb.com/news/2013/08/technology-leaders-launch-partnership-to-make-internet-access-available-to-all/.


67. Mark Zuckerberg, "Is Connectivity a Human Right?," 2013, 3, https://www.facebook.com/isconnectivityahumanright/isconnectivityahumanright.pdf.


68. Jessi Hempel, "What Happened to Facebook's Grand Plan to Wire the World?," Wired, May 17, 2018, https://www.wired.com/story/what-happened-to-facebooks-grand-plan-to-wire-the-world/.


69. Lev Grossman, "Exclusive: Inside Facebook's Plan to Wire the World," TIME.com (blog), December 15, 2014, https://time.com/facebook-world-plan/.


70. Olivia Solon, "'It's Digital Colonialism': How Facebook's Free Internet Service Has Failed Its Users," Guardian, July 27, 2017.


71. Maeve Shearlaw, "Facebook Lures Africa with Free Internet—But What Is the Hidden Cost?," Guardian, August 1, 2016.


72. Advox, "Can Facebook Connect the Next Billion?," Global Voices Advox (blog), July 27, 2017, https://advox.globalvoices.org/2017/07/27/can-facebook-connect-the-next-billion/. Global Voices, "Free Basics in Real Life: Six Case Studies on Facebook's Internet 'on Ramp' Initiative from Africa, Asia and Latin America," Advox (Global Voices, 2017).


73. Rahul Bhatia, "The Inside Story of Facebook's Biggest Setback," Guardian, May 12, 2016; Revati Prasad, "Ascendant India, Digital India: How Net Neutrality Advocates Defeated Facebook's Free Basics," Media, Culture and Society 40, no. 3 (April 1, 2018): 415-431; Subhayan Mukerjee, "Net Neutrality, Facebook, and India's Battle to #SaveTheInternet," Communication and the Public 1, no. 3 (September 1, 2016): 356-361.


74. Toussaint Nothias, "Access Granted: Facebook's Free Basics in Africa," Media, Culture and Society 42, no. 3 (April 1, 2020): 329-348.


75. Yoav Zeevi, "Facebook Introduces Discover: Exploring New Ways to Support Connectivity," Facebook Technology, May 5, 2020, https://tech.fb.com/discover/.


76. Josh Constine, "Facebook's Internet.Org Has Connected Almost 100M to the 'Internet,'" TechCrunch (blog), April 25, 2018, https://social.techcrunch.com/2018/04/25/internet-org-100-million/.


77. Manish Singh, "Discover Is Facebook's New Effort to Help People Access Websites for Free—But with Limits," TechCrunch (blog), May 6, 2020, https://social.techcrunch.com/2020/05/06/facebook-discover-connectivity-free-basics-initiative/; Lucy Pei, Benedict Salazar Olgado, and Roderic Crooks, "Market, Testbed, Backroom: The Redacted Internet of Facebook's Discover," in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (New York: ACM, 2021), 1-13.


78. Yoav Zeevi, "Facebook Introduces Discover: Exploring New Ways to Support Connectivity," Facebook Technology, May 5, 2020, https://tech.fb.com/discover/.


79. Pei et al., "Market, Testbed, Backroom."


80. Meaghan Tobin, "How Facebook Discover Replicated Many of Free Basics' Mistakes," Rest of World (blog), June 8, 2021, https://restofworld.org/2021/facebook-connectivity-discover/.


81. "About Discover," https://www.facebook.com/help/discover/314429429484693/?helpref=hc_fnav&bc[0]=Discover%20Help, accessed February 6, 2023.


82. Sarah West and Ellery Roberts Biddle, "Facebook's Free Basics Doesn't Connect You to the Global Internet—But It Does Collect Your Data," Global Voices Advox (blog), July 27, 2017, https://advox.globalvoices.org/2017/07/27/facebooks-free-basics-doesnt-connect-you-to-the-global-internet-but-it-does-collect-your-data/.


83. Douglass C. North, "Institutions," Journal of Economic Perspectives 5, no. 1 (1991): 97.


84. Lev Grossman, "Exclusive: Inside Facebook's Plan to Wire the World," TIME.com (blog), December 15, 2014, https://time.com/facebook-world-plan/. 7


85. In her testimony, Haugen described how both Facebook and Instagram target children as potential users, stating, "Facebook understands that if they want to continue to grow they have to find new users. They have to make sure that the next generation is just as engaged with Instagram as the current one, and the way they'll do that, making sure children establish habits before they have good self-regulation." Victor Ordonez, "Key Takeaways from Facebook Whistleblower Frances Haugen's Senate Testimony," ABC News, October 5, 2021, https://abcnews.go.com/Politics/key-takeaways-facebook-whistleblower-frances-haugens-senate-testimony/story?id=80419357.


86. Karen Hao, "Frances Haugen Says Facebook's Algorithms Are Dangerous. Here's Why," MIT Technology Review, October 5, 2021.


87. Eric Kohn, "Zuckerberg's Metaverse Will Create Revenue Streams for Creators, but Your Thoughts May Not Be Your Own," IndieWire (blog), October 29, 2021, https://www.indiewire.com/2021/10/facebook-metaverse-explained-meta-1234675318/.


88. There are many explainers on this topic—for instance "The Internet and the World Wide Web Are Not the Same Thing," NBC News, accessed November 3, 2021, https://www.nbcnews.com/tech/internet/internet-world-wide-web-are-not-same-thing-n51011.


89. Paul Mozur, "A Genocide Incited on Facebook, with Posts from Myanmar's Military," New York Times, October 15, 2018; Alexandra Stevenson, "Facebook Admits It Was Used to Incite Violence in Myanmar," New York Times, November 6, 2018.


90. Alex Warofka, "An Independent Assessment of the Human Rights Impact of Facebook in Myanmar," Meta, November 5, 2018, https://about.fb.com/news/2018/11/myanmar-hria.


91. The court case is called Doe v. Meta Platforms, https://edelson.com/wp-content/uploads/doevmetacomplaint.pdf, accessed February 6, 2023. Rachyl Jones, "The Rohingya's Genocide Suit Against Meta is Dismissed—For Now," Observer, December 15, 2022, https://observer.com/2022/12/the-rohingyas-genocide-suit-against-meta-is-dismissed-for-now/.


92. Amnesty International, "The Social Atrocity: Meta and the Right to Remedy for the Rohingya," 2022, https://www.amnesty.org/en/documents/ASA16/5933/2022/en/ (Accessed February 1, 2023).


93. "Corporate Human Rights Policy," Meta, April, 2021, https://about.fb.com/wp-content/uploads/2021/04/Facebooks-Corporate-Human-Rights-Policy.pdf; "Meta Human Rights Report: Insights and Actions 2020-2021" Meta, July, 2022, https://about.fb.com/wp-content/uploads/2022/07/Meta_Human-Rights-Report-July-2022.pdf. Criticism of the quality and completeness of the Oversight Board's human rights impact assessment was widespread; see Katie Paul, "Facebook-Owner Meta Releases First Human Rights Report," Reuters, July 14, 2022, https://www.reuters.com/technology/facebook-owner-meta-releases-first-human-rights-report-2022-07-14/.


94. Gillespie, Custodians of the Internet.


95. Robert Gorwa, "What Is Platform Governance?," Information, Communication and Society 22, no. 6 (May 12, 2019): 854-871.


96. Historically, others companies and private entities have played this role, see Swati Srivastava, Hybrid Sovereignty in World Politics (Cambridge: Cambridge University Press, 2022).


97. Caleb Ecarma, "'This Is Going to Be a Global Moment': All Eyes Are on Facebook as It Weighs Whether to Ban Donald Trump for Life," Vanity Fair Blogs (blog), April 22, 2021, https://www.vanityfair.com/news/2021/04/facebook-weighs-lifetime-trump-ban.


98. The original blog no longer exists. This is a reproduction of an original post. Marc Andreessen, "The Three Kinds of Platforms You Meet on the Internet," Pmarchive, September 16, 2007, https://fictivekin.github.io/pmarchive-jekyll//three_kinds_of_platforms_you_meet_on_the_internet.


99. Lawrence Lessig, Code: And Other Laws of Cyberspace, Version 2.0, 2nd rev. ed. (New York: Basic Books, 2006).


100. Gillespie, Custodians of the Internet: Platforms, chap. 1.


101. Kate Klonick, "The Facebook Oversight Board: Creating an Independent Institution to Adjudicate Online Free Expression," Yale Law Journal 129, no. 8 (2020 2019): 2418-99.


102. Milton L. Mueller, "Hyper-Transparency and Social Control: Social Media as Magnets for Regulation," Telecommunications Policy 39, no. 9 (2015): 804-810; Gillespie, Custodians of the Internet, 30-44; Jeff Kosseff, The Twenty-Six Words That Created the Internet (Ithaca, NY: Cornell University Press, 2019).


103. Alex Abad-Santos, "The Reddit Revolt That Led to CEO Ellen Pao's Resignation, Explained," Vox, July 8, 2015, https://www.vox.com/2015/7/8/8914661/reddit-victoria-protest; Andrew Marantz, "Reddit and the Struggle to Detoxify the Internet," New Yorker, March 12, 2018; Jennifer Forestal, "Beyond Gatekeeping: Propaganda, Democracy, and the Organization of Digital Publics," Journal of Politics 83, no. 1 (January 1, 2021): 306-320.


104. Gillespie, Custodians of the Internet, 5-10.


105. "Twitter: Most Users by Country," Statista, September 7, 2021, https://www.statista.com/statistics/242606/number-of-active-twitter-users-in-selected-countries/.


106. Gillespie, Custodians of the Internet, 74.


107. Harvard Law School, "Noah Feldman," accessed October 22, 2021, https://hls.harvard.edu/faculty/directory/10257/Feldman/.


108. Kate Klonick, "Inside the Making of Facebook's Supreme Court," New Yorker, February 15, 2021.


109. Klonick, "Inside the Making of Facebook's Supreme Court"; see also Naomi Nix, "Facebook Board Rejects Proposals to Curb Zuckerberg's Power," Bloomberg.com, May 26, 2021, https://www.bloomberg.com/news/articles/2021-05-26/facebook-board-rejects-proposals-to-reduce-zuckerberg-s-power about Zuckerberg's power in the company.


110. Excerpted from Radiolab, "Facebook's Supreme Court," WNYC Studios, February 12, 2021, https://www.wnycstudios.org/podcasts/radiolab/articles/facebooks-supreme-court.


111. "Harvard Law Professor Plays Instrumental Role in Creation of Facebook's Content Oversight Board," Harvard Law Today, June 27, 2019, https://today.law.harvard.edu/harvard-law-professor-plays-instrumental-role-in-creation-of-facebooks-content-oversight-board/.


112. Beth A. Simmons, Mobilizing for Human Rights (New York: Cambridge University Press, 2009).


113. Rokhaya Diallo, "Opinion: France's Latest Vote to Ban Hijabs Shows How Far It Will Go to Exclude Muslim Women," Washington Post, April 21, 2021.


114. "Your Right to Religious Freedom," American Civil Liberties Union, accessed October 27, 2021, https://www.aclu.org/other/your-right-religious-freedom.


115. Radiolab, "Facebook's Supreme Court"; Luke Barnes, "Facebook Is Banning Women for Criticizing Men, Calls It 'Hate Speech,'" ThinkProgress (blog), December 6, 2017, https://archive.thinkprogress.org/facebook-is-banning-women-for-posting-all-men-are-scum-91825328d58c/.


116. Klonick, "Inside the Making of Facebook's Supreme Court."


117. Radiolab, "Facebook's Supreme Court."


118. "The Oversight Board Is Now Accepting Cases," October 22, 2020, https://oversightboard.com/news/833880990682078-the-oversight-board-is-now-accepting-cases/.


119. "Oversight Board Charter," September 2019, https://about.fb.com/wp-content/uploads/2019/09/oversight_board_charter.pdf.


120. "Announcing the First Members of the Oversight Board," May 2020, https://oversightboard.com/news/327923075055291-announcing-the-first-members-of-the-oversight-board/.


121. "Oversight Board Charter," September 2019, https://about.fb.com/wp-content/uploads/2019/09/oversight_board_charter.pdf.


122. All data are from "An Empirical Look at the Facebook Oversight Board," Lawfare, January 26, 2021, https://www.lawfareblog.com/empirical-look-facebook-oversight-board, which is regularly updated. Some patterns are reported on Lawfare, others are based on author analysis of data available on the Oversight Board's webpage, "Case Decisions and Policy Advisory Opinions," Oversight Board, https://www.oversightboard.com/decision/, and Lawfare's page "The Oversight Board's Decided Cases," https://www.lawfareblog.com/oversight-boards-decided-cases#2022%20Decisions.


123. Evelyn Douek, "The Oversight Board Moment You Should've Been Waiting For: Facebook Responds to the First Set of Decisions," Lawfare, February 26, 2021, https://www.lawfareblog.com/oversight-board-moment-you-shouldve-been-waiting-facebook-responds-first-set-decisions.


124. Laura Reed, "Facebook's Oversight Board Needs a Broader Mandate That Integrates Human Rights Principles," Ranking Digital Rights, May 22, 2019, https://rankingdigitalrights.org/2019/05/22/facebooks-oversight-board-needs-broader-mandate-that-integrates-human-rights-principles/; David Morar, "Facebook's Oversight Board: A Toothless Supreme Court?," Internet Governance Project (blog), October 2, 2019, https://www.internetgovernance.org/2019/10/02/facebooks-oversight-board-a-judiciary-with-no-constitution/.


125. Laurence Helfer and Molly K. Land, "Is the Facebook Oversight Board an International Human Rights Tribunal?," Lawfare, May 13, 2021, https://www.lawfareblog.com/facebook-oversight-board-international-human-rights-tribunal.


126. Sue Halpern, "An Ad-Hoc Group of Activists and Academics Convene a 'Real Facebook Oversight Board,'" New Yorker, October 15, 2020; Mike Butcher, "'The Real Facebook Oversight Board' Launches to Counter Facebook's 'Oversight Board,'" TechCrunch (blog), September 30, 2020, https://social.techcrunch.com/2020/09/30/the-real-facebook-oversight-board-launches-to-counter-facebooks-oversight-board/.


127. All data from "Oversight Board Cases: Transparency Center," accessed May 11, 2022, https://transparency.fb.com/oversight/oversight-board-cases/.


128. Gilad Edelman, "Admit It: The Facebook Oversight Board Is Kind of Working," Wired, June 4, 2021, https://www.wired.com/story/facebook-oversight-board-kind-of-working-trump-ban/.


129. Nick Clegg, "In Response to Oversight Board, Trump Suspended for Two Years; Will Only Be Reinstated If Conditions Permit," About Facebook (blog), June 4, 2021, https://about.fb.com/news/2021/06/facebook-response-to-oversight-board-recommendations-trump/.


130. "The Problem with Big Tech's Wartime Push against Putin," Wired, accessed May 11, 2022, https://www.wired.com/story/plaintext-big-tech-wartime-push-against-putin/.


131. Nikos Smyrnaios, Internet Oligopoly: The Corporate Takeover of Our Digital World (Bingley, UK: Emerald Publishing, 2018).


132. Wendy H. Wong, David A. Lake, and Jamie Duncan, "Why Data Are So Hard to Govern" (International Studies Association, Virtual Panel, 2022).


133. Tony Romm, "Amazon, Apple, Facebook and Google Grilled on Capitol Hill over Their Market Power," Washington Post, July 29, 2020; Ian Bogost, "The Dot-Coms Were Better Than Facebook," Atlantic, April 13, 2018.


134. Zi-Ann Lum, "Canadian Committee Calls for Tough Actions against Pornhub," Politico, June 17, 2021, https://www.politico.com/news/2021/06/17/canadian-committee-tough-action-pornhub-495077.


135. Adam Satariano and Daisuke Wakabayashi, "Why Turkey's Regulators Became Such a Problem for Google," New York Times, July 29, 2021.


136. Levy, Facebook; Frenkel and Kang, An Ugly Truth; Gilad Edelman, "How to Fix Facebook, according to Facebook Employees," Wired, October 25, 2021, https://www.wired.com/story/how-to-fix-facebook-according-to-facebook-employees/; John Hendel, "'This Is NOT Normal': Facebook Employees Vent Their Anguish," Politico, October 25, 2021, https://www.politico.com/news/2021/10/25/facebook-employees-message-anguish-517012.




Chapter 7


1. Siva Vaidhyanathan, "Elon Musk Doesn't Understand Free Speech—or Twitter—at All," Guardian, April 28, 2022, https://www.theguardian.com/commentisfree/2022/apr/28/elon-musk-doesnt-understand-free-speech-or-twitter-at-all.


2. This definition will be unpacked, but is based on UNESCO's definition, see "Literacy," UNESCO Institute for Statistics, https://uis.unesco.org/node/3079547, accessed February 2, 2023.


3. On a similar challenge with media literacy, see Monica Bulger and Patrick Davison, "The Promises, Challenges, and Futures of Media Literacy," Journal of Media Literacy Education 10, no. 1 (2018): 1-21.


4. Elon Musk (@elonmusk), "The bird is freed," Twitter, October 28, 2022, https://twitter.com/elonmusk/status/1585841080431321088.


5. See Elon Musk (@elonmusk), "I made an offer https://sec.gov/Archives/edgar/data/0001418091/000110465922045641/Tm2212748d1_sc13da.Htm," Twitter, April 14, 2022, https://twitter.com/elonmusk/status/1514564966564651008; Ben Gilbert, "Elon Musk Is Attempting a Hostile Takeover of Twitter. Here's How That Could Work," Business Insider, April 23, 2022, https://www.businessinsider.com/elon-musk-twitter-hostile-takeover-how-it-works-2022-4.


6. Adam Serwer, "Elon Musk Isn't Buying Twitter to Defend Free Speech," Atlantic, April 27, 2022, https://www.theatlantic.com/ideas/archive/2022/04/elon-musk-twitter-free-speech/629694/.


7. Alex Hern and Dominic Rushe, "Elon Musk Says Twitter Must Be 'Neutral' as Wave of Leftwing Users Quit," Guardian, April 28, 2022, https://www.theguardian.com/technology/2022/apr/28/elon-musk-says-twitter-must-be-politically-neutral-as-some-leftwing-users-quit.


8. For a sample, see Jean Burgess, "The 'Digital Town Square'? What Does It Mean When Billionaires Own the Online Spaces Where We Gather?," The Conversation, accessed April 29, 2022, http://theconversation.com/the-digital-town-square-what-does-it-mean-when-billionaires-own-the-online-spaces-where-we-gather-182047; Elizabeth Dwoskin, "Elon Musk Wants a Free Speech Utopia. Technologists Clap Back," Washington Post, April 18, 2022, https://www.washingtonpost.com/technology/2022/04/18/musk-twitter-free-speech/; Josh Taylor, "Elon Musk's Twitter Takeover: What Will Change, Is Free Speech at Risk and Should You Delete the App?," Guardian, April 26, 2022, https://www.theguardian.com/technology/2022/apr/26/elon-musk-twitter-takeover-bought-buys-what-will-change-is-free-speech-at-risk; Pranshu Verma, "Elon Musk Wants 'Free Speech' on Twitter. But for Whom?," Washington Post, May 6, 2022, https://www.washingtonpost.com/technology/2022/05/06/twitter-harassment/; Vaidhyanathan, "Elon Musk Doesn't Understand Free Speech; Zeeshan Aleem, "Opinion: Why Elon Musk's Idea of Free Speech Makes No Sense," MSNBC.com, April 29, 2022, https://www.msnbc.com/opinion/msnbc-opinion/elon-musk-twitter-s-free-speech-philosophy-king-not-promising-n1294901; Evan Greer, "Opinion: Elon Musk's Twitter Takeover Exposes the Real Threat to Free Speech: Big Tech Monopolies," CNN, May 10, 2022, https://www.cnn.com/2022/05/10/perspectives/elon-musk-twitter-antitrust/index.html.


9. jack (@jack), "In principle, I don't believe anyone should own or run Twitter. It wants to be a public good at a protocol level, not a company. Solving for the problem of it being a company however, Elon is the singular solution I trust. I trust his mission to extend the light of consciousness," Twitter, April 26, 2022, https://twitter.com/jack/status/1518772756069773313.


10. Elon Musk (@elonmusk), "Dear Twitter Advertisers Https://T.Co/GMwHmInPAS," Twitter, October 27, 2022, https://twitter.com/elonmusk/status/1585619322239561728.


11. James Muldoon, Platform Socialism: How to Reclaim Our Digital Future from Big Tech (Pluto Press, 2022).


12. "Elon Musk Fires Twitter Board, Making Himself Sole Director," alJazeera, November 1, 2022, https://www.aljazeera.com/economy/2022/11/1/elon-musk-fires-twitter-board-making-himself-sole-director.


13. Patrick Traughber, "Twitter Blue is back. And gold checkmarks are here," Twitter, December 12, 2022, https://blog.twitter.com/en_us/topics/product/2022/twitter-blue-update. These changes were resisted loudly by a number of prominent Twitter users, including novelist Stephen King and Democratic Congresswoman Alexandria Ocasio-Cortez.


14. Donie O'Sullivan and Clare Duffy, "Elon Musk's Twitter Lays Off Employees across the Company," CNN, November 4, 2022, https://www.cnn.com/2022/11/03/tech/twitter-layoffs/index.html.


15. Vaidhyanathan, "Elon Musk Doesn't Understand Free Speech."


16. Vaidhyanathan, "Elon Musk Doesn't Understand Free Speech."


17. "Speakers' Corner," Royal Parks, accessed April 29, 2022, https://www.royalparks.org.uk/parks/hyde-park/things-to-see-and-do/speakers-corner.


18. Jon Ronson, So You've Been Publicly Shamed (New York: Riverhead Books, 2016).


19. "Act to Improve Enforcement of the Law in Social Networks (Network Enforcement Act, NetzDG)—Basic Information," Bundesministerium der Justiz, 2017, https://www.bmj.de/DE/Themen/FokusThemen/NetzDG/NetzDG_EN_node.html, accessed February 4, 2023.


20. This list of countries with similar legislation includes other European countries, like France and the United Kingdom. The list of non-European countries that have taken cues from NetzDG include Russia, Kenya, Venezuela, Belarus, Egypt, Vietnam, Australia, Malaysia, Philippines, Singapore, Turkey, Brazil, and India. See Swati Srivastava "Assessing Global Regulatory Responses to Facebook's Political Harms" (paper presented at the American Political Science Association meeting, 2022).


21. Julian Posada, Nicholas Weller, and Wendy H. Wong, "We Haven't Gone Paperless Yet: Why the Printing Press Can Help Us Understand Data and AI," in Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society (New York: ACM, 2021), 864-872, https://doi.org/10.1145/3461702.3462604.


22. The following book provides great analysis and detail about the historical process outlined here: Ronald J. Deibert, Parchment, Printing, and Hypermedia: Communication in World Order Transformation (New York: Columbia University Press, 1997).


23. Yoram Eshet-Alkalai, "Thinking in the Digital Era: A Revised Model for Digital Literacy," Issues in Informing Science and Information Technology 9 (2012): 267; T. Philip Nichols and Amy Stornaiuolo, "Assembling 'Digital Literacies': Contingent Pasts, Possible Futures," Media and Communication (Lisboa) 7, no. 2 (2019): 14-15.


24. Rahul Bhargava and Catherine D'Ignazio, "Designing Tools and Activities for Data Literacy Learners," Web Science: Data Literacy Workshop, Oxford, UK, 2015, http://www.kanarinka.com/wp-content/uploads/2021/01/Bhargava-and-DIgnazio-Designing-Tools-and-Activities-for-Data-Literacy-L.pdf. See also Bhargava and D'Ignazio, "Designing Tools and Activities for Data Literacy Learners"; Tibor Koltay, "Data Literacy: In Search of a Name and Identity," Journal of Documentation 71, no. 2 (2015): 401-415.


25. Jonas Söderholm and Jan Nolin, "Collections Redux: The Public Library as a Place of Community Borrowing," Library Quarterly 85, no. 3 (July 1, 2015): 244-260.


26. For an example, see Yves Punie, Anusca Ferrari, and Barbara N. Brečko "DIGCOMP: A Framework for Developing and Understanding Digital Competence in Europe" (Publications Office of the European Union, 2013), https://op.europa.eu/en/publication-detail/-/publication/a410aad4-10bf-4d25-8c5a-8646fe4101f1/language-en.


27. Joris Vlieghe, "Labor and Technology," in Social Epistemology and Technology: Toward Public Self-Awareness Regarding Technological Mediation, ed. Frank Scalambrino (London: Rowman & Littlefield, 2016), 135.


28. UNESCO Education Sector, "The Plurality of Literacy and Its Implications for Policies and Programmes: Position Paper" (France: United Nations Educational, Scientific and Cultural Organization, 2004), https://unesdoc.unesco.org/ark:/48223/pf0000136246; Elinor Carmi et al., "Data Citizenship: Rethinking Data Literacy in the Age of Disinformation, Misinformation, and Malinformation," Internet Policy Review 9, no. 2 (2020): 4.


29. "Literacy," UNESCO Institute for Statistics.


30. See Henry A. Giroux, "Literacy and the Pedagogy of Voice and Political Empowerment," Educational Theory 38, no. 1 (1988): 61-75; UNESCO Education Sector, "The Plurality of Literacy and Its Implications for Policies and Programmes"; Eshet-Alkalai, "Thinking in the Digital Era; Julia Feerrar, "Development of a Framework for Digital Literacy," Reference Services Review (47): 91-105.


31. Carmi et al., "Data Citizenship," 4.


32. For a discussion and definition of literacy, see "Literacy," UNESCO, accessed August 17, 2021, https://en.unesco.org/themes/literacy. A longer history is available here: UNESCO Education Sector, "The Plurality of Literacy and Its Implications for Policies and Programmes."


33. Andrew Whitworth, Radical Information Literacy: Reclaiming the Political Heart of the IL Movement (Oxford: Chandos Publishing, 2014), 1.


34. Gina Neff, Anissa Tanweer, Brittany Fiore-Gartland, and Laura Osburn, "Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science," Big Data 5, no. 2 (June 2017): 85-97.


35. Experian, "A Data Powered Future," Experian White Paper, 2017, https://www.experian.co.uk/blogs/latest-thinking/risk-analytics/a-data-powered-future/; Qlik and Data Literacy Project, "The Data Literacy Index: The $500m Enterprise Value Opportunity Results Summary," October 2018.


36. Dario Amodei et al., "Concrete Problems in AI Safety," arXiv:1606.06565 [Cs], July 25, 2016, http://arxiv.org/abs/1606.06565.


37. Sonja Špiranec, Denis Kos, and Michael George, "Searching for Critical Dimensions in Data Literacy," in Proceedings of the Tenth International Conference on Conceptions of Library and Information Science (Ljubljana, Slovenia: University of Borås, 2019), http://informationr.net/ir/24-4/colis/colis1922.html.


38. "What Is a Filter Bubble?—Definition from Techopedia," Techopedia.com, accessed May 2, 2022, http://www.techopedia.com/definition/28556/filter-bubble.


39. Douglas Hodgson, The Human Right to Education (Aldershot; Ashgate, 1998).


40. United Nations, "Universal Declaration of Human Rights," United Nations, accessed April 19, 2022, https://www.un.org/en/about-us/universal-declaration-of-human-rights; Committee on Economic, Social and Cultural Rights, "Implementation of the International Covenant on Economic, Social, and Cultural Rights: General Comment No. 13 The Right to Education (Article 13 of the Covenant)," Twenty-First Session, November 15-December 3, 1999 (United Nations, December 8, 1999), https://docstore.ohchr.org/SelfServices/FilesHandler.ashx?enc=4slQ6QSmlBEDzFEovLCuW%2BKyH%2BnXprasyMzd2e8mx4cYlD1VMUKXaG3Jw9bomilLKS84HB8c9nIHQ9mUemvt0Fbz%2F0SS7kENyDv5%2FbYPWAxMw47K5jTga59puHtt3NZr; UNESCO, "Unpacking Sustainable Development Goal 4," Education 2030 (United Nations Educational, Scientific and Cultural Organization, 2016), https://www.right-to-education.org/sites/right-to-education.org/files/resource-attachments/Unesco_Guide_to_unpacking_SDG4_2015_En.pdf.


41. Committee on Economic, Social and Cultural Rights, "Implementation of the International Covenant on Economic, Social, and Cultural Rights.


42. Klaus Dieter Beiter, The Protection of the Right to Education by International Law: Including a Systematic Analysis of Article 13 of the International Covenant on Economic, Social, and Cultural Rights (Leiden: Martinus Nijhoff, 2006).


43. Dennis Niemann, "International Organizations in Education: New Takes on Old Paradigms," in Global Pathways to Education: Cultural Spheres, Networks, and International Organizations, ed. Kerstin Martens and Michael Windzio (Cham: Springer, 2022), 127-161; K. Martens et al., Transformation of Education Policy (Basingstoke: Springer, 2010).


44. UNESCO Education Sector, "The Plurality of Literacy and Its Implications for Policies and Programmes: Position Paper" (France: United Nations Educational, Scientific and Cultural Organization, 2004), https://unesdoc.unesco.org/ark:/48223/pf0000136246.


45. Carmi et al., "Data Citizenship," 7-9.


46. Eshet-Alkalai, "Thinking in the Digital Era."


47. Giroux, "Literacy and the Pedagogy of Voice and Political Empowerment."


48. Kathleen Tyner, "Media Literacy in the Age of Big Data," in Media Literacy in a Disruptive Media Environment, ed. Belinha S. De Abreu and William G. Christ (New York: Routledge, 2020), 36.


49. Brian V. Street, "At Last: Recent Applications of New Literacy Studies in Educational Contexts," Research in the Teaching of English 39, no. 4 (2005): 417-423, offers an excellent overview of the literature in this vein.


50. For example, Luci Pangrazio and Neil Selwyn, "'Personal Data Literacies': A Critical Literacies Approach to Enhancing Understandings of Personal Digital Data," New Media and Society 21, no. 2 (February 2019): 419-437, which provides a helpful analytical framework that is premised on individual literacy and actions.


51. Carmi et al., "Data Citizenship," 11-12.


52. T. Philip Nichols and Amy Stornaiuolo, "Assembling 'Digital Literacies': Contingent Pasts, Possible Futures," Media and Communication (Lisboa) 7, no. 2 (2019): 14-24, https://doi.org/10.17645/mac.v7i2.1946; T. Philip Nichols and Robert Jean LeBlanc, "Beyond Apps: Digital Literacies in a Platform Society," Reading Teacher 74, no. 1 (2020): 103-109.


53. Catherine D'Ignazio and Rahul Bhargava, "Approaches to Building Big Data Literacy," Bloomberg Data for Good Exchange 2015, 2015, http://www.kanarinka.com/wp-content/uploads/2021/01/DIgnazio-and-Bhargava-Approaches-to-Building-Big-Data-Literacy.pdf. 2; Megan Dibble, "What Is Data Literacy?," Medium, January 15, 2022, https://towardsdatascience.com/what-is-data-literacy-9b5c3032216f.


54. Estonia has long been seen as the vanguard in online government and in helping citizens understand the media. See Amy Yee, "The Country Inoculating against Disinformation," BBC Future, January 30, 2022, https://www.bbc.com/future/article/20220128-the-country-inoculating-against-disinformation.


55. Lucia Nalbandian, "Canada Should Be Transparent in How It Uses AI to Screen Immigrants," The Conversation, April 28, 2021, http://theconversation.com/canada-should-be-transparent-in-how-it-uses-ai-to-screen-immigrants-157841.


56. Nick Heath, "How Estonia Became an E-Government Powerhouse," TechRepublic, February 19, 2019, https://www.techrepublic.com/article/how-estonia-became-an-e-government-powerhouse/.


57. "Data Culture Project," accessed October 1, 2021, https://databasic.io/en/culture/.


58. Rahul Bhargava, "Data Culture Project + World Food Programme—a Case Study," Medium (blog), March 23, 2018, https://medium.com/@rahulbot/data-culture-project-world-food-programme-a-case-study-a74c493a694f.


59. An interesting debate has arisen; see Karen Mundy, "Why Do We Keep Failing to Universalize Literacy? A Rejoinder to Girin Beeharry and an Invitation to the Gates Foundation," May 21, 2021, https://karenmundy.com/2021/05/21/why-do-we-keep-failing-to-universalize-literacy-a-rejoindre-to-girin-beeharry-and-an-invitation-to-the-gates-foundation/; David Evans and Susannah Hares, "Foundational Literacy and Numeracy Skills Are Important, Obviously. But Are They More Important Than All Other Education Investments?," May 4, 2021, https://www.cgdev.org/blog/foundational-literacy-and-numeracy-skills-are-important-obviously-are-they-more-important-all; Girindre Beeharry, "The Pathway to Progress on SDG 4 Requires the Global Education Architecture to Focus on Foundational Learning and to Hold Ourselves Accountable for Achieving It," International Journal of Educational Development 82 (April 2021): 102375, https://doi.org/10.1016/j.ijedudev.2021.102375.


60. Wendy H. Wong, Internal Affairs: How the Structure of NGOs Transforms Human Rights (Ithaca, NY: Cornell University Press, 2012).


61. "Ledger of Harms," Center for Humane Technology, June 2021, https://ledger.humanetech.com/.


62. Zadie Smith, "The North West London Blues," New York Review of Books (blog), June 2, 2012, https://www.nybooks.com/daily/2012/06/02/north-west-london-blues/.


63. Eric Klinenberg, Palaces for the People: How Social Infrastructure Can Help Fight Inequality, Polarization, and the Decline of Civic Life (New York: Crown, 2018).


64. Shannon Mattern, A City Is Not a Computer: Other Urban Intelligences (Princeton, NJ: Princeton University Press, 2021), 78.


65. Mattern, A City Is Not a Computer, 89-101, offers a helpful discussion of the various services libraries provide.


66. From Carnegie Hall in New York to Carnegie Mellon University in Pittsburgh to the Carnegie Corporation and the Carnegie Endowment for International Peace, Carnegie left an imprint on American and global culture and politics.


67. Much of the information on Carnegie's legacy on public libraries can be found in W. L. Williamson, review of Review of Carnegie Libraries: Their History and Impact on American Public Library Development, by George S. Bobinski, Library Quarterly: Information, Community, Policy 39, no. 4 (1969): 365-366.


68. As calculated on MeasuringWorth.com on October 4, 2021. https://www.measuringworth.com/calculators/ppowerus/.


69. See Abigail Ayres Van Slyck, Free to All: Carnegie Libraries and American Culture, 1890-1920 (Chicago: University of Chicago Press, 1995).


70. For a longitudinal and thoughtful history of the library, see John Budd, Self-Examination: The Present and Future of Librarianship (Westport, CT: Libraries Unlimited, 2008).


71. Taylor L. Willingham, "Libraries as Civic Agents," Public Library Quarterly 27, no. 2 (July 1, 2008): 97-110.


72. Jonas Söderholm and Jan Nolin, "Collections Redux: The Public Library as a Place of Community Borrowing," Library Quarterly 85, no. 3 (July 1, 2015): 244-260; Anne Goulding, "Engaging with Community Engagement: Public Libraries and Citizen Involvement," New Library World 110, no. 1/2 (January 1, 2009): 37-51.


73. Shannon Mattern, "Library as Infrastructure," Places Journal, June 9, 2014.


74. Joseph Janes, ed., Library 2020: Today's Leading Visionaries Describe Tomorrow's Library (Lanham, MD: Scarecrow Press, 2013).


75. Sarah Houghton, "Chapter Five," in Library 2020: Today's Leading Visionaries Describe Tomorrow's Library, ed. Joseph Janes (Lanham, MD: Scarecrow Press, 2013), 35.


76. Jon Klassen (@Burstofbeaden), "last year in a library in Alaska I read a folk tale in a random book on a random shelf & have been thinking about it since & today i wrote the librarian w/ no book title or author & in 2 hrs i had a scan of the story & cover in my inbox - librarians should be running everything," Twitter, January 9, 2019, https://twitter.com/burstofbeaden/status/1083137440120885248?lang=en.


77. "About ALA," About ALA, accessed September 28, 2021, https://www.ala.org/aboutala/.


78. "ALA Policy Manual, Section B" (2019), 5, https://www.ala.org/aboutala/governance/policymanual.


79. Kay Mathiesen, "The Human Right to a Public Library," Journal of Information Ethics 22, no. 1 (2013): 60-79, https://doi.org/10.3172/JIE.22.1.60.


80. John M. Budd, "Information Literacy and Consciousness," Journal of Documentation 76, no. 6 (January 1, 2020): 1377-1391.


81. "ALA Divisions," Text, About ALA, April 5, 2017, https://www.ala.org/aboutala/divs.


82. Julia Feerrar, "Development of a Framework for Digital Literacy," Reference Services Review 47, no. 2 (2019): 91-105.


83. Clifford A. Lynch, "Chapter 4," in Library 2020: Today's Leading Visionaries Describe Tomorrow's Library, ed. Joseph Janes (Lanham, MD: Scarecrow Press, 2013), 28.


84. Mattern, A City Is Not a Computer, chap. 3.


85. Jack J. Barry, "COVID-19 Exposes Why Access to the Internet Is a Human Right," OpenGlobalRights, May 26, 2020, https://www.openglobalrights.org/covid-19-exposes-why-access-to-internet-is-human-right/; S. Tully, "A Human Right to Access the Internet? Problems and Prospects," Human Rights Law Review 14, no. 2 (2014): 175-195; Ryan Shandler and Daphna Canetti, "A Reality of Vulnerability and Dependence: Internet Access as a Human Right," Israel Law Review 52, no. 1 (2019): 77-98; Merten Reglitz, "The Human Right to Free Internet Access," Journal of Applied Philosophy 37, no. 2 (2020): 314-331; Kay Mathiesen, "Human Rights for the Digital Age," Journal of Mass Media Ethics 29, no. 1 (January 2, 2014): 2-18; Abhinav Mehrotra, "Access to Internet as a Human Right—Justification and Comparative Study," Comparative Law Review (Toruń, Poland) 27 (2021): 313-327.


86. Sidewalk Toronto, "How Toronto's Leading the Way in Library Innovation: A Conversation with Pam Ryan," Sidewalk Toronto (blog), January 15, 2020, https://medium.com/sidewalk-toronto/how-torontos-leading-the-way-in-library-innovation-a-conversation-with-pam-ryan-df1bf02b0590.


87. See point 3 in Anne Helen Petersen, "The Librarians Are Not Okay," Substack newsletter, Culture Study (blog), May 1, 2022, https://annehelen.substack.com/p/the-librarians-are-not-okay.


88. On the New York Public Library's actions, see Anthony W. Marx, "Opinion: Libraries Must Change," New York Times, May 28, 2020, https://www.nytimes.com/2020/05/28/opinion/libraries-coronavirus.html. More generally, see Mattern, A City Is Not a Computer, chap. 3.


89. As quoted in Mattern, A City Is Not a Computer, 100.


90. Multistakeholder platform the Global Network Initiative, for example, which includes many technology companies, focuses on expression and privacy: Global Network Initiative, "GNI Home," Global Network Initiative, accessed August 22, 2022, https://globalnetworkinitiative.org/. For a discussion about why privacy is insufficient, see Julie E. Cohen, Between Truth and Power: The Legal Constructions of Informational Capitalism (New York: Oxford University Press, 2019), https://doi.org/10.1093/oso/9780190246693.001.0001.


91. Leah Pierson, "Ethics Education in U.S. Medical Schools' Curricula," Bill of Health, June 13, 2022, http://blog.petrieflom.law.harvard.edu/2022/06/13/ethics-education-in-u-s-medical-schools-curricula/. For an international view, see Olivia Miu Yung Ngan and Joong Hiong Sim, "Evolution of Bioethics Education in the Medical Programme: A Tale of Two Medical Schools," International Journal of Ethics Education 6, no. 1 (2021): 37-50, https://doi.org/10.1007/s40889-020-00112-0.


92. Casey Fiesler, "Tech Ethics: A Collection of Syllabi," Medium, July 5, 2018, https://cfiesler.medium.com/tech-ethics-curricula-a-collection-of-syllabi-3eedfb76be18. Insights valid as of February 2, 2023, as the list is continually updated.


93. "Embedded EthiCS," Embedded EthiCS @ Harvard, accessed August 22, 2022, https://embeddedethics.seas.harvard.edu/home; Stanford University, "New Program Embeds Ethics into Computer Science Courses," Stanford Report (blog), June 28, 2022, https://news.stanford.edu/report/2022/06/28/new-program-embeds-ethics-computer-science-courses/; Jovana Jankovic, "Pilot Program Embeds Ethics into U of T Undergraduate Technology Courses," University of Toronto News, July 5, 2021, https://www.utoronto.ca/news/pilot-program-embeds-ethics-u-t-undergraduate-technology-courses.


94. Diane Horton, David Liu, Sheila A. Mcilraith, and Nina Wang, "Is More Better When Embedding Ethics in CS Courses?" in Proceedings of the ACM SIGCSE Conference (Toronto: ACM, 2023); Diane Horton, Maryam Majedi, Sheila A. Mcilraith, Emma McClure, Nina Wang, and Benjamin Wald, "Embedding Ethics in Computer Science Courses: Does It Work?" in Proceedings of the ACM SIGCSE Conference (Providence: ACM, 2022).




Chapter 8


1. Lucas Matney, "NFT Startup Dapper Labs Acquires Virtual Influencer Startup Brud," TechCrunch, October 4, 2021, https://techcrunch.com/2021/10/04/nft-startup-dapper-labs-acquires-virtual-influencer-startup-brud/.


2. Matt Klein, "The Problematic Fakery of Lil Miquela Explained—An Exploration of Virtual Influencers and Realness," Forbes, November 17, 2020, https://www.forbes.com/sites/mattklein/2020/11/17/the-problematic-fakery-of-lil-miquela-explained-an-exploration-of-virtual-influencers-and-realness/.


3. "Digital People—The Future of CX," Soul Machines, accessed August 30, 2022, https://www.soulmachines.com/.


4. "Will.i.Am+Soul Machines: Will the Real Will.i.Am Please Stand Up?," Soul Machines, accessed August 30, 2022, https://www.soulmachines.com/use-cases/entertainment/.


5. The touchstone book on this subject is Tom Regan, The Case for Animal Rights, updated with a new preface (Berkeley: University of California Press, 2004). See also Will Kymlicka and Sue Donaldson, "Animal Rights, Multiculturalism, and the Left: Animal Rights, Multiculturalism, and the Left," Journal of Social Philosophy 45, no. 1 (March 2014): 116-135.


6. See, for example, Hélène Landemore, "Deliberation, Cognitive Diversity, and Democratic Inclusiveness: An Epistemic Argument for the Random Selection of Representatives," Synthese 190, no. 7 (2013): 1209-1231; Hélène Landemore, Democratic Reason: Politics, Collective Intelligence, and the Rule of the Many (Princeton, NJ: Princeton University Press, 2012).


7. Dave Eggers, The Circle (Toronto: Vintage Canada, 2014); Dave Eggers, The Every (Toronto: Vintage Canada, 2021).


8. On the BEIC, Swati Srivastava, "Corporate Sovereign Awakening and the Making of Modern State Sovereignty: New Archival Evidence from the English East India Company," International Organization, March 4, 2022, 1-23. On the DEIC, Giles Milton, Nathaniel's Nutmeg: Or, The True and Incredible Adventures of the Spice Trader Who Changed the Course of History (London: Penguin Books, 2000).


9. Srivastava, "Corporate Sovereign Awakening and the Making of Modern State Sovereignty."


10. Political scientist Abraham Newman has written extensively about European governance of tech issues. See Abraham L. Newman and David Bach, "Self-Regulatory Trajectories in the Shadow of Public Power: Resolving Digital Dilemmas in Europe and the United States," Governance 17, no. 3 (2004): 387-413; Abraham L Newman, "Building Transnational Civil Liberties: Transgovernmental Entrepreneurs and the European Data Privacy Directive," International Organization 62, no. 1 (2008): 103-130; Henry Farrell and Abraham L. Newman, Of Privacy and Power: The Transatlantic Struggle over Freedom and Security, of Privacy and Power (Princeton, NJ: Princeton University Press, 2019).


11. Amy Webb, The Big Nine: How the Tech Titans and Their Thinking Machines Could Warp Humanity (New York: PublicAffairs, 2019).


12. For a brilliant and readable analysis, see Nikos Smyrnaios, Internet Oligopoly: The Corporate Takeover of Our Digital World (Bingley, UK: Emerald Publishing, 2018).


13. The classic study of how apartheid ended is Audie Klotz, Norms in International Relations: The Struggle against Apartheid (Ithaca, NY: Cornell University Press, 1995).


14. Tim Bartley and Curtis Child, "Shaming the Corporation: The Social Production of Targets and the Anti-Sweatshop Movement," American Sociological Review 79, no. 4 (2014): 653-679.


15. Steven Levy, Facebook: The Inside Story (New York: Blue Rider Press, 2020), 186-189.


16. Swati Srivastava "Assessing Global Regulatory Responses to Facebook's Political Harms" (paper presented at American Political Science Association meeting, 2022).


17. Nicholas Kristof, "Opinion: The Children of Pornhub," New York Times, December 4, 2020, https://www.nytimes.com/2020/12/04/opinion/sunday/pornhub-rape-trafficking.html.


18. Christopher Reynolds, "Pornhub Policies Reveal Legal Gaps and Lack of Enforcement around Exploitive Videos," Montreal, February 28, 2021, https://montreal.ctvnews.ca/pornhub-policies-reveal-legal-gaps-and-lack-of-enforcement-around-exploitive-videos-1.5327610.


19. Alex Kingsbury, "Opinion: We're About to Find Out What Happens When Privacy Is All But Gone," New York Times, August 23, 2022, https://www.nytimes.com/2022/08/23/opinion/apple-internet-privacy-tracking.html.


20. Karina Vold and Jess Whittlestone, "Privacy, Autonomy, and Personalised Targeting: Rethinking How Personal Data Is Used" (Cambridge: University of Cambridge, 2019), https://philpapers.org/rec/VOLPAA-2.


21. Mardav Jain, "The Aadhaar Card: Cybersecurity Issues with India's Biometric Experiment," Henry M. Jackson School of International Studies (blog), May 9, 2019, https://jsis.washington.edu/news/the-aadhaar-card-cybersecurity-issues-with-indias-biometric-experiment/; Srijoni Sen, "A Decade of Aadhaar: Lessons in Implementing a Foundational ID System," ORF issue brief 292 (Observer Research Foundation, May 2019).


22. Karthik Muralidharan, Paul Niehaus, and Sandip Sukhtankar, "Integrating Biometric Authentication in India's Welfare Programs: Lessons from a Decade of Reforms," India Policy Forum (2021): 140.


23. Jain, "The Aadhaar Card."


24. Sen, "A Decade of Aadhaar."


25. UNDP, "The SDGS in Action," n.d., https://www.undp.org/sustainable-development-goals.


26. Isha Pali et al., "A Comprehensive Survey of Aadhar and Security Issues," arXiv:2007.09409 [Cs], July 18, 2020.


27. "ID Systems Analysed: Aadhaar," Privacy International, November 19, 2021, http://privacyinternational.org/case-study/4698/id-systems-analysed-aadhaar.


28. Pali et al., "A Comprehensive Survey of Aadhar and Security Issues."


29. Jain, "The Aadhaar Card."


30. "Aadhaar Security Fail," Privacy International, accessed September 1, 2022, https://privacyinternational.org/aadhaarsecurityfails; see also Jain, "The Aadhaar Card."


31. Anecdote recalled in Jain, "The Aadhaar Card."


32. Krishnadas Rajagopal, "Five-Judge Supreme Court Bench to Review Verdict Upholding Aadhaar on January 11," Hindu, January 10, 2021, https://www.thehindu.com/news/national/five-judge-supreme-court-bench-to-review-verdict-upholding-aadhaar-on-january-11/article33541810.ece.


33. "Initial Analysis of Indian Supreme Court Decision on Aadhaar," Privacy International, September 26, 2018, http://privacyinternational.org/long-read/2299/initial-analysis-indian-supreme-court-decision-aadhaar.


34. V. K. Anirudh, "How Aadhaar's Database Can Be Used to Train a Surveillance AI for the Indian Government," Analytics India, January 21, 2019, https://analyticsindiamag.com/how-aadhaars-database-can-be-used-to-train-a-surveillance-ai-for-the-indian-government/.


35. "Initial Analysis of Indian Supreme Court Decision on Aadhaar." For an updated global survey, see "Digital National ID Systems: Ways, Shapes and Forms," Privacy International, accessed September 1, 2022, http://privacyinternational.org/long-read/4656/digital-national-id-systems-ways-shapes-and-forms.


36. Muralidharan et al., "Integrating Biometric Authentication in India's Welfare Programs."


37. See more generally Schulz and Raman, The Coming Good Society, chapter 3.


38. The well-known work on this is Tim Wu, The Master Switch: The Rise and Fall of Information Empires (New York: Knopf, 2010). For a specific analysis of the Internet, see Milton L. Mueller, Ruling the Root: Internet Governance and the Taming of Cyberspace (Cambridge, MA: MIT Press, 2009), and Milton L. Mueller and Ernest J. Wilson III, Networks and States: The Global Politics of Internet Governance (Cambridge, MA: MIT Press, 2010).


39. Together, the DSA and DMA are referred to as the Digital Services Act Package. The landing page for basic information is here, "The Digital Services Act Package," European Commission, https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package, accessed February 6, 2023.


40. "Europe Agrees New Law to Curb Big Tech Dominance," BBC News, March 25, 2022, https://www.bbc.com/news/technology-60870287; Colin Wall and Eugenia Lostri, "The European Union's Digital Markets Act: A Primer," CSIS, February 8, 2022, https://www.csis.org/analysis/european-unions-digital-markets-act-primer.


41. William Korey, NGOs and the Universal Declaration of Human Rights: A Curious Grapevine (New York: Palgrave Macmillan, 1998); Jackie Smith, Ron Pagnucco, and George A. Lopez, "Globalizing Human Rights: The Work of Transnational Human Rights NGOs in the 1990s," Human Rights Quarterly 20, no. 2 (1998): 379-412; Kenneth Cmiel, "The Recent History of Human Rights," American Historical Review 109, no. 1 (February 1, 2004): 117-135.


42. Dauvergne, Identified, Tracked, and Profiled, offers a comprehensive account for how activists are changing the politics of FRT.


43. "Algorithmic Justice League—Unmasking AI Harms and Biases," accessed September 1, 2022, https://www.ajl.org/.


44. "The 2022 Ranking Digital Rights Big Tech Scorecard," Ranking Digital Rights, accessed September 1, 2022, https://rankingdigitalrights.org/index2022.


45. Eric Null, Isedua Oribhabor, and Willmary Escoto, "Data Minimization: Key to Protecting Privacy and Reducing Harm" (Access Now, May 2021), https://www.accessnow.org/cms/assets/uploads/2021/05/Data-Minimization-Report.pdf.


46. Margaret E. Keck and Kathryn Sikkink, Activists beyond Borders: Advocacy Networks in International Politics (Ithaca, NY: Cornell University Press, 1998); Adam Hochschild, King Leopold's Ghost: A Story of Greed, Terror, and Heroism in Colonial Africa (New York: Mariner Books, 1999); Ann Marie Clark, Diplomacy of Conscience: Amnesty International and Changing Human Rights Norms (Princeton, NJ: Princeton University Press, 2001); Wendy H. Wong, Internal Affairs: How the Structure of NGOs Transforms Human Rights (Ithaca, NY: Cornell University Press, 2012).


47. Bernard Marr, "How Much Data Do We Create Every Day? The Mind-Blowing Stats Everyone Should Read," Forbes, May 21, 2018, https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/; Jeff Desjardins, "How Much Data Is Generated Each Day?," World Economic Forum, April 17, 2019, https://www.weforum.org/agenda/2019/04/how-much-data-is-generated-each-day-cf4bddf29f/.


48. Viktor Mayer-Schönberger, Delete: The Virtue of Forgetting in the Digital Age (Princeton, NJ: Princeton University Press, 2009).


49. Jorge Luis Borges, "Funes the Memorious," in Labyrinths, trans. James E. Irby (New York: New Directions, 1962).


50. The story is part of the anthology: Ted Chiang, Exhalation: Stories (New York: Knopf, 2019).


51. Jack Hardinges, "What Is a Data Trust?—The ODI," October 7, 2018, https://theodi.org/article/what-is-a-data-trust/; Jack Hardinges, "Defining a 'Data Trust'—The ODI," The Open Data Institute (blog), 2018, https://theodi.org/article/defining-a-data-trust/; Jack Hardinges, "Data Trusts in 2020—The ODI," Open Data Institute (blog), March 17, 2020, https://theodi.org/article/data-trusts-in-2020/; Jack Hardinges et al., "ODI Report: 'Data Trusts: Lessons from Three Pilots'" (Open Data Institute, April 2019), https://docs.google.com/document/d/118RqyUAWP3WIyyCO4iLUT3oOobnYJGibEhspr2v87jg/edit; Sylvie Delacroix and Neil D Lawrence, "Bottom-Up Data Trusts: Disturbing the 'One Size Fits All' Approach to Data Governance," International Data Privacy Law 9, no. 4 (November 1, 2019): 236-252.


52. Anna Artyushina, "The Future of Data Trusts and the Global Race to Dominate AI," Bennett Institute for Public Policy (blog), June 10, 2021, https://www.bennettinstitute.cam.ac.uk/blog/data-trusts1/.


53. Wendy Hall and Jérôme Pesenti, "Growing the Artificial Intelligence Industry in the UK," October 15, 2017, https://www.gov.uk/government/publications/growing-the-artificial-intelligence-industry-in-the-uk; Delacroix and Lawrence, "Bottom-Up Data Trusts."


54. Hall and Pesenti, "Growing the Artificial Intelligence Industry in the UK."


55. For example, see Anna Artyushina, "The Future of Data Trusts and the Global Race to Dominate AI," Bennett Institute for Public Policy (blog), June 10, 2021; Hall and Pesenti, "Growing the Artificial Intelligence Industry in the UK."


56. For an explanation of the distinction, see Sandra Wachter and Brent Mittelstadt, "A Right to Reasonable Inferences: Re-Thinking Data Protection Law in the Age of Big Data and AI Survey: Privacy, Data, and Business," Columbia Business Law Review 2019, no. 2 (2019): 494-620.


57. Daragh Murray and Pete Fussey, "Bulk Surveillance in the Digital Age: Rethinking the Human Rights Law Approach to Bulk Monitoring of Communications Data," Israel Law Review 52, no. 1 (March 2019): 31-60.


58. This argument is made by Murray and Fussey, "Bulk Surveillance in the Digital Age."


59. Diksha Dutta, "Why We Need Data Unions to Support the Data Economy," Medium, April 30, 2021, https://blog.oceanprotocol.com/voices-of-the-data-economy-shiv-malik-data-unions-84243ab965ee; Stanford HAI, Divya Siddarth: Data Cooperatives Could Give Us More Power over Our Data, 2021, https://www.youtube.com/watch?v=7r-X8ZtceQY; Alex Pentland and Thomas Hardjono, "Data Cooperatives," in Building the New Economy, ed. Alex Pentland, Alexander Lipton, and Thomas Hardjono (Cambridge, MA: MIT Press, 2020), https://wip.mitpress.mit.edu/pub/pnxgvubq/release/2; "Data Dividend Project," accessed February 18, 2022, https://datadividendproject.com.


60. Hampton Smith, "Blockchain as a Platform for Data Governance 2.0," Data School, August 9, 2021, https://dataschool.com/data-conversations/blockchain-as-a-platform-for-data-governance-2-0/.


61. Fleur Johns, "Global Governance through the Pairing of List and Algorithm," Environment and Planning D: Society and Space 34, no. 1 (February 1, 2016): 126-149.


62. Bernard Marr, "The Best Examples Of DAOs Everyone Should Know About," Forbes, May 25, 2022, https://www.forbes.com/sites/bernardmarr/2022/05/25/the-best-examples-of-daos-everyone-should-know-about/.


63. Nathan Schneider, "How We Can Encode Human Rights in the Blockchain," Noema, June 7, 2022, https://www.noemamag.com/how-we-can-encode-human-rights-in-the-blockchain.


64. This fantastic piece articulates some of the problems with the bias toward automation: Divya Siddarth and Kelsie Nabben, "What Tech Futurists Get Wrong about Human Autonomy," NOEMA, December 9, 2021, https://www.noemamag.com/ai-blockchain-human-autonomy-future.


65. One example: Gulshan Kumar, Rahul Saha, Chhagan Lal, and Mauro Conti, "Internet-of-Forensic (IoF): A Blockchain-Based Digital Forensics Framework for IoT Applications," Future Generation Computer Systems 120 (July 2021): 13-25.


66. Estelle Massé, "A New EU Law Will Save the GDPR," Access Now (blog), July 12, 2022, https://www.accessnow.org/save-the-gdpr/.


67. Anna Jobin, Marcello Ienca, and Effy Vayena, "The Global Landscape of AI Ethics Guidelines," Nature Machine Intelligence 1, no. 9 (2019): 389-399; Graeme Auld et al., "Governing AI through Ethical Standards: Learning from the Experiences of Other Private Governance Initiatives," Journal of European Public Policy 29 (August 22, 2022): 1-23.


68. On the dependence of states on Big Tech as a governance question in particular, see Srivastava, "Corporate Sovereign Awakening and the Making of Modern State Sovereignty."


69. Leonora Chu, "Europe Criticizes Trump Twitter Ban—But Not for Reason You'd Expect," Christian Science Monitor, January 15, 2021, https://www.csmonitor.com/World/Europe/2021/0115/Europe-criticizes-Trump-Twitter-ban-but-not-for-reason-you-d-expect.


70. Henry Farrell, Abraham Newman, and Jeremy Wallace, "How AI Distorts Decision-Making and Makes Dictators More Dangerous," Foreign Affairs (October 2022): 18.


71. For an example, see Allan Dafoe, "AI Governance: A Research Agenda," Governance of AI Program, Future of Humanity Institute (Oxford: University of Oxford, August 27, 2018).













Index


Note: Italicized page locators refer to figures; tables are noted with a t.
Aadhaar, 170-171, 172, 174
Access Now, 41, 82, 174
ACLU, 81
Acxiom, 27, 34
African Charter on Human and Peoples' Rights, 199n70
AI. See Artificial intelligence (AI)
AI gay face study, 85-86
AI Now, 81
Aldaco, Rafaela, 15-16, 32, 193n54 193nn55
Algorithmic Justice League, 80, 173
Algorithms, 58, 59, 60, 68, 97, 98, 150, 180, 200n75
Alibaba, 5, 113
"All affected interests" concept, 189n19
Alphabet, 116, 122, 123t, 136
Amazon, 1, 5, 35, 51, 55, 114, 122, 123t, 136
Amazon Rekognition, 80-81
American Library Association (ALA), 156, 157
Amnesty International, 40, 81, 128, 220n96, 241n92
Ancestry.com, 52
Andreesen, Marc, 129
Animal rights, 164-165, 254n5
Anthony, Carmelo, digital twin version of, 164
Anti-Asian racism, 62-63, 211n76, 211n77, 211n78
Apple, 5, 35, 51, 55, 120, 122, 123t, 136, 168, 173
Apple Watch, 23
Arató, András, 72, 73, 214n36
Artificial intelligence (AI), 2, 5, 17, 22, 51, 165, 180, 181, 188n14
COVID-19 pandemic and failure of, 32, 197n46
human rights legal framework and, 37, 200n74
Atas, Nadire, 33
Authenticity, digital immortality and, 99
Authority, coercion vs., 118-119
Autonomy, 3, 7, 10, 13, 14, 17, 18, 35, 36, 40, 42, 61, 75, 79, 165, 166, 169, 180
Big Tech and, 116, 121
blockchain and, 179
data co-creation and, 38, 39
data linkage and, 33
data literacy and, 147
digital immortality and, 90, 91, 92, 95, 96, 97, 99, 104, 105, 108
DNA data and, 53
facial recognition technologies and, 68, 73, 86, 87, 88
privacy and, 76
protecting collectively, 173, 174
regulation of datafication and, 181
right to die and, 101
right to education and, 148
Autonomy (cont.)
stickiness of data and, 29
transgressions of, 12
Avant, Deborah, 119
Avatars, celebrity, 164
Axios, 62
Babcock, Guy, 33
Background reports, automated, 15-16, 193n54
Baidu, 5, 113
B corporations, rise of, 191n36
Belsky, Marcia, 131
Benjamin, Ruha, 13
Berlin, Isaiah, 92
Bhargava, Rahul, 152
Biden, Joe, 122
Big Data, 3, 5, 24, 32, 35, 48, 50-51, 55, 60, 165
Big Tech, 6, 18, 35, 36, 93, 111-137, 159, 160, 166, 173, 179
corporate datafication and, 112-114
definition of, 5
as global tech governors, 133-137
governance capacities of, 114, 118-122, 167, 168, 236n38
politics of platforms and, 128-130
power of, 116-118
what makes it "big?," 122-124
BINA48, 93, 104
Bing, 120
Biometrics, 54, 170
Bledsoe, Woody, 69, 212n12
Blink, 1
Blockchain, 178-179
Borges, Jorge Luis, 175
Bostrom, Nick, 43
Bots, digital immortality and, 91, 92, 93, 94-95, 96, 102-103, 105, 226n15
Brandeis, Justice Louis, 72, 75
Brexit referendum, social media power and, 122
British East India Company (BEIC), 167, 168, 254n8
Brooklyn Public Library, 155
Brotherhood, in Cassin's human rights "portico," 3, 4. See also Community
Brown, Arielle G., 84
Brud, acquired by Dapper Labs, 163, 254n1
Buolamwini, Joy, 65, 80, 173
Burnham, Bo, 6
Bush, Billy, 63
Business and Human Rights Resource Centre, 40
Bye, Kent, 127
Cahn, Albert Fox, 61
California Consumer Privacy Act (CCPA), 54, 60, 210n69
Cambridge Analytica scandal, 96, 234n22
Canada, disinformation concerns in, 151
Canadian Broadcasting Corporation, Internet of Everything series, 26
"Cancel culture," 37
Carnegie, Andrew, libraries and legacy of, 154-155, 251n66, 251n67
Cassin, René, human rights "portico" of, 3, 4
Cheaterbot, 33
Cheney-Lippold, John, 13
Chiang, Ted, 175, 176
Chicago Public Library, 155
China, Personal Information Protection Law (PIPL) in, 60, 108, 210n69
CIA, 69
Circle, The (Eggers), 167
Citizen/subject distinction, 188-189n17
Civil society proactivity, need for, 174
Class, stickiness of data and, 16
Cleveland, Frances (Folsom), 71
Cleveland, Grover, 71
Cloud computing services, 42
Co-creation, co-production vs., 196n33
Coercion, authority vs., 118-119
Cohn-Kanade data sets, 77
Collective action problems, 7-8, 59, 190n27
Committee on Economic, Social, and Cultural Rights, 148, 249n40
Communications Decency Act, section 230 of, 129-130
Community, 3, 7, 10, 13, 14, 35, 36, 42, 165, 166, 180
blockchain and, 179
data literacy and, 147
data rights and, 64
libraries and, 155-156, 251n72
protecting collectively, 173, 174
regulation of datafication and, 181
stakeholders and, 17
stickiness of data and, 16-17
transgressions of, 12
Complaint sites, 33
Consent, 78, 95, 96, 103
Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment, 100
Convention on the Elimination of All Forms of Discrimination against Women, 39
Conversation turns per session (CPS) measures, 98
Cookies, 49
Copyright law, protections for photography, 71
Corporate datafication, 40, 112-114
Council of Europe, 36, 199n70
COVID-19 pandemic, 41, 124-125
AI failures during, 32, 197n46
Exposure Notifications, 120, 237n49
online proctoring during, 83, 84
Ring video doorbell sales during, 1
role of libraries during, 158, 252n85
volume of data generated during, 23
Crawford, Kate, 83
Credit reporting bureaus, large-scale hacks of, 96, 227n24
Credit-worthiness, algorithms and evaluation of, 37
Criminal investigations, DNA data and, 52-53, 206n27
Cryonics, 107, 231n61
Cunche, Mathieu, 26
"Customer as product" phrase, history of, 2, 187n7
Dadbot, 101-102, 105
Data. See also Stickiness of data
co-creation of, 29, 30, 38, 47, 57, 61-62, 63, 64, 92, 169, 196n33
collective characteristic of, 54-56
global governance of, 18
immortality of, 29, 34, 39, 63, 89-109
labeling, 50, 204-205n15
linkage of, 29, 31-34
mundanity of, 29, 30-31
personal, 25-29, 54
social nature of, 24-25
stickiness of, 14, 29-34
ubiquity of, 1-19, 22-24, 27, 153
Data brokers, 2
Data citizens, 5
Data collectors, 5, 8, 19, 39, 42
Data colonialism, 188n17
Data Culture Project, 152, 153
Datafication, 13, 17, 25, 49, 50, 150, 161. See also Data literacy
altered human experience and, 22
changed collective life and, 164
corporate, 40, 112-114
defining, 3
digital immortality and, 93
explicitly political view of, 7-8
Datafication (cont.)
human rights and, 3-7, 22, 37-40
libraries and, 156
relational Big Data and, 55
two-fold private nature of, 15
Data harms, vs. other types of harms, 79-80
"Dataism," 49
Data literacy, 7, 19, 139-161, 166, 180, 181
civil society, education, and, 152-153
defining, 144, 147
demands of a literate stakeholdership and, 158-161
driving analogy and, 143-144, 145
importance of, 147-148, 151
lack of, in a datafied world, 141-146
libraries and, 145, 154-158
locating among human rights, 148
Madeup family vignette, 139-140
political persons and, 150-151
right to, 5
Data minimization, as a human right, 174
Data ownership, 56-58, 63, 64, 208n52
Data rights, 18, 45-64
Big Data and, 50-51
co-creation and, 47, 57, 61-62, 63, 64
datafication and, 49-50
genetic data issues and, 51-55
human rights problems and, 56-58
impediments to, 47
mismatched regulation and, 59-61
"right to be forgotten" and, 45-46, 47, 62-64
Data subjects, 5
Data trusts, 62, 176-177, 258n51
Data unions, 178, 259n59
DeAngelo, Joseph James, 52, 53
Death penalty, 100
Death with dignity, 100
Deep learning, 43
Deibert, Ron, 13
Delete (Mayer-Schönberger), 175
Deliberative democracy theory, 189n19
DeNardis, Laura, 3, 114
Digital afterlife industry, 93, 108
"Digital bodies," 201n82
Digital human rights, 37, 200n78
Digital ID systems, misgivings about, 171-172
Digital immortality, 18, 89-109
equity and access to, 104
4evru scenario, 93-95, 97-98, 104
HereAfter AI and, 93, 101, 102-103, 104, 106
holograms, 89, 108
human rights and, 91, 95-100, 104-105, 108
Madeup family vignette, 89-90
Digital literacy, university libraries and, 157
Digital Markets Act (DMA), 159, 172, 173, 257n39
Digital profiles, life after death and, 91
Digital remains, managing, 90
Digital Services Act (DSA), 172, 173, 257n39
D'Ignazio, Catherine, 152
Dignity, 10, 13, 14, 17, 18, 35, 36, 40, 42, 47, 61, 75, 79, 165, 166, 169, 180
Big Tech and, 116, 121
blockchain and, 179
in Cassin's human rights "portico," 3, 4, 7
data co-creation and, 38, 39
data linkage and, 33
data literacy and, 147
data rights and, 64
digital immortality and, 90, 91, 95, 105, 108
DNA data and, 53
facial recognition technologies and, 68, 73, 74, 86, 87, 88
privacy and, 76
protecting collectively, 173, 174
regulation of datafication and, 181
right to die and, 101
right to education and, 148
stickiness of data and, 16, 29
transgressions of, 12
Discrimination, facial recognition technologies and, 66, 67, 68, 69, 80-81, 83, 84, 86, 169
Disease surveillance, Big Data and, 32, 197n45
Distributed Artificial Intelligence Research Institute (DAIR), 124
DNA data
crime busting with, 52-53, 206n27
rights to, 51-52
Dobbs v. Jackson Women's Health Organization, 61
Doe v. Meta Platforms, 241n91
Donnelly, Jack, 13
Dorsey, Jack, 141
Dror- Shpoliansky, Dafna, 37
Drug development, DNA data and, 51-52, 205n23, 205n25
DuckDuckGo, 120
Dutch East India Company (DEIC), 167, 168, 254n8
Eastman, George, 71
Easy (Netflix show), 1
Education
civil society and, 152-153
right to, 148
Eggers, Dave, 167
Elections, social media power and legitimacy of, 122
Electronic Frontier Foundation, 81
Electronic Privacy Information Center, 82
Encryption, 62
Equality, 10, 13, 14, 17, 35, 36, 42, 75, 165, 166, 180
blockchain and, 179
in Cassin's human rights "portico," 3, 4, 7
data literacy and, 147
data rights and, 64
protecting collectively, 173
regulation of datafication and, 181
stickiness of data and, 16, 29
transgressions of, 12
Equifax hack, 96, 227n24
Estonia, disinformation concerns in, 151, 250n54
ETER9, 93
Ethics courses, 159-160, 253n93, 253n94
Ethiopia, 116
Eubanks, Virginia, 9
Europe, governance of tech issues in, 168, 254-255n10, 257n39, 257n40
European Convention on Human Rights, 73
Euthanasia, 100, 101
Every, The (Eggers), 167
Exabytes, 23
Experian hack, 96
FAANG (Facebook, Amazon, Apple, Netflix, and Google), 233n13
Facebook, 18, 27, 35, 41, 61, 85, 111, 117, 118, 121, 122, 123, 125, 129, 135, 144. See also Meta
Beacon app, 168, 169
Community Standards, 38, 131, 132, 133
DeepFace training, 69, 212n10
Discover program, 126
Find Friends program, 55
Free Basics program, 113, 121, 124-128
shadow profiles, 56, 208n49
Facebook Blue, 111
Facial Recognition Act, 87, 224n133
Facial Recognition and Biometric Technology Moratorium Act, 87
Facial recognition technologies (FRT), 18, 27, 65-88, 170, 173, 174
AI gayface study, 85-86
"chilling effect" of, 84-85
contextualizing, 69-70
data-based harms and, 79-80
facial data as property, 74-75
human rights and criticism of, 67-69, 83, 84, 87, 88
Madeup family vignette, 65-66, 87
privacy issues and, 72, 73, 75-79, 87-88
remote proctoring services and, 83-85
risks of, 80
safe use of, exploring, 86-87
ubiquity of, 70
FamilyTreeDNA, 52, 53
Federal Election Commission, 122
Feldman, Noah, 130-131
Fiesler, Casey, 159
Fight for the Future, 82
Finnemore, Martha, 119
First Amendment, 131
Fitbit, 25-27, 41, 48
Flickr, 87
Forgetting, human history and, 175
Foursquare, 55
Fraley v. Facebook, Inc., 72
Freedom of expression, 35, 116-117, 121, 122, 131, 133, 135, 142, 158, 159, 169, 179, 180
Frenkel, Sheera, 117
Fried, Genevieve, 77
FRT. See Facial recognition technologies (FRT)
"Funes the Memorius" (Borges), 175
GAMAM (Google, Amazon, Meta, Apple, and Microsoft), 113, 121, 122, 123, 123t, 129, 135, 136, 137
Gap, anti-sweatshop campaigners and, 168
Garvie, Clare, 80
Gates, Kelly A., 88
Gaylor, Brett, 26
Gebru, Timnit, 80, 83, 124
GEDMatch, 52, 53
Gender Shades (Buolamwini & Gebru), 80
General Data Protection Regulation (GDPR) (EU), 38, 49, 60, 62, 108, 180, 210n69, 215nn41
Genetic genealogy, investigative crime and, 52, 53
Genocide, 47, 100, 204n8
Gigabytes, 23, 194n7
GlaxoSmithKline, 51
Global governors, 119
Global Network Initiative (GNI), 41, 180
Global norms, power of, 8
Global Voices, 125, 126
Gmail, 55
Golden State Killer case, DNA as crime buster in, 52-53, 206n27
González, Mario Costeja, 45-46
Gonzalez v. Google, 130
Google, 2, 5, 26, 27, 35, 51, 95, 116, 120, 122, 123, 124, 136, 168, 173
Google Docs, 55
Google Nest doorbell and camera system, 1, 189n25
Google Spain v. AEPD and Mario Costeja González, 45
Green, Jessica, 119
Grindr, 55
Grossman, Lev, 127
Habeas corpus rights, 169
Haelan Laboratories v. Topps Chewing Gum, 214n33
Hard power, 79
Hartzog, Woodrow, 84
Harvey, Del, 130
Haugen, Frances, 37-38, 111, 117, 127, 136, 234n21, 235n33, 240n85
Hayden, Michael, 31
Hayes, Jackson, 84
HereAfter AI, 93, 101, 102, 103, 104, 106
Hill, Kashmir, 55
Holograms, 89, 108, 225n3, 230n54
Horton, Diane, 160
Hotmail, 55
Human Compatible (Russell), 43
Human rights. See also Autonomy; Community; Dignity; Equality
blockchain and, 179
challenges lying before us, 179-181
digital immortality and, 91, 95-100, 104-105, 108
ethics of the library and, 156-157
evolving dataverse and, 43
extreme violations of, 168-169
fixing datafication and, 3-7
four foundational values of, 3, 4, 4, 13, 14, 36
function of, 35-37
global application of, 131
locating data literacy among, 147, 148
Meta's Oversight Board decisions and, 133, 134t
need for, 12-14
stickiness of data and, 14-17, 30, 35, 37
virtual people and, 164-165
Human Rights Watch, 41
Human rights writing, scope of, 198-199n65
Human trafficking, fighting with facial recognition technologies, 82, 222n110
IBM, 81, 220n94
Identifiability, 28, 47, 59-61, 60, 196n31
India, Free Basics banned in, 125
Inside, 6
Instagram, 38, 111, 112, 115, 117, 118, 126, 129, 132, 135
Internal Affairs (Wong), 153
International Convention for the Protection of All Persons from Enforced Disappearance, 100
International Court of Justice, 100
International Covenant on Economic, Social, and Cultural Rights (ICESCR), 148, 249n40
Internet Freedom Foundation, 81-82
Internet in Everything, The (DeNardis), 3
Internet-of-Forensics, 179
Internet of Things, 102, 187n5
Internet.org, 125
"Is Connectivity a Human Right?" (Zuckerberg), 125
Johnson, Samantha, 16, 32, 193n54
Kang, Cecilia, 117
Kardashian, Kim, 89, 225n3
Kardashian, Robert, 92, 95
hologram of, 89, 225n3
Kaye, David, 121
Kelly, Cathal, 30
King-Hurley Research Group, 69
Klassen, Jon, 156
Klinenberg, Eric, 154
Kodak Brownie camera, amateur photography and, 71
Kosinki, Michal, 85
Kristof, Nicholas, 169
Krolik, Aaron, 33
Kyuda, Eugenia, 91, 92
Landlords, automated background reports and, 15-16, 193n54
Lanier, Jaron, 13
La Vanguardia, 45
Law enforcement
facial recognition technology use and, 66, 69, 82, 212n3
genealogical data use and, 52-54, 206n29, 206n30
partnerships with Ring, 14-15, 187n2
League of Nations, 148
Ledger of Harms (Center for Humane Technology), 153
Lessig, Lawrence, 129
Levy, Karen E. C., 55
Levy, Steven, 133, 134
Liberty, 81
Liberty, in Cassin's human rights "portico," 3, 4
Libraries
Carnegie and building of, 154-155
ethics of, 156-157
as infrastructure, 157-158
literacy and, 154-158
Linguistic literacy, libraries and, 145
LinkedIn, 55, 121
Literacy. See also Data literacy
defining, 146
libraries and, 154-158
lifelong pathway to, 149
"Logic of consequences," "logic of appropriateness" vs., 190n28
Machine-selected groups, 47, 58, 59
MacKinnon, Rebecca, 6
Mantelero, Alessandro, 59
Market inalienability, 75
Martin Luther King Jr. Library (Washington, DC), 155
Mattern, Shannon, 154, 157
Mayer-Schönberger, Viktor, 39, 175
Mazurenko, Roman, 91, 92
McCammond, Alexi, 62-63, 211n76
McIlraith, Sheila, 160
Medical data, 47-48
Medicine, data linkage in, 32
MegaFace, 87
Merkel, Angela, 129
Messenger, 111, 126
Meta, 2, 5, 18, 35, 38, 51, 55, 56, 61, 81, 104, 111, 116, 122, 123t, 168. See also Facebook
as everyday infrastructure, ascension of, 112, 113, 118, 124, 126, 127, 128
Free Basics, 124, 125-126, 128
Madeup family vignette, 115-116
outage in 2021, 112-113
Oversight Board of, 18, 114, 117, 121, 122, 128, 130-133, 134t, 135, 137, 233-234n19
platform governance and, 129
Meta AI, Casual Conversations, 78
Metadata, 31, 177
Metaverse, 114
Microsoft, 5, 35, 51, 81, 93, 95, 103, 122, 123t, 136
Mims, Christopher, 114
MindGeek, 168
Mint, 55
Monetization of facial images, 72-73
Mori, Masahiro, 103
mSpy, 55
Musk, Elon, Twitter takeover by, 141-142, 245n5
Myanmar, 113, 116, 127
MyHeritage, 34, 52
NCAA v. Alston, 215n45
Negative liberty, 92
Net neutrality debate, Free Basics and, 125, 126
NetzDG (Germany), 143, 247n20
New York Police Department, 81
New York Public Library, 155
NGOs. See Nongovernmental organizations (NGOs)
Nicklaus, Jack, digital twin version of, 164
Nike, anti-sweatshop campaigners and, 168
Noble, Safiya Umoja, 9
Nongovernmental organizations (NGOs), 7, 40-41, 136, 137, 151, 152, 153, 173, 174
Norms, 8, 63
Nothias, Toussaint, 124
Nye, Joseph, 79
O'Bannon, Ed, 74
O'Bannon v. NCAA, 74
Öhman, Carl, 97
O'Neal, Shaquille, 1
O'Neil, Cathy, 13
1000 Genomes Project, 54
One-way immortality, 93
"Online persons," need for rights for, 37
Oppression, facial recognition as tool of, 84, 223n120. See also Discrimination
Organization of American States, 36, 200n72
Ovia, 56
Ownership, data co-creation issues and, 38
Panoramic Research Incorporated, 69, 70
Pegasus Spyware, 42, 202-203n102
Pepper's ghost, 89, 225n4
Personal data, 25-29, 54
Personal Information Protection and Electronic Documents Act (PIPEDA), 73, 77, 217n65
Personicx, 27, 34
Peterson, Anne Helen, 158
Photography, ownership of facial images and, 67, 70-73
Pinterest, 129
Platform governance, 41-42
Platforms, politics of, 128-130
Poland, LGBT-free zones in, 86
Polaroid, anti-apartheid boycott and, 168
Police departments, Ring partnerships with, 14-15, 187n2. See also Law enforcement
Pornhub, 136, 168
Porter, Theodore, 49
Power, definition of, 118
Pregnancy apps, 56
Printing press, literacy and, 144
Privacy, 169, 170
facial data and, 77-78
judgments about, 75-77
world of human rights beyond, 172
Privacy International, 171
Private authority, 119
Proctoring services, remote, 83-85
Quantification, 49, 50
Quantified self, wearable technology and, 25-27, 195n22
Race, stickiness of data and, 16
Racial bias, in FRT data sets, 80-81, 87, 219n85
Racism, 169
Radin, Jane, 75
Rafaela Aldalco v. RentGrow, Inc., 193n54, 193n55
Raji, Deborah, 77
Ranking Digital Rights project (New America), 174
Reddit, 121, 130
Refugee status, AI and eligibility for, 37, 200n76
Reklaim (formerly Killi), 57
RentGrow, 15, 16
Replika, 90
Reputational harm, data linkage and, 33, 197n47
"Reputation managers" industry, 33
Research, GAMAM and influence on, 123, 238n57
RightsCon, 41
Rights of the dead, 99
Right to die, 101
Ring, 14
law enforcement partnerships with, 14-15, 187n2
Neighbors App, 11, 15
Protect plans, 192n51
Ring video doorbells, 41
convenience-driven sales of, 1
Madeup family vignette, 10-12, 21, 48-49
Ripoff Report, 33
Roberson, Abigail, 72
Robots, "uncanny valley" and, 103
Roe v. Wade, 61
Rohingya, Meta and right to remedy for, 128, 241n92
Rousseau, Jean-Jacques, 188n17
Ruggie, John, 6
Russell, Stuart, 43
Russia, Ukraine invaded by, 133-134
Ryan, Pam, 157, 253n86
Sadowski, Jathan, 57
Sandberg, Sheryl, 125, 131
Sanders, Bernie, 129
Saudi Aramco, 122
Schneider, Nathan, 178, 179
Seattle Public Library, 155
Selinger, Evan, 84
Sell, Susan, 119
Sexual orientation, AI gay face study, 85-86
Shany, Yuval, 37
Shark Tank (reality show), 1
Sharma, R. S., 171
She's a Homewrecker, 33
Siminoff, Jamie, 1
Singularity, 107
Sinha, G. Alex, 78
Slander industry, data linkage and, 33
Slavery, 174
Smart city, 81
Smart devices, Internet of Things and, 102
Smith, Zadie, 154, 156
Smyrnaios, Nikos, 135
Snap, 121
Social Dilemma, The, 38
Social media, 55, 72
freedom of expression and, 116-117, 121, 122, 131, 133, 135
harms of, 38, 153
Soft power, 79
Solove, Daniel, 76
Soul Machines, 95, 163, 164
Sousa, Miquela, 163, 164
Spanish Data Protection Authority, 45
Srivastava, Swati, 168
Stakeholders
data citizens as, 5
data-literate, 145-146, 152, 158
identifying, 8-10
we, the data sources as, 42-43, 165, 181
in the world, human rights and, 6
Stasi of East Germany, citizen surveillance by, 35
Stickiness of data, 14, 29-34, 62, 64, 88, 140, 150. See also Data literacy
background and credit data, 15-16, 193n54
beyond the state, 40-42
co-creation of data, 29, 30, 196n33
data trusts and, 176-177
digital immortality, 29, 34, 91
linkedness of data, 29, 31-34
mundanity of data, 29, 30-31
Structural power, 120
Superintelligence (Bostrom), 43
Surveillance capitalism, 2, 188n9
Talk to Me (Vlahos), 101
Tay (Microsoft chatbot), debut of, 98, 228n27
Taye, Berhan, 132
Teen Vogue, 62, 211n76
Tencent, 5, 113
Terabytes, 23
TikTok, 41, 119, 129, 144
Toronto Public Library, 157-158, 253n86
Torture, 47, 48, 68, 99, 100, 146, 174, 204n8
"Town square" metaphor, Musk's Twitter takeover and, 141-143, 245-246n8
Trolling, 37, 48-49
Trudeau, Justin, 63
Trump, Donald, 63, 129, 133, 181
"Truth of Fact, Truth of Feeling" (Chiang), 175-176
Tufecki, Zeynep, 34
23andMe, 51-52, 205n23, 206n29
Twitch, 121
Twitter, 38, 98, 121, 123, 129, 135, 181
Musk's takeover of, 141-142, 245n5
Twitter v. Taamneh, 130
"Two Concepts of Liberty" (Berlin), 92
Tyner, Kathleen, 149
UDHR. See Universal Declaration of Human Rights (UDHR)
Uganda, Sexual Offenses Bill in, 86
Ukraine, Russia's invasion of, 133-134
Uncanny valley, 103, 164
Uneeq, 95
UNESCO, literacy as defined by, 146
Unique Identification Authority of India (UIDAI), 170, 171
United Nations (UN)
Committee on Human Rights, 3
General Assembly, 59
Global Compact, 40
Guiding Principles on Business and Human Rights, 40, 112, 232n6
Privacy Policy Group, 59
Sustainable Development Goals, 148, 170
United Nations High Commissioner of Human Rights (UNHCHR), 59
Universal Declaration of Human Rights (UDHR), 3, 36, 75-76, 148, 165, 179, 199n66, 200n77, 248-249n40
University of Toronto, Citizen Lab, 42
US Department of Defense, face recognition technology (FERET) program, 69, 77
US Supreme Court, 61, 130
Vaidhyanathan, Siva, 139, 142, 187-188n8
van Dijck, Jose, 49
Very large online platforms (VLOPs), 172
Very large online search engines (VLOSEs), 172
Vhieghe, Joris, 146
Video doorbells, 1, 10-12, 14, 15, 41, 48-49
Virtual influencers, 163-164, 165
Vlahos, James, 101, 102, 103, 105, 230n47
Vold, Karina, 169
Wang, Yilun, 85
Warren, Samuel D., 72, 75
Wearable technology, 25-27, 48, 195n22
Weber, Max, 236n41
WebMd pregnancy app, 56
West, Kanye, 89, 225n3
WhatsApp, 111, 112, 118, 126, 171
Whittlestone, Jess, 169
will.i.am, 56, 57, 164, 208n51
World Food Programme (WFP), 152
World Wide Web, 24
Wu, Tim, 118
Yahoo!, 55, 120
Yelp, 55
Zelensky, Volodymyr, 135
Zombie data, background checks and, 193n54
Zuboff, Shoshana, 24, 46
Zuckerberg, Mark, 35, 111, 114, 124, 125, 131, 132, 137










Contents

Cover
Contents
1 Data Are Everywhere
2 Why Human Rights and Data Go Together
3 Data Rights
4 Is Your Face Yours?
5 Do We Need Human Rights When We're Dead?
6 Big Tech and Us
7 Data Literacy, or Why We Need Libraries, Not Twitter
8 We, the Data
Acknowledgments
Notes
Index




Guide

Cover

Contents

Acknowledgments

Index





i
ii
iii
iv
v
vi
vii
viii
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272





