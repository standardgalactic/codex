





































Daniel C. Dennett

 

INTUITION PUMPS AND OTHER TOOLS FOR THINKING





















ALLEN LANE
Published by the Penguin GroupPenguin Books Ltd, 80 Strand, London WC2R 0RL, EnglandPenguin Group (USA) Inc., 375 Hudson Street, New York, New York 10014, USAPenguin Group (Canada), 90 Eglinton Avenue East, Suite 700, Toronto, Ontario, Canada M4P 2Y3 (a division of Pearson Penguin Canada Inc.)Penguin Ireland, 25 St Stephen's Green, Dublin 2, Ireland (a division of Penguin Books Ltd)Penguin Group (Australia), 707 Collins Street, Melbourne, Victoria 3008, Australia (a division of Pearson Australia Group Pty Ltd)Penguin Books India Pvt Ltd, 11 Community Centre, Panchsheel Park, New Delhi - 110 017, IndiaPenguin Group (NZ), 67 Apollo Drive, Rosedale, Auckland 0632, New Zealand (a division of Pearson New Zealand Ltd)Penguin Books (South Africa) (Pty) Ltd, Block D, Rosebank Office Park, 181 Jan Smuts Avenue, Parktown North, Gauteng 2193, South Africa
Penguin Books Ltd, Registered Offices: 80 Strand, London WC2R 0RL, England
www.penguin.co.uk
First published in the United States of America by W. W. Norton & Company, Inc. 2013First published in Great Britain by Allen Lane 2013
Copyright © Daniel C. Dennett, 2013
The moral right of the author has been asserted
Since this page cannot legibly accommodate all copyright notices, page 461 constitutes an extension of the copyright page.
Cover photograph: John MacLean/Millennium ImagesCover design: Isabelle De Cat
All rights reserved
ISBN: 978-0-141-97012-7











Contents
Preface
I. INTRODUCTION: What Is an Intuition Pump?
II. A DOZEN GENERAL THINKING TOOLS
1. Making Mistakes
2. "By Parody of Reasoning": Using Reductio ad Absurdum
3. Rapoport's Rules
4. Sturgeon's Law
5. Occam's Razor
6. Occam's Broom
7. Using Lay Audiences as Decoys
8. Jootsing
9. Three Species of Goulding: Rathering, Piling On, and the Gould Two-Step
10. The "Surely" Operator: A Mental Block
11. Rhetorical Questions
12. What Is a Deepity?
Summary
III: TOOLS FOR THINKING ABOUT MEANING OR CONTENT
13. Murder in Trafalgar Square
14. An Older Brother Living in Cleveland
15. "Daddy Is a Doctor"
16. Manifest Image and Scientific Image
17. Folk Psychology
18. The Intentional Stance
19. The Personal/Sub-personal Distinction
20. A Cascade of Homunculi
21. The Sorta Operator
22. Wonder Tissue
23. Trapped in the Robot Control Room
IV. AN INTERLUDE ABOUT COMPUTERS
24. The Seven Secrets of Computer Power Revealed
25. Virtual Machines
26. Algorithms
27. Automating the Elevator
Summary
V. MORE TOOLS ABOUT MEANING
28. A Thing about Redheads
29. The Wandering Two-Bitser, Twin Earth, and the Giant Robot
30. Radical Translation and a Quinian Crossword Puzzle
31. Semantic Engines and Syntactic Engines
32. Swampman Meets a Cow-Shark
33. Two Black Boxes
Summary
VI. TOOLS FOR THINKING ABOUT EVOLUTION
34. Universal Acid
35. The Library of Mendel: Vast and Vanishing
36. Genes as Words or as Subroutines
37. The Tree of Life
38. Cranes and Skyhooks, Lifting in Design Space
39. Competence without Comprehension
40. Free-Floating Rationales
41. Do Locusts Understand Prime Numbers?
42. How to Explain Stotting
43. Beware of the Prime Mammal
44. When Does Speciation Occur?
45. Widowmakers, Mitochondrial Eve, and Retrospective Coronations
46. Cycles
47. What Does the Frog's Eye Tell the Frog's Brain?
48. Leaping through Space in the Library of Babel
49. Who Is the Author of Spamlet?
50. Noise in the Virtual Hotel
51. Herb, Alice, and Hal, the Baby
52. Memes
Summary
VII. TOOLS FOR THINKING ABOUT CONSCIOUSNESS
53. Two Counter-images
54. The Zombic Hunch
55. Zombies and Zimboes
56. The Curse of the Cauliflower
57. Vim: How Much Is That in "Real Money"?
58. The Sad Case of Mr. Clapgras
59. The Tuned Deck
60. The Chinese Room
61. The Teleclone Fall from Mars to Earth
62. The Self as the Center of Narrative Gravity
63. Heterophenomenology
64. Mary the Color Scientist: A Boom Crutch Unveiled
Summary
VIII. TOOLS FOR THINKING ABOUT FREE WILL
65. A Truly Nefarious Neurosurgeon
66. A Deterministic Toy: Conway's Game of Life
67. Rock, Paper, and Scissors
68. Two Lotteries
69. Inert Historical Facts
70. A Computer Chess Marathon
71. Ultimate Responsibility
72. Sphexishness
73. The Boys from Brazil: Another Boom Crutch
Summary
IX. WHAT IS IT LIKE TO BE A PHILOSOPHER?
74. A Faustian Bargain
75. Philosophy as Naïve Auto-anthropology
76. Higher-Order Truths of Chmess
77. The 10 Percent That's Good
X. USE THE TOOLS. TRY HARDER.
XI. WHAT GOT LEFT OUT
Appendix: Solutions to Register Machine Problems
Illustrations
Bibliography
Sources
Credits











FOR TUFTS UNIVERSITY, MY ACADEMIC HOME











Preface
Tufts University has been my academic home for more than forty years, and for me it has always seemed to be just right, like Goldilocks's porridge: not too burdened, not too pampered, brilliant colleagues to learn from with a minimum of academic prima donnas, good students serious enough to deserve attention without thinking they are entitled to round-the-clock maintenance, an ivory tower with a deep commitment to solving problems in the real world. Since creating the Center for Cognitive Studies in 1986, Tufts has supported my research, largely sparing me the ordeals and obligations of grantsmanship, and given me remarkable freedom to work with folks in many fields, either traveling afar to workshops, labs, and conferences or bringing visiting scholars and others to the Center. This book shows what I've been up to all these years.
In the spring of 2012, I test-flew a first draft of the chapters in a seminar I offered in the Tufts Philosophy Department. That has been my custom for years, but this time I wanted the students to help me make the book as accessible to the uninitiated as possible, so I excluded graduate students and philosophy majors and limited the class to just a dozen intrepid freshmen, the first twelve—actually thirteen, due to a clerical fumble—who volunteered. We led each other on a rollicking trip through the topics, as they learned that they really could stand up to the professor, and I learned that I really could reach back farther and explain it all better. So here's to my young collaborators, with thanks for their courage, imagination, energy, and enthusiasm: Tom Addison, Nick Boswell, Tony Cannistra, Brendan Fleig-Goldstein, Claire Hirschberg, Caleb Malchik, Carter Palmer, Amar Patel, Kumar Ramanathan, Ariel Rascoe, Nikolai Renedo, Mikko Silliman, and Eric Tondreau.
The second draft that emerged from that seminar was then read by my dear friends Bo Dahlbom, Sue Stafford, and Dale Peterson, who provided me with still further usefully candid appraisals and suggestions, most of which I have followed, and by my editor, Drake McFeely, ably assisted by Brendan Curry, at W. W. Norton, who are also responsible for many improvements, for which I am grateful. Special thanks to Teresa Salvato, program coordinator at the Center for Cognitive Studies, who contributed directly to the entire project in innumerable ways and helped indirectly by managing the Center and my travels so effectively that I could devote more time and energy to making and using my thinking tools.
Finally, as always, thanks and love to my wife, Susan. We've been a team for fifty years, and she is as responsible as I am for what we, together, have done.
DANIEL C. DENNETT
Blue Hill, Maine        
August 2012             














I. Introduction: What Is an Intuition Pump?

 


You can't do much carpentry with your bare hands and you can't do much thinking with your bare brain.
—BO DAHLBOM

Thinking is hard. Thinking about some problems is so hard it can make your head ache just thinking about thinking about them. My colleague the neuropsychologist Marcel Kinsbourne suggests that whenever we find thinking hard, it is because the stony path to truth is competing with seductive, easier paths that turn out to be dead ends. Most of the effort in thinking is a matter of resisting these temptations. We keep getting waylaid and have to steel ourselves for the task at hand. Ugh.
There is a famous story about John von Neumann, the mathematician and physicist who turned Alan Turing's idea (what we now call a Turing machine) into an actual electronic computer (what we now call a Von Neumann machine, such as your laptop or smart phone). Von Neumann was a virtuoso thinker, legendary for his lightning capacity for doing prodigious calculations in his head. According to the story—and like most famous stories, this one has many versions—a colleague approached him one day with a puzzle that had two paths to a solution, a laborious, complicated calculation and an elegant, Aha!-type solution. This colleague had a theory: in such a case, mathematicians work out the laborious solution while the (lazier, but smarter) physicists pause and find the quick-and-easy solution. Which solution would von Neumann find? You know the sort of puzzle: Two trains, 100 miles apart, are approaching each other on the same track, one going 30 miles per hour, the other going 20 miles per hour. A bird flying 120 miles per hour starts at train A (when they are 100 miles apart), flies to train B, turns around and flies back to the approaching train A, and so forth, until the two trains collide. How far has the bird flown when the collision occurs? "Two hundred and forty miles," von Neumann answered almost instantly. "Darn," replied his colleague, "I predicted you'd do it the hard way, summing the infinite series." "Ay!" von Neumann cried in embarrassment, smiting his forehead. "There's an easy way!" (Hint: How long until the trains collide?)
Some people, like von Neumann, are such natural geniuses that they can breeze through the toughest tangles; others are more plodding but are blessed with a heroic supply of "willpower" that helps them stay the course in their dogged pursuit of truth. Then there are the rest of us, not calculating prodigies and a little bit lazy, but still aspiring to understand whatever confronts us. What can we do? We can use thinking tools, by the dozens. These handy prosthetic imagination-extenders and focus-holders permit us to think reliably and even gracefully about really hard questions. This book is a collection of my favorite thinking tools. I will not just describe them; I intend to use them to move your mind gently through uncomfortable territory all the way to a quite radical vision of meaning, mind, and free will. We will begin with some tools that are simple and general, having applications to all sorts of topics. Some of these are familiar, but others have not been much noticed or discussed. Then I will introduce you to some tools that are for very special purposes indeed, designed to explode one specific seductive idea or another, clearing a way out of a deep rut that still traps and flummoxes experts. We will also encounter and dismantle a variety of bad thinking tools, misbegotten persuasion-devices that can lead you astray if you aren't careful. Whether or not you arrive comfortably at my proposed destination—and decide to stay there with me—the journey will equip you with new ways of thinking about the topics, and thinking about thinking.
The physicist Richard Feynman was perhaps an even more legendary genius than von Neumann, and he was certainly endowed with a world-class brain—but he also loved having fun, and we can all be grateful that he particularly enjoyed revealing the tricks of the trade he used to make life easier for himself. No matter how smart you are, you're smarter if you take the easy ways when they are available. His autobiographical books, "Surely You're Joking, Mr. Feynman!" and What Do You Care What Other People Think?, should be on the required reading list of every aspiring thinker, since they have many hints about how to tame the toughest problems—and even how to dazzle an audience with fakery when nothing better comes to mind. Inspired by the wealth of useful observations in his books, and his candor in revealing how his mind worked, I decided to try my own hand at a similar project, less autobiographical and with the ambitious goal of persuading you to think about these topics my way. I will go to considerable lengths to cajole you out of some of your firmly held convictions, but with nothing up my sleeve. One of my main goals is to reveal along the way just what I am doing and why.
Like all artisans, a blacksmith needs tools, but—according to an old (indeed almost extinct) observation—blacksmiths are unique in that they make their own tools. Carpenters don't make their saws and hammers, tailors don't make their scissors and needles, and plumbers don't make their wrenches, but blacksmiths can make their hammers, tongs, anvils, and chisels out of their raw material, iron. What about thinking tools? Who makes them? And what are they made of? Philosophers have made some of the best of them—out of nothing but ideas, useful structures of information. René Descartes gave us Cartesian coordinates, the x- and y-axes without which calculus—a thinking tool par excellence simultaneously invented by Isaac Newton and the philosopher Gottfried Wilhelm Leibniz—would be almost unthinkable. Blaise Pascal gave us probability theory so we can easily calculate the odds of various wagers. The Reverend Thomas Bayes was also a talented mathematician, and he gave us Bayes's theorem¸ the backbone of Bayesian statistical thinking. But most of the tools that feature in this book are simpler ones, not the precise, systematic machines of mathematics and science but the hand tools of the mind. Among them are

Labels. Sometimes just creating a vivid name for something helps you keep track of it while you turn it around in your mind trying to understand it. Among the most useful labels, as we shall see, are warning labels or alarms, which alert us to likely sources of error.
Examples. Some philosophers think that using examples in their work is, if not quite cheating, at least uncalled for—rather the way novelists shun illustrations in their novels. The novelists take pride in doing it all with words, and the philosophers take pride in doing it all with carefully crafted abstract generalizations presented in rigorous order, as close to mathematical proofs as they can muster. Good for them, but they can't expect me to recommend their work to any but a few remarkable students. It's just more difficult than it has to be.
Analogies and metaphors. Mapping the features of one complex thing onto the features of another complex thing that you already (think you) understand is a famously powerful thinking tool, but it is so powerful that it often leads thinkers astray when their imaginations get captured by a treacherous analogy.
Staging. You can shingle a roof, paint a house, or fix a chimney with the help of just a ladder, moving it and climbing, moving it and climbing, getting access to only a small part of the job at a time, but it's often a lot easier in the end to take the time at the beginning to erect some sturdy staging that will allow you to move swiftly and safely around the whole project. Several of the most valuable thinking tools in this book are examples of staging that take some time to put in place but then permit a variety of problems to be tackled together—without all the ladder-moving.
And, finally, the sort of thought experiments I have dubbed intuition pumps.

Thought experiments are among the favorite tools of philosophers, not surprisingly. Who needs a lab when you can figure out the answer to your question by some ingenious deduction? Scientists, from Galileo to Einstein and beyond, have also used thought experiments to good effect, so these are not just philosophers' tools. Some thought experiments are analyzable as rigorous arguments, often of the form reductio ad absurdum,* in which one takes one's opponents' premises and derives a formal contradiction (an absurd result), showing that they can't all be right. One of my favorites is the proof attributed to Galileo that heavy things don't fall faster than lighter things (when friction is negligible). If they did, he argued, then since heavy stone A would fall faster than light stone B, if we tied B to A, stone B would act as a drag, slowing A down. But A tied to B is heavier than A alone, so the two together should also fall faster than A by itself. We have concluded that tying B to A would make something that fell both faster and slower than A by itself, which is a contradiction.
Other thought experiments are less rigorous but often just as effective: little stories designed to provoke a heartfelt, table-thumping intuition—"Yes, of course, it has to be so!"—about whatever thesis is being defended. I have called these intuition pumps. I coined the term in the first of my public critiques of philosopher John Searle's famous Chinese Room thought experiment (Searle, 1980; Dennett, 1980), and some thinkers concluded I meant the term to be disparaging or dismissive. On the contrary, I love intuition pumps! That is, some intuition pumps are excellent, some are dubious, and only a few are downright deceptive. Intuition pumps have been a dominant force in philosophy for centuries. They are the philosophers' version of Aesop's fables, which have been recognized as wonderful thinking tools since before there were philosophers.* If you ever studied philosophy in college, you were probably exposed to such classics as Plato's cave, in The Republic, in which people are chained and can see only the shadows of real things cast on the cave wall; or his example, in Meno, of teaching geometry to the slave boy. Then there is Descartes's evil demon, deceiving Descartes into believing in a world that was entirely illusory—the original Virtual Reality thought experiment—and Hobbes's state of nature, in which life is nasty, brutish, and short. Not as famous as Aesop's "Boy Who Cried Wolf" or "The Ant and the Grasshopper," but still widely known, each is designed to pump some intuitions. Plato's cave purports to enlighten us about the nature of perception and reality, and the slave boy is supposed to illustrate our innate knowledge; the evil demon is the ultimate skepticism-generator, and our improvement over the state of nature when we contract to form a society is the point of Hobbes's parable. These are the enduring melodies of philosophy, with the staying power that ensures that students will remember them, quite vividly and accurately, years after they have forgotten the intricate surrounding arguments and analysis. A good intuition pump is more robust than any one version of it. We will consider a variety of contemporary intuition pumps, including some defective ones, and the goal will be to understand what they are good for, how they work, how to use them, and even how to make them.
Here's a short, simple example: the Whimsical Jailer. Every night he waits until all the prisoners are sound asleep and then he goes around unlocking all the doors, leaving them open for hours on end. Question: Are the prisoners free? Do they have an opportunity to leave? Not really. Why not? Here's another example: the Jewels in the Trashcan. There happens to be a fortune in jewelry discarded in the trashcan on the sidewalk that you stroll by one night. It might seem that you have a golden opportunity to become rich, except it isn't golden at all because it is a bare opportunity, one that you would be extremely unlikely to recognize and hence act on—or even consider. These two simple scenarios pump intuitions that might not otherwise be obvious: the importance of getting timely information about genuine opportunities, soon enough for the information to cause us to consider it in time to do something about it. In our eagerness to make "free" choices, uncaused—we like to think—by "external forces," we tend to forget that we shouldn't want to be cut off from all such forces; free will does not abhor our embedding in a rich causal context; it actually requires it.
I hope you feel that there is more to be said on that topic! These tiny intuition pumps raise an issue vividly, but they don't settle anything—yet. (A whole section will concentrate on free will later.) We need to become practiced in the art of treating such tools warily, watching where we step, and checking for pitfalls. If we think of an intuition pump as a carefully designed persuasion tool, we can see that it might repay us to reverse engineer the tool, checking out all the moving parts to see what they are doing.
When Doug Hofstadter and I composed The Mind's I back in 1982, he came up with just the right advice on this score: consider the intuition pump to be a tool with many settings, and "turn all the knobs" to see if the same intuitions still get pumped when you consider variations.
So let's identify, and turn, the knobs on the Whimsical Jailer. Assume—until proved otherwise—that every part has a function, and see what that function is by replacing it with another part, or transforming it slightly.

Every night
he waits
until all the prisoners
are sound asleep
and then he goes around unlocking
all the doors,
leaving them open for hours on end.

Here is one of many variations we could consider:

One night he ordered his guards to drug one of the prisoners and after they had done this they accidentally left the door of that prisoner's cell unlocked for an hour.

It changes the flavor of the scenario quite a lot, doesn't it? How? It still makes the main point (doesn't it?) but not as effectively. The big difference seems to be between being naturally asleep—you might wake up any minute—and being drugged or comatose. Another difference—"accidentally"—highlights the role of the intention or inadvertence on the part of the jailor or the guards. The repetition ("every night") seems to change the odds, in favor of the prisoners. When and why do the odds matter? How much would you pay not to have to participate in a lottery in which a million people have tickets and the "winner" is shot? How much would you pay not to have to play Russian roulette with a six-shooter? (Here we use one intuition pump to illuminate another, a trick to remember.)
Other knobs to turn are less obvious: The Diabolical Host secretly locks the bedroom doors of his houseguests while they sleep. The Hospital Manager, worried about the prospect of a fire, keeps the doors of all the rooms and wards unlocked at night, but she doesn't inform the patients, thinking they will sleep more soundly if they don't know. Or what if the prison is somewhat larger than usual, say, the size of Australia? You can't lock or unlock all the doors to Australia. What difference does that make?
This self-conscious wariness with which we should approach any intuition pump is itself an important tool for thinking, the philosophers' favorite tactic: "going meta"—thinking about thinking, talking about talking, reasoning about reasoning. Meta-language is the language we use to talk about another language, and meta-ethics is a bird's-eye view examination of ethical theories. As I once said to Doug, "Anything you can do I can do meta-." This whole book is, of course, an example of going meta: exploring how to think carefully about methods of thinking carefully (about methods of thinking carefully, etc.).* He recently (2007) offered a list of some of his own favorite small hand tools:

wild goose chases
tackiness
dirty tricks
sour grapes
elbow grease
feet of clay
loose cannons
crackpots
lip service
slam dunks
feedback

If these expressions are familiar to you, they are not "just words" for you; each is an abstract cognitive tool, in the same way that long division or finding-the-average is a tool; each has a role to play in a broad spectrum of contexts, making it easier to formulate hypotheses to test, making it easier to recognize unnoticed patterns in the world, helping the user look for important similarities, and so forth. Every word in your vocabulary is a simple thinking tool, but some are more useful than others. If any of these expressions are not in your kit, you might want to acquire them; equipped with such tools you will be able to think thoughts that would otherwise be relatively hard to formulate. Of course, as the old saw has it, when your only tool is a hammer, everything looks like a nail, and each of these tools can be overused.
Let's look at just one of these: sour grapes. It comes from Aesop's fable "The Fox and the Grapes" and draws attention to how sometimes people pretend not to care about something they can't have by disparaging it. Look how much you can say about what somebody has just said by asking, simply, "Sour grapes?" It gets her to consider a possibility that might otherwise have gone unnoticed, and this might very effectively inspire her to revise her thinking, or reflect on the issue from a wider perspective—or it might very effectively insult her. (Tools can be used as weapons too.) So familiar is the moral of the story that you may have forgotten the tale leading up to it, and may have lost touch with the subtleties—if they matter, and sometimes they don't.
Acquiring tools and using them wisely are distinct skills, but you have to start by acquiring the tools, or making them yourself. Many of the thinking tools I will present here are my own inventions, but others I have acquired from others, and I will acknowledge their inventors in due course.* None of the tools on Doug's list are his inventions, but he has contributed some fine specimens to my kit, such as jootsing and sphexishness.
Some of the most powerful thinking tools are mathematical, but aside from mentioning them, I will not devote much space to them because this is a book celebrating the power of non-mathematical tools, informal tools, the tools of prose and poetry, if you like, a power that scientists often underestimate. You can see why. First, there is a culture of scientific writing in research journals that favors—indeed insists on—an impersonal, stripped-down presentation of the issues with a minimum of flourish, rhetoric, and allusion. There is a good reason for the relentless drabness in the pages of our most serious scientific journals. As one of my doctoral examiners, the neuroanatomist J. Z. Young, wrote to me in 1965, in objecting to the somewhat fanciful prose in my dissertation at Oxford (in philosophy, not neuroanatomy), English was becoming the international language of science, and it behooves us native English-speakers to write works that can be read by "a patient Chinee [sic] with a good dictionary." The results of this self-imposed discipline speak for themselves: whether you are a Chinese, German, Brazilian—or even a French—scientist, you insist on publishing your most important work in English, bare-bones English, translatable with minimal difficulty, relying as little as possible on cultural allusions, nuances, word-play, and even metaphor. The level of mutual understanding achieved by this international system is invaluable, but there is a price to be paid: some of the thinking that has to be done apparently requires informal metaphor-mongering and imagination-tweaking, assaulting the barricades of closed minds with every trick in the book, and if some of this cannot be easily translated, then I will just have to hope for virtuoso translators on the one hand, and the growing fluency in English of the world's scientists on the other.
Another reason why scientists are often suspicious of theoretical discussions conducted in "mere words" is that they recognize that the task of criticizing an argument not formulated in mathematical equations is much trickier, and typically less conclusive. The language of mathematics is a reliable enforcer of cogency. It's like the net on the basketball hoop: it removes sources of disagreement and judgment about whether the ball went in. (Anyone who has played basketball on a playground court with a bare hoop knows how hard it can be to tell an air ball from a basket.) But sometimes the issues are just too slippery and baffling to be tamed by mathematics.
I have always figured that if I can't explain something I'm doing to a group of bright undergraduates, I don't really understand it myself, and that challenge has shaped everything I have written. Some philosophy professors yearn to teach advanced seminars only to graduate students. Not me. Graduate students are often too eager to prove to each other and to themselves that they are savvy operators, wielding the jargon of their trade with deft assurance, baffling outsiders (that's how they assure themselves that what they are doing requires expertise), and showing off their ability to pick their way through the most tortuous (and torturous) technical arguments without getting lost. Philosophy written for one's advanced graduate students and fellow experts is typically all but unreadable—and hence largely unread.
A curious side effect of my policy of trying to write arguments and explanations that can be readily understood by people outside philosophy departments is that there are philosophers who as a matter of "principle" won't take my arguments seriously! When I gave the John Locke Lectures at Oxford many years ago to a standing-room-only audience, a distinguished philosopher was heard to grumble as he left one of them that he was damned if he would learn anything from somebody who could attract non-philosophers to the Locke Lectures! True to his word, he never learned anything from me, so far as I can tell. I did not adjust my style and have never regretted paying the price. There is a time and a place in philosophy for rigorous arguments, with all the premises numbered and the inference rules named, but these do not often need to be paraded in public. We ask our graduate students to prove they can do it in their dissertations, and some never outgrow the habit, unfortunately. And to be fair, the opposite sin of high-flown Continental rhetoric, larded with literary ornament and intimations of profundity, does philosophy no favors either. If I had to choose, I'd take the hard-bitten analytic logic-chopper over the deep purple sage every time. At least you can usually figure out what the logic-chopper is talking about and what would count as being wrong.
The middle ground, roughly halfway between poetry and mathematics, is where philosophers can make their best contributions, I believe, yielding genuine clarifications of deeply puzzling problems. There are no feasible algorithms for doing this kind of work. Since everything is up for grabs, one chooses one's fixed points with due caution. As often as not, an "innocent" assumption accepted without notice on all sides turns out to be the culprit. Exploring such treacherous conceptual territories is greatly aided by using thinking tools devised on the spot to clarify the alternative paths and shed light on their prospects.
These thinking tools seldom establish a fixed fixed point—a solid "axiom" for all future inquiry—but rather introduce a worthy candidate for a fixed point, a likely constraint on future inquiry, but itself subject to revision or jettisoning altogether if somebody can figure out why. No wonder many scientists have no taste at all for philosophy; everything is up for grabs, nothing is take-it-to-the-bank secure, and the intricate webs of argument constructed to connect these "fixed" points hang provisionally in the air, untethered to clear foundations of empirical proof or falsification. So these scientists turn their backs on philosophy and get on with their work, but at the cost of leaving some of the most important and fascinating questions unconsidered. "Don't ask! Don't tell! It's premature to tackle the problem of consciousness, of free will, of morality, of meaning and creativity!" But few can live with such abstemiousness, and in recent years scientists have set out on a gold rush of sorts into these shunned regions. Seduced by sheer curiosity (or, sometimes, perhaps, a yearning for celebrity), they embark on the big questions and soon discover how hard it is to make progress on them. I must confess that one of the delicious, if guilty, pleasures I enjoy is watching eminent scientists, who only a few years ago expressed withering contempt for philosophy,* stumble embarrassingly in their own efforts to set the world straight on these matters with a few briskly argued extrapolations from their own scientific research. Even better is when they request, and acknowledge, a little help from us philosophers.
In the first section that follows, I present a dozen general, all-purpose tools, and then in subsequent sections I group the rest of the entries not by the type of tool but by the topic where the tool works best, turning first to the most fundamental philosophical topic—meaning, or content—followed by evolution, consciousness, and free will. A few of the tools I present are actual software, friendly devices that can do for your naked imagination what telescopes and microscopes can do for your naked eye.
Along the way, I will also introduce some false friends, tools that blow smoke instead of shining light. I needed a term for these hazardous devices, and found le mot juste in my sailing experience. Many sailors enjoy the nautical terms that baffle landlubbers: port and starboard, gudgeon and pintle, shrouds and spreaders, cringles and fairleads, and all the rest. A running joke on a boat I once sailed on involved making up false definitions for these terms. So a binnacle was a marine growth on compasses, and a mast tang was a citrus beverage enjoyed aloft; a snatch block was a female defensive maneuver, and a boom crutch was an explosive orthopedic device. I've never since been able to think of a boom crutch—a removable wooden stand on which the boom rests when the sail is lowered—without a momentary image of kapow! in some poor fellow's armpit. So I chose the term as my name for thinking tools that backfire, the ones that only seem to aid in understanding but that actually spread darkness and confusion instead of light. Scattered through these chapters are a variety of boom crutches with suitable warning labels, and examples to deplore. And I close with some further reflections on what it is like to be a philosopher, in case anybody wants to know, including some advice from Uncle Dan to any of you who might have discovered a taste for this way of investigating the world and wonder whether you are cut out for a career in the field.














II. A DOZEN GENERAL THINKING TOOLS

 

Most of the thinking tools in this book are quite specialized, made to order for application to a particular topic and even a particular controversy within the topic. But before we turn to these intuition pumps, here are a few general-purpose thinking tools, ideas and practices that have proved themselves in a wide variety of contexts.











1. Making Mistakes

He who says "Better to go without belief forever than believe a lie!" merely shows his own preponderant private horror of becoming a dupe. ... It is like a general informing his soldiers that it is better to keep out of battle forever than to risk a single wound. Not so are victories either over enemies or over nature gained. Our errors are surely not such awfully solemn things. In a world where we are so certain to incur them in spite of all our caution, a certain lightness of heart seems healthier than this excessive nervousness on their behalf.
—WILLIAM JAMES, "The Will to Believe"
If you've made up your mind to test a theory, or you want to explain some idea, you should always decide to publish it whichever way it comes out. If we only publish results of a certain kind, we can make the argument look good. We must publish both kinds of results.
—RICHARD FEYNMAN, "Surely You're Joking, Mr. Feynman!"

Scientists often ask me why philosophers devote so much of their effort to teaching and learning the history of their field. Chemists typically get by with only a rudimentary knowledge of the history of chemistry, picked up along the way, and many molecular biologists, it seems, are not even curious about what happened in biology before about 1950. My answer is that the history of philosophy is in large measure the history of very smart people making very tempting mistakes, and if you don't know the history, you are doomed to making the same darn mistakes all over again. That's why we teach the history of the field to our students, and scientists who blithely ignore philosophy do so at their own risk. There is no such thing as philosophy-free science, just science that has been conducted without any consideration of its underlying philosophical assumptions. The smartest or luckiest of the scientists sometimes manage to avoid the pitfalls quite adroitly (perhaps they are "natural born philosophers"—or are as smart as they think they are), but they are the rare exceptions. Not that professional philosophers don't make—and even defend—the old mistakes too. If the questions weren't hard, they wouldn't be worth working on.
Sometimes you don't just want to risk making mistakes; you actually want to make them—if only to give you something clear and detailed to fix. Making mistakes is the key to making progress. Of course there are times when it is really important not to make any mistakes—ask any surgeon or airline pilot. But it is less widely appreciated that there are also times when making mistakes is the only way to go. Many of the students who arrive at very competitive universities pride themselves in not making mistakes—after all, that's how they've come so much farther than their classmates, or so they have been led to believe. I often find that I have to encourage them to cultivate the habit of making mistakes, the best learning opportunities of all. They get "writer's block" and waste hours forlornly wandering back and forth on the starting line. "Blurt it out!" I urge them. Then they have something on the page to work with.
We philosophers are mistake specialists. (I know, it sounds like a bad joke, but hear me out.) While other disciplines specialize in getting the right answers to their defining questions, we philosophers specialize in all the ways there are of getting things so mixed up, so deeply wrong, that nobody is even sure what the right questions are, let alone the answers. Asking the wrongs questions risks setting any inquiry off on the wrong foot. Whenever that happens, this is a job for philosophers! Philosophy—in every field of inquiry—is what you have to do until you figure out what questions you should have been asking in the first place. Some people hate it when that happens. They would rather take their questions off the rack, all nicely tailored and pressed and cleaned and ready to answer. Those who feel that way can do physics or mathematics or history or biology. There's plenty of work for everybody. We philosophers have a taste for working on the questions that need to be straightened out before they can be answered. It's not for everyone. But try it, you might like it.
In the course of this book I am going to jump vigorously on what I claim are other people's mistakes, but I want to assure you that I am an experienced mistake-maker myself. I've made some dillies, and hope to make a lot more. One of my goals in this book is to help you make good mistakes, the kind that light the way for everybody.
First the theory, and then the practice. Mistakes are not just opportunities for learning; they are, in an important sense, the only opportunity for learning or making something truly new. Before there can be learning, there must be learners. There are only two non-miraculous ways for learners to come into existence: they must either evolve or be designed and built by learners that evolved. Biological evolution proceeds by a grand, inexorable process of trial and error—and without the errors the trials wouldn't accomplish anything. As Gore Vidal once said, "It is not enough to succeed. Others must fail." Trials can be either blind or foresighted. You, who know a lot, but not the answer to the question at hand, can take leaps—foresighted leaps. You can look before you leap, and hence be somewhat guided from the outset by what you already know. You need not be guessing at random, but don't look down your nose at random guesses; among its wonderful products is ... you!
Evolution is one of the central themes of this book, as of all my books, for the simple reason that it is the central, enabling process not only of life but also of knowledge and learning and understanding. If you attempt to make sense of the world of ideas and meanings, free will and morality, art and science and even philosophy itself without a sound and quite detailed knowledge of evolution, you have one hand tied behind your back. Later, we will look at some tools designed to help you think about some of the more challenging questions of evolution, but here we need to lay a foundation. For evolution, which knows nothing, the steps into novelty are blindly taken by mutations, which are random copying "errors" in DNA. Most of these typographical errors are of no consequence, since nothing reads them! They are as inconsequential as the rough drafts you didn't, or don't, hand in to the teacher for grading. The DNA of a species is rather like a recipe for building a new body, and most of the DNA is never actually consulted in the building process. (It is often called "junk DNA" for just that reason.) In the DNA sequences that do get read and acted upon during development, the vast majority of mutations are harmful; many, in fact, are swiftly fatal. Since the majority of "expressed" mutations are deleterious, the process of natural selection actually works to keep the mutation rate very low. Each of you has very, very good copying machinery in your cells. For instance, you have roughly a trillion cells in your body, and each cell has either a perfect or an almost perfect copy of your genome, over three billion symbols long, the recipe for you that first came into existence when your parents' egg and sperm joined forces. Fortunately, the copying machinery does not achieve perfect success, for if it did, evolution would eventually grind to a halt, its sources of novelty dried up. Those tiny blemishes, those "imperfections" in the process, are the source of all the wonderful design and complexity in the living world. (I can't resist adding: if anything deserves to be called Original Sin, these copying mistakes do.)
The chief trick to making good mistakes is not to hide them—especially not from yourself. Instead of turning away in denial when you make a mistake, you should become a connoisseur of your own mistakes, turning them over in your mind as if they were works of art, which in a way they are. The fundamental reaction to any mistake ought to be this: "Well, I won't do that again!" Natural selection doesn't actually think the thought; it just wipes out the goofers before they can reproduce; natural selection won't do that again, at least not as often. Animals that can learn—learn not to make that noise, touch that wire, eat that food—have something with a similar selective force in their brains. (B. F. Skinner and the behaviorists understood the need for this and called it "reinforcement" learning; that response is not reinforced and suffers "extinction.") We human beings carry matters to a much more swift and efficient level. We can actually think the thought, reflecting on what we have just done: "Well, I won't do that again!" And when we reflect, we confront directly the problem that must be solved by any mistake-maker: what, exactly, is that? What was it about what I just did that got me into all this trouble? The trick is to take advantage of the particular details of the mess you've made, so that your next attempt will be informed by it and not just another blind stab in the dark.
We have all heard the forlorn refrain "Well, it seemed like a good idea at the time!" This phrase has come to stand for the rueful reflection of an idiot, a sign of stupidity, but in fact we should appreciate it as a pillar of wisdom. Any being, any agent, who can truly say, "Well, it seemed like a good idea at the time!" is standing on the threshold of brilliance. We human beings pride ourselves on our intelligence, and one of its hallmarks is that we can remember our previous thinking, and reflect on it—on how it seemed, on why it was tempting in the first place, and then about what went wrong. I know of no evidence to suggest that any other species on the planet can actually think this thought. If they could, they would be almost as smart as we are.
So when you make a mistake, you should learn to take a deep breath, grit your teeth, and then examine your own recollections of the mistake as ruthlessly and as dispassionately as you can manage. It's not easy. The natural human reaction to making a mistake is embarrassment and anger (we are never angrier than when we are angry at ourselves), and you have to work hard to overcome these emotional reactions. Try to acquire the weird practice of savoring your mistakes, delighting in uncovering the strange quirks that led you astray. Then, once you have sucked out all the goodness to be gained from having made them, you can cheerfully set them behind you, and go on to the next big opportunity. But that is not enough: you should actively seek out opportunities to make grand mistakes, just so you can then recover from them.
At its simplest, this is a technique we all learned in grade school. Recall how strange and forbidding long division seemed at first: You were confronted by two imponderably large numbers, and you had to figure out how to start. Does the divisor go into the dividend six or seven or eight times? Who knew? You didn't have to know; you just had to take a stab at it, whichever number you liked, and check out the result. I remember being almost shocked when I was told I should start by just "making a guess." Wasn't this mathematics? You weren't supposed to play guessing games in such a serious business, were you? But eventually I appreciated, as we all do, the beauty of the tactic. If the chosen number turned out to be too small, you increased it and started over; if too large, you decreased it. The good thing about long division was that it always worked, even if you were maximally stupid in making your first choice, in which case it just took a little longer.
This general technique of making a more-or-less educated guess, working out its implications, and using the result to make a correction for the next phase has found many applications. A key element of this tactic is making a mistake that is clear and precise enough to have definite implications. Before GPS came along, navigators used to determine their position at sea by first making a guess about where they were (they made a guess about exactly what their latitude and longitude were), and then calculating exactly how high in the sky the sun would appear to be if that were—by an incredible coincidence—their actual position. When they used this method, they didn't expect to hit the nail on the head. They didn't have to. Instead they then measured the actual elevation angle of the sun (exactly) and compared the two values. With a little more trivial calculation, this told them how big a correction, and in what direction, to make to their initial guess.* In such a method it is useful to make a pretty good guess the first time, but it doesn't matter that it is bound to be mistaken; the important thing is to make the mistake, in glorious detail, so there is something serious to correct. (A GPS device uses the same guess-and-fix-it strategy to locate itself relative to the overhead satellites.)
The more complex a problem you're facing, of course, the more difficult the analysis is. This is known to researchers in artificial intelligence (AI) as the problem of "credit assignment" (it could as well be called blame assignment). Figuring out what to credit and what to blame is one of the knottiest problems in AI, and it is also a problem faced by natural selection. Every organism on the earth dies sooner or later after one complicated life story or another. How on earth could natural selection see through the fog of all these details in order to figure out what positive factors to "reward" with offspring and what negative factors to "punish" with childless death? Can it really be that some of our ancestors' siblings died childless because their eyelids were the wrong shape? If not, how could the process of natural selection explain why our eyelids came to have the excellent shapes they have? Part of the answer is familiar: following the old adage "If it ain't broke, don't fix it," leave almost all your old, conservative design solutions in place and take your risks with a safety net in place. Natural selection automatically conserves whatever has worked up to now, and fearlessly explores innovations large and small; the large ones almost always lead immediately to death. A terrible waste, but nobody's counting. Our eyelids were mostly designed by natural selection long before there were human beings or even primates or even mammals. They've had more than a hundred million years to reach the shape they are today, with only a few minor touch-ups in the last six million years, since we shared a common ancestor with the chimpanzees and the bonobos. Another part of the answer is that natural selection works with large numbers of cases, where even minuscule advantages show up statistically and can be automatically accumulated. (Other parts of the answer are technicalities beyond this elementary discussion.)
Here is a technique that card magicians—at least the best of them—exploit with amazing results. (I don't expect to incur the wrath of the magicians for revealing this trick to you, since this is not a particular trick but a deep general principle.) A good card magician knows many tricks that depend on luck—they don't always work, or even often work. There are some effects—they can hardly be called tricks—that might work only once in a thousand times! Here is what you do: You start by telling the audience you are going to perform a trick, and without telling them what trick you are doing, you go for the one-in-a-thousand effect. It almost never works, of course, so you glide seamlessly into a second try—for an effect that works about one time in a hundred, perhaps—and when it too fails (as it almost always will), you slide gracefully into effect number 3, which works only about one time in ten, so you'd better be ready with effect number 4, which works half the time (let's say). If all else fails (and by this time, usually one of the earlier safety nets will have kept you out of this worst case), you have a failsafe effect, which won't impress the crowd very much but at least it's a surefire trick. In the course of a whole performance, you will be very unlucky indeed if you always have to rely on your final safety net, and whenever you achieve one of the higher-flying effects, the audience will be stupefied. "Impossible! How on earth could you have known which was my card?" Aha! You didn't know, but you had a cute way of taking a hopeful stab in the dark that paid off. By hiding all the "mistake" cases from view—the trials that didn't pan out—you create a "miracle."
Evolution works the same way: all the dumb mistakes tend to be invisible, so all we see is a stupendous string of triumphs. For instance, the vast majority—way over 90 percent—of all the creatures that have ever lived died childless, but not a single one of your ancestors suffered that fate. Talk about a line of charmed lives!
One big difference between the discipline of science and the discipline of stage magic is that while magicians conceal their false starts from the audience as best they can, in science you make your mistakes in public. You show them off so that everybody can learn from them. This way, you get the benefit of everybody else's experience, and not just your own idiosyncratic path through the space of mistakes. (The physicist Wolfgang Pauli famously expressed his contempt for the work of a colleague as "not even wrong." A clear falsehood shared with critics is better than vague mush.) This, by the way, is another reason why we humans are so much smarter than every other species. It is not so much that our brains are bigger or more powerful, or even that we have the knack of reflecting on our own past errors, but that we share the benefits that our individual brains have won by their individual histories of trial and error.*
I am amazed at how many really smart people don't understand that you can make big mistakes in public and emerge none the worse for it. I know distinguished researchers who will go to preposterous lengths to avoid having to acknowledge that they were wrong about something. They have never noticed, apparently, that the earth does not swallow people up when they say, "Oops, you're right. I guess I made a mistake." Actually, people love it when somebody admits to making a mistake. All kinds of people love pointing out mistakes. Generous-spirited people appreciate your giving them the opportunity to help, and acknowledging it when they succeed in helping you; mean-spirited people enjoy showing you up. Let them! Either way we all win.
Of course, in general, people do not enjoy correcting the stupid mistakes of others. You have to have something worth correcting, something original to be right or wrong about, something that requires constructing the sort of pyramid of risky thinking we saw in the card magician's tricks. Carefully building on the works of others, you can get yourself cantilevered out on a limb of your own. And then there's a surprise bonus: if you are one of the big risk-takers, people will get a kick out of correcting your occasional stupid mistakes, which show that you're not so special, you're a regular bungler like the rest of us. I know extremely careful philosophers who have never—apparently—made a mistake in their work. They tend not to get a whole lot accomplished, but what little they produce is pristine, if not venturesome. Their specialty is pointing out the mistakes of others, and this can be a valuable service, but nobody excuses their minor errors with a friendly chuckle. It is fair to say, unfortunately, that their best work often gets overshadowed and neglected, drowned out by the passing bandwagons driven by bolder thinkers. In chapter 76 we'll see that the generally good practice of making bold mistakes has other unfortunate side effects as well. Meta-advice: don't take any advice too seriously!











2. "By Parody of Reasoning": Using Reductio ad Absurdum
The crowbar of rational inquiry, the great lever that enforces consistency, is reductio ad absurdum—literally, reduction (of the argument) to absurdity. You take the assertion or conjecture at issue and see if you can pry any contradictions (or just preposterous implications) out of it. If you can, that proposition has to be discarded or sent back to the shop for retooling. We do this all the time without bothering to display the underlying logic: "If that's a bear, then bears have antlers!" or "He won't get here in time for supper unless he can fly like Superman." When the issue is a tricky theoretical controversy, the crowbar gets energetically wielded, but here the distinction between fair criticism and refutation by caricature is hard to draw. Can your opponent really be so stupid as to believe the proposition you have just reduced to absurdity with a few deft moves? I once graded a student paper that had a serendipitous misspelling, replacing "parity" with "parody," creating the delicious phrase "by parody of reasoning," a handy name, I think, for misbegotten reductio ad absurdum arguments, which are all too common in the rough-and-tumble of scientific and philosophical controversy.
I recall attending a seminar on cognitive science at MIT some years ago, conducted by the linguist Noam Chomsky and the philosopher Jerry Fodor, in which the audience was regularly regaled with hilarious refutations of cognitive scientists from elsewhere who did not meet with their approval. On this day, Roger Schank, the director of Yale University's artificial intelligence laboratory, was the bête noir, and if you went by Chomsky's version, Schank had to be some kind of flaming idiot. I knew Roger and his work pretty well, and though I had disagreements of my own with it, I thought that Noam's version was hardly recognizable, so I raised my hand and suggested that perhaps he didn't appreciate some of the subtleties of Roger's position. "Oh no," Noam insisted, chuckling. "This is what he holds!" And he went back to his demolition job, to the great amusement of those in the room. After a few more minutes of this I intervened again. "I have to admit," I said, "that the views you are criticizing are simply preposterous," and Noam grinned affirmatively, "but then what I want to know is why you're wasting your time and ours criticizing such junk." It was a pretty effective pail of cold water.
What about my own reductios of the views of others? Have they been any fairer? Here are a few to consider. You decide. The French neuroscientist Jean-Pierre Changeux and I once debated neuroscientist Sir John Eccles and philosopher Sir Karl Popper about consciousness and the brain at a conference in Venice. Changeux and I were the materialists (who maintain that the mind is the brain), and Popper and Eccles the dualists (who claim that a mind is not a material thing like a brain, but some other, second kind of entity that interacts with the brain). Eccles had won the Nobel Prize many years earlier for the discovery of the synapse, the microscopic gap between neurons that glutamate molecules and other neurotransmitters and neuromodulators cross trillions of times a day. According to Eccles, the brain was like a mighty pipe organ and the trillions of synapses composed the keyboards. The immaterial mind—the immortal soul, according to Eccles, a devout Catholic—played the synapses by somehow encouraging quantum-level nudges of the glutamate molecules. "Forget all that theoretical discussion of neural networks and the like; it's irrelevant rubbish," he said. "The mind is in the glutamate!" When it was my turn to speak, I said I wanted to be sure I had understood his position. If the mind was in the glutamate and I poured a bowl of glutamate down the drain, would that not be murder? "Well," he replied, somewhat taken aback, "it would be very hard to tell, wouldn't it?"*
You would think that Sir John Eccles, the Catholic dualist, and Francis Crick, the atheist materialist, would have very little in common, aside from their Nobel Prizes. But at least for a while their respective views of consciousness shared a dubious oversimplification. Many nonscientists don't appreciate how wonderful oversimplifications can be in science; they can cut through the hideous complexity with a working model that is almost right, postponing the messy details until later. Arguably the best use of "over"-simplification in the history of science was the end run by Crick and James Watson to find the structure of DNA while Linus Pauling and others were trudging along trying to make sense of all the details. Crick was all for trying the bold stroke just in case it solved the problem in one fell swoop, but of course that doesn't always work. I was once given the opportunity to demonstrate this at one of Crick's famous teas at La Jolla. These afternoon sessions were informal lab meetings where visitors could raise issues and participate in the general discussion. On this particular occasion Crick made a bold pronouncement: it had recently been shown that neurons in cortical area V4 "cared about" (responded differentially to) color. And then he proposed a strikingly simple hypothesis: the conscious experience of red, for instance, was activity in the relevant red-sensitive neurons of that retinal area. Hmm, I wondered. "Are you saying, then, that if we were to remove some of those red-sensitive neurons and keep them alive in a petri dish, and stimulate them with a microelectrode, there would be consciousness of red in the petri dish?" One way of responding to a proffered reductio is to grasp the nettle and endorse the conclusion, a move I once dubbed outsmarting, since the Australian philosopher J. J. C. Smart was famous for saying that yes, according to his theory of ethics, it was sometimes right to frame and hang an innocent man! Crick decided to outsmart me. "Yes! It would be an isolated instance of consciousness of red!" Whose consciousness of red? He didn't say. He later refined his thinking on this score, but still, he and neuro-scientist Christof Koch, in their quest for what they called the NCC (the neural correlates of consciousness), never quite abandoned their allegiance to this idea.
Perhaps yet another encounter will bring out better what is problematic about the idea of a smidgen of consciousness in a dish. The physicist and mathematician Roger Penrose and the anesthesiologist Stuart Hameroff teamed up to produce a theory of consciousness that depended, not on glutamate, but on quantum effects in the microtubules of neurons. (Microtubules are tubular protein chains that serve as girders and highways inside the cytoplasm of all cells, not just neurons.) At Tucson II, the second international conference on the science of consciousness, after Hameroff's exposition of this view, I asked from the audience, "Stuart, you're an anesthesiologist; have you ever assisted in one of those dramatic surgeries that replaces a severed hand or arm?" No, he had not, but he knew about them. "Tell me if I'm missing something, Stuart, but given your theory, if you were the anesthesiologist in such an operation you would feel morally obliged to anesthetize the severed hand as it lay on its bed of ice, right? After all, the microtubules in the nerves of the hand would be doing their thing, just like the microtubules in the rest of the nervous system, and that hand would be in great pain, would it not?" The look on Stuart's face suggested that this had never occurred to him. The idea that consciousness (of red, of pain, of anything) is some sort of network property, something that involves coordinated activities in myriads of neurons, initially may not be very attractive, but these attempts at reductios may help people see why it should be taken seriously.











3. Rapoport's Rules
Just how charitable are you supposed to be when criticizing the views of an opponent? If there are obvious contradictions in the opponent's case, then of course you should point them out, forcefully. If there are somewhat hidden contradictions, you should carefully expose them to view—and then dump on them. But the search for hidden contradictions often crosses the line into nitpicking, sea-lawyering,* and—as we have seen—outright parody. The thrill of the chase and the conviction that your opponent has to be harboring a confusion somewhere encourages uncharitable interpretation, which gives you an easy target to attack. But such easy targets are typically irrelevant to the real issues at stake and simply waste everybody's time and patience, even if they give amusement to your supporters. The best antidote I know for this tendency to caricature one's opponent is a list of rules promulgated many years ago by the social psychologist and game theorist Anatol Rapoport (creator of the winning Tit-for-Tat strategy in Robert Axelrod's legendary prisoner's dilemma tournament).†
How to compose a successful critical commentary:

You should attempt to re-express your target's position so clearly, vividly, and fairly that your target says, "Thanks, I wish I'd thought of putting it that way."
You should list any points of agreement (especially if they are not matters of general or widespread agreement).
You should mention anything you have learned from your target.
Only then are you permitted to say so much as a word of rebuttal or criticism.

One immediate effect of following these rules is that your targets will be a receptive audience for your criticism: you have already shown that you understand their positions as well as they do, and have demonstrated good judgment (you agree with them on some important matters and have even been persuaded by something they said).*
Following Rapoport's Rules is always, for me at least, something of a struggle. Some targets, quite frankly, don't deserve such respectful attention, and—I admit—it can be sheer joy to skewer and roast them. But when it is called for, and it works, the results are gratifying. I was particularly diligent in my attempt to do justice to Robert Kane's (1996) brand of incompatibilism (a view about free will with which I profoundly disagree) in my book Freedom Evolves (2003), and I treasure the response he wrote to me after I had sent him the draft chapter:

... In fact, I like it a lot, our differences notwithstanding. The treatment of my view is extensive and generally fair, far more so than one usually gets from critics. You convey the complexity of my view and the seriousness of my efforts to address difficult questions rather than merely sweeping them under the rug. And for this, as well as the extended treatment, I am grateful.

Other recipients of my Rapoport-driven attention have been less cordial. The fairer the criticism seems, the harder to bear in some cases. It is worth reminding yourself that a heroic attempt to find a defensible interpretation of an author, if it comes up empty, can be even more devastating than an angry hatchet job. I recommend it.











4. Sturgeon's Law
The science-fiction author Ted Sturgeon, speaking at the World Science Fiction Convention in Philadelphia in September 1953, said,

When people talk about the mystery novel, they mention The Maltese Falcon and The Big Sleep. When they talk about the western, they say there's The Way West and Shane. But when they talk about science fiction, they call it "that Buck Rogers stuff," and they say "ninety percent of science fiction is crud." Well, they're right. Ninety percent of science fiction is crud. But then ninety percent of everything is crud, and it's the ten percent that isn't crud that is important, and the ten percent of science fiction that isn't crud is as good as or better than anything being written anywhere.

Sturgeon's Law is usually put a little less decorously: Ninety percent of everything is crap. Ninety percent of experiments in molecular biology, 90 percent of poetry, 90 percent of philosophy books, 90 percent of peer-reviewed articles in mathematics—and so forth—is crap. Is that true? Well, maybe it's an exaggeration, but let's agree that there is a lot of mediocre work done in every field. (Some curmudgeons say it's more like 99 percent, but let's not get into that game.) A good moral to draw from this observation is that when you want to criticize a field, a genre, a discipline, an art form, ... don't waste your time and ours hooting at the crap! Go after the good stuff, or leave it alone. This advice is often ignored by ideologues intent on destroying the reputation of analytic philosophy, evolutionary psychology, sociology, cultural anthropology, macroeconomics, plastic surgery, improvisational theater, television sitcoms, philosophical theology, massage therapy, you name it. Let's stipulate at the outset that there is a great deal of deplorable, stupid, second-rate stuff out there, of all sorts. Now, in order not to waste your time and try our patience, make sure you concentrate on the best stuff you can find, the flagship examples extolled by the leaders of the field, the prize-winning entries, not the dregs. Notice that this is closely related to Rapoport's Rules: unless you are a comedian whose main purpose is to make people laugh at ludicrous buffoonery, spare us the caricature. This is particularly true, I find, when the target is philosophers. The very best theories and analyses of any philosopher, from the greatest, most perceptive sages of ancient Greece to the intellectual heroes of the recent past (Bertrand Russell, Ludwig Wittgenstein, John Dewey, Jean Paul Sartre—to name four very different thinkers), can be made to look like utter idiocy—or tedious nitpicking—with a few deft tweaks. Yuck, yuck. Don't do it. The only one you'll discredit is yourself.











5. Occam's Razor
Attributed to William of Ockham (or Occam), the fourteenth-century logician and philosopher, this thinking tool is actually a much older rule of thumb. A Latin name for it is lex parsimoniae, the law of parsimony. It is usually put into English as the maxim "Do not multiply entities beyond necessity." The idea is straightforward: don't concoct a complicated, extravagant theory if you've got a simpler one (containing fewer ingredients, fewer entities) that handles the phenomenon just as well. If exposure to extremely cold air can account for all the symptoms of frostbite, don't postulate unobserved "snow germs" or "arctic microbes." Kepler's laws explain the orbits of the planets; we have no need to hypothesize pilots guiding the planets from control panels hidden under the surface. This much is uncontroversial, but extensions of the principle have not always met with agreement.
Conwy Lloyd Morgan, a nineteenth-century British psychologist, extended the idea to cover attributions of mentality to animals. Lloyd Morgan's Canon of Parsimony advises us not to attribute fancy minds to insects, fish, and even dolphins, dogs, and cats if their behavior can be explained in simpler terms:

In no case is an animal activity to be interpreted in terms of higher psychological processes, if it can be fairly interpreted in terms of processes which stand lower in the scale of psychological evolution and development. [1894, p. 128]

Overused, this can be seen as enjoining us to treat all animals and even human beings as having brains but no minds. As we shall see, the tensions that arise when minds are the topic are not well settled by absolute prohibitions.
One of the least impressive attempts to apply Occam's Razor to a gnarly problem is the claim (and provoked counterclaims) that postulating a God as creator of the universe is simpler, more parsimonious, than the alternatives. How could postulating something supernatural and incomprehensible be parsimonious? It strikes me as the height of extravagance, but perhaps there are clever ways of rebutting that suggestion. I don't want to argue about it; Occam's Razor is, after all, just a rule of thumb, a frequently useful suggestion. The prospect of turning it into a Metaphysical Principle or Fundamental Requirement of Rationality that could bear the weight of proving or disproving the existence of God in one fell swoop is simply ludicrous. It would be like trying to disprove a theorem of quantum mechanics by showing that it contradicted the axiom "Don't put all your eggs in one basket."
Some thinkers have carried Occam's Razor to drastic extremes, using it to deny the existence of time, matter, numbers, holes, dollars, software, and so on. One of the earliest ultra-stingy thinkers was the ancient Greek philosopher Parmenides, whose catalogue of existing things was minimal indeed. As a student of mine memorably wrote on an exam, "Parmenides is the one who said, 'There's just one thing—and I'm not it.' " I hate to say it, but that does seem to be what Parmenides was trying to tell us. No doubt it loses something in translation. We philosophers get used to taking such ideas seriously if only because we never can tell when a "crazy" idea is going to turn out to be unfairly and unwisely judged, a victim of failure of imagination.











6. Occam's Broom
The molecular biologist Sidney Brenner recently invented a delicious play on Occam's Razor, introducing the new term Occam's Broom, to describe the process in which inconvenient facts are whisked under the rug by intellectually dishonest champions of one theory or another. This is our first boom crutch, an anti-thinking tool, and you should keep your eyes peeled for it. The practice is particularly insidious when used by propagandists who direct their efforts at the lay public, because like Sherlock Holmes's famous clue about the dog that didn't bark in the night, the absence of a fact that has been swept off the scene by Occam's Broom is unnoticeable except by experts. For instance, creationists invariably leave out the wealth of embarrassing evidence that their "theories" can't handle, and to a nonbiologist their carefully crafted accounts can be quite convincing simply because the lay reader can't see what isn't there.
How on earth can you keep on the lookout for something invisible? Get some help from the experts. Stephen C. Meyer's Signature in the Cell (2009) purports to expose the systematic impossibility of life having a natural (nonsupernatural) origin, and gives what seems—even to a relatively well-informed reader—to be a fair and exhaustive survey of the theories and models being worked on around the world, showing how irredeemably hopeless they all are. So persuasive is Meyer's case that in November 2009 the eminent philosopher Thomas Nagel declared it his Best Book of the Year in London's Times Literary Supplement, one of the world's most influential publications of book reviews! In a spirited correspondence I had with him after his rave appeared, he demonstrated that he knew quite a lot about the history of work on the origin of life, enough to think he could trust his own judgment. And as he noted in a letter to the Times Literary Supplement (January 1, 2010), "Meyer's book seems to me to be written in good faith." Had Nagel consulted with scientists working in the field, he would have been able to see Meyer's exploitation of Occam's Broom, whisking inconvenient facts out of view, and he might also have been dismayed to learn that the experts hadn't been sent an early copy of Meyer's book, as he had, or been asked to referee it before publication. Learning that the book he admired was a stealth operation might have shaken his confidence in his judgment, or it might not have. The scientific establishment has been known to squelch renegade critics unjustly on occasion, and perhaps—perhaps—Meyer had no choice but to launch a sneak attack. But Nagel would have been wise to explore this prospect warily before committing himself. It is fair to say that the scientists working on the origin of life do not yet have a secure and agreed-upon theory, but there is no dearth of candidates, an embarrassment of riches rather than an almost empty arena.
Conspiracy theorists are masters of Occam's Broom, and an instructive exercise on the Internet is to look up a new conspiracy theory, to see if you (a nonexpert on the topic) can find the flaws, before looking elsewhere on the web for the expert rebuttals. When Brenner coined the term, he wasn't talking about creationism and conspiracy theories; he was pointing out that in the heat of battle, even serious scientists sometimes cannot resist "overlooking" some data that seriously undermine their pet theory. It's a temptation to be resisted, no matter what.











7. Using Lay Audiences as Decoys
One good way of preventing people from inadvertently wielding Occam's Broom is a technique that I have been recommending for years, and have several times put to the test—but never as ambitiously as I would like to do. Unlike the other practices I have been describing, this one takes time and money to do properly. I hope others will pursue this technique vigorously and report the results. I have decided to put it here because it addresses some of the same problems of communication that the other general tools confront.
In many fields—not just philosophy—there are controversies that seem never-ending and partly artifactual: people are talking past one another and not making the necessary effort to communicate effectively. Tempers flare, and disrespect and derision start creeping in. People on the sidelines take sides, even when they don't fully understand the issues.
It can get ugly, and it can have a very straightforward cause. When experts talk to experts, whether they are in the same discipline or not, they always err on the side of under-explaining. The reason is not far to seek: to overexplain something to a fellow expert is a very serious insult—"Do I have to spell it out for you?"—and nobody wants to insult a fellow expert. So just to be safe, people err on the side of under-explaining. It is not done deliberately, for the most part, and it is almost impossible to keep from doing—which is actually a good thing, since being polite in an unstudied way is a nice character trait in anyone. But this gracious disposition to assume more understanding than is apt to be present in one's distinguished audience has an unfortunate by-product: experts often talk past each other.
There is no direct cure: entreating all the experts present at a workshop or conference not to under-explain their positions may be met by earnest promises, but it won't work. If anything it will make matters worse since now people will be particularly sensitive to the issue of inadvertently insulting somebody. But there is an indirect and quite effective cure: have all experts present their views to a small audience of curious nonexperts (here at Tufts I have the advantage of bright undergraduates) while the other experts listen in from the sidelines. They don't have to eavesdrop; this isn't a devious suggestion. On the contrary, everybody can and should be fully informed that the point of the exercise is to make it comfortable for participants to speak in terms that everybody will understand. By addressing their remarks to the undergraduates (the decoy audience), speakers need not worry at all about insulting the experts because they are not addressing the experts. (I suppose they might worry about insulting the undergraduates, but that's another matter.) When all goes well, expert A explains the issues of the controversy to the undergraduates while expert B listens. At some point B's face may light up. "So that's what you've been trying to say! Now I get it." Or maybe the good effects will have to wait until it is B's turn to explain to the same undergraduates what the issues are, and provoking just such a welcome reaction in A. It may not go perfectly, but it usually goes well and everybody benefits. The experts dissolve some of the artifactual misunderstandings between their positions, and the undergraduates get a first-rate educational experience.
Several times I have set up such exercises at Tufts, thanks to generous support from the administration. I handpick a small group of undergraduates (less than a dozen) and brief them on their role: they are not to accept anything they don't understand. They will be expected to raise their hands, to interrupt, to alert the experts to anything they find confusing or vague. (They do get required reading to pore over beforehand so that they are not utter novices on the topic; they are interested amateurs.) They love the role, and so they should; they are being given made-to-order tutorials from some big guns. The experts, meanwhile, often find that being set the task (well in advance) to explain their position under these conditions helps them find better ways of making their points than they had ever found before. Sometimes these experts have been "protected" for years by layers of fellow experts, postdocs, and advanced graduate students, and they really need the challenge.











8. Jootsing
It is hard to find an application of Occam's Broom, since it operates by whisking inconvenient facts out of sight, and it is even harder to achieve what Doug Hofstadter (1979, 1985) calls jootsing, which stands for " jumping out of the system." This is an important tactic not just in science and philosophy, but also in the arts. Creativity, that ardently sought but only rarely found virtue, often is a heretofore unimagined violation of the rules of the system from which it springs. It might be the system of classical harmony in music, the rules for meter and rhyme in sonnets (or limericks, even), or the "canons" of taste or good form in some genre of art. Or it might be the assumptions and principles of some theory or research program. Being creative is not just a matter of casting about for something novel—anybody can do that, since novelty can be found in any random juxtaposition of stuff—but of making the novelty jump out of some system, a system that has become somewhat established, for good reasons. When an artistic tradition reaches the point where literally "anything goes," those who want to be creative have a problem: there are no fixed rules to rebel against, no complacent expectations to shatter, nothing to subvert, no background against which to create something that is both surprising and yet meaningful. It helps to know the tradition if you want to subvert it. That's why so few dabblers or novices succeed in coming up with anything truly creative.
Sit down at a piano and try to come up with a good new melody and you soon discover how hard it is. All the keys are available, in any combination you choose, but until you can find something to lean on, some style or genre or pattern to lay down and exploit a bit, or allude to, before you twist it, you will come up with nothing but noise. And not just any violation of the rules will do the trick. I know there are at least two flourishing—well, surviving—jazz harpists, but setting out to make your name playing Beethoven on tuned bongo drums is probably not a good plan. Here is where art shares a feature with science: there are always scads of unexamined presuppositions of any theoretical set-to, but trying to negate them one at a time until you find a vulnerable one is not a good recipe for success in science or philosophy. (It would be like taking a Gershwin melody and altering it, one note at a time, looking for a worthy descendant. Good luck! Almost always, mutations are deleterious.) It's harder than that, but sometimes you get lucky.
Advising somebody to make progress by jootsing is rather like advising an investor to buy low and sell high. Yes, of course, that's the idea, but how do you manage to do it? Notice that the investment advice is not entirely vacuous or unusable, and the call for jootsing is even more helpful, because it clarifies what your target looks like if you ever catch a glimpse of it. (Everybody knows what more money looks like.) When you are confronting a scientific or philosophical problem, the system you need to jump out of is typically so entrenched that it is as invisible as the air you breathe. As a general rule, when a long-standing controversy seems to be getting nowhere, with both "sides" stubbornly insisting they are right, as often as not the trouble is that there is something they both agree on that is just not so. Both sides consider it so obvious, in fact, that it goes without saying. Finding these invisible problem-poisoners is not an easy task, because whatever seems obvious to these warring experts is apt to seem obvious, on reflection, to just about everybody. So the recommendation that you keep an eye out for a tacit shared false assumption is not all that likely to bear fruit, but at least you're more likely to find one if you're hoping to find one and have some idea of what one would look like.
Sometimes there are clues. Several of the great instances of jootsing have involved abandoning some well-regarded thing that turned out not to exist after all. Phlogiston was supposed to be an element in fire, and caloric was the invisible, self-repellent fluid or gas that was supposed to be the chief ingredient in heat, but these were dropped, and so was the ether as a medium in which light traveled the way sound travels through air and water. But other admirable jootsings are additions, not subtractions: germs and electrons and—maybe even—the many-worlds interpretation of quantum mechanics! It's never obvious from the outset whether we should joots or not. Ray Jackendoff and I have argued that we must drop the almost always tacit assumption that consciousness is the "highest" or "most central" of all mental phenomena, and I have argued that thinking of consciousness as a special medium (rather like the ether) into which contents get transduced or translated is a widespread and unexamined habit of thought that should be broken. Along with many others, I have also argued that if you think it is simply obvious that free will and determinism are incompatible, you're making a big mistake. More about those ideas later.
Another clue: sometimes a problem gets started when somebody way back when said, "Suppose, for the sake of argument, that ... ," and folks agreed, for the sake of argument, and then in the subsequent parry and thrust everybody forgot how the problem started! I think that occasionally, at least in my field of philosophy, the opponents are enjoying the tussle so much that neither side wants to risk extinguishing the whole exercise by examining the enabling premises. Here are two ancient examples, which of course are controversial: (1) "Why is there something rather than nothing?" is a deep question in need of an answer. (2) "Does God command something because it is good, or is something good because God commands it?" is another important question. I guess it would be wonderful if somebody came up with a good answer to either of these questions, so I admit that my calling them pseudo-problems not worth anybody's attention is not very satisfying, but that doesn't show that I'm wrong. Nobody said the truth had to be fun.











9. Three Species of Goulding: Rathering, Piling On, and the Gould Two-Step
The late biologist Stephen Jay Gould was a virtuoso designer and exploiter of boom crutches. Here are three related species, of the genus Goulding, named by me in honor of their most effective wielder.
Rathering is a way of sliding you swiftly and gently past a false dichotomy. The general form of a rathering is "It is not the case that blahblahblah, as orthodoxy would have you believe; it is rather that suchandsuchandsuch—which is radically different." Some ratherings are just fine; you really must choose between the two alternatives on offer; in these cases, you are not being offered a false, but rather a genuine, inescapable dichotomy. But some ratherings are little more than sleight of hand, due to the fact that the word "rather" implies—without argument—that there is an important incompatibility between the claims flanking it.
Here is a fine example of rathering by Gould in the course of his account of punctuated equilibrium:

Change does not usually occur by imperceptibly gradual alteration of entire species but rather [my italics] by isolation of small populations and their geologically instantaneous transformation into new species. [1992b, p. 12]

This passage invites us to believe that evolutionary change could not be both "geologically instantaneous" and "imperceptibly gradual" at the same time. But of course it can be. In fact, that is just what evolutionary change must be, unless Gould is saying that evolution tends to proceed by saltations (giant leaps in Design Space)—but elsewhere he has insisted that he never ever endorsed saltationism. "Geologically instantaneous" speciation can happen over a "short" period of time—let's say fifty thousand years, an elapse of time barely detectable in most geological strata. During that brief moment a typical member of a species might increase in height from, say, half a meter to one meter, a 100 percent increase, but at a rate of a millimeter every century, which strikes me as an imperceptibly gradual change.
Let's make up some other examples of rathering, to make sure the nature of the trick is clear.

It is not that people are mere "moist robots" (as Dilbert says, with the concurrence of most researchers in cognitive science); it is rather that people have free will, and are morally responsible for their good and bad deeds.

Again, why not both? What is missing is an argument to the effect that "moist robots" cannot also be people with free will who are morally responsible. This example plays on a common—but controversial—assumption. Here's another:

Religion is not the opiate of the masses, as Marx said; it is rather a deep and consoling sign of humanity's recognition of the inevitability of death.

Yet again, why can't it be both the opiate and a consoling sign? I think you get the point by now, and you can hunt for ratherings in a document more easily than you can hunt for false dichotomies, which never get announced as such; just type "rather" in your search box and see what comes up. Remember: not all "rather"s are ratherings; some are legitimate. And some ratherings don't use the word "rather." Here is one that uses the terser for "———, not ———"; I made it up from elements in the work of several ideologues of cognitive science.

Nervous systems need to be seen as actively generating probes of their environment, not as mere computers acting passively on inputs fed to them by sense organs.

Who says computers acting on inputs fed to them can't actively generate probes? This familiar contrast between drearily "passive" computers and wonderfully "active" organisms has never been properly defended, and is one of the most ubiquitous imagination-blockers I know.
A variation on rathering used frequently by Gould may be called piling on:

We talk about the "march from monad to man" (old-style language again) as though evolution followed continuous pathways of progress along unbroken lineages. Nothing could be further from reality. [1989a, p. 14]

What could not be further from reality? At first it might appear as if Gould was saying that there is no continuous, unbroken lineage between the "monads" (single-celled organisms) and us, but of course there is. There is no more secure implication of Darwin's great idea than that. So what can Gould be saying here? Presumably we are meant to put the emphasis on "pathways of progress"—it is (only) the belief in progress that is "far from reality." The pathways are continuous, unbroken lineages all right, but not lineages of (global) progress. This is true: they are (unbroken) continuous lineages of (mainly) local progress. We come away from this passage from Gould—unless we are wary—with the sense that he has shown us something seriously wrong with the standard proposition of evolutionary theory that there are continuous pathways (unbroken lineages) from monads to man. But, to use Gould's own phrase, "Nothing could be further from reality."
Yet another trick of his is the Gould Two-Step, a device I described in print some years ago, which was then named by the evolutionary theorist Robert Trivers (personal correspondence, 1993), in honor of its inventor:

In the first stage, you create the strawperson, and "refute" it (everybody knows that trick). Second (this is the stroke of genius), you yourself draw attention to the evidence that you have taken the first step—the evidence that your opponents don't in fact hold the view you have attributed to them—but interpret these citations as their grudging concessions to your attack! [Dennett, 1993, p. 43]

In my essay, a letter to the editor of the New York Review of Books (1993), where Gould had two months earlier savagely criticized Helena Cronin's fine book The Ant and the Peacock (November 19, 1992), I presented three examples of the Gould Two-Step. Here is the most portable of those examples:
The most transparent case is Gould's invention of "extrapolationism," described as a logical extension of "Cronin's adaptationism." This is a doctrine of pan-continuity and pangradualism that is conveniently—indeed trivially—refuted by the fact of mass extinction. "But if mass extinctions are true breaks in continuity, if the slow building of adaptation in normal times does not extend into predicted success across mass extinction boundaries, then extrapolationism fails and adaptationism succumbs." I cannot see why any adaptationist would be so foolish as to endorse anything like "extrapolationism" in a form so "pure" as to deny the possibility or even likelihood that mass extinction would play a major role in pruning the tree of life, as Gould puts it. It has always been obvious that the most perfect dinosaur will succumb if a comet strikes its homeland with a force hundreds of times greater than all the hydrogen bombs ever made. There is not a word in Cronin's book that supports his contention that she has made this error. If Gould thinks the role of mass extinctions in evolution is relevant to either of the central problems Cronin addresses, sexual selection and altruism, he does not say how or why. When Cronin turns, in her last chapter, to a fine discussion of the central question in evolutionary theory she has not concentrated on, the origin of species, and points out that it is still an outstanding problem, Gould pounces on this as a last minute epiphany, an ironic admission of defeat for her "panadaptationism." Preposterous! [p. 44]

There is a good project for a student of rhetoric: combing through Gould's huge body of publications and cataloguing the different species of boom crutch he exploited, beginning with rathering, piling on, and the Gould Two-Step.











10. The "Surely" Operator: A Mental Block
When you're reading or skimming argumentative essays, especially by philosophers, here is a quick trick that may save you much time and effort, especially in this age of simple searching by computer: look for "surely" in the document, and check each occurrence. Not always, not even most of the time, but often the word "surely" is as good as a blinking light locating a weak point in the argument, a warning label about a likely boom crutch. Why? Because it marks the very edge of what the author is actually sure about and hopes readers will also be sure about. (If the author were really sure all the readers would agree, it wouldn't be worth mentioning.) Being at the edge, the author has had to make a judgment call about whether or not to attempt to demonstrate the point at issue, or provide evidence for it, and—because life is short—has decided in favor of bald assertion, with the presumably well-grounded anticipation of agreement. Just the sort of place to find an ill-examined "truism" that isn't true!
I first noticed this useful role of "surely" when commenting on an essay by Ned Block (1994), which included several prime examples directed against my theory of consciousness. Here's one, conveniently italicized* by Block to emphasize its obviousness typographically:
But surely it is nothing other than a biological fact about people—not a cultural construction—that some brain representations persevere enough to affect memory, control behavior, etc. [p. 27]

This is meant to dismiss—without argument—my theory of human consciousness as something that must, in effect, be learned, a set of cognitive micro-habits that are not guaranteed to be present at birth. "Wherever Block says 'Surely,' " I said, "look for what we might call a mental block" (Dennett, 1994a, p. 549). Block is one of the most profligate abusers of the "surely" operator among philosophers, but others routinely rely on it, and every time they do, a little alarm bell should ring. "Here is where the unintended sleight-of-hand happens, whisking the false premise by the censors with a nudge and a wink" (Dennett, 2007b, p. 252).
I decided recently to test my hunch about "surely" a bit more systematically. I went through dozens of papers—about sixty—on the philosophy of mind at philpapers.org/ and checked for occurrences of "surely." Most papers did not use the word at all. In those that did use it (between one and five times in the sample I checked), most instances were clearly innocent; a few were, well, arguable; and there were six instances where the alarm bell sounded loud and clear (for me). Of course others might have a very different threshold for obviousness, which is why I didn't bother tabulating my "data" in this informal experiment. I encourage doubters to conduct their own surveys and see what they find. A particularly egregious example of the "surely" operator will be dismantled in detail later, in chapter 64.











11. Rhetorical Questions
Just as you should keep a sharp eye out for "surely," you should develop a sensitivity for rhetorical questions in any argument or polemic. Why? Because, like the use of "surely," they represent an author's eagerness to take a short cut. A rhetorical question has a question mark at the end, but it is not meant to be answered. That is, the author doesn't bother waiting for you to answer since the answer is so flipping obvious that you'd be embarrassed to say it! In other words, most rhetorical questions are telescoped reductio ad absurdum arguments, too obvious to need spelling out. Here is a good habit to develop: Whenever you see a rhetorical question, try—silently, to yourself—to give it an unobvious answer. If you find a good one, surprise your interlocutor by answering the question. I remember a Peanuts cartoon from years ago that nicely illustrates the tactic. Charlie Brown had just asked, rhetorically, "Who's to say what is right and wrong here?" and Lucy responded, in the next panel, "I will."











12. What Is a Deepity?
My late friend, the computer scientist Joseph Weizenbaum had a yearning to be a philosopher and tried late in his career to gravitate from technicalities to profundities. He once told me that one evening, after holding forth with high purpose and furrowed brow at the dinner table, his young daughter Miriam said, "Wow! Dad just said a deepity!" What a wonderful impromptu coinage!* I decided to adopt it and put it to somewhat more analytic use.
A deepity is a proposition that seems both important and true—and profound—but that achieves this effect by being ambiguous. On one reading it is manifestly false, but it would be earth-shaking if it were true; on the other reading it is true but trivial. The unwary listener picks up the glimmer of truth from the second reading, and the devastating importance from the first reading, and thinks, Wow! That's a deepity.
Here is an example. (Better sit down: this is heavy stuff.)

Love is just a word.

Oh wow! Cosmic. Mind-blowing, right? Wrong. On one reading, it is manifestly false. I'm not sure what love is—maybe an emotion or emotional attachment, maybe an interpersonal relationship, maybe the highest state a human mind can achieve—but we all know it isn't a word. You can't find love in the dictionary!
We can bring out the other reading by availing ourselves of a convention philosophers care mightily about: when we talk about a word, we put it in quotation marks, thus:

"Love" is just a word.

This is true; "love" is an English word, but just a word, not a sentence, for example. It begins with "L" and has four letters and appears in the dictionary between "lousy" and "low-browed," which are also just words. "Cheeseburger" is just a word. "Word" is just a word.
But this isn't fair, you say. Whoever said that love is just a word meant something else, surely. No doubt, but they didn't say it. Maybe they meant that "love" is a word that misleads people into thinking that it is the term for something wonderful that doesn't really exist at all, like "unicorn," or maybe they meant that the word was so vague that nobody could ever know whether it referred to any particular thing or relation or event. But neither of these claims is actually very plausible. "Love" may be a troublesome, hard-to-define word, and love may be a hard-to-be-sure-about state, but those claims are obvious, not particularly informative or profound.
Not all deepities are quite so easily analyzed. Richard Dawkins recently alerted me to a fine deepity by Rowan Williams, the Archbishop of Canterbury, who described his faith as a

silent waiting on the truth, pure sitting and breathing in the presence of the question mark.

I leave the analysis of this as an exercise for you.











Summary
A tool wielded well becomes almost as much a part of you as your hands and feet, and this is especially true of tools for thinking. Equipped with these simple all-purpose thinking tools, you can approach the difficult explorations ahead with sharper senses: you can see an opening, hear a warning bell, smell a rat, or feel a misstep that you might well miss without their help. You also have some maxims to bear in mind—Rapoport's Rules and Sturgeon's Law, for instance—that can whisper advice in your ear like Jiminy Cricket, reminding you to control your aggression as you plunge boldly into the thicket swinging your weapons. Yes, thinking tools are also weapons, and the imagery of combat is appropriate. Competitiveness is, apparently, a natural by-product of the intellectual ambition and boldness required to tackle the toughest problems. We've seen that in the heat of battle even great thinkers can resort to dirty tricks in their eagerness to get you to see things their way, and constructive criticism can quickly shade into ridicule when an opportunity to launch a zinger arises.
The problems we will be confronting are all hot-button issues: meaning, evolution, consciousness, and especially free will. You will feel dread or repugnance welling up as you approach some of the prospects, and rest assured that you are not alone in this; even the most vaunted experts are susceptible to wishful thinking and can be blinded to a truth by a conviction that is supported more by emotional attachment than reason. People really care about whether they have free will or not, about how their minds can reside in their bodies, and about how—and even whether—there can be meaning in a world composed of nothing but atoms and molecules, photons and Higgs bosons. People should care. What could be more important, in the end, than these questions: What in the world are we, and what should we do about it? So watch your step. There is treacherous footing ahead, and the maps are unreliable.














III: TOOLS FOR THINKING ABOUT MEANING OR CONTENT

 

Why begin with meaning? I am starting with meaning because it is at the heart of all the tough problems, for a simple reason: these problems don't arise until we start talking about them, to ourselves and others. Badgers don't worry about free will, and even dolphins can't be nagged by the problem of consciousness, because asking questions is not part of their repertoire. Curiosity may have killed the cat, but it drives us reflective human beings into thickets of bafflement. Now, maybe this is the down side of language, and we'd be better off—happier, healthier mammals—if we were as oblivious to these issues as our great ape kin are. But since we have language, we're stuck with the Big Questions, and for better or worse, they don't strike us as trivial.
The first step in any effective exploration is to get as clear as we can about our starting point and our equipment. Words have meanings. How is this possible? We word-users mean things by saying things. How is this possible? How can we understand each other? Our dogs seem to be able to "understand" (sort of) a few words, even a few hundred words, but aside from such domesticated stunts, and the rudimentary signaling systems found in nature (in primates, in birds, ... in cuttlefish!), words are what distinguish our minds from all other animal minds. That is a striking difference, but it still seems that nonhuman animals—"higher" animals—have minds, and hence in some perhaps limited way deal with meanings: the meanings of their perceptual states, their urges and desires, and even their dreams.
Sometimes animals strike us as very much like us, as people dressed up in cat costumes, bearskins, and dolphin suits. This is true in every human culture: animals are seen as seeing, knowing, wanting, trying, fearing, deciding, lusting, remembering, and so forth. In short, they are seen as like us in having minds filled with meaningful ... something-or-others (ideas? beliefs? mental representations?). How can there be meanings in brains? A perennially tempting idea is that since words have meanings, maybe the meaningful things in our brains—and in animal brains—are like words, composed into mental sentences, expressing beliefs, and so on. But if words get their meanings from the minds that utter them, from where do mindwords get their meanings? Do animal brains store both the mindwords and their definitions, in a sort of cerebral dictionary? And if animals—at least "higher" animals—have always had brains full of mindwords, why can't they talk?* The idea of a language of thought is deeply problematic, but our thoughts and beliefs must be composed of something. What else could it be?†











13. Murder in Trafalgar Square
Here is our first intuition pump. Jacques shoots his uncle dead in Trafalgar Square and is apprehended on the spot by Sherlock; Tom reads about it in the Guardian, and Boris learns of it in Pravda. Now Jacques, Sherlock, Tom, and Boris have had remarkably different experiences—to say nothing of their earlier biographies and future prospects—but there is one thing they share: they all believe that a Frenchman has committed murder in Trafalgar Square. They did not all say this, not even "to themselves"; that proposition did not, we can suppose, "occur to" any of them, and even if it had, it would have had very different import for Jacques, Sherlock, Tom, and Boris. Yet they all believe that a Frenchman committed murder in Trafalgar Square. This is a shared property that is visible, in effect, only from one very limited point of view—the point of view of folk psychology. Ordinary folk psychologists—all of us—have no difficulty imputing such useful commonalities to people. We do it without knowing much of anything about what lies between the ears of those to whom we attribute those beliefs. We may think that these four fellows must also have something else in common—a similarly shaped something-or-other in their brains that somehow registers their shared belief—but if so, we are lapsing into dubious theorizing. There may in fact be some such neural structures in common—if all four brains happen to "spell" the belief that a Frenchman has committed murder in Trafalgar Square the same way—but this is not at all necessary, and is in fact very unlikely, for reasons we will briefly explore here.
The two sentences "I'm hungry" and "J'ai faim" share a property, in spite of being composed of differently shaped letters (or different phonemes when uttered aloud), in different languages, and having different grammatical structures: they mean, or are about, the same thing: the speaker's hunger. This shared property, the meaning (of the two sentences in their respective languages), or the content (of the beliefs they express), is a central topic in philosophy and cognitive science. This aboutness that, for example, sentences, pictures, beliefs, and (no doubt) some brain states exhibit, is known in philosophical jargon as intentionality, an unfortunate choice as a technical term, since outsiders routinely confuse it with the everyday idea of doing something intentionally (as in "Are your intentions honorable?"). Here is a sentence to remind you of the differences: A cigarette is not about smoking, or about anything else, in spite of the fact that it is intended to be smoked; a "NO SMOKING" sign is about smoking and hence exhibits intentionality; the belief that there's a mugger behind that tree exhibits intentionality (it's about a—possibly nonexistent—mugger), but it is surely not intentional in the ordinary sense (you don't "believe it on purpose"; it just comes to you); running away from the tree is intentional in the ordinary sense, but isn't about anything. If you simply make a habit of substituting the awkward word "aboutness" whenever you encounter the philosophical term "intentionality," you will seldom go wrong. Aside from the agreement that meaning and content are intimately related and mutually dependent phenomena, or even a single phenomenon (intentionality), there is still precious little consensus about what content (or meaning) is and how best to capture it. That's why we have to approach this topic gingerly. It's a feast of problems, but we can sneak up on the questions, taking small bites.
This example of the four different believers is meant to demonstrate how brains could have little in common while still sharing an "intentional" property: believing "the same thing." Jacques was an eyewitness—indeed, the perpetrator—of the murder, Sherlock's experiential intimacy with the event is only slightly less direct, but Tom and Boris learned this fact in strikingly different terms. There are indefinitely many different ways of acquiring the information that a Frenchman committed murder in Trafalgar Square and indefinitely many different ways of using that information to further one's projects (answering questions on quiz shows, winning bets, titillating French tourists in London, etc.). If there is a good reason to think that all these sources and outcomes must funnel through some common structure in the brain, we'll discover it eventually, but in the meantime we shouldn't jump to conclusions.
Before leaving this intuition pump we should follow Doug Hofstadter's advice and turn the knobs to see what its parts are doing. Why did I choose the particular proposition I did? Because I needed something memorable and striking enough to get reported in different languages many miles away from the scene. Most of our belief acquisition isn't like that at all, and it is worth noting this. For instance, Jacques, Sherlock, Tom, and Boris share indefinitely many other beliefs that did not arise so dramatically: the beliefs that, for instance, chairs are larger than shoes, that soup is liquid, that elephants don't fly. If I try to inform you that salmon in the wild don't wear hearing aids, you will tell me that this is not news to you, but when did you learn it? You weren't born knowing it, it was not part of any curriculum at school, and it is extremely unlikely that you ever framed a sentence in your mind to this effect. So while it might seem obvious that Boris must have learned about the Frenchman by "simply uploading" the relevant Russian sentence in Pravda into his brain, and then "translating" it into, oh, Brainish, there is nothing obvious about the supposition that Boris's brain performed a similar clerical job (from what into Brainish?) for the fact about salmon.
Here's another knob: suppose Fido, a dog, and Clyde, a pigeon, are also eyewitnesses of the murder; they may carry away something about the event, an adjustment in their brains that could influence later behavior, but it wouldn't be the fact that a Frenchman committed murder in Trafalgar Square, even though the information sustaining that fact might be present in the light and sound that impinged on their sense organs. (A videotape of the event might provide legal evidence of this, for instance, but it would be lost on Fido and Clyde.) So this intuition pump risks carrying a seriously anthropocentric bias into our exploration of meaning. Words and sentences are exemplary vehicles of meaning, but for animals that don't use them, the idea that their brains nevertheless use them is at least far-fetched—which doesn't make it false. If it turns out to be true, it will be a most eye-opening discovery, but we've had those before.
The phenomena of intentionality are both utterly familiar—as salient in our daily lives as our food, furniture, and clothes—and systematically elusive from scientific perspectives. You and I seldom have difficulty distinguishing a birthday greeting from a death threat from a promise, but consider the engineering task of making a reliable death-threat-detector. What do all death threats have in common? Only their meaning, it seems. And meaning is not like radioactivity or acidity, a property readily discriminated by a well-tuned detector. The closest we have come yet to creating a general-purpose meaning-detector is IBM's Watson, which is much better at sorting by meanings than any earlier artificial intelligence system, but notice that it is not at all simple, and would still (probably) misidentify some candidates for death threats that a child would readily get. Even small children recognize that when one laughing kid yells to another, "So help me, I'll kill you if you do that again!" this is not really a death threat. The sheer size and sophistication of Watson are at least indirect measures of how elusive the familiar property of meaning is.











14. An Older Brother Living in Cleveland
Still, meaning is not an utterly mysterious property. One way or another, structures in our brains somehow "store" our beliefs. When you learn that pudus are mammals, something has to change in your brain; something has to become relatively fixed in a way it wasn't fixed before you learned this, and whatever it is must have enough aboutness, one way or another, to account for your newfound ability to identify pudus as closer kin to buffalos than to barracudas. So it is indeed tempting to imagine that beliefs are "stored in the brain" rather the way data files are stored on your hard disk, in some systematic code—which might be different in each individual, as different as fingerprints. Jacques's beliefs would be written in his brain in Jacquish, and Sherlock's in Sherlockish. But there are problems with this attractive idea.
Suppose we have entered the golden age of neurocryptography, and it becomes possible for a "cognitive micro-neurosurgeon" to do a bit of tinkering and insert a belief into a person's brain, writing the relevant proposition in the person's neurons, using the local brain language, of course. (If we can learn to read brain-writing, presumably we can write brain-writing, if our tools are delicate enough.) Let us suppose we are going to insert into Tom's brain the following false belief: I have an older brother living in Cleveland. Let us suppose the cognitive micro-neurosurgeon can do the requisite rewiring, as much and as delicate as you please. This rewiring will either impair Tom's basic rationality or not. Consider the two outcomes. Tom is sitting in a bar and a friend asks, "Do you have any brothers or sisters?" Tom says, "Yes, I have an older brother living in Cleveland." "What's his name?" Now what is going to happen? Tom may reply, "Name? Whose name? Oh my gosh, what was I saying? I don't have an older brother! For a moment, there, it seemed to me that I had an older brother living in Cleveland!" Alternatively, he may say, "I don't know his name," and when pressed he will deny all knowledge of this brother and assert things like "I am an only child and have an older brother living in Cleveland." In neither case has our cognitive micro-neurosurgeon succeeded in wiring in a new belief. In the first case, Tom's intact rationality wipes out the (lone, unsupported) intruder as soon as it makes an appearance. An evanescent disposition to say, "I have an older brother living in Cleveland" isn't really a belief—it's more in the nature of a tic, like a manifestation of Tourette's syndrome. And if poor Tom persists with this pathology, as in the second alternative, his frank irrationality on the topic of older brothers disqualifies him as a believer. Anybody who doesn't understand that you can't be an only child and have an older brother living in Cleveland really doesn't understand the sentence he asserted, and what you really don't understand you may "parrot" but you can't believe.
This science-fiction example highlights the tacit presumption of mental competence that underlies all belief attributions; unless you have an indefinitely extensible repertoire of ways to use your candidate belief (if that is what it is) in different contexts, it is not a belief in any remotely recognizable sense. If the surgeon has done the work delicately, preserving the competence of the brain, that brain will undo this handiwork as soon as the issue arises—or else, pathologically, the brain will surround the handiwork with layers of pearly confabulation ("His name is Sebastian, and he's a circus acrobat who lives in a balloon"). Such confabulation is not unknown; people suffering from Korsakoff's syndrome (the amnesia that often afflicts alcoholics) can be astonishingly convincing in spinning tales of their "remembered" pasts that have not a shred of truth in them. But this very elaboration is clear evidence that the person doesn't just have an isolated "proposition" stored in her brain; even a delusional belief requires the support of a host of non-delusional beliefs and the ability to acknowledge the implications of all this. If she doesn't believe her older brother also is male, breathing, west of Boston, north of Panama, and so on and so forth, then it would be worse than misleading to say that the surgeon's feat was inserting a belief.
What this intuition pump shows is that nobody can have just one belief. (You can't believe a dog has four legs without believing that legs are limbs and four is greater than three, etc.)* It shows other things as well, but I won't pause to enumerate them. Nor will I try to say now how one might use a variation on this very specific thinking tool for other purposes—though you are invited to turn the knobs yourself, to see what you come up with. I want to get a varied assortment of such thinking tools on display before we reflect more on their features.











15. "Daddy Is a Doctor"
A young child is asked what her father does, and she answers, "Daddy is a doctor." Does she believe what she says? In one sense, of course, but what would she have to know to really believe it? (What if she'd said, "Daddy is an arbitrager" or "Daddy is an actuary"?) Suppose we suspected that she was speaking without understanding, and decided to test her. Must she be able to produce paraphrases or to expand on her claim by saying her father cures sick people? Is it enough if she knows that Daddy's being a doctor precludes his being a butcher, a baker, a candlestick maker? Does she know what a doctor is if she lacks the concept of a fake doctor, a quack, an unlicensed practitioner? For that matter, how much does she need to understand to know that Daddy is her father? (Her adoptive father? Her "biological" father?) Clearly her understanding of what it is to be a doctor, as well as what it is to be a father, will grow over the years, and hence her understanding of her own sentence, "Daddy is a doctor," will grow. Can we specify—in any nonarbitrary way—how much she must know in order to understand this proposition "completely"? If understanding comes in degrees, as this example shows, then belief, which depends on understanding, must come in degrees as well, even for such mundane propositions as this. She "sorta" believes her father is a doctor—which is not to say she has reservations or doubts, but that she falls short of the understanding that is an important precondition for any useful concept of belief.











16. Manifest Image and Scientific Image
It's time to erect some staging before proceeding in our quest to understand what meanings are. Here is a thinking tool that provides a valuable perspective on so many issues that it should be in everybody's kit, but so far it hasn't spread far from its home in philosophy. The philosopher Wilfrid Sellars devised it in 1963 to clarify thinking on what science shows us about the world we live in. The manifest image is the world as it seems to us in everyday life, full of solid objects, colors and smells and tastes, voices and shadows, plants and animals, and people and all their stuff: not only tables and chairs, bridges and churches, dollars and contracts, but also such intangible things as songs, poems, opportunities, and free will. Think of all the puzzling questions that arise when we try to line up all those things with the things in the scientific image: molecules, atoms, electrons, and quarks and their ilk. Is anything really solid? The physicist Sir Arthur Eddington wrote, early in the twentieth century, about the "two tables," the solid one of everyday experience and the one composed of atoms, widely separated in mainly empty space, more like a galaxy than a piece of wood. Some people said that what science showed was that nothing was really solid, solidity was an illusion, but Eddington knew better than to go that far. Some people have said that color is an illusion. Is it? Electromagnetic radiation in the narrow range that accounts for human vision (the range in between infrared and ultraviolet) is not made of little colored things, and atoms, even gold atoms, aren't colored. But still, color is not an illusion in the sense that matters: nobody thinks Sony is lying when it says that its color televisions really show the world of color, or that Sherwin-Williams should be sued for fraud for selling us many different colors in the form of paint. How about dollars? These days the vast majority of them aren't made of silver or even paper. They are virtual, made of information, not material, just like poems and promises. Does that mean that they are an illusion? No, but don't hunt for them among the molecules.
Sellars (1963, p. 1) famously said, "The aim of philosophy, abstractly formulated, is to understand how things in the broadest possible sense of the term hang together in the broadest possible sense of the term." That is the best definition of philosophy I have encountered. The task of figuring out how to put all the familiar things in our manifest image into registration with all the relatively unfamiliar things of the scientific image is not a job that scientists are especially well equipped to do. Please tell me, Dr. Physicist, just what a color is. Are there any colors according to your theory? Dr. Chemist, can you provide the chemical formula for a bargain? Surely (ding!) there are bargains. What are they made of? Hmm. Maybe there aren't any bargains, not really! But then what's the difference—the chemical difference?—between something that is a bargain and something that only seems to be a bargain? We could go on in this vein, looking at a host of puzzlers that only philosophers have tried hard to resolve, but instead, let's step back, as Sellars invites us to do, and look at the fact that there are these two remarkably different perspectives on the world. Why are there two? Or are there many? Let's try to answer this question by starting in the scientific image and seeing if we can spot the emergence of the manifest image from that vantage point.
Every organism, whether a bacterium or a member of Homo sapiens, has a set of things in the world that matter to it and which it therefore needs to discriminate and anticipate as best it can. Philosophers call a list of things deemed to exist an ontology (from the Greek word for "thing," surprise, surprise). Every organism thus has an ontology. Another name for it is the organism's Umwelt (von Uexküll, 1957; Umwelt is the German word for "environment," surprise, surprise). An animal's Umwelt consists in the first place of affordances (Gibson, 1979), things to eat or mate with, or shun, openings to walk through or look out of, holes to hide in, things to stand on, and so forth. An organism's Umwelt is in one sense an inner environment, a "subjective" and even "narcissistic" ontology, composed of only the things that most matter to it, but its Umwelt is not necessarily inner or subjective in the sense of being conscious. Umwelt is really an engineering concept; consider the ontology of a computer-controlled elevator, the set of all the things it needs to keep track of in order to do its job.* One of von Uexküll's studies was of the Umwelt of a tick. We may suppose that the Umwelt of a starfish or worm or daisy is more like the ontology of the elevator than like our Umwelt, which is, in fact, our manifest image.
Our manifest image, unlike the daisy's ontology or Umwelt, really is manifest, really is subjective in a strong sense. It's the world we live in, the world according to us.† Like the daisy's ontology, however, much of our manifest image has been shaped by natural selection over eons, and is part of our genetic heritage. One of my favorite examples of how different an Umwelt can be compares an anteater to an insectivorous bird (Wimsatt, 1980). The bird tracks individual insects and must deal with their erratic flight patterns by having a high flicker-fusion rate (in effect it sees more "frames per second" than we do, and hence a movie would be like a slide show to it). The anteater just averages over the whole ant-infested area and lets its big tongue take up the slack. A philosopher might say that "ant" for an anteater is a mass term, like "water" and "ice" and "furniture," not a sortal (you can count them) like "olives" and "raindrops" and "chairs." When an anteater sees a nice blob of ant, it slurps it up with its tongue, as oblivious to the individuals as we are to the individual glucose molecules we slurp up when we eat a sweet.
Most of our manifest image is not genetically inherited; it is somehow inculcated in our early childhood experience. Words are a very important category of thing for us, and are the medium through which much of our manifest image is transmitted, but the capacity to categorize some events in the world as words, and our desire to speak, may well be at least partly a genetically inherited talent—like the bird's capacity to make out individual flying insects, or a wasp's desire to dig a nest. Even without grammar to knit them together into sentences, words as mere labels can help bring important categories into sharply focused existence in our manifest image: Mommy, doggie, cookie. Could you ever frame a clear concept of a bargain, or a mistake, or a promise—let alone a home run or a bachelor—without the help of the words I've just used to mention them? Our consideration of Doug Hofstadter's list of favorite thinking tools has already shown us how terms can structure and flavor our minds, enriching our personal manifest images with things—loose cannons and lip service and feedback—that are otherwise almost invisible.











17. Folk Psychology
Probably the most important pattern in our manifest image, because it anchors so many other categories that matter to us, is the pattern I call folk psychology. I coined the term in its current meaning in 1981, but it apparently had an earlier incarnation in the writings of Wilhelm Wundt and Sigmund Freud and others (Volkpsychologie), where it meant something about national character (the Geist of the German Volk—you don't want to know). This was an antecedent I had missed, as did many others who adopted the term. I proposed folk psychology as a term for the talent we all have for interpreting the people around us—and the animals and the robots and even the lowly thermostats—as agents with information about the world they act in (beliefs) and the goals (desires) they strive to achieve, choosing the most reasonable course of action, given their beliefs and desires.
Some researchers like to call folk psychology "theory of mind" (or just TOM), but this strikes me as misleading, since it tends to prejudge the question of how we manage to have such a talent, suggesting that we have, and apply, a theory. In a similar spirit, we would have to say that you have a bicycle theory if you know how to ride a bicycle, and it's your nutrition theory that accounts for your ability to avoid starving to death and refrain from eating sand. This doesn't strike me as a useful way of thinking about these competences. Since everybody agrees that we have the interpretive talent, and everybody does not agree about how we manage to be so competent, I think it's best to keep "theory" out of it and to use a somewhat more neutral term for the time being. Academic or scientific psychology is also in the business of explaining and predicting the minds of others, and it really does have theories: behaviorism, cognitivism, neurocomputational models, Gestalt psychology, and a host of others. Folk psychology is a talent we excel in without formal education. Folk physics then, in parallel fashion, is the talent we have for expecting liquids to flow, unsupported things to drop, hot substances to burn us, water to quench our thirst, and rolling stones to gather no moss. It's another interesting question, how our brains manage to generate these almost always correct expectations so effortlessly, even if we've never taken a physics course.
Folk psychology is "what everyone knows" about their minds and the minds of others: people can feel pain or be hungry or thirsty and know the difference, they can remember events from their past, anticipate lots of things, see what is in front of their open eyes, hear what is said within earshot, deceive and be deceived, know where they are, recognize others, and so forth. The confidence with which we make these assumptions is breathtaking, given how little we know about what is actually going on inside the heads of these people (to say nothing of other animals). So sure are we about all this that it takes some strenuous distancing even to notice that we're doing it.
Artists and philosophers agree on one thing: one of their self-appointed tasks is to "make the familiar strange."* Some of the great strokes of creative genius get us to break through the crust of excessive familiarity, jootsing into the new perspective where we can look at ordinary, obvious things with fresh eyes. Scientists couldn't agree more. Newton's mythic moment was asking himself the weird question about why the apple fell down from the tree. ("Well, why wouldn't it?" asks the everyday non-genius. "It's heavy!"—as if this were a satisfactory explanation.) If you are not blind and have blind friends, you will probably confirm my hunch that no matter how much time you've spent in their company, you still find yourself occasionally using your hands to point to things or draw explanatory shapes in front of their unseeing eyes, in order to get them to believe and understand what you believe and understand. The "default" expectation when you are in the presence of another awake human being is that both of you can see the same things, hear the same things, smell the same things. You drive down the highway at sixty miles per hour, unperturbed by the fact that another car is coming your way in the opposite lane at the same speed. How do you know there won't be a terrible collision? You unthinkingly assume that the driver (whom you can't even see, and almost certainly don't know) wants to stay alive and knows that the best way of doing that is staying on the right side of the road. Notice that all it would take to put you in a more anxious frame of mind would be a radio news report informing the world that a new robotic car-driving system was being tested on your route today. Yes, it's designed to be safe, and after all, Google created it, but your confidence in the rationality and knowledge of the average oncoming human driver exceeds that of the vaunted robot (except perhaps late on Saturday night).
How come "everybody knows" folk psychology? Is it, after all, TOM, a sort of theory that you learn as a child? Is any of it innate, or if we learn it all, how do we do that and when? There has been a flood of research on these questions in the last thirty years. My answer (still, after all these years of research) is that it is not so much a theory as a practice, a way of investigating the world that comes so naturally it must have some genetic basis in our brains. You do have to learn some of this "at mother's knee," and if you were somehow deprived of all contact with other human beings as you grew up, you would probably be strikingly inept in folk psychology (along with your other serious disabilities), but the urge is very strong to interpret things that move in irregular ways (unlike a pendulum or a ball rolling down a hill) as agents. A favorite demonstration of this in introductory psychology courses is a brief animation made (using stop-action cinematography) by Fritz Heider and Mary-Ann Simmel in 1944. It shows two triangles and a circle moving around, in and out of a box. The geometric shapes couldn't look less like people (or animals), but it is almost irresistible to see their interactions as purposeful, driven by lust, fear, courage, and anger. This antique demo can be found on many websites, such as www.psychexchange.co.uk/videos/view/20452/.

We are born with an "agent detection device" (Barrett, 2000; see also Dennett, 1983), and it is on a hair trigger. When it misfires, as it often does in stressful circumstances, we tend to see ghosts, goblins, imps, leprechauns, fairies, gnomes, demons, and the like where all that is really there are waving branches, toppling stone walls, or creaking doors (Dennett, 2006a). From an early age we effortlessly and involuntarily see others as agents, and not just happy or angry or baffled or afraid, but as in on the secret or wondering which way to turn or even unwilling to accept the deal offered. It isn't brain surgery, or rocket science; it's easy. The power and ease of use of folk psychology is due, I have argued, to the simplifying assumptions that enable it. It is like an idealized model in science—maximally abstract and stripped down to the essentials. I call this the intentional stance.











18. The Intentional Stance
So far, so good. Almost all of us are good at folk psychology.* We have a talent for thinking of others (and ourselves) as having minds, a practice as effortless as breathing, most of the time. We depend on it without a qualm or a second thought, and it is stupendously reliable. Why is it so easy and how does it work? Here we need to pause and erect some more staging, which will make life much easier for us as we proceed with our explorations.
How does folk psychology work? We can get a good glimpse of the answer by noting how it works when we apply it to things that aren't other people. Suppose you are playing chess against a computer. You want to win, and the only good way of working toward that goal is to try to anticipate the computer's responses to your moves: "if I moved my bishop there, the computer would take it; if I moved the pawn instead, the computer would have to move its queen; ..." How do you know what the computer would do? Have you looked inside? Have you studied its chess-playing program? Of course not. You don't have to. You make your confident predictions on the quite obvious assumptions that the computer

"knows" the rules and "knows how" to play chess,
"wants" to win, and
will "see" these possibilities and opportunities for what they are, and act accordingly (that is, rationally).

In other words, you assume the computer is a good chess player, or at least not an idiotic, self-destructive chess player. You treat it, in other other words, as if it were a human being with a mind. In still further other words, when you use folk psychology to anticipate and understand its moves, you have adopted the intentional stance.
The intentional stance is the strategy of interpreting the behavior of an entity (person, animal, artifact, or whatever) by treating it as if it were a rational agent who governed its "choice" of "action" by a "consideration" of its "beliefs" and "desires."* The scare quotes around all these terms draw attention to the fact that some of their standard connotations may be set aside in the interests of exploiting their central features: their role in practical reasoning, and hence in the prediction of the behavior of practical reasoners. Anything that is usefully and voluminously predictable from the intentional stance is, by definition, an intentional system, and as we shall see, many fascinating and complicated things that don't have brains or eyes or ears or hands, and hence really don't have minds, are nevertheless intentional systems. Folk psychology's basic trick, that is to say, has some bonus applications outside the world of human interactions. (We will see applications not just in computer technology and cognitive neuroscience, but also in evolutionary and developmental biology, to name the most important areas.)
I propose we simply postpone the worrisome question of what really has a mind, about what the proper domain of the intentional stance is. Whatever the right answer to that question is—if it has a right answer—this will not jeopardize the plain fact that the intentional stance works remarkably well as a prediction method in these other areas, almost as well as it works in our daily lives as folk psychologists dealing with other people. This move of mine annoys and frustrates some philosophers, who want to blow the whistle and insist on properly settling the issue of what a mind, a belief, a desire is before taking another step. Define your terms, sir! No, I won't. That would be premature. I want to explore first the power and the extent of application of this good trick, the intentional stance. Once we see what it is good for, and why, we can come back and ask ourselves if we still feel the need for formal, watertight definitions. My move is an instance of nibbling on a tough problem instead of trying to eat (and digest) the whole thing from the outset. Many of the thinking tools I will be demonstrating are good at nibbling, at roughly locating a few "fixed" points that will help us see the general shape of the problem. In Elbow Room (1984a), I compared my method to the sculptor's method of roughing out the form in a block of marble, approaching the final surfaces cautiously, modestly, working by successive approximation. Many philosophers apparently cannot work that way and have to secure (or so they think) the utterly fixed boundaries of their problems and possible solutions before they can venture any hypotheses.
The three stances
So let's see where the power of the intentional stance comes from, by comparing it with other tactics of anticipation. Let's begin by identifying three main stances (which could be subdivided further for some purposes, but not ours here): the physical stance, the design stance, and the intentional stance.
The physical stance is simply the standard laborious method of the physical sciences, in which we use whatever we know about the laws of physics and the physical constitution of the things in question to devise our predictions. When I predict that a stone released from my hand will fall to the ground, I am using the physical stance. In general, for things that are neither alive nor artifacts, the physical stance is the only available strategy, though there are important exceptions, as we shall see. Every physical thing, whether designed or alive or not, is subject to the laws of physics and hence behaves in ways that in principle can be explained and predicted from the physical stance. If the thing I release from my hand is an alarm clock or a goldfish, I make the same prediction about its downward trajectory, on the same basis. Predicting the more interesting behaviors of alarm clocks and goldfish from the physical stance is seldom practical.
Alarm clocks, being designed objects (unlike a stone), are also amenable to a fancier style of prediction—prediction from the design stance. Suppose I categorize a novel object as an alarm clock: I can quickly reason that if I depress a few buttons just so, then some time later the alarm clock will make a loud noise. I don't need to work out the specific physical laws that explain this marvelous regularity; I simply assume that it has a particular design—the design we call an alarm clock—and that it will function properly, as designed. Design-stance predictions are riskier than physical-stance predictions because of the extra assumptions I have to take into account:

that an entity is designed as I suppose it to be, and
that it will operate according to that design—that is, it will not malfunction.

Designed things are occasionally misdesigned, and sometimes they break. Nothing that happens to, or in, a stone counts as its malfunctioning, since it has no function in the first place, and if it breaks in two, the result is two stones, not a single broken stone. When a designed thing is fairly complicated (a chainsaw in contrast to an ax, for instance), the moderate price one pays in riskiness is more than compensated for by the tremendous ease of prediction. Nobody would prefer to fall back on the fundamental laws of physics to predict the behavior of a chain saw when there is a handy diagram of its moving parts available to consult instead.
An even riskier and swifter stance is our main topic, the intentional stance, a subspecies of the design stance in which the designed thing is treated as an agent of sorts, with beliefs and desires and enough rationality to do what it ought to do given those beliefs and desires. An alarm clock is so simple that this fanciful anthropomorphism is, strictly speaking, unnecessary for our understanding of why it does what it does, but adoption of the intentional stance is more useful—indeed, well-nigh obligatory—when the artifact in question is much more complicated than an alarm clock. Let's slowly spell out the practice of adopting the intentional stance, focusing on our example of a chess-playing computer, to make sure we aren't overlooking anything important:

First, list the legal moves available to the computer when its turn to play comes up (usually there will be several-dozen candidates).
Now rank the legal moves from best (wisest, most rational) to worst (stupidest, most self-defeating).
Finally, make your prediction: the computer will make the best move.

You may well not be sure what the best move is (the computer may "appreciate" the situation better than you do!), but you can almost always eliminate all but four or five candidate moves, which still gives you tremendous predictive leverage. You could improve on this leverage and predict in advance exactly which move the computer will make—at a tremendous cost of time and effort—by falling back to the design stance. Get the "source code" (see chapter 27) of the program and then "hand simulate" it, running through the millions or billions of tiny steps the computer will take in the course of finding its response to the move you are thinking of making. This will definitely tell you what the computer would do if you made your move, but the time clock would run out—indeed your life would be over—long before you reached the conclusion. Too much information! But still, using the design stance is much easier than falling all the way back to the physical stance and trying to calculate the flow of electrons that results from pressing the computer's keys. So the physical stance is utterly impractical when trying to predict and explain the moves of the computer, and even the design stance is too much work, unless you get another computer to do it for you (which is cheating). The intentional stance finesses all that laborious information-gathering and computation and riskily settles on a pretty good bet: the computer will be "rational" enough to find and make the best move (given what it wants—to win—and what it knows—the positions and powers of all the pieces on the board). In many situations, especially when the best move for the computer to make is so obvious it counts as a "forced move," or a "no brainer," the intentional stance can predict its move with well-nigh perfect accuracy without much effort.
It is obvious that the intentional stance works effectively when the goal is predicting a chess-playing computer, since its designed purpose is to "reason" about the best move to make in the highly rationalistic setting of chess. If a computer program is running an oil refinery, it is almost equally obvious that its various moves will be made in response to its detection of conditions that more or less dictate what it should do, given its larger designed purposes. Here the presumption of excellence or rationality of design stands out vividly, since an incompetent programmer's effort might yield a program that seldom did what the experts said it ought to do in the circumstances. When information systems (or control systems) are well designed, the rationales for their actions will be readily discernible, and highly predictive—whether or not the engineers who wrote the programs attached "comments" to the source code explaining these rationales to onlookers, as good practice dictates. (More on this computer talk later.) We needn't know anything about computer programming to predict the behavior of the system; we need to know about the rational demands of running an oil refinery.
And now we can see why and how the intentional stance works to predict us. When we treat each other as intentional systems, we are similarly finessing our ignorance of the details of the processes going on in each other's skulls (and in our own!) and relying, unconsciously, on the fact that to a remarkably good first approximation, people are rational. If we are suddenly thrust into a novel human scenario, we can usually make sense of it effortlessly, indeed involuntarily, thanks to our innate ability to see what people ought to believe (the truth about what's put before them) and ought to desire (what's good for them).
There is no controversy about the fecundity of our folk-psychological anticipations, but much disagreement over how to explain this bounty. Do we learn dozens or hundreds or thousands of "laws of nature" along the lines of

"If a person is awake with eyes open and facing a bus, he will tend to believe there is a bus in front of him," and
"Whenever people believe they can win favor at low cost to themselves, they will tend to cooperate with others, even strangers,"

or are all these rough-cast laws generated on demand by an implicit sense that these are the rational responses under the circumstances? The latter, I claim. Whereas there are indeed plenty of stereotypic behavior patterns that can be encapsulated by such generalizations (which might, in principle, be learned one at a time as we go through life), it is actually hard to generate a science-fictional scenario so novel, so unlike all other human predicaments, that people are simply unable to imagine how people might behave under those circumstances. "What would you do if that happened to you?" is the natural question to ask, and along with such unhelpful responses as "I'd probably faint dead away" comes the tellingly rational answer: "Well, I hope I'd be clever enough to see that I should do X." And when we see characters behaving oh so cleverly in these remarkably non-stereotypical settings, we have no difficulty understanding what they are doing and why. Like our capacity to understand entirely novel sentences of our natural languages, sentences we have never before heard in our lives, our ability to make sense of the vast array of human interactions bespeaks a generative capacity that is to some degree innate in normal people.
We just as naturally and unthinkingly extend the intentional stance to animals, a non-optional tactic if we are trying to catch a wily beast, and a useful tactic if we are trying to organize our understanding of the behaviors of simpler animals, and even plants. The clam has its behaviors, and they are rational, given its limited outlook on the world. We are not surprised to learn that trees that are able to sense the slow encroachment of rivals (because more of the sunlight now falling on them is reflected off tall green things in the vicinity) shift resources into growing taller faster. After all, that's the smart thing for a plant to do under those circumstances. Among artifacts, even the lowly thermostat can sustain a rudimentary intentional-stance interpretation: it wants to keep the temperature at the level you asked it to, samples the temperature frequently to arrive at an up-to-date belief about the actual temperature, compares that to the desired temperature, and acts accordingly. This is what you might tell a young child, to explain the point of the thermostat without getting technical.
This simple theory of intentional systems is a theory about how and why we are able to make sense of the behaviors of so many complicated things by considering them as agents. It is not directly a theory of the internal mechanisms that somehow achieve the rational guidance thereby predicted. The intentional stance gives you the "specs," the job description, of an intentional system—what it should discriminate, remember, and do, for instance—and leaves the implementation of those specs to the engineers (or evolution and development, in the case of an intentional system that is an organism). Give me an agent that knows the difference between dollar bills and ten-dollar bills, can make change, detect counterfeits, and is willing and able to deliver the product the customer wants twenty-four hours a day. This intentional-stance characterization is either the specs for a vending machine or a rudimentary job description of a convenience store clerk, entirely noncommittal about what kind of innards or further talents the entity might have.
This equivalence, or neutrality, is not a bug but a feature, as the software engineers like to say. It permits intentional-systems theory to play the chief organizing role in bridging the chasm of confusion between our minds and our brains, as we shall see in the next three chapters. Briefly, it allows us to see what is in common between "real" beliefs (the beliefs of persons) and "mere" belief-like states (of vending machines, animals, young children, and, most usefully, sub-personal parts of persons). To anticipate, we can use the intentional stance to give the specs of the competences of subsystems in the brain in advance of any detailed knowledge of how those competences are implemented. We analyze the big, fancy "real" person into sub-personal agents, with their own agendas and methods, and then analyze these in turn into yet simpler, stupider agents. Eventually, we arrive at intentional systems that are simple enough to describe without further help from the intentional stance. Bridging the chasm between personal-level folk psychology and the sub-personal activities of neural circuits is a staggering task of imagination that benefits from this principled relaxation of the conditions that philosophers have tried to impose on (genuine, adult) human belief and desire. Where on the downward slope to insensate thinghood does "real" believing and desiring stop and mere "as if" believing and desiring take over? This demand for a bright line is ill motivated, as we have already seen in chapter 15 and will see again with the help of several other thinking tools.
The use of the intentional stance in both computer science and animal psychology is ubiquitous, and intentional-systems theory explains why this is so. Some theorists in evolutionary biology claim to do without it, but they are fooling themselves, as we shall see in the section on evolution.*











19. The Personal/Sub-Personal Distinction
Your eyes don't see; you do. Your mouth doesn't enjoy chocolate cake; you do. Your brain doesn't abhor the stinging pain in your shoulder; you do. Your hand doesn't sign a contract; you do. Your body may be aroused, but you fall in love. This is not just a "grammatical" point, like the fact that we say, "It's raining," when there is a thunderstorm, not "the thunderstorm is raining." Nor is it just a matter of definitional convention. People sometimes ask, dismissively, "Isn't this just semantics?" which is taken to mean that nothing much hangs on how we "define our terms." But how we define our terms often does make a big difference, and this is one of those times. Our way of speaking about persons and what they can do and suffer is grounded in some important facts.
At first blush it seems that there are things a whole person can do that none of a person's proper parts can do, and this is almost right, but a ghoulish exercise of imagination suggests that if this is so, then a person is (roughly) a proper part of a human body, namely, an intact and functioning brain. (Do you have a brain, or are you a brain? Not a straightforward question to answer.) If you cut off my arms, I can still sign a contract (with a pen in my toes or a vocal directive), but if you shut down my brain, nothing my arms and hands could do counts as signing a contract. Pluck out my eyes and I cannot see, unless I get fitted with prosthetic eyeballs, which are not such a distant science-fiction fantasy. What if you start "amputating" parts of my brain? If you were to remove the occipital cortex while leaving my eyes and optic nerve intact, I would be "cortically blind" but might still have some residual visual competence (for instance, the famous condition known as blindsight). No doubt we could amputate a bit more brain and wipe out the blindsight, while still leaving you a life to lead. The tempting idea is that such a process of elimination, removing hearing, touch, taste, and smell, could pare down the brain to the ultimate headquarters of you—and that is where, and what, a person would be. Tempting but wrong. The brain's multitudinous competences are so intertwined and interacting that there simply is no central place in the brain "where it all comes together" for consciousness.* For that matter, many of the competences, dispositions, preferences, and quirks that make you you depend on paths through your body outside your brain; the always popular philosophical thought experiment of the brain transplant (which would you rather be: the brain "donor" or the brain "recipient"?) is enabled by a very distorting idealization. As I once put it, "One cannot tear me from my body, leaving a nice clean edge" (1996a, p. 77).
Probably the most important property of the Internet is its decentralization; it has no hub or headquarters anywhere in the world where dropping a well-placed bomb would kill it. Its proper parts have a high degree of redundancy and versatility, so it "degrades gracefully" if it degrades at all when any proper part of it is disabled. Hal, the intelligent computer in 2001: A Space Odyssey, has a "Logic Memory Center," a room full of memory cassettes that Dave can detach one by one, shutting Hal down for good. The Internet has no such center, and while nature has not equipped us with quite such a fine level of distributed invulnerability, there is still considerable decentralization of you in your body, and considerable versatility in your parts. The organization of your brain proves to be remarkably plastic (able to morph into new configurations) so that you can go on being you, pursuing your dreams, confounding your enemies, concocting your stratagems, reliving your trials and triumphs, in spite of the removal of important but not quite "vital" bodily parts. That is one reason why you can have competences that none of your parts have. Or we can turn the point inside out: the only way to understand or make sense of the powers of the parts of a whole live body is to consider how they contribute to the coordination of the whole magnificent system.
Consider a few more instances: Your brain doesn't understand English; you do. Your brain doesn't find the joke hilarious; you do. Even if the activities of competent structures in your brain play the major causal roles in your understanding and appreciation, they couldn't do their work without the training and support provided, over the years, by all your sense organs and limbs and other effectors.*
So it is not just conventional when we posit a person, the enduring, conscious, rational agent whose live body this is, as the subject of record for most of our everyday attributions: it is you who made the mistake, won the race, had a crush on Leslie, speak passable French, want to go to Brazil, prefer blondes, committed the libel. (See also chapter 62.) You are hungry, tired, and peevish, all thanks to your sub-personal parts—and nothing else.
But then what about those proper parts? Are they just like bricks stacked up into a living human body? The answer is yes, if we look at the smallest parts: atoms. But it is no at every other level, from the molecular to the cellular and higher. The proteins that are the workhorses inside your cells are amazingly competent and discriminating little robots—nanobots, we might call them. The neurons that do most of the transmission and switching and adjusting work in your brain are more versatile and competent robots—microbots, we might call them. They form coalitions that compete and cooperate in larger structures, communicating back and forth, suppressing each other, analyzing the flood of information from the senses, waking up dormant information structures "in memory" (which is not a separate place in the brain), and orchestrating the subtle cascades of signals that move your muscles when you act.
All these levels higher than the basic atomic building blocks exhibit a degree of agency. In other words, they are interpretable as intentional systems. At the molecular level (motor proteins, DNA proofreading enzymes, gatekeepers at trillions of portals in the membranes of your cells, and the like), their competences are very "robotic" but still impressive, like the armies of marching brooms in The Sorcerer's Apprentice, or Maxwell's Demon, to take two fictional examples. At the cell level, the individual neurons are more exploratory in their behavior, poking around in search of better connections, changing their patterns of firing as a function of their recent experience. They are like prisoners or slaves rather than mere machines (like the protein nanobots); you might think of them as nerve cells in jail cells, myopically engaged in mass projects of which they have no inkling, but ever eager to improve their lot by changing their policies. At higher levels, the myopia begins to dissipate, as groups of cells—tracts, columns, ganglia, "nuclei"—take on specialized roles that are sensitive to ever-wider conditions, including conditions in the external world. The sense of agency here is even stronger, because the "jobs done" require considerable discernment and even decision-making.
These agents are like white-collar workers, analysts and executives with particular responsibilities, but also, like white-collar workers everywhere, they have a healthy dose of competitive zeal and a willingness to appropriate any power they encounter in the course of their activities, or even usurp control of any ill-defended activities engaged in by their neighbors or others in communication with them. When we reach agents at this level of competence, the sub-personal parts are intelligent bricks indeed, and we can begin to see, at least in sketchy outline, how we might fashion a whole comprehending person out of them. ("Some assembly required," as it says on the carton containing all the bicycle parts, but at least we don't have to cut and bend the metal, and make the nuts and bolts.)
This idea, that we can divide and conquer the daunting problem of imagining how a person could be composed of (nothing but) mindless molecules, can be looked at bottom-up, as we have just done, or top-down, starting with the whole person and asking what smallish collection of very smart homunculi could conspire to do all the jobs that have to be done to keep a person going. Plato pioneered the top-down approach. His analysis of the soul into three agent-like parts, analogized to the Guardians, the Auxiliaries, and the Workers, or the rational, the spirited, and the appetitive, was not a very good start, for reasons well analyzed over the last two millennia. Freud's id, ego, and superego of the last century was something of an improvement, but the enterprise of breaking down a whole mind into sub-minds really began to take shape with the invention of the computer and the birth of the field of artificial intelligence (AI), which at the outset had the explicit goal of analyzing the cognitive competences of a whole (adult, conscious, language-using) person into a vast network of sub-personal specialists, such as the goal-generator, the memory-searcher, the plan-evaluator, the perception-analyzer, the sentence-parser, and so on.











20. A Cascade of Homunculi
In the millennia-old quest to understand the mind, theorists have often succumbed to the temptation to imagine an inner agent, a little man—homunculus, in Latin—who sits in the control room in the brain and does all the clever work. If you think of the human nervous system as, say, a huge telephone switchboard network (as thinkers liked to do as recently as the 1950s and 1960s), you have the problem of the telephone operator at the center: Is his or her mind composed of a smaller telephone network with its own central operator, whose mind in turn is composed of ... ? Any theory that posits such a central homunculus is doomed by the prospect of an infinite regress.
But maybe the mistake isn't postulating a homunculus, but postulating a central homunculus. In my first book, Content and Consciousness (1969), I made a big blunder, tempted by a wisecrack I couldn't resist. I wrote,

The "little man in the brain", Ryle's "ghost in the machine", is a notorious non-solution to the problems of mind, and although it is not entirely out of the question that the "brain writing" analogy will have some useful application, it does appear merely to replace the little man in the brain with a committee. [p. 87]

And what, exactly, is wrong with a committee? (Aha—my implied reductio is called out!) I eventually came to realize (in Brainstorms, 1978a) that the idea of replacing the little man in the brain with a committee is not such a bad one; it is, I think, one of the fundamental good ideas of cognitive science. This was the classic strategy of GOFAI (good old-fashioned artificial intelligence; Haugeland, 1985) that came to be known as homuncular functionalism:

The AI programmer begins with an intentionally characterized problem, and thus frankly views the computer anthropomorphically: if he solves the problem he will say he has designed a computer that can [e.g.,] understand questions in English. His first and highest level of design breaks the computer down into subsystems, each of which is given intentionally characterized tasks; he composes a flow chart of evaluators, rememberers, discriminators, overseers and the like. These are homunculi with a vengeance. ... Each homunculus in turn is analyzed into smaller homunculi, but, more important, into less clever homunculi. When the level is reached where the homunculi are no more than adders and subtractors, by the time they need only the intelligence to pick the larger of two numbers when directed to, they have been reduced to functionaries who can be replaced by a machine. [p. 80]

The particular virtue of this strategy is that it pulled the rug out from under the infinite regress objection. According to homuncular functionalism the ominous infinite regress can be sidestepped, replaced by a finite regress that terminates, as just noted, in operators whose task is so dull they can be replaced by machines. The key insight was breaking up all the work we imagined being done by a central operator and distributing it around to lesser, stupider agents whose work was distributed in turn, and so forth.
This was a fine advance, but an unwanted artifact of the top-down approach of classic GOFAI was its rigid bureaucratic efficiency! We may have been able to imagine getting rid of the central king or the CEO, but we still had an army of midlevel executives reporting to an array of vice presidents (whose interactions constituted the highest level of the system) and ordering their subordinates into action, who in turn would call up more menial clerical workers, and so forth. This hyper-efficient organization, with no wasted motion, no featherbedding, no insubordination, was largely dictated by the fact that the large computers on which the early AI models were developed were tiny and slow by today's standards, and people wanted results fast. If you needed to impress the funding agency, your AI had better not take hours to answer a simple question. It should be businesslike in its execution. Besides, writing thousands of lines of code is a lot of work, and if you have succeeded in breaking down the target task—answering questions about moon rocks or diagnosing kidney diseases or playing chess, for instance—into a manageable set of subtasks that you can see how to program and then integrate into a workable system, you can have your "proof of concept"* at a feasibly low cost in terms of time and money.
Notice that computers have always been designed to keep needs and job performance almost entirely independent. Down in the hardware, the electric power is doled out evenhandedly and abundantly; no circuit risks starving. At the software level, a benevolent scheduler system doles out machine cycles to whatever process has the highest priority, and although there may be a bidding mechanism of one sort or another that determines which processes get priority, this is an orderly queue, not a struggle for life. As Marx would have it, "From each according to his abilities, to each according to his needs." The computer scientist Eric Baum has aptly dubbed this hierarchy "politburo" control. Probably a dim appreciation of this fact underlies the common folk intuition that a computer could never care about anything. Not because it is made out of the wrong materials—why should silicon be any less suitable a substrate for caring than carbon?—but because its internal economy has no built-in risks or opportunities, so it doesn't have to care.
Neurons are not like this. The general run of the cells that comprise our bodies are probably just willing slaves—rather like the selfless, sterile worker ants in a colony, doing stereotypic jobs and living out their lives in a relatively noncompetitive (Marxist) environment. But brain cells—I now think—must compete vigorously in a marketplace. For what? What could a neuron want? The energy and raw materials to continue to thrive—just like its unicellular eukaryote ancestors and more distant cousins, the bacteria and archaea. Neurons are biological robots of sorts; they are certainly not conscious in any rich sense. Remember, they are eukaryotic cells, akin to yeast cells or fungi. If individual neurons are conscious, then so is athlete's foot! But neurons, like their mindless unicellular cousins, are highly competent agents in a life-or-death struggle, not in the environment between your toes, but in the demanding environment of the brain, where the victories go to the cells that can network more effectively, contribute to more influential trends at the virtual machine levels where large-scale human purposes and urges are discernible. Many of the subsystems in the nervous system are organized as opponent processes, engaged in a tug-of-war between two sub-subsystems, each trying to have its own way. (Our emotions, for instance, are well seen as rival storms displacing each other as best they can, thwarting each other or collaborating against yet another storm.) I now think, then, that the opponent-process dynamics of emotions, and the roles they play in controlling our minds, are underpinned by an economy of neurochemistry that harnesses the competitive talents of individual neurons. (Note that the claim is that neurons are still good team-players within the larger economy, unlike the more radically selfish agents, cancer cells. I recall Nobel laureate biologist François Jacob's dictum that the dream of every cell is to become two cells; neurons vie to stay active and to be influential, but do not dream of multiplying.) In this view, intelligent control of an animal's behavior is still a computational process—in the same way that a transaction in the stock market is a computational process—but the neurons are selfish neurons, as neuroscientist Sebastian Seung (2007) has said, striving to maximize their intake of the different currencies of reward we have found in the brain. And what do neurons buy with their dopamine, serotonin, or oxytocin? They are purchasing greater influence in the networks in which they participate, and hence greater security. (The fact that mules are sterile doesn't stop them from fending for themselves, and neurons can similarly be moved by self-protective instincts they inherited ultimately from their reproducing ancestors.)
So bottom-up, neuroscientifically inspired, homuncular functionalism is looking better and better as a model of how the brain works, since the more chaotic and competitive "computational architectures" it generates are more plausible from a biological point of view: we can begin to discern the developmental processes that could build and revise these architectures, starting in the embryo and continuing into adulthood, and also see how they could have evolved from simpler nervous systems, themselves teams of less accomplished homunculi, which sorta perceive, signal, and remember.*











21. The Sorta Operator
Why indulge in this "sorta" talk? Because when we analyze—or synthesize—this stack of ever-more competent levels, we need to keep track of two facts about each level: what it is and what it does. What it is can be described in terms of the structural organization of the parts from which it is made—so long as we can assume that the parts function as they are supposed to function. What it does is some (cognitive) function that it (sorta) performs—well enough so that at the next level up, we can make the assumption that we have in our inventory a competent building block that performs just that function—sorta, good enough to use. This is the key to breaking the back of the mind-bogglingly complex question of how a mind could ever be composed of material mechanisms. At the dawn of the computer age, Alan Turing, who deserves credit as the inventor of the computer if anybody does, saw this prospect. He could start with mindless bits of mechanism, without a shred of mentality in them, and organize them into more competent mechanisms, which in turn could be organized into still more competent mechanisms, and so forth without apparent limit. What we might call the sorta operator is, in cognitive science, the parallel of Darwin's gradualism in evolutionary processes (more on this in part VI). Before there were bacteria, there were sorta bacteria, and before there were mammals, there were sorta mammals, and before there were dogs, there were sorta dogs, and so on.
We need Darwin's gradualism to explain the huge difference between an ape and an apple, and we need Turing's gradualism to explain the huge difference between a humanoid robot and a hand calculator. The ape and the apple are made of the same basic ingredients, differently structured and exploited in a many-level cascade of different functional competences. There is no principled dividing line between a sorta ape and an ape. Both the humanoid robot and the hand calculator are made of the same basic, unthinking, unfeeling Turing-bricks, but as we compose them into larger, more competent structures, which then become the elements of still more competent structures at higher levels, we eventually arrive at parts so (sorta) intelligent that they can be assembled into competences that deserve to be called comprehending. We use the intentional stance to keep track of the beliefs and desires (or "beliefs" and "desires" or sorta beliefs and sorta desires) of the (sorta-) rational agents at every level from the simplest bacterium through all the discriminating, signaling, comparing, remembering circuits that comprise the brains of animals from starfish to astronomers. There is no principled line above which true comprehension is to be found—even in our own case. The small child sorta understands her own sentence "Daddy is a doctor," and I sorta understand "E = mc2." Some philosophers resist this anti-essentialism (see chapter 43): either you believe that snow is white or you don't; either you are conscious or you aren't; nothing counts as an approximation of any mental phenomenon; it's all or nothing. And to such thinkers, the powers of minds are insoluble mysteries because minds are "perfect," and perfectly unlike anything to be found in mere material mechanisms.











22. Wonder Tissue
In his excellent book on Indian street magic, Net of Magic: Wonders and Deceptions in India, Lee Siegel (1991) writes,
"I'm writing a book on magic," I explain, and I'm asked, "Real magic?" By real magic people mean miracles, thaumaturgical acts, and supernatural powers. "No," I answer: "Conjuring tricks, not real magic." Real magic, in other words, refers to the magic that is not real, while the magic that is real, that can actually be done, is not real magic. [p. 425]

"Real magic" is—by definition, you might say—miraculous, a violation of the laws of nature. Many people want to believe in real magic. When the Amazing Randi, magician, skeptic, and ghost-buster supreme, duplicates the tricks of self-styled psychics such as Uri Geller, he is showing that the startling effects are not real magic; they are conjuring. But this doesn't convince some people. At the question-and-answer session following a performance in Winnipeg many years ago, a member of the audience charged Randi with a double deception: he was just as much a real psychic as Geller—he just pretended to be a mere conjurer exposing Geller's duplicity so he could ride Geller's more famous coattails to fame and fortune! It's hard to rebut that weird challenge, except by teaching everybody in the audience exactly how the tricks are done, which Randi, honoring the tradition of magicians around the world, is reluctant to do. (Penn and Teller have pioneered routines that do expose the secret mechanics of magic tricks, and after enduring condemnation by some of their fellow magicians, they have shown how the traditional taboo can be violated without spoiling the magic show.)
A similar eagerness to believe in real magic afflicts many people when the topic is the relationship between the mind and the brain. Some people, including not a few neuroscientists and psychologists—and philosophers—are at least subliminally attracted to the idea that somehow or other the dynamic properties of neural tissue can do something you might call miraculous, something that harnesses hidden forces undreamed of by science. Maybe they are right, but we mustn't assume that they are from the outset. The rule has to be: no wonder tissue!
Here is something we know with well-nigh perfect certainty: nothing physically inexplicable plays a role in any computer program, no heretofore unimagined force fields, no mysterious quantum shenanigans, no élan vital. There is certainly no wonder tissue in any computer. We know exactly how the basic tasks are accomplished in computers, and how they can be composed into more and more complex tasks, and we can explain these constructed competences with no residual mystery. So although the virtuosity of today's computers continues to amaze us, the computers themselves, as machines, are as mundane as can-openers. Lots of prestidigitation, but no "real magic."
This is a valuable fact, so valuable that it will get a detailed demonstration in the next section. Its value lies in the fact that any time we can make a computer do something that has seemed miraculous, we have a proof that it can be done without wonder tissue. Maybe the brain does it another way, maybe even with wonder tissue (maybe Randi is a real psychic, just like Geller!), but we have no good reason to believe it. Computers thus play an important role as demystifiers, and that is a good reason to insist on developing computer models of anything we are trying to understand, whether it be hurricanes or housing bubbles or HIV or human consciousness.
The term wonder tissue is a thinking tool along the lines of a policeman's billy club: you use it to chastise, to persuade others not to engage in illicit theorizing. And, like a billy club, it can be abused. It is a special attachment for the thinking tool Occam's Razor and thus enforces a certain scientific conservatism, which can be myopic. I owe my favorite example of this to William Bateson, one of the fathers of modern genetics, and here's what he had to say not so long ago, in 1916:
The properties of living things are in some way attached to a material basis, perhaps in some special degree to nuclear chromatin [chromosomes]; and yet it is inconceivable that particles of chromatin or of any other substance, however complex, can possess those powers which must be assigned to our factors or gens [genes]. The supposition that particles of chromatin, indistinguishable from each other and indeed almost homogeneous under any known test, can by their material nature confer all the properties of life surpasses the range of even the most convinced materialism. [p. 91]

He just could not imagine DNA. The idea that there might be three billion base-pairs in a double helix inside every human cell was simply not within the scope of his imagination. Fortunately, other biologists didn't share Bateson's pessimism, and they went on to discover just how the apparently miraculous feat of transmitting genetic information from generation to generation is achieved by some pretty fantastic molecules. But along that path of discovery, they held themselves to the rule: no wonder tissue. They knew—from genetics—a lot about the competences their quarry had to possess, and they set their task as the construction of a physically possible model of something that would have those competences.
We face a similar task today. Experimental psychology is giving us an ever-more detailed catalogue of the competences and frailties of the mind—the triumphs of perception and the embarrassments of illusion, the pace of language learning and the conditions of distraction, lust, fear, and mirth—and now, as "convinced materialists," we need to figure out how on earth the brain does it all, without postulating wonder tissue.
As our understanding grows, what counts as wonder tissue shifts. When "connectionist" and other "neural network" models burst on the scene in the mid-1980s,* they demonstrated learning capabilities and pattern-recognition powers that nobody would have dared postulate in small tracts of neurons a few years earlier. We still don't know exactly how—or even if—the brain exploits the computational powers exhibited by these semi-realistic models, but it is now okay to postulate a connectionist competence for some neural network that you can't yet explain as long as you are up-front about it and the competence is not clearly beyond the demonstrated range of feats. (Randi may not do the trick the very same way Geller does, but we are safe in concluding that there is some variation of Randi's method that explains Geller's ability, which gives us guidance in inquiring further into the processes that are actually involved.) The main objection to wonder tissue is that it does not give us a way of solving the problem, but a way of giving up, of assuming that it is a mystery that can never be solved.











23. Trapped in the Robot Control Room
Robots have no wonder tissue in them (by definition, in effect), so they provide an antiseptically clean platform for thought experiments, such as this one:
You wake up one morning to find yourself in an unfamiliar bed in a strange windowless room; two of its walls are covered with tiny blinking lights of various colors, and the other two are covered with thousands of pushbuttons. There are numbers on the lights and buttons, but no labels. Somebody has left you a note on the bedside table.
Good morning! You were drugged and kidnapped while you slept and brought here to your new home. There is food in the fridge and a bathroom in the corner, so your local bodily needs will all be met. You are imprisoned in the control room of a giant robot. Each light, when it turns on, provides rich and relevant information about the robot's circumstances; these lights are all outputs from highly sophisticated neural net analyzers of the raw inputs streaming from the robot's high-definition video eyes and microphone ears, touch sensors, and olfactory sensors. The buttons initiate robotic actions, all coordinated and ready to execute.
The robot inhabits a dangerous world, with many risks and opportunities. Its future lies in your hands, and so, of course, your own future as well depends on how successful you are in piloting your robot through the world. If it is destroyed, the electricity in this room will go out, there will be no more food in the fridge, and you will die. Good luck!

This is a nasty predicament. With your heart in your mouth, you start experimenting, pushing buttons to see what happens. You push the yellow button numbered 4328 and notice that when you do, the blue light numbered 496 flickers out. Were you scratching the robot's itch or closing the robot's eyes or maybe "eating" something, thereby satisfying the robot's pressing metabolic needs? You push button 4328 a second time, and a different scattering of lights turn on. What has changed in the world? What does this mean? This is tantalizing because you have been told that there is a tremendous amount of information in those lights, but which light signals which information, and which button orders which robotic action?
If only the lights and buttons were labeled! Then—if the labels were in a language you understood—you might be able to solve the problem. Or if only there were a window in the room that you could open, so you could look out and see what moved when you pushed a button! Does the robot have arms and legs? With a window, you could try to correlate events in the outside world with the flashing lights on the walls. Without a window, you have all this information ready at hand, but no way to interpret it. You have hundreds of robotic actions to set into motion, but no way of figuring out what effects they can achieve in the world.
It appears that you are in an impossible situation. However clever and imaginative you are, you won't be able to figure out the meanings of the events occurring on the walls of your room, given all the data at your disposal. But if you can't solve this problem, we have something approaching a paradox, because your predicament here is none other than your brain's predicament! It is trapped in a windowless room—your skull—with millions of input lines positively humming with information about the external world and your bodily circumstances, and millions of output lines poised to stimulate muscles into contraction or relaxation. And your brain can't open a window in your skull and look out to see what's happening that might be causing the patterns of signals streaming into your visual cortex. (What good would such a window be for your brain in any case? Unlike you, it has no eyes in addition to the eyes whose signals it is receiving and trying to interpret, and no prior memories of what things in the world look like.)
Perhaps, you may think, the brain's task is made easier by the fact that the pattern of excitation—the blinking lights—on the surface of the visual cortex when you're looking at a duck, for example, is actually shaped like a duck!* This would be fine, if we could suppose your brain had learned, as you have, what a duck looks like, but how is your brain ever going to learn that?
How can your brain learn anything if it first has to "decode" all its signals? And decode those signals into what? Turkish? Printed labels in Turkish mean nothing to you unless you understand Turkish. Does your brain have to understand a language before it can derive any value from its input? As we have already seen, this idea, that brains come equipped with an internal language that they never have to learn—Mentalese, or the Language of Thought (Fodor, 1975, 2008)—is tantalizing. It seems to be a step in the right direction, but until the details are provided about how it works, and how it evolved in the first place, declaring that there is such a language of thought is just renaming the problem without solving it. We know that the brain solves the problem somehow; our brains do manage quite reliably to find the appropriate outputs to cope with the predicaments and opportunities the input information heralds. And we know that the brain's solution—whatever it is—can't be just like a language (like English or Turkish) since it isn't acquired in childhood the way we acquire our native tongues. Would it be more like a written language (with memories duly inscribed in the brain's archives) or a purely spoken language? Does it have a vocabulary of thousands or millions or billions of "words"? Does word order matter? Is there a grammar? Can one part of the brain misunderstand another part's message?
If the language-of-thought hypothesis postulates a homunculus in the control room who understands the language (like you in the giant robot, reading the labels on the lights and buttons), it merely postpones the task of figuring out how learning and comprehension can be composed out of machinery consisting of uncomprehending parts. And if it doesn't postulate a homunculus who decodes the messages, then the system, whatever it is, isn't much like a language after all. And since nobody has yet come up with the details of how such a language of thought would work, or get installed during development and experience, it is probably better not to kid ourselves, not to lull ourselves into thinking we are making progress when we probably aren't.
This intuition pump shows us that your brain is importantly not in the same predicament you would be in, trapped in the control room. Its task is—must be—partly solved in advance by the way some inputs are "wired up" to some outputs so that there is some leverage in the brain with which to learn and refine further appropriate relationships. This is another way of dramatizing the widely recognized claim that our brains are not "blank slates" at birth (Pinker, 2002), but are already designed by natural selection to embody various preferences, anticipations, and associations. And as long as some of the appropriate connections are built in, they don't have to be labeled.
Before there can be comprehension, there has to be competence without comprehension. This is nature's way. Bacteria have all sorts of remarkable competences that they need not understand at all; their competences serve them well, but they themselves are clueless. Trees have competences whose exercise provides benefits to them, but they don't need to know why. The process of natural selection itself is famously competent, a generator of designs of outstanding ingenuity and efficacy, without a shred of comprehension.
Comprehension of the kind we human adults enjoy is a very recent phenomenon on the evolutionary scene, and it has to be composed of structures whose competence is accompanied by, enabled by, a minimal sort of semi-comprehension, or pseudo-comprehension—the kind of (hemi-semi-demi-)comprehension enjoyed by fish or worms. These structures are designed to behave appropriately most of the time, without having to know why their behavior is appropriate.
The alternative, putting a full-fledged Comprehender in the control room, confronted by all the inputs and outputs, is a guaranteed dead end. Why? Because if this power of comprehension is inexplicable, you have installed wonder tissue, a miracle, at the base of your theory, and if it is explicable—in terms of processes and activities and powers that do not themselves have the power of comprehension—you have wasted your turn and are right back at the beginning with those of us who are trying to explain how comprehension grows out of competence.














IV. AN INTERLUDE ABOUT COMPUTERS

 

Time out. As you have probably noticed, I have mentioned computers quite a bit already, and there is much more computer lore to come in the pages ahead. Computers are without a doubt the most potent thinking tools we have, not just because they take the drudgery out of many intellectual tasks, but also because many of the concepts computer scientists have invented are excellent thinking tools in their own right. We are all swimming in the flood of computer jargon these days—software and hardware, bandwidth and gigahertz—and surely most of you have absorbed fairly accurate impressions about what the new buzzwords mean. I have discovered, however, that although my students nod knowingly when I deploy such terms in my classes, their comprehension is uneven, and they sometimes confound me by arriving at weird misunderstandings of what I was trying to convey to them. So I am going to teach you how to write programs for the world's simplest computer.
If you take the time and effort now to master a few basic skills, it will pay off handsomely in deepening your understanding of what follows. (And if you are already a seasoned computer professional, you might find some of my ways of explaining the issues useful when talking to nonexperts, or you can skip ahead.) I have test-flown what follows on hundreds of computer-phobic undergraduates, with happy results: even those who would rather memorize a page of the phone book than solve a puzzle acknowledge the pleasure, the tingle of satisfaction, in making this idiotically simple computer do its stuff. And when you complete the exercise, you will have been initiated into the Seven Secrets of Computer Power.











24. The Seven Secrets of Computer Power Revealed
Computers have powers that in earlier centuries would have seemed miraculous—"real magic"—but although many computer programs are dauntingly complicated, all of them are composed of steps that are completely explainable in very simple terms. There is no room for mysteries in what computers do. That very fact is part of the value of computers as thinking tools, and explaining—in outline—how this works is philosophically interesting in its own right. How computers do their "magic" is well worth understanding at an elementary level. This chapter provides that demystification.
We start by considering what is probably the simplest imaginable computer, a register machine, to see just what its powers are and why. We will then go on to see how a Turing machine, and a Von Neumann machine (such as your laptop) are just like register machines, only more efficient. (Anything your laptop can do, a register machine can do, but don't hold your breath; it might take centuries.) Then we can understand how other computer "architectures" could further multiply the speed and capacity of our basic machine, the register machine. The architecture of the human brain is, of course, one of the most interesting and important architectures to consider.
Hang on. Am I claiming that your brain is just a gigantic computer? No—not yet in any case. I am pointing out that if your brain is a gigantic computer, then there will be a way of understanding all its activities with no residual mysteries—if only we can find it. Our method will be reverse engineering: studying a complicated system to uncover how it does what it does. Reverse engineering tells us how the heart executes its duties as a pump and how the lungs gather oxygen and expel carbon dioxide. Neuroscience is the attempt to reverse engineer the brain. We know what brains are for—for anticipating and guiding and remembering and learning—but now we need to figure out how they accomplish all this.
This is a topic of passionate controversy. The novelist Tom Wolfe (2000) pinpointed the tender spot around which the battles rage with the title of his essay "Sorry, But Your Soul Just Died." If we are to explore this dangerous territory—and not just waste everybody's time with declamations and denunciations—we need some sharper tools. We need to know what computers can do and how they do it before we can responsibly address the question of whether or not our brains harbor and exploit incomprehensible or miraculous phenomena beyond the reach of all possible computers. The only satisfactory way of demonstrating that your brain isn't—couldn't be—a computer would be to show either (1) that some of its "moving parts" engage in sorts of information-handling activities that no computers can engage in, or (2) that the simple activities its parts do engage in cannot be composed, aggregated, orchestrated, computer-fashion, into the mental feats we know and love.
Some experts—not just philosophers, but neuroscientists, psychologists, linguists, and even physicists—have argued that "the computer metaphor" for the human brain/mind is deeply misleading, and, more dramatically, that brains can do things that computers can't do. Usually, but not always, these criticisms presuppose a very naïve view of what a computer is or must be, and end up proving only the obvious (and irrelevant) truth, that brains can do lots of things that your laptop can't do (given its meager supply of transducers and effectors, its paltry memory, its speed limit). If we are to evaluate these strong skeptical claims about the powers of computers in general, we need to understand where computer power in general comes from and how it is, or can be, exercised.
The brilliant idea of a register machine was introduced at the dawn of the computer age by the logician Hao Wang (1957), a student of Kurt Gödel's, by the way, and a philosopher. It is an elegant tool for thinking, and you should have this tool in your own kit. It is not anywhere near as well known as it should be.* A register machine is an idealized, imaginary (and perfectly possible) computer that consists of nothing but some (finite number of) registers and a processing unit.
The registers are memory locations, each with a unique address (register 1, register 2, register 3, and so on) and each able to have, as contents, a single integer (0, 1, 2, 3, ...). You can think of each register as a large box that can contain any number of beans, from 0 to ... , however large the box is. We usually consider the boxes to be capable of holding any integer as contents, which would require infinitely large boxes, of course. Very large boxes will do for our purposes.
The processing unit is equipped with just three simple competences, three "instructions" it can "follow," in stepwise, one-at-a-time fashion. Any sequence of these instructions is a program, and each instruction is given a number to identify it. The three instructions are:

End. That is, it can stop or shut itself down.
Increment register n (add 1 to the contents of register n; put a bean in box n) and go to another step, step m.
Decrement register n (subtract 1 from the contents of register n; remove one bean from box n) and go to another step, step m.

The Decrement instruction works just like the Increment instruction, except for a single all-important complication: What should it do if the number in register n is 0? It cannot subtract 1 from this (registers cannot hold negative integers as contents; you can't take a bean out of an empty box), so, stymied, it must Branch. That is, it must go to some other place in the program to get its next instruction. This requires every Decrement instruction to list the place in the program to go to next if the current register has content 0. So the full definition of Decrement is:

Decrement register n (subtract 1 from the contents of register n) if you can and go to step m OR if you can't decrement register n, Branch to step p.

Here, then, is our inventory of everything a register machine can do, with handy short names: End, Inc, and Deb (for Decrement-or-Branch).
At first glance, you might not think such a simple machine could do anything very interesting; all it can do is put a bean in the box or take a bean out of the box (if it can find one, and branch to another instruction if it can't). In fact, however, it can compute anything any computer can compute.
Let's start with simple addition. Suppose you wanted the register machine to add the contents of one register (let's say, register 1) to the contents of another register (register 2). So, if register 1 has contents [3] and register 2 has contents [4], we want the program to end up with register 2 having contents [7] since 3 + 4 = 7. Here is a program that will do the job, written in a simple language we can call RAP, for Register Assembly Programing:
program 1: ADD [1,2]



STEP
INSTRUCTION
REGISTER
GO TO STEP
[BRANCH TO STEP]


1.
Deb
1
2
3


2.
Inc
2
1
 


3.
End
 
 
 




The first two instructions form a simple loop, decrementing register 1 and incrementing register 2, over and over, until register 1 is empty, which the processing unit "notices" and thereupon branches to step 3, which tells it to halt. The processing unit cannot tell what the content of a register is except in the case where the content is 0. In terms of the beans-in-boxes image, you can think of the processing unit as blind, unable to see what is in a register until it is empty, something it can detect by groping. But in spite of the fact that it cannot tell, in general, what the contents of its registers are, if it is given program 1 to run, it will always add the content of register 1 (whatever number is in register 1) to the content of register 2 (whatever number is in register 2) and then stop. (Can you see why this must always work? Go through a few cases to make sure.) Here is a striking way of looking at it: the register machine can add two numbers together perfectly without knowing which numbers it is adding (or what numbers are or what addition is)!
Exercise 1

a. How many steps will it take the register machine to add 2 + 5 and get 7, running program 1 (counting End as a step)?
b. How many steps will it take to add 5 + 2?
(What conclusion do you draw from this?)*

There is a nice way to diagram this process, in what is known as a flow graph. Each circle stands for an instruction. The number inside the circle stands for the address of the register to be manipulated (not the content of a register) and "+" stands for Inc and "-" stands for Deb. The program always starts at α, alpha, and stops when it arrives at Ω, omega. The arrows lead to the next instruction. Note that every Deb instruction has two outbound arrows, one for where to go when it can decrement, and one for where to go when it can't decrement, because the contents of the register is 0 (branching on zero).



Now let's write a program that simply moves the contents of one register to another register:
program 2: MOVE [4,5]



STEP
INSTRUCTION
REGISTER
GO TO STEP
[BRANCH TO STEP]


1.
Deb
5
1
2


2.
Deb
4
3
4


3.
Inc
5
2
 


4.
End
