


51. Herb, Alice, and Hal, the Baby
The late great evolutionary theorist George Williams insisted that it was a mistake to identify genes with DNA molecules. That would be approximately the same mistake as thinking that Hamlet is made out of ink. Of course any actual copy of Shakespeare's play has to be made out of something (if not ink, then maybe letter-shaped patterns on a computer screen, or even strings of binary code burned into a CD), but the play is an abstract, informational thing that can hop from medium to medium. Genes, as recipes for making proteins, are also abstract, informational things, according to this way of thinking—which has always struck me as the right way. But there have been those who disagree, who doubt the value of conceiving of genes in this way. For them, and in particular for philosopher of biology Peter Godfrey-Smith, I constructed a little intuition pump:

Herb and Alice want to have a baby, but here's how they do it:

They both have their genomes sequenced. They receive in the mail a data file consisting of their genomes, spelled out in two sequences of approximately three billion letters, A, C, G, T,. ...
Then they write a little computer program that does the meiosis algorithm on both their genomes, (randomly) creating virtual sperm and eggs, which are then (randomly) united in silico, to create a new genome specification (which passes all the forensic tests for being the specification of the DNA of an offspring of Herb and Alice). (So far this is all done literally in terms of the symbols A, C, G, T, a purely computational process of string-rewriting.)
This specification is then used to construct, codon by codon, an actual DNA implementation of the entire genome, A = adenine, C = cytosine, G = guanine, and T = thymine. (Craig Venter's lab can now do this.)
This genome is then implanted in the nucleus of a human egg (does it matter whose egg this is, since its own DNA will be removed before nucleus implantation?), and becomes a "test tube baby" in one of the usual ways.

Is the resulting infant, Hal, the child of Herb and Alice? It seems to me to be quite obvious that Hal is indeed their biological offspring, since it avails itself of all the genetic information they would contribute if Hal were conceived in the normal way. This intuition pump highlights the importance of what matters in reproduction: information, and the causal transmission of information (in this case, in the form of ASCII code for "A," "C," "G," and "T," not in the form of molecules). The causal link might, for instance, pass through telecommunication satellites, instead of taking the more direct, biochemical routes. [Drawn with revisions from personal correspondence, April 26, 2010]

Godfrey-Smith agreed with me that Hal is the offspring of Herb and Alice, but he had some reservations about my way of putting it (which can be read, unabridged, in Dennett, 2010; Godfrey-Smith, 2011). In the same spirit of constructive criticism, I acknowledged that there is a biologically important difference between the process Herb and Alice used to procreate and the way we normally do. What if everybody did it Herb and Alice's way? Since the genetic information from Herb does not ride in its usual vehicle—a sperm cell—on its route to the egg, sperm motility would no longer be under selection pressure, and would, other things being equal, decline over the generations. Use it or lose it. Still, I maintain—and think this intuition pump illustrates clearly—that the molecular structure that stays roughly constant over the generations is preserved because it embodies the information that it does.
If there were alternate structures that preserved the information, evolution would continue unimpeded. This claim can be further examined in yet another intuition pump. Imagine that on another planet, "odd-numbered" generations used the DNA rungs A, C, G, and T, and "even-numbered" generations used a different double helix, call it XNA, with rungs made of P, Q, R, and S (some other molecules). We can suppose that the offspring's XNA molecules were made from DNA templates from their parents by a mechanism rather like messenger RNA, but "translating" between the two different biochemical languages. These messages would be translated back by another messenger mechanism in the next generation, and so forth. You'd have to find a mate whose genes were written in the same language if you wanted offspring, but those offspring would have genomes written in the other language. Oedipal unions would be infertile—which is probably just as well—but there might be many Romeo-and-Juliet-style tragedies in which lovers could not procreate since they came from opposite communities. (They could always settle for a barren sex life and adopt babies of either type, or even play egg donor and sperm donor and raise half-siblings galore.) In such a world, aside from these complications, evolution would continue as always, transmitting genetic information about valuable adaptations (and heritable diseases, etc.) through the generations in alternative coding systems that could be as structurally different as you please. Same genes, different molecules. Each gene would have two forms, as different as "cat" and "chat," "house" and "maison." (Note the parallel with the case of the two black boxes: what the two syntactically, structurally different vehicles in both cases have in common is the same information, the same semantics.)











52. Memes
I haven't mentioned memes yet, which may make some readers wonder if I have abandoned them. Not at all. The concept of memes is one of my favorite thinking tools, and I have a lot to say about it—too much to put in this book! I have already said a great deal about memes elsewhere (Dennett, 1990, 1991a, 1995a, to cite a few examples). For various reasons many people have a visceral dislike of the concept, so they tend to fall for the many criticisms that have been raised. I have decided that I should try one more time to make the case for memes and deal with all the critics, both the serious critics and the white-knuckled meme-haters, but that will take a small book of its own. In the meantime, those who want to know more about memes should take a look at my essay on them, "The New Replicators" (2002; also in Dennett, 2006a).
But here, as a preview, is a brief introduction to the serious concept of memes (in contrast to its over-popular loose usage by Internet denizens). As Dawkins (1976) pointed out, when he introduced the concept of the meme as a cultural item that gets itself copied, the fundamental principle of biology is

that all life evolves by the differential survival of replicating entities. ...
The gene, the DNa molecule, happens to be the replicating entity which prevails on our own planet. There may be others. If there are, provided certain other conditions are met, they will almost inevitably tend to become the basis for an evolutionary process.
But do we have to go to distant worlds to find other kinds of replication and other, consequent, kinds of evolution? I think that a new kind of replicator has recently emerged on this very planet. It is staring us in the face. It is still in its infancy, still drifting clumsily about in its primeval soup, but already it is achieving evolutionary change at a rate which leaves the old gene panting far behind. [p. 206]

Two main insights that flow from this thinking tool dramatically alter the landscape of our imaginations when we think about human culture and creativity. First, memes shatter the otherwise seductive idea that there are only two routes to good Design: it's either genes or genius. For most thinkers, until memes open their eyes, if something in human life exhibits the telltale signs of adaptation of means to ends or functional efficiency, it must be either a product of genetic natural selection or a product of deliberate, comprehending, intending human thinking—intelligent design. Orgel's Second Law—evolution is cleverer than you are—might seem to enshrine the two options, but in fact there is a third possibility, and it is everywhere instanced: nongenetic, cultural selection, accomplished by the same process of mindless natural selection that gives us the genes. A vivid example is provided by an observation about Polynesian canoes more than a century old: "every boat is copied from another boat. ... it is the sea herself who fashions the boats, choosing those which function and destroying the others" (Alain, 1908). This is natural selection, plain as day: the islanders have a simple rule: if it returns from the sea intact, copy it! They may have considerable comprehension of the principles of naval architecture that retrospectively endorse their favorite designs, but it is strictly unnecessary. Evolution takes care of the quality control. The same is true of grammatical rules, words, religious practices, and many other bedrock features of human culture: nobody designed them, and they're not "in our genes," but they are nevertheless quite excellently designed.
The second insight is that the price we pay for having this extra information highway, this bounteous medium of design and transmission that no other species enjoys, is that memes have their own fitness, just like all the other symbionts that thrive in our company, and their fitness is to some degree independent of our own fitness. Blindness to this idea is endemic, and is particularly evident when people discuss evolutionary accounts of religion. "Oh, so you're working on an evolutionary theory of religion. What good do you think religions provide? They must be good for something, since apparently every human culture has religion in some form or other." Well, every human culture has the common cold too. What is it good for? It's good for itself. We should be prepared to find cultural replicators that are not beneficial but that manage to thrive nevertheless. This evens the playing field of theories of cultural evolution, replacing the blinkered idea that cultural innovations—just like genetic innovations—always enhance the fitness of those who transmit them. Memes are informational symbionts, and like the mutualist symbionts by the trillions that also inhabit us, we couldn't live without them, but that doesn't mean they are all our friends. Some are harmful plagues we could well do without.











Summary
In this section I have tried to demonstrate that Darwinian thinking does live up to its billing as universal acid: it turns the whole traditional world upside down, challenging the top-down image of designs flowing from that genius of geniuses, the Intelligent Designer, and replacing it with the bubble-up image of mindless, motiveless cyclical processes churning out ever-more robust combinations until they start replicating on their own, speeding up the design process by reusing all the best bits over and over. Some of these earliest offspring eventually join forces (one major crane, symbiosis), which leads to multicellularity (another major crane), which leads to the more effective exploration vehicles made possible by sexual reproduction (another major crane), which eventually leads in one species to language and cultural evolution (cranes again), which provide the medium for literature and science and engineering, the latest cranes to emerge, which in turn permits us to "go meta" in a way no other life form can do, reflecting in many ways on who and what we are and how we got here, modeling these processes in plays and novels, theories and computer simulations, and ever-more thinking tools to add to our impressive toolbox.
This perspective is so widely unifying and at the same time so generous with detailed insights that one might say it's a power tool, all on its own. Those who are still strangely repelled by Darwinian thinking must consider the likelihood that if they try to go it alone with only the hand tools of tradition, they will find themselves laboring far from the cutting edge of research on important phenomena as diverse as epidemics and epistemology, biofuels and brain architecture, molecular genetics, music, and morality.














VII. TOOLS FOR THINKING ABOUT CONSCIOUSNESS

 

Well armed with dozens of thinking tools, we finally arrive at the topic that many regard as the most puzzling phenomenon in the whole universe. Indeed not a few folks have claimed that it is terminally mysterious. We will never ever understand consciousness, they declare; it will systematically defy the best efforts of our science and philosophy until the end of time. Since there are no good reasons to believe in this intellectual roadblock, I have to conclude that it is wishful thinking. Some folks hate the idea of our uncovering the secrets of how the conscious mind works, and just to make sure we don't impose our understanding on them, they counsel that we give it up as a lost cause. If we take their advice, they will be right, so let's ignore them and get on with this difficult, but not impossible, quest.











53. Two Counter-images
Many of the tools I have introduced earlier deal in one way or another with the mind—believing, thinking, and so forth—but I postponed the knotty problems of consciousness until now. There is a reason: when people start musing about consciousness, they have a way of inflating their notions of what consciousness must be, of bamboozling themselves. They tackle the hardest issues before they have had a chance to see how much of the work (and play) of the mind can be accounted for without first settling the perennial questions about conscious experience. Now that we have built a base camp, are we ready to tackle the summit? Yes, but if we think of it that way, we are already making a mistake of imagination! Our minds don't have a single magnificent summit, consciousness. Contrary to a tradition going back at least to Descartes in the seventeenth century, conscious phenomena are neither the most "central" nor the "highest" phenomena in our minds (Jackendoff, 1987; Dennett, 1991a). A seductive bad image needs a counter-image to neutralize it, so here is a simple imagination-adjuster to start us off: recall Cole Porter's wonderful song "You're the Top" and reflect that maybe you're not the top—not the summit of the mountain, but the whole mountain, and what you know and can tell about the mountain that is you is not the view from the summit, but various views from halfway up. You might like to think of the phenomena of consciousness as rather like the fringe of hair around a bald man's pate. Bear that in mind.
And here is another: consciousness is not a medium, like television, into which information can get transduced or recorded, and there is no place in the brain where "it all comes together" for the appreciation of some Central Witness—I call this imaginary place the Cartesian Theater (Dennett, 1991a). Consciousness is more like fame than television: fame in the brain, cerebral celebrity, a way in which some contents come to be more influential and memorable than the competition. Instead of arguing for this (for the arguments, see Dennett, 1991a, 2005b), I am simply offering you this thinking tool, take it or leave it, with some friendly advice: whenever you find yourself thinking of entry into consciousness as Arrival at Headquarters or as Translation from unconscious neural spike-signals into Something Else, remind yourself of these counter-images and ask yourself if you might be mis-imagining the phenomena.











54. The Zombic Hunch
Most people have a hunch—it's no better than that, really—that no robot (made of silicon and metal and plastic, etc.) could ever be conscious the way we humans are. There is something about our living, breathing, organic bodies and brains that is necessary for consciousness. This is an intuition that hardly needs pumping, so ubiquitous is it, and maybe these people are right. But now that we have some insight into the way our bodies and brains can themselves be seen as made of robots made of robots made of robots, and so on, all the way down to below the neuron level where the motor proteins and other nanobots trudge along making the whole system work, we can see that maybe that hunch is just an artifact of impoverished imagination: folks have been thinking about robots that are too simple, by many orders of magnitude. A friend once responded to this opening gambit of mine by trying to nip it in the bud: "I just can't conceive of a conscious robot!" Nonsense, I replied. What you mean is that you won't conceive of a conscious robot. You think it is silly and preposterous to try to take the idea seriously. But it's actually child's play to conceive of a conscious robot, or, for that matter, a conscious choochoo train (The Little Engine That Could) or a conscious Christmas tree (all those sappy children's stories about lonely little fir trees that pine for a home). Anybody who has seen Star Wars has spent an hour or so conceiving of R2D2 and C3PO as conscious. We have done this since we were children, "without a second thought" for the most part. It's not just easy, it's almost irresistible when we are confronted with something that acts—and especially, talks—like a human being.
Here's an interesting fact: since the pioneering work of neuroscientist Wilder Penfield in Montreal back in the 1950s, there have been many operations that exposed the brains of wide-awake patients who were able to say what it was like when their brains were stimulated there or just there. I don't think any participant or observer of one of these operations has ever thought, "Oh my gosh! This isn't a person. This is a zombie. It has to be, because when we look inside, all we find is gray matter." No, it is just too obvious—look! listen!—that the patient is conscious. And it would be just as obvious, really, if we were to open up somebody's skull while he or she is conversing with us and find the cranial cavity packed with microchips. We'd learn, perhaps to our surprise, that a conscious robot was not just easy to conceive of or imagine, but actual.
Some philosophers think that your imagination would be playing a trick on you if you fell for this "merely behavioral" evidence of consciousness and jumped to that conclusion. "Don't fall, don't jump!" might be their motto. Proving that another person is conscious is much harder than that, since that person might—it's at least a logical possibility—be a "zombie." Not a voodoo-style zombie of the sort that one sees in movies or dresses up as at Halloween. The walking dead are readily distinguishable from normal people by their behavior (and their horrible appearance). Philosophers' zombies, in contrast, can be delightful company, the life of the party, as loving and joyous and spontaneous as anybody you know. Some of your best friends might be zombies. Philosophers' zombies are (by definition) behaviorally indistinguishable from normal conscious human beings, but "there's nobody home"—they entirely lack any inner life, any conscious experience. They just appear from the outside to be conscious. If you agree with these philosophers that this is a serious issue, if you wonder—given the logical possibility of philosophers' zombies—how there could ever be a scientific, materialistic theory of consciousness, then you are gripped by the Zombic Hunch.*
Let me acknowledge right away that I can feel the Zombic Hunch as vividly as anyone. When I squint just right, it does sort of seem that consciousness must be something in addition to all the things it does for us and to us, some kind of special private glow or here-I-am-ness that would be absent in any robot, and that is all but unimaginable as a "mere" physical activity of the brain. But I've learned not to credit the hunch. I think it is a flat-out mistake, a failure of imagination, not an insight into necessity. Convincing others of this, however, proves to be no easy task, and we will need several different intuition pumps to loosen the grip of the Zombic Hunch.
To start, we may get our bearings on this logical possibility by comparing it with a few others. It is logically possible that you are living in the Matrix, and all of the life you see around you and apparently participate in is a virtual reality show designed to keep you calm as your actual body lies motionless in a high-tech pod of some kind. It is logically possible that there aren't really any carbon atoms; what appear to scientists to be carbon atoms are actually kazillions of tiny spaceships piloted by aliens whose life work is to pretend that they are carbon atoms. It is logically possible that the entire universe was created about six thousand years ago, with all the so-called fossils in place and photons streaming in as if from galaxies light-years away. (It's logically possible that the world was created ten minutes ago, complete with all the purported memories of your past installed in your brain.) We may find such logical possibilities amusing premises for fiction, but we don't take them seriously as signs that our physics and chemistry and biology need to be overhauled or abandoned. Is there anything that makes the Zombic Hunch more substantial, more worthy of consideration? Many serious thinkers have thought so.
Perhaps the grandfather of all intuition pumps designed to yield something like the Zombic Hunch was invented hundreds of years ago by Gottfried Wilhelm Leibniz, the philosopher/mathematician who shares credit with Isaac Newton for inventing calculus. He was as smart and ingenious as any thinker in his age, and yet he fell for this intuition pump of his own devising.
And supposing there were a machine, so constructed as to think, feel, and have perception, it might be conceived as increased in size, while keeping the same proportions, so that one might go into it as into a mill. That being so, we should, on examining its interior, find only parts which work one upon another, and never anything by which to explain a perception. Thus [my italics] it is in a simple substance, and not in a compound or in a machine, that perception must be sought for. [Leibniz, 1714, para. 17]

This "Thus" is one of the most glaring non sequiturs in all of philosophy. Leibniz doesn't give us any intervening argument for the conclusion; he thinks it is too obvious to need any. Recall William Bateson, the early-twentieth-century geneticist who couldn't imagine that genes could be material entities (see p. 100). Just as Bateson had no way of taking seriously the extravagant idea of three-billion base-pairs in a double helix inside every cell (preposterous!), Leibniz had no way of taking seriously the idea of a "mill" with trillions of moving parts. He would insist, no doubt, that "just adding more parts" couldn't take you from machinery to mind, but that would be only his hunch, not anything he could claim to demonstrate. But if Darwin, Crick, and Watson have exposed Bateson's failure of imagination, Turing has rendered Leibniz's intuition pump obsolete. Except that he hasn't. Not yet. In due course, I think, the Zombic Hunch will fade into history, a curious relic of our spirit-haunted past, but I doubt that it will ever go extinct. It will not survive in its current, mind-besotting form, but it will survive as a less virulent mutation, still psychologically powerful but stripped of authority. We've seen this happen before. It still seems as if the earth stands still and the sun and moon go around it, but we have learned that it is wise to disregard this as mere appearance. It still seems as if there's a difference between a thing at absolute rest and a thing that is merely not accelerating within an inertial frame, but we have learned not to trust this feeling. I anticipate a day when philosophers and scientists and laypeople will chuckle over the fossil traces of our earlier bafflement about consciousness: "It still seems as if these mechanistic theories of consciousness leave something out, but of course we now understand that that's an illusion. They do, in fact, explain everything about consciousness that needs explanation."
Continuing allegiance to the Zombic Hunch is actually fostered by many philosophers' thought experiments, such as John Searle's famous Chinese Room, the inspiration for my coinage, "intuition pump." It will soon get dismantled in front of your eyes. But first I want to explore the concept of philosophical zombies a bit more carefully.











55. Zombies and Zimboes
When people say they can conceive of (philosophical) zombies, we are entitled to ask them how they know. Conceiving is not easy! Can you conceive of more than three dimensions? The curvature of space? Quantum entanglement? Just imagining something is not enough—and in fact, Descartes tells us, it is not conceiving at all. According to Descartes, imagining uses your (ultimately mechanistic) body, with all its limitations (nearsightedness, limited resolution, angle, and depth); conceiving uses just your mind, which is a much more powerful organ of discernment, unfettered by the requirements of mechanism. He offers a compelling example of the difference: the chiliagon, a regular thousand-sided polygon. Can you conceive of one? Can you imagine one? What is the difference? Let's try imagining one first. Start with a pentagon, say, then imagine a decagon. It's hard, but you know what to do: bend each side of the pentagon in the middle and push out a little bit, turning five equal sides into ten. How far should you push out? Just inscribe the pentagon inside a circle and push the new sides out till they intersect the circle. Now do it again, making an icosagon, with twenty sides.



Do it again, and again seven more times, and you'll have a regular 1,280-sided polygon, which is well-nigh indistinguishable from a circle in imagination, but is as distinct in conception from a circle—and from a chiliagon—as a square is distinct from a circle. If I ask you to imagine a circle inside a chiliagon inside a circle inside a chiliagon inside a circle, making a sort of bull's-eye target, can you tell in your mental image which are the circles and which are the chiliagons? No, they all look just like circles, but you have no difficulty conceiving of the circumstance you've been asked to think about.
Descartes doesn't tell you to perform such constructions; to him conception, like imagination, is a kind of direct and episodic mental act, glomming without bothering to picture, or something like that. You somehow just grasp (mentally) the relevant concepts (SIDE, THOUSAND, REGULAR, POLYGON), and shazam! You've got it. I have always been suspicious of this Cartesian basic act of conceiving. If you can do it, good for you, but I find that I can't (any more than I can conceive a baby, in another sense of the term). I am not confident that I have succeeded in conceiving of something until I have manipulated the relevant ideas for some time, testing out implications in my mind, doing exercises, in effect, until I get fluent in handling the tools involved. (And when I do these mental gymnastics, I make heavy use of my imagination, exploring various diagrams and pictures in my head, for instance. In short, I exploit what Descartes would disparage as merely my imagination to accomplish what he would celebrate as my conception.) Can you conceive of string theory? Do you find all that talk about umpteen dimensions filled with superstrings and "branes" and the like easy to make sense of, easy to test for logical consistency? It is unintelligible to me, but for that very reason I wouldn't be willing to declare it inconceivable or impossible (Ross, 2013). I am unconvinced, but I am also not so confident in my own conceptual abilities as to dismiss it as nonsense. I haven't been able to conceive of the truth of string theory yet. We should not put much weight on blithe verdicts of conceivability or inconceivability in the absence of careful demonstrations. Bateson said that a material gene was "inconceivable," but if he were alive today, he could readily learn to conceive of one. After all, children in grade school learn about the double helix with all its rungs, a phenomenon that proves readily conceivable to them once they get the hang of it. But no amount of new information and techniques of imagination will help us conceive of a round square (a regular four-sided polygon whose boundaries are everywhere equidistant from its center), or a largest prime number.
I am pretty sure that a philosophers' zombie is conceptually incoherent, impossible, a bankrupt idea. But don't take my word for it. What could you do to convince yourself that you can conceive of a philosophers' zombie? Suppose you try to imagine that your friend Zeke "turns out to be" a zombie. What would convince you or even tempt you to conclude that this is so?* What difference would make all the difference? Remember, nothing Zeke could do should convince you that Zeke is, or isn't, a zombie. I find that many people don't do this exercise correctly; that is, they inconveniently forget or set aside part of the definition of a philosophers' zombie when they attempt their feat of conception. It may help you see if you are making this mistake if we distinguish a special subspecies of zombies that I call zimboes (Dennett, 1991a). All zombies have nonconscious (of course) control systems that extract information from the world (via their zombie eyeballs and ears) and exploit that information to avoid walking into walls and to turn when you call, and so on. They are all intentional systems, in other words. But a zimbo is special, a zombie that is also blessed with equipment that permits it to monitor its own activities, both internal and external, so it has internal (nonconscious) higher-order informational states that are about its other internal states. Further self-monitoring allows a zimbo to have and use information about those very self-monitoring states, and so on, indefinitely. A zimbo, in other words, is equipped with recursive self-representation—unconscious recursive self-representation, if that makes any sense. It is only in virtue of this special talent that a zimbo can participate in the following sort of conversation:

YOU: Zeke, do you like me?
ZEKE: Of course I do. You're my best friend!
YOU: Did you mind my asking?
ZEKE: Well, yes, it was almost insulting. It bothered me that you asked.
YOU: How do you know?
ZEKE: Hmm. I just recall feeling a bit annoyed or threatened or maybe just surprised to hear such a question from you. Why did you ask?
YOU: Let me ask the questions, please.
ZEKE: If you insist. This whole conversation is actually not sitting well with me.

Remember: since a philosophical zombie is declared to be behaviorally indistinguishable from a conscious person, behaviors like carrying out this conversation are within its repertoire, and in order to control such behaviors the zombie is going to need recursive self-representation. It can "think" (in its unconscious zombie way) about how it feels about how it felt about what it was thinking of when it wondered whether ..., and so forth. It is easy enough to imagine your dreadful suspicions being aroused if Zeke just draws a blank, becoming weirdly unresponsive when you query him this way, but that would be discovering that if Zeke is a zombie, he isn't a zimbo. You should always make sure you're thinking of a zimbo when you ask if a philosophers' zombie is a real possibility, for only a being with recursive self-representation would be able to hold its own in everyday interactions like this conversation, to say nothing of writing poetry, framing novel scientific hypotheses, and acting in dramas, all of which are within the competences of zimboes—by definition.
Unless you go to the trouble of imagining, in detail, how indistinguishable "normal" Zeke would be from zimbo Zeke, you haven't really tried to conceive of a philosophers' zombie. You're like Leibniz, giving up without half trying. Now ask yourself a few more questions. Why would you care whether Zeke is a zimbo? Or more personally, why would you care whether you are, or became, a zimbo? In fact, you'd never know.
Really? Does Zeke have beliefs? Or does he just have sorta beliefs, "you know, the sort of informational states-minus-consciousness that guide zimboes through their lives the way beliefs guide the rest of us"? Only here, the sorta beliefs are exactly as potent, as competent, as "the real thing," so this is an improper use of the sorta operator. We can bring this out by imagining that left-handers (like me, DCD) are zimboes; only right-handers are conscious!

DCD: You say you've proved that we lefties are zombies? I never would have guessed! Poor us? In what regard?
RIGHTIE: Well, by definition you're not conscious—what could be worse than that?
DCD: Worse for whom? If there's nobody home, then there's nobody in the dark, missing out on everything. But what are you doing, trying to have a conversation with me, a zimbo?
RIGHTIE: Well, there seems to me to be somebody there.
DCD: To me too! After all, as a zimbo, I have all manner of higher-order self-monitoring competences. I know when I'm frustrated, when I'm in pain, when I'm bored, when I'm amused, and so forth.
RIGHTIE: No. You function as if you knew these things, but you don't really know anything. You only sorta know these things.
DCD: I think that's a misuse of the sorta operator. What you're calling my sorta knowledge is indistinguishable from your so-called real knowledge—except for your "definitional" point: zimbo knowledge isn't real.
RIGHTIE: But there is a difference, there must be a difference!
DCD: That sounds like bare prejudice to me.

If this isn't enough rehearsal of what it would be like to befriend a zimbo, try some more. Seriously, consider writing a novel about a zimbo stuck in a world of conscious people, or a conscious person marooned on the Island of the Zimboes. What details could you dream up that would make this a credible tale? Or you could take an easier path: read a good novel while holding onto the background hypothesis that it is a novel about zimboes. What gives it away or disconfirms your hypothesis? Novelists have a choice of standpoints or narrative modes, including the first-person narrative used by Herman Melville in Moby-Dick and by J. D. Salinger in The Catcher in the Rye:

"Call me Ishmael."
"When I really worry about something, I don't just fool around. I even have to go to the bathroom when I worry about something. Only, I don't go. I'm too worried to go. I don't want to interrupt my worrying to go."

Other novelists opt for the third-person omniscient narrative. Curiously, the first-person narrative mode might seem to lend itself more to sustaining the zombie hypothesis. After all, the entire story merely depicts that narrative behavior of Zimbo Ishmael or Zimbo Holden Caulfield. We see only their outsides, and what they purport to be accounts drawing on their inner lives! Compare those first-person narratives to these third-person narratives in Jane Austen's Persuasion and Fyodor Dostoevsky's Crime and Punishment, for example:

She [Elizabeth] felt that Mrs Musgrove and all her party ought to be asked to dine with them, but she could not bear to have the difference of style, the reduction of servants, which a dinner must betray, witnessed by those who had been always so inferior to the Elliots of Kellynch. It was a struggle between propriety and vanity; but vanity got the better, and then Elizabeth was happy again.
He [Raskolnikov] looked at Sonia and felt how great her love for him was, and, strange to say, he felt distressed and pained that he should be loved so much. Yes, it was a queer and dreadful sensation.

Here, it seems, the authors let us "look right into the minds" of Elizabeth and Raskolnikov, so how could they be zimboes? But remember: where conscious people have a stream of consciousness, zimboes have a stream of unconsciousness. After all, zimboes are not supposed to be miraculous; their behaviors are controlled by a host of internal goings-on of tremendous informational complexity, and modulated by functional emotion-analogues that amount to happiness, distress, and pain. So both Elizabeth and Raskolnikov could be zimboes, with Austen and Dostoevsky using the terms we all know and love from folk psychology to render descriptions of their inner goings-on, just as chess programmers talk about the iterated "searches" and risky "judgments" of their computer programs. A zimbo can be embarrassed by a loss of social status, or smothered by love.
Never forget William Bateson's failure of imagination. When I try my hardest to avoid that trap, hunting for loopholes in my background assumptions, keeping my eye peeled for ways in which I could be proved wrong about zombies, I always come up with imagined discoveries that show, at most, that the whole concept of consciousness is seriously confused. For instance, I can imagine that there might be two (or seven, or ninety-nine) different sorts of so-called consciousness, and lefties have one, and righties have another, and lobsters have yet another. But the only way I can imagine this (so far) is by imagining that they are distinguishable by the following functional differences: lefties can't do X, and righties can't do Y, and so on. But those distinguishable differences just go to show that we're not talking about philosophical zombies after all, for (by definition) there are no distinguishable-from-the-outside differences between philosophical zombies and "genuinely conscious" people. And nobody has yet been able to articulate a distinguishable-from-the-inside mark of genuine consciousness that doesn't somehow involve the putatively conscious person being able to do something "mental" that convinces us (and her) that she is conscious. But whatever this mental difference is would presumably have its counterpart sham version in the zombie's "stream of unconsciousness." If not, why not? So I am quite confident that the whole idea of a philosophical zombie is a sort of intellectual hallucination, an affliction one can outgrow. Try it. I'll provide some further assistance for this task of self-persuasion later in this section.











56. The Curse of the Cauliflower
I see you tucking eagerly into a helping of steaming cauliflower, the merest whiff of which makes me faintly nauseated, and I find myself wondering how you could possibly relish that taste, and then it occurs to me that to you, cauliflower probably tastes (must taste?) different. A plausible hypothesis, it seems, especially since I know that the very same food often tastes different to me at different times. For instance, my first sip of breakfast orange juice tastes much sweeter than my second sip if I interpose a bit of pancakes and maple syrup, but after a swallow or two of coffee, the orange juice goes back to tasting (roughly? exactly?) the way it did during the first sip. Surely (ding!) we want to say (or think about) such things, and surely (ding!) we are not wildly wrong when we do, so ... surely (ding!) it is quite okay to talk of the way the juice tastes to Dennett at time t, and ask whether it is just the same as or different from the way the juice tastes to Dennett at time t', or the way the juice tastes to Jones at time t. Call these ways things can seem to us qualia.
This "conclusion" seems innocent, but right here we have already made the big mistake. The final step presumes that we can isolate the "qualia" from everything else that is going on—at least in principle or for the sake of argument. What counts as the way the juice tastes to x can be distinguished, one supposes, from what is a mere accompaniment, contributory cause, or by-product of this "central" way. One dimly imagines taking such cases and stripping them down gradually to the essentials, leaving their common residuum, the way things look, sound, feel, taste, smell to various individuals at various times, independently of how those individuals are stimulated or non-perceptually affected, and independently of how they are subsequently disposed to behave or believe. The mistake is not in supposing that we can in practice ever or always perform this act of purification with certainty, but the more fundamental mistake of supposing that there is such a residual property to take seriously, however uncertain our actual attempts at isolation of instances might be.
The examples that seduce us are abundant in every modality. I cannot imagine, will never know, could never know, it seems, how Bach sounded to Glenn Gould. (I can barely recover in my memory the way Bach sounded to me when I was a child.) And I cannot know, it seems, what it is like to be a bat (Nagel, 1974), or whether you see what I see, color-wise, when we look up at a clear "blue" sky. These everyday cases convince us of the reality of these special properties—those subjective tastes, looks, aromas, sounds—that we then apparently isolate for definition by this act of philosophical distillation. Thus are qualia born.
"Qualia" is a "technical" term for something that could not be more familiar to each of us: the ways things seem to us. Nothing, it seems, could you know more intimately than your own qualia; let the entire universe be some vast illusion, some mere figment of Descartes's evil demon, and yet what the figment is made of (for you) will be the qualia of your hallucinatory experiences. Descartes claimed to doubt everything that could be doubted, but he never doubted that his conscious experiences had qualia, the properties by which he knew or apprehended them.
This definition of qualia may seem clear enough—the ways things seem to us—but although qualia thus introduced have been the subject of much analysis and discussion by philosophers, there is still no consensus about just what the term means or implies, technically speaking. Many cognitive scientists have made the charitable assumption that philosophers must know what they are talking about when they use this special term, and have added the term to their working vocabulary, but this is a tactical mistake. Controversy still rages on what qualia are and aren't, quite independently from any empirical issues. Some years ago, in an essay, I (1988a) proposed that the root concept of qualia has four conditions. Qualia are

ineffable,
intrinsic,
private, and
directly apprehensible ways things seem to one.

That is to say, they are (1) somehow atomic to introspection, and hence indescribable ("you had to be there"); (2) not relational or dispositional or functional (the color red may be anxiety-provoking to some people, but that subjective disposition is not a quale of red); (3) "You had to be there, but you can't be; they're mine and mine alone!"; and (4) your qualia are known to you more intimately than anything else.
This is still regarded as a good starting place in most circles, but since the point of that essay was to show that nothing could meet these four conditions, there has been ample discussion of revised or improved versions of the concept, with no emerging consensus. It is not unusual for a much-used and generally highly regarded technical term to have multiple incompatible definitions—think of "gene" or "species" in biology and "cause" almost everywhere in science—but the disarray surrounding "qualia" strikes me as worse, a Trojan horse to anyone in other disciplines who thinks of the concept as a gift from philosophy that might come in handy in their own research.
I trotted out thirteen other intuition pumps (in addition to the cauliflower) in that essay, and I will not repeat any of them here, since over the years I have devised other, probably more powerful tools to wield in my continuing battle against the complacency expressed in a famous response to the challenge to say what, exactly, qualia are. Ned Block (1978, p. 281) dismissed this obstreperous query "only half in jest" by invoking Louis Armstrong's legendary reply when he was asked what jazz was: "If you got to ask, you ain't never gonna get to know." This amusing tactic perfectly expresses the presumption that is my target. If I succeed in my task, this response of Block's, which still passes muster in most circles today, will look as quaint and insupportable as a jocular expression of disbelief by a vitalist who, when confronted with a person—"a living thing, mind you!"—claiming to doubt the very existence of élan vital.











57. Vim: How Much Is That in "Real Money"?
It is a common opinion that although you could construct a robotic model of color vision, say, that exhibited all the familiar phenomena we humans experience, such as complementary afterimages and color contrast illusions, and although the robot would have similar internal processes that accounted for its afterimages and so forth, since "it's just a robot" it couldn't have red and blue qualia. The functional states that signaled or represented colored things in front of the robot's television camera eyes would not have that extra something that we have. Thomas Nagel's famous essay "What Is It Like to Be a Bat?" (1974) provides us with a standard way of alluding to the conscious states, if any, of an entity. It wouldn't be like anything to be a robot having an afterimage. Why do so many people think this is obvious? It might be because they are imagining a relatively simple robot and failing to note that you can't draw conclusions about all robots from facts about all simple robots. Of course if you define qualia as intrinsic properties of experiences considered in isolation from all their causes and effects, and logically independent of all dispositional properties, then qualia are logically guaranteed to elude all functional analysis. No amount of clever engineering could endow a robot with qualia—but this is an empty victory, since there is no reason to believe such intrinsic properties exist.
To see this, compare the qualia of experience to the value of money. Some naïve Americans seem to think that dollars, unlike euros and yen, have intrinsic value. The tourist in the cartoon asks, "How much is that in real money?" meaning how much is that in dollars. Let's push this idea a little farther: These naïve Americans are willing to exchange dollars for euros; they are happy to "reduce" the value of other currencies to their exchange rate with dollars (or goods and services), but when they do this, they have a sense that dollars are different. Every dollar, they declare, has something logically independent of the functional exchange powers it shares with all other currencies in circulation. A dollar has a certain je ne sais quoi. When you contemplate it, you can detect that it has an aura of value—less than in the olden days, perhaps, but still discernible: let's call it the dollar's vim (from the Latin, vis, meaning power). Officially, then, vim is the non-relational, non-dispositional, intrinsic economic value of a dollar. Pounds sterling and euros and the like have no intrinsic value—they are just symbolic stand-ins; they are redeemable for dollars, and hence have derived economic value, but they don't have vim! How sad for those Europeans! Their currencies lack intrinsic economic value! How do they bear it? How do they motivate themselves to earn their pay, these poor vimbies! Those of us fortunate enough to get paid in dollars acquire items with a good helping of vim attached. No wonder the U.S. dollar has been adopted as the currency of choice in so many nations! Even foreigners can sense the vim of a dollar.
So say our imaginary American tourists. So defined, the vim of each dollar is guaranteed to elude the theories of economists forever, since no economic theory could ever account for intrinsic economic value. So much the worse for economics? The existence of vim would render economics a seriously incomplete science, but fortunately, we have no good reason to believe that there is such a thing as intrinsic economic value. Vim is quite obviously a figment of the imagination, an artifact of the heartfelt hunches of those naïve Americans, and we can explain the artifact without honoring it.
Some participants in the consciousness debates are rather like the imaginary tourists. They simply insist, flat out, that their intuitions about intrinsic phenomenal properties are a nonnegotiable starting point for any science of consciousness. Such a conviction should be considered an interesting symptom deserving a diagnosis, a datum that any science of consciousness must account for, in the same spirit that economists and psychologists might set out to explain why so many people succumb to the potent illusion that money has intrinsic value. (When Europe switched to the euro, people who were used to conceiving of prices in terms of francs and marks and lire and the like went through an awkward period when they could no longer rely on "translations" into their home-grown versions of "real money." See Dehaene and Marques, 2002, for a pioneering exploration of this phenomenon.)
There are many properties of conscious states that can and should be subjected to further scientific investigation right now, and once we get accounts of them in place, we may well find that they satisfy us as an explanation of what consciousness is. After all, this is what happened in the case of the erstwhile "mystery" of what life is. Vitalism—the insistence that there is some big, mysterious extra ingredient in all living things, dubbed élan vital—turns out to have been a failure of imagination. Vitalism today is all but extinct, though there are still a few cranks around who haven't given up. Inspired by that happy success story, we can proceed with our scientific exploration of consciousness. If the day arrives when all the demonstrable features of consciousness are explained, all the acknowledged intellectual debts are paid, and we plainly see that something big is missing (it should stick out like a sore thumb at some point, if it is really important), those with the unshakable hunch will get to say they told us so. In the meantime, they can worry about how to fend off the diagnosis that they, like the vitalists before them, have been misled by an illusion. Here is the challenge, then, to those who believe in qualia as intrinsic properties of experiences: How do they distinguish their conviction from the mistake of the naïve Americans? (Or are the Americans right? Dollars do have vim, as anybody can just intuit!)











58. The Sad Case of Mr. Clapgras
So what are qualia, then, if not intrinsic properties of conscious experiences? One evening, over a fine bottle of Chambertin, the philosopher Wilfrid Sellars said to me, "Dan, qualia are what make life worth living!" That's an attractive idea. Let's consider what it would imply about qualia. To see what is at issue, I will present an intuition pump against a background of recent work in cognitive neuroscience on several bizarre and counterintuitive pathologies: prosopagnosia and Capgras delusion.
Prosopagnosics have normal vision in most regards, but they cannot recognize faces. They can tell a male from a female, old from young, African from Asian, but faced with several close friends of the same gender and age, they are unable to tell which is which until they hear a voice or detect some other identifying peculiarity. Given a row of photographs of people, including famous politicians and movie stars, family members, and anonymous strangers, a prosopagnosic asked to identify any who are known to him will generally perform at chance. Those of us who are not prosopagnosics may find it difficult to imagine what it's like to look right at one's mother, say, and be unable to recognize her. Some may find it hard to believe that prosopagnosia exists. When I tell people about these phenomena, I often discover skeptics who are quite confident that I am making these facts up. But we must learn to treat such difficulties as measures of our frail powers of imagination, not insights into impossibility. Prosopagnosia (from the Greek prosopon, which means "face," and agnosia, "not knowing") is a well-studied, uncontroversial pathology afflicting thousands of people.
One of the most interesting facts about (many) prosopagnosics is that in spite of their inability to identify or recognize faces as a conscious task, they can respond differently to familiar and unfamiliar faces, and even respond in ways indicating that unbeknownst to themselves, or covertly, they were able to identify the faces they were unable to identify when asked. For instance, such covert recognition is demonstrated when prosopagnosics are shown pictures and given five candidate names from which to choose. They choose at chance, but their galvanic skin response—a measure of emotional arousal—shows a distinct rise when they hear the correct name associated with the picture. Or consider this simple test: Which of the following are names of politicians: Marilyn Monroe, Al Gore, Margaret Thatcher, Mike Tyson? You should be able to execute this task swiftly, but if each name were shown with the wrong picture of the person, your response would be markedly delayed. This could be explained only if at some level, you were actually identifying the faces, even though it was strictly irrelevant to the task. It seems, then, that there are (at least) two largely independent visual face-recognition systems in the brain: the impaired conscious system, which cannot help the subjects in the task presented by the experiment, and the unimpaired unconscious system, which responds with agitation to the mismatched names and faces. Further tests show that the impaired system is "higher," in the visual cortex, while the unimpaired system has connections to the "lower," limbic system. This oversimplifies a richer story about the varieties of prosopagnosia and what is now known about the brain areas involved, but it will do for our purposes, when we turn to the even stranger pathology known as Capgras delusion (first described by the French psychiatrist Jean Marie Joseph Capgras in 1923).
People who suffer from Capgras delusion suddenly come to believe that a loved one—a spouse or lover or parent, in most cases—has been covertly replaced with a replica impostor. Capgras sufferers are not insane; they are otherwise quite normal people who, as a result of brain injury, suddenly acquire this particular belief, which they maintain with such confidence, in spite of its extravagance or its utter unlikeliness, that in some cases they kill or seriously harm the "impostor"—actually their loved one. At first glance it must seem simply impossible for any brain damage to have precisely this weird effect. (Should we also expect there to be people who get hit on the head and thereafter believe that the moon is made of green cheese?) But cognitive neuroscientist Andrew Young saw a pattern, and proposed that the Capgras delusion was simply the "opposite" of the pathology that produces prosopagnosia. In Capgras, the conscious, cortical face-recognition system is spared—that's how the deluded sufferer recognizes the person standing in front of him as the spitting image of his loved one—but the unconscious, limbic system is disabled, draining the recognition of all the emotional resonance it ought to have. The absence of that subtle contribution to identification is so upsetting ("Something's missing!") that it amounts to a pocket veto on the positive vote of the surviving system's identification of the familiar person: the emergent result is the sufferer's heartfelt conviction that he or she is looking at an impostor. Instead of blaming the mismatch on their own faulty perceptual system, Capgras sufferers blame the world, in a way that is so metaphysically extravagant, so improbable, that there can be little doubt of the power (the political power, in effect) that the impaired unconscious face-recognition system normally has in us all. When this particular system's epistemic hunger goes unsatisfied, it throws such a fit that it overthrows the contributions of the other systems.
Haydn Ellis and Young first proposed this hypothesis in 1990, and since then Young and neuroscientist Chris Frith and others have confirmed and elaborated it. There are, of course, complications that I will not dwell on, since I want to use this particular bit of imagination-stretching cognitive neuroscience to open our minds to yet another possibility, not yet found but imaginable. This is the imaginary case of poor Mr. Clapgras, a name I have made up to remind us of its inspiration: the real syndrome of Capgras delusion. (This scenario joins a large throng of philosophers' intuition pumps exploring imaginable disruptions in a person's consciousness that might bear on the nature of qualia.)
Mr. Clapgras earns a modest living as an experimental subject in psychological and psychophysical experiments, so he is far from naïve about his own subjective states. One day he wakes up and cries out in despair as soon as he opens his eyes: "Aargh! There's something wrong! The whole world is just ... weird, just ... awful, somehow wrong! I don't know if I want to go on living in this world!" Clapgras closes his eyes and rubs them; he cautiously opens them again, only to be confronted yet again by a strangely disgusting world, familiar but also different in some way that defies description. That's what he says. "What do you see when you look up?" he is asked. "Blue sky, with a few fleecy white clouds, some yellowish-green buds on the springtime trees, a bright red cardinal perched on a twig," he replies. Apparently his color vision is normal, but just to check, he is given the standard Ishihara test, which shows he is not color-blind, and he correctly identifies a few-dozen Munsell color chips. Almost everybody is satisfied that whatever poor Mr. Clapgras's ailment is, it doesn't involve his color vision, but one researcher, Dr. Chromaphil, holds out for a few more tests.
Chromaphil has been conducting research on color preferences, emotional responses to color, and the effects of different colors on attention, concentration, blood pressure, pulse rate, metabolic activity, and a host of other subtle visceral responses. Over the past six months he has accumulated a huge database of Mr. Clapgras's responses, both idiosyncratic and common, on all these tests, and he wants to see if there have been any changes. He retests Clapgras and notices a stunning pattern: all the emotional and visceral responses Clapgras used to exhibit to blue he now exhibits to yellow, and vice versa. His preference for red over green has been reversed, as have all his other color preferences. Food disgusts him—unless he eats in the dark. Color combinations he used to rate as pleasing he now rates as jarring, while finding the combinations of their "opposites" pleasing, and so forth. The shade of shocking pink that used to set his pulse racing he still identifies as shocking pink (though he marvels that anybody could call that shade of pink shocking), but it is now calming to him, while its complement, a shade of lime green that used to be calming, now excites him. When he looks at paintings, his trajectory of saccades—the swift jumps our eyes make as they scan anything—is now profoundly unlike his earlier trajectories, which were apparently governed by subtle attention-grabbing, gaze-deflecting effects of the colors on the canvas. His ability to concentrate on mental arithmetic problems, heretofore seriously depressed by putting him in a bright-blue room, is now depressed by putting him in a bright-yellow room.
In short, although Clapgras does not complain about any problems of color vision, and indeed passes all standard color-naming and color-discriminating tests with, well, flying colors, he has undergone a profound inversion of all his emotional and attentional reactions to colors. What has happened to Clapgras, Dr. Chromaphil tells his amazed and skeptical colleagues, is simple: he's undergone a total color qualia inversion, while leaving intact his merely high-level cognitive color talents—his ability to discriminate and name colors, for instance, the talents a color-sensitive robot could have.
Now what should we say? Have Clapgras's qualia been inverted? Since the case is imaginary, it seems that we can answer it however we like, but philosophers have been taking other imaginary cases seriously for years, thinking that profound theoretical issues hinge on how they are decided, so we mustn't just dismiss the case. First, is this a possible case? It may depend on what kind of possibility we are talking about. Is it logically possible? Is it physiologically possible? These are profoundly different questions. Philosophers have tended to ignore the latter as quite irrelevant to philosophical concerns, but in this case they may relent. I can see no way of arguing that the case is logically impossible. Clapgras, as described, has a strange combination of spared abilities and shocking new inabilities; dispositions that are normally tightly linked are here dissociated in unprecedented ways, but is his condition any more radical in this regard than either prosopagnosia or Capgras delusion? I am not sure Clapgras's condition is even physiologically impossible; there are well-studied cases of subjects who can discriminate colors just fine but not name them (color anomia), and of subjects who become color-blind but don't notice their new deficit, blithely confabulating and naming colors at chance without any recognition that they are guessing. Clapgras, like a Capgras sufferer, has no problems with recognition or naming; it is the subtle ineffable flavoring that has gone all askew in him—all the personal dispositions that make paintings worth looking at, rooms worth painting, color combinations worth choosing. The effects of colors that contribute to making life worth living are what changed in Clapgras—in other words (if Sellars was right), his color qualia.
Suppose we put the issue to Clapgras and ask him if his color qualia have been inverted. He has three possible answers: Yes, No, and I don't know. Which should he answer? If we compare my story of Clapgras with the many tales of inverted qualia that have been soberly promulgated and discussed at great length by philosophers, the most disturbing innovation is the prospect that Clapgras might have his qualia inverted and be none the wiser. Remember that Dr. Chromaphil had to propose this hypothesis to his skeptical colleagues, and Clapgras may well share their skepticism. After all, he not only hasn't complained of any problem with his color qualia (as in the standard stories), but also has satisfied himself that his color vision is just fine in the same way he satisfied the researchers: by easily passing the standard color vision tests. This feature of the story ought to cause some discomfort, for it is commonly assumed in the philosophical literature that such behavioral self-testing is irrelevant: surely (ding!) those tests have no bearing at all on qualia. Those tests are standardly characterized as having no power at all to illuminate or constrain qualia quandaries. But, as my variation shows, philosophers' imaginations have overlooked the prospect of somebody being at least tempted to rely on these tests to secure his own confidence that his qualia have not changed.
Can your qualia stay constant while you undergo a change in "affect"? Philosophers are divided over how to answer such definitional questions about qualia. Consider the effect of monosodium glutamate (MSG), the flavor enhancer. There is no doubt that it makes food seem tastier, more strongly flavored, but does it change the qualia of food, or does it merely heighten the sensitivity of people to the qualia they were already enjoying? This is an appeal for a clarification of the concept of qualia, not an empirical question about the site of action of MSG, or the variation in human responses to MSG as shown by subjects' reports, since until we settle the conceptual question one way or another, any discoveries about the underlying neural processes or heterophenomenology of the subject would be systematically ambiguous. What I want to know is simply how philosophers mean to use the word "qualia"—do they identify all changes in subjective response as changes in qualia, or is there some privileged subset of responses that in effect anchor the qualia? Is the idea of changing one's aesthetic opinion about—or response to—a particular quale nonsense or not? Until one makes decisions about such questions of definition, the term is not just vague or fuzzy; it is hopelessly ambiguous, equivocating between two (or more) fundamentally different ideas.
Have Clapgras's color qualia been inverted? Some philosophers say that I haven't given enough detail in describing his condition. I have described his behavioral competences—he recognizes, and discriminates, and names colors correctly, while responding "wrong" in many other regards—while avoiding describing his subjective state. I haven't said whether, when he looks at a ripe lemon, he experiences intrinsic subjective yellow or, say, intrinsic subjective blue. But that is the point: I am challenging the presumption that these terms name any real properties of his experience at all. Suppose I add that when asked, Clapgras says, "Since I still see ripe lemons as yellow, of course my experience includes the property of intrinsic subjective yellow." Does that settle anything? Do we have any confidence that he knows what he means when he says these words? Should we believe him, or might he be in the grip of a philosophical theory that does not deserve his allegiance?
Here is the main weakness in the philosophical methods standardly used in these cases: philosophers tend to assume that all the competences and dispositions that normal people exhibit regarding, say, colors, form a monolithic block, invulnerable to decomposition or dissociation into independent sub-competences and sub-dispositions. This handily excuses them from addressing the question of whether qualia are to be anchored to some subset or specific disposition. For instance, philosophers George Graham and Terry Horgan (2000, p. 73) speak of "direct acquaintance with phenomenal character itself, acquaintance that provides the experiential basis for [a person's] recognitional/discriminatory capacities." How do they know that this "direct acquaintance" is the "basis" for recognition or discrimination? Prosopagnosics presumably have direct acquaintance with the faces they see, or at least with the "visual qualia" of those familiar faces, but prosopagnosics cannot recognize them as the qualia that are experienced when they are looking at the faces of their friends and family. If, to harken back to Wilfrid Sellars once again, qualia are what make life worth living, then qualia may not be the "experiential basis" for our ability to recognize colors from day to day, to discriminate colors, to name them.











59. The Tuned Deck
In a famous paper, the philosopher David Chalmers (1995) distinguishes the "easy" problems about consciousness from what he calls the (capital "H") Hard Problem of consciousness. What Chalmers would consider "easy" is still difficult enough. Consider, for example, these really challenging questions about consciousness:

How does consciousness enable us to talk about what we see, the sounds we hear, the aromas we smell, and so on? (Oversimplifying, how does the information from the perceiving parts of the brain get used by the language parts of the brain to form the reports and answers we can give?)
When we are doing a routine activity (one we can "almost do in our sleep"), why does our consciousness kick in whenever we run into a problem, and how does consciousness help us with the problem encountered?
How many independently moving things can we consciously track simultaneously, and how do we do it? (The answer is at least four; you can see for yourself by experiencing an amazing demonstration of this phenomenon, known as FINST indexing, at http://ruccs.rutgers.edu/finstlab/MOT-movies/MOT-Occ-baseline.mov.)
When you have something "on the tip of your tongue"—when you know you know it and can almost retrieve the answer—what is going on?
Why do you have to be conscious of a joke to find it funny? (See Hurley, Dennett, and Adams, 2011, for the book-length answer to this one.)

These problems are relatively easy, according to Chalmers, because they involve the cognitive functions of consciousness, the things we can do, using the information-processing and attention-directing processes in the brain, the tracking and reminding and recalling activities that we engage in during our waking lives. However hard it may be to think up promising solutions to them, the solutions will be testable and refinable by experiments, and in fact we're making great progress on these "easy" problems. We can build relatively simple computer models, for instance, that replicate these functions quite persuasively, so we can be quite sure the brain accomplishes them without magic or anything unparalleled in the rest of nature. A robot that exhibited all these phenomena could be constructed, if not today then in the foreseeable future.
The Hard Problem, for Chalmers, is the problem of "experience," what it is like to be conscious, the inexpressible, unanalyzable thusness of being conscious. A robot could behave just as if it were conscious, answering all our questions, tracking all the moving spots, succumbing to and recovering from the tip-of-the-tongue phenomenon, laughing at the right times, and being (unconsciously) puzzled or dumfounded at the right times, but in fact there would be nobody home in there. The robot would be a zombie, without the faintest shadow of the inner life you and I, as normal conscious people, enjoy.
According to Chalmers, you, gentle reader, and I know we are conscious whenever we are up and about. A philosophers' zombie doesn't know any such thing—it is never awake and has no inner life—it just seems from the outside to be conscious. And of course it insists, convincingly, that it is just as conscious as you and I are, and if given a lie-detector test when it says this, it passes the test for sincerity—but, being a zombie, it is mistaken! (Zombies are also indistinguishable from normal conscious people when neuroscientists probe their inner brain states using fMRI machines, and the like.) This makes it clear that telling a conscious person from a zombie is certainly a hard problem—if it is any problem at all. And if that is a problem, explaining how this difference can exist is an even harder problem; it is the Hard Problem. Some of us, myself included, think the Hard Problem is a figment of Chalmers's imagination, but others—surprisingly many—have the conviction that there is or would be a real difference between a conscious person and a perfect zombie and that this is important.
Let me review this curious situation: some of us doubt the very existence of the Hard Problem, but others think we must be crazy to doubt this: nothing, they say, could be more obvious, more immediately intuited by any conscious being, than his or her own special consciousness, and it is this wonderful property we enjoy that defies scientific understanding (so far) and thus deserves to be called the Hard Problem. There is no way to nudge these two alternative positions closer to each other. One side or the other is flat wrong. I have tried for years to show that however tempting the intuition may be, it must be abandoned. I am quite sure that the tempting idea that there is a Hard Problem is simply a mistake, but I cannot prove this. Or, better, even if I could prove this, my proof would often fall on deaf ears, since I am assured by some philosophers that their intuition here is invulnerable bedrock, an insight so obvious and undeniable that no argument could make it tremble, let alone shatter it. So I won't make the tactical error of trying to dislodge with rational argument a conviction that is beyond reason.
This attitude reminds me of the heartfelt convictions often expressed by those who have just seen a spectacular display of stage magic. Every magician knows that people have a tendency to inflate their recollections of any good magic trick they have seen. The shock and bafflement of the moment amplifies their memories, and they earnestly and sincerely insist that they have just seen something that goes beyond what the magician tried to fool them into seeing. Some people want very much to believe in magic. Recall Lee Siegel's comment about "real magic," discussed in chapter 22, on wonder tissue: "Real magic ... refers to the magic that is not real, while the magic that is real, that can actually be done, is not real magic." (See p. 98)
To many people consciousness is "real magic." If you're not talking about something that is supercalifragilisticexpialidocious, then you're not talking about consciousness, the Mystery Beyond All Understanding. The science journalist Robert Wright (2000) expresses the attitude succinctly:
Of course the problem here is with the claim that consciousness is "identical" to physical brain states. The more Dennett et al. try to explain to me what they mean by this, the more convinced I become that what they really mean is that consciousness doesn't exist. [p. 398]

Any bag of tricks in the brain just couldn't be consciousness, not real consciousness. But even those who don't make this preemptive mistake often have a weakness for exaggerating the phenomena of consciousness. (That's why so much of my book Consciousness Explained, 1991a, had to be devoted to deflation, whittling consciousness—real consciousness—down to size, showing people that the phenomena were not as spectacular as most of them think. This exercise in deflation then inspired many readers to joke that my book should have been entitled Consciousness Explained Away or—as Wright suggests—Consciousness Denied.) For those who doubt that they could be flummoxed into an inflated view of consciousness, I want to strike a glancing blow, hoping to banish their complacency by drawing attention to a delicious and disturbing parallel from the world of card magic: The Tuned Deck.

For many years, Mr. Ralph Hull, the famous card wizard from Crooksville, Ohio, has completely bewildered not only the general public, but also amateur conjurors, card connoisseurs and professional magicians with the series of card tricks which he is pleased to call "The Tuned Deck" ...
—John Northern Hilliard, Card Magic (1938)

Ralph Hull's trick looks and sounds roughly like this:

Boys, I have a new trick to show you. It's called "The Tuned Deck." This deck of cards is magically tuned [Hull holds the deck to his ear and riffles the cards, listening carefully to the buzz of the cards]. By their finely tuned vibrations, I can hear and feel the location of any card. Pick a card, any card. ... [The deck is then fanned or otherwise offered for the audience, and a card is taken by a spectator, noted, and returned to the deck by one route or another.] Now I listen to the Tuned Deck, and what does it tell me? I hear the telltale vibrations, ... [buzz, buzz, the cards are riffled by Hull's ear and various manipulations and rituals are enacted, after which, with a flourish, the spectator's card is presented].

Hull would perform the trick over and over for the benefit of his select audience of fellow magicians, challenging them to figure it out. Nobody ever did. (Remember that a cardinal rule of card magic is never to repeat a trick for the audience; this great trick audaciously flouted that rule.) Magicians offered to buy the trick from him, but he would not sell it. Late in his life he gave his account to his friend, Hilliard, who published the account in his privately printed book. Here is what Hull had to say about his trick:

For years I have performed this effect and have shown it to magicians and amateurs by the hundred and, to the very best of my knowledge, not one of them ever figured out the secret. ... the boys have all looked for something too hard [my italics].

Like much great magic, the trick is over before you even realize it has begun. The trick, in its entirety, is in the name, "The Tuned Deck," and more specifically, in one word—"The"! As soon as Hull had announced his new trick and given its name to his eager audience, the trick was over. Having set up his audience in this simple way, and having passed the time with some obviously phony and misdirecting chatter about vibrations and buzz-buzz-buzz, Hull would do a relatively simple and familiar card presentation trick of type A (at this point I will draw the traditional curtain of secrecy; the further mechanical details of legerdemain, as you will see, do not matter). His audience, savvy magicians, would see that he might possibly be performing a type A trick, a hypothesis they could test by being stubborn and uncooperative spectators in a way that would thwart any attempt at a type A trick. When they then adopted the appropriate recalcitrance to test the hypothesis, Hull would "repeat" the trick, this time executing a type B card presentation trick. The spectators would then huddle and compare notes: Might he be doing a type B trick? They test that hypothesis by adopting the recalcitrance appropriate to preventing a type B trick, and still he does "the" trick—using method C. When they test the hypothesis that he's pulling a type C trick on them, he switches to method D—or perhaps he goes back to method A or B, since his audience has "refuted" the hypothesis that he's using method A or B. And so it would go, for dozens of repetitions, with Hull staying one step ahead of his hypothesis-testers, exploiting his realization that he could always do some trick or other from the pool of tricks they all knew, and concealing the fact that he was doing a grab bag of different tricks by the simple expedient of the definite article: The Tuned Deck. As Hull explained it to Hilliard,

Each time it is performed, the routine is such that one or more ideas in the back of the spectator's head is exploded, and sooner or later he will invariably give up any further attempt to solve the mystery.

Hull's trick was introducing a single common word: "the"—for heaven's sake! This modest monosyllable seduced his audience of experts, paralyzing their minds, preventing them from jootsing. They found themselves stuck in a system in which they were sure that they had to find a big, new trick, so they couldn't see that their problem(s) had not one solution but many; they failed to jump out of the system.
I am suggesting, then, that David Chalmers has—unintentionally—perpetrated the same feat of conceptual sleight of hand in declaring to the world that he has discovered "The Hard Problem." Is there really a Hard Problem? Or is what appears to be the Hard Problem simply the large bag of tricks that constitute what Chalmers calls the Easy Problems of Consciousness? These all have mundane explanations, requiring no revolutions in physics, no emergent novelties. They succumb, with much effort, to the standard methods of cognitive science.
I cannot prove that there is no Hard Problem, and Chalmers cannot prove that there is one. He has one potent intuition going for him, and if it generated some striking new predictions, or promised to explain something otherwise baffling, we might join him in trying to construct a new theory of consciousness around it, but it stands alone, hard to deny but otherwise theoretically inert.
The inventory of known effects of consciousness is large and growing, and they range from the mundane to the exotic. It's hard to keep track of them all, so we must be alert to the possibility that we are being victimized by an error of arithmetic, in effect, when we take ourselves to have added up all the Easy Problems and discovered a residue unaccounted for. That residue may already have been accommodated, without our realizing it, in the set of mundane phenomena for which we already have explanations—or at least un-mysterious paths of explanation still to be explored. How could we commit this "error in arithmetic" and then overlook it? By double counting the phenomena or by forgetting that we had already explained some phenomenon, and hence should have erased it from our list of "Still-to-Be-Explained Phenomena." Is it plausible that we are making such a mistake? Consider this: when we looked at poor Mr. Clapgras, we saw that something was seriously amiss with him, but there seemed to be two importantly different ways of putting his plight:

A. His aesthetic and emotional reactions to his color qualia had all been inverted (while his qualia remained constant).
B. His color qualia had been inverted, even though his competence to distinguish, identify, and name colors had been preserved.

Consider how one might argue (rather like the baffled magicians trying to figure out Hull's trick): "A cannot be right, because the only reason we have for saying his color qualia have remained constant is that his naming and discriminating behavior has stayed constant, but that proves nothing about his qualia; those behaviors are (merely) cognitive, functional facts, and qualia are, of course, independent of those. And B cannot be right, because it is only his reactions that have changed; Clapgras doesn't complain that colors look different to him now, but just that those very same subjective colors don't appeal to him now the way they used to. So maybe his color qualia changed and maybe they didn't, and—you'll note—there is no empirical way of telling which hypothesis is true! This is truly a Hard Problem!"
This argument overlooks the possibility that the qualia discussed in A and B aren't doing any work. In both A and B, we see that the discrimination machinery is working just as before, while Clapgras's reactions to the deliverances of that machinery are inverted. The qualia are interposed as a sort of hard-to-pin-down intermediary that is imagined to provide the basis or raw material or ground of the emotional reactions, and there seem then to be two places where the inversion could happen: before the qualia are "presented" to the appreciation machinery, or after "presentation," in the way the appreciation machinery responds to those presented qualia. This is one presentation process too many. We know, for instance, that negative (alarming, fear-inducing) reactions can be triggered quite early in the perceptual process, and they then "color" all subsequent processing of that perceptual input, in which case we could say that the emotional reactions cause the qualia to have the subjective character they had for Clapgras, rather than (vice versa) that the "intrinsic" nature of the qualia cause or ground the emotional reactions. But if we've already arrived at the emotional (or aesthetic or affective) reactions to the perceptual input, we have no more "work" for the qualia to do, and, of course, a zimbo would be just as bummed out by inverted reactions-to-perceptions as a conscious person is.
What does the story about the Tuned Deck add to all the other intuition pumps about qualia? Just a real-life example of how very clever, knowledgeable experts can be induced to create a phantom problem simply by the way an issue is presented to them. It has happened. It can happen again. And this yields a novel perspective on the impasse, creating a new burden of proof: How do you know that you have not fallen for something like the Tuned Deck? I'm not suggesting that this is conclusive, but just that it ought to give those who credit the Zombic Hunch some second thoughts about how "obvious" it is.











60. The Chinese Room
In the late 1970s there was a wave of ballyhoo about AI—artificial intelligence—that oversold both the existing and the prophesied progress in the field. Thinking machines were just around the corner! Berkeley philosopher John Searle was sure he could see through this, and concocted a thought experiment to prove it. In 1980, he published "Minds, Brains and Programs," his famous Chinese Room thought experiment purporting to show that "Strong AI" was impossible. He defined Strong AI as the claim that "the appropriately programmed computer literally has cognitive states and that the programs thereby explain human cognition" (Searle, 1980, p. 417), and later clarified his definition: "the appropriately programmed digital computer with the right inputs and outputs would thereby have a mind in exactly the sense that human beings have minds" (Searle, 1988, p. 136). The 1980 article appeared in Behavioral and Brain Sciences, the flagship journal of cognitive science, and BBS, as it is known, has a special format: each issue contains several long "target articles" accompanied by several-dozen commentaries by experts in the field and a response by the author(s). Since BBS is energetically interdisciplinary, these experts typically come from a variety of disciplines, giving the reader a useful and timely cross section of reactions. Seeing how—and whether—these other experts take the target article seriously is a great way of gauging how to use or ignore it in your own work. You also can learn a lot about the difficulties of interdisciplinary communication, with very confident people furiously talking past each other, or participating in academic tag team wrestling of the highest caliber. Searle's target article provoked a fire storm of rebuttal, including mine (Dennett, 1980), in which I coined the term "intuition pump," in the service of trying to show what was deeply misleading about his thought experiment.
It was my term, but credit should also be given to Doug Hofstadter, for my coinage grew out of discussions with him about Searle's essay, which we reprinted, with commentary, in our anthology, The Mind's I (Hofstadter and Dennett, 1981). We found his thought experiment fascinating because it was, on the one hand, so clearly a fallacious and misleading argument, yet, on the other hand, just as clearly a tremendous crowd-pleaser and persuader. How—and why—did it work? We looked at it from the point of view of reverse engineering, and Doug came up with the tactic of "turning all the knobs" to see what makes it work. Is the story robust under deformation, or does it critically depend on details that ought to be optional?*
That was over thirty years ago, and I must grant that Searle's Chinese Room has had tremendous appeal and staying power. It is a classic, presented in probably thousands of undergraduate courses and debated over and over to this day. I have used it for years in my own classes, and have learned a lot about how it works and how to show people what's wrong with it.†
For it is, I will now show, a defective intuition pump, a boom crutch that can disable your imagination unless you handle it very carefully. But before I turn to that delicate task, I want to acknowledge that many of you are probably silently rolling your eyes, or even groaning. You don't want me to disable this device; you like the conclusion so much—Strong AI is impossible, whew!—that your eyes glaze over at the prospect of being dragged through a meticulous critique of a vivid, entertaining argument that supports your fervent hope. I used to respond to this reaction with scorn. Those of you responding this way love the fact that an eminent Berkeley professor has a famous argument that purports to show that you are right, and you are happy to take it on his authority. The details don't really interest you, only the conclusion. What an anti-intellectual copout!
But then I caught myself doing much the same thing and reconsidered my harsh verdict. I confess that I have always found quantum mechanics somehow repellent, deeply disorienting, and even, well, something I would prefer not to be true! I know that it is stunningly successful at predicting and explaining many phenomena, including everyday phenomena such as the reflection and refraction of light, and the operation of the proteins in our retinas that permit us to see. It lies at the very heart of science, but it is famously hard to make sense of, even by the experts. My several attempts to master the mathematics of quantum mechanics have failed, so I am an interested but ultimately incompetent bystander on the scientific controversies surrounding its interpretation, but this hasn't prevented me from being deeply suspicious of much that has been said about this by supposedly knowledgeable experts. And then I read Murray Gell-Man's book The Quark and the Jaguar: Adventures in the Simple and the Complex (1995, a science book for nonspecialists like me. To my delight, Gell-Mann adopts a no-nonsense, demystifying tone, and just beats the daylights out of some of the more dubious pronouncements that have gained favor. (Read his chapter "Quantum Mechanics and Flapdoodle" to see what I mean.) I found myself thinking, "Hit them again, Murray! Sock it to them!" Here was a world-famous Nobel laureate physicist supporting my prejudice, using arguments that I understood. This was my kind of quantum physics! But then it hit me. Did I really understand his arguments, or just sorta understand them? Could I be sure that I wasn't just falling for his rhetoric? I hoped there weren't other physicists who would want to drag me back through the technicalities, showing me that I had been taken in by this authoritative dismissal. I liked his conclusion so much I didn't have any stomach for the details. Same copout.
But not quite. I have conscientiously tried to assess Gell-Mann's verdicts in the light of what others have written since (and so far, so good). And I remain open to the distinct possibility that Gell-Mann's "common sense," which I find so appealing, will turn out some day to be one more case of failure of imagination instead of deep insight. I would beg you to attempt to adopt the same open-minded attitude toward my effort to dismantle and neutralize Searle's Chinese Room. I'll make the bitter pill as palatable as I can.
Way back in 1950 Alan Turing had proposed what he claimed would be the acid test of intelligence in a machine, now known as the Turing test, in which a judge has a probing conversation with two entities, one a human being and the other a computer, both hidden from view and communicating by "teletype" (think: screen and keyboard). The human being tries to demonstrate her genuine humanity to the judge, and the computer tries to fool the judge into thinking that it is the human being. If the judge can't reliably tell the difference, the computer (program) has passed the Turing test and would be declared not just to be intelligent, but to "have a mind in exactly the sense that human beings have minds," as Searle put it in 1988. Passing the Turing test would be, in the eyes of many in the field, the vindication of Strong AI. Why? Because, they thought (along with Turing), you can't have such a conversation without understanding it, so the success of the computer conversationalist would be proof of its understanding.* Notice that the Turing test doesn't—can't—distinguish between a zimbo and a "really conscious" person, since whatever a conscious person can do, a zimbo can do exactly as well. As many have remarked, the claim that an entity that passes the Turing test is not just intelligent but conscious flies in the face of the Zombic Hunch. This may seem all by itself to disqualify the Turing test as a good test of mind, but we should reserve judgment on that until we look at the details. The program that passes the Turing test may have only a zimbo's stream of unconsciousness where we have a stream of consciousness, but we have just seen a challenge to the Zombic Hunch that insists that this marks a real difference. Is the zimbo program only sorta conscious? In what dimension, exactly, does it fall short? Perhaps Searle's intuition pump will shed light on that question: it appears to be a reductio ad absurdum argument to discredit the idea of Strong AI.
He invites us to imagine him locked in a room, hand-simulating a giant AI program, which putatively understands Chinese. He stipulates that the program passes the Turing test, foiling all attempts by human interlocutors to distinguish it from a genuine understander of Chinese. Searle, who knows no Chinese, locked in the room and busily manipulating the symbol strings according to the program, doesn't thereby gain understanding of Chinese (obviously), and there is nothing else in the room that understands Chinese either. (It is empty, aside from Searle and the "bits of paper" on which Searle's instructions are written. By following them precisely he is "hand-simulating" the giant program.) If Searle doesn't understand Chinese, surely (ding!) Searle plus bits of paper doesn't understand Chinese. Ergo, there is no understanding of Chinese in the Chinese Room, in spite of its conversational prowess, which is good enough to fool native Chinese-speakers. Searle, like a computer, identifies the Chinese symbols by their shape only; they are just different meaningless "squiggles" to him and to a computer, so Searle-in-the-Chinese-Room just is an implementation of the computer program at issue. It does its job without any understanding of Chinese, whether it is running on silicon or on Searle.
This is so simple and convincing! Can there possibly be anything wrong with this thought experiment? Well, yes. When Searle presented it at Berkeley, the computer scientists retorted with what Searle calls the Systems Reply (Berkeley):
While it is true that the individual person who is locked in the room does not understand the story, the fact is that he is merely part of the whole system, and the system does understand the story. The person has a large ledger in front of him in which are written the rules, he has a lot of scratch paper and pencils for doing calculations, he has "data banks" of sets of Chinese symbols. Now, understanding is not being ascribed to the mere individual; rather it is being ascribed to this whole system of which he is a part. [Searle, 1980, p. 419]

Searle says something very telling in the course of responding to this reply:
Actually I feel somewhat embarrassed ... because the theory seems to me so implausible to start with. The idea is that while a person doesn't understand Chinese, somehow the conjunction of that person and bits of paper might understand Chinese. It is not easy for me to imagine how someone who was not in the grip of an ideology would find the idea at all plausible. [Searle, 1980, p. 419]

What Searle finds so implausible is nothing other than the fundamental insight Turing had when he created the idea of a stored-program computer! The competence is all in the software. Recall that the register machine in chapter 24 doesn't understand arithmetic at all, but the register machine in conjunction with the software does perfect arithmetic. The central processing unit in your laptop doesn't know anything about chess, but when it is running a chess program, it can beat you at chess, and so forth, for all the magnificent competences of your laptop. What Searle describes as an ideology is at the very heart of computer science, and its soundness is demonstrated in every walk of life. The way to reproduce human competence and hence comprehension (eventually) is to stack virtual machines on top of virtual machines on top of virtual machines—the power is in the system, not in the underlying hardware. Darwin's "strange inversion of reasoning" is nicely echoed by Turing's strange inversion of reasoning (Dennett, forthcoming): whereas we used to think (before Turing) that human competence had to flow from comprehension (that mysterious fount of intelligence), we now appreciate that comprehension itself is an effect created (bubbling up) from a host of competences piled on competences.
Details matter. Searle never tells his reader at what level he is hand-simulating the giant AI program. This is his account of what it is like when he works through the program:
Now suppose that after this first batch of Chinese writing [the input] I am given a second batch of Chinese script together with a set of rules for correlating the second batch with the first batch. The rules are in English, and I understand these rules as well as any other native speaker of English. They enable me to correlate one set of formal symbols with another set of formal symbols. ... Now suppose also that I am given a third batch of Chinese symbols together with some instructions, again in English, that enable me to correlate elements of this batch with the first two batches, and these rules instruct me how to give back certain Chinese symbols with certain sorts of shapes in response to certain sorts of shapes given me in the third batch. [Searle, 1980, p. 418]

He contrasts this "correlation" between "batches of symbols" with what happens when instead an English sentence or story is input, and he responds in his native English.
From the external point of view—from the point of view of somebody reading my "answers"—the answers to the Chinese questions and the English questions are equally good. But in the Chinese case, unlike the English case, I produce the answers by manipulating uninterpreted formal symbols. [Searle, 1980, p. 418]

What a contrast! But look what Searle has still left out. We know he gets his "set of rules" (instructions) in English, but are they along the lines of "add the contents of register 39021192 to the contents of register 215845085" (machine code), or "Define a constant: queue-size and set it at 100" (source code)? Is he down in the basement, doing arithmetic at a furious rate (trillions of operations per second), or is he following the source code of a program, implemented many layers higher? And if he's following the source code, does he get to read any comments? Better not, since they are not officially part of the program, and they would give him lots of hints about what was going on ("This parses the sentence, yielding nouns, pronouns, verbs, and modifiers, and identifies it as a question, declarative, imperative or interjection," and then, a few billion operations later, "pun discovered; switch to repartee mode ...," and then a more detailed set of billions of operations, in which references are refined, alternative answers are evaluated rather like different chess moves, and then finally an output sentence is generated). If the program Searle is hand-simulating is able to carry on an impressive conversation in Chinese, it will have to consult huge data banks not just of "sets of Chinese symbols," as he puts it, but of everyday knowledge that Chinese speakers share, and that is the least of it. When Searle hand-simulates, does he get any hints of all this layered cognitive activity, or is it just a whole lot of arithmetic to him?
Think of how Searle would handle the following question in English:
Imagine taking a capital letter D and turning it counterclockwise 90 degrees on its side. Now place it on top of a capital letter J. What sort of weather does that remind you of?

Now imagine that Searle is given an analogous challenge in Chinese when he is hard at work in the Chinese Room.

On June 4, 2012, the following posting of characters was blocked on Sohu Weibo [a Chinese blogging service]. Can you figure out why?


These are actual Chinese characters (Searle's "squiggles"), but the sequence is utter gibberish. Why would it be blocked by the authorities? Because June 4 is the anniversary of the Tiananmen Square massacre in which hundreds of protesters were killed by the army. ("June 4" is as evocative to the Chinese as "9/11" is to Americans.) The most famous images that emerged were of a single brave man who faced down the tanks. You can see him ("" means person) confronting four tanks on the left, which then roll over him and over him and over him and then leave at the right.*
The Chinese Room ought to "get it," but unless Searle had access to the comments on the source code, he would be none the wiser, since it would never be apparent to him that his rule-following was framing a "mental image" and manipulating it and then using the result as a probe of memory. That is, the system would go through a set of activities before responding to the Chinese question that are strikingly parallel to the activities Searle went through knowingly to respond to the English question. You could say that the system has a mind of its own, unimagined by Searle, toiling away in the engine room.
Any program capable of passing the Turing test would have to go through "mental" operations that very closely mimicked the mental operations we go through in the course of a conversation. Suppose, for instance, the questioner in the Turing test began tutoring the candidate in quantum physics, using the Socratic question-and-answer method, giving the student simple problems to solve. Searle, in the engine room, would be obliged to take the system through elaborate intellectual exercises in order to hold up its end of the conversation, but Searle would emerge from the ordeal as uncomprehending of quantum physics as when he went in. The system, in contrast, would now have a much better working understanding of the field than it had before the Turing test began, because it had done the exercises. This particular instance of the Turing test would install a new virtual machine in the program: the simple quantum physics machine.
Such facts are completely obscured by Searle's image of "bits of paper" and "rules" that permit him to "correlate" Chinese symbols. I'm not saying that Searle deliberately concealed the complexity of the program he imagined he was hand-simulating, but just that he ignored the implications of the complexity. If you think of the program as a relatively simple bunch of rules, then, like Bateson thinking of "particles of chromatin, indistinguishable from each other and indeed almost homogeneous under any known test," you are likely to find the purported comprehension powers of programs, like the powers of DNA, "inconceivable."
Look at what we've just done. We've turned the knob on Searle's intuition pump that controls the level of description of the program being followed. There are always many levels. At the highest level, the comprehending powers of the system are not unimaginable; we even get insight into just how the system comes to understand what it does. The system's reply no longer looks embarrassing; it looks obviously correct. That doesn't mean that AI of the sort Searle was criticizing actually achieves a level of competence worth calling understanding, nor that those methods, extended in the ways then imagined by those AI researchers, would likely have led to such high competences, but just that Searle's thought experiment doesn't succeed in what it claims to accomplish: demonstrating the flat-out impossibility of Strong AI.
There are other knobs to turn, but that task has been carried out extensively in the huge literature the Chinese Room has provoked. Here I am concentrating on the thinking tool itself, not the theories and propositions it was aimed at, and showing that it is a defective tool: it persuades by clouding our imagination, not exploiting it well.











61. The Teleclone Fall from Mars to Earth
You see the moon rise in the east. You see the moon rise in the west. You watch two moons moving toward each other across the cold black sky, one soon to pass behind the other as they continue on their way. You are on Mars, millions of miles from home, protected from the killing, frostless cold of the red Martian desert by fragile membranes of terrestrial technology—protected but stranded, for your spaceship has broken down beyond repair. You will never ever return to Earth, to the friends and family and places you left behind.
But perhaps there is hope. In the communication compartment of the disabled craft, you find a Teleclone Mark IV teleporter and instructions for its use. If you turn the teleporter on, tune its beam to the Teleclone receiver on Earth, and then step into the sending chamber, the teleporter will swiftly and painlessly dismantle your body, producing a molecule-by-molecule blueprint to be beamed to Earth, where the receiver, its reservoirs well stocked with the requisite atoms, will almost instantaneously produce—from the beamed instructions—you! Whisked back to Earth at the speed of light, into the arms of your loved ones, who will soon be listening with rapt attention to your tales of adventures on Mars.
One last survey of the damaged spaceship convinces you that the Teleclone is your only hope. With nothing to lose, you set the transmitter up, flip the right switches, and step into the chamber. Five, four, three, two, one, FLASH! You open the door in front of you and step out of the Teleclone receiver chamber into the sunny, familiar atmosphere of Earth. You've come home, none the worse for wear after your long-distance Teleclone fall from Mars. Your narrow escape from a terrible fate on the red planet calls for a celebration, and as your family and friends gather around, you notice how everyone has changed since last you saw them. It has been almost three years, after all, and you've all grown older. Look at Sarah, your daughter, who must now be eight and a half. You find yourself thinking, "Can this be the little girl who used to sit on my lap?" Of course it is, you reflect, even though you must admit that you do not so much recognize her as extrapolate from memory and deduce her identity. She is so much taller, looks so much older, and knows so much more. In fact, most of the cells now in her body were not there when last you cast eyes on her. But in spite of growth and change, in spite of replacement of cells, she's the same little person you kissed good-bye three years ago.
Then it hits you: "Am I, really, the same person who kissed this little girl good-bye three years ago? Am I this eight-year-old child's mother or am I actually a brand new human being, only several hours old, in spite of my memories—or apparent memories—of days and years before that?" Did this child's mother recently die on Mars, dismantled and destroyed in the chamber of a Teleclone Mark IV?
Did I die on Mars? No, certainly I did not die on Mars, since I am alive on Earth. Perhaps, though, someone died on Mars—Sarah's mother. Then I am not Sarah's mother. But I must be! The whole point of getting into the Teleclone was to return home to my family. But I keep forgetting; maybe I never got into that Teleclone on Mars. Maybe that was someone else—if it ever happened at all.
Is that infernal machine a teleporter—a mode of transportation—or, as the brand name suggests, a sort of murdering twinmaker? Did Sarah's mother survive the experience with the Teleclone or not? She thought she was going to. She entered the chamber with hope and anticipation, not suicidal resignation. Her act was altruistic, to be sure—she was taking steps to provide Sarah with a loved one to protect her—but also selfish—she was getting herself out of a jam and into something pleasant. Or so it seemed. "How do I know that's how it seemed? Because I was there; I was Sarah's mother thinking those thoughts. I am Sarah's mother. Or so it seems."
A song or a poem or a movie can undoubtedly be teleported. Is a self the sort of thing—a thing "made of information"—that can be teleported without loss? Is our reluctance to admit the teleportation of people a bit like the anachronistic resistance, recently overcome in most quarters, to electronically scanned legal signatures on documents? (I learned in 2011 that Harvard University's Society of Fellows would not accept a scanned signature on my letter of recommendation; they required some dry ink that had actually been laid down by the motion of my actual hand, and it took me half a day of riding around in taxis in Beirut to get, sign, and express-mail back the relevant form—on cream-colored bond paper. It is my understanding that the Society has now changed its policy, but I hope Harvard still insists on putting wax seals on their diplomas. There is a place for tradition, in all its glorious gratuitousness.)











62. The Self as the Center of Narrative Gravity
What is a self? Philosophers have been grappling with this question for centuries. The Christian concept of an immortal soul, immaterial and inexplicable, captivated thinkers and deflected serious investigation for centuries, but it is losing adherents daily. The idea of a mind-thingy that goes to Heaven when somebody dies grows more incoherent every day. The wishful thinking that prevents us from just discarding it, along with the goblins and witches, is only too apparent. So those of us who are materialists, confident that the mind is the brain (properly understood), have to confront the question of why it seems that each of us has some such mind-thingy, or better: that each of us is some such mind-thingy inhabiting a body, and more particularly a brain. Do we simply find our selves when we look inside?
David Hume famously disparaged this idea in 1739:

For my part, when I enter most intimately into what I call myself, I always stumble on some particular perception or other, of heat or cold, light or shade, love or hatred, pain or pleasure. I never can catch myself at any time without a perception. ... If anyone, upon serious and unprejudiced reflection, thinks he has a different notion of himself, I must confess I can reason no longer with him. All I can allow him is, that he may be in the right as well as I, and that we are essentially different in this particular. He may, perhaps, perceive something simple and continued, which he calls himself; though I am certain there is no such principle in me. [1964, I, iv, sect. 6]

Hume's tongue-in-cheek acknowledgment that others may be different echoes to this day among those who muse about whether I, for instance, am a zombie (a zimbo, of course) innocently extrapolating from my own impoverished experience to how it is with others. An amusing conjecture, but I don't think anybody takes it seriously.
It is clear what a self isn't. It isn't a part of the brain, like the amygdala or hippocampus. The frontal lobes play a crucial role in evaluating situations, intentions, perceptions, and the like, but I don't think anybody has made the mistake of locating the self in the frontal lobes. (A prefrontal lobotomy is a dire surgery, leaving behind someone who really is "a shadow of his former self," but it isn't a self-ectomy. As the old joke has it, I'd rather have a free bottle in front of me than a pre-frontal lobotomy, but in either case I'd be there to experience it.) Then what might the self be? I propose that it is the same kind of thing as a center of gravity, an abstraction that is, in spite of its abstractness, tightly coupled to the physical world. You, like every other material object, have a center of gravity (or more properly a center of mass, but we'll ignore that nicety here). If you are top-heavy, your center of gravity is higher than average for people of your height, you have to work harder to stay upright, and so forth. There are many ways of locating your center of gravity, which, depending on such factors as the shoes you have on and when you last ate a meal, moves around in a smallish area in the middle of your body. It is a mathematical point, not an atom or molecule. The center of gravity of a length of steel pipe is not made of steel and indeed is not made of anything. It is a point in space, the point on the midline running through the center of the pipe that is equidistant from the ends (roughly, depending on imperfections, etc.).
The concept of a center of gravity is a very useful thinking tool in its own right. In effect it averages over all the gravitational attractions between every particle of matter in a thing and every particle of matter on the planet, and tells us that we can boil all that down to two points—the center of the earth (its center of gravity) and the center of gravity of the thing—and calculate the behavior of the thing under varying conditions. For instance, if a thing's center of gravity at any time falls outside all the points of its supporting base, it will topple. Of course, we had an intuitive understanding of centers of gravity long before Newton figured out gravity. ("Sit down! You're rocking the boat.") Now we can explain how and why the concept works in detail, and if we're designing vehicles or floor lamps, for example, the goal of lowering a center of gravity, or moving it to a more effective location, shows the near indispensability of the concept in many of our activities. It may be a "theorist's fiction," but it is a very valuable fiction from which a lot of true predictions can be generated. Can such an abstract entity, having no material existence, actually cause anything? Not directly, but explanations that cite a center of gravity compete with explanations that are clearly causal. Why didn't that coffee mug tip over when the sailboat heeled so drastically? "Because it has an unusually low center of gravity" competes with "Because it is glued to the deck."
We may call a center of gravity a theorist's fiction because it shares with fictional characters the curious property of indeterminacy of properties. Sherlock Holmes, as described by Arthur Conan Doyle in the Sherlock Holmes mystery stories, has many properties, but where Conan Doyle was silent, there is no fact of the matter. We can extrapolate a bit: The author never mentions Sherlock having a third nostril, so we are entitled to assume that he didn't (Lewis, 1978). We can also agree that he was not a bigamist with one wife in Paris and another in New York. But for many such questions there is no answer: Did he have a mole on his left shoulder blade? Was he a first cousin of Oscar Wilde? Did he own a cottage in Scotland? These questions and kazillions more about any real human being must have true answers, even if we can never discover them. Not so for Sherlock; being fictional he has only properties his author says or implies that he has. A naïve reader who thinks the Sherlock Holmes stories are true might wonder whether the conductor on the train to Aldershot was taller or shorter than Holmes, but if you understand what fiction is, you know better than to ask. The same is true of centers of gravity. If you wondered whether they might eventually "turn out to be neutrinos," you would be missing the point of them as theorist's fictions.
What then is a center of narrative gravity? It is also a theorist's fiction, posited in order to unify and make sense of an otherwise bafflingly complex collection of actions, utterances, fidgets, complaints, promises, and so forth, that make up a person. It is the organizer of the personal level of explanation. Your hand didn't sign the contract; you did. Your mouth didn't tell the lie; you did. Your brain doesn't remember Paris; you do. You are the "owner of record" of the living body we recognize as you. (As we say, it's your body to do with what you like.) In the same way that we can simplify all the gravitational attractions between all the parts of the world and an obelisk standing on the ground by boiling it down to two points, the center of the earth and the center of gravity of the obelisk, we can simplify all the interactions—the handshakes, the spoken words, the ink scrawls, and much more—between two selves, the seller and the buyer, who have just completed a transaction. Each self is a person, with a biography, a "backstory," and many ongoing projects. Unlike centers of gravity, selves don't just have trajectories through space and time; they gather as they go, accumulating memories and devising plans and expectations.
There are parts of the backstory that each person would probably wish to disavow, but what's done is done; that part of the narrative cannot be revised. It can be reinterpreted, however, in the light of later biographical elements. "I wasn't myself when I did that," is a familiar refrain, and our tolerance for this apparently self-contradictory claim is often wise. What the person means is that he was in an extreme "out of character" state when he did that, and he is imploring us not to judge him in the present or extrapolate from that behavior into the future. Sometimes this is very plausible, sometimes not. Another familiar move: "Well, if you didn't do that, who did?" "The devil made me do it." Again, we often accept this claim, not on its face value, but as a sincere disavowal of the character and motivations that guided the action. This raises issues about responsibility and free will that we will tackle in the next chapter; just note, for now, how we need the concept of the self to draw a line (however arbitrary it may often turn out to be) between what you do and what happens to you.
Every physical object has a center of gravity and every living human body has a self, or rather, every living human body is owned by a self, a sort of live-in manager. Only one owner? Could a body be shared by two or more selves? The condition known as dissociative identity disorder (it used to be called multiple personality disorder) is apparently an example of many selves sharing a single body, with a dominant self (the "host") and a group of "alters." I say "apparently" because controversy rages about the diagnosis, running from outright fraud through folie à deux (in which a naïve psychiatrist unintentionally encourages a troubled patient to develop the symptoms) to acceptance of a few rare and genuine cases surrounded by various flavors of wannabes. After studying the phenomenon (and the people studying and treating the phenomenon) for several years, psychologist Nicholas Humphrey and I (1989) decided that everyone is right! There are frauds, exaggerations, gullible therapists with eager patients, and yes, a few cases in which the condition seems to have existed in at least rudimentary form prior to any elaboration at the hands of a fascinated interlocutor. This is not surprising, when we recognize that it is an intensification of something quite normal that we all experience to various degrees. Most of us lead several fairly distinct lives, at work, at home, at play, and acquire habits and memories in each context that turn out not to travel well to other contexts.
As sociologist Erving Goffman (1959) recounted in his classic book on the subject, The Presentation of Self in Everyday Life, we all engage in presenting ourselves as characters in real-life dramas (Professor Dennett, Dan from up the road, Dad, Grandpa, etc.), and we effortlessly enlist the help of the supporting cast, who are similarly presenting themselves. We feed each other lines we can handle readily, becoming complicit in each other's campaign of self-presentation, or we can disrupt these smoothly running scenarios by playing out of character, with results that are awkward, comic, or worse. It takes nerves of steel to do this. Can you imagine asking a person newly introduced to you at a party to show you some identification, a driver's license, perhaps, or a passport—or, heading in the other direction, attempting to give her a passionate embrace? When people are put in extremely difficult positions, they sometimes adopt extreme measures, and what starts out as feigning in desperation becomes almost second nature. When the going gets tough, you simply depart, leaving behind a different center of narrative gravity, a different character, better equipped to deal with the problem at hand.
Humphrey and I learned that when you interview a putative sufferer of dissociative identity disorder, expect to find a naïve virtuoso in the art of deflecting obtrusive inquiry. To raise the questions that cry out for answers* or—better—to set little traps to see whether one alter really doesn't have any memories of what the other alter was doing and saying, you will have to be downright rude, and risk giving serious offense, so in all likelihood you will find yourself politely going along with the gag, an accomplice enlisted without so much as a wink. Con artists do this deliberately, with great skill, but so do innocent victims of this personality disorder, without any recognition of what they are doing. So do we all, to one degree or another. But their alters are just fictional characters, right? The real person is the host, right? Well, it isn't quite that straightforward.
That thing that is you, the person you play, whether you play multiple roles or really just one monolithic role, is your center of narrative gravity. It is how your friends recognize you ("You're not yourself today!"), and how you see yourself for the most part, but it is also somewhat idealized ("Oh my God! Did I do that? I would never do that!"). Professional novelists, like con artists, create narratives with cunning and deliberate attention to the details. The rest of us are talented amateurs, spinning our tales cleverly but (in the main) unwittingly, rather the way a spider spins a web. It is nature, not art. It is not so much that we, using our brains, spin our yarns, as that our brains, using yarns, spin us. There is a core of undeniable true biography, to be sure, but over the years large parts of it become as good as gone, inert and of no relevance to who you are now. Some of it you may actively disavow, jettison, "forget," in the process of self-maintenance and self-improvement.*
Consider how easily you can answer some questions about your past. Have you ever danced with a movie star? Have you ever been to Paris? Have you ever ridden a camel? Have you ever strangled somebody to death with your bare hands? These are easy for almost all of us, whether the answer is yes or no. (Imagine someone who was asked the last question and who paused, thoughtfully, scratching his chin, before answering. Give him a wide berth!) We know the answers to these questions because we know enough about ourselves—our selves—to know that had we ever danced with a movie star, been to Paris, ridden a camel, or strangled someone, we'd now be recollecting it. When "nothing comes to mind," we interpret this absence as a negative. (How else could we be so sure? Do you keep a list of all the things you've never done? All the places you've never been?) Contrast those questions with these superficially similar questions: Have you ever danced with a person named Smith? Have you ever been to a drug store that sold floor polish? Have you ever ridden in a blue Chevrolet? Have you ever broken a white coffee mug? Some may be easy to answer, and others may lead you to say you have no idea, and some may inspire you to give false answers without realizing it, just because they are so unimportant. Why would you remember it if you had done any of these things? A lot that has happened to us is just not memorable; a lot of things that we have done, good and bad, have been shed by our centers of narrative gravity; and a lot that never happened has been added, innocently enough, because for one reason or another it just fits us to a t. What you are is that rolling sum of experience and talent, solemn intention and daydreaming fantasy, bound together in one brain and body and called by a given name. The idea that there is, in addition, a special indissoluble nugget of you, or ego, or spirit, or soul, is an attractive fantasy, but nothing that we need in order to make sense of people, their dreams and hopes, their heroism and their sins.
This center of narrative gravity may not be a mysterious nugget of mind stuff, but if it is just an abstraction, can it be studied scientifically? Yes, it can.











63. Heterophenomenology
Heterophenomenology is not an intuition pump, but another example of staging that is well worth putting in place before we tackle some difficult questions. The study of human consciousness involves phenomena that at first glance seem to occur in something rather like another dimension: the private, subjective, "first-person" dimension that each of us occupies with regard to our own consciousness, and to which nobody else can gain direct access. What, then, is the relation between the standard "third-person" objective methodologies for studying meteors or magnets (or human metabolism or bone density), and the methodologies for studying human consciousness? Do we have to create some radical or revolutionary alternative science, or can the standard methods be extended in such a way as to do justice to the phenomena of human consciousness? I defend the claim that there is a straightforward, conservative extension of objective science that handsomely covers all the ground of human consciousness, doing justice to all the data without ever having to abandon the rules and constraints of the experimental methods that have worked so well in the rest of science. This third-person methodology, heterophenomenology (phenomenology of an other not oneself), is the sound way to take the first-person point of view as seriously as it can legitimately be taken.
Why the multisyllabic name? "Phenomenology" originally meant a catalogue of phenomena, of one sort or another, before there is a good theory of them. In the sixteenth century, William Gilbert compiled a good phenomenology of magnetism, but it was centuries before all the magnetic phenomena he carefully described could be explained. In the early twentieth century, Edmund Husserl, and a group of psychologists and philosophers influenced by him, adopted the term, "Phenomenology" (with a capital "P"), for a presumably scientific study of the phenomena of subjective experience, to be observed using a "first-person" introspective method that attempted to be theory-neutral and without presuppositions. The school of thought continues to this day, embattled or ignored for the most part, for reasons good and bad. In spite of some very tantalizing results, well worth further exploration, as a first-person approach it has been shunned by objective, empirical science, which insists on data that can be accessible to all investigators. But we can study consciousness objectively, and the method is really just a simple twist on Phenomenology, so I call it heterophenomenology, to contrast it with Husserlian autophenomenology. Heterophenomenology is the study of first-person phenomena from the third-person point of view of objective science.
Obviously the key difference between experiments with rocks, roses, and rats on the one hand, and experiments with awake, cooperative human subjects on the other, is that the latter can communicate in language and hence can collaborate with experimenters, by making suggestions, interacting verbally, and telling them what it is like under various controlled conditions. That is the core of heterophenomenology: it exploits our capacity to perform and interpret speech acts, yielding a catalogue of what the subject believes to be true about his or her conscious experience. This catalogue of beliefs fleshes out the subject's heterophenomenological world, the world according to S, the subjective world of one subject. The total set of details of heterophenomenology, plus all the data we can gather about concurrent events in the brains of subjects and in the surrounding environment, comprise the total data set a theory of human consciousness must explain. It leaves out no objective phenomena and no subjective phenomena of consciousness.
The interpretation required to turn raw data about speech sounds and button pressings into reports and expressions of beliefs involves adopting the intentional stance: it requires the working hypothesis that the subject is an agent whose actions are rationally guided by beliefs and desires that are themselves rational, given the subject's perceptual history and needs. For instance, the constraints of the intentional stance can be clearly discerned in the standard precautions taken in such experiments to prevent subjects from having experiences that might give them either beliefs or desires that would tend to bias their responses in ways that would distort our interpretation of their actions: we keep them in the dark about what we hope they will say, for instance, while at the same time taking steps to assure ourselves that they understand the tasks we set them. This adoption of the intentional stance is not an irreparably subjective and relativistic affair. Rules of interpretation can be articulated; standards of intersubjective agreement on interpretation can be set and met; deviations can be identified; the unavoidable assumption of rationality can be cautiously couched and treated as an adjustable, defensible, and evolutionarily explicable assumption. (The details of these processes are articulated in Dennett, 1991a.)
This is not a proposal for a new methodology for studying consciousness. I am just being self-conscious about the standard methods already adopted by researchers in cognitive psychology, psychophysics (which studies the relationships between physical stimuli and subjective reactions), and neuroscience, and explaining and defending them. These methods, correctly understood and followed, obviate the need for any radical or revolutionary "first-person" science of consciousness, and leave no residual phenomena of consciousness inaccessible to controlled scientific study.
What kinds of things does this methodology commit us to? Beyond the unproblematic things all of science is committed to (neurons and electrons, clocks and microscopes), it commits us just to beliefs—the beliefs expressed by subjects and deemed constitutive of their subjectivity—and to desires—the desires to cooperate with the experimenters, and to tell them the truth as candidly as possible. (An important part of the method involves controlling these beliefs and desires, and any experimental result that shows evidence of failure to do this must be discarded.) What kind of things are beliefs and desires? We may stay maximally noncommittal about this—pending the confirmation of theory—by treating beliefs and their contents or objects as theorists' fictions or abstractions similar to centers of mass, the equator, and parallelograms of forces.
Mermaid-sightings are real events, however misdescribed, whereas mermaids don't exist. Similarly, a catalogue of beliefs about experience is not the same as a catalogue of experiences themselves. Philosopher Joseph Levine (1994, p. 117) has objected that "conscious experiences themselves, not merely our verbal judgments about them, are the primary data to which a theory must answer." This can't be right. How, in advance of theory, could we catalogue the experiences themselves? Consider the evidence we get from putting subjects into experimental situations and querying them (and asking them to perform whatever other actions we want them to perform). These sources are naturally nested by the cascade of interpretations we have to perform, ranked here from least raw to most raw:

(a) "conscious experiences themselves"
(b) beliefs about these conscious experiences
(c) the "verbal judgments" Levine mentions
(d) the utterances of one sort or another that (can be interpreted to) express those verbal judgments

In one sense of "primary," the utterances are primary data—recorded sounds and motions. Electroencephalographic (EEG) readings and functional magnetic resonance imaging (fMRI) readings and the like can be added to the primary data, as circumstances permit. Reliable methods of interpretation can take us to (c) and (b), so we have a catalogue of subjects' beliefs about what it is like to be them under these conditions. But should we push on to (a) in advance of theory? This is not a good idea, for two reasons.

First, if (a) outruns (b)—if you have conscious experiences you don't believe you have, then those extra conscious experiences are just as inaccessible to you as to the external observers.

So Levine's proposed alternative garners no more usable data than heterophenomenology does.
Second, if (b) outruns (a)—if you believe you have conscious experiences that you don't in fact have, then it is your beliefs that we need to explain, not the nonexistent experiences.

Sticking to the heterophenomenological standard, then, and treating (b) as the maximal set of primary data, is the way to avoid any commitment to spurious data, while ensuring that all phenomena accessible to anybody get included.
What if some beliefs are inexpressible in verbal judgments? There is nothing to prevent heterophenomenologists and subjects from collaborating on devising analogue or other nonlinguistic modes of belief expression. For instance,
Draw a vertical line across the line segment indicating how intense [in one dimension or another] the experience is:
Barely noticeable Overpowering

Or subjects can press a button with variable pressure to indicate severity of pain (or anxiety or boredom or even distrust in the experiment). Then there are a host of physiological dependent variables to measure, from galvanic skin response and heart rate to changes in facial expression and posture. And if you, the subject, believe that there are still ineffable residues unconveyed after exhausting such methods, you can tell this to the heterophenomenologists, who can add that belief to the list of beliefs in your primary data:
S claims that he has ineffable beliefs about X.

If this belief is true, then science has the obligation to explain what such beliefs are and why they are ineffable. If this belief is false, science still has to explain why S believes (falsely) that there are these particular ineffable beliefs.*











64. Mary the Color Scientist: A Boom Crutch Unveiled
Australian philosopher Frank Jackson's thought experiment about Mary the color scientist, often called "the Knowledge Argument," has been pumping philosophers' intuitions with remarkable vigor since it first appeared in 1982. For sheer volume and reliability, this must count as one of the most successful intuition pumps ever devised by an analytical philosopher. It is a classic, perennially on the list of required reading in undergraduate courses in the philosophy of mind all over the English-speaking world, and several weighty anthologies have been devoted to essays reflecting on its implications. It is interesting to note that its author has subsequently recanted, declaring that he no longer accepts its conclusion—but that has not diminished its popularity.
Here it is in its entirety, drawn slightly out of context perhaps, but still clear as can be:

Mary is a brilliant scientist who is, for whatever reason, forced to investigate the world from a black and white room via a black and white television monitor. She specializes in the neurophysiology of vision and acquires, let us suppose, all the physical information there is to obtain about what goes on when we see ripe tomatoes, or the sky, and use terms like "red", "blue", and so on. She discovers, for example, just which wavelength combinations from the sky stimulate the retina, and exactly how this produces via the central nervous system the contraction of the vocal chords and expulsion of air from the lungs that results in the uttering of the sentence "The sky is blue". (It can hardly be denied that it is in principle possible to obtain all this physical information from black and white television, otherwise the Open University would of necessity need to use color television.) What will happen when Mary is released from her black and white room or is given a color television monitor? Will she learn anything or not? It seems just obvious that she will learn something about the world and our visual experience of it. But then it is inescapable that her previous knowledge was incomplete. But she had all the physical information. Ergo there is more to have than that, and Physicalism [i.e., materialism, the denial of dualism] is false. [Jackson, 1982, p. 130]

Is it a good intuition pump? Let's turn all the knobs and see what makes it work. Actually, that would take us too long, but the job has been done, in the sizable literature on the topic. Here I will simply illustrate a few knobs that need to be examined, and leave the result as an exercise for you. (If you want, you can check your own results against the literature; two recent anthologies are referenced in Sources. Can you come up with a new twist?) More than twenty years ago, I conducted a preliminary exploration of the knobs, and issued a killjoy verdict that has been largely dismissed or disregarded: "Like a good thought experiment, its point is immediately evident even to the uninitiated. In fact it is a bad thought experiment, an intuition pump that actually encourages us to misunderstand its premises!" (Dennett, 1991a, p. 398). Let's see if this is so. I claim that it is much more difficult to imagine the scenario correctly than people suppose, so they imagine something easier, and draw their conclusions from that mistaken base.

First knob: "a black and white room via a black and white television monitor."

Presumably she wears black or white gloves and is forbidden to look at herself when she bathes, but the idea of cutting off all "external sources" of color is forlorn in any case. Would we have to rig some device that prevented her from rubbing her eyes (to create "phosphenes"—try it)? And couldn't she have colors in her dreams before actually seeing them? If not, why not? Do the colors have to "get in via the eyes" before she can "store" them in her brain? A tangle of bad folk theory of color lies behind this simple suggestion.

Second knob: she "acquires, let us suppose, all the physical information there is to obtain about what goes on when we see ripe tomatoes, or the sky, and use terms like 'red', 'blue', and so on."

All the physical information there is to obtain? How much is that? Is that like having all the money in the world? What would that be like? It's not easy to imagine, and nothing less than all will serve to make the thought experiment's intended point. It must include all the information about all the variation in responses in all the brains, including her own, especially including all the emotional or affective reactions to all the colors under all conditions. So she will know in exquisite detail which colors calm her, annoy her, would grow on her with exposure, distract her, repel her, and so on. Is she forbidden to perform experiments on herself (without cheating, without smuggling any colored things into her cell)? If you didn't imagine all this (and more), you didn't follow directions. It's like being asked to conceive of a chiliagon and imagining a circle instead. Lots of implications follow from one of these exercises of mental representation that don't follow from the other. In this case, for instance, are we supposed to ignore the fact that if Mary acquired all this information, she would no doubt be in a state of total mental collapse, burdened with thousands of encyclopedia articles and diagrams?
If Jackson had stipulated that Mary had the God-like property of being "physically omniscient"—not just about color but about every physical fact at every level from the quark to the galaxy—many if not all readers would resist, saying that imagining such a feat is just too fantastical to take seriously. But stipulating that Mary knows merely all the physical facts about color vision is not substantially less fantastical.

"Imagine that Mary has a billion heads. ..."
"Don't be silly!"
"Ok. Make it a million heads. ... "
"No problem!" (Really?)

In an earlier attempt to dramatize this problem of imagination, I encouraged people to consider a variant ending:

And so, one day, Mary's captors decided it was time for her to see colors. As a trick, they prepared a bright blue banana to present as her first color experience ever. Mary took one look at it and said "Hey! You tried to trick me! Bananas are yellow, but this one is blue!" Her captors were dumfounded. How did she do it? "Simple," she replied. "You have to remember that I know everything—absolutely everything—that could ever be known about the physical causes and effects of color vision. So of course before you brought the banana in, I had already written down, in exquisite detail, exactly what physical impression a yellow object or a blue object (or a green object, etc.) would make on my nervous system. So I already knew exactly what thoughts I would have (because, after all, the 'mere disposition' to think about this or that is not one of your famous qualia, is it?). I was not in the slightest surprised by my experience of blue (what surprised me was that you would try such a second-rate trick on me). I realize it is hard for you to imagine that I could know so much about my reactive dispositions that the way blue affected me came as no surprise. Of course it's hard for you to imagine. It's hard for anyone to imagine the consequences of someone knowing absolutely everything physical about anything!' " [Dennett, 1991a, pp. 399-400]

It is standardly assumed that things could not proceed this way. As Jackson disarmingly put it, "It seems just obvious that she will learn something about the world and our visual experience of it." Or as George Graham and Terry Horgan (2000, p. 72) say, "Surely, [ding!], we submit, she should be both surprised and delighted." That is a mistake, and that is what is wrong with Mary as a thought experiment. It just feels so good to conclude that Mary has a revelation of some sort when she first sees color that nobody wants to bother showing that this is how the story must go. In fact, it needn't go that way at all.
Jackson's intuition pump excellently exposes to the light a lot of naïve thinking about the nature of color experience and the brain that no doubt serves people well most of the time, so we might grant that he nicely draws out some of the implications of folk theory. But his aim was to refute a hypothesis about the capacity of the physical sciences to account for all color phenomena. Of course in any real-world situation, somebody in Mary's imagined position would learn something new because however much she knew about color, there would be lots of facts about physical effects of color she didn't know. It is only the stipulated extreme case that makes Jackson's "just obvious" and Graham and Horgan's "surely" out of place. If you are still inclined to think that my suggested alternative ending of the story would have to be impossible, see if you can work out an argued reason for your belief. It would be interesting to see if you come up with a consideration that has escaped the hundreds of philosophers who have labored over this for years. (Of course, that very fact might be taken to show that this is after all a wonderful intuition pump: it has provided employment for philosophers for three decades.)











Summary
A besetting problem for the scientific study of consciousness has been the fact that everybody is an expert! Not really, of course, but just about everybody who has reflected for more than a few minutes on the topic seems to think the deliverances of those reflections are as authoritative as the results of any high-tech experiment or any mountain of statistics. It can be downright comical to hear them during the question-and-answer sessions at scientific conferences firmly correcting the mistakes they have just detected in the presenter's work, by citing a recent experience they (think they) had. If we were, as these people typically think, infallible judges of the nature of our own personal experiences, they would be right!
But you can misremember, misinterpret, misdescribe your own most intimate experiences, covertly driven by some persuasive but unreliable bit of ideology. Here is a simple demonstration you can perform at home that may surprise you. Sit in front of a mirror so you can monitor your own compliance with the directions, which are to stare intently into your own eyes, fixating on them as a target instead of letting your eyes get attracted to peripheral goings-on. Now, without looking, take a card from the middle of a well-shuffled deck of cards and hold it, face-side toward you, at arm's length just outside the boundaries of your peripheral vision. Wiggle the card. You will know you are doing it, but you won't see it, of course. Start moving the card into your field of view, wiggling it as you do so. First you can see motion (the wiggling) but no color! You can't tell whether it's a red card or a black card or a face card, and you certainly can't identify its number. As you move it more and more centrally, you will be amazed at how close to straight ahead it has to be for you to identify its color, or the fact that it is a face card or not. As the card gets closer and closer to your fixation point, you must concentrate on not cheating, stealing a glance at the card as it moves in. When you are finally able to identify the card, it is almost directly in front of you. Surprising? I don't know anybody who wasn't surprised the first time they experienced this. Here you've thought all along that your vision was roughly equally detailed and colored "all the way out to the periphery," and now you learn that although that seems to "stand to reason," and seems confirmed by casual introspection, it is simply not true. This is just one dimension, one phenomenon among many, in which the apparently rich, continuous, detailed presentation of the world in our (visual) consciousness is an illusion. The moral is unmistakable: don't think you understand the phenomena of consciousness until you see what science has discovered about it in recent years. The armchair theories of philosophers who ignore this moral are negligible at best and more often deeply confused and confusing. What you "learn" about your consciousness "through introspection" is a minor but powerfully misleading portion of what we can learn about your consciousness by adopting the heterophenomenological framework and studying consciousness systematically.
There are still difficulties aplenty in need of resolution, hard problems that are not the Hard Problem. If when we have solved all these "easy" problems there is still a deeply mysterious residue, it will then be time to reconsider our starting point and cast about for some radical departure from current assumptions about biology, physics, and even logic. In the meantime, let's see how far we can get with business-as-usual science, the science that has brought us our current understanding of everything from asteroids and plate tectonics to reproduction, growth, repair, and metabolism in living things.














VIII. TOOLS FOR THINKING ABOUT FREE WILL

 

The chasm between the manifest image and the scientific image is at its most treacherous when the topic is free will. Like the questions of what color is, what it really is, and what dollars are, really, when you get right down to it the question of whether free will is an illusion or something we actually have invites us to use the scientific image to investigate this issue, which is posed in the traditional terms of the manifest image. And the invitation has been enthusiastically accepted in recent years. There has been quite a chorus of eminent scientists saying, point blank, that free will is an illusion: neuroscientists Wolf Singer, Chris Frith, and Patrick Haggard; psychologists Paul Bloom and Daniel Wegner; and a few rather well-regarded physicists, Stephen Hawking and Albert Einstein. Could so many brilliant scientists be wrong? Many—not all, and maybe not most—philosophers say yes. They say this is a job for philosophy! Are they right? I think so.
The scientists have typically been making a rookie mistake: confusing the manifest image with what we might call the folk ideology of the manifest image. The folk ideology of color is, let's face it, bonkers; color just isn't what most people think it is, but that doesn't mean that the manifest world doesn't really have any colors; it means that colors—real colors—are quite different from what most folks think they are. The folk ideology of consciousness is also bonkers—resolutely dualistic and mysterian; if that were what consciousness had to be, then Wright would be right (see p. 313): we'd have to say that consciousness doesn't exist. But we don't have to treat consciousness as "real magic"—the kind that doesn't exist, made of wonder tissue; we can recognize the reality of consciousness as a phenomenon by acknowledging that folks don't yet have a sound ideology about it. Similarly, free will isn't what some of the folk ideology of the manifest image proclaims it to be, a sort of magical isolation from causation. I've compared free will in this sense to levitation, and one of the philosophical defenders of this bonkers vision has frankly announced that a free choice is a "little miracle." I wholeheartedly agree with the scientific chorus that that sort of free will is an illusion, but that doesn't mean that free will is an illusion in any morally important sense. It is as real as colors, as real as dollars.
Unfortunately, some of the scientists who now declare that science has shown that free will is an illusion go on to say that this "discovery" matters, in a morally important sense. They think it has major implications for morality and the law: nobody is ever really responsible, for instance, so nobody ever deserves to be either punished or praised. They are making the mistake people make when they say that nothing is ever solid, not really. They are using an unreconstructed popular concept of free will, when they should be adjusting it first, the way they do with color and consciousness (and space and time and solidity and all the other things that the ideology of the manifest image gets wrong).
The intuition pumps in this part are designed to wean you from that ideology about free will and get you to see a better concept, the concept of real free will, practical free will, the phenomenon in the manifest image that matters. The controversies that have swirled around the topic of free will for several millennia are too many and too tangled to be settled in any one part or book, but we have to start somewhere, and these are thinking tools that work rather like crowbars, jimmying you out of well-worn ruts into rather new terrain with better perspectives. The first is designed to show why this is such an important task.











65. A Truly Nefarious Neurosurgeon
We are at the dawn of neurosurgical treatment of debilitating psychological conditions. Deep brain stimulation by implanted electrodes is showing striking effects in treating obsessive-compulsive disorder (OCD), for instance, as reported in a pioneering study by neuropsychiatrist Damiaan Denys and his colleagues (2010) in Amsterdam. That is fact, but this is fiction: One day a brilliant neurosurgeon said to a patient on whom she had just performed an implantation procedure in her shiny high-tech operating theater:

The device I've implanted doesn't just control your OCD; it controls your every decision, thanks to our master control system, which maintains radio contact with your microchip twenty-four hours a day. In other words, I've disabled your conscious will; your sense of free will henceforth will be an illusion.

In fact she had done no such thing; this was simply a lie she decided to tell her patient to see what would happen. It worked; the poor fellow went out into the world convinced that he was not a responsible agent, but rather a mere puppet, and his behavior began to show it: he became irresponsible, aggressive, and negligent, indulging his worst whims until he got caught and put on trial. Testifying in his own defense, he passionately protested his non-responsibility because of the implant in his brain, and the neuroscientist, when called to testify, admitted what she had said, and added, "But I was just messing with his head—a practical joke, that's all. I never thought he'd believe me!"
It really doesn't matter whether the court believed his testimony or hers, whether it sentenced him or her; either way she ruined his life with her ill-considered assertion, robbing him of his integrity and crippling his power to make decisions. In fact, her false "debriefing" of her patient actually accomplished nonsurgically much of what she claimed to accomplish surgically: she disabled him. But if she is responsible for this dire consequence, the neuroscientists currently filling the media with talk about how their science shows that free will is an illusion are risking mass-production of the same harm to all the people who take them at their word.* Neuroscientists, psychologists, and philosophers need to take seriously their moral obligation to think through the presuppositions and implications of their public pronouncements on these issues with the same care that is demanded of people who hold forth on global warming or impending asteroid strikes. For just one example, consider the message that wily social critic and observer Tom Wolfe (2000 p. 100) finds in the pronouncements of these neuroscientists:

The conclusion people out beyond the laboratory walls are drawing is:
The fix is in! We're all hardwired! That, and: Don't blame me! I'm wired wrong!

Wired wrong? What would it be, then, to be wired right—or have scientists "discovered" that nobody is, or could be, wired right for moral responsibility?











66. A Deterministic Toy: Conway's Game of Life
When physicist Richard Feynman found himself listening to a scientific talk in a field he didn't know well, he had a favorite question to ask the speaker: Can you give me a really simple example of what you're talking about? If the speaker couldn't oblige, Feynman got suspicious, and rightly so. Did this person really have something to say, or was this just fancy technical talk parading as scientific wisdom? If you can't make a hard problem relatively simple, you are probably not going about it the right way. Simplification is not just for beginners.
Biology has its "model organisms"—species that have been carefully chosen to ease the path of the experimenter; they reproduce swiftly in the lab, are relatively safe and easy to handle, and—once they have been studied by many teams—are well mapped and understood: the fruit fly, the laboratory rat, the zebra fish, the squid (for its giant nerve axon), the nematode worm Caenorhabditis elegans, and Arabidopsis thaliana, a hardy, fast-growing plant related to mustard, the first plant to have its entire genome sequenced. Artificial intelligence (AI) has its own simple cases, known as "toy problems," which, as the name suggests, are deliberately oversimplified versions of "serious" real-world problems. Many of the most interesting programs devised in AI are solutions to toy problems—such as getting a computer program to build simple structures in the blocks world, a virtual world consisting of a tabletop with a bunch of movable children's blocks on it. Playing chess is a toy problem; it is certainly much more tractable than driving a car from Maine to California, or solving the Arab-Israeli conflict, or even making a sandwich of appropriate ingredients found in a kitchen. Ethicists have their trolley problems: In the simplest version, a runaway trolley is heading down a track where if it is not diverted it will hit and kill five people who cannot get off the track; there is a switch that you can throw that will divert the trolley to a side track—where it will hit and kill a single person. Do you throw the switch?
Here is a toy world for helping people think about determinism, the idea that the facts at one moment in time—the location, mass, direction, and velocity of every particle—determine what happens in the next moment, and so on, forever. Physicists and philosophers and others have argued for several millennia about whether our universe is deterministic, or whether there are some genuinely undetermined events: utterly unpredictable "random" events that just happen without anything causing them to happen. Even experienced thinkers may find new insights by playing around with Life, the breathtakingly simple model of a deterministic world created by the mathematician John Horton Conway and his graduate students in 1970.
Life is played on a two-dimensional grid, such as a checkerboard, using simple counters, such as pebbles or pennies—or one could also go high-tech and play it on a computer screen. It is not a game one plays to win; if it is a game at all, it is solitaire. The grid divides the plane into square cells, and each cell is either ON or OFF at each moment. (If it is ON, place a penny on the square; if it is OFF, leave the square empty.) Notice that each cell has eight neighbors: the four adjacent cells—north, south, east, and west—and the four diagonals—northeast, southeast, southwest, and northwest.

Figure 1

Time in the Life world is discrete, not continuous; it advances in ticks, and the state of the world changes between each tick according to the following rule:
Life Physics: For each cell in the grid, count how many of its eight neighbors are ON at the present instant. If the answer is exactly two, the cell stays in its present state (ON or OFF) in the next instant. If the answer is exactly three, the cell is ON in the next instant whatever its current state. Under all other conditions the cell is OFF.

That's the only rule of the game. You now know all there is to know about how to play Life. The entire physics of the Life world is captured in that single, unexceptioned law. While this is the fundamental law of the "physics" of the Life world, it helps at first to conceive this curious physics in biological terms: think of cells going ON as births, cells going OFF as deaths, and succeeding instants as generations. Either overcrowding (more than three inhabited neighbors) or isolation (less than two inhabited neighbors) leads to death. Consider a few simple cases.

Figure 2

In the configuration in figure 2, only cells D and F have exactly three neighbors ON, so they will be the only birth cells in the next generation. Cells B and H each have only one neighbor ON, so they die in the next generation. Cell E has two neighbors ON, so it stays on. So the next "instant" will look like figure 3.

Figure 3

Obviously, the configuration will revert back in the next instant, and this little pattern will flip-flop back and forth indefinitely, unless some new ON cells are brought onto the scene somehow. It is called a flasher or traffic light. What will happen to the configuration in figure 4?

Figure 4

Nothing. Each ON cell has three neighbors ON, so it is reborn just as it is. No OFF cell has three neighbors ON, so no other births happen. This configuration is called a still life.
By the scrupulous application of our single law, one can predict with perfect accuracy the next instant of any configuration of ON and OFF cells, and the instant after that, and so forth. In other words, the Life world is a toy world that perfectly instantiates the determinism made famous by the early-nineteenth-century French scientist Pierre Laplace: given the state description of this world at an instant, we observers can perfectly predict the future instants by the simple application of our one law of physics. Or we could put it this way: when we adopt the physical stance toward a configuration in the Life world, our powers of prediction are perfect: there is no noise, no uncertainty, no probability less than one. Moreover, it follows from the two-dimensionality of the Life world that nothing is hidden from view. There is no backstage; there are no hidden variables; the unfolding of the physics of objects in the Life world is directly and completely visible.
If you find following the simple rule a tedious exercise, there are computer simulations of the Life world in which you can set up configurations on the screen and let the computer execute the algorithm for you, changing the configuration again and again according to the single rule. In the best simulations, the scale of both time and space can be changed, alternating between close-up and bird's-eye views.
One soon discovers that some simple configurations are more interesting than others. Consider a diagonal line segment, such as the one shown in figure 5.

Figure 5

This pattern is not a flasher; each generation, the two ON cells at either end of the row die of isolation and there are no birth cells. The whole segment soon evaporates. In addition to the configurations that never change—the still lifes—and those that evaporate entirely, such as the diagonal line segment, there are configurations with all kinds of periodicity. The flasher, we saw, has a two-generation period that continues ad infinitum, unless some other configuration encroaches. Encroachment is what makes Life interesting: among the periodic configurations are some that swim, amoeba-like, across the plane. The simplest is the glider, the five-pixel configuration shown taking a single stroke to the southeast in figure 6.

Figure 6

Then there are the eaters, the puffer trains and space rakes, and a host of other aptly named denizens of the Life world that emerge as recognizable objects at a new level (analogous to the design level). This level has its own language, a transparent foreshortening of the tedious descriptions one could give at the physical level. For instance:

An eater can eat a glider in four generations. Whatever is being consumed, the basic process is the same. A bridge forms between the eater and its prey. In the next generation, the bridge region dies from overpopulation, taking a bite out of both eater and prey. The eater then repairs itself. The prey usually cannot. If the remainder of the prey dies out as with the glider, the prey is consumed. [Poundstone, 1985, p. 38]


Figure 7

Notice that something curious happens to our "ontology"—our catalogue of what exists—as we move between levels. At the physical level there is no motion, only ON and OFF, and the only individual things that exist, cells, are defined by their fixed spatial location. At the design level we suddenly have the motion of persisting objects; it is one and the same glider (though each generation is composed of different cells) that has moved southeast in figure 5, changing shape as it moves; and there is one less glider in the world after the eater has eaten it in figure 7.
Notice too that whereas at the physical level, there are absolutely no exceptions to the general law, at this level our generalizations have to be hedged: they require "usually" or "provided nothing encroaches" clauses. Stray bits of debris from earlier events can "break" or "kill" one of the objects in the ontology at this level. Their salience as real things is considerable, but not guaranteed. To say that their salience is considerable is to say that one can, with some small risk, ascend to this design level, adopt its ontology, and proceed to predict—sketchily and riskily—the behavior of larger configurations or systems of configurations, without bothering to compute the physical level. For instance, one can set oneself the task of designing some interesting super-system out of the "parts" that the design level makes available.
This is just what Conway and his students set out to do, and they succeeded majestically. They designed, and proved the viability of the design of, a self-reproducing entity composed entirely of Life cells. Grinding away deterministically on its infinite plane, it would copy itself perfectly, and then its copy would copy itself, and so forth. It was also (for good measure) a Universal Turing machine: a two-dimensional computer that in principle can compute any computable function! What on earth inspired Conway and his students to create first this world and then this amazing denizen of that world? They were trying to answer at a very abstract level one of the central questions of biology: What is the minimal complexity required for a self-reproducing thing? They were following up the brilliant early speculations of John von Neumann, who had been working on the question at the time of his death in 1957. Francis Crick and James Watson had discovered the structure of DNA in 1953, but how it worked was a mystery for many years. Von Neumann had imagined in some detail a sort of floating robot that picked up pieces of flotsam and jetsam that could be used to build a duplicate of itself, and that would then be able to repeat the process. His description (posthumously published, 1966) of how an automaton would read its own blueprint and then copy it into its new creation anticipated in impressive detail many of the later discoveries about the mechanisms of DNA expression and replication, but in order to make his proof of the possibility of a self-reproducing automaton mathematically rigorous and tractable, von Neumann had switched to simple, two-dimensional abstractions, now known as cellular automata. Conway's Life world cells are a particularly agreeable example of cellular automata.
Conway and his students wanted to confirm von Neumann's proof in detail by actually creating a two-dimensional world with a simple physics in which such a self-replicating construction would be a stable, working structure. Like von Neumann, they wanted their answer to be as general as possible, and hence as independent as possible of actual (earthly? local?) physics and chemistry. They wanted something dead simple, easy to visualize and easy to calculate, so they not only dropped from three dimensions to two but also "digitized" both space and time: all times and distances, as we saw, are in whole numbers of "instants" and "cells." It was von Neumann who had taken Alan Turing's abstract conception of a mechanical computer (now called a Turing machine) and engineered it into the specification for a general-purpose stored-program serial-processing computer (now called a von Neumann machine), and in his brilliant explorations of the spatial and structural requirements for such a computer, he had realized—and proved—that a Universal Turing machine (see part IV) could in principle be "built" in a two-dimensional world.* Conway and his students also set out to confirm this with their own exercise in two-dimensional engineering.†
It was far from easy, but they showed how they could "build" a working computer out of simpler Life forms. Glider streams can provide the input-output "tape," for instance, and the tape-reader can be some huge assembly of eaters, gliders, and other bits and pieces. What does this machine look like? Poundstone (1985) calculated that the whole construction would be on the order of 1013 cells or pixels.
Displaying a 1013-pixel pattern would require a video screen about 3 million pixels across at least. ... [Imagine a screen with the high resolution of your laptop or iPad, a half a mile wide.] Perspective would shrink the pixels of a self-reproducing pattern to invisibility. If you got far enough away from the screen so that the entire pattern was comfortably in view, the pixels (and even the gliders, eaters and guns) would be too tiny to make out. A self-reproducing pattern would be a hazy glow, like a galaxy. [pp. 227-288]

In other words, by the time you have built up enough pieces into something that can reproduce itself (in a two-dimensional world), it is roughly as much larger than its smallest bits as an organism is larger than its atoms. You probably can't do it with anything much less complicated, though this has not been strictly proved.
The game of Life illustrates many important principles and can be used to construct many different arguments or thought experiments, but I will use it to illustrate just three points, leaving you to discover others for yourself: First, notice how the distinction between the physical stance and the design stance gets blurred here. Do gliders, for instance, count as designed things, or as natural objects—like atoms and molecules? The tape-reader that Conway and his students cobbled out of gliders and eaters and the like must count as designed if anything does, but its ingredients are quite raw materials—the simplest "things" in the Life world. Nobody had to design or invent the glider; it was discovered to be implied by the physics of the Life world. But that, of course, is actually true of everything in the Life world. Nothing happens in the Life world that isn't strictly implied—logically deducible by straightforward theorem-proving—by the physics and the initial configuration of cells. Some of the things in the Life world are just more marvelous and unanticipated (by us, with our puny intellects) than others. There is a sense in which the Conway self-reproducing computer pixel-galaxy is "just" one more Life macromolecule with a very long and complicated periodicity in its behavior. This nicely illustrates a parallel point about biology and the origin of life: amino acids, one might say, just are; they didn't have to be designed. But proteins, composed of nothing but amino acids, are too fancy; they are at least sorta designed. Darwin's gradualism makes yet another appearance.
Second, the Life world, being deterministic, has a perfectly predictable future for every possible configuration, but, somewhat surprisingly, its past is often perfectly inscrutable! Consider the still life consisting of four ON pixels in a square. You can't tell from looking at it, or even looking at it and its neighborhood, what its past was. To see this, note that any three of the four pixels ON would lead, in the next generation, to this still life of four pixels ON. Whether any of those cells were OFF in the past is an inert historical fact.
Third, recall how important "noise" and collisions were in creating the mutations that evolution—like other creative processes—feeds on. Conway's huge construction reproduced itself, but it couldn't mutate. It would always make a perfect copy of itself, and in order to introduce mutations into the picture, the whole construction would have to be enlarged many fold. Why? Because the Life world is deterministic, so the only way a "random" mutation can occur is if some stray bit of stuff wanders (pseudo-randomly) onto the scene and breaks something. But the smallest moving thing is a glider, so think of it as like a single photon, or cosmic ray, moving at the speed of (Life physics) light. A single glider can do a lot of damage; if it must barely "tweak" something in the genome of the self-reproducing thing without destroying the genome, that genome is going to have to be very large, relative to the glider, and quite robust. It might well be provable that evolution could not occur in the Life world, no matter how large we made the entities, if it turned out that these galaxy-sized assemblages were just too fragile to survive an occasional rain of gliders.











67. Rock, Paper, and Scissors
Probably every one of you knows the game of rock, paper, and scissors. Two people face each other, and count, "One, two, three, shoot!" and each, simultaneously, extends a hand either clenched in a fist (rock), with a pair of fingers extended (scissors), or with the palm flat open facing down (paper). Rock breaks (beats) scissors, scissors cut (beat) paper, and paper covers (beats) rock. Unless both players display the same hand symbol—a tie—one player wins and the other loses. It's a tantalizing game because if you can outguess your opponent, you can seem to read her mind and come up with the right hand shape to win consistently. It's unnerving when that happens. Can some people play the game better than others? Apparently, since there have been tournaments with large cash prizes in which national and international winners have emerged, and—this is important since every tournament has to be won by somebody even if there is no skill involved—the better players have a history of winning.
How do they do it? Perhaps by picking up subtle hints from the faces and postures of their opponents. Poker players speak of reading the "tell" of another player, sensing when they are bluffing and when they aren't, while maintaining a "poker face" of their own. Perhaps most people who play rock, paper, and scissors have a tell that they can't control and that the best players pick up at the last instant. So what is the best strategy to use to prevent your opponent from picking up a pattern from your outer demeanor? Playing absolutely randomly is the best, since if you play a random sequence of moves, there is simply no pattern for your opponent to detect. (While you are playing randomly, and breaking even, you can try to find a pattern in your opponent's moves, and use that pattern to craft a nonrandom strategy, and then strike.)
People are notoriously bad at creating actually random series. They tend to switch too often, avoiding choosing the same move two or three times in a row, for instance (which ought to occur fairly often in a genuinely random series). Knowing that your casual effort to create a genuinely patternless series is apt to fail, you should consider a better strategy: get a table of random numbers from the library (or online). Put your finger "at random" somewhere on the table, and copy down the next hundred digits. Throw out all the 0s (say), and then replace each 1, 2, or 3 with "R" (for rock), each 4, 5, and 6, with "P" (for paper), and each 7, 8, and 9 with "S" (for scissors). This will give you a sequence of approximately ninety moves (since you threw out approximately ten 0s), which should be enough for a match.
Now you're ready to play, and the cardinal rule is: keep your list secret. If your opponent gets to peek at it, you will be completely at her mercy. As the saying goes, she will turn you into a money pump. If she has no way of getting at your list, on the other hand, she will have to start trying to second-guess you, anticipating your line of thinking as best she can. (In short, she will have to treat you from the intentional stance and reason about your reasoning, instead of treating you as a simple mechanism whose behavior she can reliably read off your list.)
This simple principle of keeping your intended choices secret from your opponent turns out to be one of the central pivots in the long-standing controversies over free will. In fact, the invention of game theory by von Neumann and Morgenstern (1944) began with the recognition that while a solitary agent (or intentional system) trying to predict the future from the information it gathers can get by with a calculation of expected utility using probability theory (of one sort or another), as soon as there are two agents, two intentional systems in the environment, the circumstance is changed radically. Now each agent has to take into account the attempts at prediction by the other agent, and include the other agent's observation of, and attempts to anticipate and exploit, its own behavior, creating feedback loops of indefinite complexity.*
This fundamental fog of inscrutability or unpredictability of agents by each other creates the conditions in which game theory flourishes. Evolution has discovered this, and many species can be seen to apply the principles of game theory (competence without comprehension!) in their interactions. Gazelle stotting is just one simple example. Another is the erratic flight of butterflies, whose trajectories are hard for insectivorous birds to predict, though evolution has helped the birds by giving them a faster "flicker fusion rate" than we have, for instance (they see more "frames per second" than we do—a movie would look like a slide show to them).
Be unpredictable, and look out for others following this advice! "Appreciation" of this principle can be seen in the widespread instinct found in animals who, when confronted with any complicated moving entity, try to treat it as an agent—"Who goes there, and what do you want?" not just "What's that?"—in order to be safe, because maybe it is an agent and it wants to eat the animal, or to mate with it, or to fight over some prize. This instinctual response is the source in evolution of the invention of all the invisible elves, goblins, leprechauns, fairies, ogres, and gods that eventually evolve into God, the ultimate invisible intentional system (Dennett, 2006a).
Like animals who adopt this strategy without having to know why, we human beings appreciate the tactic of preserving our unpredictability without having to understand why this is such a good idea. But it's pretty obvious, most of the time. When you are shopping and spot an antique that you have to have, you know better than to gush over it before you've learned the selling price. You'll get scalped by the seller if you don't. When you advertise something for sale, you put a reasonable asking price on it—one you would be content to receive—because you can't tell when a buyer would actually pay more, and you hope that the buyer can't tell how much less you would accept than your asking price. (You can refuse to bargain, of course.) Auctions are ways of exploring this terra incognita, and if you trust the auctioneer with an advance bid, you are counting on his integrity not to reveal your maximum to the other bidders.
Similarly, when you fall head over heels in love at first sight of somebody, you do your best not to swoon and pant, keeping as reserved and casual as possible; you don't want to scare this paragon off or, alternatively, let this delight have too much of the upper hand, wrapping you around her little finger. A poker face is not just for poker. In general, you improve your chances of getting what you want by keeping the competition—the other agents in the neighborhood—off balance, so they don't have good enough hunches about which way you're going to leap to prepare for your landing. (It's costly to prepare another agent's environment, so your opponents won't try to anticipate you unless they have very good evidence of what you will do.)
Magicians know how to do a "psychological force" to get you to pick from the deck ("of your own free will") the card they want you to take. There are many methods, all subtle and hard to detect, and a really good magician can do it most of the time. This is a genuine abridgement of your agency, a manipulation that turns you into a tool, a pawn, an extension of the magician's will, not a free agent.
Contrary to ancient ideology, we don't want our free choices to be utterly uncaused. What we all want, and should want, is that when we act, we act based on good information about the best options available to us. If only the environment will cause us to have lots of relevant true beliefs about what's out there, and also cause us to act on the most judicious assessment of that evidence we could achieve! That would give us almost everything we want as agents—except this: we wouldn't want the environment to include a manipulative agent that usurps control from us, so we wouldn't want the environment to make our best moves too obvious to all the other agents out there, for then they can exploit us, knowing too much about what we want and how much we want it. So add to our wish list the capacity to keep our thought processes and our decisions to ourselves, even if it means on occasion choosing our second-best option, just to keep the others off balance. (Clegg, 2012, provides a pioneering formal analysis of this.)
Some people, dimly appreciating the importance of this unpredictability, think that "just to be safe" they should hold out for absolute unpredictability, which can be achieved only if, down in the basement of our brains, matters are physically indeterministic. Here is how the philosopher Jerry Fodor (2003) once put it with characteristic vividness:
One wants to be what tradition has it that Eve was when she bit the apple. Perfectly free to do otherwise. So perfectly free, in fact, that even God couldn't tell which way she'd jump. [p. 18]

But why does "one want" this? Is this absolute unpredictability any better, really, than practical unpredictability? Many philosophers, over several thousand years, have insisted on absolute unpredictability as a condition of genuine free will. Do they know something that we don't know? If so, they are keeping it secret. To almost all of them, the idea that free will is incompatible with determinism has seemed too obvious to need an argument. I say that the burden of proof is on them. Show us why we ought to despair if we can't have absolute unpredictability. I've shown why we—like evolution itself—are wise to arrange for as much practical unpredictability as we can. Tell us, please, why that is not enough.
I will give you one good reason: if you plan to play rock, paper, and scissors with God, for high stakes (salvation, maybe), then you have reason, just as Fodor says, to want "perfect" freedom. I, for one, don't anticipate that contest, so I am content with the practical freedom I have by just keeping my counsel and staying away from sleight-of-hand artists and other manipulators when the stakes are high.











68. Two Lotteries
Compare the following two lotteries for fairness. In Lottery A—for "After"—all the tickets are sold, their stubs are placed in a suitable mixer and mixed, as randomly as you like, and then the winning ticket is blindly drawn. (Most lotteries we encounter are like this.) In Lottery B—for "Before"—the mixing of stubs and the blind drawing of the winner take place before the tickets are sold (and the winning stub is put in a safe), but otherwise the lotteries are conducted the same way. Someone might think the second lottery is unfair because the winning ticket is determined before people even buy their tickets. One of those tickets is already the winner (even if nobody knows which one); the other tickets are worthless paper, and selling them to unsuspecting people is some sort of fraud. But in fact both lotteries are equally fair. Everyone who buys a ticket has an equal chance of winning; the timing of the selection of the winner is an utterly inessential feature.
The drawing in most lotteries is postponed until after the sale of the tickets in order to provide the public with firsthand eyewitness evidence that there have been no shenanigans. No sneaky person with inside knowledge has manipulated the distribution of tickets, because the knowledge of the winning ticket did not (and could not) exist in any agent until after the tickets were sold. It is interesting that not all lotteries follow this practice. Publisher's Clearing House used to mail out millions of envelopes each year that had written on them in bold letters "YOU MAY ALREADY HAVE WON"—a million dollars, or some other prize. (The organization now runs its lottery largely online.) These expensive campaigns are based on market research showing that in general people do think lotteries with preselected winners are fair so long as they are honestly conducted. But perhaps people go along with these lotteries uncomplainingly because they get their tickets for free. Would many people buy a ticket in a lottery in which the winning stub, sealed in a special envelope, was known to have been deposited in a bank vault from the outset? People buy scratch tickets by the millions, and whether or not any ticket is a winner is already determined when it is bought. Apparently these people consider themselves to have a real opportunity to win. I think they are right, but whether they are right or wrong, their calm conviction that such lotteries are fair, and that they have a real opportunity to win, should undo the confidence of the philosophers (going back two millennia to Democritus and Lucretius) who have somehow convinced themselves that no opportunity is a real opportunity unless the outcome is undetermined up to the last instant. These philosophers have maintained that without a continuing supply of truly random, undetermined branch points to break up the fabric of causation, there is no possibility of free choices, no real chances to do the right thing.
The two lotteries give us a new perspective on the problem of determinism. If the world is determined, then we have pseudo-random number generators in us, not truly (quantum-mechanical) random randomizers. If our world is determined, all our lottery tickets were drawn at once, in effect, about fourteen billion years ago at the moment of the Big Bang, put in an envelope for us, and doled out as we needed them through life. Whenever you need to "flip a coin" or in some less ostentatious way make a chancy decision, your brain opens the envelope and takes out the next "random" number, letting its value determine what you do, just like the list of moves in "rock, paper, and scissors." "But that is unfair," someone may say, "for some people will have been dealt more winners than others." Indeed, on any particular deal, some people have more high cards than others, but one should remember that in the long run the luck averages out. "But if all the drawings take place before we are born, some people are determined to get more luck than others!" That will be true, however, even if the drawings are not held before we are born, but periodically, on demand, throughout our lives. Even in a perfectly random and unbiased drawing, a genuinely undetermined drawing, it is still determined that some people will get more winners than others. Even in a perfectly fair, perfectly random, coin-tossing tournament, it is determined that someone—or other—will win, and everybody else in the tournament will lose. The winner cannot properly claim it was his "destiny" to win, but whatever advantages accrue to winning are his, destiny or not, and what could be fairer than that? Fairness does not consist in everybody winning.
Probably the most frequently cited reason for hoping for indeterminism is that without it, when we choose an act, "we could not have done otherwise," and surely (ding!) that is something that should be important to us. This, too, is not as obvious as it has often seemed, and in order to get a glimpse at how this familiar idea might be misleading us, consider the curious category of inert historical facts.











69. Inert Historical Facts
An inert historical fact is any fact about a perfectly ordinary arrangement of matter in the world at some point in the past that is no longer discernible, a fact that has left no footprints at all in the world today. My favorite example of an inert historical fact is this:

A. Some of the gold in my teeth once belonged to Julius Caesar.

Or maybe this:

B. It is false that some of the gold in my teeth once belonged to Julius Caesar.

Now (logic tells us) one of these two must be a fact. (Hang on: Wasn't the moral of the sorta operator chapter that we should distrust these "obvious" disjunctions? Let's check this one out. How could it be the case that neither A nor B had a clear claim on truth? Well, what if ownership in Caesar's day was either vague or ill defined so that Caesar only sorta owned some of his gold—perhaps the way the Queen of England to this day is the owner of all the swans in the land?*) Assuming that the relevant concept of ownership is nicely demarcated, the way the concept of gold is, one of the two sentences must express a fact, but which of the two is true is almost certainly not discoverable by any physical investigation, no matter how sophisticated it is and no matter how long it takes to conduct it.
Really? We can imagine cases where we could be almost certain that either A or B is the true alternative. If it turned out that, because of various well-recorded historical processes, the "provenance" of some of the gold in my teeth was scrupulously controlled and recorded through the millennia (like the "chain of custody" for exhibits of evidence in murder trials), we could be quite sure that A is true. Let's say that my dentist purchased an ancient gold ring from a museum, the famous "Caesar's pinky ring," which a host of documents attest had been handed down over the centuries from monarch to monarch until it arrived in the museum, and there is a videotape showing the dentist melting the ring and pouring the molten gold into the plaster cast for my filling. Outlandish, sure, but clearly within the bounds of physical possibility. Or alternatively, suppose that I am a gold-panning hobbyist and made a trip to the base of a receding glacier in Alaska that had covered the land for ten thousand years, where I carefully collected all the gold that subsequently went into my teeth. Then B would be even more certainly the truth. But if nothing like these extreme tales is well evidenced, then it is as good as certain that we could never know which of A or B is true. The true one, whichever it is, is an inert historical fact.
Quantum physics raises an interesting difficulty: even if the trajectory of each atom of gold in my teeth could be traced back through the centuries, if there were occasions when two or more gold atoms—one from the Caesar legacy and one from outside sources—collided (or just got very close to each other), it would be impossible in principle to tell which atom was which after the "collision." Atoms and smaller particles don't have anything like fingerprints or other distinguishing characteristics, and cannot be continuously tracked, so the continuing identity of particles does not always make sense, adding another barrier to knowing the facts about the gold.
Now whether or not the whole universe is deterministic, computers are designed to be deterministic in the face of submicroscopic noise and even quantum randomness, absorbing these fluctuations by being digital, not analog. (We saw a vivid example of that with Conway's game of Life in chapter 66, but digital determinism is everywhere.) The fundamental idea behind digitizing in order to produce determinism is that we can create inert historical facts by design. Forcibly sorting all the pivotal events into two categories—high vs. low; ON vs. OFF; 0 vs. 1—guarantees that the micro-differences (between different high voltages, different flavors of being ON, different shades of 0) are ruthlessly discarded. Nothing is allowed to hinge on them, and they vanish without a trace, facts about actual historical variations that make no difference at all to the subsequent series of states through which the computer passes. For instance, your friend downloads a song from a website and burns it onto two CDs. These two unlabeled CDs—we'll call them A and B, but we won't write their names on them—will be digital duplicates, of course. Ask him to "copy" one of them onto your laptop in a pitch-black room. Don't tell him which CD to use, and have him handle both CDs after completing the action (wiping out the possibility of using fingerprint evidence, or DNA traces) before depositing them in a bag full of other CDs and shaking it vigorously. Now we have two candidate inert historical facts:

(a) Your laptop has a copy made from the A disc.
(b) Your laptop has a copy made from the B disc.

Any actual physical encoding of the bit stream of the song will have a microscopically fine structure that differs from the fine structure of any other encoding. And when you "copy" one of the CDs to RAM (random-access memory), the voltage patterns in RAM will also have a unique fine structure, and if we then "copy" the file from RAM to a hard disk or a flash drive, these too will contain microscopic differences that distinguish them. What is conveniently called copying is always the creation of yet another continuous or analog physical signal that has its own unique fine structure because that's the way the world is way down among the electrons and protons. But the genius of digitization is that all this fine structure is "ignored," wiped out in the "correction to the norm." When you have norms, like an alphabet, it doesn't make any difference wHat f LAvOr the individual symbols have; they all read the same. In a computer it's all 0s and 1s.
So unless one of the CDs happens to have an "error" discernible at the level of the digital idealization (has a flipped bit, that is—a 0 instead of a 1, or vice versa), there will be no way of telling. The digitization prevents the propagation of the individuality of the two CDs to later versions, and ultimately to the digital-to-analog conversion that drives the speakers or ear buds. Some music lovers are reputed to be "golden eared," able to tell vinyl records from the best digital CDs, and compressed (e.g., MPEG) from uncompressed digital files, but no music lover could do better than chance if asked whether two successive playings of some music were from the same, or different, CDs. That is an inert historical fact, undetectable not just by human ears but by electron microscopy of the fine structure of their "copies" in RAM. Any human being who could do this would be a strong candidate for having supernatural ESP, since digitization creates a physical barrier to the information transmission that would enable a guesser to do better than chance.
Because a computer is a digital device, it is trivial to get it to execute a few trillion steps, and then place it back in exactly the same (digital) state it was in before, and watch it execute exactly the same few trillion (digital) steps again, and again, and again.
Wait a minute, comes an objection: You say computers are deterministic? You can get them to replay exactly the same trillion steps over and over? Gimme a break! Then why does my laptop crash every so often? Why does my word processor freeze on Tuesday when I was doing the very same thing that worked just fine on Monday?

You weren't doing the very same thing. It froze not because it is indeterministic, but because it was not in exactly the same state on Tuesday that it was in on Monday. Your laptop must have done something in the interval that raised a hidden "flag" or called up some part of the word processor that had never before been activated by you, which flipped a bit somewhere that got saved in its new position when you shut down, and now the word processor has stubbed its toe on that tiny change and crashed. And if you somehow manage to put it back in exactly the same Tuesday-morning state a second time, it will crash again.
What about the "random-number generator"? I thought my computer had a built-in device for creating randomness on demand.

Every computer these days comes equipped with a built-in "random number" generator that can be consulted whenever needed by any program running on it. (Before there were computers you could buy a book that was nothing but a table of random numbers to use in your research, page after page of digits that had been scrupulously generated in such a way as to pass all the tests of randomness that mathematicians had devised. Of course each copy of a particular edition of such a book had exactly the same sequence of random numbers in it. RAND Corporation published one of the best books in 1955; it consisted of a million random digits.) The sequence of numbers generated by a so-called random-number generator isn't really random, but just pseudo-random: it is "mathematically compressible" in the sense that this infinitely long sequence can be captured in a finitely specified mechanism that will crank it out. Suppose, for instance, your random-number generator is a program that can be specified in, say, a megabyte—eight million bits—but it generates a sequence (the same sequence every time) that is actually infinite. If you wanted to send somebody this infinite series, you wouldn't have to do so in an infinitely long e-mail message recording the series verbatim. You could just send them your megabyte-long algorithm and they'd have access to the entire infinite series. That is the fundamental idea of a pseudo-random-number generator. Whenever you start the random-number generator from a cold start—whenever you reboot your computer, for instance—it will always yield exactly the same sequence of digits, but a sequence that is as apparently patternless as if it were generated by genuinely random quantum fluctuations. It is a builtin "table of random numbers," you might say, rather like what you would get on a very long loop of videotape, recording the history of a fair roulette wheel over millions of spins. The loop always returns to "the beginning" when you start up your computer. Sometimes this matters. Computer programs that avail themselves of randomness at various "choice" points will nevertheless spin out exactly the same sequence of states if run over and over again from a cold start, and sometimes, if you want to test a program for bugs, you will always test the same "random sample" of states, unless you take steps (easy enough) to jog the program to dip elsewhere, now and then, into the stream of digits for its next "random" number.











70. A Computer Chess Marathon
It is fiendishly difficult to think clearly about determinism and choice. If determinism is true, are there ever any real choices? If an agent that apparently has free will is actually deterministic, living in a deterministic world, does this remove all choice, all opportunity? Here is an intuition pump that explores the question by looking at a simplified world—chess playing—in an artificially constructed deterministic world: the world of activity performed by a computer.
Suppose you install two different chess-playing programs on your computer and yoke them together with a little supervisory program that pits them against each other, game after game, in a potentially endless series. Will they play the same game, over and over, until you turn off the computer? You could set it up like that, but then you wouldn't learn anything interesting about the two programs, A and B. Suppose A beats B in this oft-repeated game. You couldn't infer from this that A is a better program in general than B, or that A would beat B in a different game, and you wouldn't be able to learn anything from the exact repetition about the strengths and weaknesses of the two different programs. Much more informative would be to set up the tournament so that A and B play a succession of different games. This can be readily arranged. If either chess program consults the random-number generator during its calculations (if, for instance, it periodically "flips a coin" to escape from situations where it has no handy reason for doing one thing versus another in the course of its searching for a good move), then in the following game the state of the random-number generator will have changed (unless you arrange to have it re-initialized), and hence different alternatives will be explored, in a different order, leading on occasion to different moves being "chosen." A variant game will blossom, and the third game will be different in different ways, resulting in a series in which no two games, like no two snowflakes, are alike. Nevertheless, if you turned off the computer and then restarted it running the same program, exactly the same variegated series of games would spin out, because the same pseudo-random series of numbers would determine all the "coin flips" used by both programs.
Suppose, then, we set up such a chess universe involving two programs, A and B, and study the results of a run of, say, a thousand games. We will find lots of highly reliable patterns. Suppose we find that A always beats B, in a thousand different games. That is a pattern that we will want to explain, and saying, "Since the program is deterministic, A was caused always to beat B," would utterly fail to address our very reasonable curiosity. We will want to know what it is about the structure, the methods, the dispositions, of A that accounts for its superiority at chess. A has a competence or power that B lacks, and we need to isolate this interesting factor. It might be that the explanation lies at a low level; it might turn out, for instance, that program A and program B are actually the same program, identical chess-move evaluators at the source code level, but program A is more efficiently compiled than B so that it can explore the game further than program B can in the same number of machine cycles. In effect A "thinks exactly the same thoughts" about chess as B, and B "knows" everything about chess that A does, but A just thinks faster. (Serious chess, tournament chess, is always played with a time clock; if you run out of time before you've made all your moves, you lose.) More likely the superiority of A to B would require explanation at a higher-level perspective at which the everyday topics of chess decision-making appear: representations of board positions, evaluations of possible continuations, decisions about which continuations to pursue further, and so forth. Thus program A may adjust the relative value of its pieces as the game progresses, or have better evaluation functions of board positions, or decide to terminate certain sorts of explorations earlier or later. It doesn't "think the same thoughts" as B; it "thinks better, more sophisticated thoughts." (It sorta thinks these thoughts, of course. It isn't a conscious person.)
It might actually be more telling if one program didn't always win. Suppose A almost always beats B, and suppose A evaluates moves using a different set of principles. Then we would have something even more interesting to explain. To investigate this causal question, we would need to study the history of the thousand different games, looking for further patterns. We would be sure to find plenty of them. Some of them would be endemic to chess wherever it is played (e.g., the near certainty of B's loss in any game where B falls a rook behind), and some of them would be peculiar to A and B as particular chess players (e.g., B's penchant for getting its queen out too early). We would find the standard patterns of chess strategy, such as the fact that when B's time is running out, B searches less deeply in the remaining nodes of the game tree than it does when it has more time remaining and is in the same local position. In short, we would find a cornucopia of explanatory regularities, some exceptionless (in our run of a thousand games) and others statistical.
These recognizable chess-move patterns are salient moments in the unfolding of a deterministic pageant that, observed from the perspective of micro-causation, is pretty much all the same. What from one vantage point appear to us to be two chess programs in suspenseful combat can be seen through the "microscope" (as we watch instructions and data streaming through the computer's CPU) to be a single deterministic automaton unfolding in the only way it can, its jumps already predictable by examining the precise state of the pseudo-random-number generator and the rest of the program and data. There are no "real" forks or branches in its future; all the "choices" made by A and B are already determined by the total state of the computer and its memory. Nothing, it seems, is really possible in this world other than what actually happens. Suppose, for instance, that an ominous mating net (a guaranteed win that might be hard to discern) looms over A at time t, but it collapses when B runs out of time and terminates its search for the key move one pulse too soon. That mating net was never going to happen. (This is something we could prove, if we doubted it, by running exactly the same tournament another day. At the same moment in the series, B would run out of time again and terminate its search at exactly the same point.)
So what are we to say? Is this toy world really a world without prevention or avoidance, without offense and defense, without lost opportunities, without the thrust and parry of genuine agency, without genuine possibilities? Admittedly, our chess programs, like insects and fish, are much too simple agents to be plausible candidates for morally significant free will, but the determinism of their world does not rob them of their different powers, their different abilities to avail themselves of the opportunities presented. If we want to understand what is happening in that world, we may, indeed must, talk about how their informed choices cause their circumstances to change, and about what they can and cannot do. If we want to uncover the causal regularities that account for the patterns we discover in those thousand games, we have to take seriously the perspective that describes the world as containing two agents, A and B, trying to beat each other in chess.
Suppose we rig the tournament program so that whenever A wins, a bell rings, and whenever B wins, a buzzer sounds. We start the marathon and an observer who knows nothing about the program notes that the bell rings quite frequently, the buzzer hardly ever. What explains this regularity, she wants to know. The regularity with which A beats B can be discerned and described independently of adopting the intentional stance, but it stands in need of explanation. The only explanation—the right explanation—may be that A generates better "beliefs" about what B will do if ... than B generates about what A will do if ... In such a case, adopting the intentional stance is required for finding the explanation (see chapters 33 and 42 for other examples of causal links that are utterly inexplicable until you adopt the intentional stance).
So far so good, but these "decisions" and "choices" seem to be only sorta decisions and choices. They lack something, it seems, that genuine choices have: "could have done otherwise." But let's look more closely at a specific example, since appearances can be deceiving. It will help to add a third chess-playing program, C, to our tournament program, and let's suppose that C is better than A and B, beating them both almost all the time, and let's suppose that the first twelve moves in a pair of these games are exactly the same, and that C wins both games, beating both A and B, though by somewhat different routes after those first twelve moves. The experts huddle in retrospect and figure out that at move 12, the last common move, if either A or B had castled, C would likely have lost. Castling at move 12 was the key to victory, missed by both A and B.
The designer of program A shrugs and says, "Well, A could have castled," and the designer of B adds, "My program too; B could have castled." But the designer of A was right and the designer of B was wrong! How could this be? The tournament program T is deterministic, and if we run through the games again, in exactly the same state, neither A nor B castles. Isn't the designer of A kidding herself? Not necessarily. What are we trying to find out when we ask if A could have done otherwise? Looking at precisely the same case, again and again, is utterly uninformative, but looking at similar cases is in fact diagnostic. If we find that in many similar circumstances in other games, A does pursue the evaluation slightly farther, discovering the virtues of such moves and making them, then we support the designer's conviction that A could have castled then.
We might find, in the minimal case, that flipping a single bit in the (pseudo-)random-number generator would have resulted in A's castling. Suppose A's designer digs deep down into the actual execution of the program and shows that on this occasion A stopped "thinking" one pulse too early. (Every chess program, no matter how brilliant, has to truncate its searches arbitrarily at some point.) A considered castling, and had begun its analysis of that outcome, but since time was running out, it consulted its random-number generator, flipping a coin, in effect, and settled on the move it had so far identified as the best—and it wasn't castling. But if the pseudo-random number had been a 1 instead of a 0, A would have gone on considering for a little while longer, and castled instead. "Just flip one bit in the random number and A wins!" says the designer. We would say that in this case A's failure to castle was a fluke, bad luck with the random-number generator.
When we turn to the designer of B, no such story in support of the claim that B could have castled under the circumstances is forthcoming. It is true that B "knows" that castling is legal under these conditions, and perhaps actually briefly "considered" castling, but B was nowhere near choosing castling on this occasion. Castling was a deep move, the sort of move that is followed by "(!)" in the newspaper chess columns, and far beyond program B's limited analytical powers. So here we have an entirely deterministic world—program T—in which A could have castled but B could not have castled. The difference between A and B is real and explanatory, a difference in competence or ability. One way we could put it is apparently paradoxical:
A could have castled at time t but the universe couldn't have had a castling event at time t.

What could possibly license this way of describing the situation? Simply this: if we consider A divorced from its immediate environment—which includes the random-number generator— then whether A castles or not is undetermined. It depends on something that is strictly speaking outside A. Given the way the rest of the universe was arranged at t, castling was not possible for A, but that is "not A's fault." B, in contrast, could not have castled; it was not in B's nature to castle. To imagine B castling would require too many alterations of reality.
This is a useful discovery: a distinction between what A and B "could do" that does not depend on indeterminism. Even in a deterministic world we can see that A can do kinds of things that B cannot do, and this difference is part of the explanation of why A beats B. The fact that, because determinism is true in this world, A and B can only do what they actually do on the specific occasion (and would do it again and again if exactly the same circumstances were repeated) is simply not interesting, not relevant to the explanation we get of the perfectly objective and visible regularity: A beats B.
A chess-playing program is not a moral agent, and it is not morally responsible for the choices it makes—its world is entirely amoral, and violating one of the rules of chess is simply unthinkable for a chess program, and hence requires no penalties for violations. But as we have just seen, even in the simple deterministic world of computer chess we can make a real and important distinction between A and B. Sometimes when A does something stupid or clever we can say, "A could have done otherwise, but B could not have done otherwise." If you think that this must be a mistake "because neither A nor B could ever have done otherwise since the world is deterministic," it is you making the mistake.
A and B differ in chess competence, and "could have done otherwise" nicely captures an aspect of that difference, as we have just seen. What about moral competence? When people say of some human beings who act badly that "they could have done otherwise," and use this as their justification for not excusing them, while agreeing that other human beings in similar circumstances could not have done otherwise, they are not making a mistake either—and this is independent of whether or not determinism is true. They are pointing to a real difference in moral competence that does not depend on either indeterminism or determinism, and that can ground a difference in our response.
To see this more clearly, take the perspective of the programmer who designed program B. She wants to know if she has uncovered a weakness in B. Here is a game in which not castling cost B the victory; could B have castled then? If all it would take for that to happen was the flip of a single bit in the random-number generator, then perhaps no design improvements are called for. As often as not, in similar circumstances, B will castle, and perhaps that's as good as anyone could hope for. A program must always use random numbers now and then (as coin flips) to terminate search and get on with the game, and hence there will always be cases where thanks to the flip, the search stops just this side of discovery. And notice that the situation is not improved if we give program B (or program A) a quantum random-number generator, say, a Geiger counter that spews out bits based on the undetermined trajectories of subatomic particles. Then consider what we would say about B in the case where B doesn't castle because of a single I where a 0 might have been. If the quantum number generator yields a 0, B castles; if it yields a 1, B doesn't castle. "B could have castled," says an observer when the 1 comes up. Yes, but B is no freer for all that. In a series of games in which this sort of opportunity comes up, half the time B will castle and half the time B won't, whether B's random-number generator is "genuine" or "pseudo." The philosopher David Wiggins (1973, p. 54) once wrote of the "cosmic unfairness" of determinism, but what our intuition pump about the computer chess tournament shows is the equal "cosmic unfairness" of indeterminism. B is "at the mercy of" its random-number generator or its pseudo-random-number generator. (So, of course, is A; so are we all.) There is no reason to prefer the genuinely random-number generator—unless of course you plan to play chess against an omniscient God who can see into your pseudo-random-number generator and plan accordingly!
So we're still looking for a reason to want indeterminism to be true. Perhaps we can have all the free will worth wanting without indeterminism playing any role. Here's another candidate reason:
I can't change the past, but if indeterminism is true, I can change the future!

Nope. Change the future from what to what? From what it was going to be to what it is going to be? You can no more change the future than you can change the past. The concept is incoherent. So:
If determinism is true, I can't change the future, and if determinism is false, I can't change the future. So it follows that I can't change the future.

Why does it seem that we want to change the future? Because we want to be able to foresee disasters and do something so those disasters don't happen. And we can do this, independently of indeterminism. If somebody throws a brick at you, and you see it and duck, you can avoid being hit by the brick. Good for you. Was the collision going to happen? In one sense yes, since the brick was clearly on a trajectory right for your head, but since you saw it (since you were caused to see it by the light bouncing off of it into your eyes, where your brain calculated the risk and was caused to take action), you avoided it. Of course, if you had wanted to avoid avoiding it (if some reason had occurred to you for why you might actually do better by letting it hit you), you could do just that. Some observer might not be able to tell, right up to the last moment, whether you were going to take the hit or not. And if he was betting on you to duck, he'd lose. We're back at our reason for wanting to be unpredictable, which does not require indeterminism.
What does this intuition pump accomplish? It takes the familiar phrase "could have done otherwise" and shows that contrary to widespread but ill-examined opinion, a valuable version of it does not depend on indeterminism. If there is a sense of "could have done otherwise" that is both incompatible with determinism and morally important—not merely a metaphysical curiosity, you might say—this has yet to be established, and the burden of proof lies with those who think so. One more "obvious" point exposed as not so obvious after all.











71. Ultimate Responsibility
So far, we've been looking at trivial choices, not involving moral responsibility at all: rock, paper, scissors; chess moves; and brick-ducking. Perhaps it is when we are specifically looking at our attempts to be moral agents, not just intentional systems like chess-playing computers and stotting gazelles, that indeterminism is truly desirable. Many thinkers have thought so. For them, these exercises are just so much distraction. Here is a nice clear version of what some thinkers take to be the decisive argument. It is due in this form to the philosopher Galen Strawson (2010):

You do what you do, in any given situation, because of the way you are.
So in order to be ultimately responsible for what you do, you have to be ultimately responsible for the way you are—at least in certain crucial mental respects.
But you cannot be ultimately responsible for the way you are in any respect at all.
So you cannot be ultimately responsible for what you do.

The first premise is undeniable: "the way you are" is meant to include your total state at the time, however you got into it. Whatever state it is, your action flows from it non-miraculously. The second premise observes that you couldn't be "ultimately" responsible for what you do unless you were "ultimately" responsible for getting yourself into that state—at least in some regards. But according to step (3) this is impossible.
So step (4), the conclusion, does seem to follow logically. Several thinkers have found this argument decisive and important. But is it really? Let's look more closely at step (3). Why can't you be ultimately responsible for some respects, at least, of the way you are? In everyday life we make exactly this distinction, and it matters morally. Suppose you design and build a robot and send it out into the world unattended and unsupervised and knowing full well the sorts of activities it might engage in, and suppose it seriously injures somebody. Aren't you responsible for this, at least in some respects? Most people would say so. You made it; you should have foreseen the dangers—indeed you did foresee some of the dangers—and now you are to blame, at least in part, for the damage done. Few would have any sympathy for you if you insisted that you weren't responsible at all for the harm done by your robot.
Now consider a slightly different case: you design and build a person (yourself at a later time) and send yourself out into the risky world knowing full well the possible dangers you would encounter. You get yourself drunk in a bar and then get in your car and drive off. Aren't you responsible, at least in part, for the "way you were" when you crashed into a school bus? Common sense says of course. (The bartender, or your compliant host, may share the responsibility.) But how could this be, in the face of Strawson's knockdown argument? Well, remember that Strawson says you can't be absolutely responsible for the way you are. Okay, but so what? Who would think it was important to be absolutely responsible? That is indeed a state that is utterly impossible, even if indeterminism is true! (So much for the idea that this argument gives us a reason for hoping for indeterminism.) Here is what Strawson (2010) says:
To be absolutely responsible for what one does, one would have to be causa sui, the cause of oneself, and this is impossible (it certainly wouldn't be more possible if we had immaterial souls rather than being wholly material).

Absolute responsibility is a red herring, a blessing nobody should hanker for. Strawson (2003) thinks otherwise, and criticizes me for neglecting it:
He doesn't establish the kind of absolute free will and moral responsibility that most people want to believe in and do believe in. That can't be done, and he knows it.

He's exactly right: I don't establish the kind of free will most people want to believe in, and I know it. But I think they are wrong to want to believe in it, and wrong to believe in it if they do. The burden falls on Strawson and others to show why we ought to care about ultimate responsibility—or the determinism/indeterminism issue—in our lives. They can define a variety of free will that is incompatible with determinism, and show that lots of folks think that's important, but they also need to show that those folks aren't deceiving themselves. Why should anybody care? (Note my rhetorical question. I'm leading with my chin. I would be happy for Strawson or somebody else to step up to the plate and try to answer it, but so far there are no volunteers.)
Before leaving Strawson's argument, let me ask if you have noticed its uncanny resemblance to an earlier argument. I will revise the earlier argument into a form a little closer to Strawson's to bring out the similarities.

A mammal is a mammal, in any given context, because of the way it is.
In order to be a mammal, you have to become the way you are by having a mammal for a mother.
But this must be true of your mother, and her mother, and so forth ad infinitum, which is impossible.
So you cannot be a mammal because mammals are utterly impossible.

You should always be leery of any such "ancestral" argument. It is almost certain to be a disguised instance of the ancient fallacy known as the sorites (or "heap") argument:

A single grain of wheat is not a heap.
Adding a grain to a single grain does not make a heap.
You can't make a non-heap into a heap by adding a single grain.
Therefore there are no such things as heaps!

Philosophers have written about the sorites paradox and the problems of the vague boundaries of terms (which is what the paradox obviously depends on) for thousands of years, and there is still no resolution of just how to diagnose and avoid the fallacy. (See the Stanford Encyclopedia of Philosophy online for an excellent and up-to-the-minute survey.) There are even a few brave philosophers who declare the sorites to be valid, and they try to live with the "fact" that there are no bald men and no non-bald men either. Tough position to defend! But as chapter 43 showed, Darwin taught us how to turn our backs on the sorites; we don't need to find "principled" dividing lines between categories ranked in ancestral order.
To my knowledge I am the first to point out the resemblance of Strawson's argument—and the other arguments of this ilk in the free-will literature—to a sorites paradox, but there it is. I think it is just as obvious that people can gradually become morally responsible during their passage from infancy to adulthood as it is that lineages of reptiles and then therapsids can gradually become a lineage of mammals over the eons. You don't have to be an absolute mammal to be a mammal, and you don't have to be absolutely responsible to be responsible, or have absolute free will to have a kind of free will worth wanting. In fact, since absolute free will would be miraculous, there really needs to be a powerful argument to show why anybody would covet such a thing. Do they want to be God? Too bad, they're out of luck, but the next best thing is quite a good thing to be.











72. Sphexishness
Doug Hofstadter (1982) coined the term "sphexishness" for a familiar sort of rigid, robotic mindlessness that is often mistaken for great cleverness. The defining example of sphexishness, and the source of the term, is a wasp with a curious behavior. Doug and I had independently been struck by a passage in a popular science book, The Machinery of the Brain, written by Dean Wooldridge (1963), who described the Sphex wasp thus:
When the time comes for egg-laying, the wasp Sphex builds a burrow for the purpose and seeks out a cricket which she stings in such a way as to paralyze but not kill it. She drags the cricket into the burrow, lays her eggs alongside, closes the burrow, then flies away, never to return. In due course, the eggs hatch and the wasp grubs feed off the paralyzed cricket, which has not decayed, having been kept in the wasp equivalent of deep freeze. To the human mind, such an elaborately organized and seemingly purposeful routine conveys a convincing flavor of logic and thoughtfulness—until more details are examined. For example, the wasp's routine is to bring the paralyzed cricket to the burrow, leave it on the threshold, go inside to see that all is well, emerge, and then drag the cricket in. If the cricket is moved a few inches away while the wasp is inside making her preliminary inspection, the wasp, on emerging from the burrow, will bring the cricket back to the threshold, but not inside, and will then repeat the preparatory procedure of entering the burrow to see that everything is all right. If again the cricket is removed a few inches while the wasp is inside, once again she will move the cricket up to the threshold and re-enter the burrow for a final check. The wasp never thinks of pulling the cricket straight in. On one occasion this procedure was repeated forty times, always with the same result. [p. 82]

A perfect example, it seems, of the imperfect, uncomprehending competence that we unmask when we expose the superficial, pseudo-understanding of a second-rate computer program. We have recently learned, however, that Wooldridge gave us—as popular science writers so often do—an oversimplified sketch of the phenomenon. The psychologist Lars Chittka, in a note to me, quoted from the work of Jean-Henri Fabre (in 1879), which had apparently been the source for Wooldridge, who, if he had read on in Fabre, would have found that in fact only some Sphex wasps are sphexish. In fact, Fabre was eager to make the point. If at first blush you thought Sphex was clever, and at second blush you thought Sphex was stupid, try third blush, and find that some Sphex wasps are not so sphexish after all. Chittka sent me the German translation of Fabre (I still haven't located the French), which includes the following sentence: "Nach zwei oder drei Malen, ... packt ihre Fuehler mit den Kieferzangen und schleift sie in die Hoehle. Wer war nun der Dummkopf?" (After two or three times, ... she grabbed her [the prey's] antennae with her pincers and slid it into the hole. Now who's the dummy?)
So the adjective sphexish is a bit of a misnomer, but since it has caught on, the Sphex wasps will just have to endure the insult. They are lucky, in a way, to be in the limelight of nonspecialist attention, which probably has a nonnegligible fitness enhancement. (Which habitat would you vote to protect, Sphex habitat or Humdrumbeetle habitat?) Sphex wasps may not be "charismatic" like elephants and tigers and wolves, but they are rather well known—for their problematic sphexishness.
Sphexishness is an important property not so much because so many whole, simple animals—insects, worms, fish—exhibit it (though they do, in varying degrees), but because it gives us a term for the limited, robotic, myopic, competences out of which we can build fancier, more versatile, comprehending minds. The building  blocks in any mind model had better be sphexish! Or, as I noted earlier, the building blocks should be sorta minds, pale shadows of our minds. Sphexishness is also useful to distinguish morally competent minds from morally incompetent minds. To the extent that a human being is sphexish, because of a brain tumor or brain injury or serious imbalance of neuromodulators or mental illness or sheer ignorance or immaturity, that human being could not have done otherwise in the relevant sense.
The persistent biologist interfering with the Sphex wasp in order to expose its sphexishness is the very model of the manipulative agent we rightly dread. Many philosophers' thought experiments about free will depend on invoking just such a puppeteer, or a nefarious neurosurgeon who has secretly wired someone up to do his bidding. Presumably the moral of these scary tales is that even if there is no actual puppeteer, the fact that our behavior is caused by various features of our environments, as processed through our perceptual systems and brains, shows that there might as well be a puppeteer. (The cover illustration of Sam Harris's little book Free Will (2012) is a set of puppeteer control strings.) But this invited conclusion is a clear non sequitur. When the "control" by the environment runs through our well-working perceptual systems and our undeluded brains, it is nothing to dread; in fact, nothing is more desirable than our being caused by the things and events around us to generate true beliefs about them that we can then use in modulating our behavior to our advantage. Photons bouncing off air holes in the tidal flats into my eyes are apt to cause me to grab my clam rake and basket and start digging. If this is a case of being controlled by my environment, I'm all for it. And, like most people, I do not feel threatened or manipulated when my friends offer me sumptuous meals, knowing full well that I will be unable to resist the temptation to eat them.
Another thing you will notice about the puppeteer and neurosurgeon examples in the literature on free will is that the intervention is always—always—secret. Why should this be? Because it is only when we are unwittingly being caused to act or choose by some other, secret agent that the intuitions flood in to the effect that our will is not free. The reason for this is not far to seek, and harks back to the insight that inaugurated game theory: when an agent knows about the attempt at manipulation by another agent, it thereupon seeks countermeasures, and at the very least adjusts its behavior to better cope with this discovery. The competitive interactions between the two agents involve multiple levels of feedback, and hence diminishing "control" by the would-be manipulator. And if the intervention is not only not secret, but requested by the "puppet," the tables are turned completely.
This we can demonstrate by simply altering the standard cases slightly, turning the knobs on the intuition pumps. The philosopher Harry Frankfurt (1969) invented an intuition pump in which a person is being monitored by secretly implanted neurostimulation gadgetry that is controlled by a neurosurgeon who makes sure that that person decides the way he wants that person to decide. If you, as that person, are faced with a dilemma and choose option A rather than B, then if this is what the neurosurgeon wanted you to choose, he does nothing; if instead his gauges show that you are about to choose option B, he hits the button and prevents you from choosing option B, and you choose A after all. You don't feel a thing. Did you choose freely in either case? Over the years philosophers have written hundreds of pages about "Frankfurt cases" with many variations, but one turning of the knobs that makes a huge difference has never to my knowledge been explored. Here it is:
Wanting to tame your profligate love of rich desserts, you have contracted with the good doctor to implant this device, and you are paying him a handsome fee to monitor your every meal, providing a safety net that keeps you from ordering the hot fudge sundaes and slices of cheese cake. Both of you hope he never has to deploy his button, and soon you almost forget that he or his assistant is electronically at your elbow; you are cured and the gadget has recorded an unbroken string of hundreds of A choices ("No thanks, just a cup of black coffee"). If those aren't examples of you choosing responsibly and freely, why not? Wasn't it wise of you to get help with your somewhat fragile willpower?











73. The Boys from Brazil: Another Boom Crutch
In 2004, the psychologists Joshua Greene and Jonathan Cohen co-authored "For the Law, Neuroscience Changes Everything and Nothing," which was published in the prestigious journal Philosophical Transactions of the Royal Society. This influential article conjures up a revolution in the law, triggered by scientific discoveries.
[T]he law says that it presupposes nothing more than a metaphysically modest notion of free will that is perfectly compatible with determinism. However, we argue that the law's intuitive support is ultimately grounded in a metaphysically overambitious, libertarian [indeterministic] notion of free will that is threatened by determinism and, more pointedly, by forthcoming cognitive neuroscience. [p. 1776]

The case they make is subtle; it has to be, because they grant that there are a wealth of arguments in support of compatibilism (the view defended by me here), but they want to show that actually "we are all of two minds" about free will. They offer a thought experiment designed to reveal the dependence of everyday commonsense thinking on indeterminism. Note that this is pretty much what I've just been calling for: show us, please, both that and why we should care about indeterminism. Their thought experiment was inspired by the film The Boys from Brazil, about Nazi scientists who raise Hitler clones (thanks to some salvaged DNA). Here it is:
Let us suppose, then, that a group of scientists has managed to create an individual—call him Mr Puppet—who, by design, engages in some criminal behavior: say, a murder done during a drug deal gone bad. [p. 1780]

Here is what they say about their thought experiment:
Yes, he is as rational as other criminals, and, yes, it was his desires and beliefs that produced his actions. But those beliefs and desires were rigged by external forces, and that is why, intuitively, he deserves our pity more than our moral condemnation. ... what is the real difference between us and Mr Puppet? One obvious difference is that Mr Puppet is the victim of a diabolical plot whereas most people, we presume, are not. But does this matter? The thought that Mr Puppet is not fully responsible depends on the idea that his actions were externally determined. ... But the fact that these forces are connected to the desires and intentions of evil scientists is irrelevant, is it not? What matters is only that these forces are beyond Mr Puppet's control, that they're not really his. [p. 1780]

What do you think? Is this a good intuition pump for its purpose, or not? Amusingly, the authors note that this is a question worth considering: "Daniel Dennett might object that the story of Mr Puppet is just a misleading 'intuition pump.' " Indeed I do. I say it's a boom crutch. But they shrug off this hunch and carry on: "It seems to us that the more one knows about Mr Puppet and his life the less inclined one is to see him as truly responsible for his actions and consider our punishing him as a worthy end in itself."
So let's take a closer look, turning the knobs on this intuition pump to see what is actually doing the work. I propose to adjust four knobs. First, let's get rid of the diabolical plot—which the authors themselves insist does not matter. I am surprised that they are so blithe about introducing a "nefarious neurosurgeon" and supposing that this is obviously innocent, since doubts have been expressed for years about such a move, but we can test their conviction by replacing "a group of scientists" with "an indifferent environment":
BEFORE: Let us suppose, then, that a group of scientists has managed to create an individual—call him Mr Puppet—who, by design, engages in some criminal behavior: say, a murder done during a drug deal gone bad.
AFTER: Let us suppose, then, that an indifferent environment has managed to create an individual—call him Mr Puppet—who, by design, engages in some criminal behavior: say, a murder done during a drug deal gone bad.

Second knob: With the plotters gone, we have to replace "by design" with "with high probability":
BEFORE: Let us suppose, then, that an indifferent environment has managed to create an individual—call him Mr Puppet—who, by design, engages in some criminal behavior: say, a murder done during a drug deal gone bad.
AFTER: Let us suppose, then, that an indifferent environment has managed to create an individual—call him Mr Puppet—who, with high probability, engages in some criminal behavior: say, a murder done during a drug deal gone bad.

Third knob: I want to change the motivation of the crime—still a murder, but in a rather different setting. (That shouldn't matter, should it?)
BEFORE: Let us suppose, then, that an indifferent environment has managed to create an individual—call him Mr Puppet—who, with high probability, engages in some criminal behavior: say, a murder done during a drug deal gone bad.
AFTER: Let us suppose, then, that an indifferent environment has managed to create an individual—call him Mr Puppet—who, with high probability, engages in some criminal behavior: say, a murder done to cover up an embezzlement.

Fourth knob: Let's change the culprit's name. After all, it's just a name.
BEFORE: Let us suppose, then, that an indifferent environment has managed to create an individual—call him Mr Puppet—who, with high probability, engages in some criminal behavior: say, a murder done to cover up an embezzlement.
AFTER: Let us suppose, then, that an indifferent environment has managed to create an individual—call him Captain Autonomy—who, with high probability, engages in some criminal behavior: say, a murder done to cover up an embezzlement.

What intuitions does this pump for you now? The same intuitions? Are you more inclined to pity than condemnation? Maybe it would help if we fleshed out the details a little further. Here is my friendly amendment to Greene and Cohen's intuition pump:
Captain Autonomy majored in economics at Harvard, and after graduation he went to work at Lehman Brothers, where all around him people were cheating and making fortunes in the process. He fell in love with a gold-digging heartbreaker, who threatened to leave him if he didn't get rich quick. He saw his chance, a virtually undetectable embezzlement, almost surely an invisible crime, and he knowingly took the risk, but, alas, against all the odds a witness arose, who made the mistake of standing too close to the railing on the penthouse garden, ... a quick "stumble"—Oops!—and the witness was sent to his death on the avenue below. Suspicions were aroused, and soon enough Captain Autonomy was under arrest.

Are you still inclined to think that because his actions were externally determined he is "not truly responsible"? Even if you are still tempted to see Captain Autonomy as a victim of his (posh) environment, I think you have to agree that the temptation is much diminished, and might even be a hangover of sorts from the earlier telling. (Or maybe I've just exploited your emotions in the opposite direction, counting on your bloodthirsty desire to punish the greedy Wall Street types for their role in our economic malaise.) I'm not claiming that my variations prove that people are or can be responsible in spite of being determined; I am just claiming that this particular intuition pump is not at all to be trusted, since the (available, permissible) knob settings are interfering so much with our judgments. It may not have been designed to blow smoke, but it certainly manages to hinder clear thinking.











Summary
People care deeply about having free will, but they also seem to have misguided ideas about what free will is or could be (like their misguided ideas about color and consciousness). Our decisions are not little miracles in the brain that violate the physics and chemistry that account for the rest of our bodies' processes, even if many folk think this must be what happens if our decisions are to be truly free. We can't conclude from this, however, that then we don't have free will, because free will in this bonkers sense is not the only concept of free will. The law, according with common sense, contrasts signing a contract "of your own free will" with signing a contract under duress or under the influence of hallucination or other mental derangement. Here is a perfectly familiar sense of free will, a distinction presupposed by many of the practices and attitudes that comprise our manifest image, that has no demonstrated dependence on the bonkers sense.
For hundreds of years there have been philosophers who have insisted that this sense of free will is the important sense, the one we should care about, and it is compatible with determinism, with materialism, with physics and chemistry reigning unchallenged. The intuition pumps and other thinking tools in this part are designed to support and advance the understanding of this prospect, compatibilism. It has had many versions over the years, and is probably the consensus not only among philosophers but also among judges, lawyers, and others who have to make distinctions about who is responsible for what and who is excused because they didn't have free will when they acted. Some scientists are now challenging this consensus, and they may of course be right to do so. Let's take a hard look at the arguments.
Maybe science is teaching us something radical, even revolutionary: that nobody is ever responsible for anything they do, and there is no sound basis for distinguishing some acts as praiseworthy and others as blameworthy. But such a revolutionary conclusion needs a lot more conscientious attention to detail than it has so far received from the scientists declaring it. The nefarious neurosurgeon disabled her patient with nothing but a false idea; a mistaken view promulgated by influential scientists could rob people of a legitimate and life-enhancing variety of free will. Caution is called for, on all sides.
Compatibilism, for all its popularity among philosophers, has always provoked suspicion. Immanuel Kant famously called it a "wretched subterfuge," and writers today often express doubts about the sincerity of those of us who maintain it. This is as it should be, actually. Science teaches us to be especially on guard against wishful thinking, and many of the rules of scientific investigation are specifically designed to prevent us from being taken in by our hopes when we think we are being convinced by the evidence. Imagine that some astronomers announce that a giant asteroid is going to strike our planet in ten years, obliterating all life, and then another group of astronomers claim that their reanalysis of the data shows that we can all breathe easily; the asteroid will miss Earth by a narrow margin. Good news, but how do we know the second group of astronomers aren't deceiving themselves—or just deceiving us with a benign lie? Check and recheck their calculations; mount an independent attempt at replication; don't just gratefully accept their conclusion because it doesn't contain any obvious errors and it appeals to you. But also never forget that it is possible that they are right. Don't make the opposite mistake of discrediting—on "general principles"—something that seems "too good to be true."
Is compatibilism too good to be true? I think not; I think it is true, and we can soundly and roundly dismiss the alarmists, at the same time reforming and revising our understanding of what underwrites our concept of moral responsibility. But that is a task for the future, and it should be the work of many hands. So far as I can see, it is both the most difficult and the most important philosophical problem confronting us today. The stakes are high, the issues thorny, and emotions tend to cloud our judgment. We will need all our thinking tools and more, which we will have to make as we go along.














IX. WHAT IS IT LIKE TO BE A PHILOSOPHER?

 

When we are too close to something, it is hard to see what it is. In recent years I have spent less time interacting with other philosophers than I used to, and more time interacting with scientists and other kinds of thinkers. I am still a philosopher (no matter what some philosophers say!), and I relish the task of explaining to non-philosophers why philosophy is worth doing. To those utterly outside philosophy, it often looks preposterous, the very paradigm of useless cleverness. They are missing something (recall Sturgeon's Law: 90 percent of everything is crap; they are missing the 10 percent). Having spent fifty years inside philosophy, I am familiar with it; now that I spend so much time far away from philosophy, I can also see its strange side vividly. Some of my scientific friends and colleagues confess that they cannot for the life of them see why I don't abandon ship and join them. The short answer is that I have managed, by straddling the boundaries, to have the best of both worlds. By working with scientists I get a rich diet of fascinating and problematic facts to think about, but by staying a philosopher without a lab or a research grant, I get to think about all the theories and experiments and never have to do the dishes. To quote one of my favorite Gershwin songs, it's "nice work if you can get it."
It seems to me that these days, happily, scientists are paying more respectful attention to philosophers than they used to, especially in my main area, the mind, where the phenomena that cognitive scientists study are almost exactly the phenomena that philosophers have been thinking about for centuries: perception, memory, meaning, volition, and consciousness. And philosophers—some of them—have earned the attention by informing themselves of the relevant science and coming up with useful proposals for clarifying and advancing the scientific research and composing better ways of explaining the results to the nonscientific world. There are still large failures of communication when the tribes try to interact, however, and I am going to address a few of the differences with the goal of achieving more mutual understanding in the future.











74. A Faustian Bargain
For several years, I have been posing the following choice for my fellow philosophers: If Mephistopheles offered you the following two options, which would you choose?

(A) You solve the major philosophical problem of your choice so conclusively that there is nothing left to say (thanks to you, part of the field closes down forever, and you get a footnote in history).
(B) You write a book of such tantalizing perplexity and controversy that it stays on the required reading list for centuries to come.

Some philosophers reluctantly admit that they would have to go for option (B). If they had to choose, they would rather be read than right. Like composers, poets, novelists, and other creators in the arts, they tend to want their work to be experienced, over and over, by millions (billions, if possible!). But they are also tugged in the direction of the scientists' quest. After all, philosophers are supposed to be trying to get at the truth.
When I present the same Faustian bargain to scientists, they tend to opt for (A) without any hesitation—it's a no-brainer for them. And then they shake their heads in wonder (or disgust?) when they learn that this is a hard choice for many philosophers, some of whom opt, somewhat sheepishly, for (B). But this reaction by scientists misses the important point made by Nicholas Humphrey (1987) (see chapter 48):

In Two Cultures, C. P. Snow extolled the great discoveries of science as "scientific Shakespeare," but in one way he was fundamentally mistaken. Shakespeare's plays were Shakespeare's plays and no one else's; scientific discoveries, by contrast, belong—ultimately—to no one in particular.

If Shakespeare hadn't existed, nobody else would have written Hamlet or Romeo and Juliet or King Lear. If Van Gogh hadn't existed, nobody else would have painted Starry Night. This may be a slight exaggeration, but there's something to it. On the one hand, there is an individuality to the contributions of great artists that seems to be not just rare in science, but positively beside the point. The famous priority disputes in science, and the races for one Nobel Prize clincher or another, are ferocious precisely because somebody else could make exactly the contribution you were striving to make—and you won't get points for style if you come in second. These contests have no parallel in the arts, where a different set of goals reigns.
Some scientists aspire to reach large readerships, and to delight the readers they catch, and the best write works of surpassing literary value. Darwin's books come to mind. But the goal of getting it right, of persuading the readers of a discovered truth, still comes first, as we can tell at a glance by comparing Darwin's Voyage of the Beagle with Melville's Moby-Dick. One can learn a great deal about whales and whaling from Moby-Dick, but Melville didn't write it to be an artful and persuasive compendium of whaling facts.
Bearing in mind the difference between the goals of science and the goals of art, then, here is a question for scientists that appropriately parallels the teaser I ask my philosophical colleagues: If Mephistopheles offered you the following two options, which would you choose?

You win the race (and the accompanying Nobel Prize) for pinning down a discovery that becomes the basis for a huge expansion of scientific knowledge but that, in retrospect, epitomized Humphrey's epithet, belonging to no one in particular. (Crick and Watson come to mind; there is scant doubt that if they hadn't won the race when they did, Linus Pauling or somebody else would have done so soon.)
You propose a theory so original, so utterly unimagined before your work, that your surname enters the language, but your theory turns out to be mostly wrong, though it continues to generate years—even centuries—of valuable controversy. (I think of Cartesian dualism about the mind, Lamarckian theories of evolution, Skinnerian behaviorism, and Freudian views of everything from infant sexuality and neurosis to art, music, and literature.)

A better, though less well-known example of option (2) might be Descartes's ambitious work on physics, which was so influential and so brilliantly wrong that it was a major provocation for Isaac Newton, whose world-changing Philosophiae Naturalis Principia Mathematica (written in 1687) deliberately echoed the title of Descartes's Principia Philosophiae (of 1644) to make it clear what worldview he intended to replace. And then there is Chomskian linguistics. It certainly passes the originality test. Like the victory of the America in the race that gave the America's Cup its name, there was no second place anywhere in sight when Chomsky burst on the scene. In subsequent years the original theoretical seed—the "transformational" theory of Chomsky's Syntactic Structures (published in 1957)—has been largely abandoned, superseded by descendant theories in several species as different from their common ancestor as ostriches, hummingbirds, and albatrosses are from the dinosaurs from whom they evolved. Was Chomsky fruitfully wrong in 1957, or did he rather (ding!) discover a great truth? "Yes" answers the question pretty well.
We honor scientists who are wrong in useful ways—recall Wolfgang Pauli's insult about the theorist who "isn't even wrong." But forced to choose, would you trade being first and right for being original and provocative? Not quite so easy to decide, is it?











75. Philosophy as Naïve Auto-anthropology
Patrick Hayes, the artificial intelligence researcher, once set out on a project to axiomatize the naïve (or folk) physics of liquids. The idea was to provide a robot with the propositions it would need to use as its core beliefs if it was going to interact with people (who rely on folk physics every day). It proved to be more challenging than he had anticipated, and he wrote an interesting paper about the project, "The Naïve Physics Manifesto" (Hayes, 1978). In the naïve physics of liquids, everything that strikes naïve folks as counterintuitive is, of course, ruled out: siphons are "impossible" and so are pipettes, but you can mop up liquid with a fluffy towel, and pull water out of a well with a suction pump. A robot equipped with such a store of "knowledge" would be as surprised by a siphon as most of us were when first we saw one in action. Hayes's project was what I would call sophisticated naïve physics, because he was under no illusions; he knew the theory he was trying to axiomatize was false, however useful in daily life. This was an exercise in what might be called axiomatic anthropology: you treat what the folks say—and agree about—as your axioms or theorems, and try to render the data set consistent, resolving any contradictions encountered. And, of course, he didn't bother rounding up any actual informants; he figured that he knew the naïve physics of liquids as well as any normal person did, so he used himself as his sole informant: axiomatic auto-anthropology.*
Now compare Hayes's project with the philosophical projects in analytic metaphysics, which often strike me as naïve naïve auto-anthropology since the participants in this research seem to be convinced that their program actually gets at something true, not just something believed true by a particular subclass of human beings (Anglophone philosophers of the analytic metaphysics persuasion). Otherwise, the programs seem identical: you gather your shared intuitions, test and provoke them by engaging in mutual intuition-pumping, and then try to massage the resulting data set into a consistent "theory," based on "received" principles that count, ideally, as axioms. I've asked a number of analytic metaphysicians whether they can distinguish their enterprise from naïve naïve auto-anthropology of their clan, and have not yet received any compelling answers.
The alternative is sophisticated naïve anthropology (both auto- and hetero-)—the anthropology that reserves judgment about whether any of the theorems adduced by the exercise deserve to be trusted—and this is a feasible and frequently valuable project. I propose that this is the enterprise to which analytic metaphysicians should turn, since it requires rather minimal adjustments to their methods and only one major revision of their raison d'être: they must roll back their pretensions and acknowledge that their research is best seen as a preparatory reconnaissance of the terrain of the manifest image, suspending both belief and disbelief the way anthropologists do when studying an exotic culture: let's pretend for the nonce that the natives are right, and see what falls out. Since at least a large part of philosophy's task, in my vision of the discipline, consists in negotiating the traffic back and forth between the manifest and scientific images, it is a good idea for philosophers to analyze what they are up against in the way of folk assumptions before launching into their theory-building and theory-criticizing.
One of the hallmarks of sophisticated naïve anthropology is its openness to counterintuitive discoveries. As long as you're doing naïve anthropology, counterintuitiveness (to the natives) counts against your reconstruction; when you shift gears and begin asking which aspects of the naïve "theory" are true, counterintuitiveness loses its force as an objection and even becomes, on occasion, a sign of significant progress. In science in general, counterintuitive results are prized, after all.
One of the weaknesses of auto-anthropology is that one's own intuitions are apt to be distorted by one's theoretical predilections. Linguists have known for a long time that they get so wrapped up in their theories they are no longer reliable sources of linguistic intuition. Can you really say in English, "The boy the man the woman kissed punched ran away," or is my theory of clause embedding tricking my "ear"? Their raw, untutored intuitions have been sullied by too much theory, so they recognize that they must go out and ask nonlinguists for their linguistic intuitions. Philosophers have recently begun to appreciate this point, in the new enthusiasm for so-called experimental philosophy (see Knobe and Nichols, 2008). It is early days still, and some of the pioneer efforts are unimpressive, but at least philosophers are getting used to the idea that they can no longer declare various propositions to be obviously true on the sole grounds that they seem smashingly obvious to them. (In a similar vein, Hayes might have surprised himself about the chief tenets of folk physics if he had gone to the trouble of interviewing a random sample of folk instead of just treating himself as exemplary.)
So here is a project, a particular sort of sophisticated naïve anthropology, that philosophers should seriously consider undertaking as a survey of the terrain of the commonsense or manifest image of the world before launching into their theories of knowledge, justice, beauty, truth, goodness, time, causation, and so on, to make sure they actually aim their analyses and arguments at targets that are relevant to the rest of the world, both lay concerns and scientific concerns. Such a systematic inquiry would yield something like a catalogue of the unreformed conceptual terrain that sets the problems for the theorist, the metaphysics of the manifest image, if you like. This is where we philosophers have to start in our attempts to negotiate back and forth between the latest innovations in the scientific image, and it wouldn't hurt to have a careful map of this folk terrain instead of just eyeballing it. This is the other half, one might say, of the reform that turned philosophy of science from an armchair fantasy field into a serious partnership with actual science, when philosophers of science decided that they really had to know a lot of current science from the inside. Once we think about our philosophical tasks with this image in mind, we can see that a great deal of the informal trudging around, backing and filling, counterexample-mongering and intuition-busting that fills the pages of philosophy journals is—at best—an attempt to organize a mutually acceptable consensus about this territory.











76. Higher-Order Truths of Chmess
Consider this chess puzzle.* White to checkmate in two.

It appeared recently in the Boston Globe, and what startled me was that I had thought it had been proved that you can't checkmate with a lone knight (and a king, of course). I was wrong; as David Misialowski pointed out to me in a recent e-mail, it has been proved that you cannot checkmate your opponent's king when only his king and your king and knight are on the board. The fact that the proposition you can never checkmate with a lone knight and king is not a truth of chess is a higher-order truth of chess.
Philosophy is traditionally an a priori discipline, like mathematics, or at least it has an a priori methodology at its core, and this fact cuts two ways. On the one hand, it excuses philosophers from spending tedious hours in the lab or out in the field, and from learning data-gathering techniques, statistical methods, geography, history, foreign languages, empirical science, and so on, so they have plenty of time for honing their philosophical skills. On the other hand, as is often noted, philosophy can be created out of just about anything, and this is not always a blessing. For those of you younger readers who are thinking of undertaking a career in the field, and I hope some of you are, this chapter is a warning that the very freedom and abstractness of philosophy can be weaknesses. This chapter is also a travel guide for outsiders about some of the folkways and pitfalls of philosophy.
Consider, as a paradigm of a priori truths, the truths of chess. It is an empirical fact that people play chess, and there are mountains of other empirical facts about chess, about how people have been playing it for centuries, how they often use handsomely carved pieces on inlaid boards, and so forth. No knowledge of these empirical facts plays an indispensable role in the activity of working out the a priori truths of chess, which also exist in abundance. All you need to know are the rules of the game. There are exactly twenty legal opening moves (sixteen pawn moves and four knight moves); a king and a lone bishop cannot achieve checkmate against a lone king, and neither can a king and a lone knight, and so forth. Working out these a priori truths about chess is not always easy. Proving just what is and is not possible within the rules of chess is an intricate task, and mistakes can be made. For instance, a few years ago, a computer chess program discovered a mating net—a guaranteed or forced win—consisting of more than two hundred moves without a capture. This disproved a long-standing "theorem" of chess and has forced a change in the rules of the game. It used to be that fifty moves without a capture by either side constituted a draw (stalemate), but since this lengthy mating net is unbreakable, and leads to a win, it is unreasonable to maintain the fifty-move stalemate. (Before computers began playing chess, nobody imagined that there could be a guaranteed win anywhere near this length.) All this can be pretty interesting, and many highly intelligent people have devoted their minds to investigating this system of a priori truths of chess.*
Some philosophical research projects—or problematics, to speak with the more literary types—are rather like working out the truths of chess. A set of mutually agreed-upon rules are presupposed—and seldom discussed—and the implications of those rules are worked out, articulated, debated, refined. So far, so good. Chess is a deep and important human artifact, about which much of value has been written. But some philosophical research projects are more like working out the truths of chmess. Chmess is just like chess except that the king can move two squares in any direction, not one. I just invented it—though no doubt others have explored it in depth to see if it is worth playing. Probably it isn't. It probably has other names. I didn't bother investigating these questions because although they have true answers, they just aren't worth my time and energy to discover. Or so I think. There are just as many a priori truths of chmess as there are of chess (an infinity), and they are just as hard to discover. And that means that if people actually did get involved in investigating the truths of chmess, they would make mistakes, which would need to be corrected, and this opens up a whole new field of a priori investigation, the higher-order truths of chmess, such as the following:

Jones's (1989) proof that p is a truth of chmess is flawed: he overlooks the following possibility. ...
Smith's (2002) claim that Jones's (1989) proof is flawed presupposes the truth of Brown's lemma (1975), which has recently been challenged by Garfinkle (2002). ...

Now none of this is child's play. In fact, one might be able to demonstrate considerable brilliance in the group activity of working out the higher-order truths of chmess. Here is where psychologist Donald Hebb's dictum comes in handy:

If it isn't worth doing, it isn't worth doing well.

Probably every philosopher can readily think of an ongoing controversy in philosophy whose participants would be out of work if Hebb's dictum were ruthlessly applied, but we no doubt disagree on just which cottage industries should be shut down. Probably there is no investigation in our capacious discipline that is not believed by some school of thought to be a wasted effort, brilliance squandered on taking in each other's laundry. Voting would not yield results worth heeding, and dictatorship would be even worse, so let a thousand flowers bloom, I say. But just remember: if you let a thousand flowers bloom, count on 995 of them to wilt. The alert I want to offer you is just this: try to avoid committing your precious formative years to a research agenda with a short shelf life. Philosophical fads quickly go extinct, and there may be some truth to the rule of thumb: the hotter the topic, the sooner it will burn out.
One good test to make sure a philosophical project is not just exploring the higher-order truths of chmess is to see if people aside from philosophers actually play the game. Can anybody outside of academic philosophy be made to care about whether Jones's counterexample works against Smith's principle? Another such test is to try to teach the stuff to uninitiated undergraduates. If they don't "get it," you really should consider the hypothesis that you're following a self-supporting community of experts into an artifactual trap.
Here is one way the trap works. Philosophy is to some extent an unnatural act, and the more intelligent you are, the more qualms and reservations you are likely to have about whether you get it, whether you're "doing it right," whether you have any talent for this discipline, and even whether the discipline is worth entering in the first place. So bright student Jones is appropriately insecure about going into philosophy. Intrigued by Professor Brown's discussion, Jones takes a stab at it, writing a paper on hot topic H that is given an "A" by Professor Brown. "You've got real talent, Jones," says Brown, and Jones has just discovered something that might be suitable lifework. Jones begins to invest in learning the rules of this particular game and in playing it ferociously with the other young aspirants. "Hey, we're good at this!" they say, egging each other on. Doubts about the enabling assumptions of the enterprise tend to be muffled or squelched "for the sake of argument." Publications follow.
So don't count on the validation of your fellow graduate students or your favorite professors to settle the issue. They all have a vested interest in keeping the enterprise going. It's what they know how to do; it's what they are good at. This is a problem in other fields too, and it can be even harder to break out of. Experimentalists who master a technique and equip an expensive lab for pursuing it sometimes get stuck filling in the blanks of data matrices that nobody cares about any longer. What are they supposed to do? Throw away all that expensive apparatus? It can be a nasty problem. It is actually easier and cheaper for philosophers to retool. After all, our "training" is not, in general, high-tech. It's mainly a matter of learning our way around in various literatures, learning the moves that have been tried and tested. And here the trap to avoid is simply this: You see that somebody eminent has asserted something untenable or dubious in print; Professor Goofmaker's clever but flawed piece is a sitting duck, just the right target for an eye-catching debut publication. Go for it. You weigh in, along with a dozen others, and now you must watch your step, because by the time you've all cited each other and responded to the responses, you're a budding expert on how to deal with how to deal with responses to Goofmaker's minor overstatement. (And remember, too, that if Goofmaker hadn't made his thesis a little too bold, he never would have attracted all the attention in the first place; the temptation to be provocative is not restricted to graduate students on the lookout for a splashy entrance into the field.)
Some people are quite content to find a congenial group of smart people with whom to share "the fun of discovery, the pleasures of cooperation, and the satisfaction of reaching agreement," as the philosopher John Austin (1961, p. 123) once put it in a published lecture, without worrying about whether the joint task is worth doing. And if enough people take up the task, it eventually becomes a phenomenon in its own right, worth studying. As philosopher Burton Dreben used to say to the graduate students at Harvard, "Philosophy is garbage, but the history of garbage is scholarship." Some garbage is more important than other garbage, however, and it's hard to decide which of it is worthy of scholarship. In another lecture published in the same book, Austin (1961) gave us the following snide masterpiece:

It is not unusual for an audience at a lecture to include some who prefer things to be important, and to them now, in case there are any such present, there is owed a peroration. [p. 179]

Austin was a brilliant philosopher, but most of the very promising philosophers who orbited around him, no doubt chuckling at this remark, have vanished without a trace, their oh-so-clever work in ordinary language philosophy (a school Austin more or less invented) duly published and then utterly and deservedly ignored within a few years after publication. It has happened many times.
So what should you do? The tests I have mentioned—seeing if folks outside philosophy, or bright undergraduates, can be made to care—provide only warning signs; they are not definitive. Certainly there have been, and will be, forbiddingly abstruse and difficult topics of philosophical investigation well worth pursuing, in spite of the fact that the uninitiated remain unimpressed. I certainly don't want to discourage explorations that defy the ambient presumptions about what is interesting and important. On the contrary, the best bold strokes in the field will almost always be met by stony incredulity or ridicule at first, and these should not deter you. My point is that you should not settle complacently into a seat on the bandwagon just because you have found some brilliant fellow travelers who find your work on the issue as unignorable as you find theirs. You may all be taking each other for a ride.











77. The 10 Percent That's Good
So if Sturgeon's Law holds for philosophy as it does for everything else, what, in my view, is the good stuff? First of all, the classics really are classics for a good reason. From Plato to Russell, the standard fare in history-of-philosophy courses holds up well even after centuries of examination, and the best of the secondary literature about this primary literature is also very valuable. You will get something—a lot, really—out of reading Aristotle or Kant or Nietzsche on your own, without any background, but you'll get a lot more if you accept some guidance from those who have specialized in these thinkers for their entire careers.
Not all historians of philosophy have the same goals and attitudes, and I for one see no good reason for disqualifying any of the contenders. Some insist on placing their thinkers in the historical context in which they wrote, which means, for instance, learning a lot of seventeenth-century science if you really want to understand Descartes, and a lot of seventeenth- and eighteenth-century political history if you really want to understand Locke or Hume, and always, of course, a lot of the philosophy of their lesser contemporaries as well. Why bother with the also-rans? There's a good reason. I found I never really appreciated many of the painters of the sixteenth and seventeenth centuries until I visited European museums where I could see room after room full of second-rate paintings of the same genres. If all you ever see is the good stuff—which is all you see in the introductory survey courses, and in the top museums—it's very hard to see just how wonderful the good stuff is. Do you know the difference between a good library and a great library? A good library has all the good books. A great library has all the books. If you really want to understand a great philosopher, you have to spend some time looking at the less great contemporaries and predecessors that are left in the shadows of the masters.
Other specialists touch only lightly on the historic contexts in which their heroes worked, and instead concentrate on showing how to translate the ideas into today's contexts. After all, Leibniz didn't write the Monadology to be an exemplary work of seventeenth-century rationalism; he wrote it to get at the truth. In the end, you're not taking any philosopher seriously until you ask whether or not what they say is right. Philosophy students—and professors—sometimes forget this, and concentrate on pigeonholing and engaging in "compare and contrast," as we say in examination questions. Whole philosophy departments sometimes fall into this vision of their goal. That's not philosophy; that's just philosophy appreciation. This is how I try to help my students break this habit:

You've stumbled on a terrible secret—a plot to destroy the Statue of Liberty, say, or to bring down the national electric grid. You work feverishly to gather and marshal your evidence, and then compose a letter that draws on all your eloquence. You send copies to the police, the FBI, the New York Times, CNN, and this is the response you get: "Ah, another very clever example of post-9/11 conspiracy theory" and "a gripping read, actually, and quite plausible in its own way, with excellent touches" and "reminds me of Dom De Lillo, with echoes of Pynchon." Aaargh! Pay attention! I'm trying to tell you the truth! Respect the philosopher you are reading by asking yourself, about every sentence and paragraph, "Do I believe this, and if not, why not?"

In addition to the history of philosophy there is excellent work in the philosophy of science—mathematics, logic, physics, biology, psychology, economics, political theory. There is almost no work in the philosophy of chemistry or astronomy or geology or engineering as such, but there is good work on some of the conceptual issues that arise in these fields. And then there is ethics. In 1971 John Rawls published A Theory of Justice, a towering work that opened up a fruitful era of philosophers who approach the traditional topics of ethics with an eye on the social sciences, especially economics and political science, but also biology and psychology. Thanks largely to Rawls, philosophers working in ethics raised their game, and the result has been a bounty of valuable philosophical research, which deserves and receives attention from researchers in other disciplines—and politicians and social critics.
Finally, there are philosophers who are not interdisciplinary at all, and also lean lightly on the history of the field, specializing on contemporary problems that arise in the work of philosophers specializing on contemporary problems that arise in the work of other contemporary philosophers. Some of this, as I have already noted, succumbs to Hebb's rule: if it's not worth doing, it's not worth doing well. But some of it is excellent and valuable. I have mentioned quite a few contemporary philosophers in this book, and I wouldn't mention them if I didn't think their ideas were worth taking seriously, especially when I claim they are making a mistake. Aside from my targets, there are several-dozen other philosophers whose work I particularly admire, but I won't make the mistake of listing them. Several times in my career I have relied on the judgment of a colleague who told me not to bother with X's work because it was foolish junk, only to learn some time later that I had been misled into ignoring a thinker with valuable ideas whose contribution to my own thinking was delayed by the bum steer. I am painfully aware of how easily I could be read as excusing interested thinkers from reading some philosopher who didn't make it onto my list. So please treat this book as an open-ended introduction to some ways of doing philosophy, and if you find them useful, they can be a springboard into your own exploration of the questions and answers that have been worked on for so long by so many thinkers.














X. USE THE TOOLS. TRY HARDER.

 

"It's inconceivable!" That's what some people declare when they confront the "mystery" of consciousness, or the claim that life arose on this planet more than three billion years ago without any helping hand from an Intelligent Designer, for instance. When I hear this, I am always tempted to say, "Well of course it's inconceivable to you. You left your thinking tools behind and you're hardly trying." Recall William Bateson's firm declaration that a material basis for genes was inconceivable. Even schoolchildren have little difficulty conceiving of DNA today, and it's not because they are more brilliant than Bateson was. It's because in the last century we have devised and refined the thinking tools that make it a snap. Of course some people really don't want to conceive of these things. They want to protect the mysteries from even an attempt at explanation, for fear that an explanation might make the treasures disappear.
When other people start getting inquisitive, they find that "God works in mysterious ways" is a convenient anti-thinking tool. By hinting that the questioner is arrogant and overreaching, it can quench curiosity in an instant. It used to work well, and still works well in the communities where ignorance of science is regarded as a negligible flaw if not actually a virtue. I think we should stop treating this "pious" observation as any kind of wisdom and recognize it as the transparently defensive propaganda that it is. A positive response might be, "Oh good! I love a mystery. Let's see if we can solve this one, too. Do you have any ideas?"
Conceiving of something new is hard work, not just a matter of framing some idea in your mind, giving it a quick once-over and then endorsing it. What is inconceivable to us now may prove to be obviously conceivable when we've done some more work on it. And when we confidently declare that some things are truly impossible—a largest prime number, or a plane triangle with interior angles adding up to more than two right angles, or a married bachelor—it is not so much because we find these things inconceivable as that we find we have conceived of their components so well, so exhaustively, that the impossibility of their conjunction is itself clearly conceivable.
We haven't yet succeeded in fully conceiving how meaning could exist in the material world, or how life arose and evolved, or how consciousness works, or whether free will can be one of our endowments, but we've made progress: the questions we're posing and addressing now are better than the questions of yesteryear. We're hot on the trail of the answers.














XI. WHAT GOT LEFT OUT

 

Some readers of the draft of this book expressed surprise and disappointment that it didn't include some of my best-known intuition pumps. In fact, several dozen don't appear here, including some of my favorites. In a few cases I think an explanation is necessary.
"Where am I?" is probably my best-known intuition pump, but for that very reason it could be left out. It first appeared in Brainstorms in 1978, and then was included in The Mind's I in 1981. Translations have been published in about a dozen languages, and it has often been anthologized. The movie Victim of the Brain (1984) has a half-hour dramatization of it (with me playing the later Dennett body), and a scene also appears in a 1981 BBC documentary on consciousness and the brain. Then there was the Javanese shadow-puppet dramatization produced by the well-known puppeteer Lynn Jeffries at Harvard's Loeb Theater in 1984. Google leaves no doubt that it is readily available, along with a flood of commentaries.
"The Ballad of Shakey's Pizza Parlor" (in Dennett, 1982a) does serious work dismantling a set of presumptions about "de re and de dicto belief" that were once dominant in the thinking of some philosophers working on intentionality, but that were unfamiliar to everyone else. If I had included it here, I would first have had to infect all of you with some seductive but misguided intuitions so that I could then cure you with my intuition pump. Some philosophers have to know all about this, but others can remain blissfully ignorant without loss.
In the part on evolution I reluctantly left out my favorite new thinking tool, philosopher Peter Godfrey-Smith's Darwinian Spaces, the best use of a multidimensional space as a thinking tool in philosophy that I know, because it would have required too large a review of evolutionary theory and biological phenomena to make it effective. I explain it in somewhat technical terms for other philosophers, and exhibit some of its excellent uses, in my review essay "Homunculi Rule" (2010) on Godfrey-Smith's book Population Thinking and Natural Selection (2009). See also Godfrey-Smith's (2010) response.
My essay "Quining Qualia" (1988a) consists of no less than fourteen intuition pumps, designed to clarify, and then banish as hopelessly confused, the philosophical concept of qualia. Only one of those is included here, the Curse of the Cauliflower, to help introduce the concept and a major problem with qualia. "Quining Qualia" might be considered supplementary reading on the topic for anyone who still thinks the concept of qualia (as philosophers like to define it) is a good idea. The essay has often been anthologized and is readily available on the Internet, in several languages. My book Sweet Dreams (2005b) contains yet other arguments and intuition pumps on the topic. Others on consciousness are "curare-cum-amnestic" in "Why You Can't Make a Computer That Feels Pain" (Dennett, 1978c), "Swamp Mary and RoboMary" in "What RoboMary Knows" (Dennett, 2007d), and "Orwellian and Stalinesque" models of consciousness in Consciousness Explained (Dennett, 1991a). These all require more stage setting than I could comfortably provide if I was to keep this book relatively short.
I've also left out the various intuition pumps for thinking about religion that I introduced in Breaking the Spell (Dennett, 2006a), and my example of Superman tweaking the Burgess Shale in Science and Religion: Are They Compatible? (Dennett and Plantinga, 2011).
