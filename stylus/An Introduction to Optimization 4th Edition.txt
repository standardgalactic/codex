













Contents
Cover
Half Title page
Title page
Copyright page
Dedication
Preface
Part I: Mathematical Review
Chapter 1: Methods of Proof and Some Notation
1.1 Methods of Proof
1.2 Notation
Exercises
Chapter 2: Vector Spaces and Matrices
2.1 Vector and Matrix
2.2 Rank of a Matrix
2.3 Linear Equations
2.4 Inner Products and Norms
Exercises
Chapter 3: Transformations
3.1 Linear Transformations
3.2 Eigenvalues and Eigenvectors
3.3 Orthogonal Projections
3.4 Quadratic Forms
3.5 Matrix Norms
Exercises
Chapter 4: Concepts from Geometry
4.1 Line Segments
4.2 Hyperplanes and Linear Varieties
4.3 Convex Sets
4.4 Neighborhoods
4.5 Polytopes and Polyhedra
Exercises
Chapter 5: Elements of Calculus
5.1 Sequences and Limits
5.2 Differentiability
5.3 The Derivative Matrix
5.4 Differentiation Rules
5.5 Level Sets and Gradients
5.6 Taylor Series
Exercises
Part II: Unconstrained Optimization
Chapter 6: Basics of Set-Constrained and Unconstrained Optimization
6.1 Introduction
6.2 Conditions for Local Minimizers
Exercises
Chapter 7: One-Dimensional Search Methods
7.1 Introduction
7.2 Golden Section Search
7.3 Fibonacci Method
7.4 Bisection Method
7.5 Newton's Method
7.6 Secant Method
7.7 Bracketing
7.8 Line Search in Multidimensional Optimization
Exercises
Chapter 8: Gradient Methods
8.1 Introduction
8.2 The Method of Steepest Descent
8.3 Analysis of Gradient Methods
Exercises
Chapter 9: Newton's Method
9.1 Introduction
9.2 Analysis of Newton's Method
9.3 Levenberg-Marquardt Modification
9.4 Newton's Method for Nonlinear Least Squares
Exercises
Chapter 10: Conjugate Direction Methods
10.1 Introduction
10.2 The Conjugate Direction Algorithm
10.3 The Conjugate Gradient Algorithm
10.4 The Conjugate Gradient Algorithm for Nonquadratic Problems
Exercises
Chapter 11: Quasi-Newton Methods
11.1 Introduction
11.2 Approximating the Inverse Hessian
11.3 The Rank One Correction Formula
11.4 The DFP Algorithm
11.5 The BFGS Algorithm
Exercises
Chapter 12: Solving Linear Equations
12.1 Least-Squares Analysis
12.2 The Recursive Least-Squares Algorithm
12.3 Solution to a Linear Equation with Minimum Norm
12.4 Kaczmarz's Algorithm
12.5 Solving Linear Equations in General
Exercises
Chapter 13: Unconstrained Optimization and Neural Networks
13.1 Introduction
13.2 Single-Neuron Training
13.3 The Backpropagation Algorithm
Exercises
Chapter 14: Global Search Algorithms
14.1 Introduction
14.2 The Nelder-Mead Simplex Algorithm
14.3 Simulated Annealing
14.4 Particle Swarm Optimization
14.5 Genetic Algorithms
Exercises
Part III: Linear Programming
Chapter 15: Introduction to Linear Programming
15.1 Brief History of Linear Programming
15.2 Simple Examples of Linear Programs
15.3 Two-Dimensional Linear Programs
15.4 Convex Polyhedra and Linear Programming
15.5 Standard Form Linear Programs
15.6 Basic Solutions
15.7 Properties of Basic Solutions
15.8 Geometric View of Linear Programs
Exercises
Chapter 16: Simplex Method
16.1 Solving Linear Equations Using Row Operations
16.2 The Canonical Augmented Matrix
16.3 Updating the Augmented Matrix
16.4 The Simplex Algorithm
16.5 Matrix Form of the Simplex Method
16.6 Two-Phase Simplex Method
16.7 Revised Simplex Method
Exercises
Chapter 17: Duality
17.1 Dual Linear Programs
17.2 Properties of Dual Problems
Exercises
Chapter 18: Nonsimplex Methods
18.1 Introduction
18.2 Khachiyan's Method
18.3 Affine Scaling Method
18.4 Karmarkar's Method
Exercises
Chapter 19: Integer Linear Programming
19.1 Introduction
19.2 Unimodular Matrices
19.3 The Gomory Cutting-Plane Method
Exercises
Part IV: Nonlinear Constrained Optimization
Chapter 20: Problems with Equality Constraints
20.1 Introduction
20.2 Problem Formulation
20.3 Tangent and Normal Spaces
20.4 Lagrange Condition
20.5 Second-Order Conditions
20.6 Minimizing Quadratics Subject to Linear Constraints
Exercises
Chapter 21: Problems with Inequality Constraints
21.1 Karush-Kuhn-Tucker Condition
21.2 Second-Order Conditions
Exercises
Chapter 22: Convex Optimization Problems
22.1 Introduction
22.2 Convex Functions
22.3 Convex Optimization Problems
22.4 Semidefinite Programming
Exercises
Chapter 23: Algorithms for Constrained Optimization
23.1 Introduction
23.2 Projections
23.3 Projected Gradient Methods with Linear Constraints
23.4 Lagrangian Algorithms
23.5 Penalty Methods
Exercises
Chapter 24: Multiobjective Optimization
24.1 Introduction
24.2 Pareto Solutions
24.3 Computing the Pareto Front
24.4 From Multiobjective to Single-Objective Optimization
24.5 Uncertain Linear Programming Problems
Exercises
References
Index







AN INTRODUCTION TO OPTIMIZATION







WILEY SERIES IN DISCRETE MATHEMATICS AND OPTIMIZATION
AARTS AND KORST • Simulated Annealing and Boltzmann Machines: A Stochastic Approach to Combinatorial Optimization and Neural Computing
AARTS AND LENSTRA • Local Search in Combinatorial Optimization
ALON AND SPENCER • The Probabilistic Method, Third Edition
ANDERSON AND NASH • Linear Programming in Infinite-Dimensional Spaces: Theory and Application
ARLINGHAUS, ARLINGHAUS, AND HARARY • Graph Theory and Geography: An Interactive View E-Book
AZENCOTT • Simulated Annealing: Parallelization Techniques
BARTHÉLEMY AND GUÉNOCHE • Trees and Proximity Representations
BAZARRA, JARVIS, AND SHERALI • Linear Programming and Network Flows
BRUEN AND FORCINITO • Cryptography, Information Theory, and Error-Correction: A Handbook for the 21st Century
CHANDRU AND HOOKER • Optimization Methods for Logical Inference
CHONG AND ŹK • An Introduction to Optimization, Fourth Edition
COFFMAN AND LUEKER • Probabilistic Analysis of Packing and Partitioning Algorithms
COOK, CUNNINGHAM, PULLEYBLANK, AND SCHRIJVER • Combinatorial Optimization
DASKIN • Network and Discrete Location: Modes, Algorithms and Applications
DINITZ AND STINSON • Contemporary Design Theory: A Collection of Surveys
DU AND KO • Theory of Computational Complexity
ERICKSON • Introduction to Combinatorics
GLOVER, KLINGHAM, AND PHILLIPS • Network Models in Optimization and Their Practical Problems
GOLSHTEIN AND TRETYAKOV • Modified Lagrangians and Monotone Maps in Optimization
GONDRAN AND MINOUX • Graphs and Algorithms (Translated by S. Vajd)
GRAHAM, ROTHSCHILD, AND SPENCER • Ramsey Theory, Second Edition
GROSS AND TUCKER • Topological Graph Theory
HALL • Combinatorial Theory, Second Edition
HOOKER • Logic-Based Methods for Optimization: Combining Optimization and Constraint Satisfaction
IMRICH AND KLAVŽAR • Product Graphs: Structure and Recognition
JANSON, LUCZAK, AND RUCINSKI • Random Graphs
JENSEN AND TOFT • Graph Coloring Problems
KAPLAN • Maxima and Minima with Applications: Practical Optimization and Duality
LAWLER, LENSTRA, RINNOOY KAN, AND SHMOYS, Editors • The Traveling Salesman Problem: A Guided Tour of Combinatorial Optimization
LAYWINE AND MULLEN • Discrete Mathematics Using Latin Squares
LEVITIN • Perturbation Theory in Mathematical Programming Applications
MAHMOUD • Evolution of Random Search Trees
MAHMOUD • Sorting: A Distribution Theory
MARTELLI • Introduction to Discrete Dynamical Systems and Chaos
MARTELLO AND TOTH • Knapsack Problems: Algorithms and Computer Implementations
McALOON AND TRETKOFF • Optimization and Computational Logic
MERRIS • Combinatorics, Second Edition
MERRIS • Graph Theory
MINC • Nonnegative Matrices
MINOUX • Mathematical Programming: Theory and Algorithms (Translated by S. Vajd)
MIRCHANDANI AND FRANCIS, Editors • Discrete Location Theory
NEMHAUSER AND WOLSEY • Integer and Combinatorial Optimization
NEMIROVSKY AND YUDIN • Problem Complexity and Method Efficiency in Optimization (Translated by E. R. Dawson)
PACH AND AGARWAL • Combinatorial Geometry
PLESS • Introduction to the Theory of Error-Correcting Codes, Third Edition
ROOS AND VIAL • Ph. Theory and Algorithms for Linear Optimization: An Interior Point Approach
SCHEINERMAN AND ULLMAN • Fractional Graph Theory: A Rational Approach to the Theory of Graphs
SCHIFF • Cellular Automata: A Discrete View of the World
SCHRIJVER • Theory of Linear and Integer Programming
SPALL • Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control
STIEBITZ, SCHEIDE, TOFT, AND FAVRHOLDT • Graph Edge Coloring: Vizing's Theorem and Goldberg's Conjecture
SZPANKOWSKI • Average Case Analysis of Algorithms on Sequences
TOMESCU • Problems in Combinatorics and Graph Theory (Translated by R.A. Melter)
TUCKER • Applied Combinatorics, Second Edition
WOLSEY • Integer Programming
YE • Interior Point Algorithms: Theory and Analysis

















Copyright © 2013 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing this book, they make no representations or warranties with respect to the accuracy or completeness of the contents of this book and specifically disclaim any implied warranties of merchantability or fitness for a particular purpose. No warranty may be created or extended by sales representatives or written sales materials. The advice and strategies contained herein may not be suitable for your situation. You should consult with a professional where appropriate. Neither the publisher nor author shall be liable for any loss of profit or any other commercial damages, including but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our Customer Care Department within the United States at (800) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic formats. For more information about Wiley products, visit our web site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data
Chong, Edwin Kah Pin.An introduction to optimization / Edwin K. P. Chong, Colorado State University, Stanislaw H. Zak, Purdue University. — Fourth edition.pages cmSummary: "The purpose of the book is to give the reader a working knowledge of optimization theory and methods" — Provided by publisher.Includes bibliographical references and index.ISBN 978-1-118-27901-4 (hardback)1. Mathematical optimization. I. Zak, Stanislaw H. II. Title.QA402.5.C476 2012519.6—dc232012031772







To my wife, Yat-Yee,and to my parents, Pauland Julienne Chong.Edwin K. P. Chong
To JMJ; my wife,Mary Ann; and myparents, Janina andKonstanty ak.Stanislaw H. ak







PREFACE
Optimization is central to any problem involving decision making, whether in engineering or in economics. The task of decision making entails choosing among various alternatives. This choice is governed by our desire to make the "best" decision. The measure of goodness of the alternatives is described by an objective function or performance index. Optimization theory and methods deal with selecting the best alternative in the sense of the given objective function.
The area of optimization has received enormous attention in recent years, primarily because of the rapid progress in computer technology, including the development and availability of user-friendly software, high-speed and parallel processors, and artificial neural networks. A clear example of this phenomenon is the wide accessibility of optimization software tools such as the Optimization Toolbox of MATLAB1 and the many other commercial software packages.
There are currently several excellent graduate textbooks on optimization theory and methods (e.g., [3], [39], [43], [51], [87], [88], [104], [129]), as well as undergraduate textbooks on the subject with an emphasis on engineering design (e.g., [1] and [109]). However, there is a need for an introductory textbook on optimization theory and methods at a senior undergraduate or beginning graduate level. The present text was written with this goal in mind. The material is an outgrowth of our lecture notes for a one-semester course in optimization methods for seniors and beginning graduate students at Purdue University, West Lafayette, Indiana. In our presentation, we assume a working knowledge of basic linear algebra and multivariable calculus. For the reader's convenience, a part of this book (Part I) is devoted to a review of the required mathematical background material. Numerous figures throughout the text complement the written presentation of the material. We also include a variety of exercises at the end of each chapter. A solutions manual with complete solutions to the exercises is available from the publisher to instructors who adopt this text. Some of the exercises require using MATLAB. The student edition of MATLAB is sufficient for all of the MATLAB exercises included in the text. The MATLAB source listings for the MATLAB exercises are also included in the solutions manual.
The purpose of the book is to give the reader a working knowledge of optimization theory and methods. To accomplish this goal, we include many examples that illustrate the theory and algorithms discussed in the text. However, it is not our intention to provide a cookbook of the most recent numerical techniques for optimization; rather, our goal is to equip the reader with sufficient background for further study of advanced topics in optimization.
The field of optimization is still a very active research area. In recent years, various new approaches to optimization have been proposed. In this text, we have tried to reflect at least some of the flavor of recent activity in the area. For example, we include a discussion of randomized search methods—these include particle swarm optimization and genetic algorithms, topics of increasing importance in the study of complex adaptive systems. There has also been a recent surge of applications of optimization methods to a variety of new problems. An example of this is the use of descent algorithms for the training of feedforward neural networks. An entire chapter in the book is devoted to this topic. The area of neural networks is an active area of ongoing research, and many books have been devoted to this subject. The topic of neural network training fits perfectly into the framework of unconstrained optimization methods. Therefore, the chapter on feedforward neural networks not only provides an example of application of unconstrained optimization methods but also gives the reader an accessible introduction to what is currently a topic of wide interest.
The material in this book is organized into four parts. Part I contains a review of some basic definitions, notations, and relations from linear algebra, geometry, and calculus that we use frequently throughout the book. In Part II we consider unconstrained optimization problems. We first discuss some theoretical foundations of set-constrained and unconstrained optimization, including necessary and sufficient conditions for minimizers and maximizers. This is followed by a treatment of various iterative optimization algorithms, including line search methods, together with their properties. A discussion of global search algorithms is included in this part. We also analyze the least-squares optimization problem and the associated recursive least-squares algorithm. Parts III and IV are devoted to constrained optimization. Part III deals with linear programming problems, which form an important class of constrained optimization problems. We give examples and analyze properties of linear programs, and then discuss the simplex method for solving linear programs. We also provide a brief treatment of dual linear programming problems. We then describe some nonsimplex algorithms for solving linear programs: Khachiyan's method, the affine scaling method, and Karmarkar's method. We wrap up Part III by discussing integer linear programming problems. In Part IV we treat nonlinear constrained optimization. Here, as in Part II, we first present some theoretical foundations of nonlinear constrained optimization problems, including convex optimization problems. We then discuss different algorithms for solving constrained optimization problems. We also treat multiobjective optimization.
Although we have made every effort to ensure an error-free text, we suspect that some errors remain undetected. For this purpose, we provide online updated errata that can be found at the Web site for the book, accessible via
http://www.wiley.com/mathematics
We are grateful to several people for their help during the course of writing this book. In particular, we thank Dennis Goodman of Lawrence Livermore Laboratories for his comments on early versions of Part II and for making available to us his lecture notes on nonlinear optimization. We thank Moshe Kam of Drexel University for pointing out some useful references on nonsimplex methods. We are grateful to Ed Silverman and Russell Quong for their valuable remarks on Part I of the first edition. We also thank the students of ECE 580 at Purdue University and ECE 520 and MATH 520 at Colorado State University for their many helpful comments and suggestions. In particular, we are grateful to Christopher Taylor for his diligent proofreading of early manuscripts of this book. This fourth edition incorporates many valuable suggestions of users of the first, second, and third editions, to whom we are grateful.
E. K. P. CHONG AND S. H. AKFort Collins, Colorado, and West Lafayette, Indiana
1 MATLAB is a registered trademark of The MathWorks, Inc.







PART I
MATHEMATICAL REVIEW







CHAPTER 1
METHODS OF PROOF AND SOME NOTATION
1.1 Methods of Proof
Consider two statements, "A" and "B," which could be either true or false. For example, let "A" be the statement "John is an engineering student," and let "B" be the statement "John is taking a course on optimization." We can combine these statements to form other statements, such as "A and B" or "A or B." In our example, "A and B" means "John is an engineering student, and he is taking a course on optimization." We can also form statements such as "not A," "not B," "not (A and B)," and so on. For example, "not A" means "John is not an engineering student." The truth or falsity of the combined statements depend on the truth or falsity of the original statements, "A" and "B." This relationship is expressed by means of truth tables; see Tables 1.1 and 1.2.
Table 1.1 Truth Table for "A and B" and "A or B"

Table 1.2 Truth Table for "not A"


A
not A


F
T


T
F


From the tables, it is easy to see that the statement "not (A and B)" is equivalent to "(not A) or (not B)" (see Exercise 1.3). This is called DeMorgan's law.
In proving statements, it is convenient to express a combined statement by a conditional, such as "A implies B," which we denote "A⇒B." The conditional "A⇒B" is simply the combined statement "(not A) or B" and is often also read "A only if B," or "if A then B," or "A is sufficient for B," or "B is necessary for A."
We can combine two conditional statements to form a biconditional statement of the form "AB," which simply means "(A⇒B) and (B⇒A)." The statement "AB" reads "A if and only if B," or "A is equivalent to B," or "A is necessary and sufficient for B." Truth tables for conditional and biconditional statements are given in Table 1.3.
Table 1.3 Truth Table for Conditionals and Biconditionals

It is easy to verify, using the truth table, that the statement "A⇒B" is equivalent to the statement "(not B)⇒ (not A)." The latter is called the contrapositive of the former. If we take the contrapositive to DeMorgan's law, we obtain the assertion that "not (A or B)" is equivalent to "(not A) and (not B)."
Most statements we deal with have the form "A⇒B." To prove such a statement, we may use one of the following three different techniques:

1. The direct method
2. Proof by contraposition
3. Proof by contradiction or reductio ad absurdum

In the case of the direct method, we start with "A," then deduce a chain of various consequences to end with "B."
A useful method for proving statements is proof by contraposition, based on the equivalence of the statements "A⇒B" and "(not B)⇒(not A)." We start with "not B," then deduce various consequences to end with "not A" as a conclusion.
Another method of proof that we use is proof by contradiction, based on the equivalence of the statements "A⇒B" and "not (A and (not B))." Here we begin with "A and (not B)" and derive a contradiction.
Occasionally, we use the principle of induction to prove statements. This principle may be stated as follows. Assume that a given property of positive integers satisfies the following conditions:

The number 1 possesses this property.
If the number n possesses this property, then the number n + 1 possesses it too.

The principle of induction states that under these assumptions any positive integer possesses the property.
The principle of induction is easily understood using the following intuitive argument. If the number 1 possesses the given property, then the second condition implies that the number 2 possesses the property. But, then again, the second condition implies that the number 3 possesses this property, and so on. The principle of induction is a formal statement of this intuitive reasoning.
For a detailed treatment of different methods of proof, see [130].
1.2 Notation
Throughout, we use the following notation. If X is a set, then we write x  X to mean that x is an element of X. When an object x is not an element of a set X, we write x . X. We also use the "curly bracket notation" for sets, writing down the first few elements of a set followed by three dots. For example, {x1, x2, x3,...} is the set containing the elements x1, x2, x3, and so on. Alternatively, we can explicitly display the law of formation. For example, {x : x  , x > 5} reads "the set of all x such that x is real and x is greater than 5." The colon following x reads "such that." An alternative, notation for the same set is {x   : x > 5}.
If X and Y are sets, then we write X ⊂ Y to mean that every element of X is also an element of Y. In this case, we say that X is a subset of Y. If X and Y are sets, then we denote by X \ Y ("X minus Y") the set of all points in X that are not in Y. Note that X \ Y is a subset of X. The notation f : X → Y means "f is a function from the set X into the set Y." The symbol := denotes arithmetic assignment. Thus, a statement of the form x := y means "x becomes y." The symbol  means "equals by definition."
Throughout the text, we mark the end of theorems, lemmas, propositions, and corollaries using the symbol . We mark the end of proofs, definitions, and examples by.
We use the IEEE style when citing reference items. For example, [77] represents reference number 77 in the list of references at the end of the book.
EXERCISES

1.1 Construct the truth table for the statement "(not B)⇒(not A)," and use it to show that this statement is equivalent to the statement "A⇒B."
1.2 Construct the truth table for the statement "not (A and (not B))," and use it to show that this statement is equivalent to the statement "A⇒B."
1.3 Prove DeMorgan's law by constructing the appropriate truth tables.
1.4 Prove that for any statements A and B, we have "A  (A and B) or (A and (not B))." This is useful because it allows us to prove a statement A by proving the two separate cases "(A and B)" and "(A and (not B))." For example, to prove that |x| ≥ x for any x  , we separately prove the cases "|x| ≥ x and x ≥ 0" and "|x| ≥ x and x < 0." Proving the two cases turns out to be easier than proving the statement |x| ≥ x directly (see Section 2.4 and Exercise 2.7).
1.5 (This exercise is adopted from [22, pp. 80-81]) Suppose that you are shown four cards, laid out in a row. Each card has a letter on one side and a number on the other. On the visible side of the cards are printed the symbols

S   8   3   A
Determine which cards you should turn over to decide if the following rule is true or false: "If there is a vowel on one side of the card, then there is an even number on the other side."







CHAPTER 2
VECTOR SPACES AND MATRICES
2.1 Vector and Matrix
We define a column n-vector to be an array of n numbers, denoted

The number ai is called the ith component of the vector a. Denote by  the set of real numbers and by n the set of column n-vectors with real components. We call n an n-dimensional real vector space. We commonly denote elements of n by lowercase bold letters (e.g., x). The components of x  n are denoted x1, ..., xn.
We define a row n-vector as

The transpose of a given column vector a is a row vector with corresponding elements, denoted a. For example, if

then

Equivalently, we may write a = [a1, a2, ..., an]. Throughout the text we adopt the convention that the term vector (without the qualifier row or column) refers to a column vector.
Two vectors a = [a1, a2, ..., an] and b = [b1, b2, ..., bn] are equal if ai = bi, i = 1, 2, ..., n.
The sum of the vectors a and b, denoted a + b, is the vector

The operation of addition of vectors has the following properties:

1. The operation is commutative:

2. The operation is associative:

3. There is a zero vector

such that

The vector


is called the difference between a and b and is denoted a − b.
The vector 0 − b is denoted −b. Note that

The vector b − a is the unique solution of the vector equation

Indeed, suppose that x = [x1, x2, ..., xn] is a solution to a + x = b. Then,

and thus

We define an operation of multiplication of a vector a  n by a real scalar α   as

This operation has the following properties:

1. The operation is distributive: for any real scalars α and β,

2. The operation is associative:

3. The scalar 1 satisfies

4. Any scalar α satisfies

5. The scalar 0 satisfies

6. The scalar −1 satisfies


Note that αa = 0 if and only if α = 0 or a = 0. To see this, observe that αa = 0 is equivalent to αa1 = αa2 = ·· · = αan = 0. If α = 0 or a = 0, then αa = 0. If a ≠ 0, then at least one of its components ak ≠ 0. For this component, αak = 0, and hence we must have α = 0. Similar arguments can be applied to the case when α ≠ 0.
A set of vectors {a1, ..., ak} is said to be linearly independent if the equality

implies that all coefficients αi, i = 1, ..., k, are equal to zero. A set of the vectors {a1, ..., ak} is linearly dependent if it is not linearly independent.
Note that the set composed of the single vector 0 is linearly dependent, for if α ≠ 0, then α0 = 0. In fact, any set of vectors containing the vector 0 is linearly dependent.
A set composed of a single nonzero vector a ≠ 0 is linearly independent since αa = 0 implies that α = 0.
A vector a is said to be a linear combination of vectors a1, a2, ..., ak if there are scalars α1, ..., αk such that

Proposition 2.1 A set of vectors {a1, a2, ..., ak} is linearly dependent if and only if one of the vectors from, the set is a linear combination of the remaining vectors.
Proof ⇒: If {a1, a2, ..., ak} is linearly dependent, then

where at least one of the scalars αi ≠ 0, whence

⇐: Suppose that

then

Because the first scalar is nonzero, the set of vectors {a1, a2, ..., ak} is linearly dependent. The same argument holds if ai, i = 2, ..., k, is a linear combination of the remaining vectors.
A subset  of n is called a subspace of n if  is closed under the operations of vector addition and scalar multiplication. That is, if a and b are vectors in , then the vectors a + b and αa are also in  for every scalar α.
Every subspace contains the zero vector 0, for if a is an element of the subspace, so is (−1)a = −a. Hence, a − a = 0 also belongs to the subspace.
Let a1, a2, ··· ak be arbitrary vectors in n. The set of all their linear combinations is called the span of a1, a2, ..., ak and is denoted

Given a vector a, the subspace span[a] is composed of the vectors αa, where α is an arbitrary real number (α  ). Also observe that if a is a linear combination of a1, a2, ..., ak, then

The span of any set of vectors is a subspace.
Given a subspace , any set of linearly independent vectors {a1, a2, ..., ak} ⊂  such that  = span[a1, a2, ..., ak] is referred to as a basis of the subspace . All bases of a subspace  contain the same number of vectors. This number is called the dimension of , denoted dim .
Proposition 2.2 If {a1, a2, ..., ak} is a basis of , then any vector a of  can be represented uniquely as

where αi  , i = 1, 2, ..., k.
Proof To prove the uniqueness of the representation of a in terms of the basis vectors, assume that

and

We now show that αi = βi, i = 1, ..., k. We have

or

Because the set {ai : i = 1, 2, ..., k} is linearly independent, α1 − β1 = α2 − β2 = ··· = αk − βk = 0, which implies that αi = βi, i = 1, ..., k.
Suppose that we are given a basis {a1, a2, ..., ak} of  and a vector a   such that

The coefficients αi, i = 1, ..., k, are called the coordinates of a with respect to the basis {a1, a2, ..., ak}.
The natural basis for n is the set of vectors

The reason for calling these vectors the natural basis is that

We can similarly define complex vector spaces. For this, let  denote the set of complex numbers and n the set of column n-vectors with complex components. As the reader can easily verify, the set n has properties similar to those of n, where scalars can take complex values.
A matrix is a rectangular array of numbers, commonly denoted by uppercase bold letters (e.g., A). A matrix with m rows and n columns is called an m × n matrix, and we write

The real number aij located in the ith row and jth column is called the (i, j)th entry. We can think of A in terms of its n columns, each of which is a column vector in m. Alternatively, we can think of A in terms of its m rows, each of which is a row n-vector.
Consider the m × n matrix A above. The transpose of matrix A, denoted A, is the n × m matrix

that is, the columns of A are the rows of A, and vice versa.
Let the symbol mxn denote the set of m × n matrices whose entries are real numbers. We treat column vectors in n as elements of n × 1. Similarly, we treat row n-vectors as elements of 1 × n. Accordingly, vector transposition is simply a special case of matrix transposition, and we will no longer distinguish between the two. Note that there is a slight inconsistency in the notation of row vectors when identified as 1 × n matrices: We separate the components of the row vector with commas, whereas in matrix notation we do not generally use commas. However, the use of commas in separating elements in a row helps to clarify their separation. We use use such commas even in separating matrices arranged in a horizontal row.
2.2 Rank of a Matrix
Consider the m × n matrix

Let us denote the kth column of A by ak:

The maximal number of linearly independent columns of A is called the rank of the matrix A, denoted rank A. Note that rank A is the dimension of span[a1, ..., an].
Proposition 2.3 The rank of a matrix A is invariant under the following operations:

1. Multiplication of the columns of A by nonzero scalars.
2. Interchange of the columns.
3. Addition to a given column a linear combination of other columns.

Proof.

1. Let bk = αkak, where αk ≠ 0, k = 1, ..., n, and let B = [b1, b2, ··· bn], Obviously,

and thus

2. The number of linearly independent vectors does not depend on their order.
3. Let


So, for any α1, ..., αn,

and hence

On the other hand,

Hence,

Therefore, rank A = rank B.
A matrix A is said to be square if the number of its rows is equal to the number of its columns (i.e., it is n × n). Associated with each square matrix A is a scalar called the determinant of the matrix A, denoted det A or |A|. The determinant of a square matrix is a function of its columns and has the following properties:

1. The determinant of the matrix A = [a1, a2, ..., an] is a linear function of each column; that is,

for each α, β  , a(1)k a(2)k  n.
2. If for some k we have ak = ak+1, then

3. Let

where {e1, ..., en} is the natural basis for n. Then

Note that if α = β = 0 in property 1, then


Thus, if one of the columns is 0, then the determinant is equal to zero.
The determinant does not change its value if we add to a column another column multiplied by a scalar. This follows from properties 1 and 2 as shown below:

However, the determinant changes its sign if we interchange columns. To show this property, note that

A pth-order minor of an m × n matrix A, with p ≤ min{m, n}, is the determinant of a p × p matrix obtained from A by deleting m − p rows and n − p columns. (The notation min{m, n} represents the smaller of m and n.)
We can use minors to investigate the rank of a matrix. In particular, we have the following proposition.
Proposition 2.4 If an m × n (m ≥ n) matrix A has a nonzero nth-order minor, then the columns of A are linearly independent; that is, rank A = n.
Proof. Suppose that A has a nonzero nth-order minor. Without loss of generality, we assume that the nth-order minor corresponding to the first n rows of A is nonzero. Let xi, i = 1, ..., n, be scalars such that

The vector equality above is equivalent to the following set of m equations:

For i = 1, ..., n, let

Then, .
The nth-order minor is det[1, 2, ..., n], assumed to be nonzero. From the properties of determinants it follows that the columns 1, 2, ..., n are linearly independent. Therefore, all xi = 0, i = 1, ..., n. Hence, the columns a1, a2, ..., an are linearly independent.
From the above it follows that if there is a nonzero minor, then the columns associated with this nonzero minor are linearly independent.
If a matrix A has an rth-order minor |M| with the properties (i) |M| ≠ 0 and (ii) any minor of A that is formed by adding a row and a column of A to M is zero, then

Thus, the rank of a matrix is equal to the highest order of its nonzero minor(s).
A nonsingular (or invertible) matrix is a square matrix whose determinant is nonzero. Suppose that A is an n × n square matrix. Then, A is nonsingular if and only if there is another n × n matrix B such that

where In denotes the n × n identity matrix:

We call the matrix B above the inverse matrix of A, and write B = A−1.
2.3 Linear Equations
Suppose that we are given m equations in n unknowns of the form

We can represent the set of equations above as a vector equation

where

Associated with this system of equations is the matrix

and an augmented matrix

We can also represent the system of equations above as

where

Theorem 2.1 The system of equations Ax = b has a solution if and only if

Proof ⇒: Suppose that the system Ax = b has a solution. Therefore, b is a linear combination of the columns of A; that is, there exist x1, ..., xn such that x1a1 + x2a2 + · · · + xnan = b. It follows that b belongs to span[a1, ..., an] and hence

⇐: Suppose that rank A = rank [A, b] = r. Thus, we have r linearly independent columns of A. Without loss of generality, let a1, a, ..., ar be these columns. Therefore, a1, a2, ..., ar are also linearly independent columns of the matrix [A, b]. Because rank[A, b] = r, the remaining columns of [A, b] can be expressed as linear combinations of a1, a2, ..., ar. In particular, b can be expressed as a linear combination of these columns. Hence, there exist x1, ..., xn such that x1a1 + x2a2 + · · · + xnan = b.
Theorem 2.2 Consider the equation Ax = b, where A  mxn and rank A = m. A solution to Ax = b can be obtained by assigning arbitrary values for n − m variables and solving for the remaining ones.
Proof. We have rank A = m, and therefore we can find m linearly independent columns of A. Without loss of generality, let a1, a2, ..., am be such columns. Rewrite the equation Ax = b as

Assign to xm+1, xm+2, ..., xn arbitrary values, say

and let

Note that det B ≠ 0. We can represent the system of equations above as

The matrix B is invertible, and therefore we can solve for [x1, X2, ..., xm]. Specifically,

2.4 Inner Products and Norms
The absolute value of a real number a, denoted |a|, is defined as

The following formulas hold:

1. |a| = |−a|.
2. −|a| ≤ a ≤ |a|.
3. |a + b| ≤ |a| + |b|.
4. ||a| − |b|| ≤ |a − b| ≤ |a| + |b|.
5. |ab| = |a||b|.
6. |a| ≤ c and |b| ≤ d imply that |a + b| ≤ c + d.
7. The inequality |a| < b is equivalent to −b < a < b (i.e., a < b and −a < b). The same holds if we replace every occurrence of "<" by "≤."
8. The inequality |a| > b is equivalent to a > b or -a > b. The same holds if we replace every occurrence of ">" by "≥."
For x, y , n, we define the Euclidean inner product by


The inner product is a real-valued function : n × n →  having the following properties:

1. Positivity: x, x ≥ 0, x, x = 0 if and only if x = 0.
2. Symmetry: x, y = y, x.
3. Additivity: x + y, z = x, z + y, z.
4. Homogeneity: rx, y = rx, y for every r  .

The properties of additivity and homogeneity in the second vector also hold; that is,

The above can be shown using properties 2 to 4. Indeed,

and

It is possible to define other real-valued functions on n × n that satisfy properties 1 to 4 above (see Exercise 2.8). Many results involving the Euclidean inner product also hold for these other forms of inner products.
The vectors x and y are said to be orthogonal if x, y = 0.
The Euclidean norm of a vector x is defined as

Theorem 2.3 Cauchy-Schwarz Inequality. For any two vectors x and y in n, the Cauchy-Schwarz inequality

holds. Furthermore, equality holds if and only if x = αy for some α  .
Proof. First assume that x and y are unit vectors; that is, ||x|| = ||y|| = 1. Then,

or

with equality holding if and only if x = y.
Next, assuming that neither x nor y is zero (for the inequality obviously holds if one of them is zero), we replace x and y by the unit vectors x/||x|| and y/||y||. Then, apply property 4 to get

Now replace x by −x and again apply property 4 to get

The last two inequalities imply the absolute value inequality. Equality holds if and only if x/||x|| = ±y/||y||; that is, x = αy for some α  .
The Euclidean norm of a vector ||x|| has the following properties:

1. Positivity: ||x|| ≥ 0, ||x|| = 0 if and only if x = 0.
2. Homogeneity: ||rx|| = |r| ||x||, r  .
3. Triangle inequality: ||x + y|| ≤ ||x|| + ||y||.

The triangle inequality can be proved using the Cauchy-Schwarz inequality, as follows. We have

By the Cauchy-Schwarz inequality,

and therefore

Note that if x and y are orthogonal: x, y = 0, then

which is the Pythagorean theorem for n.
The Euclidean norm is an example of a general vector norm, which is any function satisfying the three properties of positivity, homogeneity, and triangle inequality. Other examples of vector norms on n include the 1-norm, defined by ||x||1 = |x1| + · · · + |xn|, and the ∞-norm, defined by ||x||∞ = maxi |xi| (where the notation maxi represents the largest over all the possible index values of i). The Euclidean norm is often referred to as the 2-norm, and denoted ||x||2. The norms above are special cases of the p-norm, given by

We can use norms to define the notion of a continuous function, as follows. A function f : n → m is continuous at x if for all ε > 0, there exists δ > 0 such that ||y − x|| < δ ⇒ ||f(y) − f(x)|| ε. If the function f is continuous at every point in n, we say that it is continuous on n. Note that f = [f1, ..., fm] is continuous if and only if each component fi, i = 1, ..., m, is continuous.
For the complex vector space n, we define an inner product x, y to be , where the bar denotes complex conjugation. The inner product on n is a complex-valued function having the following properties:

1. x, x ≤ 0, x, x = 0 if and only if x = 0.
2. .
3. x + y, z = x, z + y, z.
4. rx, y = rx, y, where r  .

From properties 1 to 4, we can deduce other properties, such as

where r1, r2  . For n, the vector norm can similarly be defined by ||x||2 = x,x. For more information, consult Gel'fand [47].
EXERCISES

2.1 Let A  m×n and rank A = m. Show that m ≤ n.
2.2 Prove that the system Ax = b, A  mxn, has a unique solution if and only if rank A = rank [A, b] = n.
2.3 (Adapted from [38].) We know that if k ≤ n + 1, then the vectors a1, a2, ..., ak  n are linearly dependent; that is, there exist scalars α1, ..., αk such that at least one αi ≠ 0 and Σki = 1 αiai = 0. Show that if k ≥ n + 2, then there exist scalars α1, ..., αk such that at least one αi = 0, Σki = 1 αiai = 0, and Σki = 1 αi = 0.
Hint: Introduce the vectors i = [1, ai]  n+1, i = 1, ..., k, and use the fact that any n + 2 vectors in n+1 are linearly dependent.
2.4 Consider an m × m matrix M that has block form

where Mk,k is k × k, Mm-k, k is (m − k) × k, Im-k is the (m − k) × (m − k) identity matrix, and Ok,m-k is the k × (m − k) zero matrix.
a. Show that

This result is relevant to the proof of Proposition 19.1.
b. Under certain assumptions, the following stronger result holds:

Identify cases where this is true, and show that it is false in general.
2.5 It is well known that for any a, b, c, d  ,

Suppose now that A, B, C, and D are real or complex square matrices of the same size. Give a sufficient condition under which

An interesting discussion on determinants of block matrices is provided in [121].
2.6 Consider the following system of linear equations:

Use Theorem 2.1 to check if the system has a solution. Then, use the method of Theorem 2.2 to find a general solution to the system.
2.7 Prove the seven properties of the absolute value of a real number.
2.8 Consider the function 2 : 2 × 2 → , defined by x, y2 = 2x1y1 + 3x2y1 + 3x1y2 + 5x2y2, where x = [x1, x2] and y = [y1, y2]. Show that 2 satisfies conditions 1 to 4 for inner products.
Note: This is a special case of Exercise 3.21.
2.9 Show that for any two vectors x, y  n, |||x|| − ||y||| ≤ ||x − y||.
Hint: Write x = (x − y) + y, and use the triangle inequality. Do the same for y.
2.10 Use Exercise 2.9 to show that the norm ||·|| is a uniformly continuous function; that is, for all ε > 0, there exists δ > 0 such that if ||x − y|| < δ, then |||x|| − ||y||| < ε.








CHAPTER 3
TRANSFORMATIONS
3.1 Linear Transformations
A function  : n → m is called a linear transformation if:

1. (ax) = a(x) for every x  n and a  .
2. (x + y) = (x) + (y) for every x, y  n.

If we fix the bases for n and m, then the linear transformation  can be represented by a matrix. Specifically, there exists A  m × n such that the following representation holds. Suppose that x  n is a given vector, and x′ is the representation of x with respect to the given basis for n. If y = (x), and γ' is the representation of y with respect to the given basis for m, then

We call A the matrix representation of  with respect to the given bases for n and m. In the special case where we assume the natural bases for n and m, the matrix representation A satisfies

Let {e1, e2, ..., en} and {e′, e′2, ..., e′n} be two bases for n. Define the matrix

We call T the transformation matrix from {e1, e2, ..., en} to {e′1, e′2, ..., e′n}. It is clear that

that is, the ith column of T is the vector of coordinates of ei with respect to the basis {e′, e′2, ..., e′n}.
Fix a vector in n. Let x be the column of the coordinates of the vector with respect to {e1, e2, ..., en} and x′ the coordinates of the same vector with respect to {e′1, e′2, ..., e′n}. Then, we can show that x′ = Tx (see Exercise 3.1).
Consider a linear transformation

and let A be its representation with respect to {e1, e2, ..., en} and B its representation with respect to {e′1, e′2, ..., e′n}. Let y = Ax and y′ = Bx′. Therefore, y′ = Ty = TAx = Bx′ = BTx, and hence TA = BT, or A = T−1BT.
Two n × n matrices A and B are similar if there exists a nonsingular matrix T such that A = T−1BT. In conclusion, similar matrices correspond to the same linear transformation with respect to different bases.
3.2 Eigenvalues and Eigenvectors
Let A be an n × n real square matrix. A scalar λ (possibly complex) and a nonzero vector v satisfying the equation Av = λv are said to be, respectively, an eigenvalue and an eigenvector of A. For λ to be an eigenvalue it is necessary and sufficient for the matrix λI − A to be singular; that is, det[λI − A] = 0, where I is the n × n identity matrix. This leads to an nth-order polynomial equation

We call the polynomial det[λI − A] the characteristic polynomial of the matrix A, and the equation above the characteristic equation. According to the fundamental theorem of algebra, the characteristic equation must have n (possibly nondistinct) roots that are the eigenvalues of A. The following theorem states that if A has n distinct eigenvalues, then it also has n linearly independent eigenvectors.
Theorem 3.1 Suppose that the characteristic equation det[λI − A] = 0 has n distinct roots λ1, λ2, ..., λn. Then, there exist n linearly independent vectors v1, v2, ..., vn such that

Proof. Because det[λiI − A] = 0, i = 1, ..., n, there exist nonzero vi, i = 1, ..., n, such that Avi = λivi, = 1 ..., n. We now prove the linear independence of {v1, v2, ..., vn}. To do this, let c1, ..., cn be scalars such that Σni=1 civi = 0. We show that ci = 0, i = 1, ..., n.
Consider the matrix

We first show that c1 = 0. Note that

since λnvn − Avn = 0.
Repeating the argument above, we get

But

Using the equation above, we see that

Because the λi are distinct, it must follow that c1 = 0.
Using similar arguments, we can show that all ci must vanish, and therefore the set of eigenvectors {v1, v2, ..., vn} is linearly independent.
Consider a basis formed by a linearly independent set of eigenvectors {v1, v2, ..., vn}. With respect to this basis, the matrix A is diagonal [i.e., if aij is the (i, j)th element of A, then aij = 0 for all i ≠ j]. Indeed, let

Then,

because TT−1 = I.
A matrix A is symmetric if A = A.
Theorem 3.2 All eigenvalues of a real symmetric matrix are real.
Proof. Let

where x ≠ 0. Taking the inner product of Ax with x yields

On the other hand,

The above follows from the definition of the inner product on n. We note that x, x is real and x, x > 0. Hence,

and

Because x, x > 0,

Thus, λ is real.
Theorem 3.3 Any real symmetric n × n matrix has a set of n eigenvectors that are mutually orthogonal.
Proof. We prove the result for the case when the n eigenvalues are distinct. For a general proof, see [62, p. 104].
Suppose that Av1 = λ1v1, Av2 = λ2v2, where λ1 ≠ λ2. Then,

Because A = A,

Therefore,

Because λ1 ≠ λ2, it follows that

If A is symmetric, then a set of its eigenvectors forms an orthogonal basis for n. If the basis {v1, v2, ..., vn} is normalized so that each element has norm of unity, then defining the matrix

we have

and hence

A matrix whose transpose is its inverse is said to be an orthogonal matrix.
3.3 Orthogonal Projections
Recall that a subspace  of n is a subset that is closed under the operations of vector addition and scalar multiplication. In other words,  is a subspace of n if x1, x2   ⇒ αx1 + βx2   for all α, β  . Furthermore, the dimension of a subspace  is equal to the maximum number of linearly independent vectors in . If  is a subspace of n, then the orthogonal complement of , denoted ⊥, consists of all vectors that are orthogonal to every vector in . Thus,

The orthogonal complement of  is also a subspace (see Exercise 3.7). Together,  and ⊥ span n in the sense that every vector x  n can be represented uniquely as

where x1   and x2  ⊥. We call the representation above the orthogonal decomposition of x (with respect to ). We say that x1 and x2 are orthogonal projections of x onto the subspaces  and ⊥, respectively. We write n =  ⊕ ⊥ and say that n is a direct sum of  and ⊥. We say that a linear transformation P is an orthogonal projector onto  if for all x  n, we have Px   and x − Px  ⊥.
In the subsequent discussion we use the following notation. Let A  m × n. Let the range, or image, of A be denoted

and the nullspace, or kernel, of A be denoted

Note that (A) and (A) are subspaces (see Exercise 3.9).
Theorem 3.4 Let A be a given matrix. Then, (A)⊥ = (A) and (A)⊥ = (A).
Proof. Suppose that x  (A)⊥. Then, y(Ax) = (Ay)x = 0 for all y, so that Ax = 0. Hence, x  (A). This implies that (A)⊥ ⊂ (A).
If now x  (A), then (Ay)x = y(Ax) = 0 for all y, so that x  (A)⊥, and consequently, (A) ⊂ (A)⊥. Thus, (A)⊥ = (A).
The equation (A)⊥ = (A) follows from what we have proved above and the fact that for any subspace , we have (⊥)⊥ =  (see Exercise 3.11).
Theorem 3.4 allows us to establish the following necessary and sufficient condition for orthogonal projectors. For this, note that if P is an orthogonal projector onto , then Px = x for all x  , and (P) =  (see Exercise 3.14).
Theorem 3.5 A matrix P is an orthogonal projector [onto the subspace  = (P)] if and only if P2 = P = P.
Proof. ⇒: Suppose that P is an orthogonal projector onto  = (P). Then, (I −P) ⊂ (P)⊥. But, (P)⊥ = (P) by Theorem 3.4. Therefore, (I − P) ⊂ (P). Hence, P(I − P)y = 0 for all y, which implies that P (I − P) = O, where O is the matrix with all entries equal to zero; i.e., the zero matrix. Therefore, P = PP, and thus P = P = P2.
⇐: Suppose that P2 = P = P. For any x, we have (Py)(I − P)x = yP(I − P)x − yP(I −P)x = 0 for all y. Thus, (I − P)x  (P)⊥, which means that P is an orthogonal projector.
3.4 Quadratic Forms
A quadratic form f : n →  is a function

where Q is an n × n real matrix. There is no loss of generality in assuming Q to be symmetric: Q = Q. For if the matrix Q is not symmetric, we can always replace it with the symmetric matrix

Note that

A quadratic form xQx, Q = Q, is said to be positive definite if xQx > 0 for all nonzero vectors x. It is positive semidefinite if x Qx > 0 for all x. Similarly, we define the quadratic form to be negative definite, or negative semidefinite, if xQx < 0 for all nonzero vectors x, or x Qx ≤ 0 for all x, respectively.
Recall that the minors of a matrix Q are the determinants of the matrices obtained by successively removing rows and columns from Q. The principal minors are det Q itself and the determinants of matrices obtained by successively removing an ith row and an ith column. That is, the principal minors are

The leading principal minors are det Q and the minors obtained by successively removing the last row and the last column. That is, the leading principal minors are

We now prove Sylvester's criterion, which allows us to determine if a quadratic form xQx is positive definite using only the leading principal minors of Q.
Theorem 3.6 Sylvester's Criterion. A quadratic form xQx, Q = Q, is positive definite if and only if the leading principal minors of Q are positive.
Proof. The key to the proof of Sylvester's criterion is the fact that a quadratic form whose leading principal minors are nonzero can be expressed in some basis as a sum of squares

where i are the coordinates of the vector x in the new basis, Δ0  1, and Δ1,...,Δn are the leading principal minors of Q.
To this end, consider a quadratic form f(x) = xQx, where Q = Q. Let {e1,e2,...,en} be the natural basis for n, and let

be a given vector in n. Let {v,v2,...,vn} be another basis for n. Then, the vector x is represented in the new basis as , where

Accordingly, the quadratic form can be written as

where

Note that . Our goal is to determine conditions on the new basis {v1,v2,...,vn} such that ij = 0 for i ≠ j.
We seek the new basis in the form

Observe that for j = 1,...,i − 1, if

then

Our goal then is to determine the coefficients αi1,αi2,...,αii, i = 1,...,n, such that the vector

satisfies the i relations

In this case, we get

For each i = 1,...,n, the i relations above determine the coefficients αi1,...,αii in a unique way. Indeed, upon substituting the expression for vi into the equations above, we obtain the set of equations

The set of equations above can be expressed in matrix form as

If the leading principal minors of the matrix Q do not vanish, then the coefficients αij can be obtained using Cramer's rule. In particular,

Hence,

In the new basis, the quadratic form can be expressed as a sum of squares

We now show that a necessary and sufficient condition for the quadratic form to be positive definite is Δi > 0, i = 1,...,n.
Sufficiency is clear, for if Δi > 0, i = 1,...,n, then by the previous argument there is a basis such that

for any x ≠ 0 (or, equivalently, any  ≠ 0).
To prove necessity, we first show that for i = 1,...,n, we have Δi ≠ 0. To see this, suppose that Δk = 0 for some k. Note that Δk = det Qk,

Then, there exists a vector v  k,v  0, such that vQk = 0. Now let x  n be given by x = [v,0]. Then,

But x ≠ 0, which contradicts the fact that the quadratic form f is positive definite. Therefore, if xQx > 0, then Δi ≠ 0, i = 1,...,n. Then, using our previous argument, we may write

where  = [v1,...,vn]x. Hence, if the quadratic form is positive definite, then all leading principal minors must be positive.
Note that if Q is not symmetric, Sylvester's criterion cannot be used to check positive definiteness of the quadratic form xQx. To see this, consider an example where

The leading principal minors of Q are Δ1 = 1 > 0 and Δ2 = det Q = 1 > 0. However, if x = [1,1], then xQx = −2 < 0, and hence the associated quadratic form is not positive definite. Note that

The leading principal minors of Q0 are Δ1 = 1 > 0 and Δ2 = det Q0 = −3 < 0, as expected.
A necessary condition for a real quadratic form to be positive semidefinite is that the leading principal minors be nonnegative. However, this is not a sufficient condition (see Exercise 3.16). In fact, a real quadratic form is positive semidefinite if and only if all principal minors are nonnegative (for a proof of this fact, see [44, p. 307]).
A symmetric matrix Q is said to be positive definite if the quadratic form xQx is positive definite. If Q is positive definite, we write Q > 0. Similarly, we define a symmetric matrix Q to be positive semidefinite (Q ≥ 0), negative definite (Q < 0), and negative semidefinite (Q ≤ 0) if the corresponding quadratic forms have the respective properties. The symmetric matrix Q is indefinite if it is neither positive semidefinite nor negative semidefinite. Note that the matrix Q is positive definite (semidefinite) if and only if the matrix −Q is negative definite (semidefinite).
Sylvester's criterion provides a way of checking the definiteness of a quadratic form, or equivalently, a symmetric matrix. An alternative method involves checking the eigenvalues of Q, as stated below.
Theorem 3.7 A symmetric matrix Q is positive definite (or positive semidefinite) if and only if all eigenvalues of Q are positive (or nonnegative).
Proof. For any x, let y = T−1x = Tx, where T is an orthogonal matrix whose columns are eigenvectors of Q. Then, xQx = yT QTy = Σni=1 λiy2i. For this, the result follows.
Through diagonalization, we can show that a symmetric positive semidefinite matrix Q has a positive semidefinite (symmetric) square root Q1/2 satisfying Q1/2Q1/2 = Q. For this, we use T as above and define

which is easily verified to have the desired properties. Note that the quadratic form xQx can be expressed as ||Q1/2x||2.
In summary, we have presented two tests for definiteness of quadratic forms and symmetric matrices. We point out again that nonnegativity of leading principal minors is a necessary but not a sufficient condition for positive semidefiniteness.
3.5 Matrix Norms
The norm of a matrix may be chosen in a variety of ways. Because the set of matrices mxn can be viewed as the real vector space mn, matrix norms should be no different from regular vector norms. Therefore, we define the norm of a matrix A, denoted ||A||, to be any function ||·|| that satisfies the following conditions:

1. ||A|| > 0 if A ≠ O, and ||0|| = 0, where O is a matrix with all entries equal to zero.
2. ||cA|| = |c|||A||, for any c  .
3. ||A + B||≤||A|| + ||B||.

An example of a matrix norm is the Frobenius norm, defined as

where A  m × n. Note that the Frobenius norm is equivalent to the Euclidean norm on mn.
For our purposes, we consider only matrix norms that satisfy the following additional condition:

4. ||AB||≤||A||||B||.

It turns out that the Frobenius norm satisfies condition 4 as well.
In many problems, both matrices and vectors appear simultaneously. Therefore, it is convenient to construct the norm of a matrix in such a way that it will be related to vector norms. To this end we consider a special class of matrix norms, called induced norms. Let ||·||(n) and ||·||(m) be vector norms on n and m, respectively. We say that the matrix norm is induced by, or is compatible with, the given vector norms if for any matrix A  m × n and any vector x  n, the following inequality is satisfied:

We can define an induced matrix norm as

that is, ||A|| is the maximum of the norms of the vectors Ax where the vector x runs over the set of all vectors with unit norm. When there is no ambiguity, we omit the subscripts (m) and (n) from ||·||(m) and ||·||(n).
Because of the continuity of a vector norm (see Exercise 2.10), for each matrix A the maximum

is attainable; that is, a vector x0 exists such that ||x0|| = 1 and ||Ax0|| = ||A||. This fact follows from the theorem of Weierstrass (see Theorem 4.2).
The induced norm satisfies conditions 1 to 4 and the compatibility condition, as we prove below.
Proof of Condition 1. Let A ≠ O. Then, a vector x, ||x|| = 1, can be found such that Ax ≠ 0, and thus ||Ax|| ≠ 0. Hence, ||A|| = max||x||=1 ||Ax|| ≠ 0. If, on the other hand, A = O, then ||A|| = max||x||=1 ||Ox|| = 0.
Proof of Condition 2. By definition, ||cA|| = max||x||=1 ||cAx||. Obviously, ||cAx|| = |c|||Ax||, and therefore ||cA|| = max||x||=1 |c|||Ax|| = |c|max||x||=1 ||Ax|| = |c|||A||.
Proof of Compatibility Condition. Let y ≠ 0 be any vector. Then, x = y/||y|| satisfies the condition ||x|| = 1. Consequently, ||Ay|| = ||A(||y||x)|| = ||y||||Ax||≤||y||||A||.
Proof of Condition 3. For the matrix A + B, we can find a vector x0 such that ||A + B|| = ||(A + B)x0|| and ||x0|| = 1. Then, we have

which shows that condition 3 holds.
Proof of Condition 4. For the matrix AB, we can find a vector x0 such that ||x0|| = 1 and ||ABx0|| = ||AB||. Then, we have

which shows that condition 4 holds.
Theorem 3.8 Let

The matrix norm induced by this vector norm is

where λ1 is the largest eigenvalue of the matrix A A.
Proof. We have

The matrix AA is symmetric and positive semidefinite. Let λ1 ≥ λ2 ≥ · · · ≥ λn ≥ 0 be its eigenvalues and x1,x2,...,xn the orthonormal set of the eigenvectors corresponding to these eigenvalues. Now, we take an arbitrary vector x with ||x|| = 1 and represent it as a linear combination of xi, i = 1,...,n:

Note that

Furthermore,

For the eigenvector x1 of AA corresponding to the eigenvalue λ1, we have

and hence

This completes the proof.
Using arguments similar to the above, we can deduce the following important inequalities.
Rayleigh's Inequalities. If an n × n matrix P is real symmetric positive definite, then

where λmin(P) denotes the smallest eigenvalue of P, and λmax(P) denotes the largest eigenvalue of P.
Example 3.1 Consider the matrix

and let the norm in 2 be given by

Then,

and det [λI2 − AA] = λ2 − 10λ + 9 = (λ − 1) (λ − 9). Thus, ||A|| =  = 3.
The eigenvector of AA corresponding to λ1 = 9 is

Note that ||Ax1|| = ||A||. Indeed,

Because A = A in this example, we also have ||A|| = max1≤i≤n |λi(A)|, where λ1(A),...,λn(A) are the eigenvalues of A (possibly repeated).
Warning: In general, max1≤i≤n |λi(A)| ≠ ||A||. Instead, we have ||A|| ≥ max1≤i≤n |λi(A)|, as illustrated in the following example (see also Exercise 5.2).
Example 3.2 Let

then

and

Note that 0 is the only eigenvalue of A. Thus, for i = 1, 2, ||A|| = 1 > |λi(A)| = 0.
For a more complete but still basic treatment of topics in linear algebra as discussed in this and the preceding chapter, see [47], [66], [95], [126]. For a treatment of matrices, we refer the reader to [44], [62]. Numerical aspects of matrix computations are discussed in [41], [53].
EXERCISES

3.1 Fix a vector in n. Let x be the column of the coordinates of the vector with respect to the basis {e1,e2,...,en} and x′ the coordinates of the same vector with respect to the basis {e′1,e′2,...,e′n}. Show that x′ = Tx, where T is the transformation matrix from {e1,e2,...,en} to {e′1,e′2,...,e′n}.
3.2 For each of the following cases, find the transformation matrix T from {e1,e2,e3} to {e′1,e′2,e′3}:
a. 
b. 
3.3 Consider two bases of 3, {e1,e2,e3} and {e′1,e′2,e3}, where e = 2e′1 + e′2 − e′3,e2 = 2e′1 − e′2 − 2e′3, and e3 = 3e′1 + e′3. Suppose that a linear transformation has a matrix representation in {e1,e2,e3} of the form

Find the matrix representation of this linear transformation in the basis {e′1,e′2,e′3}.
3.4 Consider two bases of 4, {e1,e2,e3e4} and e′1,e′2,e′3e′4, where e′1 = e1,e′2 = e1 + e2,e′3 = e1 + e2 + e3, and e′4 = e1 + e2 + e3 + e4.Suppose that a linear transformation has a matrix representation in {e1,e2,e3e4} of the form

Find the matrix representation of this linear transformation in the basis {e′1,e′2,e′3e′4}
3.5 Consider a linear transformation given by the matrix

Find a basis for 4 with respect to which the matrix representation for the linear transformation above is diagonal.
3.6 Let λ1,...,λn be the eigenvalues of the matrix A  n × n. Show that the eigenvalues of the matrix In − A are 1 − λ1,...,1 − λn.
3.7 Let  be a subspace. Show that ⊥ is also a subspace.
3.8 Find the nullspace of

3.9 Let A  m × n be a matrix. Show that (A) is a subspace of m and (A) is a subspace of n.
3.10 Prove that if A and B are two matrices with m rows, and (A) ⊂ (B), then (B) ⊂ (A).
Hint: Use the fact that for any matrix M with m rows, we have dim (M) + dim (M) = m [this is one of the fundamental theorems of linear algebra (see [126, p. 75])].
3.11 Let  be a subspace. Show that (⊥)⊥ = .
Hint: Use Exercise 3.10.
3.12 Let  and  be subspaces. Show that if  ⊂ , then ⊥ ⊂ ⊥.
3.13 Let  be a subspace of n. Show that there exist matrices V and U such that  = (V) = (U).
3.14 Let P be an orthogonal projector onto a subspace . Show that
a. Px = x for all x  .
b. (P) = .
3.15 Is the quadratic form

positive definite, positive semidefinite, negative definite, negative semidefinite, or indefinite?
3.16 Let

Show that although all leading principal minors of A are nonnegative, A is not positive semidefinite.
3.17 Consider the matrix

a. Is this matrix positive definite, negative definite, or indefinite?
b. Is this matrix positive definite, negative definite, or indefinite on the subspace

3.18 For each of the following quadratic forms, determine if it is positive definite, negative definite, positive semidefinite, negative semidefinite, or in definite.
a. f(x1,x2,x3) = x22
b. f(x1,x2,x3) = x21 + 2x22 − x1x3
c. f(x1,x2,x3) = x21 + x23 + 2x1x2 + 2x1x3 + 2x2x3
3.19 Find a transformation that brings the following quadratic form into the diagonal form:

Hint: Read the proof of Theorem 3.6.
3.20 Consider the quadratic form

Find the values of the parameter ξ for which this quadratic form is positive definite.
3.21 Consider the function ·,·Q : n × n → , defined by x,yQ = xQy, where x,y  n and Q  n × n is a symmetric positive definite matrix. Show that ·,·Q satisfies conditions 1 to 4 for inner products (see Section 2.4).
3.22 Consider the vector norm ||·||∞ on n given by ||x||∞ = maxi |xi|, where x = [x1,...,xn]. Define the norm ||·||∞ on m similarly. Show that the matrix norm induced by these vector norms is given by

where aij is the (i,j)th element of A  m × n.
3.23 Consider the vector norm ||·||1 on n given by ||x||1 = Σni=1 |xi||, where x = [x1,...,xn]. Define the norm ||·||1 on m similarly. Show that the matrix norm induced by these vector norms is given by

where aij is the (i,j)th element of A  m × n.








Chapter 4
CONCEPTS FROM GEOMETRY
4.1 Line Segments
In the following analysis we concern ourselves only with n. The elements of this space are the n-component vectors x = [x1, x2,..., xn].
The line segment between two points x and y in n is the set of points on the straight line joining points x and y (see Figure 4.1). Note that if z lies on the line segment between x and y, then

Figure 4.1 Line segment.



where α is a real number from the interval [0,1]. The equation above can be rewritten as z = αx + (1 − α)y. Hence, the line segment between x and y can be represented as

4.2 Hyperplanes and Linear Varieties
Let u1, u2,..., un, v  , where at least one of the ui is nonzero. The set of all points x = [x1, x2,..., xn] that satisfy the linear equation

is called a hyperplane of the space n. We may describe the hyperplane by

where

A hyperplane is not necessarily a subspace of n since, in general, it does not contain the origin. For n = 2, the equation of the hyperplane has the form u1 x1 + u2x2 = v, which is the equation of a straight line. Thus, straight lines are hyperplanes in 2. In 3 (three-dimensional space), hyperplanes are ordinary planes. By translating a hyperplane so that it contains the origin of n, it becomes a subspace of n (see Figure 4.2). Because the dimension of this subspace is n − 1, we say that the hyperplane has dimension n − 1.

Figure 4.2 Translation of a hyperplane.


The hyperplane H = {x : u1x1 + · · · + unxn = v} divides n into two half-spaces. One of these half-spaces consists of the points satisfying the inequality u1x1 + u2x2 + · · · + unxn ≥ v, denoted

where, as before,

The other half-space consists of the points satisfying the inequality u1x1 + u2x2 · · · + unxn ≤ v, denoted

The half-space H+ is called the positive half-space, and the half-space H- is called the negative half-space.
Let a = [a1, a2,..., an] be an arbitrary point of the hyperplane H. Thus, u a − v = 0. We can write

The numbers (x1 − ai), i = 1,..., n, are the components of the vector x − a. Therefore, the hyperplane H consists of the points x for which u, x − a = 0. In other words, the hyperplane H consists of the points x for which the vectors u and x − a are orthogonal (see Figure 4.3). We call the vector u the normal to the hyperplane H. The set H+ consists of those points x for which u, x − a ≥ 0, and H− consists of those points x for which u, x − a ≤ 0.

Figure 4.3 The hyperplane H = {x  n : u (x − a) = 0}.


A linear variety is a set of the form

for some matrix A  m × n and vector b  m. If dim (A) = r, we say that the linear variety has dimension r. A linear variety is a subspace if and only if b = 0. If A = O, the linear variety is n. If the dimension of the linear variety is less than n, then it is the intersection of a finite number of hyperplanes.
4.3 Convex Sets
Recall that the line segment between two points u, v  n is the set {w  n : w = αu + (1 − α) v, α  [0, 1]}. A point w = αu + (1 − α)v (where α  [0, 1]) is called a convex combination of the points u and v.
A set Θ ⊂ n is convex if for all u, v  Θ, the line segment between u and v is in Θ. Figure 4.4 gives examples of convex sets, whereas Figure 4.5 gives examples of sets that are not convex. Note that Θ is convex if and only if αu + (1 − α)v  Θ for all u, v  Θ and α  (0,1).

Figure 4.4 Convex sets.



Figure 4.5 Sets that are not convex.


Examples of convex sets include the following:

 The empty set
 A set consisting of a single point
 A line or a line segment


 A subspace
 A hyperplane
 A linear variety
 A half-space
 n

Theorem 4.1 Convex subsets of n have the following properties:

a. If Θ is a convex set and β is a real number, then the set

is also convex.
b. If Θ1 and Θ2 are convex sets, then the set

is also convex.
c. The intersection of any collection of convex sets is convex (see Figure 4.6 for an illustration of this result for two sets).

Figure 4.6 Intersection of two convex sets.



Proof.

a. Let βv1, βv2  βΘ, where v1, v2  Θ. Because Θ is convex, we have αv1 + (1 − α)v2  Θ for any α  (0, 1). Hence,

and thus βΘ is convex.
b. Let v1, v2  Θ1 + Θ2. Then, v1 = v′1 + v″1, and v2 = v′2 + v″2, where v′1, v′2  Θ1, and v″1, v″2  Θ2. Because Θ1 and Θ2 are convex, for all α  (0,1),

and

By definition of Θ1 + Θ2, x1 + x2  Θ1 + Θ2. Now,

Hence, Θ1 + Θ2 is convex.
c. Let C be a collection of convex sets. Let  (where  represents the intersection of all elements in C). Then, x1, x2  Θ for each Θ  C. Because each Θ  C is convex, αx1 + (1 − α)x2  Θ for all α  (0, 1) and each  Θ C. Thus, .

A point x in a convex set Θ is said to be an extreme point of Θ if there are no two distinct points u and v in Θ such that x = αu + (1 − α)v for some α  (0, 1). For example, in Figure 4.4, any point on the boundary of the disk is an extreme point, the vertex (corner) of the set on the right is an extreme point, and the endpoint of the half-line is also an extreme point.
4.4 Neighborhoods
A neighborhood of a point x  n is the set

where ε is some positive number. The neighborhood is also called a ball with radius ε and center x.
In the plane 2, a neighborhood of x = [x1, x2] consists of all the points inside a disk centered at x. In 3, a neighborhood of x = [x1, x2, x3] consists of all the points inside a sphere centered at x (see Figure 4.7).

Figure 4.7 Examples of neighborhoods of a point in 2 and 3.


A point x  S is said to be an interior point of the set S if the set S contains some neighborhood of x; that is, if all points within some neighborhood of x are also in S (see Figure 4.8). The set of all the interior points of S is called the interior of S.

Figure 4.8 x is an interior point; y is a boundary point.


A point x is said to be a boundary point of the set S if every neighborhood of x contains a point in S and a point not in S (see Figure 4.8). Note that a boundary point of S may or may not be an element of S. The set of all boundary points of S is called the boundary of S.
A set S is said to be open if it contains a neighborhood of each of its points; that is, if each of its points is an interior point, or equivalently, if S contains no boundary points.
A set S is said to be closed if it contains its boundary (see Figure 4.9). We can show that a set is closed if and only if its complement is open.

Figure 4.9 Open and closed sets.


A set that is contained in a ball of finite radius is said to be bounded. A set is compact if it is both closed and bounded. Compact sets are important in optimization problems for the following reason.
Theorem 4.2 Theorem of Weierstrass. Let f : Ω, →  be a continuous function, where Ω, ⊂ n is a compact set Then, there exists a point x0  Ω such that f(x0) ≤ f(x) for all x  Ω. In other words, f achieves its minimum on Ω.
Proof. See [112, p. 89] or [2, p. 154].
4.5 Polytopes and Polyhedra
Let Θ be a convex set, and suppose that y is a boundary point of Θ. A hyperplane passing through y is called a hyperplane of support (or supporting hyperplane) of the set Θ if the entire set Θ lies completely in one of the two half-spaces into which this hyperplane divides the space n.
Recall that by Theorem 4.1, the intersection of any number of convex sets is convex. In what follows we are concerned with the intersection of a finite number of half-spaces. Because every half-space H+ or H- is convex in n, the intersection of any number of half-spaces is a convex set.
A set that can be expressed as the intersection of a finite number of half-spaces is called a convex polytope (see Figure 4.10).

Figure 4.10 Polytopes.


A nonempty bounded polytope is called a polyhedron (see Figure 4.11).

Figure 4.11 One-dimensional polyhedron.


For every convex polyhedron Θ ⊂ n, there exists a nonnegative integer k ≤ n such that Θ is contained in a linear variety of dimension k, but is not entirely contained in any (k − 1)-dimensional linear variety of n. Furthermore, there exists only one k-dimensional linear variety containing Θ, called the carrier of the polyhedron Θ, and k is called the dimension of Θ. For example, a zero-dimensional polyhedron is a point of n, and its carrier is itself. A one-dimensional polyhedron is a segment, and its carrier is the straight line on which it lies. The boundary of any k-dimensional polyhedron, k > 0, consists of a finite number of (k − 1)-dimensional polyhedra. For example, the boundary of a one-dimensional polyhedron consists of two points that are the endpoints of the segment.
The (k − 1)-dimensional polyhedra forming the boundary of a k-dimensional polyhedron are called the faces of the polyhedron. Each of these faces has, in turn, (k − 2)-dimensional faces. We also consider each of these (k − 2)-dimensional faces to be faces of the original k-dimensional polyhedron. Thus, every k-dimensional polyhedron has faces of dimensions k − 1, k − 2,..., 1, 0. A zero-dimensional face of a polyhedron is called a vertex, and a one-dimensional face is called an edge.
EXERCISES

4.1 Show that a set S ⊂ n is a linear variety if and only if for all x, y  S and α  , we have αx + (1 − α)y  S.
4.2 Show that the set {x  n : ||x|| ≤ r} is convex, where r > 0 is a given real number and  is the Euclidean norm of x  n.
4.3 Show that for any matrix A  m × n and vector b  m, the set (linear variety) {x  n : Ax = b} is convex.
4.4 Show that the set {x  n : x ≥ 0} is convex (where x ≥ 0 means that every component of x is nonnegative).








CHAPTER 5
ELEMENTS OF CALCULUS
5.1 Sequences and Limits
A sequence of real numbers is a function whose domain is the set of natural numbers 1, 2,..., k,... and whose range is contained in . Thus, a sequence of real numbers can be viewed as a set of numbers {x1, x2,..., xk,...}, which is often also denoted as {xk} (or sometimes as , to indicate explicitly the range of values that k can take).
A sequence {xk} is increasing if x1 < x2 < · · · < xk · · ·; that is, xk < xk+1 for all k. If xk ≤ xk+1, then we say that the sequence is nondecreasing. Similarly, we can define decreasing and nonincreasing sequences. Nonincreasing or nondecreasing sequences are called monotone sequences.
A number x*   is called the limit of the sequence {xk} if for any positive ε there is a number K (which may depend on ε) such that for all k > K, |xk − x*| < ε; that is, xk lies between x* − ε and x* + ε for all k > K. In this case we write

or

A sequence that has a limit is called a convergent sequence.
The notion of a sequence can be extended to sequences with elements in n. Specifically, a sequence in n is a function whose domain is the set of natural numbers 1, 2,..., k,... and whose range is contained in n. We use the notation {x(1), x(2),...} or {x(k)} for sequences in n. For limits of sequences in n, we need to replace absolute values with vector norms. In other words, x* is the limit of {x(k)} if for any positive ε there is a number K (which may depend on ε) such that for all k > K, ||x(k) − x*|| < ε. As before, if a sequence {x(k)} is convergent, we write x* = lim(k)k→∞ or x(k) → x*.
Theorem 5.1 A convergent sequence has only one limit.
Proof. We prove this result by contradiction. Suppose that a sequence {x(k)} has two different limits, say x1 and x2. Then, we have ||x1 − x2|| > 0. Let

From the definition of a limit, there exist K1 and K2 such that for k > K1 we have ||x(k) − x1|| < ε, and for k > K2 we have ||x(k) − x2|| < ε. Let K = max{K1, K2}. Then, if k > K, we have ||x(k) − x1 < ε and ||x(k) − x2|| < ε. Adding ||x(k) − x1|| < ε and ||x(k) − x2|| < ε yields

Applying the triangle inequality gives

Therefore,

However, this contradicts the assumption that ||x1 − x2|| = 2ε, which completes the proof.
A sequence {x(k)} in n is bounded if there exists a number B ≥ 0 such that ||x(k)|| ≤ B for all k = 1, 2,....
Theorem 5.2 Every convergent sequence is bounded.
Proof. Let {x(k)} be a convergent sequence with limit x*. Choose ε = 1. Then, by definition of the limit, there exists a natural number K such that for all k > K,

By the result of Exercise 2.9, we get

Therefore,

Letting

we have

which means that the sequence {x(k)} is bounded.
For a sequence {xk} in , a number B is called an upper bound if xk ≤ B for all k = 1, 2,.... In this case, we say that {xk} is bounded above. Similarly, B is called a lower bound if xk ≥ B for all k = 1,2,.... In this case, we say that {xk} is bounded below. Clearly, a sequence is bounded if it is both bounded above and bounded below.
Any sequence {xk} in  that has an upper bound has a least upper bound (also called the supremum), which is the smallest number B that is an upper bound of {xk}. Similarly, any sequence {xk} in  that has a lower bound has a greatest lower bound (also called the infimum). If B is the least upper bound of the sequence {xk}, then xk ≤ B for all k, and for any ε > 0, there exists a number K such that xK > B − ε. An analogous statement applies to the greatest lower bound: If B is the greatest lower bound of {xk}, then xk ≥ B for all k, and for any ε > 0, there exists a number K such that xK < B + ε.
Theorem 5.3 Every monotone bounded sequence in  is convergent.
Proof. We prove the theorem for nondecreasing sequences. The proof for nonincreasing sequences is analogous.
Let {xk} be a bounded nondecreasing sequence in  and x* the least upper bound. Fix a number ε > 0. Then, there exists a number K such that xK > x* − ε. Because {xk} is nondecreasing, for any k ≥ K,

Also, because x* is an upper bound of {xk}, we have

Therefore, for any k ≥ K,

which means that xk → x*.
Suppose that we are given a sequence {x(k)} and an increasing sequence of natural numbers {mk}. The sequence

is called a subsequence of the sequence {x(k)}. A subsequence of a given sequence can thus be obtained by neglecting some elements of the given sequence.
Theorem 5.4 Consider a convergent sequence {x(k)} with limit x*. Then, any subsequence of {x(k)} also converges to x*.
Proof. Let {x(mk)} be a subsequence of {x(k)}, where {mk} is an increasing sequence of natural numbers. Observe that mk ≥ k for all k = 1, 2,.... To show this, first note that m1 ≥ 1 because m1 is a natural number. Next, we proceed by induction by assuming that mk. ≥ k. Then, we have mk+1 > mk ≥ k, which implies that mk+1 ≥ k + 1. Therefore, we have shown that mk ≥ k for all k = 1, 2,....
Let ε > 0 be given. Then, by definition of the limit, there exists K such that ||x(k) − x*|| < ε for any k > K. Because mk ≥ k, we also have ||x(mk) − x*|| < ε for any k > K. This means that

It turns out that any bounded sequence contains a convergent subsequence. This result is called the Bolzano-Weierstrass theorem (see [2, p. 70]).
Consider a function  and a point x0  n. Suppose that there exists f such that for any convergent sequence {x(k)} with limit x0, we have

Then, we use the notation

to represent the limit f*.
It turns out that f is continuous at x0 if and only if for any convergent sequence {x(k)} with limit x0, we have

(see [2, p. 137]). Therefore, using the notation introduced above, the function f is continuous at x0 if and only if

We end this section with some results involving sequences and limits of matrices. These results are useful in the analysis of algorithms (e.g., Newton's algorithm in Chapter 9).
We say that a sequence {Ak} of m × n matrices converges to the m × n matrix A if

Lemma 5.1 Let A  n × n. Then, limk→∞ Ak = O if and only if the eigenvalues of A satisfy |λi(A)| < 1, i = 1,..., n.
Proof. To prove this theorem, we use the Jordan form (see, e.g., [47]). Specifically, it is well known that any square matrix is similar to the Jordan form: There exists a nonsingular T such that

where Jr(λ) is the r × r matrix:

The λ1,..., λq above are distinct eigenvalues of A, the multiplicity of λ1 is m1 + · · · + ms, and so on.
We may rewrite the above as A = T−1 JT. To complete the proof, observe that

where

Furthermore,

Hence,

if and only if |λi| < 1, i = 1,..., n.
Lemma 5.2 The series of n × n matrices

converges if and only if limk→∞ Ak = O. In this case the sum of the series equals (In − A)−1.
Proof. The necessity of the condition is obvious.
To prove the sufficiency, suppose that limk→∞ A = O. By Lemma 5.1 we deduce that |λi(A)| < 1, i = 1,..., n. This implies that det(In − A) ≠ 0, and hence (In − A)−1 exists. Consider now the following relation:

Postmultiplying the equation above by (In − A)−1 yields

Hence,

because limk→∞ Ak+1 = O. Thus,

which completes the proof.
A matrix-valued function  is continuous at a point ξ0  r if

Lemma 5.3 Let A : r → n × n be an n × n matrix-valued function that is continuous at ξ0. If A (ξ0)−1 exists, then A(ξ)−1 exists for ξ sufficiently close to ξ0 and A(·)−1 is continuous at ξ0.
Proof. We follow [114]. We first prove the existence of A(ξ)−1 for all ξ sufficiently close to ξ0. We have

where

Thus,

and

Because A is continuous at ξ0, for all ξ close enough to ξ0, we have

where θ  (0,1). Then,

and

exists. But then

which means that A(ξ)−1 exists for ξ sufficiently close to ξ0.
To prove the continuity of A(·)−1 note that

However, since ||K(ξ)|| < 1, it follows from Lemma 5.2 that

Hence,

when ||K(ξ)|| < 1. Therefore,

Because

we obtain

which completes the proof.
5.2 Differentiability
Differential calculus is based on the idea of approximating an arbitrary function by an affine function. A function  : n → m is affine if there exists a linear function  : n → m and a vector y  m such that

for every x  n. Consider a function f : n → m and a point x0  n. We wish to find an affine function  that approximates f near the point x0. First, it is natural to impose the condition

Because (x) = (x) + y, we obtain y = f(x0) − (x0). By the linearity of 

Hence, we may write

Next, we require that (x) approaches f(x) faster than x approaches x0; that is,

The conditions above on  ensure that  approximates f near x0 in the sense that the error in the approximation at a given point is "small" compared with the distance of the point from x0.
In summary, a function f : Ω → m, Ω ⊂ n, is said to be differentiable at x0  Ω if there is an affine function that approximates f near x0; that is, there exists a linear function  : n → m such that

The linear function  above is determined uniquely by f and x0 and is called the derivative of f at x0. The function f is said to be differentiable on Ω if f is differentiable at every point of its domain Ω.
In , an affine function has the form ax + b, with a, b  . Hence, a real-valued function f(x) of a real variable x that is differentiable at x0 can be approximated near x0 by a function

Because f(x0) = (x0) = ax0 + b, we obtain

The linear part of (x), denoted earlier by (x), is in this case just ax. The norm of a real number is its absolute value, so by the definition of differentiability we have

which is equivalent to

The number a is commonly denoted f′(x0) and is called the derivative of f at x0. The affine function  is therefore given by

This affine function is tangent to f at x0 (see Figure 5.1).

Figure 5.1 Illustration of the notion of the derivative.


5.3 The Derivative Matrix
Any linear transformation from n to m, and in particular the derivative  of f : n → m, can be represented by an m × n matrix. To find the matrix representation L of the derivative  of a differentiate function f : n → m, we use the natural basis {e1,..., en} for n. Consider the vectors

By the definition of the derivative, we have

for j = 1,..., n. This means that

for j = 1,..., n. But Lej is the jth column of the matrix L. On the other hand, the vector xj differs from x0 only in the jth coordinate, and in that coordinate the difference is just the number t. Therefore, the left side of the preceding equation is the partial derivative

Because vector limits are computed by taking the limit of each coordinate function, it follows that if

then

and the matrix L has the form

The matrix L is called the Jacobian matrix, or derivative matrix, of f at x0, and is denoted Df(x0). For convenience, we often refer to Df(x0) simply as the derivative of f at x0. We summarize the foregoing discussion in the following theorem.
Theorem 5.5 If a function f : n → m is differentiate at x0, then the derivative of f at x0 is determined uniquely and is represented by the m × n derivative matrix Df(x0). The best affine approximation to f near x0 is then given by

in the sense that

and limx→x0 ||r(x)||/||x − x0|| = 0. The columns of the derivative matrix Df(x0) are vector partial derivatives. The vector

is a tangent vector at x0 to the curve f obtained by varying only the jth coordinate of x.
If f : n →  is differentiable, then the function ∇f defined by

is called the gradient of f. The gradient is a function from n to n, and can be pictured as a vector field, by drawing the arrow representing ∇f(x) so that its tail starts at x.
Given f : n → , if ∇f is differentiable, we say that f is twice differentiable, and we write the derivative of ∇f as

(The notation  represents taking the partial derivative of f with respect to xj first, then with respect to xi.) The matrix D2f(x) is called the Hessian matrix of f at x, and is often also denoted F(x).
A function f : Ω → m, Ω ⊂ n, is said to be continuously differentiable on Ω if it is differentiable (on Ω), and Df : Ω → mxn is continuous; that is, the components of f have continuous partial derivatives. In this case, we write f  1. If the components of f have continuous partial derivatives of order p, then we write f  p.
Note that the Hessian matrix of a function f : n →  at x is symmetric if f is twice continuously differentiable at x. This is a well-known result from calculus called Clairaut's theorem or Schwarz's theorem. However, if the second partial derivatives of f are not continuous, then there is no guarantee that the Hessian is symmetric, as shown in the following well-known example.
Example 5.1 Consider the function

Let us compute its Hessian at the point 0 = [0,0]. We have

We now proceed with computing the components of the Hessian and evaluating them at the point [0,0] one by one. We start with

where

Note that

Hence,

Also,

Hence, the mixed partial is

We next compute

where

Note that

Hence,

Also,

Hence, the mixed partial is

Therefore, the Hessian evaluated at the point 0 is

which is not symmetric.
5.4 Differentiation Rules
We now introduce the chain rule for differentiating the composition g(f(t)), of a function f :  → n and a function g : n → .
Theorem 5.6 Let g :  →  be differentiable on an open set  ⊂ n, and let f : (a, b) →  be differentiable on (a, b). Then, the composite function h : (a, b) →  given by h(t) = g(f(t)) is differentiable on (a, b), and

Proof. By definition,

if the limit exists. By Theorem 5.5 we write

where lims→tr(s)/(s − t) = 0. Therefore,

Letting s → t yields

Next, we present the product rule. Let f : n → m and g : n → m be two differentiable functions. Define the function h : n →  by h(x) = f(x)g(x). Then, h is also differentiable and

We end this section with a list of some useful formulas from multivariable calculus. In each case, we compute the derivative with respect to x. Let A  mxn be a given matrix and y  m a given vector. Then,

It follows from the first formula above that if y  n, then

It follows from the second formula above that if Q is a symmetric matrix, then

In particular,

5.5 Level Sets and Gradients
The level set of a function f : n →  at level c is the set of points

For f : 2 → , we are usually interested in S when it is a curve. For f : 3 → , the sets S most often considered are surfaces.
Example 5.2 Consider the following real-valued function on 2:

The function above is called Rosenbrock's function. A plot of the function f is shown in Figure 5.2. The level sets of f at levels 0.7, 7, 70, 200, and 700 are depicted in Figure 5.3. These level sets have a particular shape resembling bananas. For this reason, Rosenbrock's function is also called the banana function.

Figure 5.2 Graph of Rosenbrock's function.



Figure 5.3 Level sets of Rosenbrock's (banana) function.


To say that a point x0 is on the level set S at level c means that f(x0) = c. Now suppose that there is a curve γ lying in S and parameterized by a continuously differentiate function g :  → n. Suppose also that g(t0) = x0 and Dg(t0) = v ≠ 0, so that v is a tangent vector to γ at x0 (see Figure 5.4). Applying the chain rule to the function h(t) = f(g(t)) at t0 gives

Figure 5.4 Orthogonality of the gradient to the level set.



But since γ lies on S, we have

that is, h is constant. Thus, h′(t0) = 0 and

Hence, we have proved, assuming f continuously differentiate, the following theorem (see Figure 5.4).
Theorem 5.7 The vector ∇f(x0) is orthogonal to the tangent vector to an arbitrary smooth curve passing through x0 on the level set determined by f(x) = f(x0).
It is natural to say that ∇f(x0) is orthogonal or normal to the level set S corresponding to x0, and it is also natural to take as the tangent plane (or line) to S at x0 the set of all points x satisfying

As we shall see later, ∇f(x0) is the direction of maximum rate of increase of f at x0. Because ∇f(x0) is orthogonal to the level set through x0 determined by f(x) = f(x0), we deduce the following fact: The direction of maximum rate of increase of a real-valued differentiable function at a point is orthogonal to the level set of the function through that point.
Figure 5.5 illustrates the discussion above for the case f : 2 → . The curve on the shaded surface in Figure 5.5 running from bottom to top has the property that its projection onto the (x1,x2)-plane is always orthogonal to the level curves and is called a path of steepest ascent because it always heads in the direction of maximum rate of increase for f.

Figure 5.5 Illustration of a path of steepest ascent.


The graph of f : n →  is the set {[x, f(x)] : x  n} ⊂ n+1. The notion of the gradient of a function has an alternative useful interpretation in terms of the tangent hyperplane to its graph. To proceed, let x0  n and z0 = f(x0). The point  is a point on the graph of f. If f is differentiate at ξ, then the graph admits a nonvertical tangent hyperplane at . The hyperplane through ξ is the set of all points [x1,..., xn, z]  n+1 satisfying the equation

where the vector [u1,...,un,v]  n+1 is normal to the hyperplane. Assuming that this hyperplane is nonvertical (that is, v ≠ 0), let

Thus, we can rewrite the hyperplane equation above as

We can think of the right side of the above equation as a function z : n → . Observe that for the hyperplane to be tangent to the graph of f, the functions f and z must have the same partial derivatives at the point x0. Hence, if f is differentiable at x0, its tangent hyperplane can be written in terms of its gradient, as given by the equation

5.6 Taylor Series
The basis for many numerical methods and models for optimization is Taylor's formula, which is given by Taylor's theorem.
Theorem 5.8 Taylor's Theorem. Assume that a function f :  →  is m times continuously differentiable (i.e., f  m) on an interval [a, b]. Denote h = b − a. Then,

(called Taylor's formula) where f(i) is the ith derivative of f, and

with θ, θ′  (0,1).
Proof. We have

Denote by gm(x) an auxiliary function obtained from Rm by replacing a by x. Hence,

Differentiating gm(x) yields

Observe that gm(b) = 0 and gm(a) = Rm. Applying the mean-value theorem yields

where θ  (0,1). The equation above is equivalent to

Hence,

To derive the formula

see, e.g., [81] or [83].
An important property of Taylor's theorem arises from the form of the remainder Rm. To discuss this property further, we introduce the order symbols, O and o.
Let g be a real-valued function defined in some neighborhood of 0  n, with g(x) ≠ 0 if x ≠ 0. Let f : Ω → m be defined in a domain Ω  n that includes 0. Then, we write

1. f(x) = O(g(x)) to mean that the quotient ||f(x)||/|g(x)| is bounded near 0; that is, there exist numbers K > 0 and δ > 0 such that if ||x|| < δ, x  Ω, then ||f(x)||/|g(x)| ≤ K.
2. f(x) = o(g(x)) to mean that


The symbol O(g(x)) [read "big-oh of g(x)"] is used to represent a function that is bounded by a scaled version of g in a neighborhood of 0. Examples of such a function are:

 x = O(x).
 
 cosx = O(1).
 sinx = O(x).

On the other hand, o(g(x)) [read "little-oh of g(x)"] represents a function that goes to zero "faster" than g(x) in the sense that limx→0 ||o(g(x))||/|g(x)| = 0. Examples of such functions are:

 x2 = O(x).
 
 x3 = O(x2).
 x = O(1).

Note that if f(x) = o(g(x)), then f(x) = O(g(x)) (but the converse is not necessarily true). Also, if f(x) = O(||x||p), then f(x) = o(||x||p-ε) for any ε > 0.
Suppose that f  m. Recall that the remainder term in Taylor's theorem has the form

where θ  (0,1). Substituting this into Taylor's formula, we get

By the continuity of f(m), we have f(m)(a + θh) → f(m)(a) as h → 0; that is, f(m)(a + θh) = f(m)(a) + o(1). Therefore,

since hmo(1) = o(hm). We may then write Taylor's formula as

If, in addition, we assume that f  m+1, we may replace the term o(hm) above by O(hm+1). To see this, we first write Taylor's formula with Rm+1:

where

with θ′  (0,1). Because f(m+1) is bounded on [a, b] (by Theorem 4.2),

Therefore, if f  m+1, we may write Taylor's formula as

We now turn to the Taylor series expansion of a real-valued function f : n →  about the point x0  n. Suppose that f  2. Let x and x0 be points in n, and let z(α) = x0 + α(x − x0)/||x − x0||. Define φ :  →  by

Using the chain rule, we obtain

and

where we recall that

Observe that

Hence,

If we assume that f  3, we may use the formula for the remainder term R3 to conclude that

We end with a statement of the mean value theorem, which is closely related to Taylor's theorem.
Theorem 5.9 If a function f : n → m is differentiable on an open set Ω ⊂ n, then for any pair of points x, y  Ω, there exists a matrix M such that

The mean value theorem follows from Taylor's theorem (for the case where m = 1) applied to each component of f. It is easy to see that M is a matrix whose rows are the rows D f evaluated at points that lie on the line segment joining x and y (these points may differ from row to row).
For further reading in calculus, consult [13], [81], [83], [115], [120], [134]. A basic treatment of real analysis can be found in [2], [112], whereas a more advanced treatment is provided in [89], [111]. For stimulating reading on the "big-oh" notation, see [77, pp. 104-108].
EXERCISES

5.1 Show that a sufficient condition for limk→∞ Ak = O is ||A|| < 1.
5.2 Show that for any matrix A  nxn,

Hint: Use Exercise 5.1.
5.3 Consider the function

where a, b, and x are n-dimensional vectors.
a. Find ∇f(x).
b. Find the Hessian F(x).
5.4 Define the functions f : 2 →  and g :  → 2 by , g(t) = [3t + 5, 2t − 6]. Let F :  →  be given by F(t) = f(g(t)). Evaluate  using the chain rule.
5.5 Consider f(x) = x1x2/2, g(s,t) = [4s+3t, 2s + t]. Evaluate  and  using the chain rule.
5.6 Let x(t) = [et + t3, t2, t + 1], t  , and , x = [x1, x2, x3] → 3. Find  in terms of t.
5.7 Suppose that f(x) = o(g(x)). Show that for any given ε > 0, there exists δ > 0 such that if ||x|| < δ, then ||f(x)|| < ε|g(x)|.
5.8 Use Exercise 5.7 to show that if functions f : n →  and g : n →  satisfy f(x) = −g(x) + o(g(x)) and g(x) > 0 for all x ≠ 0, then for all x ≠ 0 sufficiently small, we have f(x) < 0.
5.9 Let

Sketch the level sets associated with f1(x1, x2) = 12 and f2(x1, x2) = 16 on the same diagram. Indicate on the diagram the values of x = [x1, x2] for which f(x) = [f1(x1, x2), f2(x1, x2)] = [12, 16].
5.10 Write down the Taylor series expansion of the following functions about the given points x0. Neglect terms of order three or higher.
a. f(x) = x1e−x2 + x2 + 1, x0 = [1, 0].
b. f(x) = x41 + 2x21x22 + x42, x0 = [1, 1].
c. f(x) = ex1-x2 + ex1 + x2 + x1 + x2 + 1, x0 = [1, 0].








PART II
UNCONSTRAINED OPTIMIZATION







CHAPTER 6
BASICS OF SET-CONSTRAINED AND UNCONSTRAINED OPTIMIZATION
6.1 Introduction
In this chapter we consider the optimization problem

The function f : n →  that we wish to minimize is a real-valued function called the objective function or cost function. The vector x is an n-vector of independent variables: x = [x1, x2, ..., xn]  n. The variables x1, ..., xn are often referred to as decision variables. The set Ω is a subset of n called the constraint set or feasible set.
The optimization problem above can be viewed as a decision problem that involves finding the "best" vector x of the decision variables over all possible vectors in Ω. By the "best" vector we mean the one that results in the-smallest value of the objective function. This vector is called the minimizer of f over Ω. It is possible that there may be many minimizers. In this case, finding any of the minimizers will suffice.
There are also optimization problems that require maximization of the objective function, in which case we seek maximizers. Minimizers and maximizes are also called extremizers. Maximization problems, however, can be represented equivalently in the minimization form above because maximizing f is equivalent to minimizing −f. Therefore, we can confine our attention to minimization problems without loss of generality.
The problem above is a general form of a constrained optimization problem, because the decision variables are constrained to be in the constraint set Ω. If Ω = n, then we refer to the problem as an unconstrained optimization problem. In this chapter we discuss basic properties of the general optimization problem above, which includes the unconstrained case. In the remaining chapters of this part, we deal with iterative algorithms for solving unconstrained optimization problems.
The constraint "x  Ω" is called a set constraint Often, the constraint set Ω takes the form Ω = {x : h(x) = 0, g(x) ≤ 0}, where h and g are given functions. We refer to such constraints as functional constraints. The remainder of this chapter deals with general set constraints, including the special case where Ω = n. The case where Ω = n is called the unconstrained case. In Parts III and IV we consider constrained optimization problems with functional constraints.
In considering the general optimization problem above, we distinguish between two kinds of minimizers, as specified by the following definitions.
Definition 6.1 Suppose that f : n →  is a real-valued function defined on some set Ω ⊂ n. A point x*  Ω is a local minimizer of f over Ω if there exists ε > 0 such that f(x) ≥ f(x*) for all x  Ω \ {x*} and ||x − x*|| < ε. A point x*  Ω is a global minimizer of f over Ω if f(x) ≥ f(x*) for all x  Ω\{x*}.
If in the definitions above we replace "≥" with ">," then we have a strict local minimizer and a strict global minimizer, respectively. In Figure 6.1, we illustrate the definitions for n = 1.

Figure 6.1 Examples of minimizers: x1: strict global minimizer; x2: strict local minimizer; x3: local (not strict) minimizer.


If x* is a global minimizer of f over Ω, we write f(x*) = minxΩ f(x) and x* = arg minxΩ f(x). If the minimization is unconstrained, we simply write x* = arg minx f(x) or x* = arg min f(x). In other words, given a real-valued function f, the notation arg min f(x) denotes the argument that minimizes the function f (a point in the domain of f), assuming that such a point is unique (if there is more than one such point, we pick one arbitrarily). For example, if f :  →  is given by f(x) = (x + 1)2 + 3, then arg min f(x) = −1. If we write arg minxΩ, then we treat "x  Ω" to be a constraint for the minimization. For example, for the function f above, arg minx≥0 f(x) = 0.
Strictly speaking, an optimization problem is solved only when a global minimizer is found. However, global minimizers are, in general, difficult to find. Therefore, in practice, we often have to be satisfied with finding local minimizers.
6.2 Conditions for Local Minimizers
In this section we derive conditions for a point x* to be a local minimizer. We use derivatives of a function f : n → . Recall that the first-order derivative of f, denoted Df, is

Note that the gradient ∇f is just the transpose of Df; that is, ∇f = (Df). The second derivative of f : n →  (also called the Hessian of f) is

Example 6.1 Let f(x1, x2) = 5x1 + 8x2 + x1x2 − x21 − 2x22. Then,

and

Given an optimization problem with constraint set Ω, a minimizer may lie either in the interior or on the boundary of Ω. To study the case where it lies on the boundary, we need the notion of feasible directions.
Definition 6.2 A vector d  n, d ≠ 0, is a feasible direction at x  Ω if there exists α0 > 0 such that x + αd  Ω for all α  [0, α0].
Figure 6.2 illustrates the notion of feasible directions.

Figure 6.2 Two-dimensional illustration of feasible directions; d1 is a feasible direction, d2 is not a feasible direction.


Let f : n →  be a real-valued function and let d be a feasible direction at x  Ω. The directional derivative of f in the direction d, denoted ∂f/∂d, is the real-valued function defined by

If ||d|| = 1, then ∂f/∂d is the rate of increase of f at x in the direction d. To compute the directional derivative above, suppose that x and d are given. Then, f(x + αd) is a function of α, and

Applying the chain rule yields

In summary, if d is a unit vector (||d|| = 1), then  is the rate of increase of f at the point x in the direction d.
Example 6.2 Define f : 3 →  by f(x) = x1x2x3, and let

The directional derivative of f in the direction d is

Note that because ||d|| = 1, the above is also the rate of increase of f at x in the direction d.
We are now ready to state and prove the following theorem.
Theorem 6.1 First-Order Necessary Condition (FONC). Let Ω be a subset of n and f  ⊂1 a real-valued function on Ω. If x* is a local minimizer of f over Ω, then for any feasible direction d at x*, we have

Proof. Define

Note that x(0) = x*. Define the composite function

Then, by Taylor's theorem,

where α ≥ 0 [recall the definition of o(α) ("little-oh of α") in Part I]. Thus, if ϕ(α) ≥ ϕ(0), that is, f(x* + αd) ≥ f(x*) for sufficiently small values of α > 0 (x* is a local minimizer), then we have to have d ∇ f(x*) ≥ 0 (see Exercise 5.8).
Theorem 6.1 is illustrated in Figure 6.3.

Figure 6.3 Illustration of the FONC for a constrained case; x1 does not satisfy the FONC, whereas x2 satisfies the FONC.


An alternative way to express the FONC is

for all feasible directions d. In other words, if x* is a local minimizer, then the rate of increase of f at x* in any feasible direction d in Ω is nonnegative. Using directional derivatives, an alternative proof of Theorem 6.1 is as follows. Suppose that x* is a local minimizer. Then, for any feasible direction d, there exists  > 0 such that for all α  (0, ),

Hence, for all α  (0, ), we have

Taking the limit as α → 0, we conclude that

A special case of interest is when x* is an interior point of Ω (see Section 4.4). In this case, any direction is feasible, and we have the following result.
Corollary 6.1 Interior Case. Let Ω be a subset of n and f  ⊂1 a real-valued function on Ω. If x* is a local minimizer of f over Ω and if x* is an interior point of Ω, then

Proof. Suppose that f has a local minimizer x* that is an interior point of Ω. Because x* is an interior point of Ω, the set of feasible directions at x* is the whole of n. Thus, for any d  n, d ∇ f(x*) ≥ 0 and −d ∇ f(x*) ≥ 0. Hence, d ∇ f(x*) = 0 for all d  n, which implies that ∇ f(x*) = 0.
Example 6.3 Consider the problem


a. Is the first-order necessary condition (FONC) for a local minimizer satisfied at x = [1, 3]?
b. Is the FONC for a local minimizer satisfied at x = [0, 3]?
c. Is the FONC for a local minimizer satisfied at x = [1, 0]?
d. Is the FONC for a local minimizer satisfied at x = [0, 0]?

Solution: First, let f : 2 →  be defined by f(x) = x21 + 0.5x22 + 3x2 + 4.5, where x = [x1, x2]. A plot of the level sets of f is shown in Figure 6.4.

Figure 6.4 Level sets of the function in Example 6.3.



a. At x = [1, 3], we have ∇ f(x) = [2x1, x2 + 3] = [2, 6]. The point x = [1, 3] is an interior point of Ω = {x : x1 ≥ 0, x2 ≥ 0}. Hence, the FONC requires that ∇ f(x) = 0. The point x = [1, 3] does not satisfy the FONC for a local minimizer.
b. At x = [0, 3], we have ∇ f(x) = [0, 6], and hence d ∇ f(x) = 6d2, where d = [d1, d2]. For d to be feasible at x, we need d1 ≥ 0, and d2 can take an arbitrary value in . The point x = [0, 3] does not satisfy the FONC for a minimizer because d2 is allowed to be less than zero. For example, d = [1, − 1] is a feasible direction, but d ∇ f(x) = −6 < 0.
c. At x = [1, 0], we have ∇ f(x) = [2, 3], and hence d ∇ f(x) = 2d1 + 3d2. For d to be feasible, we need d2 ≥ 0, and d1 can take an arbitrary value in . For example, d = [−5, 1] is a feasible direction. But d ∇ f(x) = −7 < 0. Thus, x = [1, 0] does not satisfy the FONC for a local minimizer.
d. At x = [0, 0], we have ∇ f(x) = [0, 3], and hence d ∇ f(x) = 3d2. For d to be feasible, we need d2 ≥ 0 and d1 ≥ 0. Hence, x = [0, 0] satisfies the FONC for a local minimizer.

Example 6.4 Figure 6.5 shows a simplified model of a cellular wireless system (the distances shown have been scaled down to make the calculations simpler). A mobile user (also called a mobile) is located at position x (see Figure 6.5).

Figure 6.5 Simplified cellular wireless system in Example 6.4.


There are two base station antennas, one for the primary base station and another for the neighboring base station. Both antennas are transmitting signals to the mobile user, at equal power. However, the power of the received signal as measured by the mobile is the reciprocal of the squared distance from the associated antenna (primary or neighboring base station). We are interested in finding the position of the mobile that maximizes the signal-to-interference ratio, which is the ratio of the signal power received from the primary base station to the signal power received from the neighboring base station.
We use the FONC to solve this problem. The squared distance from the mobile to the primary antenna is 1 + x2, while the squared distance from the mobile to the neighboring antenna is 1 + (2 − x)2. Therefore, the signal-to-interference ratio is

We have

By the FONC, at the optimal position x* we have f′(x*) = 0. Hence, either x* = 1 −  or x* = 1 + . Evaluating the objective function at these two candidate points, it easy to see that x* = 1 −  is the optimal position.
The next example illustrates that in some problems the FONC is not helpful for eliminating candidate local minimizers. However, in such cases, there may be a recasting of the problem into an equivalent form that makes the FONC useful.
Example 6.5 Consider the set-constrained problem

where Ω = {[x1, x2] : x21 + x22 = 1}.

a. Consider a point x*  Ω. Specify all feasible directions at x*.
b. Which points in Ω satisfy the FONC for this set-constrained problem?
c. Based on part b, is the FONC for this set-constrained problem useful for eliminating local-minimizer candidates?
d. Suppose that we use polar coordinates to parameterize points x  Ω in terms of a single parameter \θ:

Now use the FONC for unconstrained problems (with respect to θ) to derive a necessary condition of this sort: If x*  Ω is a local minimizer, then d ∇ f(x*) = 0 for all d satisfying a "certain condition." Specify what this certain condition is.

Solution:

a. There are no feasible directions at any x*.
b. Because of part a, all points in Ω satisfy the FONC for this set-constrained problem.
c. No, the FONC for this set-constrained problem is not useful for eliminating local-minimizer candidates.
d. Write h(θ) = f(g(θ)), where g :  → 2 is given by the equations relating θ to x = [x1, x2]. Note that Dg(θ) = [−sin θ, cos θ]. Hence, by the chain rule,

Notice that Dg(θ) is tangent to Ω at x = g(θ). Alternatively, we could say that Dg(θ) is orthogonal to x = g(θ).
Suppose that x*  Ω is a local minimizer. Write x* = g(θ*). Then θ* is an unconstrained minimizer of h. By the FONC for unconstrained problems, h′(θ*) = 0, which implies that d ∇ f(x*) = 0 for all d tangent to Ω at x* (or, alternatively, for all d orthogonal to x*).

We now derive a second-order necessary condition that is satisfied by a local minimizer.
Theorem 6.2 Second-Order Necessary Condition (SONC). Let Ω, ⊂ n, f  ⊂2 a function on Ω, x* a local minimizer of f over Ω, and d a feasible direction at x*. If d ∇ f(x*) = 0, then

where F is the Hessian of f.
Proof. We prove the result by contradiction. Suppose that there is a feasible direction d at x* such that d ∇ f(x*) = 0 and d F(x*)d < 0. Let x(α) = x* + αd and define the composite function ϕ(α) = f(x* + αd) = f(x(α)). Then, by Taylor's theorem,

where by assumption, ϕ′(0) = d ∇ f(x*) = 0 and ϕ″(O) = d F(x*)d < 0. For sufficiently small α,

that is,

which contradicts the assumption that x* is a local minimizer. Thus,

Corollary 6.2 Interior Case. Let x* be an interior point of Ω, ⊂ n. If x* is a local minimizer of f : Ω → , f  ⊂2, then

and F(x*) is positive semidefinite (F(x*) ≥ 0); that is, for all d  n,

Proof. If x* is an interior point, then all directions are feasible. The result then follows from Corollary 6.1 and Theorem 6.2.
In the examples below, we show that the necessary conditions are not sufficient.
Example 6.6 Consider a function of one variable f(x) = x3, f :  → . Because f′(0) = 0, and f″(0) = 0, the point x = 0 satisfies both the FONC and SONC. However, x = 0 is not a minimizer (see Figure 6.6).

Figure 6.6 The point 0 satisfies the FONC and SONC but is not a minimizer.


Example 6.7 Consider a function f : 2 → , where f(x) = x21 − x22. The FONC requires that ∇f(x) = [2x1,-2x2] = 0. Thus, x = [0,0] satisfies the FONC. The Hessian matrix of f is

The Hessian matrix is indefinite; that is, for some d1  2 we have d1 Fd1 > 0 (e.g., d1 = [1,0]) and for some d2 we have d2Fd2 < 0 (e.g., d2 = [0,1]). Thus, x = [0,0] does not satisfy the SONC, and hence it is not a minimizer. The graph of f(x) = x21 − x22 is shown in Figure 6.7.

Figure 6.7 Graph of f(x) = x21 − x22. The point 0 satisfies the FONC but not SONC; this point is not a minimizer.


We now derive sufficient conditions that imply that x* is a local minimizer.
Theorem 6.3 Second-Order Sufficient Condition (SOSC), Interior Case. Let f  ⊂2 be defined on a region in which x* is an interior point. Suppose that

1. ∇f(x*) = 0.
2. F(x*) > 0.

Then, x* is a strict local minimizer of f.
Proof. Because f  2, we have F(x*) = F(x*). Using assumption 2 and Rayleigh's inequality it follows that if d ≠ 0, then 0 < λmin(F(x*))||d||2 ≤ d F(x*)d. By Taylor's theorem and assumption 1,

Hence, for all d such that ||d|| is sufficiently small,

which completes the proof.
Example 6.8 Let f(x) = x21 + x22. We have ∇f(x) = [2x1, 2x2] = 0 if and only if x = [0,0]. For all x  2, we have

The point x = [0,0] satisfies the FONC, SONC, and SOSC. It is a strict local minimizer. Actually, x = [0,0] is a strict global minimizer. Figure 6.8 shows the graph of f(x) = x21 + x22.

Figure 6.8 Graph of f(x) = x21 + x22.


In this chapter we presented a theoretical basis for the solution of nonlinear unconstrained problems. In the following chapters we are concerned with iterative methods of solving such problems. Such methods are of great importance in practice. Indeed, suppose that one is confronted with a highly nonlinear function of 20 variables. Then, the FONC requires the solution of 20 nonlinear simultaneous equations for 20 variables. These equations, being nonlinear, will normally have multiple solutions. In addition, we would have to compute 210 second derivatives (provided that f  2) to use the SONC or SOSC. We begin our discussion of iterative methods in the next chapter with search methods for functions of one variable.
EXERCISES

6.1 Consider the problem

where f  2. For each of the following specifications for Ω, x*, and f, determine if the given point x* is: (i) definitely a local minimizer; (ii) definitely not a local minimizer; or (iii) possibly a local minimizer.
a. , and gradient .
b. , and gradient .
c. , gradient , and Hessian F(x*) = I (identity matrix).
d. , gradient , and Hessian

6.2 Find minimizers and maximizers of the function

6.3 Show that if x* is a global minimizer of f over Ω, and x*  Ω′ ⊂ Ω, then x* is a global minimizer of f over Ω′.
6.4 Suppose that x* is a local minimizer of f over Ω, and Ω ⊂ Ω′. Show that if x* is an interior point of Ω, then x* is a local minimizer of f over Ω′. Show that the same conclusion cannot be made if x* is not an interior point of Ω.
6.5 Consider the problem of minimizing f :  → , f  3, over the constraint set Ω. Suppose that 0 is an interior point of Ω.
a. Suppose that 0 is a local minimizer. By the FONC we know that f′(0) = 0 (where f′ is the first derivative of f). By the SONC we know that f″(0) ≥ 0 (where f″ is the second derivative of f). State and prove a third-order necessary condition (TONC) involving the third derivative at 0, f″′(0).
b. Give an example of f such that the FONC, SONC, and TONC (in part a) hold at the interior point 0, but 0 is not a local minimizer of f over Ω. (Show that your example is correct.)
c. Suppose that f is a third-order polynomial. If 0 satisfies the FONC, SONC, and TONC (in part a), then is this sufficient for 0 to be a local minimizer?
6.6 Consider the problem of minimizing f :  → , f  3, over the constraint set Ω = [0,1]. Suppose that x* → 0 is a local minimizer.
a. By the FONC we know that f′(0) ≥ 0 (where f′ is the first derivative of f). By the SONC we know that if f′(0) = 0, then f″(0) ≥ 0 (where f″ is the second derivative of f). State and prove a third-order necessary condition involving the third derivative at 0, f″′(0).
b. Give an example of f such that the FONC, SONC, and TONC (in part a) hold at the point 0, but 0 is not a local minimizer of f over Ω = [0,1].
6.7 Let f : n → , x0  n, and Ω ⊂ n. Show that

where Ω′ = {y : y − x0  Ω}.
6.8 Consider the following function f : 2 → :

a. Find the gradient and Hessian of f at the point [1,1].
b. Find the directional derivative of f at [1,1] with respect to a unit vector in the direction of maximal rate of increase.
c. Find a point that satisfies the FONC (interior case) for f. Does this point satisfy the SONC (for a minimizer)?
6.9 Consider the following function:

a. In what direction does the function f decrease most rapidly at the point x(0) = [2,1]?
b. What is the rate of increase of f at the point x(0) in the direction of maximum decrease of f?
c. Find the rate of increase of f at the point x(0) in the direction d = [3,4].
6.10 Consider the following function f : 2 → :

a. Find the directional derivative of f at [0,1] in the direction [1,0].
b. Find all points that satisfy the first-order necessary condition for f. Does f have a minimizer? If it does, then find all minimizer(s); otherwise, explain why it does not.
6.11 Consider the problem

where x1, x2  .
a. Does the point [x1,x2] = 0 satisfy the first-order necessary condition for a minimizer? That is, if f is the objective function, is it true that d∇f(0) ≥ 0 for all feasible directions d at 0?
b. Is the point [x1,x2] = 0 a local minimizer, a strict local minimizer, a local maximizer, a strict local maximizer, or none of the above?
6.12 Consider the problem

where f : 2 →  is given by f(x) = 5x2 with x = [x1,x2], and Ω = {x = [x1,x2] : x21 + x2 ≥ 1}.
a. Does the point x* = [0,1] satisfy the first-order necessary condition?
b. Does the point x* = [0,1] satisfy the second-order necessary condition?
c. Is the point x* = [0,1] a local minimizer?
6.13 Consider the problem

where f : 2 →  is given by f(x) = −3x1 with x = [x1,x2], and Ω = {x = [x1,x2] : x1 + x22 ≤ 2}. Answer each of the following questions, showing complete justification.
a. Does the point x* = [2,0] satisfy the first-order necessary condition?
b. Does the point x* = [2,0] satisfy the second-order necessary condition?
c. Is the point x* = [2,0] a local minimizer?
6.14 Consider the problem

where Ω = {x  2 : x21 + x22 ≥ 1} and f(x) = x2.
a. Find all point (s) satisfying the FONC.
b. Which of the point(s) in part a satisfy the SONC?
c. Which of the point(s) in part a are local minimizers?
6.15 Consider the problem

where f : 2 →  is given by f(x) = 3x1 with x = [x1,x2], and Ω = {x = [x1,x2] : x1 + x22 ≥ 2}. Answer each of the following questions, showing complete justification.
a. Does the point x* = [2,0] satisfy the first-order necessary condition?
b. Does the point x* = [2,0] satisfy the second-order necessary condition?
c. Is the point x* = [2,0] a local minimizer?
Hint: Draw a picture with the constraint set and level sets of f.
6.16 Consider the problem

where x = [x1,x2], f : 2 →  is given by f(x) = 4x21 − x22, and Ω = {x : x21 + 2x1 − x2 ≥ 0, x1 ≥ 0, x2 ≥ 0}.
a. Does the point x* = 0 = [0,0] satisfy the first-order necessary condition?
b. Does the point x* = 0 satisfy the second-order necessary condition?
c. Is the point x* = 0 a local minimizer of the given problem?
6.17 Consider the problem

where Ω ⊂ {x  2 : x1 > 0,x2 > 0} and f : Ω →  is given by f(x) = log(x1) + log(x2) with x = [x1,x2], where "log" represents natural logarithm. Suppose that x* is an optimal solution. Answer each of the following questions, showing complete justification.
a. Is it possible that x* is an interior point of Ω?
b. At what point(s) (if any) is the second-order necessary condition satisfied?
6.18 Suppose that we are given n real numbers, x1,..., xn. Find the number    such that the sum of the squared difference between  and the numbers above is minimized (assuming that the solution  exists).
6.19 An art collector stands at a distance of x feet from the wall, where a piece of art (picture) of height a feet is hung, b feet above his eyes, as shown in Figure 6.9 Find the distance from the wall for which the angle θ subtended by the eye to the picture is maximized.

Figure 6.9 Art collector's eye position in Exercise 6.19.


Hint: (1) Maximizing θ is equivalent to maximizing tan(θ).
(2) If θ = θ2 − θ1, then tan(θ) = (tan(θ2) − tan(θ1))/(1 + tan(θ2) tan(θ1)).
6.20 Figure 6.10 shows a simplified model of a fetal heart monitoring system (the distances shown have been scaled down to make the calculations simpler). A heartbeat sensor is located at position x (see Figure 6.10).

Figure 6.10 Simplified fetal heart monitoring system for Exercise 6.20.


   The energy of the heartbeat signal measured by the sensor is the reciprocal of the squared distance from the source (baby's heart or mother's heart). Find the position of the sensor that maximizes the signal-to-interference ratio, which is the ratio of the signal energy from the baby's heart to the signal energy from the mother's heart.
6.21 An amphibian vehicle needs to travel from point A (on land) to point B (in water), as illustrated in Figure 6.11. The speeds at which the vehicle travels on land and water are v1 and v2, respectively.

Figure 6.11 Path of amphibian vehicle in Exercise 6.21.


a. Suppose that the vehicle traverses a path that minimizes the total time taken to travel from A to B. Use the first-order necessary condition to show that for the optimal path above, the angles θ1 and θ2 in Figure 6.11 satisfy Snell's law:

b. Does the minimizer for the problem in part a satisfy the second-order sufficient condition?
6.22 Suppose that you have a piece of land to sell and you have two buyers. If the first buyer receives a fraction x1 of the piece of land, the buyer will pay you U1(x1) dollars. Similarly, the second buyer will pay you U2(x2) dollars for a fraction of x2 of the land. Your goal is to sell parts of your land to the two buyers so that you maximize the total dollars you receive. (Other than the constraint that you can only sell whatever land you own, there are no restrictions on how much land you can sell to each buyer.)
a. Formulate the problem as an optimization problem of the kind

by specifying f and Ω. Draw a picture of the constraint set.
b. Suppose that Ui(xi) = aixi, i = 1,2, where a1 and a2 are given positive constants such that a1 > a2. Find all feasible points that satisfy the first-order necessary condition, giving full justification.
c. Among those points in the answer of part b, find all that also satisfy the second-order necessary condition.
6.23 Let f : 2 →  be defined by

where x = [x1,x2]. Suppose that we wish to minimize f over 2. Find all points satisfying the FONC. Do these points satisfy the SONC?
6.24 Show that if d is a feasible direction at a point x  Ω, then for all β > 0, the vector βd is also a feasible direction at x.
6.25 Let Ω = {x  n : Ax = b}. Show that d  n is a feasible direction at x  Ω, if and only if Ad = 0.
6.26 Let f : 2 → . Consider the problem

where x = [x1,x2]. Suppose that ∇f(0) ≠ 0, and

Show that 0 cannot be a minimizer for this problem.
6.27 Let c  n, c ≠ 0, and consider the problem of minimizing the function f(x) = cx over a constraint set Ω ⊂ n. Show that we cannot have a solution lying in the interior of Ω.
6.28 Consider the problem

where c1 and c2 are constants such that c1 > c2 ≥ 0. This is a linear programming problem (see Part III). Assuming that the problem has an optimal feasible solution, use the first-order necessary condition to show that the unique optimal feasible solution x* is [1,0].
Hint: First show that x* cannot lie in the interior of the constraint set. Then, show that x* cannot lie on the line segments L1 = {x : x1 = 0,0 ≤ x2 < 1}, L2 = {x : 0 ≤ x1 < 1, x2 = 0}, L3 = {x : 0 ≤ x1 < 1, x2 = 1 − x1}.
6.29 Line Fitting. Let [x1,y1],..., [xn,yn], n ≥ 2, be points on the 2 plane (each xi,yi  ). We wish to find the straight line of "best fit" through these points ("best" in the sense that the average squared error is minimized); that is, we wish to find a, b   to minimize

a. Let

Show that f(a,b) can be written in the form zQz − 2cz + d, where z = [a,b],Q = Q  2×2, c  2 and d  , and find expressions for Q, c, and d in terms of , and .
b. Assume that the xi, i = 1,..., n, are not all equal. Find the parameters a* and b* for the line of best fit in terms of , and . Show that the point [a*, b*] is the only local minimizer of f.
Hint: .
c. Show that if a* and b* are the parameters of the line of best fit, then  (and hence once we have computed a*, we can compute b* using the formula .
6.30 Suppose that we are given a set of vectors {x(1),...,x(p)}, x(i)  n, i = 1,...,p. Find the vector x  n such that the average squared distance (norm) between  and x(1),..., x(p),

is minimized. Use the SOSC to prove that the vector  found above is a strict local minimizer. How is  related to the centroid (or center of gravity) of the given set of points {x(1),...,x(p)}?
6.31 Consider a function f : Ω → , where Ω ⊂ n is a convex set and f  ⊂1. Given x*  Ω, suppose that there exists c > 0 such that d∇f(x*) ≥ c||d|| for all feasible directions d at x*. Show that x* is a strict local minimizer of f over Ω.
6.32 Prove the following generalization of the second-order sufficient condition:
Theorem: Let Ω be a convex subset of n, f  2 a real-valued function on Ω, and x* a point in Ω. Suppose that there exists c  , c > 0, such that for all feasible directions d at x* (d ≠ 0), the following hold:
1. d∇f(x*) ≥ 0.
2. dF(x*)d ≥ c||d||2.
Then, x* is a strict local minimizer of f.
6.33 Consider the quadratic function f : n →  given by

where Q = Q > 0. Show that x* minimizes f if and only if x* satisfies the FONC.
6.34 Consider the linear system xk+1 = axk + buk+1, k ≥ 0, where xi  , ui  , and the initial condition is x0 = 0. Find the values of the control inputs u1,...,un to minimize

where q, r > 0 are given constants. This can be interpreted as desiring to make xn as large as possible but at the same time desiring to make the total input energy Σni = 1 u2i as small as possible. The constants q and r reflect the relative weights of these two objectives.








CHAPTER 7
ONE-DIMENSIONAL SEARCH METHODS
7.1 Introduction
In this chapter, we are interested in the problem of minimizing an objective function f :  →  (i.e., a one-dimensional problem). The approach is to use an iterative search algorithm, also called a line-search method. One-dimensional search methods are of interest for the following reasons. First, they are special cases of search methods used in multivariable problems. Second, they are used as part of general multivariable algorithms (as described later in Section 7.8).
In an iterative algorithm, we start with an initial candidate solution x(0) and generate a sequence of iterates x(1), x(2),.... For each iteration k = 0,1,2,..., the next point x(k+1) depends on x(k) and the objective function f. The algorithm may use only the value of f at specific points, or perhaps its first derivative f′, or even its second derivative f″. In this chapter, we study several algorithms:

 Golden section method (uses only f)
 Fibonacci method (uses only f)
 Bisection method (uses only f′)
 Secant method (uses only f′)
 Newton's method (uses f′ and f″)

The exposition here is based on [27].
7.2 Golden Section Search
The search methods we discuss in this and the next two sections allow us to determine the minimizer of an objective function f :  →  over a closed interval, say [a0,b0]. The only property that we assume of the objective function f is that it is unimodal, which means that f has only one local minimizer. An example of such a function is depicted in Figure 7.1.

Figure 7.1 Unimodal function.


The methods we discuss are based on evaluating the objective function at different points in the interval [a0,b0]. We choose these points in such a way that an approximation to the minimizer of f may be achieved in as few evaluations as possible. Our goal is to narrow the range progressively until the minimizer is "boxed in" with sufficient accuracy.
Consider a unimodal function f of one variable and the interval [a0,b0]. If we evaluate f at only one intermediate point of the interval, we cannot narrow the range within which we know the minimizer is located. We have to evaluate f at two intermediate points, as illustrated in Figure 7.2. We choose the intermediate points in such a way that the reduction in the range is symmetric, in the sense that

Figure 7.2 Evaluating the objective function at two intermediate points.



where

We then evaluate f at the intermediate points. If f(a1) < f(b1), then the minimizer must lie in the range [a0,b1]. If, on the other hand, f(a1) ≥ f(b1), then the minimizer is located in the range [a1,b0] (see Figure 7.3).

Figure 7.3 The case where f(a1) < f(b1); the minimizer x*  [a0, b1].


Starting with the reduced range of uncertainty, we can repeat the process and similarly find two new points, say a2 and b2, using the same value of ρ <  as before. However, we would like to minimize the number of objective function evaluations while reducing the width of the uncertainty interval. Suppose, for example, that f(a1) < f(b1), as in Figure 7.3. Then, we know that x*  [a0, b1]. Because a1 is already in the uncertainty interval and f(a1) is already known, we can make a1 coincide with b2. Thus, only one new evaluation of f at a2 would be necessary. To find the value of ρ that results in only one new evaluation of f, see Figure 7.4. Without loss of generality, imagine that the original range [a0, b0] is of unit length. Then, to have only one new evaluation of f it is enough to choose ρ so that

Figure 7.4 Finding value of ρ resulting in only one new evaluation of f.



Because b1 − a0 = 1 − ρ and b1 − b2 = 1 − 2p, we have

We write the quadratic equation above as

The solutions are

Because we require that ρ < , we take

Observe that

and

that is,

Thus, dividing a range in the ratio of ρ to 1 − ρ has the effect that the ratio of the shorter segment to the longer equals the ratio of the longer to the sum of the two. This rule was referred to by ancient Greek geometers as the golden section.
Using the golden section rule means that at every stage of the uncertainty range reduction (except the first), the objective function f need only be evaluated at one new point. The uncertainty range is reduced by the ratio 1 − ρ ≈ 0.61803 at every stage. Hence, N steps of reduction using the golden section method reduces the range by the factor

Example 7.1 Suppose that we wish to use the golden section search method to find the value of x that minimizes

in the interval [0,2] (this function comes from an example in [21]). We wish to locate this value of x to within a range of 0.3.
After N stages the range [0,2] is reduced by (0.61803)N. So, we choose N so that

Four stages of reduction will do; that is, N = 4.
Iteration 1. We evaluate f at two intermediate points a1 and b1. We have

where ρ = (3 − )/2. We compute

Thus, f(a1) < f(b1), so the uncertainty interval is reduced to

Iteration 2. We choose b2 to coincide with a1, and so f need only be evaluated at one new point,

We have

Now, f(b2) < f(a2), so the uncertainty interval is reduced to

Iteration 3. We set a3 = b2 and compute b3:

We have

So f(b3) > f(a3). Hence, the uncertainty interval is further reduced to

Iteration 4. We set b4 = a3 and

We have

Hence, f(a4) > f(b4). Thus, the value of x that minimizes f is located in the interval

Note that b3 − a4 = 0.292 < 0.3.
7.3 Fibonacci Method
Recall that the golden section method uses the same value of ρ throughout. Suppose now that we are allowed to vary the value ρ from stage to stage, so that at the kth stage in the reduction process we use a value ρk, at the next stage we use a value ρk+1, and so on.
As in the golden section search, our goal is to select successive values of ρk 0 ≤ ρk ≤ 1/2, such that only one new function evaluation is required at each stage. To derive the strategy for selecting evaluation points, consider Figure 7.5. From this figure we see that it is sufficient to choose the ρk such that

Figure 7.5 Selecting evaluation points.



After some manipulations, we obtain

There are many sequences ρ1, ρ2, ... that satisfy the law of formation above and the condition that 0 ≤ ρk ≤ 1/2. For example, the sequence ρ1 = ρ2 = ρ3 = ··· = (3 −)/2 satisfies the conditions above and gives rise to the golden section method.
Suppose that we are given a sequence ρ1, ρ2, ... satisfying the conditions above and we use this sequence in our search algorithm. Then, after N iterations of the algorithm, the uncertainty range is reduced by a factor of

Depending on the sequence ρ1, ρ2, ..., we get a different reduction factor. The natural question is as follows: What sequence ρ1, ρ2, ... minimizes the reduction factor above? This problem is a constrained optimization problem that can be stated formally as

Before we give the solution to the optimization problem above, we need to introduce the Fibonacci sequence F1, F2, F3, .... This sequence is defined as follows. First, let F−1 = 0 and F0 = 1 by convention. Then, for k ≥ 0,

Some values of elements in the Fibonacci sequence are:

It turns out that the solution to the optimization problem above is

where the Fk are the elements of the Fibonacci sequence. The resulting algorithm is called the Fibonacci search method. We present a proof for the optimality of the Fibonacci search method later in this section.
In the Fibonacci search method, the uncertainty range is reduced by the factor

Because the Fibonacci method uses the optimal values of ρ1, p2, ..., the reduction factor above is less than that of the golden section method. In other words, the Fibonacci method is better than the golden section method in that it gives a smaller final uncertainty range.
We point out that there is an anomaly in the final iteration of the Fibonacci search method, because

Recall that we need two intermediate points at each stage, one that comes from a previous iteration and another that is a new evaluation point. However, with ρN = 1/2, the two intermediate points coincide in the middle of the uncertainty interval, and therefore we cannot further reduce the uncertainty range. To get around this problem, we perform the new evaluation for the last iteration using ρN = 1/2 − ε, where ε is a small number. In other words, the new evaluation point is just to the left or right of the midpoint of the uncertainty interval. This modification to the Fibonacci method is, of course, of no significant practical consequence.
As a result of the modification above, the reduction in the uncertainty range at the last iteration may be either

or

depending on which of the two points has the smaller objective function value. Therefore, in the worst case, the reduction factor in the uncertainty range for the Fibonacci method is

Example 7.2 Consider the function

Suppose that we wish to use the Fibonacci search method to find the value of x that minimizes f over the range [0,2], and locate this value of x to within the range 0.3.
After N steps the range is reduced by (1 + 2ε)/FN+1 in the worst case. We need to choose N such that

Thus, we need

If we choose ε ≤ 0.1, then N = 4 will do.
Iteration 1. We start with

We then compute

The range is reduced to

Iteration 2. We have

so the range is reduced to

Iteration 3. We compute

The range is reduced to

Iteration 4. We choose ε = 0.05. We have

The range is reduced to

Note that b3 − a4 = 0.275 < 0.3.
We now turn to a proof of the optimality of the Fibonacci search method. Skipping the rest of this section does not affect the continuity of the presentation.
To begin, recall that we wish to prove that the values of ρ1, ρ2, ···, ρN used in the Fibonacci method, where ρk = 1 − FN−k+1/FN−k+2, solve the optimization problem

It is easy to check that the values of ρ1, ρ2, ... above for the Fibonacci search method satisfy the feasibility conditions in the optimization problem above (see Exercise 7.4). Recall that the Fibonacci method has an overall reduction factor of (1 − ρ1) ··· (1 − ρN) = 1/FN+1. To prove that the Fibonacci search method is optimal, we show that for any feasible values of ρ1, ..., ρN, we have (1 − ρ1) ··· (1 − ρN) ≥ 1 /FN+1.
It is more convenient to work with rk = 1 − ρk rather than ρk. The optimization problem stated in terms of rk is

Note that if r1, r2, ... satisfy rk+1  − 1, then rk ≥ 1/2 if and only if rk+1 ≤ 1. Also, rk ≥ 1/2 if and only if rk−1 ≤ 2/3 ≤ 1. Therefore, in the constraints above, we may remove the constraint rk ≤ 1, because it is implied implicitly by rk ≥ 1/2 and the other constraints. Therefore, the constraints above reduce to

To proceed, we need the following technical lemmas. In the statements of the lemmas, we assume that r1, r2, ... is a sequence that satisfies

Lemma 7.1 For k ≥ 2,

Proof. We proceed by induction. For k = 2 we have

and hence the lemma holds for k = 2. Suppose now that the lemma holds for k ≥ 2. We show that it also holds for k + 1. We have

where we used the formation law for the Fibonacci sequence.
Lemma 7.2 For k ≥ 2,

Proof. We proceed by induction. For k = 2, we have

But r1 = 1/(1 + r2) ≤ 2/3, and hence 1 − r1 > 0. Therefore, the result holds for k = 2. Suppose now that the lemma holds for k ≥ 2. We show that it also holds for k + 1. We have

By Lemma 7.1,

Substituting for 1/rk+1, we obtain

which completes the proof.
Lemma 7.3 For k ≥ 2,

Proof. Because rk+1 =  − 1 and rk ≥ , we have rk+1 ≤ 1. Substituting for rk+1 from Lemma 7.1, we get

Multiplying the numerator and denominator by (−1)k yields

By Lemma 7.2, (−1)k(Fk−2 − Fk−1r1) > 0, and therefore we can multiply both sides of the inequality above by (−1)k(Fk−2 - Fk−1r1) to obtain

Rearranging yields

Using the law of formation of the Fibonacci sequence, we get

which upon dividing by Fk+1 on both sides gives the desired result.
We are now ready to prove the optimality of the Fibonacci search method and the uniqueness of this optimal solution.
Theorem 7.1 Let r1, ..., rN, N ≥ 2, satisfy the constraints

Then,

Furthermore,

if and only if rk = FN-k+1/FN−k+2, k = 1, ..., N. In other words, the values of r1, ..., rN used in the Fibonacci search method form a unique solution to the optimization problem.
Proof. By substituting expressions for r1, ..., rN from Lemma 7.1 and performing the appropriate cancellations, we obtain

Using Lemma 7.3 yields

By Exercise 7.5, it is readily checked that the following identity holds: (−1)N(FN−2FN + 1 − FN−1FN) = 1. Hence,

From the above we see that

if and only if

This is simply the value of r1 for the Fibonacci search method. Note that fixing r1 determines r2, ··· rN uniquely.
For further discussion on the Fibonacci search method and its variants, see [133].
7.4 Bisection Method
Again we consider finding the minimizer of an objective function f :  →  over an interval [a0, b0]. As before, we assume that the objective function f is unimodal. Further, suppose that f is continuously differentiate and that we can use values of the derivative f′ as a basis for reducing the uncertainty interval.
The bisection method is a simple algorithm for successively reducing the uncertainty interval based on evaluations of the derivative. To begin, let x(0) = (a0 + b0)/2 be the midpoint of the initial uncertainty interval. Next, evaluate f′(x(0)). If f′(x(0)) > 0, then we deduce that the minimizer lies to the left of x(0). In other words, we reduce the uncertainty interval to [a0, x(0)]. On the other hand, if f′(x(0)) < 0, then we deduce that the minimizer lies to the right of x(0). In this case, we reduce the uncertainty interval to [x(0), b0]. Finally, if f′(x(0)) = 0, then we declare x(0) to be the minimizer and terminate our search.
With the new uncertainty interval computed, we repeat the process iteratively. At each iteration k, we compute the midpoint of the uncertainty interval. Call this point x(k). Depending on the sign of f′(x(k)) (assuming that it is nonzero), we reduce the uncertainty interval to the left or right of x(k). If at any iteration k we find that f′(x(k)) = 0, then we declare x(k) to be the minimizer and terminate our search.
Two salient features distinguish the bisection method from the golden section and Fibonacci methods. First, instead of using values of f, the bisection methods uses values of f′. Second, at each iteration, the length of the uncertainty interval is reduced by a factor of 1/2. Hence, after N steps, the range is reduced by a factor of (1/2)N. This factor is smaller than in the golden section and Fibonacci methods.
Example 7.3 Recall Example 7.1 where we wish to find the minimizer of

in the interval [0,2] to within a range of 0.3. The golden section method requires at least four stages of reduction. If, instead, we use the bisection method, we would choose N so that

In this case, only three stages of reduction are needed.
7.5 Newton's Method
Suppose again that we are confronted with the problem of minimizing a function f of a single real variable x. We assume now that at each measurement point x(k) we can determine f(x(k)), f′(x(k)), and f″(x(k)). We can fit a quadratic function through x(k) that matches its first and second derivatives with that of the function f. This quadratic has the form

Note that q(x(k)) = f(x(k)), q′(x(k)) = f′(x(k)), and q″(x(k)) = f″(x(k)). Then, instead of minimizing f, we minimize its approximation q. The first-order necessary condition for a minimizer of q yields

Setting x = x(k+1), we obtain

Example 7.4 Using Newton's method, we will find the minimizer of

Suppose that the initial value is x(0) = 0.5, and that the required accuracy is  = 10−5, in the sense that we stop when |x(k+1) − x(k)| < .
We compute

Hence,

Proceeding in a similar manner, we obtain

Note that |x(4) − x(3)| <  = 10−5. Furthermore, f′(x(4)) = −8.6 × 10−6 ≈ 0. Observe that f″(x(4)) = 1.673 > 0, so we can assume that x* ≈ x(4) is a strict minimizer.
Newton's method works well if f″(x) > 0 everywhere (see Figure 7.6). However, if f″(x) < 0 for some x, Newton's method may fail to converge to the minimizer (see Figure 7.7).

Figure 7.6 Newton's algorithm with f″(x) > 0.



Figure 7.7 Newton's algorithm with f″(x) < 0.


Newton's method can also be viewed as a way to drive the first derivative of f to zero. Indeed, if we set g(x) = f′(x), then we obtain a formula for iterative solution of the equation g(x) = 0:

In other words, we can use Newton's method for zero finding.
Example 7.5 We apply Newton's method to improve a first approximation, x(0) = 12, to the root of the equation

We have g′(x) = 3x2 − 24.4x + 7.45.
Performing two iterations yields

Newton's method for solving equations of the form g(x) = 0 is also referred to as Newton's method of tangents. This name is easily justified if we look at a geometric interpretation of the method when applied to the solution of the equation g(x) = 0 (see Figure 7.8).

Figure 7.8 Newton's method of tangents.


If we draw a tangent to g(x) at the given point x(k), then the tangent line intersects the x-axis at the point x(k+1), which we expect to be closer to the root x* of g(x) = 0. Note that the slope of g(x) at x(k) is

Hence,

Newton's method of tangents may fail if the first approximation to the root is such that the ratio g(x(0))/g′(x(0)) is not small enough (see Figure 7.9). Thus, an initial approximation to the root is very important.

Figure 7.9 Example where Newton's method of tangents fails to converge to the root x* of g(x) = 0.


7.6 Secant Method
Newton's method for minimizing f uses second derivatives of f:

If the second derivative is not available, we may attempt to approximate it using first derivative information. In particular, we may approximate f″(x(k)) above with

Using the foregoing approximation of the second derivative, we obtain the algorithm

called the secant method. Note that the algorithm requires two initial points to start it, which we denote x(−1) and x(0). The secant algorithm can be represented in the following equivalent form:

Observe that, like Newton's method, the secant method does not directly involve values of f(x(k)). Instead, it tries to drive the derivative f′ to zero. In fact, as we did for Newton's method, we can interpret the secant method as an algorithm for solving equations of the form g(x) = 0. Specifically, the secant algorithm for finding a root of the equation g(x) = 0 takes the form

or, equivalently,

The secant method for root finding is illustrated in Figure 7.10 (compare this with Figure 7.8). Unlike Newton's method, which uses the slope of g to determine the next point, the secant method uses the "secant" between the (k − 1)th and kth points to determine the (k + 1)th point.

Figure 7.10 Secant method for root finding.


Example 7.6 We apply the secant method to find the root of the equation

We perform two iterations, with starting points x(−1) = 13 and x(0) = 12. We obtain

Example 7.7 Suppose that the voltage across a resistor in a circuit decays according to the model V(t) = e−Rt, where V(t) is the voltage at time t and R is the resistance value.
Given measurements V1, ..., Vn of the voltage at times t1, ..., tn, respectively, we wish to find the best estimate of R. By the best estimate we mean the value of R that minimizes the total squared error between the measured voltages and the voltages predicted by the model.
We derive an algorithm to find the best estimate of R using the secant method. The objective function is

Hence, we have

The secant algorithm for the problem is

For further reading on the secant method, see [32]. Newton's method and the secant method are instances of quadratic fit methods. In Newton's method, x(k+1) is the stationary point of a quadratic function that fits f′ and f″ at x(k). In the secant method, x(k+1) is the stationary point of a quadratic function that fits f′ at x(k) and x(k−1). The secant method uses only f′ (and not f″) but needs values from two previous points. We leave it to the reader to verify that if we set x(k+1) to be the stationary point of a quadratic function that fits f at x(k), x(k−1), and x(k−2), we obtain a quadratic fit method that uses only values of f:

where σij = (x(k−i))2 − (x(k−j))2 and δij = x(k−i) − x(k−j) (see Exercise 7.9) This method does not use f′ or f″, but needs values of f from three previous points. Three points are needed to initialize the iterations. The method is also sometimes called inverse parabolic interpolation.
An approach similar to fitting (or interpolation) based on higher-order polynomials is possible. For example, we could set x(k+1) to be a stationary point of a cubic function that fits f′ at x(k), x(k−1), and x(k−2).
It is often practically advantageous to combine multiple methods, to overcome the limitations in any one method. For example, the golden section method is more robust but slower than inverse parabolic interpolation. Brent's method combines the two [17], resulting in a method that is faster than the golden section method but still retains its robustness properties.
7.7 Bracketing
Many of the methods we have described rely on an initial interval in which the minimizer is known to lie. This interval is also called a bracket, and procedures for finding such a bracket are called bracketing methods.
To find a bracket [a, b] containing the minimizer, assuming unimodality, it suffices to find three points a < c < b such that f(c) < f(a) and f(c) < f(b). A simple bracketing procedure is as follows. First, we pick three arbitrary points x0 < x1 < x2. If f(x1) < f(x0) and f(x2) < f(x2), then we are done—the desired bracket is [x0, x2]. If not, say f(x0) > f(x1) > f(x2), then we pick a point x3 > x2 and check if f(x2) < f(x3). If it holds, then again we are done—the desired bracket is [x1, x3]. Otherwise, we continue with this process until the function increases. Typically, each new point chosen involves an expansion in distance between successive test points. For example, we could double the distance between successive points, as illustrated in Figure 7.11. An analogous process applies if the initial three points are such that f(x0) < f(x1) < f(x2).

Figure 7.11 An illustration of the process of bracketing a minimizer.


In the procedure described above, when the bracketing process terminates, we have three points xk−2, xk−1, and xk such that f(xk−1) < f(xk−2) and f(xk−1) < f(xk). The desired bracket is then [xk−2, xk], which we can then use to initialize any of a number of search methods, including the golden section, Fibonacci, and bisection methods. Note that at this point, we have already evaluated f(xk−2), f(xk−1), and f(xk). If function evaluations are expensive to obtain, it would help if the point xk−1 inside the bracket also coincides with one of the points used in the search method. For example, if we intend to use the golden section method, then it would help if xk−1 − xk−2 = ρ(xk − xk−2), where ρ = (3 − )/2. In this case, xk−1 would be one of the two points within the initial interval used in the golden section method. This is achieved if each successive point xk is chosen such that xk = xk−1 + (2 − ρ)(xk−1 − xk−2). In this case, the expansion in the distance between successive points is a factor 2 − ρ ≈ 1.618, which is less than double.
7.8 Line Search in Multidimensional Optimization
One-dimensional search methods play an important role in multidimensional optimization problems. In particular, iterative algorithms for solving such optimization problems (to be discussed in the following chapters) typically involve a line search at every iteration. To be specific, let f : n →  be a function that we wish to minimize. Iterative algorithms for finding a minimizer of f are of the form

where x(0) is a given initial point and αk ≥ 0 is chosen to minimize

The vector d(k) is called the search direction and αk is called the step size. Figure 7.12 illustrates a line search within a multidimensional setting. Note that choice of αk involves a one-dimensional minimization. This choice ensures that under appropriate conditions,

Figure 7.12 Line search in multidimensional optimization.



Any of the one-dimensional methods discussed in this chapter (including bracketing) can be used to minimize ϕk. We may, for example, use the secant method to find αk. In this case we need the derivative of ϕk, which is

This is obtained using the chain rule. Therefore, applying the secant method for the line search requires the gradient ∇f, the initial line-search point x(k), and the search direction d(k) (see Exercise 7.11). Of course, other one-dimensional search methods may be used for line search (see, e.g., [43] and [88]).
Line-search algorithms used in practice involve considerations that we have not yet discussed thus far. First, determining the value of αk that exactly minimizes ϕk may be computationally demanding; even worse, the minimizer of ϕk may not even exist. Second, practical experience suggests that it is better to allocate more computational time on iterating the multidimensional optimization algorithm rather than performing exact line searches. These considerations led to the development of conditions for terminating line-search algorithms that would result in low-accuracy line searches while still securing a sufficient decrease in the value of the f from one iteration to the next. The basic idea is that we have to ensure that the step size αk is not too small or too large.
Some commonly used termination conditions are as follows. First, let ε  (0, 1), γ > 1, and η  (ε, 1) be given constants. The Armijo condition ensures that αk is not too large by requiring that

Further, it ensures that αk is not too small by requiring that

The Goldstein condition differs from Armijo in the second inequality:

The first Armijo inequality together with the Goldstein condition are often jointly called the Armijo-Goldstein condition. The Wolfe condition differs from Goldstein in that it involves only ϕ′k:

A stronger variation of this is the strong Wolfe condition:

A simple practical (inexact) line-search method is the Armijo backtracking algorithm, described as follows. We start with some candidate value for the step size αk. If this candidate value satisfies a prespecified termination condition (usually the first Armijo inequality), then we stop and use it as the step size. Otherwise, we iteratively decrease the value by multiplying it by some constant factor τ  (0, 1) (typically τ = 0.5) and re-check the termination condition. If α(0) is the initial candidate value, then after m iterations the value obtained is αk = τmα(0). The algorithm backtracks from the initial value until the termination condition holds. In other words, the algorithm produces a value for the step size of the form αk = τmα(0) with m being the smallest value in {0, 1, 2, ...} for which αk satisfies the termination condition.
For more information on practical line-search methods, we refer the reader to [43, pp. 26-40], [96, Sec. 10.5], [11, App. C], [49], and [50].1
EXERCISES

7.1 Suppose that we have a unimodal function over the interval [5, 8]. Give an example of a desired final uncertainty range where the golden section method requires at least four iterations, whereas the Fibonacci method requires only three. You may choose an arbitrarily small value of ε for the Fibonacci method.
7.2 Let f(x) = x2 + 4 cos x, x  . We wish to find the minimizer x* of f over the interval [1, 2]. (Calculator users: Note that in cos x, the argument x is in radians.)
a. Plot f(x) versus x over the interval [1, 2].
b. Use the golden section method to locate x* to within an uncertainty of 0.2. Display all intermediate steps using a table:

c. Repeat part b using the Fibonacci method, with ε = 0.05. Display all intermediate steps using a table:

d. Apply Newton's method, using the same number of iterations as in part b, with x(0) = 1.
7.3 Let f(x) = 8e1−x + 7 log(x), where "log" represents the natural logarithm function.
a. Use MATLAB to plot f(x) versus x over the interval [1, 2], and verify that f is unimodal over [1, 2].
b. Write a simple MATLAB program to implement the golden section method that locates the minimizer of f over [1, 2] to within an uncertainty of 0.23. Display all intermediate steps using a table as in Exercise 7.2.
c. Repeat part b using the Fibonacci method, with ε = 0.05. Display all intermediate steps using a table as in Exercise 7.2.
7.4 Suppose that ρ1, ..., ρN are the values used in the Fibonacci search method. Show that for each k = 1, ..., N, 0 ≥ ρk ≥ 1/2, and for each k = 1, ..., N − 1,

7.5 Show that if F0, F1, ... is the Fibonacci sequence, then for each k = 2, 3, ...,

7.6 Show that the Fibonacci sequence can be calculated using the formula

7.7 Suppose that we have an efficient way of calculating exponentials. Based on this, use Newton's method to devise a method to approximate log(2) [where "log" is the natural logarithm function]. Use an initial point of x(0) = 1, and perform two iterations.
7.8 Consider the problem of finding the zero of g(x) = (ex − 1)/(ex + 1), x  , where ex is the exponential of x. (Note that 0 is the unique zero of g.)
a. Write down the algorithm for Newton's method of tangents applied to this problem. Simplify using the identity sinh x = (ex − e−x)/2.
b. Find an initial condition x(0) such that the algorithm cycles [i.e., x(0) = x(2) = x(4) = ···]. You need not explicitly calculate the initial condition; it suffices to provide an equation that the initial condition must satisfy.
Hint: Draw a graph of g.
c. For what values of the initial condition does the algorithm converge?
7.9 Derive a one-dimensional search (minimization) algorithm based on quadratic fit with only objective function values. Specifically, derive an algorithm that computes x(k+1) based on x(k), x(k−1), x(k−2), f(x(k)), f(x(k−1)), and f(x(k−2).
Hint: To simplify, use the notation σij = (x(k−i))2 − (x(k−j))2 and δij = x(k−i) − x(k−j). You might also find it useful to experiment with your algorithm by writing a MATLAB program. Note that three points are needed to initialize the algorithm.
7.10 The objective of this exercise is to implement the secant method using MATLAB.
a. Write a simple MATLAB program to implement the secant method to locate the root of the equation g(x) = 0. For the stopping criterion, use the condition |x(k+1) − x(k)| < |x(k)|ε, where ε > 0 is a given constant.
b. Let g(x) = (2x − 1)2 + 4(4 − 1024x)4. Find the root of g(x) = 0 using the secant method with x(−1) = 0, x(0) = 1, and ε = 10−5. Also determine the value of g at the solution obtained.
7.11 Write a MATLAB function that implements a line search algorithm using the secant method. The arguments to this function are the name of the M-file for the gradient, the current point, and the search direction. For example, the function may be called linesearch_secant and be used by the function call alpha=linesearch_secant('grad',x,d), where grad.m is the M-file containing the gradient, × is the starting line search point, d is the search direction, and alpha is the value returned by the function [which we use in the following chapters as the step size for iterative algorithms (see, e.g., Exercises 8.25 and 10.11)].
   Note: In the solutions manual, we used the stopping criterion |d ∇f(x + αd)| ≥ ε|d ∇f(x)|, where ε > 0 is a prespecified number, ∇f is the gradient, x is the starting line search point, and d is the search direction. The rationale for the stopping criterion above is that we want to reduce the directional derivative of f in the direction d by the specified fraction ε. We used a value of ε = 10−4 and initial conditions of 0 and 0.001.
7.12 Consider using a gradient algorithm to minimize the function

with the initial guess x(0) = [0.8, −0.25].
a. To initialize the line search, apply the bracketing procedure in Figure 7.11 along the line starting at x(0) in the direction of the negative gradient. Use ε = 0.075.
b. Apply the golden section method to reduce the width of the uncertainty region to 0.01. Organize the results of your computation in a table format similar to that of Exercise 7.2.
c. Repeat the above using the Fibonacci method.

1 We thank Dennis M. Goodman for furnishing us with references [49] and [50].







CHAPTER 8
GRADIENT METHODS
8.1 Introduction
In this chapter we consider a class of search methods for real-valued functions on n. These methods use the gradient of the given function. In our discussion we use such terms as level sets, normal vectors, and tangent vectors. These notions were discussed in some detail in Part I.
Recall that a level set of a function f : n →  is the set of points x satisfying f(x) = c for some constant c. Thus, a point x0  n is on the level set corresponding to level c if f(x0) = c. In the case of functions of two real variables, f : 2 → , the notion of the level set is illustrated in Figure 8.1.

Figure 8.1 Constructing a level set corresponding to level c for f.


The gradient of f at x0, denoted ∇f(x0), if it is not a zero vector, is orthogonal to the tangent vector to an arbitrary smooth curve passing through x0 on the level set f(x) = c. Thus, the direction of maximum rate of increase of a real-valued differentiable function at a point is orthogonal to the level set of the function through that point. In other words, the gradient acts in such a direction that for a given small displacement, the function f increases more in the direction of the gradient than in any other direction. To prove this statement, recall that ∇f(x), d, ||d|| = 1, is the rate of increase of f in the direction d at the point x. By the Cauchy-Schwarz inequality,

because ||d|| = 1. But if d = ∇f(x)/||∇f(x)||, then

Thus, the direction in which ∇f(x) points is the direction of maximum rate of increase of f at x. The direction in which − ∇f(x) points is the direction of maximum rate of decrease of f at x. Hence, the direction of negative gradient is a good direction to search if we want to find a function minimizer.
We proceed as follows. Let x(0) be a starting point, and consider the point x(0) − α∇f(x(0)). Then, by Taylor's theorem, we obtain

Thus, if ∇f(x(0)) ≠ 0, then for sufficiently small α > 0, we have

This means that the point x(0) − α∇f(x(0)) is an improvement over the point x(0) if we are searching for a minimizer.
To formulate an algorithm that implements this idea, suppose that we are given a point x(k). To find the next point x(k+1), we start at x(k) and move by an amount − αk ∇f(x(k)), where αk is a positive scalar called the step size. This procedure leads to the following iterative algorithm:

We refer to this as a gradient descent algorithm (or simply a gradient algorithm). The gradient varies as the search proceeds, tending to zero as we approach the minimizer. We have the option of either taking very small steps and reevaluating the gradient at every step, or we can take large steps each time. The first approach results in a laborious method of reaching the minimizer, whereas the second approach may result in a more zigzag path to the minimizer. The advantage of the second approach is possibly fewer gradient evaluations. Among many different methods that use this philosophy the most popular is the method of steepest descent, which we discuss next.
Gradient methods are simple to implement and often perform well. For this reason, they are used widely in practical applications. For a discussion of applications of the steepest descent method to the computation of optimal controllers, we recommend [85, pp. 481-515]. In Chapter 13 we apply a gradient method to the training of a class of neural networks.
8.2 The Method of Steepest Descent
The method of steepest descent is a gradient algorithm where the step size αk is chosen to achieve the maximum amount of decrease of the objective function at each individual step. Specifically, αk is chosen to minimize ϕk(α)  f(x(k) − α∇f(x(k))). In other words,

To summarize, the steepest descent algorithm proceeds as follows: At each step, starting from the point x(k) we conduct a line search in the direction −∇f(x(k)) until a minimizer, x(k+1), is found. A typical sequence resulting from the method of steepest descent is depicted in Figure 8.2.

Figure 8.2 Typical sequence resulting from the method of steepest descent.


Observe that the method of steepest descent moves in orthogonal steps, as stated in the following proposition.
Proposition 8.1 If  is a steepest descent sequence for a given function f : n → , then for each k the vector x(k+1) − x(k) is orthogonal to the vector x(k+2) − x(k+1).
Proof. From the iterative formula of the method of steepest descent it follows that

To complete the proof it is enough to show that

To this end, observe that αk is a nonnegative scalar that minimizes ϕk(α)  f(x(k) − α∇f(x(k))). Hence, using the FONC and the chain rule gives us

which completes the proof.
The proposition above implies that ∇f(x(k)) is parallel to the tangent plane to the level set {f(x) = f(x(k+1))} at x(k+1). Note that as each new point is generated by the steepest descent algorithm, the corresponding value of the function f decreases in value, as stated below.
Proposition 8.2 If  is the steepest descent sequence for f : n →  and if ∇f(x(k)) ≠ 0, then f(x(k+1)) < f(x(k)).
Proof. First recall that

where αk > 0 is the minimizer of

over all α ≥ 0. Thus, for α ≥ 0, we have

By the chain rule,

because ∇f(x(k)) ≠ 0 by assumption. Thus, ϕk'(0) < 0 and this implies that there is an  such that ϕk(0) > ϕk(α) for all α  (0,]. Hence,

which completes the proof.
In Proposition 8.2, we proved that the algorithm possesses the descent property: f(x(k+1)) < f(x(k)) if ∇f(x(k)) ≠ 0. If for some k, we have ∇f(x(k)) = 0, then the point x(k) satisfies the FONC. In this case, x(k+1) = x(k). We can use the above as the basis for a stopping (termination) criterion for the algorithm.
The condition ∇f(x(k+1)) = 0, however, is not directly suitable as a practical stopping criterion, because the numerical computation of the gradient will rarely be identically equal to zero. A practical stopping criterion is to check if the norm ||∇f(x(k))|| of the gradient is less than a prespecified threshold, in which case we stop. Alternatively, we may compute the absolute difference |f(x(k+1)) − f(x(k))| between objective function values for every two successive iterations, and if the difference is less than some prespecified threshold, then we stop; that is, we stop when

where ε > 0 is a prespecified threshold. Yet another alternative is to compute the norm ||x(k+1) − x(k)|| of the difference between two successive iterates, and we stop if the norm is less than a prespecified threshold:

Alternatively, we may check "relative" values of the quantities above; for example,

or

The two (relative) stopping criteria above are preferable to the previous (absolute) criteria because the relative criteria are "scale-independent." For example, scaling the objective function does not change the satisfaction of the criterion |f(x(k+1))−f(x(k))|/|f(x(k))| < ε. Similarly, scaling the decision variable does not change the satisfaction of the criterion ||x(k+1) − x(x)||/||x(k))|| < ε. To avoid dividing by very small numbers, we can modify these stopping criteria as follows:

or

Note that the stopping criteria above are relevant to all the iterative algorithms we discuss in this part.
Example 8.1 We use the method of steepest descent to find the minimizer of

The initial point is x(0) = [4,2,−1]. We perform three iterations.
We find that

Hence,

To compute x(1), we need

Using the secant method from Section 7.6, we obtain

For illustrative purpose, we show a plot of ϕ0(α) versus α in Figure 8.3, obtained using MATLAB. Thus,

Figure 8.3 Plot of φ0(α) versus α.



To find x(2), we first determine

Next, we find α1, where

Using the secant method again, we obtain α1 = 0.5000. Figure 8.4 depicts a plot of ϕ1 (α) versus α. Thus,

Figure 8.4 Plot of ϕ1(α) versus α.



To find x(3), we first determine

and

We proceed as in the previous iterations to obtain α2 = 16.29. A plot of ϕ2(α) versus α is shown in Figure 8.5.

Figure 8.5 Plot of φ2(α) versus α.


The value of x(3) is

Note that the minimizer of f is [4,3,−5], and hence it appears that we have arrived at the minimizer in only three iterations. The reader should be cautioned not to draw any conclusions from this example about the number of iterations required to arrive at a solution in general.
It goes without saying that numerical computations, such as those in this example, are performed in practice using a computer (rather than by hand). The calculations above were written out explicitly, step by step, for the purpose of illustrating the operations involved in the steepest descent algorithm. The computations themselves were, in fact, carried out using a MATLAB program (see Exercise 8.25).
Let us now see what the method of steepest descent does with a quadratic function of the form

where Q  n×n is a symmetric positive definite matrix, b  n, and x  n. The unique minimizer of f can be found by setting the gradient of f to zero, where

because D (xQx) = x(Q + Q) = 2xQ, and D(bx) = b. There is no loss of generality in assuming Q to be a symmetric matrix. For if we are given a quadratic form xAx and A ≠ A, then because the transposition of a scalar equals itself, we obtain

Hence,

Note that

The Hessian of f is F(x) = Q = Q > 0. To simplify the notation we write g(k) = ∇f(x(k)). Then, the steepest descent algorithm for the quadratic function can be represented as

where

In the quadratic case, we can find an explicit formula for αk. We proceed as follows. Assume that g(k) ≠ 0, for if g(k) = 0, then x(k) = x* and the algorithm stops. Because αk ≥ 0 is a minimizer of ϕk(α) = f(x(k) − αg(k)), we apply the FONC to ϕk(α) to obtain

Therefore, ϕk'(α) = 0 if αg(k) Qg(k) = (x(k) Q − b)g(k). But

Hence,

In summary, the method of steepest descent for the quadratic takes the form

where

Example 8.2 Let

Then, starting from an arbitrary initial point x(0)  2, we arrive at the solution x* = 0  2 in only one step. See Figure 8.6.

Figure 8.6 Steepest descent method applied to f(x1, x2) = x21 + x22.


However, if

then the method of steepest descent shuffles ineffectively back and forth when searching for the minimizer in a narrow valley (see Figure 8.7). This example illustrates a major drawback in the steepest descent method. More sophisticated methods that alleviate this problem are discussed in subsequent chapters.

Figure 8.7 Steepest descent method in search for minimizer in a narrow valley.


To understand better the method of steepest descent, we examine its convergence properties in the next section.
8.3 Analysis of Gradient Methods
Convergence
The method of steepest descent is an example of an iterative algorithm. This means that the algorithm generates a sequence of points, each calculated on the basis of the points preceding it. The method is a descent method because as each new point is generated by the algorithm, the corresponding value of the objective function decreases in value (i.e., the algorithm possesses the descent property).
We say that an iterative algorithm is globally convergent if for any arbitrary starting point the algorithm is guaranteed to generate a sequence of points converging to a point that satisfies the FONC for a minimizer. When the algorithm is not globally convergent, it may still generate a sequence that converges to a point satisfying the FONC, provided that the initial point is sufficiently close to the point. In this case we say that the algorithm is locally convergent How close to a solution point we need to start for the algorithm to converge depends on the local convergence properties of the algorithm. A related issue of interest pertaining to a given locally or globally convergent algorithm is the rate of convergence; that is, how fast the algorithm converges to a solution point.
In this section we analyze the convergence properties of descent gradient methods, including the method of steepest descent and gradient methods with fixed step size. We can investigate important convergence characteristics of a gradient method by applying the method to quadratic problems. The convergence analysis is more convenient if instead of working with f we deal with

where Q = Q > 0. The solution point x* is obtained by solving Qx = b; that is, x* = Q−1b. The function V differs from f only by a constant x*Qx*. We begin our analysis with the following useful lemma that applies to a general gradient algorithm.
Lemma 8.1 The iterative algorithm

with g(k) = Qx(k) − b satisfies

where if g(k) = 0, then γk = 1, and if g(k) ≠ 0, then

Proof The proof is by direct computation. Note that if g(k) = 0, then the desired result holds trivially. In the remainder of the proof, assume that g(k) ≠ 0. We first evaluate the expression

To facilitate computations, let y(k) = x(k) − x*. Then, V(x(k)) = y(k)Qy(k). Hence,

Therefore,

Because

we have

Therefore, substituting the above, we get

Note that γk ≤ 1 for all k, because γk = 1 − V(x(k+1))/V(x(k)) and V is a nonnegative function. If γk = 1 for some k, then V(x(k+1)) = 0, which is equivalent to x(k+1) = x*. In this case we also have that for all i ≥ k + 1, x(i) = x* and γi = 1. It turns out that γk = 1 if and only if either g(k) = 0 or g(k) is an eigenvector of Q (see Lemma 8.3).
We are now ready to state and prove our key convergence theorem for gradient methods. The theorem gives a necessary and sufficient condition for the sequence {x(k)} generated by a gradient method to converge to x*; that is, x(k) → x* or limk→∞ x(k) = x*.
Theorem 8.1 Let {x(k)} be the sequence resulting from a gradient algorithm x(k+1) = x(k) − αkg(k). Let γk be as defined in Lemma 8.1, and suppose that γk > 0 for all k. Then, {x(k)} converges to x* for any initial condition x(0) if and only if

Proof. From Lemma 8.1 we have V(x(k+1)) = (1 − γk) V(x(k)), from which we obtain

Assume that γk < 1 for all k, for otherwise the result holds trivially. Note that x(k) → x* if and only if V(x(k)) → 0. By the equation above we see that this occurs if and only if , which, in turn, holds if and only if Σ∞i=0 − log(1 − γi) = ∞ (we get this simply by taking logs). Note that by Lemma 8.1, 1 − γi ≥ 0 and log(1 − γi) is well-defined [log(0) is taken to be −∞]. Therefore, it remains to show that Σ∞i=0 −log(1 − γi) = ∞ if and only if

We first show that  implies that . For this, first observe that for any x  , x > 0, we have log(x) ≤ x − 1 [this is easy to see simply by plotting log(x) and x − 1 versus x]. Therefore, log(1 − γi) ≤ 1 − γi − 1 = −γi, and hence −log(1 − γi) ≥ γi. Thus, if Σ∞i=0 γi = ∞, then clearly Σ∞i=0 − log(1 − γi) = ∞.
Finally, we show that Σ∞i=0 − log(1 − γi) = ∞ implies that Σ∞i=0 γi = ∞. We proceed by contraposition. Suppose that Σ∞i=0 γi < ∞. Then, it must be that γi → 0. Now observe that for x  , x ≤ 1 and x sufficiently close to 1, we have log(x) ≥ 2(x − 1) [as before, this is easy to see simply by plotting log(x) and 2(x − 1) versus x]. Therefore, for sufficiently large i, log(1 − γi) ≥ 2(1 − γi − 1) = −2γi, which implies that −log(1 − γi) ≤ 2γi. Hence, Σ∞i=0 γi < ∞ implies that Σ∞i=0 − log(1 − γi) < ∞.
This completes the proof.
The assumption in Theorem 8.1 that γk > 0 for all k is significant in that it corresponds to the algorithm having the descent property (see Exercise 8.23). Furthermore, the result of the theorem does not hold in general if we do not assume that γk > 0 for all k, as shown in the following example.
Example 8.3 We show, using a counterexample, that the assumption that γk > 0 in Theorem 8.1 is necessary for the result of the theorem to hold. Indeed, for each k = 0,1,2,..., choose αk in such a way that γ2k = −1/2 and γ2k+1 = 1/2 (we can always do this if, for example, Q = In). From Lemma 8.1 we have

Therefore, V(x(2k)) → 0. Because V(x(2k+1)) = (3/2)V(x(2k)), we also have that V(x(2k+1)) → 0. Hence, V(x(k)) → 0, which implies that x(k) → 0 (for all x(0)). On the other hand, it is clear that

for all k. Hence, the result of the theorem does not hold if γk ≤ 0 for some k.
Using the general theorem above, we can now establish the convergence of specific cases of the gradient algorithm, including the steepest descent algorithm and algorithms with fixed step size. In the analysis to follow, we use Rayleigh's inequality, which states that for any Q = Q > 0, we have

where λmin(Q) denotes the minimal eigenvalue of Q and λmax(Q) denotes the maximal eigenvalue of Q. For Q = Q > 0, we also have

and

Lemma 8.2 Let Q = Q > 0 be an n × n real symmetric positive definite matrix. Then, for any x  n, we have

Proof. Applying Rayleigh's inequality and using the properties of symmetric positive definite matrices listed previously, we get

and

We are now ready to establish the convergence of the steepest descent method.
Theorem 8.2 In the steepest descent algorithm, we have x(k) → x* for any x(0).
Proof. If g(k) = 0 for some k, then x(k) = x* and the result holds. So assume that g(k) ≠ 0 for all k. Recall that for the steepest descent algorithm,

Substituting this expression for αk in the formula for γk yields

Note that in this case γk > 0 for all k. Furthermore, by Lemma 8.2, we have γk ≥ (λmin(Q)/λmax(Q)) > 0. Therefore, we have Σ∞k=0 γk = ∞, and hence by Theorem 8.1 we conclude that x(k) → x*.
Consider now a gradient method with fixed step size; that is, αk = α   for all k. The resulting algorithm is of the form

We refer to the algorithm above as a fixed-step-size gradient algorithm. The algorithm is of practical interest because of its simplicity. In particular, the algorithm does not require a line search at each step to determine αk, because the same step size α is used at each step. Clearly, the convergence of the algorithm depends on the choice of α, and we would not expect the algorithm to work for arbitrary α. The following theorem gives a necessary and sufficient condition on α for convergence of the algorithm.
Theorem 8.3 For the fixed-step-size gradient algorithm, x(k) → x* for any x(0) if and only if

Proof. ⇐: By Rayleigh's inequality we have

and

Therefore, substituting the above into the formula for γk, we get

Therefore, γk > 0 for all k, and Σ∞k=0 γk = ∞. Hence, by Theorem 8.1 we conclude that x(k) → x*.
⇐: We use contraposition. Suppose that either α ≤ 0 or α ≥ 2/λmax(Q). Let x(0) be chosen such that x(0) − x* is an eigenvector of Q corresponding to the eigenvalue λmax(Q). Because

we obtain

where in the last line we used the property that x(0) − x* is an eigenvector of Q. Taking norms on both sides, we get

Because a ≤ 0 or α > 2/λmax (Q),

Hence, ||x(k+1) − x*|| cannot converge to 0, and thus the sequence {x(k)} does not converge to x*.
Example 8.4 Let the function f be given by

We wish to find the minimizer of f using a fixed-step-size gradient algorithm

where α   is a fixed step size.
To apply Theorem 8.3, we first symmetrize the matrix in the quadratic term of f to get

The eigenvalues of the matrix in the quadratic term are 6 and 12. Hence, using Theorem 8.3, the algorithm converges to the minimizer for all x(0) if and only if α lies in the range 0 < α < 2/12.
Convergence Rate
We now turn our attention to the issue of convergence rates of gradient algorithms. In particular, we focus on the steepest descent algorithm. We first present the following theorem.
Theorem 8.4 In the method of steepest descent applied to the quadratic function, at every step k we have

Proof. In the proof of Theorem 8.2, we showed that γk ≥ γmin (Q)/λmax (Q). Therefore,

and the result follows.
Theorem 8.4 is relevant to our consideration of the convergence rate of the steepest descent algorithm as follows. Let

called the condition number of Q. Then, it follows from Theorem 8.4 that

The term (1 − 1/r) plays an important role in the convergence of {V(x(k))} to 0 (and hence of {x(k)} to x*). We refer to (1 − 1/r) as the convergence ratio. Specifically, we see that the smaller the value of (1 − 1/r), the smaller V(x(k+1)) will be relative to V(x(k)), and hence the "faster" V (x(k)) converges to 0, as indicated by the inequality above. The convergence ratio (1 − 1/r) decreases as r decreases. If r = 1, then λmax (Q) = λmin (Q), corresponding to circular contours of f (see Figure 8.6). In this case the algorithm converges in a single step to the minimizer. As r increases, the speed of convergence of {V(x(k))} (and hence of {x(k)}) decreases. The increase in r reflects that fact that the contours of f are more eccentric (see, e.g., Figure 8.7). We refer the reader to [88, pp. 238, 239] for an alternative approach to the analysis above.
To investigate the convergence properties of {x(k)} further, we need the following definition.
Definition 8.1 Given a sequence {x(k)} that converges to x*, that is, limk→∞ ||x(k) − x*|| = 0, we say that the order of convergence is p, where p  , if

If for all p > 0,

then we say that the order of convergence is ∞.
Note that in the definition above, 0/0 should be understood to be 0.
The order of convergence of a sequence is a measure of its rate of convergence; the higher the order, the faster the rate of convergence. The order of convergence is sometimes also called the rate of convergence (see, e.g., [96]). If p = 1 (first-order convergence) and limk→∞ ||x(k+1) − x*||/||x(k) − x*|| = 1, we say that the convergence is sublinear. If p = 1 and limk→∞ ||x(k+1) − x*||/||x(k) − x*|| < 1, we say that the convergence is linear. If p > 1, we say that the convergence is superlinear. If p = 2 (second-order convergence), we say that the convergence is quadratic.
Example 8.5 1. Suppose that x(k) = 1/k and thus x(k) → 0. Then,


If p < 1, the sequence above converges to 0, whereas if p > 1, it grows to ∞. If p = 1, the sequence converges to 1. Hence, the order of convergence is 1 (i.e., we have linear convergence).
2. Suppose that x(k) = γk, where 0 < γ < 1, and thus x(k) → 0. Then,

If p < 1, the sequence above converges to 0, whereas if p > 1, it grows to γ. If p = 1, the sequence converges to γ (in fact, remains constant at γ). Hence, the order of convergence is 1.
3. Suppose that x(k) = γ(qk), where q > 1 and 0 < γ < 1, and thus x(k) → 0. Then,

If p < q, the sequence above converges to 0, whereas if p > q, it grows to ∞. If p = q, the sequence converges to 1 (in fact, remains constant at 1). Hence, the order of convergence is q.
4. Suppose that x(k) = 1 for all k, and thus x(k) → 1 trivially. Then,

for all p. Hence, the order of convergence is ∞.

The order of convergence can be interpreted using the notion of the order symbol O, as follows. Recall that a = O(h) ("big-oh of ft") if there exists a constant c such that |a| ≤ c|h| for sufficiently small h. Then, the order of convergence is at least p if

(see Theorem 8.5 below). For example, the order of convergence is at least 2 if

(this fact is used in the analysis of Newton's algorithm in Chapter 9).
Theorem 8.5 Let {x(k)} be a sequence that converges to x*. If

then the order of convergence (if it exists) is at least p.
Proof. Let s be the order of convergence of {x(k)}. Suppose that

Then, there exists c such that for sufficiently large k,

Hence,

Taking limits yields

Because by definition s is the order of convergence,

Combining the two inequalities above, we get

Therefore, because limk→∞ ||x(k) − x* = 0, we conclude that s ≥ p; that is, the order of convergence is at least p.
By an argument similar to the above, we can show that if

then the order of convergence (if it exists) strictly exceeds p.
Example 8.6 Suppose that we are given a scalar sequence {x(k)} that converges with order of convergence p and satisfies

The limit of {x(k)} must be 2, because it is clear from the equation that |x(k+1) − 2| → 0. Also, we see that x(k+1) − 2| = o(|x(k) − 2|3). Hence, we conclude that p > 3.
It turns out that the order of convergence of any convergent sequence cannot be less than 1 (see Exercise 8.3). In the following, we provide an example where the order of convergence of a fixed-step-size gradient algorithm exceeds 1.
Example 8.7 Consider the problem of finding a minimizer of the function f :  →  given by

Suppose that we use the algorithm x(k+1) = x(k) − αf′(x(k)) with step size α = 1/2 and initial condition x(0) = 1. (The notation f′ represents the derivative of f.)
We first show that the algorithm converges to a local minimizer of f. Indeed, we have f′(x) = 2x − x2. The fixed-step-size gradient algorithm with step size α = 1/2 is therefore given by

With α(0) = 1, we can derive the expression x(k) = (1/2)2k−1. Hence, the algorithm converges to 0, a strict local minimizer of f.
Next, we find the order of convergence. Note that

Therefore, the order of convergence is 2.
Finally, we show that the steepest descent algorithm has an order of convergence of 1 in the worst case; that is, there are cases for which the order of convergence of the steepest descent algorithm is equal to 1. To proceed, we will need the following simple lemma.
Lemma 8.3 In the steepest descent algorithm, if g(k) ≠ 0 for all k, then γk = 1 if and only if g(k) is an eigenvector of Q.
Proof. Suppose that g(k) ≠ 0 for all k. Recall that for the steepest descent algorithm,

Sufficiency is easy to show by verification. To show necessity, suppose that γk = 1. Then, V(x(k+1)) = 0, which implies that x(k+1) = x*. Therefore,

Premultiplying by Q and subtracting b from both sides yields

which can be rewritten as

Hence, g(k) is an eigenvector of Q.
By the lemma, if g(k) is not an eigenvector of Q, then γk < 1 (recall that γk cannot exceed 1). We use this fact in the proof of the following result on the worst-case order of convergence of the steepest descent algorithm.
Theorem 8.6 Let {x(k)} be a convergent sequence of iterates of the steepest descent algorithm applied to a function f. Then, the order of convergence of {x(k)} is 1 in the worst case; that is, there exist a function f and an initial condition x(0) such that the order of convergence of {x(k)} is equal to 1.
Proof Let f : n →  be a quadratic function with Hessian Q. Assume that the maximum and minimum eigenvalues of Q satisfy λmax (Q) > λmin (Q). To show that the order of convergence of {x(k)} is 1, it suffices to show that there exists x(0) such that

for some c > 0 (see Exercise 8.2). Indeed, by Rayleigh's inequality,

Similarly,

Combining the inequalities above with Lemma 8.1, we obtain

Therefore, it suffices to choose x(0) such that γk ≤ d for some d < 1.
Recall that for the steepest descent algorithm, assuming that g(k) ≠ 0 for all k, γk depends on g(k) according to

First consider the case where n = 2. Suppose that x(0) ≠ x* is chosen such that x(0) − x* is not an eigenvector of Q. Then, g(0) = Q(x(0) − x*) ≠ 0 is also not an eigenvector of Q. By Proposition 8.1, g(k) = (x(k+1) −x(k))/αk is not an eigenvector of Q for any k [because any two eigenvectors corresponding to λmax (Q) and λmin (Q) are mutually orthogonal]. Also, g(k) lies in one of two mutually orthogonal directions. Therefore, by Lemma 8.3, for each k, the value of γk is one of two numbers, both of which are strictly less than 1. This proves the n = 2 case.
For the general n case, let V1 and v2 be mutually orthogonal eigenvectors corresponding to λmax (Q) and λmin (Q). Choose x(0) such that x(0) − x* ≠ 0 lies in the span of v1 and v2 but is not equal to either. Note that g(0) = Q (x(0) − x*) also lies in the span of v1 and v2, but is not equal to either. By manipulating x(k+1) = x(k) − αkg(k) as before, we can write g(k+1) = (I − αkQ)g(k). Any eigenvector of Q is also an eigenvector of I − αkQ. Therefore, g(k) lies in the span of v1 and v2 for all k; that is, the sequence {g(k)} is confined within the two-dimensional subspace spanned by v1 and v2. We can now proceed as in the n = 2 case.
In the next chapter we discuss Newton's method, which has order of convergence at least 2 if the initial guess is near the solution.
EXERCISES

8.1 Perform two iterations leading to the minimization of

using the steepest descent method with the starting point x(0) = 0. Also determine an optimal solution analytically.
8.2 Let {x(k)} be a sequence that converges to x*. Show that if there exists c > 0 such that

for sufficiently large k, then the order of convergence (if it exists) is at most p.
8.3 Let {x(k)} be a sequence that converges to x*. Show that there does not exist p < 1 such that

8.4 Consider the sequence {x(k)} given by x(k) = 2−2k2.
a. Write down the value of the limit of {x(k)}.
b. Find the order of convergence of {x(k)}.
8.5 Consider the two sequences {x(k)} and {y(k)} defined iteratively as follows:

where a  , b  , 0 < a < 1, b > 1, x(0) ≠ 0, y(0) ≠ 0, and |y(0)| < 1.
a. Derive a formula for x(k) in terms of x(0) and a. Use this to deduce that x(k) 0. → 0.
b. Derive a formula for y(k) in terms of y(0) and b. Use this to deduce that y(k) → 0.
c. Find the order of convergence of {x(k)} and the order of convergence of {y(k)}.
d. Calculate the smallest number of iterations k such that |x(k)| ≤ c|x(0)|, where 0 < c < 1.
Hint: The answer is in terms of a and c. You may use the notation  to represent the smallest integer not smaller than z.
e. Calculate the smallest number of iterations k such that |y(k)| ≤ c|y(0)|, where 0 < c < 1.
f. Compare the answer of part e with that of part d, focusing on the case where c is very small.
8.6 Suppose that we use the golden section algorithm to find the minimizer of a function. Let uk be the uncertainty range at the kth iteration. Find the order of convergence of {uk}.
8.7 Suppose that we wish to minimize a function f :  →  that has a derivative f′. A simple line search method, called derivative descent search (DDS), is described as follows: given that we are at a point x(k), we move in the direction of the negative derivative with step size α; that is, x(k+1) = x(k) − αf′(x(k)), where α > 0 is a constant.
   In the following parts, assume that f is quadratic: f(x) = ax2 − bx + c (where a, b, and c are constants, and a > 0).
a. Write down the value of x* (in terms of a, b, and c) that minimizes f.
b. Write down the recursive equation for the DDS algorithm explicitly for this quadratic f.
c. Assuming that the DDS algorithm converges, show that it converges to the optimal value x* (found in part a).
d. Find the order of convergence of the algorithm, assuming that it does converge.
e. Find the range of values of α for which the algorithm converges (for this particular f) for all starting points x(0).
8.8 Consider the function

where x = [x1, x2]  2. Suppose that we use a fixed-step-size gradient algorithm to find the minimizer of f:

Find the largest range of values of α for which the algorithm is globally convergent.
8.9 This exercise explores a zero-finding algorithm.
Suppose that we wish to solve the equation h(x) = 0, where

Consider using an algorithm of the form x(k+1) = x(k) − αh(x(k)), where α is scalar constant that does not depend on k.
a. Find the solution of h(x) = 0.
b. Find the largest range of values of α such that the algorithm is globally convergent to the solution of h(x) = 0.
c. Assuming that α is outside the range of values in part b, give an example of an initial condition x(0) of the form [x1, 0] such that the algorithm is guaranteed not to satisfy the descent property.
8.10 Consider the function f : 2 →  given by

where a and b are some unknown real-valued parameters.
a. Write the function f in the usual multivariable quadratic form.
b. Find the largest set of values of a and b such that the unique global minimizer of f exists, and write down the minimizer (in terms of the parameters a and b).
c. Consider the following algorithm:

Find the largest set of values of a and b for which this algorithm converges to the global minimizer of f for any initial point x(0).
8.11 Consider the function f :  →  given by f(x) = (x − c)2, c  . We are interested in computing the minimizer of f using the iterative algorithm

where f′ is the derivative of f and αk is a step size satisfying 0 < αk < 1.
a. Derive a formula relating f(x(k+1)) with f(x(k)), involving αk.
b. Show that the algorithm is globally convergent if and only if

Hint: Use part a and the fact that for any sequence {αk} ⊂ (0, 1), we have

8.12 Consider the function f :  →  given by f(x) = x3 − x. Suppose that we use a fixed-step-size algorithm x(k+1) = x(k) − αf′ (x(k)) to find a local minimizer of f. Find the largest range of values of α such that the algorithm is locally convergent (i.e., for all x0 sufficiently close to a local minimizer x*, we have x(k) → x*).
8.13 Consider the function f given by f(x) = (x − 1)2, x  . We are interested in computing the minimizer of f using the iterative algorithm x(k+1) = x(k) − α2−k f′(x(k)), where f′ is the derivative of f and 0 < α < 1. Does the algorithm have the descent property? Is the algorithm globally convergent?
8.14 Let f :  → , f  3, with first derivative f′, second derivative f″, and unique minimizer x*. Consider a fixed-step-size gradient algorithm

Suppose that f″ (x*) ≠ 0 and α = 1/f″ (x*). Assuming that the algorithm converges to x*, show that the order of convergence is at least 2.
8.15 Consider the problem of minimizing f(x) = ||ax − b||2, where a and b are vectors in n, and a ≠ 0.
a. Derive an expression (in terms of a and b) for the solution to this problem.
b. To solve the problem, suppose that we use an iterative algorithm of the form

where f′ is the derivative of f. Find the largest range of values of α (in terms of a and b) for which the algorithm converges to the solution for all starting points x(0).
8.16 Consider the optimization problem

where A  m × n, m ≥ n, and b  m.
a. Show that the objective function for this problem is a quadratic function, and write down the gradient and Hessian of this quadratic.
b. Write down the fixed-step-size gradient algorithm for solving this optimization problem.
c. Suppose that

Find the largest range of values for α such that the algorithm in part b converges to the solution of the problem.
8.17 Consider a function f : n → n given by f(x) = Ax + b, where A  n × n and b  n. Suppose that A is invertible and x* is the zero of f [i.e., f(x*) = 0]. We wish to compute x* using the iterative algorithm

where α  , α > 0. We say that the algorithm is globally monotone if for any x(0), ||x(k+1) − x*|| ≤ ||x(k) − x*|| for all k.
a. Assume that all the eigenvalues of A are real. Show that a necessary condition for the algorithm above to be globally monotone is that all the eigenvalues of A are nonnegative.
Hint: Use contraposition.
b. Suppose that

Find the largest range of values of α for which the algorithm is globally convergent (i.e., x(k) → x* for all x(0)).
8.18 Let f : n →  be given by f(x) = x Qx − x b, where b  n and Q is a real symmetric positive definite n × n matrix. Suppose that we apply the steepest descent method to this function, with x(0) ≠ Q−1 b. Show that the method converges in one step, that is, x(1) = Q−1 b, if and only if x(0) is chosen such that g(0) = Qx(0) − b is an eigenvector of Q.
8.19 Suppose that we apply the steepest descent algorithm x(k+1) = x(k) − αkg(k) to a quadratic function f with Hessian Q > 0. Let λmax and λmin be the largest and smallest eigenvalue of Q, respectively. Which of the following two inequalities are possibly true? (When we say here that an inequality is "possibly" true, we mean that there exists a choice of f and x(0) such that the inequality holds.)
a. α0 ≥ 2/λmax.
b. α0 > 1/λmin.
8.20 Suppose that we apply a fixed-step-size gradient algorithm to minimize

a. Find the range of values of the step size for which the algorithm converges to the minimizer.
b. Suppose that we use a step size of 1000 (which is too large). Find an initial condition that will cause the algorithm to diverge (not converge).
8.21 Consider a fixed-step-size gradient algorithm applied to each of the functions f : 2 →  in parts a and b below. In each case, find the largest range of values of the step size a for which the algorithm is globally convergent.
a. f(x) = 1 + 2x1 + 3(x21 + x22) + 4x1x2.
b. 
8.22 Let f : n →  be given by f(x) = x Qx − x b, where b  n and Q is a real symmetric positive definite n × n matrix. Consider the algorithm

where g(k) = Qx(k) − b, αk = g(k) g(k)/g(k) Qg(k) and β   is a given constant. (Note that the above reduces to the steepest descent algorithm if β = 1.) Show that {x(k)} converges to x* = Q−1 b for any initial condition x(0) if and only if 0 < β < 2.
8.23 Let f : n →  be given by f(x) = x Qx − x b, where b  n and Q is a real symmetric positive definite n × n matrix. Consider a gradient algorithm

where g(k) = Qx(k) − b is the gradient of f at x(k) and αk is some step size. Show that the algorithm has the descent property [i.e., f(x(k+1)) < f(x(k)) whenever g(k) ≠ 0] if and only if γk > 0 for all k.
8.24 Given f : n → , consider the general iterative algorithm

where d(1), d(2),... are given vectors in n and αk is chosen to minimize f(x(k) + αd(k));that is,

Show that for each k, the vector x(k+1) − x(k) is orthogonal to ∇f(x(k+1)) (assuming that the gradient exists).
8.25 Write a simple MATLAB program for implementing the steepest descent algorithm using the secant method for the line search (e.g., the MATLAB function of Exercise 7.11). For the stopping criterion, use the condition ||g(k)|| ≤ ε, where ε = 10−6. Test your program by comparing the output with the numbers in Example 8.1. Also test your program using an initial condition of [−4,5,1], and determine the number of iterations required to satisfy the stopping criterion. Evaluate the objective function at the final point to see how close it is to 0.
8.26 Apply the MATLAB program from Exercise 8.25 to Rosenbrock's function:

Use an initial condition of x(0) = [-2, 2]. Terminate the algorithm when the norm of the gradient of f is less than 10−4.








CHAPTER 9
NEWTON'S METHOD
9.1 Introduction
Recall that the method of steepest descent uses only first derivatives (gradients) in selecting a suitable search direction. This strategy is not always the most effective. If higher derivatives are used, the resulting iterative algorithm may perform better than the steepest descent method. Newton's method (sometimes called the Newton-Raphson method) uses first and second derivatives and indeed does perform better than the steepest descent method if the initial point is close to the minimizer. The idea behind this method is as follows. Given a starting point, we construct a quadratic approximation to the objective function that matches the first and second derivative values at that point. We then minimize the approximate (quadratic) function instead of the original objective function. We use the minimizer of the approximate function as the starting point in the next step and repeat the procedure iteratively. If the objective function is quadratic, then the approximation is exact, and the method yields the true minimizer in one step. If, on the other hand, the objective function is not quadratic, then the approximation will provide only an estimate of the position of the true minimizer. Figure 9.1 illustrates this idea.

Figure 9.1 Quadratic approximation to the objective function using first and second derivatives.


We can obtain a quadratic approximation to the twice continuously differentiable objection function f : n →  using the Taylor series expansion of f about the current point x(k) neglecting terms of order three and higher. We obtain

where, for simplicity, we use the notation g(k) = ∇ f(x(k)). Applying the FONC to q yields

If F(x(k)) > 0, then q achieves a minimum at

This recursive formula represents Newton's method.
Example 9.1 Use Newton's method to minimize the Powell function:

Use as the starting point x(0) = [3, −1, 0, 1]. Perform three iterations. Note that f(x(0)) = 215. We have

and F(x) is given by

Iteration 1

Hence,

Iteration 2

Hence,

Iteration 3

Observe that the kth iteration of Newton's method can be written in two steps as

1. Solve F(x(k))d(k) = −g(k) for d(k).
2. Set x(k+1) = x(k) + d(k).

Step 1 requires the solution of an n × n system of linear equations. Thus, an efficient method for solving systems of linear equations is essential when using Newton's method.
As in the one-variable case, Newton's method can also be viewed as a technique for iteratively solving the equation

where x  n and g : n → n. In this case F(x) is the Jacobian matrix of g at x; that is, F(x) is the n × n matrix whose (i,j) entry is (∂gi/∂xj)(x), i, j = 1, 2, ..., n.
9.2 Analysis of Newton's Method
As in the one-variable case there is no guarantee that Newton's algorithm heads in the direction of decreasing values of the objective function if F(x(k)) is not positive definite (recall Figure 7.7 illustrating Newton's method for functions of one variable when f″ < 0). Moreover, even if F(x(k)) > 0, Newton's method may not be a descent method; that is, it is possible that f(x((k+1)) ≥ f(x(k)). For example, this may occur if our starting point x(0) is far away from the solution. See the end of this section for a possible remedy to this problem. Despite these drawbacks, Newton's method has superior convergence properties when the starting point is near the solution, as we shall see in the remainder of this section.
The convergence analysis of Newton's method when f is a quadratic function is straightforward. In fact, Newton's method reaches the point x* such that ∇ f(x*) = 0 in just one step starting from any initial point x(0). To see this, suppose that Q = Q is invertible and

Then,

and

Hence, given any initial point x(0) by Newton's algorithm

Therefore, for the quadratic case the order of convergence of Newton's algorithm is ∞ for any initial point x(0) (compare this with Exercise 8.18, which deals with the steepest descent algorithm).
To analyze the convergence of Newton's method in the general case, we use results from Section 5.1. Let {x(k)} be the Newton's method sequence for minimizing a function f : n → . We show that {x(k)} converges to the minimizer x* with order of convergence at least 2.
Theorem 9.1 Suppose that f  ⊂3 and x*  1 is a point such that ∇ f(x*) = 0 and F(x*) is invertible. Then, for all x(0) sufficiently close to x*, Newton's method is well-defined for all k and converges to x* with an order of convergence at least 2.
Proof. The Taylor series expansion of ∇f about x(0) yields

Because by assumption f  3 and F(x*) is invertible, there exist constants ε > 0, c1 > 0, and c2 > 0 such that if x(0), x  {x : ||x − x*|| ≤ ε}, we have

and by Lemma 5.3, F(x)−1 exists and satisfies

The first inequality above holds because the remainder term in the Taylor series expansion contains third derivatives of f that are continuous and hence bounded on {x : ||x − x*|| ≤ ε}.
Suppose that x(0)  {x : ||x − x*|| ≤ ε}. Then, substituting x = x* in the inequality above and using the assumption that ∇ f(x*) = 0, we get

Now, subtracting x* from both sides of Newton's algorithm and taking norms yields

Applying the inequalities above involving the constants c1 and c2 gives

Suppose that x(0) is such that

where α ε (0, 1). Then,

By induction, we obtain

Hence,

and therefore the sequence {x(k)} converges to x*. The order of convergence is at least 2 because ||x(k+1)−x*|| ≤ c1c2||x(k)−x*||2; that is, ||x(k+1)−x*|| = O(||x(k)−x*||2).
Warning: In the Theorem 9.1, we did not state that x* is a local minimizer. For example, if x* is a local maximizer, then provided that f  3 and F(x*) is invertible, Newton's method would converge to x* if we start close enough to it.
As stated in Theorem 9.1, Newton's method has superior convergence properties if the starting point is near the solution. However, the method is not guaranteed to converge to the solution if we start far away from it (in fact, it may not even be well-defined because the Hessian may be singular). In particular, the method may not be a descent method; that is, it is possible that f(x(k+1)) ≥ f(x(k)). Fortunately, it is possible to modify the algorithm such that the descent property holds. To see this, we need the following result.
Theorem 9.2 Let {x(k)} be the sequence generated by Newton's method for minimizing a given objective function f(x). If the Hessian F(x(k)) > 0 and g(k) = ∇ f (x(k)) ≠ 0, then the search direction

from x(k) to x(k+1) is a descent direction for f in the sense that there exists an  > 0 such that for all α  (0, ),

Proof Let

Then, using the chain rule, we obtain

Hence,

because F(x(k))−1 > 0 and g(k) ≠ 0. Thus, there exists an  > 0 so that for all α  (0, ), (ϕ)(α) < ϕ(0). This implies that for all α  (0, ),

which completes the proof.
Theorem 9.2 motivates the following modification of Newton's method:

where

that is, at each iteration, we perform a line search in the direction −F(x(k))−1g(k). By Theorem 9.2 we conclude that the modified Newton's method has the descent property; that is,

whenever g(k) ≠ 0.
A drawback of Newton's method is that evaluation of F(x(k)) for large n can be computationally expensive. Furthermore, we have to solve the set of n linear equations F(x(k))d(k) = −g(k). In Chapters 10 and 11 we discuss methods that alleviate this difficulty.
Another source of potential problems in Newton's method arises from the Hessian matrix not being positive definite. In the next section we describe a simple modification of Newton's method to overcome this problem.
9.3 Levenberg-Marquardt Modification
If the Hessian matrix F(x(k)) is not positive definite, then the search direction d(k) = −F(x(k))−1g(k) may not point in a descent direction. A simple technique to ensure that the search direction is a descent direction is to introduce the Levenberg-Marquardt modification of Newton's algorithm:

where μk ≥ 0.
The idea underlying the Levenberg-Marquardt modification is as follows. Consider a symmetric matrix F, which may not be positive definite. Let λ1, ..., λn be the eigenvalues of F with corresponding eigenvectors v1, ..., vn. The eigenvalues λ1, ..., λn are real, but may not all be positive. Next, consider the matrix G = F + μI, where μ ≥ 0. Note that the eigenvalues of G are λ1 + μ, ..., λn + μ. Indeed,

which shows that for all i = 1, ..., n, vi is also an eigenvector of G with eigenvalue λi + μ. Therefore, if μ is sufficiently large, then all the eigenvalues of G are positive and G is positive definite. Accordingly, if the parameter μk in the Levenberg-Marquardt modification of Newton's algorithm is sufficiently large, then the search direction d(k) = −(Fx(k) + μkI)−1 g(k) always points in a descent direction (in the sense of Theorem 9.2). In this case if we further introduce a step size αk as described in Section 9.2,

then we are guaranteed that the descent property holds.
The Levenberg-Marquardt modification of Newton's algorithm can be made to approach the behavior of the pure Newton's method by letting μk → 0. On the other hand, by letting μk → ∞, the algorithm approaches a pure gradient method with small step size. In practice, we may start with a small value of μk and increase it slowly until we find that the iteration is descent: f(x(k+1)) < f(x(k)).
9.4 Newton's Method for Nonlinear Least Squares
We now examine a particular class of optimization problems and the use of Newton's method for solving them. Consider the following problem:

where ri : n → , i = 1, ..., m, are given functions. This particular problem is called a nonlinear least-squares problem. The special case where the ri are linear is discussed in Section 12.1.
Example 9.2 Suppose that we are given m measurements of a process at m points in time, as depicted in Figure 9.2 (here, m = 21). Let t1, ..., tm denote the measurement times and y1, ..., ym the measurement values. Note that t1 = 0 while t21 = 10. We wish to fit a sinusoid to the measurement data. The equation of the sinusoid is

Figure 9.2 Measurement data for Example 9.2.



with appropriate choices of the parameters A, ω, and ϕ. To formulate the data-fitting problem, we construct the objective function

representing the sum of the squared errors between the measurement values and the function values at the corresponding points in time. Let x = [A, ω, ϕ] represent the vector of decision variables. We therefore obtain a nonlinear least-squares problem with

Defining r = [r1, ..., rm], we write the objective function as f(x) = r(x) r(x). To apply Newton's method, we need to compute the gradient and the Hessian of f. The jth component of ∇ f(x) is

Denote the Jacobian matrix of r by

Then, the gradient of f can be represented as

Next, we compute the Hessian matrix of f. The (k, j)th component of the Hessian is given by

Letting S(x) be the matrix whose (k, j)th component is

we write the Hessian matrix as

Therefore, Newton's method applied to the nonlinear least-squares problem is given by

In some applications, the matrix S(x) involving the second derivatives of the function r can be ignored because its components are negligibly small. In this case Newton's algorithm reduces to what is commonly called the Gauss-Newton method:

Note that the Gauss-Newton method does not require calculation of the second derivatives of r.
Example 9.3 Recall the data-fitting problem in Example 9.2, with

The Jacobian matrix J(x) in this problem is a 21 × 3 matrix with elements given by

Using the expressions above, we apply the Gauss-Newton algorithm to find the sinusoid of best fit, given the data pairs (t1, y1), ..., (tm, ym). Figure 9.3 shows a plot of the sinusoid of best fit obtained from the Gauss-Newton algorithm. The parameters of this sinusoid are: A = 2.01, Ω = 0.992, and ϕ = 0.541.

Figure 9.3 Sinusoid of best fit in Example 9.3.


A potential problem with the Gauss-Newton method is that the matrix J(x) J(x) may not be positive definite. As described before, this problem can be overcome using a Levenberg-Marquardt modification:

This is referred to in the literature as the Levenberg-Marquardt algorithm, because the original Levenberg-Marquardt modification was developed specifically for the nonlinear least-squares problem. An alternative interpretation of the Levenberg-Marquardt algorithm is to view the term μkI as an approximation to S(x) in Newton's algorithm.
EXERCISES

9.1 Let f :  →  be given by f(x) = (x − x0)4, where x0   is a constant. Suppose that we apply Newton's method to the problem of minimizing f.
a. Write down the update equation for Newton's method applied to the problem.
b. Let y(k) = |x(x) − x0|, where x(k) is the kth iterate in Newton's method. Show that the sequence {y(k)} satisfies y(k+1) = y(k).
c. Show that x(k) → x0 for any initial guess x(0).
d. Show that the order of convergence of the sequence {x(k)} in part b is 1.
e. Theorem 9.1 states that under certain conditions, the order of convergence of Newton's method is at least 2. Why does that theorem not hold in this particular problem?
9.2 This question relates to the order of convergence of the secant method, using an argument similar to that of the proof of Theorem 9.1.
a. Consider a function f :  → , f  ⊂2, such that x* is a local minimizer and f″(x*) ≠ 0. Suppose that we apply the algorithm x(k+1) = x(k) − αkf′(x(k)) such that {αk} is a positive step-size sequence that converges to 1/f″(x*). Show that if x(k) → x*, then the order of convergence of the algorithm is superlinear (i.e., strictly greater than 1).
b. Given part a, what can you say about the order of convergence of the secant algorithm?
9.3 Consider the problem of minimizing . Note that 0 is the global minimizer of f.
a. Write down the algorithm for Newton's method applied to this problem.
b. Show that as long as the starting point is not 0, the algorithm in part a does not converge to 0 (no matter how close to 0 we start).
9.4 Consider Rosenbrock's Function: f(x) = 100(x2 − x21)2 + (1 − x1)2, where x = [x1, x2] (known to be a "nasty" function—often used as a benchmark for testing algorithms). This function is also known as the banana function because of the shape of its level sets.
a. Prove that [1, 1] is the unique global minimizer of f over 2.
b. With a starting point of [0, 0], apply two iterations of Newton's method.

c. Repeat part b using a gradient algorithm with a fixed step size of αk = 0.05 at each iteration.
9.5 Consider the modified Newton's algorithm

where αk = arg minα≥0 f(x(k) − αF(x(k))−1g(k)). Suppose that we apply the algorithm to a quadratic function f(x) = 1/2x Qx − xb, where Q = Q > 0. Recall that the standard Newton's method reaches point x* such that ∇ f(x*) = 0 in just one step starting from any initial point x(0). Does the modified Newton's algorithm above possess the same property?








CHAPTER 10
CONJUGATE DIRECTION METHODS
10.1 Introduction
The class of conjugate direction methods can be viewed as being intermediate between the method of steepest descent and Newton's method. The conjugate direction methods have the following properties:

1. Solve quadratics of n variables in n steps.
2. The usual implementation, the conjugate gradient algorithm, requires no Hessian matrix evaluations.
3. No matrix inversion and no storage of an n × n matrix are required.

The conjugate direction methods typically perform better than the method of steepest descent, but not as well as Newton's method. As we saw from the method of steepest descent and Newton's method, the crucial factor in the efficiency of an iterative search method is the direction of search at each iteration. For a quadratic function of n variables f(x) = xQx − xb, x  n, Q = Q > 0, the best direction of search, as we shall see, is in the Q-conjugate direction. Basically, two directions d(1) and d(2) in n are said to be Q-conjugate if d(1) Qd(2) = 0. In general, we have the following definition.
Definition 10.1 Let Q be a real symmetric n × n matrix. The directions d(0), d(1), d(2), ..., d(m) are Q-conjugate if for all i ≠ j, we have d(i) Qd(j) = 0.
Lemma 10.1 Let Q be a symmetric positive definite n × n matrix. If the directions d(0), d(1), ..., d(k)  n, k ≤ n − 1, are nonzero and Q-conjugate, then they are linearly independent.
Proof. Let α0, ..., αk be scalars such that

Premultiplying this equality by d(j) Q, 0 ≤ j ≤ k, yields

because all other terms d(j) Qd(i) = 0, i ≠ j, by Q-conjugacy. But Q = Q > 0 and d(j) ≠ 0; hence αj = 0, j = 0, 1, ..., k. Therefore, d(0), d(1), ..., d(k), k ≤ n − 1, are linearly independent.
Example 10.1 Let

Note that Q = Q > 0. The matrix Q is positive definite because all its leading principal minors are positive:

Our goal is to construct a set of Q-conjugate vectors d(0), d(1), d(2).
Let . We require that d(0) Qd(1) = 0. We have

Let d(1)1 = 1, d(1)2 = 0, d(1)3 = −3. Then, d(1) = [1, 0, −3], and thus d(0)Qd(1) = 0.
To find the third vector d(2) which would be Q-conjugate with d(0) and d(1), we require that d(0)Qd(2) = 0 and d(1) Qd(2) = 0. We have

If we take d(2) = [1, 4, −3], then the resulting set of vectors is mutually conjugate.
This method of finding Q-conjugate vectors is inefficient. A systematic procedure for finding Q-conjugate vectors can be devised using the idea underlying the Gram-Schmidt process of transforming a given basis of n into an orthonormal basis of n (see Exercise 10.1).
10.2 The Conjugate Direction Algorithm
We now present the conjugate direction algorithm for minimizing the quadratic function of n variables

where Q = Q > 0, x  n. Note that because Q > 0, the function f has a global minimizer that can be found by solving Qx = b.
Basic Conjugate Direction Algorithm. Given a starting point x(0) and Q-conjugate directions d(0), d(1) ..., d(n−1); for k ≥ 0,

Theorem 10.1 For any starting point x(0) the basic conjugate direction algorithm converges to the unique x* (that solves Qx = b) in n steps; that is, x(n) = x*.
Proof. Consider x* − x(0)  n. Because the d(i) are linearly independent, there exist constants βi, i = 0, ..., n − 1, such that

Now premultiply both sides of this equation by d(k) Q, 0 ≤ k < n, to obtain

where the terms d(k) Qd(i) = 0, k ≠ i, by the Q-conjugate property. Hence,

Now, we can write

Therefore,

So writing

and premultiplying the above by d(k) Q, we obtain

because g(k) = Qx(k) − b and Qx* = b Thus,

and x* = xn which completes the proof.
Example 10.2 Find the minimizer of

using the conjugate direction method with the initial point x(0) = [0,0], and Q-conjugate directions d(0) = [1,0] and d1 = .
We have

and hence

Thus,

To find x(2) we compute

and

Therefore,

Because f is a quadratic function in two variables, x(2) = x*.
For a quadratic function of n variables, the conjugate direction method reaches the solution after n steps. As we shall see below, the method also possesses a certain desirable property in the intermediate steps. To see this, suppose that we start at x(0) and search in the direction d(0) to obtain

We claim that

To see this,

The equation g d(0) = 0 implies that α0 has the property that α0 = arg min ϕ0 (α), where ϕ0(α) = f(x(0) + αd(0)). To see this, apply the chain rule to get

Evaluating the above at α = α0, we get

Because ϕ0 is a quadratic function of α, and the coefficient of the α2 term in ϕ0 is d(0) Qd(0) > 0, the above implies that α0 = arg minαϕ0(α).
Using a similar argument, we can show that for all k,

and hence

In fact, an even stronger condition holds, as given by the following lemma.
Lemma 10.2 In the conjugate direction algorithm,

for all k, 0 ≤ k ≤ n − 1, and 0 ≤ i ≤ k.
Proof. Note that

because g(k) = Qx(k) − b. Thus,

We prove the lemma by induction. The result is true for k = 0 because g(1) d(0) = 0, as shown before. We now show that if the result is true for k − 1 (i.e., g(k) d(i) = 0, i ≤ k − 1), then it is true for k (i.e., g(k+1) d(i) = 0, i ≤ k). Fix k > 0 and 0 ≤ i < k. By the induction hypothesis, g(k) d(i) = 0.
Because

and d(k) Qd(i) = 0 by Q-conjugacy, we have

It remains to be shown that

Indeed,

because Qx(k) − b = g(k).
Therefore, by induction, for all 0 ≤ k ≤ n − 1 and 0 ≤ i < k,

By Lemma 10.2 we see that g(k+1) is orthogonal to any vector from the subspace spanned by d(0) d(1), ..., d(k) Figure 10.1 illustrates this statement.

Figure 10.1 Illustration of Lemma 10.2.


The lemma can be used to show an interesting optimal property of the conjugate direction algorithm. Specifically, we now show that not only does f(x(k+1)) satisfy f(x(k+1)) = minα f(x(k) + αd(k)), as indicated before, but also

In other words, if we write

then we can express f(x(k+1)) = minxk f(x). As k increases, the subspace span[d(0), d(1), ..., d(k)] "expands," and will eventually fill the whole of n (provided that the vectors d(0), d(1), ..., are linearly independent). Therefore, for some sufficiently large k, x* will lie in k. For this reason, the above result is sometimes called the expanding subspace theorem (see, e.g., [88, p. 266]).
To prove the expanding subspace theorem, define the matrix D(k) by

that is, d(i) is the ith column of D(k). Note that x(0) + (D(k)) = k. Also,

where α = [α0, ..., αk]. Hence,

Now, consider any vector x  k. There exists a vector a such that x = x(0) + D(k)a. Let ϕk(a) = f(x(0) + D(k)a). Note that ϕk is a quadratic function and has a unique minimizer that satisfies the FONC (see Exercises 6.33 and 10.7). By the chain rule,

Therefore,

By Lemma 10.2, g(k+1) D(k) = 0. Therefore, α satisfies the FONC for the quadratic function ϕk, and hence α is the minimizer of ϕk; that is,

which completes the proof of our result.
The conjugate direction algorithm is very effective. However, to use the algorithm, we need to specify the Q-conjugate directions. Fortunately, there is a way to generate Q-conjugate directions as we perform iterations. In the next section we discuss an algorithm that incorporates the generation of Q-conjugate directions.
10.3 The Conjugate Gradient Algorithm
The conjugate gradient algorithm does not use prespecified conjugate directions, but instead computes the directions as the algorithm progresses. At each stage of the algorithm, the direction is calculated as a linear combination of the previous direction and the current gradient, in such a way that all the directions are mutually Q-conjugate—hence the name conjugate gradient algorithm. This calculation exploits the fact that for a quadratic function of n variables, we can locate the function minimizer by performing n searches along mutually conjugate directions.
As before, we consider the quadratic function

where Q = Q > 0. Our first search direction from an initial point x(0) is in the direction of steepest descent; that is,

Thus,

where

In the next stage, we search in a direction d(1) that is Q-conjugate to d(0) We choose d(1) as a linear combination of g(1) and d(0). In general, at the (k + 1)th step, we choose d(k+1) to be a linear combination of g(k+1) and d(k). Specifically, we choose

The coefficients βk, k = 1, 2, ..., are chosen in such a way that d(k+1) is Q-conjugate to d(0), d(1), ..., d(k). This is accomplished by choosing βk to be

The conjugate gradient algorithm is summarized below.

1. Set k := 0; select the initial point x(0).
2.  If g(0) = 0 stop; else, set d(0) = −g(0).
3. 
4. 
5. 
6. 
7. 
8. Set k := k + 1; go to step 3.

Proposition 10.1 In the conjugate gradient algorithm, the directions d(0), d(1), ..., d(n−1) are Q-conjugate.
Proof. We use induction. We first show that d(0) Qd(1) = 0. To this end we write

Substituting for

in the equation above, we see that d(0) Qd(1) = 0.
We now assume that d(0), d(1), ..., d(k), k < n − 1, are Q-conjugate directions. From Lemma 10.2 we have g(k+1) d(j) = 0, j = 0, 1, ..., k. Thus, g(k+1) is orthogonal to each of the directions d(0), d(1), ..., d(k). We now show that

Fix j  {0, ..., k}. We have

Substituting this equation into the previous one yields

Because g(k+1) d(j−1) = 0, it follows that g(k+1) g(j) = 0.
We are now ready to show that d(k+1) Qd(j) = 0, j = 0, ..., k. We have

If j < k, then d(k) Qd(j) = 0, by virtue of the induction hypothesis. Hence, we have

But g(j+1) = g(j) + αj Qd(j). Because g(k+1) g(i) = 0, i = 0, ..., k,

Thus,

It remains to be shown that d(k+1) Qd(k) = 0. We have

Using the expression for βk, we get d(k+1) Qd(k) = 0, which completes the proof.
Example 10.3 Consider the quadratic function

We find the minimizer using the conjugate gradient algorithm, using the starting point x(0) = [0, 0, 0].
We can represent f as

where

We have

Hence,

and

The next stage yields

We can now compute

Hence,

and

To perform the third iteration, we compute

Hence,

and

Note that

as expected, because f is a quadratic function of three variables. Hence, x* = x(3).
10.4 The Conjugate Gradient Algorithm for Nonquadratic Problems
In Section 10.3, we showed that the conjugate gradient algorithm is a conjugate direction method, and therefore minimizes a positive definite quadratic function of n variables in n steps. The algorithm can be extended to general nonlinear functions by interpreting f(x) = x Qx − xb as a second-order Taylor series approximation of the objective function. Near the solution such functions behave approximately as quadratics, as suggested by the Taylor series expansion. For a quadratic, the matrix Q, the Hessian of the quadratic, is constant. However, for a general nonlinear function the Hessian is a matrix that has to be reevaluated at each iteration of the algorithm. This can be computationally very expensive. Thus, an efficient implementation of the conjugate gradient algorithm that eliminates the Hessian evaluation at each step is desirable.
Observe that Q appears only in the computation of the scalars αk and βk. Because

the closed-form formula for αk in the algorithm can be replaced by a numerical line search procedure. Therefore, we need only concern ourselves with the formula for βk. Fortunately, elimination of Q from the formula is possible and results in algorithms that depend only on the function and gradient values at each iteration. We now discuss modifications of the conjugate gradient algorithm for a quadratic function for the case in which the Hessian is unknown but in which objective function values and gradients are available. The modifications are all based on algebraically manipulating the formula βk in such a way that Q is eliminated. We discuss three well-known modifications.
Hestenes-Stiefel Formula. Recall that

The Hestenes-Stiefel formula is based on replacing the term Qd(k) by the term (g(k+1) − g(k))/αk. The two terms are equal in the quadratic case, as we now show. Now, x(k+1) = x(k) + αkd(k). Premultiplying both sides by Q, subtracting b from both sides, and recognizing that g(k) = Qx(k) − b, we get g(k+1) = g(k) + αk Qd(k), which we can rewrite as Qd(k) = (g(k+1) − g(k))/αk. Substituting this into the original equation for βk gives the Hestenes-Stiefel formula

Polak-Ribière Formula. Starting from the Hestenes-Stiefel formula, we multiply out the denominator to get

By Lemma 10.2, d(k) g(k+1) = 0. Also, since d(k) = −g(k) + βk−1 d(k+1), and premultiplying this by g(k), we get

where once again we used Lemma 10.2. Hence, we get the Polak-Ribière formula

Fletcher-Reeves Formula. Starting with the Polak-Ribière formula, we multiply out the numerator to get

We now use the fact that g(k+1) g(k) = 0, which we get by using the equation

and applying Lemma 10.2. This leads to the Fletcher-Reeves formula

The formulas above give us conjugate gradient algorithms that do not require explicit knowledge of the Hessian matrix Q. All we need are the objective function and gradient values at each iteration. For the quadratic case the three expressions for βk are exactly equal. However, this is not the case for a general nonlinear objective function.
We need a few more slight modifications to apply the algorithm to general nonlinear functions in practice. First, as mentioned in our discussion of the steepest descent algorithm (Section 8.2), the stopping criterion ∇f(x(k+1)) = 0 is not practical. A suitable practical stopping criterion, such as those discussed in Section 8.2, needs to be used.
For nonquadratic problems, the algorithm will not usually converge in n steps, and as the algorithm progresses, the "Q-conjugacy" of the direction vectors will tend to deteriorate. Thus, a common practice is to reinitialize the direction vector to the negative gradient after every few iterations (e.g., n or n + 1) and continue until the algorithm satisfies the stopping criterion.
A very important issue in minimization problems of nonquadratic functions is the line search. The purpose of the line search is to minimize ϕk(α) = f(x(k) + αd(k)) with respect to α ≥ 0. A typical approach is to bracket or box in the minimizer and then estimate it. The accuracy of the line search is a critical factor in the performance of the conjugate gradient algorithm. If the line search is known to be inaccurate, the Hestenes-Stiefel formula for βk is recommended [69].
In general, the choice of which formula for βk to use depends on the objective function. For example, the Polak-Ribière formula is known to perform far better than the Fletcher-Reeves formula in some cases but not in others. In fact, there are cases in which the g(k), k = 1, 2, ..., are bounded away from zero when the Polak-Ribière formula is used (see [107]). In the study by Powell in [107], a global convergence analysis suggests that the Fletcher-Reeves formula for βk is superior. Powell further suggests another formula for βk:

For general results on the convergence of conjugate gradient methods, we refer the reader to [135]. For an application of conjugate gradient algorithms to Wiener filtering, see [116], [117], and [118].
Conjugate gradient algorithms are related to Krylov subspace methods (see Exercise 10.6). Krylov-subspace-iteration methods, initiated by Magnus Hestenes, Eduard Stiefel, and Cornelius Lanczos, have been declared one of the 10 algorithms with the greatest influence on the development and practice of science and engineering in the twentieth century [40].
For control perspective on the conjugate gradient algorithm, derived from a proportional-plus-derivative (PD) controller architecture, see [4]. In addition, these authors offer a control perspective on Krylov-subspace-iteration methods as discrete feedback control systems.
EXERCISES

10.1 (Adopted from [88, Exercise 9.8(1)]) Let Q be a real symmetric positive definite n × n matrix. Given an arbitrary set of linearly independent vectors {p(0), ..., p(n−1)} in n, the Gram-Schmidt procedure generates a set of vectors {d(0), ..., d(n−1)} as follows:

Show that the vectors d(0), ..., d(n−1) are Q-conjugate.
10.2 Let f : n →  be the quadratic function

where Q = Q > 0. Given a set of directions {d(0), d(1), ...} ⊂ n, consider the algorithm

where αk is the step size. Suppose that g(k+1) d(i) = 0 for all k = 0, ..., n − 1 and i = 0, ..., k, where g(k+1) = ∇f(x(k+1)). Show that if g(k) d(k) ≠ 0 for all k = 0, ..., n − 1, then d(0), ..., d(n−1) are Q-conjugate.
10.3 Let f : n →  be given by f(x) = x Qx − x b, where b  n and Q is a real symmetric positive definite n × n matrix. Show that in the conjugate gradient method for this f, d(k) Qd(k) = −d(k) Qg(k).
10.4 Let Q be a real n × n symmetric matrix.
a. Show that there exists a Q-conjugate set {d(1), ..., d(n)} such that each d(i) (i = 1, ..., n) is an eigenvector of Q.
Hint: Use the fact that for any real symmetric n × n matrix, there exists a set {v1, ..., vn} of its eigenvectors such that vivj = 0 for all i, j = 1, ..., n, i ≠ j.
b. Suppose that Q is positive definite. Show that if {d(1), ..., d(n)} is a Q-conjugate set that is also orthogonal (i.e., d(i) d(j) = 0 for all i, j = 1, ..., n, i ≠ j), and d(i) ≠ 0, i = 1, ..., n, then each d(i) i = 1, ..., n, is an eigenvector of Q.
10.5 Consider the following algorithm for minimizing a function f:

where αk = arg minα f(x(k) + αd(k)). Let g = ∇f(x(k)) (as usual).
   Suppose that f is quadratic with Hessian Q. We choose d(k+1) = γkg(k+1) + d(k), and we wish the directions d(k) and d(k+1) to be Q-conjugate. Find a formula for γk in terms of d(k), g(k+1), and Q.
10.6 Consider the algorithm

with αk   scalar and x(0) = 0, applied to the quadratic function f : n →  given by

where Q > 0. As usual, write g(k) = ∇f(x(k)). Suppose that the search directions are generated according to

where ak and bk are real constants, and by convention we take d(−1) = 0.
a. Define the subspace k = span[b, Qb, ..., Qk−1b] (called the Krylov subspace of order k). Show that  and .
Hint: Use induction. Note that 0 = {0} and 1 = span[b].
b. In light of part a, what can you say about the "optimality" of the conjugate gradient algorithm with respect to the Krylov subspace?
10.7 Consider the quadratic function f : n →  given by

where Q = Q > 0. Let D  nxr be of rank r and x0  n. Define the function ϕ :  →  by

Show that ϕ is a quadratic function with a positive definite quadratic term.
10.8 Consider a conjugate gradient algorithm applied to a quadratic function.
a. Show that the gradients associated with the algorithm are mutually orthogonal. Specifically, show that g(k+1) g(i) = 0 for all 0 ≤ k ≤ n − 1 and 0 ≤ i ≤ k.
Hint: Write g(i) in terms of d(i) and d(i−1).
b. Show that the gradients associated with the algorithm are Q-conjugate if separated by at least two iterations. Specifically, show that g(k+1) Qd(i) = 0 for all 0 ≤ k ≤ n − 1 and 0 ≤ i ≤ k − 1.
10.9 Represent the function

in the form f(x) = x Qx − x b + c. Then use the conjugate gradient algorithm to construct a vector d(1) that is Q-conjugate with d(0) = ∇f(x(0)), where x(0) = 0.
10.10 Let f(x), x = [x1, x2]  2, be given by

a. Express f(x) in the form of f(x) = 1/2x Qx − xb.
b. Find the minimizer of f using the conjugate gradient algorithm. Use a starting point of x(0) = [0, 0].
c. Calculate the minimizer of f analytically from Q and b, and check it with your answer in part b.
10.11 Write a MATLAB program to implement the conjugate gradient algorithm for general functions. Use the secant method for the line search (e.g., the MATLAB function of Exercise 7.11). Test the different formulas for βk on Rosenbrock's function (see Exercise 9.4) with an initial condition x(0) = [−2,2]. For this exercise, reinitialize the update direction to the negative gradient every six iterations.








CHAPTER 11
QUASI-NEWTON METHODS
11.1 Introduction
Newton's method is one of the more successful algorithms for optimization. If it converges, it has a quadratic order of convergence. However, as pointed out before, for a general nonlinear objective function, convergence to a solution cannot be guaranteed from an arbitrary initial point x(0) In general, if the initial point is not sufficiently close to the solution, then the algorithm may not possess the descent property  for some k].
Recall that the idea behind Newton's method is to locally approximate the function f being minimized, at every iteration, by a quadratic function. The minimizer for the quadratic approximation is used as the starting point for the next iteration. This leads to Newton's recursive algorithm

We may try to guarantee that the algorithm has the descent property by modifying the original algorithm as follows:

where αk is chosen to ensure that

For example, we may choose αk = arg minα≥0 f(x(k) − αF(x(k))−1g(k)) (see Theorem 9.2). We can then determine an appropriate value of αk by performing a line search in the direction −F(x(k))−1g(k). Note that although the line search is simply the minimization of the real variable function φk(α) = f(x(k) − αF(x(k))−1g(k)), it is not a trivial problem to solve.
A computational drawback of Newton's method is the need to evaluate F(x(k)) and solve the equation F(x(k))d(k) = −g(k) [i.e., compute d(k) = −F(x(k))−1g(k)]. To avoid the computation of F(x(k))−1, the quasi-Newton methods use an approximation to F(x(k))−1 in place of the true inverse. This approximation is updated at every stage so that it exhibits at least some properties of F(x(k))−1. To get some idea about the properties that an approximation to F(x(k))−1 should satisfy, consider the formula

where Hk is an n × n real matrix and α is a positive search parameter. Expanding f about x(k) yields

As α tends to zero, the second term on the right-hand side of this equation dominates the third. Thus, to guarantee a decrease in f for small α, we have to have

A simple way to ensure this is to require that Hk be positive definite. We have proved the following result.
Proposition 11.1 Let f  ⊂1, x(k)  n, g(k) = ∇f(x(k)) ≠ 0, and Hk an n × n real symmetric positive definite matrix. If we set x(k+1) = x(k) − αkHkg(k), where αk = arg minα≥0 f(x(k) − αHk), then αk > 0 and f (x(k+1) < f (x(k)).
In constructing an approximation to the inverse of the Hessian matrix, we should use only the objective function and gradient values. Thus, if we can find a suitable method of choosing Hk, the iteration may be carried out without any evaluation of the Hessian and without the solution of any set of linear equations.
11.2 Approximating the Inverse Hessian
Let H0,H1,H2,... be successive approximations of the inverse F(x(k))−1 of the Hessian. We now derive a condition that the approximations should satisfy, which forms the starting point for our subsequent discussion of quasi-Newton algorithms. To begin, suppose first that the Hessian matrix F(x) of the objective function f is constant and independent of x. In other words, the objective function is quadratic, with Hessian F(x) = Q for all x, where Q = Q. Then,

Let

and

Then, we may write

We start with a real symmetric positive definite matrix H0. Note that given k, the matrix Q−1 satisfies

Therefore, we also impose the requirement that the approximation Hk+1 of the Hessian satisfy

If n steps are involved, then moving in n directions Δx(0), Δx1,..., Δx(n−1) yields

This set of equations can be represented as

Note that Q satisfies

and

Therefore, if  is nonsingular, then Q−1 is determined uniquely after n steps, via

As a consequence, we conclude that if Hn satisfies the equations HnΔg(i) = Δx(i), 0 ≤ i ≤ n − 1, then the algorithm x(k+1) = x(k) − αkHkg(k), αk = arg minα≥0 f(x(k) − αHkg(k)), is guaranteed to solve problems with quadratic objective functions in n + 1 steps, because the update x(n+1) = x(n) − αnHng(n) is equivalent to Newton's algorithm. In fact, as we shall see below (Theorem 11.1), such algorithms solve quadratic problems of n variables in at most n steps.
The considerations above illustrate the basic idea behind the quasi-Newton methods. Specifically, quasi-Newton algorithms have the form

where the matrices H0, H1,... are symmetric. In the quadratic case these matrices are required to satisfy

where Δx(i) = x(i+1) − x(i) = αid(i) and Δg(i) = g(i+1) − g(i) = QΔx(i). It turns out that quasi-Newton methods are also conjugate direction methods, as stated in the following.
Theorem 11.1 Consider a quasi-Newton algorithm applied to a quadratic function with Hessian Q = Q such that for 0 ≤ k < n − 1,

where . If αi ≠ 0, 0 ≤ i ≤ k, then d(0),...,d(k+1) are Q-conjugate.
Proof. We proceed by induction. We begin with the k = 0 case: that d(0) and d(1) are Q-conjugate. Because α0 ≠ 0, we can write d(0) = Δx(0)/α0.
Hence,

But g(1) d(0) = 0 as a consequence of α0 > 0 being the minimizer of φ(α) = f(x(0) + αd(0)) (see Exercise 11.1). Hence, d(1) Qd(0) = 0.
Assume that the result is true for k − 1 (where k < n − 1). We now prove the result for k, that is, that d(0)...,d(k+1) are Q-conjugate. It suffices to show that d(k+1)Qd(i) = 0, 0 ≤ i ≤ k. Given i, 0 ≤ i ≤ k, using the same algebraic steps as in the k = 0 case, and using the assumption that αi ≠ 0, we obtain

Because d(0),...,d(k) are Q-conjugate by assumption, we conclude from Lemma 10.2 that g(k+1)d(i) = 0. Hence, d(k+1)Qd(i) = 0, which completes the proof.
By Theorem 11.1 we conclude that a quasi-Newton algorithm solves a quadratic of n variables in at most n steps.
Note that the equations that the matrices Hk are required to satisfy do not determine those matrices uniquely. Thus, we have some freedom in the way we compute the Hk. In the methods we describe, we compute Hk+1 by adding a correction to Hk. In the following sections we consider three specific updating formulas.
11.3 The Rank One Correction Formula
In the rank one correction formula, the correction term is symmetric and has the form akz(k)z(k), where ak   and z(k)  n. Therefore, the update equation is

Note that

and hence the name rank one correction [it is also called the single-rank symmetric (SRS) algorithm]. The product z(k)z(k) is sometimes referred to as the dyadic product or outer product Observe that if Hk is symmetric, then so is Hk+1.
Our goal now is to determine ak and z(k), given Hk, Δg(k), Δx(k), so that the required relationship discussed in Section 11.2 is satisfied; namely, . To begin, let us first consider the condition . In other words, given Hk, Δg(k), and Δx(k), we wish to find ak and z(k) to ensure that

First note that z(k) Δg(k) is a scalar. Thus,

and hence

We can now determine

Hence,

The next step is to express the denominator of the second term on the right-hand side of the equation above as a function of the given quantities Hk, Δg(k), and Δx(k). To accomplish this, premultiply  to obtain

Observe that ak is a scalar and so is . Thus,

Taking this relation into account yields

We summarize the above development in the following algorithm.
Rank One Algorithm

1. Set k := 0; select x(0) and a real symmetric positive definite H0
2. If g(k) = 0, stop; else, d(k) = −Hkg(k).
3. Compute

4. Compute

5. Set k := k + 1; go to step 2.

The rank one algorithm is based on satisfying the equation

However, what we want is

It turns out that the above is, in fact, true automatically, as stated in the following theorem.
Theorem 11.2 For the rank one algorithm applied to the quadratic with Hessian Q = Q, we have Hk+1Δg = Δx(i), 0 ≤ i ≤ k.
Proof. We prove the result by induction. Prom the discussion before the theorem, it is clear that the claim is true for k = 0. Suppose now that the theorem is true for k − 1 ≥ 0; that is, HkΔg(i), = Δx(i), i < k. We now show that the theorem is true for k. Our construction of the correction term ensures that

So we only have to show that

To this end, fix i < k. We have

By the induction hypothesis, HkΔg(i) = Δx(i). To complete the proof, it is enough to show that the second term on the right-hand side of the equation above is equal to zero. For this to be true it is enough that

Indeed, since

by the induction hypothesis, and because Δg(k) = QΔx(k), we have

Hence,

which completes the proof.
Example 11.1 Let

Apply the rank one correction algorithm to minimize f. Use x(0) = [1,2] and H0 = I2 (2 × 2 identity matrix).
We can represent f as

Thus,

Because H0 = I2,

The objective function is quadratic, and hence

and thus

We then compute

Because

we obtain

Therefore,

and

We now compute

Note that g(2) = 0, and therefore x(2) = x*. As expected, the algorithm solves the problem in two steps.
Note that the directions d(0) and d1 are Q-conjugate, in accordance with Theorem 11.1.
Unfortunately, the rank one correction algorithm is not very satisfactory, for several reasons. First, the matrix Hk+1 that the rank one algorithm generates may not be positive definite (see Example 11.2 below) and thus may not be a descent direction. This happens even in the quadratic case (see Example 11.10). Furthermore, if

is close to zero, then there may be numerical problems in evaluating Hk+1
Example 11.2 Assume that Hk > 0. It turns out that if Δg(k) (Δx(k) − HkΔg(k)) > 0, then Hk+1 > 0 (see Exercise 11.7). However, if Δg(k) (Δx(k) − HkΔg(k)) < 0, then Hk+1 may not be positive definite. As an example of what might happen if Δg(k)(Δx(k) − HkΔg(k)) < 0, consider applying the rank one algorithm to the function

with an initial point

and initial matrix

Note that H0 > 0. We have

and

It is easy to check that H1 is not positive definite (it is indefinite, with eigenvalues 0.96901 and −1.3030).
Fortunately, alternative algorithms have been developed for updating Hk. In particular, if we use a "rank two" update, then Hk is guaranteed to be positive definite for all k, provided that the line search is exact. We discuss this in the next section.
11.4 The DFP Algorithm
The rank two update was originally developed by Davidon in 1959 and was subsequently modified by Fletcher and Powell in 1963: hence the name DFP algorithm. The DFP algorithm is also known as the variable metric algorithm. We summarize the algorithm below.
DFP Algorithm

1. Set k := 0; select x and a real symmetric positive definite H0.
2. If g(k) = 0, stop; else, d(k) = −Hkg(k).
3. Compute

4. Compute

5. Set k := k + 1; go to step 2.

We now show that the DFP algorithm is a quasi-Newton method, in the sense that when applied to quadratic problems, we have Hk+1Δg(i) = Δx(i), 0 ≤ i ≤ k.
Theorem 11.3 In the DFP algorithm applied to the quadratic with Hessian Q = Q, we have Hk+1Δg(i) = Δx(i), 0 ≤ i ≤ k.
Proof. We use induction. For k = 0, we have

Assume that the result is true for k − 1; that is, HkΔg(i) = Δx(i), 0 ≤ i ≤ k − 1. We now show that Hk+1Δg(i) = Δx(i) 0 ≤ i ≤ k. First, consider i = k. We have

It remains to consider the case i < k. To this end,

Now,

by the induction hypothesis and Theorem 11.1. The same arguments yield Δg(k)Δx(i) = 0. Hence,

which completes the proof.
By Theorems 11.1 and 11.3 we conclude that the DFP algorithm is a conjugate direction algorithm.
Example 11.3 Locate the minimizer of

Use the initial point x(0) = [0,0] and H0 = H2.
Note that in this case

Hence,

Because f is a quadratic function,

Therefore,

We then compute

and

Observe that

Thus,

and

Using the above, we now compute H1:

We now compute d(1) = −H1g(1) = [0,1] and

Hence,

because f is a quadratic function of two variables.
Note that we have d(0)Qd(1) = d(1)Qd(0) = 0; that is, d(0) and d(1) are Q-conjugate directions.
We now show that in the DFP algorithm, Hk+1 inherits positive definiteness from Hk.
Theorem 11.4 Suppose that g(k) ≠ 0. In the DFP algorithm, if Hk is positive definite, then so is Hk+1.
Proof. We first write the following quadratic form:

Define

where

Note that because Hk > 0, its square root is well-defined; see Section 3.4 for more information on this property of positive definite matrices. Using the definitions of a and b, we obtain

and

Hence,

We also have

since Δx(k)gk+1 = αkd(k+1) = 0 by Lemma 10.2 (see also Exercise 11.1). Because

we have

This yields

Both terms on the right-hand side of the above equation are nonnegative—the first term is nonnegative because of the Cauchy-Schwarz inequality, and the second term is nonnegative because Hk > 0 and αk > 0 (by Proposition 11.1). Therefore, to show that x Hk+1 x > 0 for x ≠ 0, we only need to demonstrate that these terms do not both vanish simultaneously.
The first term vanishes only if a and b are proportional, that is, if a = βb for some scalar β. Thus, to complete the proof it is enough to show that if a = βb, then (x Δx(k))2/(αkg(k) Hkg(k)) > 0. Indeed, first observe that

Hence,

Using the expression for x above and the expression Δx(k) Δg(k) = αkg(k) Hkg(k), we obtain

Thus, for all x ≠ 0,

which completes the proof.
The DFP algorithm is superior to the rank one algorithm in that it preserves the positive definiteness of Hk. However, it turns out that in the case of larger nonquadratic problems the algorithm has the tendency of sometimes getting "stuck." This phenomenon is attributed to Hk becoming nearly singular [19]. In the next section we discuss an algorithm that alleviates this problem.
11.5 The BFGS Algorithm
In 1970, an alternative update formula was suggested independently by Broyden, Fletcher, Goldfarb, and Shanno. The method, now called the BFGS algorithm, is discussed in this section.
To derive the BFGS update, we use the concept of duality, or complementarity, as presented in [43] and [88]. To discuss this concept, recall that the updating formulas for the approximation of the inverse of the Hessian matrix were based on satisfying the equations

which were derived from Δg(i) = QΔx(i), 0 ≤ i ≤ k. We then formulated update formulas for the approximations to the inverse of the Hessian matrix Q−1. An alternative to approximating Q−1 is to approximate Q itself. To do this let Bk be our estimate of Q at the kth step. We require Bk+1 to satisfy

Notice that this set of equations is similar to the previous set of equations for Hk+1, the only difference being that the roles of Δx(i) and Δg(i) are interchanged. Thus, given any update formula for Hk, a corresponding update formula for Bk can be found by interchanging the roles of Bk and Hk and of Δg(k) and Δx(k). In particular, the BFGS update for Bk corresponds to the DFP update for Hk. Formulas related in this way are said to be dual or complementary [43].
Recall that the DFP update for the approximation Hk of the inverse Hessian is

Using the complementarity concept, we can easily obtain an update equation for the approximation Bk of the Hessian:

This is the BFGS update of Bk.
Now, to obtain the BFGS update for the approximation of the inverse Hessian, we take the inverse of Bk+1 to obtain

To compute HBFGSk+1 by inverting the right-hand side of this equation, we apply the following formula for a matrix inverse, known as the Sherman-Morrison formula (see [63, p. 123] or [53, p. 50]).
Lemma 11.1 Let A be a nonsingular matrix. Let u and v be column vectors such that 1 + v A−1 u ≠ 0. Then, A + uv is nonsingular, and its inverse can be written in terms of A−1 using the following formula:

Proof. We can prove the result easily by verification.
From Lemma 11.1 it follows that if A−1 is known, then the inverse of the matrix A augmented by a rank one matrix can be obtained by a modification of the matrix A−1.
Applying Lemma 11.1 twice to Bk+1 (see Exercise 11.12) yields

which represents the BFGS formula for updating Hk.
Recall that for the quadratic case the DFP algorithm satisfies . Therefore, the BFGS update for Bk satisfies Bk+1 Δx(i) = Δg(i), 0 ≤ i ≤ k. By construction of the BFGS formula for , we conclude that . Hence, the BFGS algorithm enjoys all the properties of quasi-Newton methods, including the conjugate directions property. Moreover, the BFGS algorithm also inherits the positive definiteness property of the DFP algorithm; that is, if g(k) ≠ 0 and Hk > 0, then 
The BFGS update is reasonably robust when the line searches are sloppy (see [19]). This property allows us to save time in the line search part of the algorithm. The BFGS formula is often far more efficient than the DFP formula (see [107] for further discussion).
We conclude our discussion of the BFGS algorithm with the following numerical example.
Example 11.4 Use the BFGS method to minimize

where

Take H0 = I2 and x(0) = [0, 0]. Verify that H2 = Q−1.
We have

The objective function is a quadratic, and hence we can use the following formula to compute α0:

Therefore,

To compute H1 = HBFGS1, we need the following quantities:

Therefore,

Hence, we have

Therefore,

Because our objective function is a quadratic on 2, x(2) is the minimizer. Notice that the gradient at x(2) is 0; that is, g(2) = 0.
To verify that H2 = Q−1, we compute Hence,

Hence,

Note that indeed H2Q = QH2 = I2, and hence H2 = Q−1.
For nonquadratic problems, quasi-Newton algorithms will not usually converge in n steps. As in the case of the conjugate gradient methods, here, too, some modifications may be necessary to deal with nonquadratic problems. For example, we may reinitialize the direction vector to the negative gradient after every few iterations (e.g., n or n + 1), and continue until the algorithm satisfies the stopping criterion.
EXERCISES

11.1 Given f : n → , f  ⊂1, consider the algorithm

where d(1), d(2), ... are vectors in n, and αk ≥ 0 is chosen to minimize f(x(k) + αd(k)); that is,

Note that the general algorithm above encompasses almost all algorithms that we discussed in this part, including the steepest descent, Newton, conjugate gradient, and quasi-Newton algorithms.
Let g(k) = ∇f(x(k)), and assume that d(k) g(k) < 0.
a. Show that d(k) is a descent direction for f in the sense that there exists  > 0 such that for all α  (0, ],

b. Show that αk > 0.
c. Show that d(k) g(k+1) = 0.
d. Show that the following algorithms all satisfy the condition d(k) g(k) < 0, if g(k) ≠ 0:

1. Steepest descent algorithm.
2. Newton's method, assuming that the Hessian is positive definite.
3. Conjugate gradient algorithm.
4. Quasi-Newton algorithm, assuming that Hk > 0.

e. For the case where f(x) = xQx − x b, with Q = Q > 0, derive an expression for αk in terms of Q, d(k), and g(k).
11.2 Consider Newton's algorithm applied to a function f  C2:

where αk is chosen according to a line search. Is this algorithm a member of the quasi-Newton family?
11.3 In some optimization methods, when minimizing a given function f(x), we select an initial guess x(0) and a real symmetric positive definite matrix H0. Then we iteratively compute Hk, d(k) = −Hkg(k) (where g(k) = ∇ f(x(x))), and x(k+1) = x(k) + αkd(k), where

Suppose that the function we wish to minimize is a standard quadratic of the form

a. Find an expression for αk in terms of Q, Hk, g(k), and d(k);
b. Give a sufficient condition on Hk for αk to be positive.
11.4 Consider the algorithm

where, as usual, g(k) = ∇f(x(k)) and H is a fixed symmetric matrix.
a. Suppose that f  ⊂3 and there is a point x* such that ∇f(x*) = 0 and F(x*)−1 exists. Find H such that if x(0) is sufficiently close to x*, then x(k) converges to x* with order of convergence of at least 2.
b. With the setting of H in part a, is the given algorithm a quasi-Newton method?
11.5 Minimize the function

using the rank one correction method with the starting point x(0) = 0.
11.6 Consider the algorithm

where f : 2 → , f  ⊂1, Mk  2 × 2 is given by

with a  , and

   Suppose that at some iteration k we have ∇f(x(k)) = [1, 1]. Find the largest range of values of a that guarantees that αk > 0 for any f.
11.7 Consider the rank one algorithm. Assume that Hk > 0. Show that if Δg(k) (Δx(k) − HkΔg(k)) > 0, then Hk+1 > 0.
11.8 Based on the rank one update equation, derive an update formula using complementarity and the matrix inverse formula.
11.9 Let

and x(0) = 0. Use the rank one correction method to generate two Q-conjugate directions.
11.10 Apply the rank one algorithm to the problem in Example 11.3.
11.11 Consider the DFP algorithm applied to the quadratic function

where Q = Q > 0.
a. Write down a formula for αk in terms of Q, g(k), and d(k).
b. Show that if g(k) ≠ 0, then αk > 0.
11.12 Use Lemma 11.1 to derive the BFGS update formula based on the DFP formula, using complementarity.
Hint: Define

Using the notation above, represent Bk+1 as

Apply Lemma 11.1 to the above.
11.13 Assuming exact line search, show that if H0 = In (n × n identity matrix), then the first two steps of the BFGS algorithm yield the same points x(1) and x(2) as conjugate gradient algorithms with the Hestenes-Stiefel, the Polak-Ribière, and the Fletcher-Reeves formulas.
11.14 Let f : n →  be such that f  ⊂1. Consider an optimization algorithm applied to this f, of the usual form x(k+1) = x(k) + αkd(k), where αk ≥ 0 is chosen according to line search. Suppose that d(k) = −Hkg(k), where g(k) = ∇f(x(k)) and Hk is symmetric.
a. Show that if Hk satisfies the following conditions whenever the algorithm is applied to a quadratic, then the algorithm is quasi-Newton:

1. Hk+1 = Hk + Uk.
2. UkΔg(k) = Δx(k) − HkΔg(k).
3. Uk = a(k) Δx(k) + b(k)Δg(k) Hk, where a(k) and b(k) are in n.

b. Which (if any) among the rank-one, DFP, and BFGS algorithms satisfy the three conditions in part a (whenever the algorithm is applied to a quadratic)? For those that do, specify the vectors a(k) and b(k).
11.15 Given a function f : n → , consider an algorithm x(k+1) = x(k) − αkHkg(k) for finding the minimizer of f, where g(k) = ∇f(x(k)) and Hk  nxn is symmetric. Suppose that , where φ  , and HDFPk and HBFGSk are matrices generated by the DFP and BFGS algorithms, respectively.
a. Show that the algorithm above is a quasi-Newton algorithm. Is the above algorithm a conjugate direction algorithm?
b. Suppose that 0 ≤ 0 ≤ 1. Show that if HDFP0 > 0 and HBFGS0 > 0, then Hk > 0 for all k. What can you conclude from this about whether or not the algorithm has the descent property?
11.16 Consider the following simple modification of the quasi-Newton family of algorithms. In the quadratic case, instead of the usual quasi-Newton condition Hk+1 Δg(i) = Δx(i), 0 ≤ i ≤ k, suppose that we have Hk+1 Δg(i) = ρiΔx(i), 0 ≤ i ≤ k, where ρi > 0. We refer to the set of algorithms that satisfy the condition above as the symmetric Huang family.
   Show that the symmetric Huang family algorithms are conjugate direction algorithms.
11.17 Write a MATLAB program to implement the quasi-Newton algorithm for general functions. Use the secant method for the line search (e.g., the MATLAB function of Exercise 7.11). Test the various update formulas for Hk on Rosenbrock's function (see Exercise 9.4), with an initial condition x(0) = [-2,2]. For this exercise, reinitialize the update direction to the negative gradient every six iterations.
11.18 Consider the function

a. Use MATLAB to plot the level sets of f at levels −0.72, −0.6, −0.2, 0.5, 2. Locate the minimizers of f from the plots of the level sets.
b. Apply the DFP algorithm to minimize the function above with the following starting initial conditions: (i) [0, 0]; (ii) [1.5,1]. Use H0 = I2. Does the algorithm converge to the same point for the two initial conditions? If not, explain.








CHAPTER 12
SOLVING LINEAR EQUATIONS
12.1 Least-Squares Analysis
Consider a system of linear equations

where A  m×n, b  m, m ≥ n, and rank A = n. Note that the number of unknowns, n, is no larger than the number of equations, m. If b does not belong to the range of A, that is, if b ∉ (A), then this system of equations is said to be inconsistent or overdetermined. In this case there is no solution to the above set of equations. Our goal then is to find the vector (or vectors) x minimizing ||Ax − b||2. This problem is a special case of the nonlinear least-squares problem discussed in Section 9.4.
Let x* be a vector that minimizes ||Ax − b||2; that is, for all x  n,

We refer to the vector x* as a least-squares solution to Ax = b. In the case where Ax = b has a solution, then the solution is a least-squares solution. Otherwise, a least-squares solution minimizes the norm of the difference between the left- and right-hand sides of the equation Ax = b. To characterize least-squares solutions, we need the following lemma.
Lemma 12.1 Let A  mxn, m ≥ n. Then, rank A = n if and only if rank A A = n (i.e., the square matrix A A is nonsingular).
Proof. ⇒: Suppose that rank A = n. To show rank A A = n, it is equivalent to show (A A) = {0}. To proceed, let x  (AA); that is, A Ax = 0. Therefore,

which implies that Ax = 0. Because rank A = n, we have x = 0.
⇐ Suppose that rank A A = n; that is, (A A) = {0}. To show rank A = n, it is equivalent to show that (A) = {0}. To proceed, let x  (A); that is, Ax = 0. Then, A Ax = 0, and hence x = 0.
Recall that we assume throughout that rank A = n. By Lemma 12.1 we conclude that (A A)−1 exists. The following theorem characterizes the least-squares solution.
Theorem 12.1 The unique vector x* that minimizes ||Ax − b||2 is given by the solution to the equation A Ax = A b; that is, x* = (A A)−1 A b.
Proof. Let x* = (A A)−1 A b. First observe that

We now show that the last term in this equation is zero. Indeed, substituting the expression above for x*,

Hence,

If x ≠ x*, then ||A(x − x*)||2 > 0, because rank A = n. Thus, if x ≠ x*, we have

Thus, x* = (A A)−1 A b is the unique minimizer of ||Ax − b||2.
We now give a geometric interpretation of the Theorem 12.1. First note that the columns of A span the range (A) of A, which is an n-dimensional subspace of m. The equation Ax = b has a solution if and only if b lies in this n-dimensional subspace (A). If m = n, then b  (A) always, and the solution is x* = A−1 b. Suppose now that m > n. Intuitively, we would expect the "likelihood" of b  (A) to be small, because the subspace spanned by the columns of A is very "thin." Therefore, let us suppose that b does not belong to (A). We wish to find a point h  (A) that is "closest" to b. Geometrically, the point h should be such that the vector e = h − b is orthogonal to the subspace (A) (see Figure 12.1). Recall that a vector e  m is said to be orthogonal to the subspace (A) if it is orthogonal to every vector in this subspace. We call h the orthogonal projection of b onto the subspace (A). It turns out that h = Ax* = A(AA)−1A b. Hence, the vector h  (A) minimizing ||b − h|| is exactly the orthogonal projection of b onto (A). In other words, the vector x* minimizing ||Ax − b|| is exactly the vector that makes Ax − b orthogonal to (A).

Figure 12.1 Orthogonal projection of b on the subspace (A).


To proceed further, we write A = [a1, ..., an], where a1, ..., an are the columns of A. The vector e is orthogonal to (A) if and only if it is orthogonal to each of the columns a1, ..., an of A, To see this, note that

if and only if for any set of scalars {x1, x2, ..., xn}, we also have

Any vector in (A) has the form x1a1 + ··· + xnan.
Proposition 12.1 Let h  (A) be such that h − b is orthogonal to (A). Then, h = Ax* = A(AA)−1A b.
Proof. Because h  (A) = span[a1, ..., an], it has the form h = x1a1 + ··· + xnan, where x1, ..., xn  . To find x1, ..., xn, we use the assumption that e = h − b is orthogonal to span[a1, ..., an]; that is, for all i = 1, ..., n, we have

or, equivalently,

Substituting h into the equations above, we obtain a set of n linear equations of the form

In matrix notation this system of n equations can be represented as

Note that we can write

We also note that

Because rank A = n, A A is nonsingular, and thus we conclude that

Notice that the matrix

plays an important role in the least-squares solution. This matrix is often called the Gram matrix (or Grammian).
An alternative method of arriving at the least-squares solution is to proceed as follows. First, we write

Therefore, f is a quadratic function. The quadratic term is positive definite because rank A = n. Thus, the unique minimizer of f is obtained by solving the FONC (see Exercise 6.33); that is,

The only solution to the equation ∇f(x) = 0 is x* = (AA)−1Ab.
Example 12.1 Suppose that you are given two different types of concrete. The first type contains 30% cement, 40% gravel, and 30% sand (all percentages of weight). The second type contains 10% cement, 20% gravel, and 70% sand. How many pounds of each type of concrete should you mix together so that you get a concrete mixture that has as close as possible to a total of 5 pounds of cement, 3 pounds of gravel, and 4 pounds of sand?
The problem can be formulated as a least-squares problem with

where the decision variable is x = [x1,x2] and x1 and x2 are the amounts of concrete of the first and second types, respectively. After some algebra, we obtain the solution:

(For a variation of this problem solved using a different method, see Example 15.7.)
We now give an example in which least-squares analysis is used to fit measurements by a straight line.
Example 12.2 Line Fitting. Suppose that a process has a single input t   and a single output y  . Suppose that we perform an experiment on the process, resulting in a number of measurements, as displayed in Table 12.1. The ith measurement results in the input labeled ti and the output labeled yi. We would like to find a straight line given by
Table 12.1 Experimental Data for Example 12.2.


that fits the experimental data. In other words, we wish to find two numbers, m and c, such that yi = mti + c, i = 0,1,2. However, it is apparent that there is no choice of m and c that results in the requirement above; that is, there is no straight line that passes through all three points simultaneously. Therefore, we would like to find the values of m and c that best fit the data. A graphical illustration of our problem is shown in Figure 12.2.

Figure 12.2 Fitting a straight line to experimental data.


We can represent our problem as a system of three linear equations of the form

We can write this system of equations as

where

Note that since

the vector b does not belong to the range of A. Thus, as we have noted before, the system of equations above is inconsistent.
The straight line of best fit is the one that minimizes

Therefore, our problem lies in the class of least-squares problems. Note that the foregoing function of m and c is simply the total squared vertical distance (squared error) between the straight line defined by m and c and the experimental points. The solution to our least-squares problem is

Note that the error vector e = Ax* −b is orthogonal to each column of A.
Next, we give an example of the use of least-squares in wireless communications.
Example 12.3 Attenuation Estimation. A wireless transmitter sends a discrete-time signal {s0, s1, s2} (of duration 3) to a receiver, as shown in Figure 12.3. The real number si is the value of the signal at time i.

Figure 12.3 Wireless transmission in Example 12.3.


The transmitted signal takes two paths to the receiver: a direct path, with delay 10 and attenuation factor a1, and an indirect (reflected) path, with delay 12 and attenuation factor a2. The received signal is the sum of the signals from these two paths, with their respective delays and attenuation factors.
Suppose that the received signal is measured from times 10 through 14 as r10, r11,...,r14, as shown in the figure. We wish to compute the least-squares estimates of a1 and a2, based on the following values:

The problem can be posed as a least-squares problem with

The least-squares estimate is given by

We now give a simple example where the least-squares method is used in digital signal processing.
Example 12.4 Discrete Fourier Series. Suppose that we are given a discrete-time signal, represented by the vector

We wish to approximate this signal by a sum of sinusoids. Specifically, we approximate b by the vector

where y0, y1,..., yn, z1,...,zn   and the vectors c(k) and s(k) are given by

We call the sum of sinusoids above a discrete Fourier series (although, strictly speaking, it is not a series but a finite sum). We wish to find y0, y1,...,yn, z1,...,zn such that

is minimized.
To proceed, we define

Our problem can be reformulated as minimizing

We assume that m ≥ 2n+1. To find the solution, we first compute AA. We make use of the following trigonometric identities: For any nonzero integer k that is not an integral multiple of m, we have

With the aid of these identities, we can verify that

Hence,

which is clearly nonsingular, with inverse

Therefore, the solution to our problem is

We represent the solution as

We call these discrete Fourier coefficients.
Finally, we show how least-squares analysis can be used to derive formulas for orthogonal projectors.
Example 12.5 Orthogonal Projectors. Let  ⊂ n be a subspace. Given a vector x  n, we write the orthogonal decomposition of x as

where x   is the orthogonal projection of x onto  and x⊥  ⊥ is the orthogonal projection of x onto ⊥. (See Section 3.3; also recall that ⊥ is the orthogonal complement of .) We can write x = Px for some matrix P called the orthogonal projector. In the following, we derive expressions for P for the case where  = (A) and the case where  = (A).
Consider a matrix A  m × n, m ≥ n, and rank A = n. Let  = (A) be the range of A (note that any subspace can be written as the range of some matrix). In this case we can write an expression for P in terms of A, as follows. By Proposition 12.1 we have x = A(AA)−1Ax, whence P = A(AA)−1A. Note that by Proposition 12.1, we may also write

Next, consider a matrix A  m×n, m ≤ n, and rank A = m. Let  = (A) be the nullspace of A (note that any subspace can be written as the nullspace of some matrix). To derive an expression for the orthogonal projector P in terms of A for this case, we use the formula derived above and the identity (A)⊥ = (A) (see Theorem 3.4). Indeed, if  = (A), then the orthogonal decomposition with respect to  is x = x + x⊥, where x = A(AA)−1Ax (using the formula derived above). Because (A)⊥ = (A), we deduce that x⊥ = x = A(AA)−1Ax. Hence,

Thus, the orthogonal projector in this case is P = I − A(AA)−1 A.
12.2 The Recursive Least-Squares Algorithm
Consider again the example in Section 12.1 We are given experimental points (t0, y0), (t1, y1), and (t2, y2), and we find the parameters m* and c* of the straight line that best fits these data in the least-squares sense. Suppose that we are now given an extra measurement point (t3, y3), so that we now have a set of four experimental data points: (t0, y0), (t1, y1), (t2, y2), and (t3, y3). We can similarly go through the procedure for finding the parameters of the line of best fit for this set of four points. However, as we shall see, there is a more efficient way: We can use previous calculations of m* and c* for the three data points to calculate the parameters for the four data points. In effect, we simply "update" our values of m* and c* to accommodate the new data point. This procedure, called the recursive least-squares (RLS) algorithm, is the topic of this section.
To derive the RLS algorithm, first consider the problem of minimizing ||A0x − b(0)||2. We know that the solution to this is given by x(0) = G0−1A0b(0), where G0 = A0A0. Suppose now that we are given more data, in the form of a matrix A1 and a vector b(1). Consider now the problem of minimizing

The solution is given by

where

Our goal is to write x(1) as a function of x(0), G0, and the new data A1 and b(1). To this end, we first write G1 as

Next, we write

To proceed further, we write A0b(0) as

Combining these formulas, we see that we can write x(1) as

where G1 can be calculated using

We note that with this formula, x(1) can be computed using only x(0), A1, b(0), and G0. Hence, we have a way of using our previous efforts in calculating x(0) to compute x(1), without having to compute x(1) directly from scratch. The solution x(1) is obtained from x(0) by a simple update equation that adds to x(0) a "correction term" . Observe that if the new data are consistent with the old data, that is, A1x(0) = b(1), then the correction term is 0 and the updated solution x(1) is equal to the previous solution x(0).
We can generalize the argument above to write a recursive algorithm for updating the least-squares solution as new data arrive. At the (k + 1)th iteration, we have

The vector b(k+1) − Ak+1x(k) is often called the innovation. As before, observe that if the innovation is zero, then the updated solution x(k+1) is equal to the previous solution x(k).
We can see from the above that to compute x(k+1) from x(k) we need G−1k+1, rather than Gk+1. It turns out that we can derive an update formula for G−1k+1 itself. To do so, we need the following technical lemma, which is a generalization of the Sherman-Morrison formula (Lemma 11.1), due to Woodbury, and hence also called the Sherman-Morrison-Woodbury formula (see [63, p. 124] or [53, p. 50]).
Lemma 12.2 Let A be a nonsingular matrix. Let U and V be matrices such that I + VA−1U is nonsingular. Then, A + UV is nonsingular, and

Proof. We can prove the result easily by verification.
Using Lemma 12.2 we get

For simplicity of notation, we rewrite G−1k as Pk.
We summarize by writing the RLS algorithm using Pk:

In the special case where the new data at each step are such that Ak+1 is a matrix consisting of a single row, Ak+1 = ak+1, and b(k+1) is a scalar, b(k+1) = bk+1, we get

Example 12.6 Let

First compute the vector x(0) minimizing ||A0x − b(0)||2. Then, use the RLS algorithm to find x(2) minimizing

We have

Applying the RLS algorithm twice, we get

We can easily check our solution by computing x directly using the formula x(2) = (AA)−1Ab, where

12.3 Solution to a Linear Equation with Minimum Norm
Consider now a system of linear equations

where A  m × n, b  m, m ≤ n, and rank A = m. Note that the number of equations is no larger than the number of unknowns. There may exist an infinite number of solutions to this system of equations. However, as we shall see, there is only one solution that is closest to the origin: the solution to Ax = b whose norm ||x|| is minimal. Let x* be this solution; that is, Ax* = b and ||x*|| ≤ ||x|| for any x such that Ax = b. In other words, x* is the solution to the problem

In Part IV, we study problems of this type in more detail.
Theorem 12.2 The unique solution x* to Ax = b that minimizes the norm ||x|| is given by

Proof. Let x* = A(AA)−1b. Note that

We now show that

Indeed,

Therefore,

Because ||x − x*||2 > 0 for all x ≠ x*, it follows that for all x ≠ x*,

which implies that

Example 12.7 Find the point closest to the origin of 3 on the line of intersection of the two planes defined by the following two equations:

Note that this problem is equivalent to the problem

where

Thus, the solution to the problem is

In the next section we discuss an iterative algorithm for solving Ax = b.
12.4 Kaczmarz's Algorithm
As in Section 12.3, let A  m × n, b  m, m ≤ n, and rank A = m. We now discuss an iterative algorithm for solving Ax = b, originally analyzed by Kaczmarz in 1937 [70]. The algorithm converges to the vector x* = A(AA)−1b without explicitly having to invert the matrix AA. This is important from a practical point of view, especially when A has many rows.
Let aj denote the jth row of A, and bj the jth component of b, and μ a positive scalar, 0 < μ < 2. With this notation, Kaczmarz's algorithm is:

1. Set i := 0, initial condition x(0).
2. For j = 1,...,m, set

3. Set i := i + 1; go to step 2.

In words, Kaczmarz's algorithm works as follows. For the first m iterations (k = 0,..., m − 1), we have

where, in each iteration, we use rows of A and corresponding components of b successively. For the (m + 1)th iteration, we revert back to the first row of A and the first component of b; that is,

We continue with the (m + 2)th iteration using the second row of A and second component of b, and so on, repeating the cycle every m iteration. We can view the scalar μ as the step size of the algorithm. The reason for requiring that μ be between 0 and 2 will become apparent from the convergence analysis.
We now prove the convergence of Kaczmarz's algorithm, using ideas from Kaczmarz's original paper [70] and subsequent exposition by Parks [102].
Theorem 12.3 In Kaczmarz's algorithm, if x(0) = 0, then x(k) → x* = A(AA)−1b as k → ∞.
Proof. We may assume without loss of generality that ||ai|| = 1, i = 1,...,m. For if not, we simply replace each ai by ai/||a|| and each bi by bi/||ai||.
We first introduce the following notation. For each j = 0,1,2,..., let R(j) denote the unique integer in {0,..., m − 1} satisfying j = lm + R(j) for some integer l; that is, R(j) is the remainder that results if we divide j by m.
Using the notation above, we can write Kaczmarz's algorithm as

Using the identity ||x + y||2 = ||x||2 + ||y||2 + 2x,y, we obtain

Substituting aR(k)+1 x* = bR(k)+1 into this equation, we get

Because 0 < μ < 2, the second term on the right-hand side is nonnegative, and hence

Therefore, {||x(k) − x*||2} is a nonincreasing sequence that is bounded below, because ||x(k) − x*||2 ≥ 0 for all k. Hence, {||x(k) − x*||2} converges (see Theorem 5.3). Furthermore, we may write

Because {||x(k) − x*||2} converges, we conclude that

which implies that

Observe that

and therefore ||x(k+1) − x(k)||2 → 0. Note also that because {||x(k) − x*||2} converges, {x(k)} is a bounded sequence (see Theorem 5.2).
Following Kaczmarz [70], we introduce the notation  = 0,1,2,..., s = 0,..., m − 1. With this notation, we have, for each s = 0,...,m − 1,

as r → ∞. Consider now the sequence {x(r,0) : r ≥ 0}. Because this sequence is bounded, we conclude that it has a convergent subsequence—this follows from the Bolzano-Weierstrass theorem (see [2, p. 70]; see also Section 5.1 for a discussion of sequences and subsequences). Denote this convergent subsequence by {x(r, 0) : r  ε}, where ε is a subset of {0,1,...}. Let z* be the limit of {x(r,0) : r  ε}. Hence,

Next, note that because ||x(k+1) − x(k)||2 → 0 as k → ∞, we also have ||x(r,1) − x(r,0)||2 → 0 as r → ∞. Therefore, the subsequence {x(r,1) : r  ε} also converges to z*. Hence,

Repeating the argument, we conclude that for each i = 1,..., m,

In matrix notation, the equations above take the form

Now, x(k)  (A) for all k because x(0) = 0 (see Exercise 12.25). Therefore, z*  (A), because (A) is closed. Hence, there exists y* such that z* = A y*. Thus,

Because rank A = m, y* = (AA)−1 b and hence z* = x*. Therefore, the subsequence {||xr,0-x*||2 : r  ε} converges to 0. Because {||xr,0 − x*||2 : r  ε} is a subsequence of the convergent sequence {||x(k) − x*||2}, we conclude that the sequence {||x(k) − x*||2} converges to 0; that is, x(k) → x*.
For the case where x(0) ≠ 0, Kaczmarz's algorithm converges to the unique point on {x : Ax = b} minimizing the distance ||x − x(0)|| (see Exercise 12.26).
If we set μ = 1, Kaczmarz's algorithm has the property that at each iteration A:, the "error" bR(k)+1 − aR(k)+1 x(k+1) satisfies

(see Exercise 12.28). Substituting bR(k)+1 = aR(k)+1 x*, we may write

Hence, the difference between x(k+1) and the solution x* is orthogonal to aR(k)+1. This property is illustrated in the following example.
Example 12.8 Let

In this case, x* = [5, 3]. Figure 12.4 shows a few iterations of Kaczmarz's algorithm with μ = 1 and x(0) = 0. We have a1 = [1, −1], a2 − [0, 1], b1 = 2, b2 = 3. In Figure 12.4, the diagonal line passing through the point [2, 0] corresponds to the set {x : a1 x = b1}, and the horizontal line passing through the point [0, 3] corresponds to the set {x : a2 x = b2}. To illustrate the algorithm, we perform three iterations:

Figure 12.4 Iterations of Kaczmarz's algorithm in Example 12.8.



As Figure 12.4 illustrates, the property

holds at every iteration. Note the convergence of the iterations of the algorithm to the solution.
12.5 Solving Linear Equations in General
Consider the general problem of solving a system of linear equations

where A  m × n, and rank A = r. Note that we always have r ≤ min{m, n}. In the case where A  n × n and rank A = n, the unique solution to the equation above has the form x* = A1 b. Thus, to solve the problem in this case it is enough to know the inverse A−1. In this section we analyze a general approach to solving Ax − b. The approach involves defining a pseudoinverse or generalized inverse of a given matrix A  m×n, which plays the role of A−1 when A does not have an inverse (e.g., when A is not a square matrix). In particular, we discuss the Moore-Penrose inverse of a given matrix A, denoted A†.
In our discussion of the Moore-Penrose inverse we use the fact that a nonzero matrix of rank r can be expressed as the product of a matrix of full column rank r and a matrix of full row rank r. Such a factorization is referred to as a full-rank factorization, a term which in this context was proposed by Gantmacher [45] and Ben-Israel and Greville [6]. We state and prove the above result in the following lemma.
Lemma 12.3 Full-Rank Factorization. Let A  m × n, rank A = r ≤ min{m, n}. Then, there exist matrices B  m×r and C  r×n such that

where

Proof. Because rank A = r, we can find r linearly independent columns of A. Without loss of generality, let a1, a2, ..., ar be such columns, where ai is the ith column of A. The remaining columns of A can be expressed as linear combinations of a1, a2, ..., ar. Thus, a possible choice for the full-rank matrices B and C are

where the entries ci,j are such that for each j = r + 1, ..., n, we have aj = c1,j a1 + ··· + cr,j ar. Thus, A = BC.
Note that if m < n and rank A = m, then we can take

where Im is the m×m identity matrix. If, on the other hand, m > n and rank A = n, then we can take

Example 12.9 Let A be given by

Note that rank A = 2. We can write a full-rank factorization of A based on the proof of Lemma 12.3:

We now introduce the Moore-Penrose inverse and discuss its existence and uniqueness. For this, we first consider the matrix equation

where A  m × n is a given matrix and X  n × m is a matrix we wish to determine. Observe that if A is a nonsingular square matrix, then the equation above has the unique solution X = A−1. We now define the Moore-Penrose inverse, also called the pseudoinverse or generalized inverse.
Definition 12.1 Given A  m×n, a matrix A†  n×m is called a pseudoinverse of the matrix A if

and there exist matrices U  n×n, V  m×m such that

The requirement A† = UA = AV can be interpreted as follows. Each row of the pseudoinverse matrix A† of A is a linear combination of the rows of A, and each column of A† is a linear combination of the columns of A.
For the case in which a matrix A  m×n with m ≥ n and rank A = n, we can easily check that the following is a pseudoinverse of A:

Indeed, A(A A)−1 A A = A, and if we define U = (A A)−1 and V = A(AA)−1(AA)−1 A, then A† = U A = AV. Note that, in fact, we have A† A = In. For this reason, (AA)−1 A is often called the left pseudoinverse of A. This formula also appears in least-squares analysis (see Section 12.1).
For the case in which a matrix A  m×n with m ≤ n and rank A = m, we can easily check, as we did in the previous case, that the following is a pseudoinverse of A:

Note that in this case we have A A† = Im. For this reason, A (AA)−1 is often called the right pseudoinverse of A. This formula also appears in the problem of minimizing ||x|| subject to Ax = b (see Section 12.3).
Theorem 12.4 Let A  m×n. If a pseudoinverse A† of A exists, then it is unique.
Proof. Let A†1 and A†2 be pseudoinverses of A. We show that A†1 = A†2. By definition,

and there are matrices U1, U2  n×n, V1, V2  m×m such that

Let

Then, we have

Therefore, using the two equations above, we have

which implies that

On the other hand, because DA = O, we have

which implies that

and hence

From Theorem 12.4, we know that if a pseudoinverse matrix exists, then it is unique. Our goal now is to show that the pseudoinverse always exists. In fact, we show that the pseudoinverse of any given matrix A is given by the formula

where B† and C† are the pseudoinverses of the matrices B and C that form a full-rank factorization of A; that is, A = BC, where B and C are of full rank (see Lemma 12.3). Note that we already know how to compute B† and C†:

Theorem 12.5 Let a matrix A  m×n have a full-rank factorization A = BC, with rank A = rank B = rank C = r, B  m×r, C  r×n. Then,

Proof. We show that

satisfies the conditions of Definition 12.1 for a pseudoinverse. Indeed, first observe that

Next, define

and

It is easy to verify that the matrices U and V above satisfy

Hence,

is the pseudoinverse of A.
Example 12.10 Continued from Example 12.9. Recall that

We compute

and

Thus,

We emphasize that the formula A† = C†B† does not necessarily hold if A = BC is not a full-rank factorization. The following example, which is taken from [45], illustrates this point.
Example 12.11 Let

Obviously, A† = A−1 = A = [1]. Observe that A can be represented as

The above is not a full-rank factorization of A. Let us now compute B† and C†. We have

(Note that the formulas for B† and C† here are different from those in Example 12.10 because of the dimensions of B and C in this example.) Thus,

which is not equal to A†.
We can simplify the expression

to

The expression above is easily verified simply by substituting A = BC. This explicit formula for A† is attributed to C. C. MacDuffee by Ben-Israel and Greville [6]. Ben-Israel and Greville report that around 1959, MacDuffee was the first to point out that a full-rank factorization of A leads to the above explicit formula. However, they mention that MacDuffee did it in a private communication, so there is no published work by MacDuffee that contains the result.
We now prove two important properties of A† in the context of solving a system of linear equations Ax = b.
Theorem 12.6 Consider a system of linear equations Ax = b, A  m×n, rank A = r. The vector x* = A†b minimizes ||Ax − b||2 on n. Furthermore, among all vectors in n that minimize ||Ax − b||2, the vector x* = A†b is the unique vector with minimal norm.
Proof. We first show that x* = A†b minimizes ||Ax − b||2 over n. To this end, observe that for any x  n,

We now show that

Indeed,

However,

Hence,

Thus, we have

Because

we obtain

and thus x* minimizes ||Ax − b||2.
We now show that among all x that minimize ||Ax − b||2, the vector x* − A†b is the unique vector with minimum norm. So let  be a vector minimizing ||Ax − b||2. We have

Observe that

To see this, note that

where the superscript − denotes the transpose of the inverse. Now, ||Ax − b||2 = ||B(Cx) − b||2. Because  minimizes ||Ax − b||2 and C is of full rank, then y* = C minimizes ||By − b||2 over r (see Exercise 12.29). Because B is of full rank, by Theorem 12.1, we have C = y* = (BB)−1 Bb. Substituting this into the equation above, we get x* ( − x*) = 0.
Therefore, we have

For all  ≠ x*, we have

and hence

or, equivalently,

Hence, among all vectors minimizing ||Ax − b||2, the vector x* = A†b is the unique vector with minimum norm.
The generalized inverse has the following useful properties (see Exercise 12.30):

a. (A)† = (A†).
b. (A†)† = A.

These two properties are similar to those that are satisfied by the usual matrix inverse. However, we point out that the property (A1A2)† = A†2A†1 does not hold in general (see Exercise 12.32).
Finally, it is important to note that we can define the generalized inverse in an alternative way, following the definition of Penrose. Specifically, the Penrose definition of the generalized inverse of a matrix A  m×n is the unique matrix A†  n×m satisfying the following properties:

1. AA†A = A.
2. A†AA† = A†.
3. (AA†) = AA†.
4. (A†A) = A† A.

The Penrose definition above is equivalent to Definition 12.1 (see Exercise 12.31). For more information on generalized inverses and their applications, we refer the reader to the books by Ben-Israel and Greville [6] and Campbell and Meyer [23].
EXERCISES

12.1 A rock is accelerated to 3, 5, and 6 m/s2 by applying forces of 1, 2, and 3 N, respectively. Assuming that Newton's law F = ma holds, where F is the force and a is the acceleration, estimate the mass m of the rock using the least-squares method.
12.2 A spring is stretched to lengths L = 3, 4, and 5 cm under applied forces F = 1, 2, and 4 N, respectively. Assuming that Hooke's law L = a + bF holds, estimate the normal length a and spring constant b using the least-squares approach.
12.3 Suppose that we perform an experiment to calculate the gravitational constant g as follows. We drop a ball from a certain height and measure its distance from the original point at certain time instants. The results of the experiment are shown in the following table.

   The equation relating the distance s and the time t at which s is measured is given by

a. Find a least-squares estimate of g using the experimental results from the table above.
b. Suppose that we take an additional measurement at time 4.00 and obtain a distance of 78.5. Use the recursive least-squares algorithm to calculate an updated least-squares estimate of g.
12.4 Suppose that we have a speech signal, represented as a finite sequence of real numbers x1, x2, ..., xn. Suppose that we record this signal onto magnetic tape. The recorded speech signal is represented by another sequence of real numbers y1, y2, ..., yn.
   Suppose that we model the recording process as a simple scaling of the original signal (i.e., we believe that a good model of the relationship between the recorded signal and the original signal is yi = αxi for some constant α that does not depend on i). Suppose that we know exactly the original signal x1, x2, ..., xn as well as the recorded signal y1, y2, ..., yn. Use the least-squares method to find a formula for estimating the scale factor α given this data. (You may assume that at least one xi is nonzero.)
12.5 Suppose that we wish to estimate the value of the resistance R of a resistor. Ohm's law states that if V is the voltage across the resistor and I is the current through the resistor, then V = IR. To estimate R, we apply a 1-ampere current through the resistor and measure the voltage across it. We perform the experiment on n voltage-measuring devices and obtain measurements of V1, ..., Vn. Show that the least-squares estimate of R is simply the average of V1, ..., Vn.
12.6 The table below shows the stock prices for three companies, X, Y, and Z, tabulated over three days:

Suppose that an investment analyst proposes a model for the predicting the stock price of X based on those of Y and Z:

where pX, pY, and pZ are the stock prices of X, Y, and Z, respectively, and a and b are real-valued parameters. Calculate the least-squares estimate of parameters a and b based on the data in the table above.
12.7 We are given two mixtures, A and B. Mixture A contains 30% gold, 40% silver, and 30% platinum, whereas mixture B contains 10% gold, 20% silver, and 70% platinum (all percentages of weight). We wish to determine the ratio of the weight of mixture A to the weight of mixture B such that we have as close as possible to a total of 5 ounces of gold, 3 ounces of silver, and 4 ounces of platinum. Formulate and solve the problem using the linear least-squares method.
12.8 Background: If Ax + w = b, where w is a, "white noise" vector, then define the least-squares estimate of x given b to be the solution to the problem

This problem is related to Wiener filtering.
   Application: Suppose that a given speech signal {uk : k = 1, ..., n} (with uk  ) is transmitted over a telephone cable with input-output behavior given by yk = ayk-1 + buk + vk, where, at each time k, yk   is the output, uk   is the input (speech signal value), and vk represents white noise. The parameters a and b are fixed known constants, and the initial condition is y0 = 0.
   We can measure the signal {yk} at the output of the telephone cable, but we cannot directly measure the desired signal {uk} or the noise signal {vk}. Derive a formula for the linear least-squares estimate of the signal {uk : k − 1, ..., n} given the signal {yk : k = 1, ..., n}.Note: Even though the vector v − [v1, ..., vn] is a white noise vector, the vector Dv (where D is a matrix) is, in general, not.
12.9 Line Fitting. Let [x1, y1], ..., [xp, yp], p ≥ 2, be points in 2. We wish to find the straight line of best fit through these points ("best" in the sense that the total squared error is minimized); that is, we wish to find a*, b*   to minimize

Assume that the xi, i = 1, ..., p, are not all equal. Show that there exist unique parameters a* and b* for the line of best fit, and find the parameters in terms of the following quantities:
.
12.10 Suppose that we take measurements of a sinusoidal signal y(t) = sin(ωt + θ) at times t1, ..., tp, and obtain values y1, ..., yp, where −4π/2 ≥ ωti + θ ≤ π/2, i = 1, ..., p, and the ti are not all equal. We wish to determine the values of the frequency ω and phase θ.
a. Express the problem as a system of linear equations.
b. Find the least-squares estimate of ω and θ based on part a. Use the following notation:
.
12.11 We are given a point [x0, y0]  2. Consider the straight line on the 2 plane given by the equation y = mx. Using a least-squares formulation, find the point on the straight line that is closest to the given point [x0, y0], where the measure of closeness is in terms of the Euclidean norm on 2.Hint: The given line can be expressed as the range of the matrix A = [1, m].
12.12 Consider the affine function f : n →  of the form f(x) = ax + c, where a  n and c  .
a. We are given a set of p pairs (x1, y1), ..., (xp, yp), where xi  n, yi  , i = 1, ..., p. We wish to find the affine function of best fit to these points, where "best" is in the sense of minimizing the total square error

Formulate the above as an optimization problem of the form: minimize ||Az − b||2 with respect to z. Specify the dimensions of A, z, and b.
b. Suppose that the points satisfy

and

Find the affine function of best fit in this case, assuming that it exists and is unique.
12.13 For the system shown in Figure 12.5, consider a set of input-output pairs (u1, y1), ..., (un, yn), where uk  , yk  , k = 1, ..., n.

Figure 12.5 Input-output system in Exercise 12.13.


a. Suppose that we wish to find the best linear estimate of the system based on the input-output data above. In other words, we wish to find a n   to fit the model yk = n uk, k = 1, ..., n. Using the least-squares approach, derive a formula for n based on u1, ..., un and y1, ..., yn.
b. Suppose that the data in part a are generated by

where θ   and uk = 1 for all k. Show that the parameter n in part a converges to θ as n → ∞ if and only if
.
12.14 Consider a discrete-time linear system xk+1 = axk + buk, where uk is the input at time k, xk is the output at time k, and a, b   are system parameters. Suppose that we apply a constant input uk = 1 for all k ≥ 0 and measure the first four values of the output to be x0 = 0, x1 = 1, x2 = 2, x3 = 8. Find the least-squares estimate of a and b based on the data above.
12.15 Consider a discrete-time linear system xk+1 = axk + buk, where uk is the input at time k, xk is the output at time k, and a, b   are system parameters. Given the first n + 1 values of the impulse response h0, ..., hn, find the least-squares estimate of a and b. You may assume that at least one hk is nonzero.Note: The impulse response is the output sequence resulting from an input of u0 = 1, uk = 0 for k ≠ 0 and zero initial condition x0 = 0.
12.16 Consider a discrete-time linear system xk+1 = axk + buk, where uk is the input at time k, xk is the output at time k, and a, b   are system parameters. Given the first n + 1 values of the step response s0, ..., sn, where n > 1, find the least-squares estimate of a and b. You may assume that at least one sk is nonzero.Note: The step response is the output sequence resulting from an input of uk = 1 for k ≥ 0, and zero initial condition x0 = 0 (i.e., s0 = x0 = 0).
12.17 Consider a known discrete-time signal on the time interval {1,..., n}, represented by the vector x  n (xi is the value of the signal at time i). We transmit the signal ax over a communication channel, where a   represents the "amplitude" of the transmission, a quantity unknown to the receiver. The receiver receives a signal y  n, which is a distorted version of the transmitted signal (so that y may not be equal to ax for any a). Formulate the problem of estimating the quantity a according to a least-squares criterion, and solve it (stating whatever appropriate assumptions are necessary, if any).
12.18 Let A  m×n, b  m, m ≥ n, and rank A = n. Consider the constrained optimization problem

where (A) denotes the range of A. Derive an expression for the global minimizer of this problem in terms of A and b.
12.19 Solve the problem

where x0 = [0, −3, 0].
12.20 Let A  m×n, b  m, m ≤ n, rank A = m, and x0  n. Consider the problem
.
Show that this problem has a unique solution given by
.
12.21 Given A  m×n, m ≥ n, rank A = n, and b1, ..., bp  m, consider the problem

Suppose that xi* is a solution to the problem

where i = 1, ..., p. Write the solution to the problem in terms of x1*, ..., xp*.
12.22 Given A  m×n, m ≥ n, rank A = n, b1, ..., bp  m, and α1, ..., αp  , consider the problem
.
Suppose that xi* is the solution to the problem

where i = 1, ..., p. Assuming that α1 + ...+αp > 0, derive a simple expression for the solution to this problem in terms of x1*, ..., xp* and α1, ..., αp.
12.23 Let A  m×n, b  m, ≤ n, and rank A = m. Show that x* = A (AA)−1b is the only vector in (A) satisfying Ax* = b.
12.24 The purpose of this question is to derive a recursive least-squares algorithm where we remove (instead of add) a data point. To formulate the algorithm, suppose that we are given matrices A0 and A1 such that

where a1  n. Similarly, suppose that vectors b(0) and b(1) satisfy

where b1  . Let x(0) be the least-squares solution associated with (A0, b(0)) and x(1) the least-squares solution associated with (A1, b(1)). Our goal is to write x(1) in terms of x(0) and the data point "removed," (a1, b1). As usual, let G0 and G1 be the Grammians associated with x(1), and x(1) respectively.
a. Write down expressions for the least-squares solutions x(0) and x(1) in terms of A0, b(0), A1 and b(1).
b. Derive a formula for G1 in terms of G0 and a1.
c. Let P0 = G−10 and P1 = G−11. Derive a formula for P1 in terms of P0 and a1. (The formula must not contain any matrix inversions.)
d. Derive a formula for A0 b(0) in terms of G1, x(0), and a1.
e. Finally, derive a formula for x(1) in terms of x(0), P1, a1, and b1. Use this and part c to write a recursive algorithm associated with successive removals of rows from (Ak, b(k)).
12.25 Show that in Kaczmarz's algorithm, if x(0) = 0, then x(k)  (A) for all k.
12.26 Consider Kaczmarz's algorithm with x(0) ≠ 0.
a. Show that there exists a unique point minimizing ||x − x(0)|| subject to {x : Ax = b}.
b. Show that Kaczmarz's algorithm converges to the point in part a.
12.27 Consider Kaczmarz's algorithm with x(0) = 0, where m = 1; that is, A = [a]  1×n and a ≠ 0, and 0 < μ < 2. Show that there exists 0 ≤ γ < 1 such that ||x(k+1) −x*|| ≤ γ||x(k) − x*|| for all k ≥ 0.
12.28 Show that in Kaczmarz's algorithm, if μ = 1, then bR(k)+1 −aR(k)+1x(k+1) = 0 for each k.
12.29 Consider the problem of minimizing Ax − b2 over n, where A  m×n, b  m. Let x* be a solution. Suppose that A = BC is a full-rank factorization of A; that is, rank A = rank B = rank C = r, and B  m×r, C  r×n. Show that the minimizer of ||By − b|| over r is Cx*.
12.30 Prove the following properties of generalized inverses:
a. (A)† = (A†).
b. (A†)† = A.
12.31 Show that the Penrose definition of the generalized inverse is equivalent to Definition 12.1.
12.32 Construct matrices A1 and A2 such that (A1A2)† ≠ A†2A†1.








CHAPTER 13
UNCONSTRAINED OPTIMIZATION AND NEURAL NETWORKS
13.1 Introduction
In this chapter we apply the techniques of previous chapters to the training of feedforward neural networks. Neural networks have found numerous practical applications, ranging from telephone echo cancellation to aiding in the interpretation of EEG data (see, e.g., [108] and [72]). The essence of neural networks lies in the connection weights between neurons. The selection of these weights is referred to as training or learning. For this reason, we often refer to the weights as the learning parameters. A popular method for training a neural network is the backpropagation algorithm, based on an unconstrained optimization problem and an associated gradient algorithm applied to the problem. This chapter is devoted to a description of neural networks and the use of techniques developed in preceding chapters for the training of neural networks.
An artificial neural network is a circuit composed of interconnected simple circuit elements called neurons. Each neuron represents a map, typically with multiple inputs and a single output. Specifically, the output of the neuron is a function of the sum of the inputs, as illustrated in Figure 13.1. The function at the output of the neuron is called the activation function. We use the symbol shown in Figure 13.2 to represent a single neuron. Note that the single output of the neuron may be used as an input to several other neurons, and therefore the symbol for a single neuron has multiple arrows emanating from it. A neural network may be implemented using an analog circuit. In this case inputs and outputs may be represented by currents and voltages.

Figure 13.1 Single neuron.



Figure 13.2 Symbol for a single neuron.


A neural network consists of interconnected neurons, with the inputs to each neuron consisting of weighted outputs of other neurons. The interconnections allow exchange of data or information between neurons. In a feedforward neural network, the neurons are interconnected in layers, so that the data flow in only one direction. Thus, each neuron receives information only from neurons in the preceding layer: The inputs to each neuron are weighted outputs of neurons in the preceding layer. Figure 13.3 illustrates the structure of feedforward neural networks. The first layer in the network is called the input layer, and the last layer is called the output layer. The layers in between the input and output layers are called hidden layers.

Figure 13.3 Structure of a feedforward neural network.


We can view a neural network as simply a particular implementation of a map from n to m, where n is the number of inputs x1, ..., xn and m is the number of outputs y1, ..., ym. The map that is implemented by a neural network depends on the weights of the interconnections in the network. Therefore, we can change the mapping that is implemented by the network by adjusting the values of the weights in the network. The information about the mapping is "stored" in the weights over all the neurons, and thus the neural network is a distributed representation of the mapping. Moreover, for any given input, computation of the corresponding output is achieved through the collective effect of individual input-output characteristics of each neuron; therefore, the neural network can be considered as a parallel computation device. We point out that the ability to implement or approximate a map encompasses many important practical applications. For example, pattern recognition and classification problems can be viewed as function implementation or approximation problems.
Suppose that we are given a map F : n → m that we wish to implement using a given neural network. Our task boils down to selecting the interconnection weights in the network appropriately. As mentioned earlier, we refer to this task as training of the neural network or learning by the neural network. We use input-output examples of the given map to train the neural network. Specifically, let (xd,1, yd,1), ..., (xd,p, yd,p)  n × m? where each yd,i is the output of the map F corresponding to the input xd,i; that is, yd,i = F(xd,i). We refer to the set {(xd,1, yd,1), ..., (xd,p, yd,p)} as the training set. We train the neural network by adjusting the weights such that the map that is implemented by the network is close to the desired map F. For this reason, we can think of neural networks as function approximators.
The form of learning described above can be thought of as learning with a teacher. The teacher supplies questions to the network in the form of xd, 1, ..., xd,p and tells the network the correct answers yd,1 ..., yd,p. Training of the network then comprises applying a training algorithm that adjusts weights based on the error between the computed and desired outputs; that is, the difference between yd, i = F(xd,i) and the output of the neural network corresponding to xd,i. Having trained the neural network, our hope is that the network correctly generalizes the examples used in the training set. By this we mean that the network should correctly implement the mapping F and produce the correct output corresponding to any input, including those that were not a part of the training set.
As we shall see in the remainder of this chapter, the training problem can be formulated as an optimization problem. We can then use optimization techniques and search methods (e.g., steepest descent, conjugate gradients [69], and quasi-Newton) for selection of the weights. The training algorithms are based on such optimization algorithms.
In the literature, for obvious reasons, the form of learning described above is referred to as supervised learning, a term which suggests that there is also a form of learning called unsupervised learning. Indeed, this is the case. However, unsupervised learning does not fit into the framework described above. Therefore, we do not discuss the idea of unsupervised learning any further. We refer the interested reader to [60].
13.2 Single-Neuron Training
Consider a single neuron, as shown in Figure 13.4. For this particular neuron, the activation function is simply the identity (linear function with unit slope). The neuron implements the following (linear) map from n to :

Figure 13.4 Single linear neuron.



where x = [x1, ..., xn]  n is the vector of inputs, y   is the output, and w = [w1, ..., wn]  n is the vector of weights. Suppose that we are given a map F : n → . We wish to find the value of the weights w1, ..., wn such that the neuron approximates the map F as closely as possible. To do this, we use a training set consisting of p pairs {(xd,1, yd, 1), ..., (xd,p, yd,p)}, where xd,i  n and yd,i  , i = 1, ..., p. For each i, yd,i = F(xd,i) is the "desired" output corresponding to the given input xd,i. The training problem can then be formulated as the following optimization problem:

where the minimization is taken over all w = [w1, ..., wn]  n. Note that the objective function represents the sum of the squared errors between the desired outputs yd,i and the corresponding outputs of the neuron xd,i w. The factor of 1/2 is added for notational convenience and does not change the minimizer.
The objective function above can be written in matrix form as follows. First define the matrix Xd  n × p and vector yd  p by

Then, the optimization problem becomes minimize

There are two cases to consider in this optimization problem: p ≤ n and p > n. We first consider the case where p ≤ n, that is, where we have at most as many training pairs as the number of weights. For convenience, we assume that rank Xd = p. In this case there are an infinitely many points satisfying yd = Xd w. Hence, there are infinitely many solutions to the optimization problem above, with the optimal objective function value of 0. Therefore, we have a choice of which optimal solution to select. A possible criterion for this selection is that of minimizing the solution norm. This is exactly the problem considered in Section 12.3. Recall that the minimum-norm solution is w* = Xd(Xd Xd)−1 yd. An efficient iterative algorithm for finding this solution is Kaczmarz's algorithm (discussed in Section 12.4). Kaczmarz's algorithm in this setting takes the form

where w(0) = 0 and

Recall that R(k) is the unique integer in {0, ..., p − 1} satisfying k = lp + R(k) for some integer l; that is, R(k) is the remainder that results if we divide k by p (see Section 12.4 for further details on the algorithm).
The algorithm above was applied to the training of linear neurons by Widrow and Hoff (see [132] for some historical remarks). The single neuron together with the training algorithm above is illustrated in Figure 13.5 and is often called Adaline, an acronym for adaptive linear element.

Figure 13.5 Adaline.


We now consider the case where p > n. Here, we have more training points than the number of weights. We assume that rank . In this case the objective function  is simply a strictly convex quadratic function of w, because the matrix  is a positive definite matrix. To solve this optimization problem, we have at our disposal the whole slew of unconstrained optimization algorithms considered in earlier chapters. For example, we can use a gradient algorithm, which in this case takes the form

where e(k) = yd − Xdw(k).
The discussion above assumed that the activation function for the neuron is the identity map. The derivation and analysis of the algorithms can be extended to the case of a general differentiate activation function fa. Specifically, the output of the neuron in this case is given by

The algorithm for the case of a single training pair (xd, yd) has the form

where the error is given by

For a convergence analysis of the algorithm above, see [64].
13.3 The Backpropagation Algorithm
In Section 13.2 we considered the problem of training a single neuron. In this section we consider a neural network consisting of many layers. For simplicity of presentation, we restrict our attention to networks with three layers, as depicted in Figure 13.6. The three layers are referred to as the input, hidden, and output layers. There are n inputs xi, where i = 1, ..., n. We have m outputs ys, s = 1, ..., m. There are l neurons in the hidden layer. The outputs of the neurons in the hidden layer are zj, where j = 1, ..., l. The inputs x1, ..., xn are distributed to the neurons in the hidden layer. We may think of the neurons in the input layer as single-input-single-output linear elements, with each activation function being the identity map. In Figure 13.6 we do not explicitly depict the neurons in the input layer; instead, we illustrate the neurons as signal spliters. We denote the activation functions of the neurons in the hidden layer by fhj, where j = 1,..., l, and the activation functions of the neurons in the output layer by fos, where s = 1, ..., m. Note that each activation function is a function from  to .

Figure 13.6 Three-layered neural network.


We denote the weights for inputs into the hidden layer by whji, i = 1, ..., n, j = 1, ..., l. We denote the weights for inputs from the hidden layer into the output layer by wosj, j = 1, ..., l, s = 1, ..., m. Given the weights whji and wosj, the neural network implements a map from n to m. To find an explicit formula for this map, let us denote the input to the jth neuron in the hidden layer by vj and the output of the jth neuron in the hidden layer by zj. Then, we have

The output from the sth neuron of the output layer is

Therefore, the relationship between the inputs xi, i = 1, ..., n, and the sth output ys is given by

The overall mapping that the neural network implements is therefore given by

We now consider the problem of training the neural network. As for the single neuron considered in Section 13.2, we analyze the case where the training set consists of a single pair (xd, yd), where xd  n and yd  m. In practice, the training set consists of many such pairs, and training is typically performed with each pair at a time (see, e.g., [65] or [113]). Our analysis is therefore also relevant to the general training problem with multiple training pairs.
The training of the neural network involves adjusting the weights of the network such that the output generated by the network for the given input xd = [xd1, ..., xdn] is as close to yd as possible. Formally, this can be formulated as the following optimization problem:

where ys, s = 1, ..., m, are the actual outputs of the neural network in response to the inputs xd1, ..., xdn, as given by

This minimization is taken over all whji, wosj, i = 1, ..., n, j = 1, ..., l, s = 1, ..., m. For simplicity of notation, we use the symbol w for the vector

and the symbol E for the objective function to be minimized; that is,

To solve the optimization problem above, we use a gradient algorithm with fixed step size. To formulate the algorithm, we need to compute the partial derivatives of E with respect to each component of w. For this, let us first fix the indices i, j, and s. We first compute the partial derivative of E with respect to wosj For this, we write

where for each q = 1, ..., l,

Using the chain rule, we obtain

where fo′s :  →  is the derivative of fos. For simplicity of notation, we write

We can think of each δs as a scaled output error, because it is the difference between the actual output ys of the neural network and the desired output yds, scaled by . Using the δs notation, we have

We next compute the partial derivative of E with respect to whji. We start with the equation

Using the chain rule once again, we get

where fh′j :  →  is the derivative of fhj. Simplifying the above yields

We are now ready to formulate the gradient algorithm for updating the weights of the neural network. We write the update equations for the two sets of weights wosj and whji separately. We have

where η is the (fixed) step size and

The update equation for the weights wosj of the output layer neurons is illustrated in Figure 13.7, whereas the update equation for the weights whji of the hidden layer neurons is illustrated in Figure 13.8.

Figure 13.7 Illustration of the update equation for the output layer weights.



Figure 13.8 Illustration of the update equation for the hidden layer weights.


The update equations above are referred to in the literature as the backpropagation algorithm. The reason for the name backpropagation is that the output errors  are propagated back from the output layer to the hidden layer and are used in the update equation for the hidden layer weights, as illustrated in Figure 13.8. In the discussion above we assumed only a single hidden layer. In general, we may have multiple hidden layers—in this case the update equations for the weights will resemble the equations derived above. In the general case the output errors are propagated backward from layer to layer and are used to update the weights at each layer.
We summarize the backpropagation algorithm qualitatively as follows. Using the inputs xdi and the current set of weights, we first compute the quantities v(k)j, z(k)j, y(k)s, and δ(k)s, in turn. This is called the forward pass of the algorithm, because it involves propagating the input forward from the input layer to the output layer. Next, we compute the updated weights using the quantities computed in the forward pass. This is called the reverse pass of the algorithm, because it involves propagating the computed output errors δ(k)s backward through the network. We illustrate the backpropagation procedure numerically in the following example.
Example 13.1 Consider the simple feedforward neural network shown in Figure 13.9. The activation functions for all the neurons are given by f(v) = 1/(1 + e−v). This particular activation function has the convenient property that f′(v) = f(v)(1 − f(v)). Therefore, using this property, we can write

Figure 13.9 Neural network for Example 13.1.



Suppose that the initial weights are wh(0)11 = 0.1, wh(0)12 = 0.3, wh(0)21 = 0.3, wh(0)22 = 0.4, wo(0)11 = 0.4, and wo(0)12 = 0.6. Let xd = [0.2,0.6] and yd = 0.7. Perform one iteration of the backpropagation algorithm to update the weights of the network. Use a step size of η = 10.
To proceed, we first compute

Next, we compute

We then compute

which gives an output error of

This completes the forward pass.
To update the weights, we use

and, using the fact that f′(v(0)j) = f(v(0)j)(1 − f(0)j) = z(0)z (1 − z(0)j) we get

Thus, we have completed one iteration of the backpropagation algorithm. We can easily check that y(1)1 = 0.6588, and hence |yd − y(1)1| < |yd − y(0)1|; that is, the actual output of the neural network has become closer to the desired output as a result of updating the weights.
After 15 iterations of the backpropagation algorithm, we get

The resulting value of the output corresponding to the input xd = [0.2,0.6] is y(15)1 = 0.6997.
In the example above, we considered an activation function of the form

This function is called a sigmoid and is a popular activation function used in practice. The sigmoid function is illustrated in Figure 13.10. It is possible to use a more general version of the sigmoid function, of the form

Figure 13.10 Sigmoid function.



The parameters β and θ represent scale and shift (or location) parameters respectively. The parameter θ is often interpreted as a threshold. If such an activation function is used in a neural network, we would also want to adjust the values of the parameters β and θ, which also affect the value of the objective function to be minimized. However, it turns out that these parameters can be incorporated into the backpropagation algorithm simply by treating them as additional weights in the network. Specifically, we can represent a neuron with activation function g as one with activation function f with the addition of two extra weights, as shown in Figure 13.11.

Figure 13.11 Two configurations that are equivalent.


Example 13.2 Consider the same neural network as in Example 13.1. We introduce shift parameters θ1, θ2, and θ3 to the activation functions in the neurons. Using the configuration illustrated in Figure 13.11, we can incorporate the shift parameters into the backpropagation algorithm. We have

where f is the sigmoid function:

The components of the gradient of the objective function E with respect to the shift parameters are

In the next example, we apply the network discussed in Example 13.2 to solve the celebrated exclusive OR (XOR) problem (see [113]).
Example 13.3 Consider the neural network of Example 13.2. We wish to train the neural network to approximate the exclusive OR (XOR) function, defined in Table 13.1. Note that the XOR function has two inputs and one output.
Table 13.1 Truth Table for XOR Function


x1
x2
F(x1, x2)


0
0
0


0
1
1


1
0
1


1
1
0


To train the neural network, we use the following training pairs:

We now apply the backpropagation algorithm to train the network using the training pairs above. To do this, we apply one pair per iteration in a cyclic fashion. In other words, in the kth iteration of the algorithm, we apply the pair (xd,R(k)+1,yd,R(k)+1) where, as in Kaczmarz's algorithm, R(k) is the unique integer in {0,1,2,3} satisfying k = 4l + R(k) for some integer l; that is, R(k) is the remainder that results if we divide k by 4 (see Section 12.4).
The experiment yields the following weights (see Exercise 13.5):

Table 13.2 shows the output of the neural network with the weights above corresponding to the training input data. Figure 13.12 shows a plot of the function that is implemented by this neural network.

Figure 13.12 Plot of the function implemented by the trained network of Example 13.3.


Table 13.2 Response of the Trained Network of Example 13.3


x1
x2
y1


0
0
0.007


0
1
0.99


1
0
0.99


1
1
0.009


For a more comprehensive treatment of neural networks, see [58], [59], or [137]. For applications of neural networks to optimization, signal processing, and control problems, see [28] and [67].
EXERCISES

13.1 Consider a single linear neuron, with n inputs (see Figure 13.4). Suppose that we are given Xd  n × p and yd  P representing p training pairs, where p > n. The objective function to be minimized in the training of the neuron is

a. Find the gradient of the objective function.
b. Write the conjugate gradient algorithm for training the neuron.
c. Suppose that we wish to approximate the function F : 2 →  given by

Use the conjugate gradient algorithm from part b to train the linear neuron, using the following training points:

It may helpful to use the MATLAB program from Exercise 10.11.
d. Plot the level sets of the objective function for the problem in part c, at levels 0.01, 0.1, 0.2, and 0.4. Check if the solution in part c agrees with the level sets.
e. Plot the error function e(x) = F(x) − w*x versus x1 and x2, where w* is the optimal weight vector obtained in part c.
13.2 Consider the Adaline, depicted in Figure 13.5. Assume that we have a single training pair (xd, yd), where xd ≠ 0. Suppose that we use the Widrow-Hoff algorithm to adjust the weights:

where ek = yd − xdw(k)
a. Write an expression for ek+1 as a function of ek and μ.
b. Find the largest range of values for μ for which ek → 0 (for any initial condition w(0)).
13.3 As in Exercise 13.2, consider the Adaline. Consider the case in which there are multiple pairs in the training set {(xd,1, yd,1),...,(xd,p, yd,p)} where p ≤ n and rank Xd = p (the matrix Xd has xd,i as its ith column). Suppose that we use the following training algorithm:

where e(k) = yd − Xd and μ is a given constant p × p matrix.
a. Find an expression for e(k+1) as a function of e(k) and μ.
b. Find a necessary and sufficient condition on μ for which e(k) → 0 (for any initial condition w(0)).
13.4 Consider the three-layered neural network described in Example 13.1 (see Figure 13.9). Implement the backpropagation algorithm for this network in MATLAB. Test the algorithm for the training pair xd = [0,1] and yd = 1. Use a step size of η = 50 and initial weights as in the Example 13.1.
13.5 Consider the neural network of Example 13.3, with training pairs for the XOR problem. Use MATLAB to implement the training algorithm described in Example 13.3, with a step size of η = 10.0. Tabulate the outputs of the trained network corresponding to the training input data.








CHAPTER 14
GLOBAL SEARCH ALGORITHMS
14.1 Introduction
The iterative algorithms in previous chapters, in particular gradient methods, Newton's method, conjugate gradient methods, and quasi-Newton methods, start with an initial point and then generate a sequence of iterates. Typically, the best we can hope for is that the sequence converges to a local minimizer. For this reason, it is often desirable for the initial point to be close to a global minimizer. Moreover, these methods require first derivatives (and also second derivatives in the case of Newton's method).
In this chapter we discuss various search methods that are global in nature in the sense that they attempt to search throughout the entire feasible set. These methods use only objective function values and do not require derivatives. Consequently, they are applicable to a much wider class of optimization problems. In some cases, they can also be used to generate "good" initial (starting) points for the iterative methods discussed in earlier chapters. Some of the methods we discuss in this chapter (specifically, the randomized search methods) are also used in combinatorial optimization, where the feasible set is finite (discrete), but typically large.
14.2 The Nelder-Mead Simplex Algorithm
The method originally proposed by Spendley, Hext, and Himsworth [122] in 1962 was improved by Nelder and Mead [97] in 1965 and it is now commonly referred to as the Nelder-Mead simplex algorithm. A contemporary view of the algorithm is provided in the well-written paper by Lagarias et al. [82]. In our exposition, we use the notation of this paper.
The Nelder-Mead algorithm is a derivative-free method. The method uses the concept of a simplex. A simplex is a geometric object determined by an assembly of n + 1 points, P0,P1,...,pn, n-dimensional space such that

This condition ensures that two points in  do not coincide, three points in 2 are not colinear, four points in 3 are not coplanar, and so on. Thus, simplex in  is a line segment, in 2 it is a triangle, while a simplex in 3 is a tetrahedron; in each case it encloses a finite n-dimensional volume.
Suppose that we wish to minimize f(x), x  n. To start the algorithm, we initialize a simplex of n + 1 points. A possible way to set up a simplex, as suggested by Jang, Sun, and Mizutani [67], is to start with an initial point x(0) = p0 and generate the remaining points of the initial simplex as follows:

where the ei are unit vectors constituting the natural basis of n as described in Section 2.1. The positive constant coefficients λi are selected in such a way that their magnitudes reflect the length scale of the optimization problem. Our objective is to modify the initial simplex stage by stage so that the resulting simplices converge toward the minimizer. At each iteration we evaluate the function f at each point of the simplex. In the function minimization process, the point with the largest function value is replaced with another point. The process for modifying the simplex continues until it converges toward the function minimizer.
We now present the rules for modifying the simplex stage by stage. To aid in our presentation, we use a two-dimensional example to illustrate the rules. We begin by selecting the initial set of n + 1 points that are to form the initial simplex. We next evaluate f at each point and order the n + 1 vertices to satisfy

For the two-dimensional case we let pl, pnl, and ps denote the points of the simplex for which f is largest, next largest, and smallest; that is, because we wish to minimize f, the vertex ps is the best vertex, pl is the worst vertex, and pnl is the next-worst vertex. We next compute pg, the centroid (center of gravity) of the best n points:

In our two-dimensional case, n = 2, we would have

We then reflect the worst vertex, pl, in pg using a reflection coefficient ρ > 0 to obtain the reflection point

A typical value is ρ = 1. The operation above is illustrated in Figure 14.1. We proceed to evaluate f at pr to obtain fr = f (pr). If f0 ≤ fr < fn−1 [i.e., if fr lies between fs = f (ps) and fnl = f (pnl)], then the pr replaces pl to from a new simplex, and we terminate the iteration. We proceed to repeat the process. Thus, we compute the centroid of the best n vertices of the new simplex and again reflect the point with the largest function f value in the centroid obtained for the best n points of the new simplex.

Figure 14.1 Reflecting pl in pg with a reflection coefficient ρ.


If, however, fr < fs = f0, so that the point pr yields the smallest function value among the points of the simplex, we argue that this direction is a good one. In this case we increase the distance traveled using an expansion coefficient χ > 1 (e.g., χ = 2) to obtain

The operation above yields a new point on the line plpgpr extended beyond pr. We illustrate this operation in Figure 14.2. If fe < fr now, the expansion is declared a success and pe replaces pl in the next simplex. If, on the other hand, fe ≥ fr, the expansion is a failure and pr replaces pl.

Figure 14.2 Expansion operation with the expansion coefficient χ.



Figure 14.3 Outside contraction operation for the case when fr  [fnl, fl).


Finally, if fr ≥ fnl, the reflected point pr would constitute the point with the largest function value in the new simplex. Then in the next step it would be reflected in pg, probably an unfruitful operation. Instead, this case is dealt with by a contraction operation in one of two ways. First, if fr ≥ fnl and fr < fl, then we contract (pr − pg) with a contraction coefficient 0 < γ < 1 (e.g., γ = 1/2) to obtain

We refer to this operation as the outside contraction. See Figure 14.3 for an illustration of this operation. If, on the other hand, fr ≥ fnl and fr ≥ fl, then pl replaces pr in the contraction operation and we get

This operation, referred to as the inside contraction, is illustrated in Figure 14.4.

Figure 14.4 Inside contraction operation for the case when fr ≥ fl.


If, in either case, fc ≤ fl, the contraction is considered a success, and we replace pl with pc in the new simplex. If, however, fc > fl, the contraction is a failure, and in this case a new simplex can be formed by retaining ps only and halving the distance from ps to every other point in the simplex. We can refer to this event as a shrinkage operation. The shrinkage operation is illustrated in Figure 14.5. In general, the shrink step produces the n new vertices of the new simplex according to the formula

Figure 14.5 Shrinkage operation.



where σ = 1/2. Hence, the vertices of the new simplex are ps, v1, ..., vn.
When implementing the simplex algorithm, we need a tie-breaking rule to order points in the case of equal function values. Lagarias et al. [82] propose tie-breaking rules that assign to the new vertex the highest possible index consistent with the relation

In Figure 14.6 we illustrate the simplex search method by showing the first few stages of the search for a minimizer of a function of two variables. Our drawing is inspired by a figure in Layton [84, p. 225]. The starting simplex is composed of the vertices A, B, and C. The vertices D and E are obtained by the expansion operation. The vertex F is obtained by the reflection operation. The vertex G is obtained using the outside contraction operation, while the vertex I is obtained employing the inside contraction operation. For the sake of clarity we terminate the process with the simplex composed of the vertices E, H, and I. The process may, of course, be continued beyond this simplex.

Figure 14.6 The simplex search method applied to minimization of a function of two variables.


We add that a variant of the simplex method described above is presented in Jang et al. [67], where they use the centroid of the entire simplex rather than the centroid of the best n vertices of the simplex. That is, Jang et al. [67] compute the point pg using the n + 1 vertices of the simplex. In addition, they use only the inside contraction and they do not use the outside contraction operation.
14.3 Simulated Annealing
Randomized Search
Simulated annealing is an instance of a randomized search method. A randomized search method, also sometimes called a probabilistic search method, is an algorithm that searches the feasible set of an optimization problem by considering randomized samples of candidate points in the set. The simulated annealing algorithm was first suggested for optimization by Kirkpatrick et al. [75] based on techniques of Metropolis et al. [91]. An early application to image processing was described by Geman and Geman [48].
As usual, suppose that we wish to solve an optimization problem of the form

The basic assumption in randomized search is our ability to select a random sample from the feasible set Ω. Typically, we start a randomized search process by selecting a random initial point x(0)  ω. Then, we select a random next-candidate point, usually close to x(0).
More formally, we assume that for any x  Ω, there is a set N(x) ⊂ Ω such that we can generate a random sample from this set. Typically, N(x) is a set of points that are "close" to x, and for this reason we usually think of N(x) as a "neighborhood" of x [we use the term neighborhood for N(x) even in the general case where the points in it are arbitrary, not necessarily close to x]. When we speak of generating a random point in N(x), we mean that there is a prespecified distribution over N(x), and we sample a point with this distribution. Often, this distribution is chosen to be uniform over N(x); other distributions are also used, including Gaussian and Cauchy.
Before discussing the simulated annealing method, we first consider a simple randomized search algorithm, which we will call naive random search.
Naive Random Search Algorithm

1. Set k := 0. Select an initial point x(0)  Ω.
2. Pick a candidate point z(k) at random from N(x(k)).
3. If f(z(k)) < f(x(k)), then set x(k+1) = z(k); else, set x(k+1) = z(k).
4. If stopping criterion satisfied, then stop.
5. Set k := k + 1, go to step 2.

Note that the algorithm above has the familiar form x(k+1) = x(k) + d(k), where d(k) is randomly generated. By design, the direction d(k) either is 0 or is a descent direction. Typical stopping criteria include reaching a certain number of iterations, or reaching a certain objective function value.
Simulated Annealing Algorithm
The main problem with the naive random search method is that it may get stuck in a region around a local minimizer. This is easy to imagine; for example, if x(0) is a local minimizer and N(x(0)) is sufficiently small that all points in it have no smaller objective function value than x(0) [i.e., x(0) is a global minimizer of f over N(x(0))], then clearly the algorithm will be stuck and will never find a point outside of N(x(0)). To prevent getting stuck in a region around a local minimizer, we need a way to consider points outside this region. One way to achieve this goal is to make sure that at each k, the neighborhood N(x(k)) is a very large set. Indeed, if N(x(k)) is sufficiently large, then we are guaranteed that the algorithm will converge (in some sense) to a global minimizer. An extreme example of this case is where N(x) = Ω for any x  Ω (in this case running k iterations of the naive random search algorithm amounts to finding the best point among k randomly chosen points in Ω). However, having too large a neighborhood in the search algorithm results in a slow search process, because the sampling of candidate points to consider is spread out, making it more unlikely to find a better candidate point.
Another way to overcome the problem of getting stuck in a region around a local minimizer is to modify the naive search algorithm so that we can "climb out" of such a region. This means that the algorithm may accept a new point that is worse than the current point. The simulated annealing algorithm incorporates such a mechanism.
Simulated Annealing Algorithm

1. Set k := 0; select an initial point x(0)  Ω.
2. Pick a candidate point z(k) at random from N(x(k)).
3. Toss a coin with probability of HEAD equal to p(k, f(z(k)), f(x(k)). If HEAD, then set x(k+1) = z(k); else, set x(k+1) = z(k).
4. If the stopping criterion is satisfied, then stop.
5. Set k := k + 1, go to step 2.

In step 3, the use of a "coin toss" is simply descriptive for a randomized decision—we do not mean literally that an actual coin needs to be tossed.
As in naive random search, the simulated annealing algorithm above has the familiar form x(k+1) = x(k) + d(k), where d(k) is randomly generated. But in simulated annealing the direction d(k) might be an ascent direction. However, as the algorithm progresses, we can keep track of the best-so-far point—this is a point , which, at each k, is equal to a x(j), j  {0, ..., k}, such that f(x(j)) ≤ f(x(i)) for all i  {0, ..., k}. The best-so-far point can be updated at each step k as follows:

By keeping track of the best-so-far point, we can treat the simulated annealing algorithm simply as a search procedure; the best-so-far point is what we eventually use when the algorithm stops. This comment applies not only to simulated annealing, but other search techniques as well (including the randomized algorithms presented in the next two sections).
The major difference between simulated annealing and naive random search is that in step 5, there is some probability that we set the next iterate to be equal to the random point selected from the neighborhood, even if that point turns out to be worse than the current iterate. This probability is called the acceptance probability. For the algorithm to work properly, the acceptance probability must be chosen appropriately. A typical choice is

where exp is the exponential function and Tk represents a positive sequence called the temperature schedule or cooling schedule. This form of acceptance probability is usually credited to Boltzmann and leads to a simulated annealing algorithm that behaves as a Gibbs sampler (a method of probabilistic sampling based on the Gibbs distribution).
Notice that if f(z(k)) ≤ f(x(k)), then p(k, f(z(k))f(x(k))) = 1, which means that we set x(k+1) = z(k) (i.e., we move to the point z(k)). However, if f(z(k)) > f(x(k)), there is still a positive probability of setting x(k+1) = z(k); this probability is equal to

Note that the larger the difference between f(z(k)) and f(x(k)), the less likely we are to move to the worse point z(k). Similarly, the smaller the value of Tk, the less likely we are to move to z(k). It is typical to let the "temperature" Tk be monotonically decreasing to 0 (hence the word cooling). In other words, as the iteration index k increases, the algorithm becomes increasingly reluctant to move to a worse point. The intuitive reason for this behavior is that initially we wish to actively explore the feasible set, but with time we would like to be less active in exploration so that we spend more time in a region around a global minimizer. In other words, the desired behavior is this: Initially, the algorithm jumps around and is more likely to climb out of regions around local minimizers, but with time it settles down and is more likely to spend time around a global minimizer.
The term annealing comes from the field of metallurgy, where it refers to a technique for improving the property of metals. The basic procedure is to heat up a piece of metal and then cool it down in a controlled fashion. When the metal is first heated, the atoms in it become unstuck from their initial positions (with some level of internal energy). Then, as cooling takes place, the atoms gradually configure themselves in states of lower internal energy. Provided that the cooling is sufficiently slow, the final internal energy is lower than the initial internal energy, thereby refining the crystalline structure and reducing defects.
In an analogous way, the temperature in simulated annealing must be cooled in a controlled fashion. In particular, the cooling should be sufficiently slow. In a seminal paper, Hajek [56] provides a rigorous analysis of the cooling schedule for convergence of the algorithm to a global minimizer. Specifically, he shows that an appropriate cooling schedule is

where γ > 0 is a problem-dependent constant (large enough to allow the algorithm to "climb out" of regions around local minimizers that are not global minimizers). See also [57] for an analysis of a generalized version of simulated annealing.
Simulated annealing is often also used in combinatorial optimization, where the feasible set is finite (but typically large). An example of such a problem is the celebrated traveling salesperson problem. In the most basic form of this problem, we are given a number of cities and the cost of traveling from any city to any other city. The optimization problem is to find the cheapest round-trip route, starting from a given city, that visits every other city exactly once. For a description of how to apply simulated annealing to the traveling salesperson problem, see [67, p. 183].
14.4 Particle Swarm Optimization
Particle swarm optimization (PSO) is a randomized search technique presented by James Kennedy (a social psychologist) and Russell C. Eberhart (an engineer) in 1995 [73]. This optimization method is inspired by social interaction principles. The PSO algorithm differs from the randomized search methods discussed in Section 14.3 in one key way: Instead of updating a single candidate solution x(k) at each iteration, we update a population (set) of candidate solutions, called a swarm. Each candidate solution in the swarm is called a particle. We think of a swarm as an apparently disorganized population of moving individuals that tend to cluster together while each individual seems to be moving in a random direction. (This description was adapted from a presentation by R. C. Eberhart.) The PSO algorithm aims to mimic the social behavior of animals and insects, such as a swarm of bees, a flock of birds, or a herd of wildebeest.
Suppose that we wish to minimize an objective function over n. In the PSO algorithm, we start with an initial randomly generated population of points in n. Associated with each point in the population is a velocity vector. We think of each point as the position of a particle, moving with an associated velocity. We then evaluate the objective function at each point in the population. Based on this evaluation, we create a new population of points together with a new set of velocities. The creation of points in the new population, and their velocities, involve certain operations on points and velocities of the particles in the preceding population, described later.
Each particle keeps track of its best-so-far position—this is the best position it has visited so far (with respect to the value of the objective function). We will call this particle-dependent best-so-far position a personal best (pbest). In contrast, the overall best-so-far position (best among all the positions encountered so far by the entire population) is called a global best (gbest).
The particles "interact" with each other by updating their velocities according to their individual personal best as well as the global best. In the gbest version of the PSO algorithm, presented below, the velocity of each particle is changed, at each time step, toward a combination of its pbest and the gbest locations. The velocity is weighted by a random term, with separate random numbers being generated for velocities toward pbest and gbest locations. Thus, the particles are drawn both to their own personal best positions as well as to the best position of the entire swarm. As usual, typical stopping criteria of the algorithm consist of reaching a certain number of iterations, or reaching a certain objective function value.
Basic PSO Algorithm
We now present a simple version of the gbest version of the PSO algorithm, where at each time step the velocity of each particle is changed toward its pbest and the gbest locations. Let f : n →  be the objective function that we wish to minimize. Let d be the population size, and index the particles in the swarm by i = 1, ..., d. Denote the position of particle i by xi  n and its velocity by vi  n. Let pi be the pbest of particle i and g the gbest.
It is convenient to introduce the Hadamard product (or Schur product) operator, denoted by : If A and B are matrices with the same dimension, then AB is a matrix of the same dimension as A (or B) resulting from entry-by-entry multiplication of A and B. This operation is denoted in MATLAB by ".*" (the dot before an operator indicates entry-by-entry operations). Thus, if A and B have the same dimension, then A.*B returns a matrix whose entries are simply the products of the corresponding individual entries of A and B. The PSO gbest algorithm uses three given constant real parameters, ω, c1, and c2, which we discuss after presenting the algorithm.
PSO Gbest Algorithm

1. Set k := 0. For i = 1, ..., d, generate initial random positions x(0)i and velocities v(0)i, and set p(0)i = x(0)i. Set g(0) = arg 
2. For i = 1, ..., d, generate random n-vectors r(k)i and s(k)i with components uniformly in the interval (0,1), and set

3. For i = 1, ..., d, if f(x(k+1))i < f(p(k)i), then set p(k+1)i = x(k+1)i; else, set p(k+1)i = p(k)i.
4. If there exists i  {1, ..., d} such that f(x(k+1)i) < f(g(k)), then set g(k+1) = x(k+1)i; else, set g(k+1) = g(k).
5. If stopping criterion satisfied, then stop.
6. Set k := k + 1, go to step 2.

In the algorithm, the parameter ω is referred to as an inertial constant. Recommended values are slightly less than 1. The parameters c1 and c2 are constants that determine how much the particle is directed toward "good" positions. They represent a "cognitive" and a "social" component, respectively, in that they affect how much the particle's personal best and the global best influence its movement. Recommended values are c1, c2 ≈ 2.
Variations
The PSO techniques have evolved since 1995. For example, recently Clerc [29] proposed a constriction-factor version of the algorithm, where the velocity is updated as

where the constriction coefficient κ is computed as

where ϕ = c1 + c2 and ϕ > 4. For example, for ϕ = 4.1, we have κ = 0.729. The role of the constriction coefficient is to speed up the convergence.
When using PSO in practice, one might wish to clamp the velocities to a certain maximum amount, say, vmax. In other words, we replace each component v of the velocity vector by

For an up-to-date literature survey and other modifications and heuristics, we recommend the first part of the proceedings of the 8th International Conference on Adaptive and Natural Computing Algorithms, held in April 2007 in Warsaw, Poland [5]. In these proceedings, one can find a number of papers dealing with applications of PSO to multiobjective optimization problems, versions of PSO for constrained optimization problems, as well as "niching" versions designed to find multiple solutions, that is, applications of PSO to multimodal optimization problems. For a mathematical analysis of the PSO algorithm, see Clerc and Kennedy [30].
14.5 Genetic Algorithms
Basic Description
A genetic algorithm is a randomized, population-based search technique that has its roots in the principles of genetics. The beginnings of genetic algorithms is credited to John Holland, who developed the basic ideas in the late 1960s and early 1970s. Since its conception, genetic algorithms have been used widely as a tool in computer programming and artificial intelligence (e.g., [61], [79], and [94]), optimization (e.g., [36], [67], and [127]), neural network training (e.g., [80]), and many other areas.
Suppose that we wish to solve an optimization problem of the form

(notice that the problem is a maximization, which is more convenient for describing genetic algorithms). The underlying idea of genetic algorithms applied to this problem is as follows. We start with an initial set of points in Ω, denoted P(0), called the initial population. We then evaluate the objective function at points in P(0). Based on this evaluation, we create a new set of points P(1). The creation of P(1) involves certain operations on points in P(0), called crossover and mutation, discussed later. We repeat the procedure iteratively, generating populations P(2), P(3), ..., until an appropriate stopping criterion is reached. The purpose of the crossover and mutation operations is to create a new population with an average objective function value that is higher than that of the previous population. To summarize, the genetic algorithm iteratively performs the operations of crossover and mutation on each population to produce a new population until a chosen stopping criterion is met.
The terminology used in describing genetic algorithms is adopted from genetics. To proceed with describing the details of the algorithm, we need the additional ideas and terms described below.
Chromosomes and Representation Schemes First, we point out that, in fact, genetic algorithms do not work directly with points in the set Ω, but rather, with an encoding of the points in Ω. Specifically, we need first to map Ω onto a set consisting of strings of symbols, all of equal length. These strings are called chromosomes. Each chromosome consists of elements from a chosen set of symbols, called the alphabet. For example, a common alphabet is the set {0,1}, in which case the chromosomes are simply binary strings. We denote by L the length of chromosomes (i.e., the number of symbols in the strings). To each chromosome there corresponds a value of the objective function, referred to as the fitness of the chromosome. For each chromosome x, we write f(x) for its fitness. Note that, for convenience, we use f to denote both the original objective function and the fitness measure on the set of chromosomes. We assume that f is a nonnegative function.
The choice of chromosome length, alphabet, and encoding (i.e., the mapping from Ω onto the set of chromosomes) is called the representation scheme for the problem. Identification of an appropriate representation scheme is the first step in using genetic algorithms to solve a given optimization problem.
Once a suitable representation scheme has been chosen, the next phase is to initialize the first population P(0) of chromosomes. This is usually done by a random selection of a set of chromosomes. After we form the initial population of chromosomes, we then apply the operations of crossover and mutation on the population. During each iteration k of the process, we evaluate the fitness f(x(k)) of each member x(k) of the population P(k). After the fitness of the entire population has been evaluated, we form a new population P(k + 1) in two stages.
Selection and Evolution In the first stage we apply an operation called selection, where we form a set M(k) with the same number of elements as P(k). This number is called the population size, which we denote by N. The set M(k), called the mating pool, is formed from P(k) using a random procedure as follows: Each point m(k) in M(k) is equal to x(k) in P(k) with probability

where

and the sum is taken over the whole of P(k). In other words, we select chromosomes into the mating pool with probabilities proportional to their fitness.
The selection scheme above is also called the roulette-wheel scheme, for the following reason. Imagine a roulette wheel in which each slot is assigned to a chromosome in P(k); some chromosomes may be assigned multiple slots. The number of slots associated with each chromosome is in proportion to its fitness. We then spin the roulette wheel and select [for inclusion in M(k)] the chromosome on whose slot the ball comes to rest. This procedure is repeated N times, so that the mating pool M(k) contains N chromosomes.
An alternative selection scheme is the tournament scheme, which proceeds as follows. First, we select a pair of chromosomes at random from P(k). We then compare the fitness values of these two chromosomes, and place the fitter of the two into M(k). We repeat this operation until the mating pool M(k) contains N chromosomes.
The second stage is called evolution: in this stage, we apply the crossover and mutation operations. The crossover operation takes a pair of chromosomes, called the parents, and gives a pair of offspring chromosomes. The operation involves exchanging substrings of the two parent chromosomes, described below. Pairs of parents for crossover are chosen from the mating pool randomly, such that the probability that a chromosome is chosen for crossover is pc. We assume that whether or not a given chromosome is chosen is independent of whether or not any other chromosome is chosen for crossover.
We can pick parents for crossover in several ways. For example, we may randomly choose two chromosomes from the mating pool as parents. In this case if N is the number of chromosomes in the mating pool, then pc = 2/N. Similarly, if we randomly pick 2k chromosomes from the mating pool (where k < N/2), forming k pairs of parents, we have pc = 2k/N. In the two examples above, the number of pairs of parents is fixed and the value of pc is dependent on this number. Yet another way of choosing parents is as follows: Given a value of pc, we pick a random number of pairs of parents such that the average number of pairs is pcN/2.
Once the parents for crossover have been determined, we apply the crossover operation to the parents. There are many types of possible crossover operations. The simplest crossover operation is the one-point crossover. In this operation, we first choose a number randomly between 1 and L − 1 according to a uniform distribution, where L is the length of chromosomes. We refer to this number as the crossing site. Crossover then involves exchanging substrings of the parents to the left of the crossing site, as illustrated in Figure 14.7 and in the following example.

Figure 14.7 Illustration of basic crossover operation.


Example 14.1 Suppose that we have chromosomes of length L = 6 over the binary alphabet {0,1}. Consider the pair of parents 000000 and 111111. Suppose that the crossing site is 4. Then, the crossover operation applied to the parent chromosomes yields the two offspring 000011 and 111100.
We can also have crossover operations with multiple crossing sites, as illustrated in Figure 14.8 and in the following example.

Figure 14.8 Illustration of two-point crossover operation.


Example 14.2 Consider two chromosomes, 000000000 and 111111111, of length L = 9. Suppose that we have two crossing sites, at 3 and 7. Then, the crossover operation applied to the parent chromosomes above yields the two offspring 000111100 and 111000011.
After the crossover operation, we replace the parents in the mating pool by their offspring. The mating pool has therefore been modified but maintains the same number of elements.
Next, we apply the mutation operation, which takes each chromosome from the mating pool and randomly changes each symbol of the chromosome with a given probability pm. In the case of the binary alphabet, this change corresponds to complementing the corresponding bits; that is, we replace each bit with probability pm from 0 to 1, or vice versa. If the alphabet contains more than two symbols, then the change involves randomly substituting the symbol with another symbol from the alphabet. Typically, the value of pm is very small (e.g., 0.01), so that only a few chromosomes will undergo a change due to mutation, and of those that are affected, only a few of the symbols are modified. Therefore, the mutation operation plays only a minor role in the genetic algorithm relative to the crossover operation.
After applying the crossover and mutation operations to the mating pool M(k), we obtain the new population P(k + 1). We then repeat the procedure of evaluation, selection, and evolution, iteratively. We summarize the genetic algorithm as follows.
Genetic Algorithm

1. Set k := 0. Generate an initial population P(0).
2. Evaluate P(k).
3. If the stopping criterion is satisfied, then stop.
4. Select M(k) from P(k).
5. Evolve M(k) to form P(k + 1).
6. Set k := k + 1, go to step 2.

A flowchart illustrating this algorithm is shown in Figure 14.9.

Figure 14.9 Flowchart for the genetic algorithm.


During execution of the genetic algorithm, we keep track of the best-so-far chromosome, that is, the chromosome with the highest fitness of all the chromosomes evaluated. After each iteration, the best-so-far chromosome serves as the candidate for the solution to the original problem. Indeed, we may even copy the best-so-far chromosome into each new population, a practice referred to as elitism. The elitist strategy may result in domination of the population by "superchromosomes." However, practical experience suggests that elitism often improves the performance of the algorithm.
The stopping criterion can be implemented in a number of ways. For example, a simple stopping criterion is to stop after a prespecified number of iterations. Another possible criterion is to stop when the fitness for the best-so-far chromosome does not change significantly from iteration to iteration.
The genetic algorithm differs from the algorithms discussed in previous chapters in several respects. First, it does not use derivatives of the objective function (like the other methods in this chapter). Second, it uses operations that are random within each iteration (like the other randomized search methods). Third, it searches from a set of points rather than a single point at each iteration (like the PSO algorithm). Fourth, it works with an encoding of the feasible set rather with than the set itself.
We illustrate an application of the genetic algorithm to an optimization problem in the following example.
Example 14.3 Consider the MATLAB "peaks" function f : 2 →  given by

(see also [67, pp. 178-180] for an example involving the same function). We wish to maximize f over the set Ω = {[x, y]  2: −3 ≤ x, y ≤ 3}. A plot of the objective function f over the feasible set Ω is shown in Figure 14.10. Using the MATLAB function fminunc (from the Optimization Toolbox), we found the optimal point to be [−0.0093,1.5814], with objective function value 8.1062.

Figure 14.10 Plot of f for Example 14.3.


To apply the genetic algorithm to solve the optimization problem above, we use a simple binary representation scheme with length L = 32, where the first 16 bits of each chromosome encode the x component, whereas the remaining 16 bits encode the y component. Recall that x and y take values in the interval [−3,3]. We first map the interval [−3,3] onto the interval [0, 216 - 1], via a simple translation and scaling. The integers in the interval [0,216 - 1] are then expressed as binary 16-bit strings. This defines the encoding of each component x and y. The chromosome is obtained by juxtaposing the two 8-bit strings. For example, the point [x,y] = [−1,3] is encoded as (see Exercise 14.4 for a simple algorithm for converting from decimal into binary)

Using a population size of 20, we apply 50 iterations of the genetic algorithm on the problem above. We used parameter values of pc = 0.75 and pm = 0.0075. Figure 14.11 shows plots of the best, average, and worst objective function values in the population for every iteration (generation) of the algorithm. The best-so-far solution obtained at the end of the 50 iterations is [0.0615, 1.5827], with objective function value 8.1013. Note that this solution and objective function value are very close to those obtained using MATLAB.
Analysis of Genetic Algorithms
In this section we use heuristic arguments to describe why genetic algorithms work. As pointed out before, the genetic algorithm was motivated by ideas from natural genetics [61]. Specifically, the notion of "survival of the fittest" plays a central role. The mechanisms used in the genetic algorithm mimic this principle. We start with a population of chromosomes, and selectively pick the fittest ones for reproduction. From these selected chromosomes, we form the new generation by combining information encoded in them. In this way, the goal is to ensure that the fittest members of the population survive and their information content is preserved and combined to produce even better offspring.

Figure 14.11 The best, average, and worst objective function values in the population for every iteration (generation) of the genetic algorithm in Example 14.3.


To further analyze the genetic algorithm in a more quantitative fashion, we need to define a few terms. For convenience, we only consider chromosomes over the binary alphabet. We introduce the notion of a schema (plural: schemata) as a set of chromosomes with certain common features. Specifically, a schema is a set of chromosomes that contain 1s and 0s in particular locations. We represent a schema using a string notation over an extended alphabet {0, 1, *}. For example, the notation 1 * 01 represents the schema

and the notation 0 * 101* represents the schema

In the schema notation, the numbers 0 and 1 denote the fixed binary values in the chromosomes that belong to the schema. The symbol *, meaning "don't care," matches either 0 or 1 at the positions it occupies. Thus, a schema describes a set of chromosomes that have certain specified similarities. A chromosome belongs to a particular schema if for all positions j = 1,..., L the symbol found in the jth position of the chromosome matches the symbol found in the jth position of the schema, with the understanding that any symbol matches *. Note that if a schema has r "don't care" symbols, then it contains 2r chromosomes. Moreover, any chromosome of length L belongs to 2L schemata.
Given a schema that represents good solutions to our optimization problem, we would like the number of matching chromosomes in the population P(k) to grow as k increases. This growth is affected by several factors, which we analyze in the following discussion. We assume throughout that we are using the roulette-wheel selection method.
The first key idea in explaining why the genetic algorithm works is the observation that if a schema has chromosomes with better-than-average fitness, then the expected (mean) number of chromosomes matching this schema in the mating pool M(k) is larger than the number of chromosomes matching this schema in the population P(k). To quantify this assertion, we need some additional notation. Let H be a given schema, and let e(H, k) be the number of chromosomes in P(k) that match H; that is, e(H, k) is the number of elements in the set P(k) ∩ H. Let f(H, k) be the average fitness of chromosomes in P(k) that match schema H. This means that if H ∩ P(k) = {x1,..., xe(H, k)}, then

Let N be the number of chromosomes in the population and F(k) be the sum of the fitness values of chromosomes in P(k), as before. Denote by (k) the average fitness of chromosomes in the population; that is,

Finally, let m(H, k) be the number of chromosomes in M(k) that match H, in other words, the number of elements in the set M(k) ∩ H.
Lemma 14.1 Let H be a given schema and (H,k) be the expected value of m(H,k) given P(k). Then,

Proof. Let P(k) ∩ H = {x1,..., xe(H,k)}. In the remainder of the proof, the term expected should be taken to mean "expected, given P(k)." For each element m(k)  M(k) and each i = 1,..., e(H, k), the probability that m(k) = xi is given by f(xi)/F(k). Thus, the expected number of chromosomes in M(k) equal to xi is

Hence, the expected number of chromosomes in P(k) ∩ H that are selected into M(k) is

Because any chromosome in M(k) is also a chromosome in P(k), the chromosomes in M(k) ∩ H are simply those in P(k) ∩ H that are selected into M(k). Hence,

Lemma 14.1 quantifies our assertion that if a schema H has chromosomes with better than average fitness [i.e., f(H,k)/(k) > 1], then the expected number of chromosomes matching H in the mating pool M(k) is larger than the number of chromosomes matching H in the population P(k).
We now analyze the effect of the evolution operations on the chromosomes in the mating pool. For this, we need to introduce two parameters that are useful in the characterization of a schema: order and length. The order o(S) of a schema S is the number of fixed symbols (non* symbols) in its representation (the notation o(S) is standard in the literature on genetic algorithms, and should not be confused with the "little-oh" symbol defined in Section 5.6). If the length of chromosomes in S is L, then o(S) is L minus the number of * symbols in S. For example,

whereas

The length l(S) of a schema S is the distance between the first and last fixed symbols (i.e., the difference between the positions of the rightmost fixed symbol and the leftmost fixed symbol). For example,

Note that for a schema S with chromosomes of length L, the order o(S) is a number between 0 and L and the length l(S) is a number between 0 in L − 1. The order of a schema with all * symbols is 0; its length is also 0. The order of a schema containing only a single element (i.e., its representation has no * symbols) is L [e.g., o(1011) = 4 − 0 = 4]. The length of a schema with fixed symbols in its first and last positions is L − 1 [e.g., l(0 * *1) = 4 − 1 = 3].
We first consider the effect of the crossover operation on the mating pool. The basic observation in the following lemma is that given a chromosome in M(k) ∩ H, the probability that it leaves H after crossover is bounded above by a quantity that is proportional to pc and l(H).
Lemma 14.2 Given a chromosome in M(k) ∩ H, the probability that it is chosen for crossover and neither of its offspring is in H is bounded above by

Proof. Consider a given chromosome in M(k) ∩ H. The probability that it is chosen for crossover is pc. If neither of its offspring is in H, then the crossover point must be between the corresponding first and last fixed symbols of H. The probability of this is l(H)/(L − 1). Hence, the probability that the given chromosome is chosen for crossover and neither of its offspring is in H is bounded above by

From Lemma 14.2 we conclude that given a chromosome in M(k) ∩ H, the probability either that it is not selected for crossover or that at least one of its offspring is in H after the crossover operation, is bounded below by

Note that if a chromosome in H is chosen for crossover and the other parent chromosome is also in H, then both offspring are automatically in H (see Exercise 14.5). Hence, for each chromosome in M(k) ∩ H, there is a certain probability that it will result in an associated chromosome in H (either itself or one of its offspring) after going through crossover (including selection for crossover) and that probability is bounded below by the foregoing expression.
We next consider the effect of the mutation operation on the mating pool M(k).
Lemma 14.3 Given a chromosome in M(k) ∩ H, the probability that it remains in H after the mutation operation is given by

Proof. Given a chromosome in M(k) ∩ H, it remains in H after the mutation operation if and only if none of the symbols in this chromosome that correspond to fixed symbols in H are changed by the mutation operation. The probability of this event is (1 − pm)o(H).
Note that if pm is small, the expression (1 − pm)o(H) above is approximately equal to

The following theorem combines the results of the preceding lemmas.
Theorem 14.1 Let H be a given schema and ε(H, k + 1) be the expected value of e(H, k + 1) given P(k). Then,

Proof. Consider a given chromosome in M(k) ∩ H. If, after the evolution operations, it has a resulting chromosome that is in H, then that chromosome is in P(k + 1) ∩ H. By Lemmas 14.2 and 14.3, the probability of this event is bounded below by

Therefore, because each chromosome in M(k) ∩ H results in a chromosome in P(k + 1) ∩ H with a probability bounded below by the expression above, the expected value of e(H, k + 1) given M(k) is bounded below by

Taking the expectation given P(k), we get

Finally, using Lemma 14.1, we arrive at the desired result.
Theorem 14.1 indicates how the number of chromosomes in a given schema changes from one population to the next. Three factors influence this change, reflected by the three terms on the right-hand side of inequality in Theorem 14.1: 1 − pcl(H)/(L − 1), (1 − pm)o(H), and f(H,k)/(k). Note that the larger the values of these terms, the higher the expected number of matches of the schema H in the next population. The effect of each term is summarized as follows:

 The term f(H, k)/(k) reflects the role of average fitness of the given schema H—the higher the average fitness, the higher the expected number of matches in the next population.
 The term 1 − pcl(H)/(L − 1) reflects the effect of crossover—the smaller the term pcl(H)/(L − 1), the higher the expected number of matches in the next population.
 The term (1 − pm)o(H) reflects the effect of mutation—the larger the term, the higher the expected number of matches in the next population.

In summary, we see that a schema that is short, low order, and has above-average fitness will have on average an increasing number of its representatives in the population from iteration to iteration. Observe that the encoding is relevant to the performance of the algorithm. Specifically, a good encoding is one that results in high-fitness schemata having small lengths and orders.
Real-Number Genetic Algorithms
The genetic algorithms described thus far operate on binary strings, representing elements of the feasible set Ω. (For this reason, genetic algorithms are also suitably applied to combinatorial optimization problems, where Ω is not n but some discrete set.) Binary encodings allow us to use the schema theory, described in the preceding section, to analyze genetic algorithms. However, there are some disadvantages to operating on binary strings. To see this, let g : {0, 1}L → Ω represent the binary "decoding" function; that is, if x is a binary chromosome, g(x)  Ω is the point in the feasible set Ω ⊂ n whose encoding is x. Therefore, the objective function being maximized by the genetic algorithm is not f itself but rather the composition of f and the decoding function g. In other words, the optimization problem being solved by the genetic algorithm is

This optimization problem may be more complex than the original optimization problem. For example, it may have extra maximizers, making the search for a global maximizer more difficult.
The above motivates a consideration of genetic algorithms that operate directly on the original optimization problem. In other words, we wish to implement a genetic algorithm that operates directly on n. The steps of this algorithm will be the same as before (see Figure 14.9), except that the elements of the population are points in the feasible set Ω rather than binary strings. We will need to define appropriate crossover and mutation operations for this case.
For crossover, we have several options. The simplest is to use averaging: For a pair of parents x and y, the offspring is z = (x + y)/2 (this type of crossover operation is used, e.g., in [103]). This offspring can then replace one of the parents. Alternatively, we may produce two offspring as follows: z1 = (x + y)/2 + w1 and z2 = (x + y)/2 + w2, where w1 and w2 are two randomly generated vectors (with zero mean). If either offspring lies outside Ω, we have to bring the offspring back into Ω, using, for example, a projection (see Section 23.2). A third option for crossover is to take random convex combinations of the parents. Specifically, given a pair of parents x and y, we generate a random number α  (0, 1) and then produce two offspring z1 = αx + (1 − α)y and z2 = (1 − α)x + αy. This method of crossover ensures that the offspring are always in the feasible set, provided that the feasible set is convex. A fourth option is to perturb the two points above by some random amount: z = αx + (1 − α)y + w1 and z2 = (1 − α)x + αy + w2, where w1 and w2 are two randomly generated vectors (with zero mean). In this case we have to check for feasibility of the offspring and use projections if needed.
For mutation, a simple implementation is to add a random vector to the chromosome. Specifically, given a chromosome x, we produce its mutation as x′ = x + w, where w is a random vector with zero mean. This mutation operation is also called a real number creep (see, e.g., [103]). As before, we have to ensure that the mutated chromosome is feasible. If not, we may use a projection. An alternative method for mutation is to replace the chosen chromosome with a random convex combination of the chromosome with a random point in the feasible set; that is, we generate a random number α  (0, 1) and a random point w  Ω, and set x′ = αx + (1 − α)w. Provided that the feasible set is convex, the mutated chromosom will always be feasible.
Example 14.4 Consider again the function f : 2 →  from Example 14.3. We apply a real-number genetic algorithm to find a maximizer of f using a crossover operation of the fourth type described above and a mutation operation of the second type above. With a population size of 20, we apply 50 iterations of the genetic algorithm. As before, we used parameter values of pc = 0.75 and pm = 0.0075. Figure 14.12 shows plots of the best, average, and worst objective function values in the population for every iteration (generation) of the algorithm. The best-so-far solution obtained at the end of the 50 iterations is [−0.0096, 1.5845], with objective function value 8.1061, which is close to the result of Example 14.3.

Figure 14.12 The best, average, and worst objective function values in the population for every iteration (generation) of the real-number genetic algorithm in Example 14.4.


EXERCISES

14.1 Write a MATLAB program to implement the Nelder-Mead algorithm applied to minimizing the function

on Ω = {x  2 : x1, x2  [−1, 1]}. Locate the iteration points on the level sets of f. Connect the successive points with lines to show clearly the progression of the optimization process. Test your program with two starting points:

14.2 Write MATLAB programs to implement naive random search and simulated annealing. Use the neighborhood

where α > 0 is prespecified, and pick z(k) to be uniformly distributed on N(x(k)). Test both algorithms on maximizing the MATLAB "peaks" function given in Example 14.3. Observe the effect of varying α.
14.3 Write a MATLAB program to implement a particle swarm optimization algorithm. Test your implementation on maximizing the MATLAB "peaks" function given in Example 14.3.
14.4 This problem has four parts and is related to binary encoding for genetic algorithms.
a. Let (I)10 be the decimal representation for a given integer, and let amam−1 ··· a0 be its binary representation; that is, each ai is either 0 or 1, and

Verify that the following is true:

b. The second expression in part a suggests a simple algorithm for converting from decimal representation to equivalent binary representation, as follows. Dividing both sides of the expression in part a by 2, the remainder is a0. Subsequent divisions by 2 yield the remaining bits a1, a2,..., am as remainders.
Use this algorithm to find the binary representation of the integer (I)10 = 1995.
c. Let (F)10 be the decimal representation for a given number in [0, 1], and let 0.a−1a−2 ··· be its binary representation, that is,

If this expression is multiplied by 2, the integer part of the product is a−1. Subsequent multiplications yield the remaining bits a−2, a−3,.... As in part b, the above gives a simple algorithm for converting from a decimal fraction to its binary representations. Use this algorithm to find the binary representation of (F)10 = 0.7265625.
Note that we can combine the algorithms from parts b and c to convert an arbitrary positive decimal representation into its equivalent binary representation. Specifically, we apply the algorithms in parts b and c separately to the integer and fraction parts of the given decimal number, respectively.
d. The procedure in part c may yield an infinitely long binary representation. If this is the case, then we need to determine the number of bits required to keep at least the same accuracy as the given decimal number. If we have a d-digit decimal fraction, then the number of bits b in the binary representation must satisfy 2−b ≤ 10−d, which yields b ≥ 3.32d. Convert 19.95 to its equivalent binary representation with at least the same degree of accuracy (i.e., to two decimal places).
14.5 Given two chromosomes in a schema H, suppose that we swap some (or all) of the symbols between them at corresponding positions. Show that the resulting two chromosomes are also in H. Prom this fact we conclude that given two chromosomes in H, both offspring after the crossover operation are also in H. In other words, the crossover operation preserves membership in H.
14.6 Consider a two-point crossover scheme (see Example 14.2), described as follows. Given a pair of binary chromosomes of length L, we independently choose two random numbers, uniform over 1,...,L − 1. We call the two numbers c1 and c2, where c1 ≤ c2. If c1 − c2, we do not swap any symbols (i.e., leave the two given parent chromosomes unchanged). If c1 < c2, we interchange the (c1 + 1)th through c2th bits in the given parent chromosomes.
   Prove the analog of Lemma 14.2 for this case, given below.
Lemma: Given a chromosome in M(k) ∩ H, the probability that it is chosen for crossover and neither of its offspring is in H is bounded above by

Hint: Note that the two-point crossover operation is equivalent to a composition of two one-point crossover operations (i.e., doing two one-point crossover operations in succession).
14.7 State and prove the analog of Lemma 14.2 for an n-point crossover operation.
Hint: See Exercise 14.6.
14.8 Implement the roulette-wheel selection scheme using MATLAB.
Hint: Use the MATLAB functions sum, cumsum, and find.
14.9 Implement the crossover operation (one-point) using the MATLAB, assuming that we are given two binary parent chromosomes.
14.10 Implement the mutation operation using the MATLAB function xor, assuming that the chromosomes in the mating pool are binary vectors.
14.11 Write a MATLAB program to implement a genetic algorithm using binary encoding. Test your implementation on the following functions:
a. f(x) = −15 sin2(2x) − (x − 2)2 + 160, |x| < 10.
b.  (considered in Example 14.3).
14.12 Write a MATLAB program to implement a real-number genetic algorithm. Test your implementation on the function f(x) = x1 sin(x1) + x2 sin(5x2) with the constraint set Ω = {x : 0 ≤ x1 ≤ 10, 4 ≤ x2 ≤ 6}.








PART III
LINEAR PROGRAMMING







CHAPTER 15
INTRODUCTION TO LINEAR PROGRAMMING
15.1 Brief History of Linear Programming
The goal of linear programming is to determine the values of decision variables that maximize or minimize a linear objective function, where the decision variables are subject to linear constraints. A linear programming problem is a special case of a general constrained optimization problem. In the general setting, the goal is to find a point that minimizes the objective function and at the same time satisfies the constraints. We refer to any point that satisfies the constraints as a feasible point. In a linear programming problem, the objective function is linear, and the set of feasible points is determined by a set of linear equations and/or inequalities.
In this part we study methods for solving linear programming problems. Linear programming methods provide a way of choosing the best feasible point among the many possible feasible points. In general, the number of feasible points is infinitely large. However, as we shall see, the solution to a linear programming problem can be found by searching through a particular finite number of feasible points, known as basic feasible solutions. Therefore, in principle, we can solve a linear programming problem simply by comparing the finite number of basic feasible solutions and finding one that minimizes or maximizes the objective function—we refer to this approach as the brute-force approach. For most practical decision problems, even this finite number of basic feasible solutions is so large that the method of choosing the best solution by comparing them to each other is impractical. To get a feel for the amount of computation needed in a brute-force approach, consider the following example. Suppose that we have a small factory with 20 different machines producing 20 different parts. Assume that any of the machines can produce any part. We also assume that the time for producing each part on each machine is known. The problem then is to assign a part to each machine so that the overall production time is minimized. We see that there are 20! (20 factorial) possible assignments. The brute-force approach to solving this assignment problem would involve writing down all the possible assignments and then choosing the best one by comparing them. Suppose that we have at our disposal a computer that takes 1 μs (10−6 second) to determine each assignment. Then, to find the best (optimal) assignment this computer would need 77,147 years (working 24 hours a day, 365 days a year) to find the best solution. An alternative approach to solving this problem is to use experienced planners to optimize this assignment problem. Such an approach relies on heuristics. Heuristics come close, but give suboptimal answers. Heuristics that do reasonably well, with an error of, say, 10%, may still not be good enough. For example, in a business that operates on large volumes and a small profit margin, a 10% error could mean the difference between loss and profit.
Efficient methods for solving linear programming problems became available in the late 1930s. In 1939, Kantorovich presented a number of solutions to some problems related to production and transportation planning. During World War II, Koopmans contributed significantly to the solution of transportation problems. Kantorovich and Koopmans were awarded a Nobel Prize in Economics in 1975 for their work on the theory of optimal allocation of resources. In 1947, Dantzig developed a new method for solving linear programs, known today as the simplex method (see [34] for Dantzig's own treatment of the algorithm). In the following chapters we discuss the simplex method in detail. The simplex method is efficient and elegant and has been declared one of the 10 algorithms with the greatest influence on the development and practice of science and engineering in the twentieth century [40].
The simplex method has the undesirable property that in the worst case, the number of steps (and hence total time) required to find a solution grows exponentially with the number of variables. Thus, the simplex method is said to have exponential worst-case complexity. This led to an interest in devising algorithms for solving linear programs that have polynomial complexity—algorithms that find a solution in an amount of time that is bounded by a polynomial in the number of variables. Khachiyan, in 1979, was the first to devise such an algorithm. However, his algorithm gained more theoretical than practical interest. Then, in 1984, Karmarkar proposed a new linear programming algorithm that has polynomial complexity and appears to solve some complicated real-world problems of scheduling, routing, and planning more efficiently than the simplex method. Karmarkar's work led to the development of many other nonsimplex methods commonly referred to as interior-point methods. This approach is currently still an active research area. For more details on Karmarkar's and related algorithms, see [42], [55], [71], [119], and [124]. Some basic ideas illustrating Khachiyan's and Karmarkar's algorithms are presented in Chapter 18.
15.2 Simple Examples of Linear Programs
Formally, a linear program is an optimization problem of the form

where c  n, b  m, and A  m × n. The vector inequality x ≥ 0 means that each component of x is nonnegative. Several variations of this problem are possible; for example, instead of minimizing, we can maximize, or the constraints may be in the form of inequalities, such as Ax ≥ b or Ax ≤ b. We also refer to these variations as linear programs. In fact, as we shall see later, these variations can all be rewritten into the standard form shown above.
The purpose of this section is to give some simple examples of linear programming problems illustrating the importance and the various applications of linear programming methods.
Example 15.1 This example is adapted from [123]. A manufacturer produces four different products: X1, X2, X3, and X4. There are three inputs to this production process: labor in person-weeks, kilograms of raw material A, and boxes of raw material B. Each product has different input requirements. In determining each week's production schedule, the manufacturer cannot use more than the available amounts of labor and the two raw materials. The relevant information is presented in Table 15.1. Every production decision must satisfy the restrictions on the availability of inputs. These constraints can be written using the data in Table 15.1. In particular, we have
Table 15.1 Data for Example 15.1


Because negative production levels are not meaningful, we must impose the following nonnegativity constraints on the production levels:

Now, suppose that one unit of product X1 sells for $6, and X2, X3, and X4 sell for $4, $7, and $5, respectively. Then, the total revenue for any production decision (x1, x2, x3, x4) is

The problem is then to maximize f subject to the given constraints (the three inequalities and four nonnegativity constraints). Using vector notation with

the problem can be written in the compact form

where

Another example that illustrates linear programming involves determining the most economical diet that satisfies the basic minimum requirements for good health.
Example 15.2 Diet Problem. This example is adapted from [88]. Assume that n different food types are available. The jth food sells at a price cj per unit. In addition, there are m basic nutrients. To achieve a balanced diet, you must receive at least bi units of the ith nutrient per day. Assume that each unit of food j contains aij units of the ith nutrient. Denote by xj the number of units of food j in the diet. The objective is to select the xj to minimize the total cost of the diet:

subject to the nutritional constraints

and the nonnegativity constraints

In the more compact vector notation, this problem becomes

where x = [x1, x2,..., xn] is an n-dimensional column vector, c is an n-dimensional row vector, A is an m × n matrix, and b is an m-dimensional column vector. We call this problem the diet problem and will return to it in Chapter 17.
In the next example we consider a linear programming problem that arises in manufacturing.
Example 15.3 A manufacturer produces two different products, X1 and X2, using three machines: M1, M2, and M3. Each machine can be used for only a limited amount of time. Production times of each product on each machine are given in Table 15.2. The objective is to maximize the combined time of utilization of all three machines.
Table 15.2 Data for Example 15.3

Every production decision must satisfy the constraints on the available time. These restrictions can be written down using data from Table 15.2. In particular, we have

where x1 and x2 denote the production levels. The combined production time of all three machines is

Thus, writing x = [x1, x2], the problem in compact notation has the form

where

In the following example we discuss an application of linear programming in transportation.
Example 15.4 A manufacturing company has plants in cities A, B, and C. The company produces and distributes its product to dealers in various cities. On a particular day, the company has 30 units of its product in A, 40 in B, and 30 in C. The company plans to ship 20 units to D, 20 to E, 25 to F, and 35 to G, following orders received from dealers. The transportation costs per unit of each product between the cities are given in Table 15.3. In the table, the quantities supplied and demanded appear at the right and along the bottom of the table. The quantities to be transported from the plants to different destinations are represented by the decision variables.
Table 15.3 Data for Example 15.4

This problem can be stated in the form

In this problem one of the constraint equations is redundant because it can be derived from the rest of the constraint equations. The mathematical formulation of the transportation problem is then in a linear programming form with twelve (3 × 4) decision variables and six (3 + 4 − 1) linearly independent constraint equations. Obviously, we also require nonnegativity of the decision variables, since a negative shipment is impossible and does not have a valid interpretation.
Next, we give an example of a linear programming problem arising in electrical engineering.
Example 15.5 This example is adapted from [100]. Figure 15.1 shows an electric circuit that is designed to use a 30-V source to charge 10-V, 6-V, and 20-V batteries connected in parallel. Physical constraints limit the currents I1, I2, I3, I4, and I5 to a maximum of 4 A, 3 A, 3 A, 2 A, and 2 A, respectively. In addition, the batteries must not be discharged; that is, the currents I1, I2, I3, I4, and I5 must not be negative. We wish to find the values of the currents I1,..., I5 such that the total power transferred to the batteries is maximized.

Figure 15.1 Battery charger circuit for Example 15.5.


The total power transferred to the batteries is the sum of the powers transferred to each battery and is given by 10I2 + 6I4 + 20I5 W. From the circuit in Figure 15.1, we observe that the currents satisfy the constraints I1 = I2 + I3 and I3 = I4 + I5. Therefore, the problem can be posed as the following linear program:

Finally, we present an example from wireless communications.
Example 15.6 Consider the wireless communication system shown in Figure 15.2. There are n "mobile" users. For each i = 1,..., n, user i transmits a signal to the base station with power pi and an attenuation factor of hi (i.e., the actual signal power received at the base station from user i is hipi). When the base station is receiving from user i, the total power received from all other users is considered interference (i.e., the interference for user i is Σj≠i hjpj). For the communication with user i to be reliable, the signal-to-interference ratio must exceed a threshold γi, where the "signal" is the power received from user i.

Figure 15.2 Wireless communication system in Example 15.6.


We are interested in minimizing the total power transmitted by all users subject to having reliable communications for all users. We can formulate the problem as a linear programming problem of the form

We proceed as follows. The total power transmitted is p1 + · · · + pn. The signal-to-interference ratio for user i is

Hence, the problem can be written as

We can write the above as the linear programming problem

In matrix form, we have

For more examples of linear programming and their applications in a variety of engineering problems, we refer the reader to [1], [34], [35], [46], and [109]. For applications of linear programming to the design of control systems, see [33]. Linear programming also provides the basis for theoretical applications, as, for example, in matrix game theory (discussed in [18]).
15.3 Two-Dimensional Linear Programs
Many fundamental concepts of linear programming are easily illustrated in two-dimensional space. Therefore, we consider linear problems in 2 before discussing general linear programming problems.
Consider the following linear program (adapted from [123]):

where x = [x1, x2] and

First, we note that the set of equations {c x = x1 + 5x2 = f, f  } specifies a family of straight lines in 2. Each member of this family can be obtained by setting f equal to some real number. Thus, for example, x1 + 5x2 = −5, x1 + 5x2 = 0, and x1 + 5x2 = 3 are three parallel lines belonging to the family. Now, suppose that we try to choose several values for x1 and x2 and observe how large we can make f while still satisfying the constraints on x1 and x2. We first try x1 = 1 and x2 = 3. This point satisfies the constraints. For this point, f = 16. If we now select x1 = 0 and x2 = 5, then f = 25 and this point yields a larger value for f than does x = [1, 3]. There are infinitely many points [x1, x2] satisfying the constraints. Therefore, we need a better method than trial and error to solve the problem. In the following sections we develop a systematic approach that simplifies considerably the process of solving linear programming problems.
For the example above we can easily solve the problem using geometric arguments. First let us sketch the constraints in 2. The region of feasible points (the set of points x satisfying the constraints Ax ≤ b, x ≥ 0) is depicted by the shaded region in Figure 15.3.

Figure 15.3 Geometric solution of a linear program in 2.


Geometrically, maximizing c x = x1+ 5x2 subject to the constraints can be thought of as finding the straight line f = x1 + 5x2 that intersects the shaded region and has the largest f. The coordinates of the point of intersection will then yield a maximum value of c x. In our example, the point [0,5] is the solution (see Figure 15.3).
Example 15.7 Suppose that you are given two different types of concrete. The first type contains 30% cement, 40% gravel, and 30% sand (all percentages of weight). The second type contains 10% cement, 20% gravel, and 70% sand. The first type of concrete costs $5 per pound and the second type costs $1 per pound. How many pounds of each type of concrete should you buy and mix together so that your cost is minimized but you get a concrete mixture that has at least a total of 5 pounds of cement, 3 pounds of gravel, and 4 pounds of sand?
The problem can be represented as

where

Using the graphical method described above, we get a solution of [0,50], which means that we should purchase 50 pounds of the second type of concrete. (For a variation of this problem solved using a different method, see Example 12.1.)
In some cases, when using the graphical method, there may be more than one point of intersection of the optimal straight line f = c x with the boundary of the feasible region. In this case all of the intersection points will yield the same value for the objective function c x, and therefore any one of them is a solution.
15.4 Convex Polyhedra and Linear Programming
The goal of linear programming is to minimize (or maximize) a linear objective function

subject to constraints that are represented by linear equalities and/or inequalities. For the time being, let us consider only constraints of the form Ax ≤ b, x, ≥ 0. In this section we discuss linear programs from a geometric point of view (for a review of geometric concepts used in the section, see Chapter 4). The set of points satisfying these constraints can be represented as the intersection of a finite number of closed half-spaces. Thus, the constraints define a convex polytope. We assume, for simplicity, that this polytope is nonempty and bounded. In other words, the equations of constraints define a polyhedron M in n. Let H be a hyperplane of support of this polyhedron. If the dimension of M is less than n, then the set of all points common to the hyperplane H and the polyhedron M coincides with M. If the dimension of M is equal to n, then the set of all points common to the hyperplane H and the polyhedron M is a face of the polyhedron. If this face is (n − 1)-dimensional, then there exists only one hyperplane of support, namely, the carrier of this face. If the dimension of the face is less than n − 1, then there exist an infinite number of hyperplanes of support whose intersection with this polyhedron yields this face (see Figure 15.4).

Figure 15.4 Hyperplanes of support at different boundary points of the polyhedron M.


The goal of our linear programming problem is to maximize a linear objective function f(x) = c x = c1x1 + · · · + cn xn on the convex polyhedron M. Next, let H be the hyperplane defined by the equation

Draw a hyperplane of support  to the polyhedron M, which is parallel to H and positioned such that the vector c points in the direction of the half-space that does not contain M (see Figure 15.5). The equation of the hyperplane  has the form

Figure 15.5 Maximization of a linear function on the polyhedron M.



and for all x  M we have c x ≤ β. Denote by  the convex polyhedron that is the intersection of the hyperplane of support  with the polyhedron M. We now show that f is constant on  and that  is the set of all points in M for which f attains its maximum value. To this end, let y and z be two arbitrary points in . This implies that both y and z belong to . Hence,

which means that f is constant on .
Let y be a point of , and let x be a point of M\; that is, x is a point of M that does not belong to  (see Figure 15.5). Then,

which implies that

Thus, the values of f at the points of M that do not belong to  are smaller than the values at points of . Hence, f achieves its maximum on M at points in .
It may happen that  contains only a single point, in which case f achieves its maximum at a unique point. This occurs when the the hyperplane of support passes through an extreme point of M (see Figure 15.6).

Figure 15.6 Unique maximum point of f on the polyhedron M.


15.5 Standard Form Linear Programs
We refer to a linear program of the form

as a linear program in standard form. Here A is an m × n matrix composed of real entries, m < n, rank A = m. Without loss of generality, we assume that d ≥ 0. If a component of b is negative, say the ith. component, we multiply the ith constraint by −1 to obtain a positive right-hand side.
Theorems and solution techniques for linear programs are usually stated for problems in standard form. Other forms of linear programs can be converted to the standard form, as we now show. If a linear program is in the form

then by introducing surplus variables yi, we can convert the original problem into the standard form

In more compact notation, the formulation above can be represented as

where Im is the m × m identity matrix.
If, on the other hand, the constraints have the form

then we introduce slack variables yi to convert the constraints into the form

where y is the vector of slack variables. Note that neither surplus nor slack variables contribute to the objective function cx.
At first glance, it may appear that the two problems

and

are different, in that the first problem refers to the intersection of half-spaces in the n-dimensional space, whereas the second problem refers to an intersection of half-spaces and hyperplanes in the (n + m)-dimensional space. It turns out that both formulations are algebraically equivalent in the sense that a solution to one of the problems implies a solution to the other. To illustrate this equivalence, we consider the following examples.
Example 15.8 Suppose that we are given the inequality constraint

We convert this to an equality constraint by introducing a slack variable x2 ≥ 0 to obtain

Consider the sets C1 = {x1 : x1 ≤ 7} and C2 = {x : x + x2 = 7, x2 ≥ 0}. Are the sets C1 and C2 equal? It is clear that indeed they are; in this example, we give a geometric interpretation for their equality. Consider a third set C3 = {[x1, x2] : x1 + x2 = 7, x2 ≥ 0}. Prom Figure 15.7 we can see that the set C3 consists of all points on the line to the left and above the point of intersection of the line with the x1-axis. This set, being a subset of 2, is of course not the same set as the set C1 (a subset of ). However, we can project the set C3 onto the x1-axis (see Figure 15.7). We can associate with each point x1  C1 a point [x1, 0] on the orthogonal projection of C3 onto the x1-axis, and vice versa. Note that C2 = {x1 : [x1, x2]  C3} = C1.

Figure 15.7 Projection of the set C3 onto the x1-axis.


Example 15.9 Consider the inequality constraints

where a1, a2, and b are positive numbers. Again, we introduce a slack variable x3 ≥ 0 to get

Define the sets

We again see that C3 is not the same as C1. However, the orthogonal projection of C3 onto the (x1, x2)-plane allows us to associate the resulting set with the set C1. We associate the points [x1, x2, 0] resulting from the orthogonal projection of C3 onto the (x1, x2)-plane with the points in C1 (see Figure 15.8). Note that C2 = {[x1, x2] : [x1, x2, x3]  C3} = C1.

Figure 15.8 Projection of the set C3 onto the (x1, x2)-plane.


Example 15.10 Suppose that we wish to maximize

subject to the constraints

where, for simplicity, we assume that each aij > 0 and b1, b2 ≥ 0. The set of feasible points is depicted in Figure 15.9. Let C1 ⊂ 2 be the set of points satisfying the constraints.

Figure 15.9 The feasible set for Example 15.10.


Introducing a slack variable, we convert the constraints into standard form:

Let C2 ⊂ 3 be the set of points satisfying the constraints. As illustrated in Figure 15.10, this set is a line segment (in 3). We now project C2 onto the (x1, x2)-plane. The projected set consists of the points [x1, x2, 0], with [x1, x2, x3]  C2 for some x3 ≥ 0. In Figure 15.10 this set is marked by a heavy line in the (x1, x2)-plane. We can associate the points on the projection with the corresponding points in the set C1.

Figure 15.10 Projection of C2 onto the (x1, x2)-plane.


In the following example we convert an optimization problem into a standard form linear programming problem.
Example 15.11 Consider the following optimization problem

To convert the problem into a standard form linear programming problem, we perform the following steps:

1. Change the objective function to: minimize x1 − x′2.
2. Substitute x1 = −x′1.
3. Write |x2| ≤ 2 as x2 ≤ 2 and −x2 ≤ 2.
4. Introduce slack variables x3 and x4, and convert the inequalities above to x2 + x3 = 2 and −x2 + x4 = 2.
5. Write x2 = u − v, u,v ≥ 0.

Hence, we obtain

15.6 Basic Solutions
We have seen in Section 15.5 that any linear programming problem involving inequalities can be converted to standard form, that is, a problem involving linear equations with nonnegative variables:

where c  n, A  mxn, b  m, m < n, rank A = m, and b ≥ 0. In the following discussion we only consider linear programming problems in standard form.
Consider the system of equalities

where rank A − m. In dealing with this system of equations, we frequently need to consider a subset of columns of the matrix A. For convenience, we often reorder the columns of A so that the columns we are interested in appear first. Specifically, let B be a square matrix whose columns are m linearly independent columns of A. If necessary, we reorder the columns of A so that the columns in B appear first: A has the form A = [B,D], where D is an m × (n − m) matrix whose columns are the remaining columns of A. The matrix B is nonsingular, and thus we can solve the equation

for the m-vector xB. The solution is xB = B−1b. Let x be the n-vector whose first m components are equal to xB and the remaining components are equal to zero; that is, x = [xB, 0]. Then, x is a solution to Ax = b.
Definition 15.1 We call [xB, 0] a basic solution to Ax = b with respect to the basis B. We refer to the components of the vector xB as basic variables and the columns of B as basic columns.
If some of the basic variables of a basic solution are zero, then the basic solution is said to be a degenerate basic solution.
A vector x satisfying Ax = b, x ≥ 0, is said to be a feasible solution.
A feasible solution that is also basic is called a basic feasible solution.
If the basic feasible solution is a degenerate basic solution, then it is called a degenerate basic feasible solution.
Note that in any basic feasible solution, xB ≥ 0.
Example 15.12 Consider the equation Ax = b with

where ai denotes the ith column of the matrix A.
Then, x = [6, 2, 0, 0] is a basic feasible solution with respect to the basis B = [a1, a2], x = [0, 0, 0, 2] is a degenerate basic feasible solution with respect to the basis B = [a3, a4] (as well as [a1, a4] and [a2, a4]), x = [3,1,0,1] is a feasible solution that is not basic, and x = [0, 2, −6,0] is a basic solution with respect to the basis B = [a2, a3], but is not feasible.
Example 15.13 As another example, consider the system of linear equations Ax = b, where

We now find all solutions of this system. Note that every solution x of Ax = b has the form x = v + h, where v is a particular solution of Ax = b and h is a solution to Ax − 0.
We form the augmented matrix [A, b] of the system:

Using elementary row operations, we transform this matrix into the form (see Chapter 16) given by

The corresponding system of equations is given by

Solving for the leading unknowns x1 and x2, we obtain

where x3 and x4 are arbitrary real numbers. If [x1, x2, x3, x4] is a solution, then we have

where we have substituted s and t for x3 and x4, respectively, to indicate that they are arbitrary real numbers.
Using vector notation, we may write the system of equations above as

Note that we have infinitely many solutions, parameterized by s, t  . For the choice s = t = 0 we obtain a particular solution to Ax = b, given by

Any other solution has the form v + h, where

The total number of possible basic solutions is at most

To find basic solutions that are feasible, we check each of the basic solutions for feasibility.
Our first candidate for a basic feasible solution is obtained by setting x3 = x4 = 0, which corresponds to the basis B = [a1, a2]. Solving BxB = b, we obtain xB = [14/5,-11/5], and hence x = [14/5,-11/5, 0, 0]′ is a basic solution that is not feasible.
For our second candidate basic feasible solution, we set x2 = x4 = 0. We have the basis B = [a1, a3]. Solving BxB = b yields xB = [4/3, 11/3]. Hence, x = [4/3, 0, 11/3, 0] is a basic feasible solution.
A third candidate basic feasible solution is obtained by setting x2 = x3 = 0. However, the matrix

is singular. Therefore, B cannot be a basis, and we do not have a basic solution corresponding to B = [a1, a4.]
We get our fourth candidate for a basic feasible solution by setting x1 = x4= 0. We have a basis B = [a2, a3], resulting in x = [0,2,7,0], which is a basic feasible solution.
Our fifth candidate for a basic feasible solution corresponds to setting x1 = x3 = 0, with the basis B = [a2, a4]. This results in x = [0, −11/5, 0, −28/5], which is a basic solution that is not feasible.
Finally, the sixth candidate for a basic feasible solution is obtained by setting x1 = x2 = 0. This results in the basis B = [a3, a4], and x = [0,0,11/3, −8/3], which is a basic solution but is not feasible.
15.7 Properties of Basic Solutions
In this section we discuss the importance of basic feasible solutions in solving linear programming (LP) problems. We first prove the fundamental theorem of LP, which states that when solving an LP problem, we need only consider basic feasible solutions. This is because the optimal value (if it exists) is always achieved at a basic feasible solution. We need the following definitions.
Definition 15.2 Any vector x that yields the minimum value of the objective function cx over the set of vectors satisfying the constraints Ax = b, x ≥ 0, is said to be an optimal feasible solution.
An optimal feasible solution that is basic is said to be an optimal basic feasible solution.
Theorem 15.1 Fundamental Theorem of LP. Consider a linear program in standard form.

1. If there exists a feasible solution, then there exists a basic feasible solution.
2. If there exists an optimal feasible solution, then there exists an optimal basic feasible solution.

Proof. We first prove part 1. Suppose that x = [x1, ..., xn] a feasible solution and it has p positive components. Without loss of generality, we can assume that the first p components are positive, whereas the remaining components are zero. Then, in terms of the columns of A = [a1, ..., ap, ..., an], this solution satisfies

There are now two cases to consider.
Case 1: If a1, a2, ..., ap are linearly independent, then p ≤ m. If p = m, then the solution x is basic and the proof is done. If, on the other hand, p < m, then, since rank A = m, we can find m − p columns of A from the remaining n − p columns so that the resulting set of m columns forms a basis. Hence, the solution a? is a (degenerate) basic feasible solution corresponding to the basis above.
Case 2: Assume that a1, a2, ..., ap are linearly dependent. Then, there exist numbers yi, i = 1, ..., p, not all zero, such that

We can assume that there exists at least one yi that is positive, for if all the yi are nonpositive, we can multiply the equation above by −1. Multiply the equation by a scalar ε and subtract the resulting equation from x1a1 + x2a2 + ... + xpap = b to obtain

Let

Then, for any ε we can write

Let ε = min{xi/yi : i = 1, ..., p, yi > 0}. Then, the first p components of x − εy are nonnegative, and at least one of these components is zero. We then have a feasible solution with at most p − 1 positive components. We can repeat this process until we get linearly independent columns of A, after which we are back to case 1. Therefore, part 1 is proved.
We now prove part 2. Suppose that x = [x1, ..., xn] is an optimal feasible solution and only the first p variables are nonzero. Then, we have two cases to consider. The first case is exactly the same as in part 1. The second case follows the same arguments as in part 1, but in addition we must show that x − εy is optimal for any ε. We do this by showing that cy = 0. To this end, assume that cy ≠ 0. Note that for ε of sufficiently small magnitude (|ε| ≤ min{|xi/yi| : i = 1, ..., p, yi ≠ 0}), the vector x − εy is feasible. We can choose ε such that cx > cx − εcy = c(x − εy). This contradicts the optimality of x. We can now use the procedure from part 1 to obtain an optimal basic feasible solution from a given optimal feasible solution.
Example 15.14 Consider the system of equations given in Example 15.13. Find a nonbasic feasible solution to this system and use the method in the proof of the fundamental theorem of LP to find a basic feasible solution.
Recall that solutions for the system given in Example 15.13 have the form

where s, t  . Note that if s = 4 and t = 0, then

is a nonbasic feasible solution.
There are constants yi i = 1,2,3, such that

For example, let

Note that

where

If ε = 1/3, then

is a basic feasible solution.
Observe that the fundamental theorem of LP reduces the task of solving a linear programming problem to that of searching over a finite number of basic feasible solutions. That is, we need only check basic feasible solutions for optimality. As mentioned before, the total number of basic solutions is at most

Although this number is finite, it may be quite large. For example, if m − 5 and n = 50, then

This is potentially the number of basic feasible solutions to be checked for optimality. Therefore, a more efficient method of solving linear programs is needed. To this end, in the next section we analyze a geometric interpretation of the fundamental theorem of LP. This leads us to the simplex method for solving linear programs, which we discuss in Chapter 16.
15.8 Geometric View of Linear Programs
Recall that a set Θ ⊂ n is said to be convex if, for every x, y  Θ and every real number α, 0 < α < 1, the point αx + (1 − α)y  Θ. In other words, a set is convex if given two points in the set, every point on the line segment joining these two points is also a member of the set.
Note that the set of points satisfying the constraints

is convex. To see this, let x1 and x2 satisfy the constraints, that is, Axi = b, xi ≥ 0, i = 1,2. Then, for all α  (0,1), A(αx1 + (1 − α)x2) = αAx1 + (1 −α)Ax2 = b. Also, for α  (0,1), we have αx1 + (1 − α)x2 ≥ 0.
Recall that a point x in a convex set Θ is said to be an extreme point of Θ if there are no two distinct points x1 and x2 in Θ such that x = αx1 + (1 − α)x2 for some α  (0,1). In other words, an extreme point is a point that does not lie strictly within the line segment connecting two other points of the set. Therefore, if x is an extreme point, and x = αx1 + (1 − α)x2 for some x1, x2  Θ and α  (0,1), then x = x2. In the following theorem we show that extreme points of the constraint set are equivalent to basic feasible solutions.
Theorem 15.2 Let Ω be the convex set consisting of all feasible solutions, that is, all n-vectors × satisfying

where A  mxn, m < n. Then, x is an extreme point of Ω, if and only if x is a basic feasible solution to Ax − b, x ≥ 0.
Proof ⇒: Suppose that x satisfies Ax = b, x ≥ 0, and has p positive components. As before, without loss of generality, we can assume that the first p components are positive and the remaining components are zero. We have

Let yi, i = 1, ..., p, be numbers such that

We show that each yi = 0. To begin, multiply this equation by ε > 0, then add and subtract the result from the equation x1a1 + x2a2 + · · · + xpap = b to get

Because each xi > 0, ε > 0 can be chosen such that each xi + εyi, xi − εyi ≥ 0 (e.g., ε = min{|xi/yi : i = 1, ..., p, yi ≠ 0}). For such a choice of ε, the vectors

belong to Ω. Observe that . Because x is an extreme point, z1 = z2. Hence, each yi = 0, which implies that the ai are linearly independent.
⇐: Let x  Ω be a basic feasible solution. Let y, z  Ω be such that

for some α  (0,1). We show that y = z and conclude that x is an extreme point. Because y, z ≥ 0, and the last n − m components of x are zero, the last n − m components of y and z are zero as well. Furthermore, since Ay = Az = b,

and

Combining these two equations yields

Because the columns a1, ..., am are linearly independent, we have yi = zi, i = 1, ..., m. Therefore, y = z, and hence x is an extreme point of Ω.
From Theorem 15.2 it follows that the set of extreme points of the constraint set Ω = {x : Ax = b, x ≥ 0} is equal to the set of basic feasible solutions to Ax = b, x ≥ 0. Combining this observation with the fundamental theorem of LP (Theorem 15.1), we can see that in solving linear programming problems we need only examine the extreme points of the constraint set.
Example 15.15 Consider the following LP problem:

We introduce slack variables x3, x4, x5 to convert this LP problem into standard form:

In the remainder of the example we consider only the problem in standard form. We can represent the constraints above as

that is, x1a1 + x2a2 + x3a3 + x4a4 + x5a5 = b, x ≥ 0. Note that

is a feasible solution. But for this x, the value of the objective function is zero. We already know that the minimum of the objective function (if it exists) is achieved at an extreme point of the constraint set Ω defined by the constraints. The point [0, 0, 40, 20, 12] is an extreme point of the set of feasible solutions, but it turns out that it does not minimize the objective function. Therefore, we need to seek the solution among the other extreme points. To do this we move from one extreme point to an adjacent extreme point such that the value of the objective function decreases. Here, we define two extreme points to be adjacent if the corresponding basic columns differ by only one vector. We begin with x = [0, 0, 40, 20, 12]. We have

To select an adjacent extreme point, let us choose to include a1 as a basic column in the new basis. We need to remove either a3, a4, or a5 from the old basis. We proceed as follows. We first express a1 as a linear combination of the old basic columns:

Multiplying both sides of this equation by ε1 > 0, we get

We now add this equation to the equation 0a1 + 0a2 + 40a3 + 20a4 + 12a5 = b. Collecting terms yields

We want to choose ε1 in such a way that each of the coefficients above is nonnegative and at the same time, one of the coefficients a3, a4, or a5 becomes zero. Clearly, ε1 = 10 does the job. The result is

The corresponding basic feasible solution (extreme point) is

For this solution, the objective function value is −30, which is an improvement relative to the objective function value at the old extreme point.
We now apply the same procedure as above to move to another adjacent extreme point, which hopefully further decreases the value of the objective function. This time, we choose a2 to enter the new basis. We have

and

Substituting ε2 = 4, we obtain

The solution is [8, 4, 12, 0, 0] and the corresponding value of the objective function is −44, which is smaller than the value at the previous extreme point. To complete the example we repeat the procedure once more. This time, we select a4 and express it as a combination of the vectors in the previous basis, a1, a2, and a3:

and hence

The largest permissible value for ε3 is 3. The corresponding basic feasible solution is [5, 7, 0, 3, 0], with an objective function value of −50. The solution [5, 7, 0, 3, 0] turns out to be an optimal solution to our problem in standard form. Hence, the solution to the original problem is [5, 7], which we can easily obtain graphically (see Figure 15.11).

Figure 15.11 Graphical solution to the LP problem in Example 15.15.


The technique used in this example for moving from one extreme point to an adjacent extreme point is also used in the simplex method for solving LP problems. The simplex method is essentially a refined method of performing these manipulations.
EXERCISES

15.1 Convert the following linear programming problem to standard form:

15.2 Consider a discrete-time linear system xk+1 = axk+buk, where uk is the input at time k, xk is the output at time k, and a, b   are system parameters. Given an initial condition x0 = 1, consider the problem of minimizing the output x2 at time 2 subject to the constraint that |ui| ≤ 1, i = 0, 1.
   Formulate the problem as a linear programming problem, and convert it into standard form.
15.3 Consider the optimization problem

where ci ≠ 0, i = 1, ..., n. Convert this problem into an equivalent standard form linear programming problem.
Hint: Given any x  , we can find unique numbers x+, x−  , x+, x− ≥ 0, such that |x| = x+ + x− and x = x+ − x−.
15.4 Does every linear programming problem in standard form have a nonempty feasible set? If "yes," provide a proof. If "no," give a specific example.
   Does every linear programming problem in standard form (assuming a nonempty feasible set) have an optimal solution? If "yes," provide a proof. If "no," give a specific example.
15.5 Suppose that a computer supplier has two warehouses, one located in city A and another in city B. The supplier receives orders from two customers, one in city C and another in city D. The customer in city C orders 50 units, and the customer in city D orders 60 units. The number of units at the warehouse in city A is 70, and the number of units at the warehouse in city B is 80. The cost of shipping each unit from A to C is 1, from A to D is 2, from B to C is 3, and from B to D is 4.
   Formulate the problem of deciding how many units from each warehouse should be shipped to each customer to minimize the total shipping cost (assuming that the values of units to be shipped are real numbers). Express the problem as an equivalent standard form linear programming problem.
15.6 Consider a computer network consisting of six computers, A through F. The computers are connected according to the following links, with maximum data rates (in Mbps) shown: AC (10), BC (7), BF (3), CD (8), DE (12), DF (4). For example, "AC (10)" means that computers A and C are connected with a link that supports data rates up to 10 Mbps.
   Suppose that A and B need to send data to E and F, respectively (no other communication is taking place in the network). Any path through the given links above may be used as long as the path has no loop. Also, multiple paths (say from A to E) can be used simultaneously. Link bandwidth can be shared as long as the total data rate through the link does not exceed its maximum (the total data rate through a link is the sum of the data rates of communication in both directions).
   For every Mbps of data rate the network can support for transmission from A to E, we receive 2 dollars. For every Mbps of data rate the network can support for transmission from B to F, we receive 3 dollars. Formulate a linear programming problem to represent the goal of maximizing the total revenue. Then, convert this problem into standard form.
   Hint: Draw a picture of the network, then label each link with the maximum data rate and the paths that share that link.
15.7 A cereal manufacturer wishes to produce 1000 pounds of a cereal that contains exactly 10% fiber, 2% fat, and 5% sugar (by weight). The cereal is to be produced by combining four items of raw food material in appropriate proportions. These four items have certain combinations of fiber, fat, and sugar content, and are available at various prices per pound:

The manufacturer wishes to find the amounts of each item to be used to produce the cereal in the least expensive way. Formulate the problem as a linear programming problem. What can you say about the existence of a solution to this problem?
15.8 Suppose that a wireless broadcast system has n transmitters. Transmitter j broadcasts at a power of pj ≥ 0. There are m locations where the broadcast is to be received. The path gain from transmitter j to location i is gi,j; that is, the power of the signal transmitted from transmitter j received at location i is gi,jpj. The total power received at location i is the sum of the powers received from all the transmitters. Formulate the problem of finding the minimum sum of the powers transmitted subject to the requirement that the power received at each location is at least P.
15.9 Consider the system of equations

Check if the system has basic solutions. If yes, find all basic solutions.
15.10 Solve the following linear program graphically:

15.11 The optimization toolbox in MATLAB provides a function, linprog, for solving linear programming problems. Use the function linprog to solve the problem in Example 15.5. Use the initial condition 0.








CHAPTER 16
SIMPLEX METHOD
16.1 Solving Linear Equations Using Row Operations
The examples in previous chapters illustrate that solving linear programs involves the solution of systems of linear simultaneous algebraic equations. In this section we describe a method for solving a system of n linear equations in n unknowns that we use in subsequent sections. The method uses elementary row operations and corresponding elementary matrices. For a discussion of numerical issues involved in solving a system of simultaneous linear algebraic equations, we refer the reader to [41] and [53].
An elementary row operation on a given matrix is an algebraic manipulation of the matrix that corresponds to one of the following:

1. Interchanging any two rows of the matrix
2. Multiplying one of its rows by a real nonzero number
3. Adding a scalar multiple of one row to another row

An elementary row operation on a matrix is equivalent to premultiplying the matrix by a corresponding elementary matrix, which we define next.
Definition 16.1 We call E an elementary matrix of the first kind if E is obtained from the identity matrix I by interchanging any two of its rows.
An elementary matrix of the first kind formed from I by interchanging the ith and the jth rows has the form

Note that E is invertible and E = E−1.
Definition 16.2 We call E an elementary matrix of the second kind if E is obtained from the identity matrix I by multiplying one of its rows by a real number α ≠ 0.
The elementary matrix of the second kind formed from I by multiplying the ith row by α ≠ 0 has the form

Note that E is invertible and

Definition 16.3 We call E an elementary matrix of the third kind if E is obtained from the identity matrix I by adding β times one row to another row of I.
An elementary matrix of the third kind obtained from I by adding β times the jth row to the ith. row has the form

Observe that E is the identity matrix with an extra β in the (i, j)th location. Note that E is invertible and

Definition 16.4 An elementary row operation (of first, second, or third kind) on a given matrix is a premultiplication of the given matrix by a corresponding elementary matrix of the respective kind.
Because elementary matrices are invertible, we can define the corresponding inverse elementary row operations.
Consider a system of n linear equations in n unknowns x1,x2, ..., xn with right-hand sides b1, bn ...,. In matrix form this system may be written as

where

If A is invertible, then

Thus, the problem of solving the system of equations Ax = b, with A  n × n invertible, is related to the problem of computing A−1. We now show that A−1 can be computed effectively using elementary row operations. In particular, we prove the following theorem.
Theorem 16.1 Let A  n × n be a given matrix. Then, A is nonsingular (invertible) if and only if there exist elementary matrices Ei, i = 1, ..., t, such that

Proof ⇒: If A is nonsingular, then its first column must have at least one nonzero element, say aj1 ≠ 0. Premultiplying A by an elementary matrix of the first kind of the form

brings the nonzero element aj1 to the location (1, 1). Hence, in the matrix E1 A, the element a11 ≠ 0. Note that since E1 is nonsingular, E1A is also nonsingular.
Next, we premultiply E1 A by an elementary matrix of the second kind of the form

The result of this operation is the matrix E2E1A with unity in the location (1,1). We next apply a sequence of elementary row operations of the third kind on the matrix E2E1A. Specifically, we premultiply E2E1A by n − 1 elementary matrices of the form

where r = 2+n − 1 = n + 1. The result of these operations is the nonsingular matrix

Because the matrix Er ··· E1 A is nonsingular, its submatrix

must also be nonsingular. This implies that there is a nonzero element j2, where 2 ≤ j ≤ n. Using an elementary operation of the first kind, we bring this element to the location (2, 2). Thus, in the matrix

the (2, 2)th element is nonzero. Premultiplying the matrix by an elementary matrix of the second kind yields the matrix

in which the element in the location (2,2) is unity. As before, we premultiply this matrix by n − 1 elementary row operations of the third kind, to get a matrix of the form

where s = r + 2 + n − 1 = 2(n + 1). This matrix is nonsingular. Hence, there is a nonzero element j3, 3 ≤ j ≤ n. Proceeding in a similar fashion as before, we obtain

where t = n(n + 1).
⇐: If there exist elementary matrices E1, ..., Et such that

then clearly A is invertible, with

Theorem 16.1 suggests the following procedure for finding A−1, if it exists. We first form the matrix

We then apply elementary row operations to [A, I] so that A is transformed into I; that is, we obtain

It then follows that

Example 16.1 Let

Find A−1.
We form the matrix

and perform row operations on this matrix. Applying row operations of the first and third kinds yields

We then interchange the second and fourth rows and apply elementary row operations of the second and third kinds to get

Now multiply the third row by 1/2 and then perform a sequence of elementary operations of the third kind to obtain

Hence,

We now return to the general problem of solving the system of equations Ax = b, A  n × n. If A−1 exists, then the solution is x = A−1b. However, we do not need an explicit expression for A−1 to find the solution. Indeed, let A−1 be expressed as a product of elementary matrices

Thus,

and hence

The discussion above leads to the following procedure for solving the system Ax = b. Form an augmented matrix

Then, perform a sequence of row elementary operations on this augmented matrix until we obtain

From the above we have that if x is a solution to Ax = b, then it is also a solution to EAx = Eb, where E = Et ··· E1 represents a sequence of elementary row operations. Because EA = I, and Eb = β, it follows that x = β is the solution to Ax = b, A  n × n invertible.
Suppose now that A  m × n where m < n, and rank A = m. Then, A is not a square matrix. Clearly, in this case the system of equations Ax = b has infinitely many solutions. Without loss of generality, we can assume that the first m columns of A are linearly independent. Then, if we perform a sequence of elementary row operations on the augmented matrix [A, b] as before, we obtain

where D is an m × (n − m) matrix. Let x  n be a solution to Ax = b and write , where xB  m, xD  (n-m). Then, [I, D]x = , which we can rewrite as xB + DxD = , or xB =  − DxD. Note that for an arbitrary xD  (n-m, if xB =  − DxD, then the resulting vector  is a solution to Ax = b. In particular,  is a solution to Ax = b. We often refer to the basic solution  as a particular solution to Ax = b. Note that  is a solution to Ax = 0. Any solution to Ax = b has the form

for some xD  (n-m).
16.2 The Canonical Augmented Matrix
Consider the system of simultaneous linear equations Ax = b, rank A = m. Using a sequence of elementary row operations and reordering the variables if necessary, we transform the system Ax = b into the following canonical form:

This can be represented in matrix notation as

Formally, we define the canonical form as follows.
Definition 16.5 A system Ax = b is said to be in canonical form if among the n variables there are m variables with the property that each appears in only one equation, and its coefficient in that equation is unity.
A system is in canonical form if by some reordering of the equations and the variables it takes the form [Im, Ym,n-m]x = y0. If a system of equations Ax = b is not in canonical form, we can transform the system into canonical form by a sequence of elementary row operations. The system in canonical form has the same solution as the original system Ax = b and is called the canonical representation of the system with respect to the basis a1,..., am. There are, in general, many canonical representations of a given system, depending on which columns of A we transform into the columns of Im (i.e., basic columns). We call the augmented matrix [Im, Ym,n-m, y0] of the canonical representation of a given system the canonical augmented matrix of the system with respect to the basis a1,..., am. Of course, there may be many canonical augmented matrices of a given system, depending on which columns of A are chosen as basic columns.
The variables corresponding to basic columns in a canonical representation of a given system are the basic variables, whereas the other variables are the nonbasic variables. In particular, in the canonical representation [Im, Ym,n-m] x = y0 of a given system, the variables x1,..., xm are the basic variables and the other variables are the nonbasic variables. Note that in general the basic variables need not be the first m variables. However, in the following discussion we assume, for convenience and without loss of generality, that the basic variables are indeed the first m variables in the system. Having done so, the corresponding basic solution is

that is,

Given a system of equations Ax = b, consider the associated canonical augmented matrix

From the arguments above we conclude that

In other words, the entries in the last column of the canonical augmented matrix are the coordinates of the vector b with respect to the basis {a1,..., am}. The entries of all the other columns of the canonical augmented matrix have a similar interpretation. Specifically, the entries of the jth column of the canonical augmented matrix, j = 1,...,n, are the coordinates of aj with respect to the basis {a1,..., am}. To see this, note that the first m columns of the augmented matrix form a basis (the standard basis). Every other vector in the augmented matrix can be expressed as a linear combination of these basis vectors by reading the coefficients down the corresponding column. Specifically, let a′i, i = 1,...,n + 1, be the ith column in the augmented matrix above. Clearly, since a1′,..., a′m form the standard basis, then for m < j ≤ n,

Let ai, i = 1,...,n, be the ith column of A, and an+1 = b. Now, a′i = Eai, i = 1,...,n + 1, where E is a nonsingular matrix that represents the elementary row operations needed to transform [A, b] into [Im,Ym,n-m,y0]. Therefore, for m < j ≤ n, we also have

16.3 Updating the Augmented Matrix
To summarize Section 16.2, the canonical augmented matrix of a given system Ax = b specifies the representations of the columns aj, m < j ≤ n, in terms of the basic columns a1,..., am. Thus, the elements of the jth column of the canonical augmented matrix are the coordinates of the vector aj with respect to the basis a1,..., am. The coordinates of b are given in the last column.
Suppose that we are given the canonical representation of a system Ax = b. We now consider the following question: If we replace a basic variable by a nonbasic variable, what is the new canonical representation corresponding to the new set of basic variables? Specifically, suppose that we wish to replace the basis vector ap, 1 ≤ p ≤ m, by the vector aq, m < q ≤ n. Provided that the first m vectors with ap replaced by aq are linearly independent, these vectors constitute a basis and every vector can be expressed as a linear combination of the new basic columns.
Let us now find the coordinates of the vectors a1,..., an with respect to the new basis. These coordinates form the entries of the canonical augmented matrix of the system with respect to the new basis. In terms of the old basis, we can express aq as

Note that the set of vectors {a1,..., ap−1, aq, ap+1,..., am} is linearly independent if and only if ypq ≠ 0. Solving the equation above for ap, we get

Recall that in terms of the old augmented matrix, any vector aj, m < j ≤ n, can be expressed as

Combining the last two equations yields

Denoting the entries of the new augmented matrix by y'ij, we obtain

Therefore, the entries of the new canonical augmented matrix can be obtained from the entries of the old canonical augmented matrix via the formulas above. These equations are often called the pivot equations, and ypq, the pivot element.
We refer to the operation on a given matrix by the formulas above as pivoting about the (p,q)th element. Note that pivoting about the (p,q)th element results in a matrix whose qth column has all zero entries, except the (p,q)th entry, which is unity. The pivoting operation can be accomplished via a sequence of elementary row operations, as was done in the proof of Theorem 16.1.
16.4 The Simplex Algorithm
The essence of the simplex algorithm is to move from one basic feasible solution to another until an optimal basic feasible solution is found. The canonical augmented matrix discussed in Section 16.3 plays a central role in the simplex algorithm.
Suppose that we are given the basic feasible solution

or equivalently

In Section 16.3 we saw how to update the canonical augmented matrix if we wish to replace a basic column by a nonbasic column, that is, if we wish to change from one basis to another by replacing a single basic column. The values of the basic variables in a basic solution corresponding to a given basis are given in the last column of the canonical augmented matrix with respect to that basis; that is, xi = yi0, i = 1,..., m. Basic solutions are not necessarily feasible—the values of the basic variables may be negative. In the simplex method we want to move from one basic feasible solution to another. This means that we want to change basic columns in such a way that the last column of the canonical augmented matrix remains nonnegative. In this section we discuss a systematic method for doing this.
In the remainder of this chapter we assume that every basic feasible solution of

is a nondegenerate basic feasible solution. We make this assumption primarily for convenience—all arguments can be extended to include degeneracy.
Let us start with the basic columns a1,..., am, and assume that the corresponding basic solution x = [y10,..., ym0, 0,...,0] is feasible; that is, the entries yi0, i = 1,..., m, in the last column of the canonical augmented matrix are positive. Suppose that we now decide to make the vector aq, q > m, a basic column. We first represent aq in terms of the current basis as

Multiplying the above by ε > 0 yields

We combine this equation with

to get

Note that the vector

where ε appears in the qth position, is a solution to Ax = b. If ε = 0, then we obtain the old basic feasible solution. As ε is increased from zero, the qth component of the vector above increases. All other entries of this vector will increase or decrease linearly as ε is increased, depending on whether the corresponding yiq is negative or positive. For small enough ε, we have a feasible but nonbasic solution. If any of the components decreases as ε increases, we choose ε to be the smallest value where one (or more) of the components vanishes. That is,

With this choice of ε we have a new basic feasible solution, with the vector aq replacing ap, where p corresponds to the minimizing index p = arg mini {yi0/yiq : yiq > 0}. So, we now have a new basis a1,..., ap-1, ap+1,..., am, aq. As we can see, ap was replaced by aq in the new basis. We say that aq enters the basis and ap leaves the basis. If the minimum in mini{yi0/yiq : yiq > 0} is achieved by more than a single index, then the new solution is degenerate and any of the zero components can be regarded as the component corresponding to the basic column that leaves the basis. If none of the yiq are positive, then all components in the vector [y10 − εy1q,..., ym0 − εymq, 0,..., ε,..., 0] increase (or remain constant) as ε is increased, and no new basic feasible solution is obtained, no matter how large we make ε. In this case there are feasible solutions having arbitrarily large components, which means that the set Ω of feasible solutions is unbounded.
So far, we have discussed how to change from one basis to another, while preserving feasibility of the corresponding basic solution, assuming that we have already chosen a nonbasic column to enter the basis. To complete our development of the simplex method, we need to consider two more issues. The first issue concerns the choice of which nonbasic column should enter the basis. The second issue is to find a stopping criterion, that is, a way to determine if a basic feasible solution is optimal or is not. To this end, suppose that we have found a basic feasible solution. The main idea of the simplex method is to move from one basic feasible solution (extreme point of the set Ω) to another basic feasible solution at which the value of the objective function is smaller. Because there is only a finite number of extreme points of the feasible set, the optimal point will be reached after a finite number of steps.
We already know how to move from one extreme point of the set Ω to a neighboring one by updating the canonical augmented matrix. To see which neighboring solution we should move to and when to stop moving, consider the following basic feasible solution:

together with the corresponding canonical augmented matrix, having an identity matrix appearing in the first m columns. The value of the objective function for any solution x is

For our basic solution, the value of the objective function is

where

To see how the value of the objective function changes when we move from one basic feasible solution to another, suppose that we choose the qth column, m < q ≤ n, to enter the basis. To update the canonical augmented matrix, let p = arg mini{yi0/yiq : yiq > 0} and ε = yp0/ypq. The new basic feasible solution is

Note that the single ε appears in the qth component, whereas the pth component is zero. Observe that we could have arrived at the basic feasible solution above simply by updating the canonical augmented matrix using the pivot equations from the previous Section 16.3:

where the qth column enters the basis and the pth column leaves [i.e., we pivot about the (p, q)th element]. The values of the basic variables are entries in the last column of the updated canonical augmented matrix.
The cost for this new basic feasible solution is

where z0 = c1y10 + ···+ cmym0. Let

Then,

Thus, if

then the objective function value at the new basic feasible solution above is smaller than the objective function value at the original solution (i.e., z < z0). Therefore, if cq − zq < 0, then the new basic feasible solution with aq entering the basis has a lower objective function value.
On the other hand, if the given basic feasible solution is such that for all q = m + 1,...,n,

then we can show that this solution is in fact an optimal solution. To show this, recall from Section 16.1 that any solution to Ax = b can be represented as

for some x0 = [xm+1,...,xn]  (n-m). Using manipulations similar to the above, we obtain

where zi = c1y1i + ··· + cmymi, i = m + 1,...,n. For a feasible solution we have xi ≥ 0, i = 1,..., n. Therefore, if ci − zi = 0 for all i = m + 1,..., n, then any feasible solution x will have objective function value cx no smaller than z0.
Let ri − 0 for i = 1,..., m and ri = ci − zi for i = m + 1,..., n. We call ri the ith reduced cost coefficient or relative cost coefficient. Note that the reduced cost coefficients corresponding to basic variables are zero.
We summarize the discussion above with the following result.
Theorem 16.2 A basic feasible solution is optimal if and only if the corresponding reduced cost coefficients are all nonnegative.
At this point we have all the necessary steps for the simplex algorithm.
Simplex Algorithm

1. Form a canonical augmented matrix corresponding to an initial basic feasible solution.
2. Calculate the reduced cost coefficients corresponding to the nonbasic variables.
3. If rj ≥ 0 for all j, stop—the current basic feasible solution is optimal.
4. Select a q such that rq < 0.
5. If no yiq > 0, stop—the problem is unbounded; else, calculate p = arg mini{yi0/yiq : yiq > 0}. (If more than one index i minimizes yi0/yiq, we let p be the smallest such index.)
6. Update the canonical augmented matrix by pivoting about the (p, q)th element.
7. Go to step 2.

We state the following result for the simplex algorithm, which we have already proved in the foregoing discussion.
Theorem 16.3 Suppose that we have an LP problem in standard form that has an optimal feasible solution. If the simplex method applied to this problem terminates and the reduced cost coefficients in the last step are all nonnegative, then the resulting basic feasible solution is optimal.
Example 16.2 Consider the following linear program (see also Exercise 15.10):

We solve this problem using the simplex method.
Introducing slack variables, we transform the problem into standard form:

The starting canonical augmented matrix for this problem is

Observe that the columns forming the identity matrix in the canonical augmented matrix above do not appear at the beginning. We could rearrange the augmented matrix so that the identity matrix would appear first. However, this is not essential from the computational point of view.
The starting basic feasible solution to the problem in standard form is

The columns a3, a4, and a5 corresponding to x3, x4, and x5 are basic, and they form the identity matrix. The basis matrix is B = [a3,a4,a5] = I3.
The value of the objective function corresponding to this basic feasible solution is z = 0. We next compute the reduced cost coefficients corresponding to the nonbasic variables x1 and x2. They are

We would like now to move to an adjacent basic feasible solution for which the objective function value is lower. Naturally, if there is more than one such solution, it is desirable to move to the adjacent basic feasible solution with the lowest objective value. A common practice is to select the most negative value of rj and then to bring the corresponding column into the basis (see Exercise 16.18 for an alternative rule for choosing the column to bring into the basis). In our example, we bring a2 into the basis; that is, we choose a2 as the new basic column. We then compute p = arg min{yi0/yi2 : yi2 > 0} = 2. We now update the canonical augmented matrix by pivoting about the (2,2)th entry using the pivot equations:

The resulting updated canonical augmented matrix is

Note that a2 entered the basis and a4 left the basis. The corresponding basic feasible solution is x = [0,6,4,0,2]. We now compute the reduced cost coefficients for the nonbasic columns:

Because r1 = −2 < 0, the current solution is not optimal, and a lower objective function value can be obtained by bringing a1 into the basis. Proceeding to update the canonical augmented matrix by pivoting about the (3, 1)th element, we obtain

The corresponding basic feasible solution is x = [2,6,2,0,0]. The reduced cost coefficients are

Because no reduced cost coefficient is negative, the current basic feasible solution x = [2,6,2,0,0] is optimal. The solution to the original problem is therefore x1 = 2, x2 = 6, and the objective function value is 34.
We can see from Example 16.2 that we can solve a linear programming problem of any size using the simplex algorithm. To make the calculations in the algorithm more efficient, we discuss the matrix form of the simplex method in the next section.
16.5 Matrix Form of the Simplex Method
Consider a linear programming problem in standard form:

Let the first m columns of A be the basic columns. The columns form a square m × m nonsingular matrix B. The nonbasic columns of A form an m × (n-m) matrix D. We partition the cost vector correspondingly as c = [cB, cD]. Then, the original linear program can be represented as follows:

If xD = 0, then the solution  is the basic feasible solution corresponding to the basis B. It is clear that for this to be a solution, we need xB = B−1b; that is, the basic feasible solution is

The corresponding objective function value is

If, on the other hand, xD ≠ 0, then the solution  is not basic. In this case xB is given by

and the corresponding objective function value is

Defining

we obtain

The elements of the vector rD are the reduced cost coefficients corresponding to the nonbasic variables.
If rD ≥ 0, then the basic feasible solution corresponding to the basis B is optimal. If, on the other hand, a component of rD is negative, then the value of the objective function can be reduced by increasing a corresponding component of xD, that is, by changing the basis.
We now use the foregoing observations to develop a matrix form of the simplex method. To this end we first add the cost coefficient vector c to the bottom of the augmented matrix [A, b] as follows:

We refer to this matrix as the tableau of the given LP problem. The tableau contains all relevant information about the linear program.
Suppose that we now apply elementary row operations to the tableau such that the top part of the tableau corresponding to the augmented matrix [A,b] is transformed into canonical form. This corresponds to premultiplying the tableau by the matrix

The result of this operation is

We now apply elementary row operations to the tableau above so that the entries of the last row corresponding to the basic columns become zero. Specifically, this corresponds to premultiplication of the tableau by the matrix

The result is

We refer to the resulting tableau as the canonical tableau corresponding to the basis B. Note that the first m entries of the last column of the canonical tableau, B−1b, are the values of the basic variables corresponding to the basis B. The entries of  in the last row are the reduced cost coefficients. The last element in the last row of the tableau, , is the negative of the value of the objective function corresponding to the basic feasible solution.
Given an LP problem, we can in general construct many different canonical tableaus, depending on which columns are basic. Suppose that we have a canonical tableau corresponding to a particular basis. Consider the task of computing the tableau corresponding to another basis that differs from the previous basis by a single vector. This can be accomplished by applying elementary row operations to the tableau in a similar fashion as discussed above. We refer to this operation as updating the canonical tableau. Note that updating of the tableau involves using exactly the same update equations as we used before in updating the canonical augmented matrix, namely, for i = 1,...,m + 1,

where yij and y'ij are the (i,j)th entries of the original and updated canonical tableaus, respectively.
Working with the tableau is a convenient way of implementing the simplex algorithm, since updating the tableau immediately gives us the values of both the basic variables and the reduced cost coefficients. In addition, the (negative of the) value of the objective function can be found in the lower right-hand corner of the tableau. We illustrate the use of the tableau in the following example.
Example 16.3 Consider the following linear programming problem:

We first transform the problem into standard form so that the simplex method can be applied. To do this we change the maximization to minimization by multiplying the objective function by −1. We then introduce two nonnegative slack variables, x3 and x4, and construct the tableau for the problem:

Notice that this tableau is already in canonical form with respect to the basis [a3,a4]. Hence, the last row contains the reduced cost coefficients, and the rightmost column contains the values of the basic variables. Because r1 = −7 is the most negative reduced cost coefficient, we bring a1 into the basis. We then compute the ratios y10/y11 = 3/2 and y20/y21 = 4. Because y10/y11 < y20/y21, we get p = arg mini{yi0/yi1 : yi1 > 0} = 1. We pivot about the (1, 1)th element of the tableau to obtain

In the second tableau above, only r2 is negative. Therefore, q = 2 (i.e., we bring a2 into the basis). Because

we have p = 2. We thus pivot about the (2,2)th element of the second tableau to obtain the third tableau:

Because the last row of the third tableau above has no negative elements, we conclude that the basic feasible solution corresponding to the third tableau is optimal. Thus, x1 = 8/7, x2 = 5/7, x3 = 0, x4 = 0 is the solution to our LP in standard form, and the corresponding objective value is −86/7. The solution to the original problem is simply x1 = 8/7, x2 = 5/7, and the corresponding objective value is 86/7.
Degenerate basic feasible solutions may arise in the course of applying the simplex algorithm. In such a situation, the minimum ratio yi0/yiq is 0. Therefore, even though the basis changes after we pivot about the (p, q)th element, the basic feasible solution does not (and remains degenerate). It is possible that if we start with a basis corresponding to a degenerate solution, several iterations of the simplex algorithm will involve the same degenerate solution, and eventually the original basis will occur. The entire process will then repeat indefinitely, leading to what is called cycling. Such a scenario, although rare in practice, is clearly undesirable. Fortunately, there is a simple rule for choosing q and p, due to Bland, that eliminates the cycling problem (see Exercise 16.18):

16.6 Two-Phase Simplex Method
The simplex method requires starting with a tableau for the problem in canonical form; that is, we need an initial basic feasible solution. A brute-force approach to finding a starting basic feasible solution is to choose m basic columns arbitrarily and transform the tableau for the problem into canonical form. If the rightmost column is positive, then we have a legitimate (initial) basic feasible solution. Otherwise, we would have to pick another candidate basis. Potentially, this brute-force procedure requires  tries, and is therefore not practical.
Certain LP problems have obvious initial basic feasible solutions. For example, if we have constraints of the form Ax ≤ b and we add m slack variables z1,..., zm, then the constraints in standard form become

where z = [z1,..., zm]. The obvious initial basic feasible solution is

and the basic variables are the slack variables. This was the case in the example in Section 16.5.
Suppose that we are given a linear program in standard form:

In general, an initial basic feasible solution is not always apparent. We therefore need a systematic method for finding an initial basic feasible solution for general LP problems so that the simplex method can be initialized. For this purpose, suppose that we are given an LP problem in standard form. Consider the following associated artificial problem:

where y = [y1,..., ym]. We call y the vector of artificial variables. Note that the artificial problem has an obvious initial basic feasible solution:

We can therefore solve this problem by the simplex method.
Proposition 16.1 The original LP problem has a basic feasible solution if and only if the associated artificial problem has an optimal feasible solution with objective function value zero.
Proof. ⇒: If the original problem has a basic feasible solution x, then the vector [x,0] is a basic feasible solution to the artificial problem. Clearly, this solution has an objective function value of zero. This solution is therefore optimal for the artificial problem, since there can be no feasible solution with negative objective function value.
⇐: Suppose that the artificial problem has an optimal feasible solution with objective function value zero. Then, this solution must have the form [x, 0], where x ≥ 0. Hence, we have Ax = b, and x is a feasible solution to the original problem. By the fundamental theorem of LP, there also exists a basic feasible solution.
Assume that the original LP problem has a basic feasible solution. Suppose that the simplex method applied to the associated artificial problem has terminated with an objective function value of zero. Then, as indicated in the proof above, the solution to the artificial problem will have all yi = 0, i = 1,..., m. Hence, assuming nondegeneracy, the basic variables are in the first n components; that is, none of the artificial variables are basic. Therefore, the first n components form a basic feasible solution to the original problem. We can then use this basic feasible solution (resulting from the artificial problem) as the initial basic feasible solution for the original LP problem (after deleting the components corresponding to artificial variables). Thus, using artificial variables, we can attack a general linear programming problem by applying the two-phase simplex method. In phase I we introduce artificial variables and the artificial objective function and find a basic feasible solution. In phase II we use the basic feasible solution resulting from phase I to initialize the simplex algorithm to solve the original LP problem. The two-phase simplex method is illustrated in Figure 16.1.

Figure 16.1 Illustration of the two-phase simplex method.


Example 16.4 Consider the following linear programming problem:

First, we express the problem in standard form by introducing surplus variables:

For the LP problem above there is no obvious basic feasible solution that we can use to initialize the simplex method. Therefore, we use the two-phase method.
Phase I. We introduce artificial variables x5, x6 ≥ 0, and an artificial objective function x5 + x6. We form the corresponding tableau for the problem:

To initiate the simplex procedure, we must update the last row of this tableau to transform it into canonical form. We obtain

The basic feasible solution corresponding to this tableau is not optimal. Therefore, we proceed with the simplex method to obtain the next tableau:

We still have not yet reached an optimal basic feasible solution. Performing another iteration, we get

Both of the artificial variables have been driven out of the basis, and the current basic feasible solution is optimal. We now proceed to phase II.
Phase II. We start by deleting the columns corresponding to the artificial variables in the last tableau in phase I and revert back to the original objective function. We obtain

We transform the last row so that the zeros appear in the basis columns; that is, we transform the tableau above into canonical form:

All the reduced cost coefficients are nonnegative. Hence, the optimal solution is

and the optimal cost is 54/7.
16.7 Revised Simplex Method
Consider an LP problem in standard form with a matrix A of size m × n. Suppose that we use the simplex method to solve the problem. Experience suggests that if m is much smaller than n, then, in most instances, pivots will occur in only a small fraction of the columns of the matrix A. The operation of pivoting involves updating all the columns of the tableau. However, if a particular column of A never enters any basis during the entire simplex procedure, then computations performed on this column are never used. Therefore, if m is much smaller than n, the effort expended on performing operations on many of the columns of A may be wasted. The revised simplex method reduces the amount of computation leading to an optimal solution by eliminating operations on columns of A that do not enter the bases.
To be specific, suppose that we are at a particular iteration in the simplex algorithm. Let B be the matrix composed of the columns of A forming the current basis, and let D be the matrix composed of the remaining columns of A. The sequence of elementary row operations on the tableau leading to this iteration (represented by matrices E1,..., Ek) corresponds to premultiplying B, D, and b by B−1 = Ek ··· E1. In particular, the vector of current values of the basic variables is B−1b. Observe that computation of the current basic feasible solution does not require computation of B−1D; all we need is the matrix B−1. In the revised simplex method we do not compute B−1 D. Instead, we only keep track of the basic variables and the revised tableau, which is the tableau [B−1, B−1b]. Note that this tableau is only of size m × (m + 1) [compared to the tableau in the original simplex method, which is m × (n+1)]. To see how to update the revised tableau, suppose that we choose the column aq to enter the basis. Let yq = B−1aq, y0 = [y01,..., y0m] = B−1b, and p = arg mini{yi0/yiq : yiq > 0} (as in the original simplex method). Then, to update the revised tableau, we form the augmented revised tableau [B−1, y0, yq] and pivot about the pth element of the last column. We claim that the first m + 1 columns of the resulting matrix comprise the updated revised tableau (i.e., we simply remove the last column of the updated augmented revised tableau to obtain the updated revised tableau). To see this, write B−1 as B−1 = Ek ··· = E1, and let the matrix Ek+1 represent the pivoting operation above (i.e., Ek+1yq = ep, the pth column of the m × m identity matrix). The matrix Ek+1 is given by

Then, the updated augmented tableau resulting from the pivoting operation above is [Ek+1B−1, Ek+1y0, ep]. Let Bnew be the new basis. Then, we have B−1new = Ek+1 ··· E1. But notice that B−1new = Ek+1B−1, and the values of the basic variables corresponding to Bnew are given by y0new = Ek+1y0.Hence, the updated tableau is indeed [B−1new, y0new] = [Ek+1B−1, Ek+1y0].
We summarize the foregoing discussion in the following algorithm.
Revised Simplex Method

1. Form a revised tableau corresponding to an initial basic feasible solution [B−1, y0].
2. Calculate the current reduced cost coefficients vector via

where

3. If rj ≥ 0 for all j, stop—the current basic feasible solution is optimal.
4. Select a q such that rq < 0 (e.g., the q corresponding to the most negative rq), and compute

5. If no yiq > 0, stop—the problem is unbounded; else, compute p = arg mini{yi0/yiq : yiq > 0}.
6. Form the augmented revised tableau [B−1,y0,yq], and pivot about the pth element of the last column. Form the updated revised tableau by taking the first m + 1 columns of the resulting augmented revised tableau (i.e., remove the last column).
7. Go to step 2.

The reason for computing rD in two steps as indicated in step 2 is as follows. We first note that . To compute , we can do the multiplication in the order either  or . The former involves two vector-matrix multiplications, whereas the latter involves a matrix-matrix multiplication followed by a vector-matrix multiplication. Clearly, the former is more efficient.
As in the original simplex method, we can use the two-phase method to solve a given LP problem using the revised simplex method. In particular, we use the revised tableau from the final step of phase I as the initial revised tableau in phase II. We illustrate the method in the following example.
Example 16.5 Consider solving the following LP problem using the revised simplex method:

First, we express the problem in standard form by introducing one slack and one surplus variable, to obtain

There is no obvious basic feasible solution to this LP problem. Therefore, we use the two-phase method.
Phase I. We introduce one artificial variable x5 and an artificial objective function x5. The tableau for the artificial problem is

We start with an initial basic feasible solution and corresponding B−1, as shown in the following revised tableau:

We compute

Because r1 is the most negative reduced cost coefficient, we bring a1 into the basis. To do this, we first compute y1 = B−1a1. In this case, y1 = a1. We get the augmented revised tableau:

We then compute p = arg mini{yi0/yiq : yiq > 0} = 2 and pivot about the second element of the last column to get the updated revised tableau:

We next compute

The reduced cost coefficients are all nonnegative. Hence, the solution to the artificial problem is [8/5,0,12/5,0,0]. The initial basic feasible solution for phase II is therefore [8/5,0,12/5,0].
Phase II. The tableau for the original problem (in standard form) is

As the initial revised tableau for phase II, we take the final revised tableau from phase I. We then compute

We bring a2 into the basis, and compute y2 = B−1a2 to get

In this case we get p = 2. We update this tableau by pivoting about the second element of the last column to get

We compute

We now bring a4 into the basis:

We update the tableau to obtain

We compute

The reduced cost coefficients are all positive. Hence, [0,4,0,4] is optimal. The optimal solution to the original problem is [0,4].
EXERCISES

16.1 This question is concerned with elementary row operations and rank.
a. For the matrix

find its rank by first transforming the matrix using elementary row operations into an upper triangular form.
b. Find the rank of the following matrix for different values of the parameter γ by first transforming the matrix using elementary row operations into an upper triangular form:

16.2 Consider the following standard form LP problem:

a. Write down the A, b, and c matrices/vectors for the problem.
b. Consider the basis consisting of the third and fourth columns of A, ordered according to [a4,a3]. Compute the canonical tableau corresponding to this basis.
c. Write down the basic feasible solution corresponding to the basis above, and its objective function value.
d. Write down the values of the reduced cost coefficients (for all the variables) corresponding to the basis.
e. Is the basic feasible solution in part c an optimal feasible solution? If yes, explain why. If not, determine which element of the canonical tableau to pivot about so that the new basic feasible solution will have a lower objective function value.
f. Suppose that we apply the two-phase method to the problem, and at the end of phase I, the tableau for the artificial problem is

Does the original problem have a basic feasible solution? Explain.
g. From the final tableau for phase I in part f, find the initial canonical tableau for phase II.
16.3 Use the simplex method to solve the following linear program:

16.4 Consider the linear program

Convert the problem to standard form and solve it using the simplex method.
16.5 Consider a standard form linear programming problem with

where the "?" symbols signify unknowns to be determined. Suppose that the canonical tableau corresponding to some basis is

a. Find all entries of A.
b. Find all entries of c.
c. Find the basic feasible solution corresponding to the canonical tableau above.
d. Find all entries in the rightmost column of the tableau.
16.6 Consider the optimization problem

We can convert this problem into an equivalent standard form linear programming problem by introducing the new variables

and

(See also Exercise 15.3.) Then we can apply the simplex method to solve the equivalent problem. Explain, in two or three sentences, why we will always have that only either x+i or x−i can be positive but never both x+i and x−i can be positive. In other words, we will always have, x+ix−i = 0.
16.7 Suppose that we are given a linear programming problem in standard form (written in the usual notation) and are told that the vector x = [1,0,2,3,0] is a basic feasible solution with corresponding relative cost coefficient vector r = [0,1,0,0, −1] and objective function value 6. We are also told that the vector [−2,0,0,0,4] lies in the nullspace of A.
a. Write down the canonical tableau corresponding to the given basic feasible solution above, filling in as many values of entries as possible (use the symbol * for entries that cannot be determined from the information given). Clearly indicate the dimensions of the tableau.
b. Find a feasible solution with an objective function value that is strictly less than 6.
16.8 Consider a standard form linear programming problem (with the usual A, b, and c). Suppose that it has the following canonical tableau:

a. Find the basic feasible solution corresponding to this canonical tableau and the corresponding value of the objective function.
b. Find all the reduced cost coefficient values associated with the tableau.
c. Does the given linear programming problem have feasible solutions with arbitrarily negative objective function values?
d. Suppose that column a2 enters the basis. Find the canonical tableau for the new basis.
e. Find a feasible solution with objective function value equal to −100.
f. Find a basis for the nullspace of A.
16.9 Consider the problem

a. Convert the problem into a standard form linear programming problem.
b. Use the two-phase simplex method to compute the solution to this problem and the value of the objective function at the optimal solution of the problem.
16.10 Consider the linear programming problem

a. Write down the basic feasible solution for x1 as a basic variable.
b. Compute the canonical augmented matrix corresponding to the basis in part a.
c. If we apply the simplex algorithm to this problem, under what circumstance does it terminate? (In other words, which stopping criterion in the simplex algorithm is satisfied?)
d. Show that in this problem, the objective function can take arbitrarily negative values over the constraint set.
16.11 Find the solution and the value of the optimal cost for the following problem using the revised simplex method:

Hint: Start with x1 and x2 as basic variables.
16.12 Solve the following linear programs using the revised simplex method:
a. Maximize −4x1 − 3x2 subject to

b. Maximize 6x1 + 4x2 + 7x3 + 5x4 subject to

16.13 Consider a standard form linear programming problem with

Suppose that we are told that the reduced cost coefficient vector corresponding to some basis is r = [0,1,0,0].
a. Find an optimal feasible solution to the problem.
b. Find c2.
16.14 Consider the linear programming problem

where c1, c2  . Suppose that the problem has an optimal feasible solution that is not basic.
a. Find all basic feasible solutions.
b. Find all possible values of c1 and c2.
c. At each basic feasible solution, compute the reduced cost coefficients for all nonbasic variables.
16.15 Suppose that we apply the simplex method to a given linear programming problem and obtain the following canonical tableau:

For each of the following conditions, find the set of all parameter values α, β, γ, δ that satisfy the condition.
a. The problem has no solution because the objective function values are unbounded.
b. The current basic feasible solution is optimal, and the corresponding objective function value is 7.
c. The current basic feasible solution is not optimal, and the objective function value strictly decreases if we remove the first column of A from the basis.
16.16 You are given a linear programming problem in standard form. Suppose that you use the two-phase simplex method and arrive at the following canonical tableau in phase I:

The variables α, β, γ, and δ are unknowns to be determined. Those entries marked with "?" are unspecified. The only thing you are told is that the value of γ is either 2 or −1.
a. Determine the values of α, β, γ, and δ.
b. Does the given linear programming problem have a feasible solution? If yes, find it. If not, explain why.
16.17 Suppose we are given a matrix A  m × n and a vector b  m such that b ≥ 0. We are interested in an algorithm that, given this A and b, is guaranteed to produce one of following two outputs: (1) If there exists x such that Ax ≥ b, then the algorithm produces one such x. (2) If no such x exists, then the algorithm produces an output to declare so.
   Describe in detail how to design this algorithm based on the simplex method.
16.18 Consider the following linear programming problem (attributed to Beale—see [42, p. 43]):

a. Apply the simplex algorithm to the problem using the rule that q is the index corresponding to the most negative rq. (As usual, if more than one index i minimizes yi0/yiq, let p be the smallest such index.) Start with x1, x2, and x3 as initial basic variables. Notice that cycling occurs.
b. Repeat part a using Bland's rule for choosing q and p:

Note that Bland's rule for choosing p corresponds to our usual rule that if more than one index i minimizes yi0/yiq, we let p be the smallest such index.
16.19 Consider a standard form LP problem. Suppose that we start with an initial basic feasible solution x(0) and apply one iteration of the simplex algorithm to obtain x1.
   We can express x in terms of x(0) as

where α0 minimizes ϕ(α) = f(x(0) + αd(90)) over all α > 0 such that x(0) + αd(0) is feasible.
a. Show that d(0)  (A).
b. As usual, assume that the initial basis is the first m columns of A, and the first iteration involves inserting αq into the basis, where q > m. Let the qth column of the canonical augmented matrix be yq = [y1q, ..., ymg]. Express d(0) in terms of yq.
16.20 Write a simple MATLAB function that implements the simplex algorithm. The inputs are c, A, b, and v, where v is the vector of indices of basic columns. Assume that the augmented matrix [A, b] is already in canonical form; that is, the vith column of A is [0, ..., 1, ..., 0], where 1 occurs in the ith position. The function should output the final solution and the vector of indices of basic columns. Test the MATLAB function on the problem in Example 16.2.
16.21 Write a MATLAB routine that implements the two-phase simplex method. It may be useful to use the MATLAB function of Exercise 16.20. Test the routine on the problem in Example 16.5.
16.22 Write a simple MATLAB function that implements the revised simplex algorithm. The inputs are c, A, b, v, and B−1, where v is the vector of indices of basic columns; that is, the ith column of B is the vith column of A. The function should output the final solution, the vector of indices of basic columns, and the final B−1. Test the MATLAB function on the problem in Example 16.2.
16.23 Write a MATLAB routine that implements the two-phase revised simplex method. It may be useful to use the MATLAB function of Exercise 16.22. Test the routine on the problem in Example 16.5.








CHAPTER 17
DUALITY
17.1 Dual Linear Programs
Associated with every linear programming problem is a corresponding dual linear programming problem. The dual problem is constructed from the cost and constraints of the original, or primal, problem. Being an LP problem, the dual can be solved using the simplex method. However, as we shall see, the solution to the dual can also be obtained from the solution of the primal problem, and vice versa. Solving an LP problem via its dual may be simpler in certain cases, and also often provides further insight into the nature of the problem. In this chapter we study basic properties of duality and provide an interpretive example of duality. Duality can be used to improve the performance of the simplex algorithm (leading to the primal-dual algorithm), as well as to develop nonsimplex algorithms for solving LP problems (such as Khachiyan's algorithm and Karmarkar's algorithm). We do not discuss this aspect of duality further in this chapter. For an in-depth discussion of the primal-dual method, as well as other aspects of duality, see, for example, [88]. For a description of Khachiyan's algorithm and Karmarkar's algorithm, see Chapter 18.
Suppose that we are given a linear programming problem of the form

We refer to the above as the primal problem. We define the corresponding dual problem as

We refer to the variable λ  m as the dual vector. Note that the cost vector c in the primal has moved to the constraints in the dual. The vector b on the right-hand side of Ax ≥ b becomes part of the cost in the dual. Thus, the roles of b and c are reversed. The form of duality defined above is called the symmetric form of duality.
To define the dual of an arbitrary linear programming problem, we use the following procedure. First, we convert the given linear programming problem into an equivalent problem of the primal form shown above. Then, using the symmetric form of duality, we construct the dual to the equivalent problem. We call the resulting problem the dual of the original problem.
Note that based on the definition of duality above, the dual of the dual problem is the primal problem. To see this, we first represent the dual problem in the form

Therefore, by the symmetric form of duality, the dual to the above is

Upon rewriting, we get the original primal problem.
Now consider an LP problem in standard form. This form has equality constraints Ax = b. To formulate the corresponding dual problem, we first convert the equality constraints into equivalent inequality constraints. Specifically, observe that Ax = b is equivalent to

Thus, the original problem with the equality constraints can be written in the form

The LP problem above is in the form of the primal problem in the symmetric form of duality. The corresponding dual is therefore

After simple manipulation the dual above can be represented as

Let λ = u − v. Then, the dual problem becomes

Note that since λ = u − v and u, v ≥ 0, the dual vector λ is not restricted to be nonnegative. We have now derived the dual for a primal in standard form. The form of duality above is referred to as the asymmetric form of duality.
We summarize the forms of duality in Tables 17.1 and 17.2. Note that in the asymmetric form of duality, the dual of the dual is also the primal. We can show this by reversing the arguments we used to arrive at the asymmetric form of duality and using the symmetric form of duality.
Table 17.1 Symmetric Form of Duality

Table 17.2 Asymmetric Form of Duality

Recall that at the beginning of this chapter we defined the dual of an arbitrary linear programming problem by first transforming the problem into an equivalent problem of the form of the primal in the symmetric form of duality. We then derived the asymmetric form of duality based on the symmetric form. In both forms of duality the dual of the dual is the primal. Therefore, we now have four forms of primal-dual linear programming pairs: Each of the four linear programming problems in Tables 17.1 and 17.2 is a primal in these four pairs. So, given an arbitrary linear programming problem, we can obtain its dual by converting the problem into any of the four problems in Tables 17.1 and 17.2.
Example 17.1 Suppose that we are given the given linear programming problem

This problem is already close to the form of the dual in Table 17.2. In particular, let us rewrite the above as

Its associated dual is then given by the primal in Table 17.2, which has the form

which can be written in the equivalent form

If we change the sign of the dual variable, we can rewrite the above in a more "natural" form:

Example 17.2 This example is adapted from [88]. Recall the diet problem (see Example 15.2). We have n different types of food. Our goal is to create the most economical diet and at the same time meet or exceed nutritional requirements. Specifically, let aij be the amount of the ith nutrient per unit of the jth food, bi the amount of the ith nutrient required, 1 ≤ i ≤ m, cj the cost per unit of the jth food, and xi the number of units of food i in the diet. Then, the diet problem can be stated as follows:

Now, consider a health food store that sells nutrient pills (all m types of nutrients are available). Let λi be the price of a unit of the ith nutrient in the form of nutrient pills. Suppose that we purchase nutrient pills from the health food store at this price such that we exactly meet our nutritional requirements. Then, λ b is the total revenue to the store. Note that since prices are nonnegative, we have λ ≥ 0. Consider now the task of substituting nutrient pills for natural food. The cost of buying pills to create the nutritional equivalent of the ith food synthetically is simply λ1 a1i +···+ λm ami. Because ci is the cost per unit of the ith food, if

then the cost of the unit of the ith food made synthetically from nutrient pills is less than or equal to the market price of a unit of the real food. Therefore, for the health food store to be competitive, the following must hold:

The problem facing the health food store is to choose the prices λ1, ..., λm such that its revenue is maximized. This problem can be stated as

Note that this is simply the dual of the diet problem.
Example 17.3 Consider the following linear programming problem:

Find the corresponding dual problem and solve it.
We first write the primal problem in standard form by introducing slack variables x4, x5, x6. This primal problem in standard form is

where x = [x1, ..., x6] and

The corresponding dual problem (asymmetric form) is

Note that the constraints in the dual can be written as

To solve the dual problem above, we use the simplex method. For this, we need to express the problem in standard form. We substitute λ by −λ and introduce surplus variables to get

There is no obvious basic feasible solution. Thus, we use the two-phase simplex method to solve the problem.
Phase I. We introduce artificial variables λ7, λ8, λ9 and the artificial objective function λ7 + λ8 + λ9. The tableau for the artificial problem is

We start with an initial feasible solution and corresponding B−1:

We compute

Because r3 is the most negative reduced cost coefficient, we bring the third column into the basis. In this case, y3 = [3, 6, 1]. We have

By inspection, p = 1, so we pivot about the first element of the last column. The updated tableau is

We compute

We bring the second column into the basis to get

We update the tableau to get

We compute

We bring the fourth column into the basis:

The updated tableau becomes

We compute

Because all the reduced cost coefficients are nonnegative, we terminate phase I.
Phase II. We use the last tableau in phase I (where none of the artificial variables are basic) as the initial tableau in phase II. Note that we now revert back to the original cost of the dual problem in standard form. We compute

We bring the first column into the basis to obtain the augmented revised tableau

We update the tableau to get

We compute

Because all the reduced cost coefficients are nonnegative, the current basic feasible solution is optimal for the dual in standard form. Thus, an optimal solution to the original dual problem is

17.2 Properties of Dual Problems
In this section we present some basic results on dual linear programs. We begin with the weak duality lemma.
Lemma 17.1 Weak Duality Lemma. Suppose that x and λ are feasible solutions to primal and dual LP problems, respectively (either in the symmetric or asymmetric form). Then, c x ≥ λ b.
Proof. We prove this lemma only for the asymmetric form of duality. The proof for the symmetric form involves only a slight modification (see Exercise 17.1).
Because x and λ are feasible, we have Ax = b, x ≥ 0, and λτ A ≤ c. Postmultiplying both sides of the inequality λ A ≤ c by x ≥ 0 yields λ Ax ≤ c x. But Ax = b, hence λ b ≤ c x.
The weak duality lemma states that a feasible solution to either problem yields a bound on the optimal cost of the other problem. The cost in the dual is never above the cost in the primal. In particular, the optimal cost of the dual is less than or equal to the optimal cost of the primal, that is, "maximum ≤ minimum." Hence, if the cost of one of the problems is unbounded, then the other problem has no feasible solution. In other words, if "minimum = −∞" or "maximum = +∞," then the feasible set in the other problem must be empty.
Example 17.4 Consider the problem

which is clearly unbounded. By Example 17.1, the dual is

which is clearly infeasible.
It follows from the weak duality lemma that if we are given feasible primal and dual solutions with equal cost, then these solutions must be optimal in their respective problems.
Theorem 17.1 Suppose that x0 and λ0 are feasible solutions to the primal and dual, respectively (either in symmetric or asymmetric form). If c x0 = λ0 b, then x0 and λ0 are optimal solutions to their respective problems.
Proof. Let x be an arbitrary feasible solution to the primal problem. Because λ0 is a feasible solution to the dual, by the weak duality lemma, . So, if , then . Hence, x0 is optimal for the primal.
On the other hand, let λ be an arbitrary feasible solution to the dual problem. Because x0 is a feasible solution to the primal, by the weak duality lemma, . Therefore, if , then . Hence, λ0 is optimal for the dual.
We can interpret Theorem 17.1 as follows. The primal seeks to minimize its cost, and the dual seeks to maximize its cost. Because the weak duality lemma states that "maximum ≤ minimum," each problem "seeks to reach the other." When their costs are equal for a pair of feasible solutions, both solutions are optimal, and we have "maximum = minimum."
It turns out that the converse of Theorem 17.1 is also true, that is, "maximum = minimum" always holds. In fact, we can prove an even stronger result, known as the duality theorem.
Theorem 17.2 Duality Theorem. If the primal problem (either in symmetric or asymmetric form) has an optimal solution, then so does the dual, and the optimal values of their respective objective functions are equal.
Proof We first prove the result for the asymmetric form of duality. Assume that the primal has an optimal solution. Then, by the fundamental theorem of LP, there exists an optimal basic feasible solution. As is our usual notation, let B be the matrix of the corresponding m basic columns, D the matrix of the n - m nonbasic columns, cB the vector of elements of c corresponding to basic variables, cD the vector of elements of c corresponding to nonbasic variables, and rD the vector of reduced cost coefficients. Then, by Theorem 16.2,

Hence,

Define

Then,

We claim that λ is a feasible solution to the dual. To see this, assume for convenience (and without loss of generality) that the basic columns are the first m columns of A. Then,

Hence, λ A ≤ c and thus λ = cBB−1 is feasible.
We claim that λ is also an optimal feasible solution to the dual. To see this, note that

Thus, by Theorem 17.1, λ is optimal.
We now prove the symmetric case. First, we convert the primal problem for the symmetric form into the equivalent standard form by adding surplus variables:

Note that x is optimal for the original primal problem if and only if [x, (Ax - b)] is optimal for the primal in standard form. The dual to the primal in standard form is equivalent to the dual to the original primal in symmetric form. Therefore, the result above for the asymmetric case applies also to the symmetric case.
This completes the proof.
Example 17.5 Recall Example 17.2, where we formulated the dual of the diet problem. From the duality theorem, the maximum revenue for the health food store is the same as the minimum cost of a diet that satisfies all of the nutritional requirements; that is, cx = λb.
Consider a primal-dual pair in asymmetric form. Suppose that we solve the primal problem using the simplex method. The proof of the duality theorem suggests a way of obtaining an optimal solution to the dual by using the last row of the final simplex tableau for the primal. First, we write the tableau for the primal problem:

Suppose that the matrix B is the basis for an optimal basic feasible solution. Then, the final simplex tableau is

where . In the proof of the duality theorem we have shown that  is an optimal solution to the dual. The vector λ can be obtained from the final tableau above. Specifically, if rank D = m, then we can solve for λ using the vector rD, via the equation

Of course, it may turn out that rank D < m. In this case as we now show, we have additional linear equations that allow us to solve for λ. To this end, recall that . Therefore, if we define , then combining the equations  and  yields

The vector λ may be easy to obtain from the equation  if D takes certain special forms. In particular, this is the case if D has an m × m identity matrix embedded in it; that is, by rearranging the positions of the columns of D, if necessary, D has the form D = [Im, G], where G is an m × (n − 2m) matrix. In this case we can write the equation  as

Hence, λ is given by

In other words, the solution to the dual is obtained by subtracting the reduced costs coefficients corresponding to the identity matrix in D from the corresponding elements in the vector c (i.e., cI).
For example, if we have a problem where we introduced slack variables, and the basic variables for the optimal basic feasible solution do not include any of the slack variables, then the matrix D has an identity matrix embedded in it. In addition, in this case we have cI = 0. Therefore, λ = −rI is an optimal solution to the dual.
Example 17.6 In Example 17.3, the tableau for the primal in standard form is

If we now solve the problem using the simplex method, we get the following final simplex tableau:

We can now find the solution of the dual from the above simplex tableau using the equation :

Solving the above, we get

which agrees with our solution in Example 17.3.
We now summarize our discussion relating the solutions of the primal and dual problems. If one has unbounded objective function values, then the other has no feasible solution. If one has an optimal feasible solution, then so does the other (and their objective function values are equal). One final case remains: What can we say if one (the primal, say) has no feasible solution? In this case clearly the other (the dual, say) cannot have an optimal solution. However, is it necessarily the case that the dual is unbounded? The answer is no: If one of the problems has no feasible solution, then the other may or may not have a feasible solution. The following example shows that there exists a primal-dual pair of problems for which both have no feasible solution.
Example 17.7 Consider the primal problem

The problem has no feasible solution, because the constraints require that x1 - x2 ≥ 2 and x1 − x2 ≤ 1. Based on symmetric duality, the dual is

The dual also has no feasible solution, because the constraints require that λ1 - λ2 ≤ 1 and λ1 - λ2 ≥ 2.
We end this chapter by presenting the following theorem, which describes an alternative form of the relationship between the optimal solutions to the primal and dual problems.
Theorem 17.3 Complementary Slackness Condition. The feasible solutions x and λ to a dual pair of problems (either in symmetric or asymmetric form) are optimal if and only if:

1. (c − λA)x = 0.
2. λ(Ax − b) = 0.

Proof. We first prove the result for the asymmetric case. Note that condition 2 holds trivially for this case. Therefore, we consider only condition 1.
⇒: If the two solutions are optimal, then by Theorem 17.2, cx = λ b. Because Ax = b, we also have (c − λA)x = 0.
⇐ If (c − λA)x = 0, then cx = λAx = λb. Therefore, by Theorem 17.1, x and λ are optimal.
We now prove the result for the symmetric case.
⇒ We first show condition 1. If the two solutions are optimal, then by Theorem 17.2, cx = λb. Because Ax ≥ b and λ ≥ 0, we have

On the other hand, since λA ≤ c and x ≥ 0, we have (c − λA)x ≥ 0. Hence, (c - λA)x = 0. To show condition 2, note that since Ax ≥ b and λ ≥ 0, we have λ(Ax − b) ≥ 0. On the other hand, since λA ≤ c and x ≥ 0, we have λ(Ax − b) = (λ A - c)x ≤ 0.
⇐ Combining conditions 1 and 2, we get cx = λ Ax = λb. Hence, by Theorem 17.1, x and λ are optimal.
Note that if x and λ are feasible solutions for the dual pair of problems, we can write condition 1, that is, (c - λA)x = 0, as "xi > 0 implies that λ ai = ci, i = 1, ..., n," that is, for any component of x that is positive, the corresponding constraint for the dual must be an equality at λ. Also, observe that the statement "xi > 0 implies that λai = ci″ is equivalent to "λ ai < ci implies that xi = 0." A similar representation can be written for condition 2.
Consider the asymmetric form of duality. Recall that for the case of an optimal basic feasible solution x, r = c - λ A is the vector of reduced cost coefficients. Therefore, in this case, the complementary slackness condition can be written as rx = 0.
Example 17.8 Suppose that you have 26 dollars and you wish to purchase some gold. You have a choice of four vendors, with prices (in dollars per ounce) of 1/2, 1, 1/7, and 1/4, respectively. You wish to spend your entire 26 dollars by purchasing gold from these four vendors, where xi is the dollars you spend on vendor i, i = 1, 2, 3, 4.

a. Formulate the linear programming problem (in standard form) that reflects your desire to obtain the maximum weight in gold.
b. Write down the dual of the linear programming problem in part a, and find the solution to the dual.
c. Use the complementary slackness condition together with part b to find the optimal values of x1, ..., x4.

Solution:

a. The corresponding linear programming problem is

b. The dual problem is

The solution is clearly λ = −7. (Note: It is equally valid to have a dual problem with variable λ′ = −λ.)
c. By the complementary slackness condition, we know that if we can find a vector x that is feasible in the primal and satisfies (−[2, 1, 7, 4] − (−7)[1, 1, 1, 1])x = 0, then this x is optimal in the primal (original) problem. We can rewrite the conditions above as

By x ≥ 0 and [5, 6, 0, 3]x = 0, we conclude that x1 = x2 = x4 = 0, and by [1, 1, 1, 1]x = 26 we then conclude that x = [0, 0, 26, 0].

EXERCISES

17.1 Prove the weak duality lemma for the symmetric form of duality.
17.2 Find the dual of the optimization problem in Exercise 15.8.
17.3 Consider the following linear program:

a. Use the simplex method to solve the problem.
b. Write down the dual of the linear program and solve the dual.
17.4 Consider the linear program

Write down the corresponding dual problem and find the solution to the dual. (Compare this problem with the one in Exercise 16.12, part a.)
17.5 Consider the following primal problem:

a. Construct the dual problem corresponding to the primal problem above.
b. It is known that the solution to the primal above is x* = [3, 5, 3, 0, 0]. Find the solution to the dual.
17.6 Consider the linear programming problem

a. Find the dual to this problem.
b. Suppose that b = 0 and there exists a vector y ≥ 0 such that y A + c = 0. Does this problem have an optimal feasible solution? If yes, find it. If no, explain why not. Give complete explanations.
17.7 Convert the following optimization problem into a linear programming problem and solve it:

Then construct its dual program and solve it.
Hint: Introduce two sets of nonnegative variables: x+i ≥ 0, x−i ≥ 0. Then represent the optimization problem using the variables above. Note that only one x+i and x−i can be nonzero at a time. If xi ≥ 0 then x+i = xi and x−i = 0. On the other hand, if xi < 0 then x+i = 0 and xi = −x−i. See Exercise 16.6.
17.8 Consider the linear program

where 0 < a1 < a2 < ··· < an.
a. Write down the dual to the problem and find a solution to the dual in terms of a1, ..., an.
b. State the duality theorem and use it to find a solution to the primal problem above.
c. Suppose that we apply the simplex algorithm to the primal problem. Show that if we start at a nonoptimal initial basic feasible solution, the algorithm terminates in one step if and only if we use the rule where the next nonbasic column to enter the basis is the one with the most negative reduced cost coefficient.
17.9 You are given the following linear programming problem:

where c1, ..., cn   are constants.
a. Write down the dual linear program for the primal problem.
b. Suppose you know that c4 > ci for all i ≠ 4. Use this information to solve the dual.
c. Use part b to solve the linear programming problem.
17.10 Consider the linear programming problem

where c = [1, 1, ..., 1]. Assume that the problem has a solution.
a. Write down the dual of this problem.
b. Find the solution to the problem.
c. What can you say about the constraint set for the problem?
17.11 Consider a given linear programming problem in standard form (written in the usual notation).
a. Write down the associated artificial problem for the problem (used in the two-phase method).
b. Write down the dual to the artificial problem from part a.
c. Prove that if the original linear programming problem has a feasible solution, then the dual problem in part b has an optimal feasible solution.
17.12 Consider a pair of primal and dual linear programming problems (either in symmetric or asymmetric form). Identify which of the following situations are possible (depending on the particular primal-dual pair) and which are impossible (regardless of the primal-dual pair). In each case, justify your answer (citing results such as the weak duality lemma and the duality theorem whenever needed).
a. The primal has a feasible solution, and the dual has no feasible solution.
b. The primal has an optimal feasible solution, and the dual has no optimal feasible solution.
c. The primal has a feasible solution but no optimal feasible solution, and the dual has an optimal feasible solution.
17.13 Consider an LP problem in standard form. Suppose that x is a feasible solution to the problem. Show that if there exist λ and μ such that

then x is an optimal feasible solution to the problem and λ is an optimal feasible solution to the dual. The conditions above, called the Karush-Kuhn-Tucker optimality conditions for LP, are discussed in detail in Chapters 21 and 22.
17.14 Consider the linear program

where c  n, b m, and A ε m × n. Use the symmetric form of duality to derive the dual of this linear program and show that the constraint in the dual involving A can be written as an equality constraint.
Hint: Write x = u - v, with u, v ≥ 0.
17.15 Consider the linear program

The solution to the problem is [1, 1] (see Exercise 16.11). Write down the dual to the problem, solve the dual, and verify that the duality theorem holds.
17.16 Consider the problem

For this problem we have the following theorem.
Theorem: A solution to the foregoing problem exists if and only if c ≥ 0. Moreover, if a solution exists, 0 is a solution.
   Use the duality theorem to prove this theorem (see also Exercise 22.15).
17.17 Let A be a given matrix and b a given vector. Show that there exists a vector x such that Ax ≥ b and x ≥ 0 if and only if for any vector y satisfying Ay ≤ 0 and y ≥ 0, we have b y ≤ 0.
17.18 Let A be a given matrix and b a given vector. We wish to prove the following result: There exists a vector x such that Ax = b and x ≥ 0 if and only if for any given vector y satisfying Ay ≤ 0, we have by ≤ 0. This result is known as Farkas's transposition theorem. Our argument is based on duality theory, consisting of the following parts.
a. Consider the primal linear program

Write down the dual of this problem using the notation y for the dual variable.
b. Show that the feasible set of the dual problem is guaranteed to be nonempty.
Hint: Think about an obvious feasible point.
c. Suppose that for any y satisfying Ay ≤ 0, we have by ≤ 0. In this case what can you say about whether or not the dual has an optimal feasible solution?
Hint: Think about the obvious feasible point in part b.
d. Suppose that for any y satisfying Ay ≤ 0, we have b y ≤ 0. Use parts b and c to show that there exists x such that Ax = b and x ≥ 0. (This proves one direction of Farkas's transposition theorem.)
e. Suppose that x satisfies Ax = b and x ≥ 0. Let y be an arbitrary vector satisfying Ay ≤ 0. Show that by ≤ 0. (This proves the other direction of Farkas's transposition theorem.)
17.19 Let A be a given matrix and b a given vector. Show that there exists a vector x such that Ax ≤ b if and only if for any given vector y satisfying Ay = 0 and y ≥ 0, we have by ≥ 0. This result is known as Gale's transposition theorem.
17.20 Let A be a given matrix. Show that there exists a vector x such that Ax < 0 if and only if for any given vector y satisfying Ay = 0 and y ≥ 0, we have y = 0 (i.e., y = 0 is the only vector satisfying Ay = 0 and y ≥ 0). This result is known as Gordan's transposition theorem.
17.21 Let P  nxn be a matrix with the property that each element is in the real interval [0, 1], and the sum of the elements of each row is equal to 1; call such a matrix a stochastic matrix. Now consider a vector x ≥ 0 such that xe = 1, where e = [1, ..., 1]; call such a vector x a probability vector.
   We wish to prove the following result: For any stochastic matrix P, there exists a probability vector x such that xP = x. Although this is a key result in probability theory (under the topic of Markov chains), our argument is based on duality theory (for linear programming), consisting of the following parts.
a. Consider the primal linear program:

Write down the dual of this problem.
b. Show that the dual is not feasible (i.e., there does not exist a feasible solution to the dual).
Hint: Derive a contradiction based on Py > y; think about the largest element of y (call it yi).
c. Is the primal feasible? What can you deduce about whether or not the primal is unbounded?
d. Use part c to deduce the desired result: that there exists a vector x ≥ 0 such that x P = x and xe = 1.
17.22 Suppose that you are presented with a "black box" that implements a function ϕ defined as follows: Given positive integers m and n, a matrix A  mxn, and a vector b  m, the value of ϕ(m, n, A, b) is a vector x = ϕ(m, n, A, b) that satisfies Ax ≥ b, if such a vector exists. In other words, the black box solves a linear feasibility problem.
   Now, given A  mxn, b  m, and c  n, consider the linear programming problem

Express a solution to this problem in terms of the function ϕ given above. In other words, show how we can use the black box to solve this linear programming problem.
Hint: Find the appropriate inputs to the black box such that the output immediately gives a solution to the linear programming problem. You should use the black box only once.
17.23 This exercise illustrates the use of duality to compute the sensitivity of the optimal objective function value with respect to perturbations in the constraint.
   Consider a primal linear programming problem and its dual (in either symmetric or asymmetric form). Let us view the b vector in the primal as a parameter that we can vary, and that we wish to calculate the change in the optimal objective function value if we perturb b by a small perturbation Δb (i.e., replace b by b + Δb).
a. To make the problem precise, let z(b) be the optimal value of the primal objective function. Let λ denote the corresponding optimal dual vector. Calculate the gradient of z at b: ∇z(b). Write the answer in terms of λ. You may assume that the optimal dual vector remains fixed in a neighborhood of b; but if you do, you must explain why this assumption is reasonable.
Hint: Use the duality theorem to see how z(b) depends on b.
b. Suppose that the first component of the optimal dual vector is λ1 = 3. Now suppose that we increase b1 by a very small amount Δb1. Determine the amount by which the optimal objective function value will change.
17.24 Consider the quadratic programming problem

where A  mxn and b  m. Call this problem the primal problem.
   Consider the associated dual quadratic programming problem

Let f1 and f2 be the objective functions of the primal and dual, respectively.
a. State and prove a weak duality lemma in this setting.
b. Show that if x0 and y0 are feasible points in the primal and dual, and f1(x0) = f2(y0), then x0 and y0 are optimal solutions to the primal and dual, respectively.
Hint: The techniques used in the linear programming duality results are applicable in this exercise.








CHAPTER 18
NONSIMPLEX METHODS
18.1 Introduction
In previous chapters we studied the simplex method and its variant, the revised simplex method, for solving linear programming problems. The method remains widely used in practice for solving LP problems. However, the amount of time required to compute a solution using the simplex method grows rapidly as the number of components n of the variable x  n increases. Specifically, it turns out that the relationship between the required amount of time for the algorithm to find a solution and the size n of x is exponential in the worst case. An example of an LP problem for which this relationship is evident was devised by Klee and Minty in 1972 [76]. Below, we give a version of the Klee-Minty example, taken from [9]. Let n be given. Let

Consider the following LP problem:

The simplex algorithm applied to the LP problem above requires 2n − 1 steps to find the solution. Clearly, in this example the relationship between the required amount of time for the simplex algorithm to find a solution and the size n of the variable x is exponential. This relationship is also called the complexity of the algorithm. The simplex algorithm is therefore said to have exponential complexity. The complexity of the simplex algorithm is also often written as O(2n − 1).
Naturally, we would expect that any algorithm that solves LP problems would have the property that the time required to arrive at a solution increases with the size n of the variable x. However, the issue at hand is the rate at which this increase occurs. As we have seen above, the simplex algorithm has the property that this rate of increase is exponential. For a number of years, computer scientists have distinguished between exponential complexity and polynomial complexity. If an algorithm for solving LP problems has polynomial complexity, then the time required to obtain the solution is bounded by a polynomial in n. Obviously, polynomial complexity is more desirable than exponential complexity. Therefore, the existence of an algorithm for solving LP problems with polynomial complexity is an important issue. This issue was partially resolved in 1979 by Khachiyan (also transliterated as Haijan) [74], who proposed an algorithm that has a complexity O(n4L), where, roughly speaking, L represents the number of bits used in the computations. The reason that we consider Khachiyan's algorithm (also called the ellipsoid algorithm) as only a partial resolution of this issue is that the complexity depends on L, which implies that the time required to solve a given LP problem increases with the required accuracy of the computations. The existence of a method for solving LP problems with a polynomial complexity bound based only on the size of the variable n (and possibly the number of constraints) remains a difficult open problem [55]. In any case, computational experience with Khachiyan's algorithm has shown that it is not a practical alternative to the simplex method [14]. The theoretical complexity advantage of Khachiyan's method relative to the simplex method remains to be demonstrated in practice.
Another nonsimplex algorithm for solving LP problems was proposed in 1984 by Karmarkar [71]. Karmarkar's algorithm has a complexity of O(n3.5L), which is lower than that of Khachiyan's algorithm. The algorithm is superior to the simplex algorithm from a complexity viewpoint, but has its drawbacks. Improved methods along similar lines, called interior-point methods, have received considerable interest since Karmarkar's original paper. Well-implemented versions of these methods are very efficient, especially when the problem involves a large number of variables [55].
This chapter is devoted to a discussion of nonsimplex methods for solving LP problems. In the next section we discuss some ideas underlying Khachiyan's algorithm. We then present Karmarkar's algorithm in the section to follow.
18.2 Khachiyan's Method
Our description of the Khachiyan's algorithm is based on [8] and [9]. The method relies on the concept of duality (see Chapter 17). Our exposition of Khachiyan's algorithm is geared toward a basic understanding of the method. For a detailed rigorous treatment of the method, we refer the reader to [101].
Consider the (primal) linear programming problem

We write the corresponding dual problem,

Recall that the two LP problems above constitute the symmetric form of duality. From Theorem 17.1, if x and λ are feasible solutions to the primal and dual problems, respectively, and cx = λb, then x and λ are optimal solutions to their respective problems. Using this result, we see that to solve the primal problem it is enough to find a vector [x, λ] that satisfies the following set of relations:

Note that the equality cx = b λ is equivalent to the two inequalities

Taking this into account, we can represent the previous set of relations as

Therefore, we have reduced the problem of finding an optimal solution to the primal-dual pair into one of finding a vector [x, λ] that satisfies the system of inequalities above. In other words, if we can find a vector that satisfies the system of inequalities, then this vector gives an optimal solution to the primal-dual pair. On the other hand, if there does not exist a vector satisfying the system of inequalities, then the primal-dual pair has no optimal feasible solution. In the subsequent discussion, we simply represent the system of inequalities as

where

In our discussion of Khachiyan's algorithm, we will not be using these forms of P, q, and z specifically; we simply treat Pz ≤ q as a generic matrix inequality, with P, q, and z as generic entities. Let r and s be the sizes of q and z, respectively; that is, P  r×s, z  s, and q  r.
Khachiyan's method solves the LP problem by first determining if there exists a vector z that satisfies the inequality Pz ≤ q; that is, the algorithm decides if the system of linear inequalities above is consistent If the system is consistent, then the algorithm finds a vector z satisfying the system. In the following we refer to any vector satisfying the system as a solution to the system. We assume that the entries in P and q are all rational numbers. This is not a restriction in practice, since any representation of our LP problem on a digital computer will involve only rational numbers. In fact, we assume further that the entries in P and q are all integers. We can do this without loss of generality since we can always multiply both sides of the inequality Pz ≤ q by a sufficiently large number to get only integer entries on both sides.
Before discussing Khachiyan's algorithm, we introduce the idea of an ellipsoid. To this end, let z  s be a given vector and let Q be an s × s nonsingular matrix. Then, the ellipsoid associated with Q centered at z is defined as the set

The main idea underlying Khachiyan's algorithm is as follows. Khachiyan's algorithm is an iterative procedure, where at each iteration we update a vector z(k) and a matrix Qk. Associated with z(k) and Qk is an ellipsoid EQk (z(k)). At each step of the algorithm, the associated ellipsoid contains a solution to the given system of linear inequalities. The algorithm updates z(k) and Qk in such a way that the ellipsoid at the next step is "smaller" than that of the current step, but at the same time is guaranteed to contain a solution to the given system of inequalities, if one exists. If we find that the current point z(k) satisfies Pz(k) ≤ q, then we terminate the algorithm and conclude that z(k) is a solution. Otherwise, we continue to iterate. The algorithm has a fixed prespecified maximum number of iterations N to be performed, where N is a number that depends on L and s. Note that we are not free to choose N—it is computed using a formula that uses the values of L and s. The constant L is itself a quantity that we have to compute beforehand using a formula that involves P and q. When we have completed N iterations without finding a solution in an earlier step, we terminate the algorithm. The associated ellipsoid will then have shrunk to the extent that it is smaller than the precision of computation. At this stage, we will either discover a solution inside the ellipsoid, if indeed a solution exists, or we will find no solution inside the ellipsoid, in which case we conclude that no solution exists.
As we can see from the description above, Khachiyan's approach is a radical departure from the classical simplex method for solving LP problems. The method has attracted a lot of attention, and many studies have been devoted to it. However, as we pointed out earlier, the algorithm is of little practical value for solving real-world LP problems. Therefore, we do not delve any further into the details of Khachiyan's algorithm. We refer the interested reader to [101].
Despite its practical drawbacks, Khachiyan's method has inspired other researchers to pursue the development of computationally efficient algorithms for solving LP problems with polynomial complexity. One such algorithm is attributed to Karmarkar, which we discuss in Section 18.4.
18.3 Affine Scaling Method
Basic Algorithm
In this section we describe a simple algorithm, called the affine scaling method, for solving linear programming problems. This description is to prepare the reader for our discussion of Karmarkar's method in the next section. The affine scaling method is a an interior-point method. Such methods differ fundamentally from the classical simplex method in one main respect: An interior-point method starts inside the feasible set and moves within it toward an optimal vertex. In contrast, the simplex method jumps from vertex to vertex of the feasible set seeking an optimal vertex.
To begin our description of the affine scaling method, consider the LP problem

Note that the feasibility constraints have two parts: Ax = b and x ≥ 0. Suppose that we have a feasible point x(0) that is strictly interior (by strictly interior we mean that all of the components of x(0) are strictly positive). We wish to find a new point x(1) by searching in a direction d0 that decreases the objective function. In other words, we set

where α0 is a step size. In the gradient method (Chapter 8) we used the negative gradient of the objective function for the search direction. For the LP problem, the negative gradient of the objective function is −c. However, if we set d(0) = −c, the point x(1) may not lie inside the feasible set. For x(1) to lie inside the feasible set, it is necessary that d(0) be a vector in the nullspace of A. Indeed, because x(0) is feasible, we have Ax(0) = b. We also require that Ax(1) = b. Combining these two equations yields

To choose a direction d(0) that lies in the nullspace of A but is still "close" to −c, we orthogonally project −c onto the nullspace of A and take the resulting projection as d(0) The orthogonal projection of any vector onto the nullspace of A involves multiplication by the following matrix P, called the orthogonal projector (see Section 3.3 and Example 12.5):

We set d(0) to be in the direction of the orthogonal projection of −c onto the nullspace of A:

It is easy to check that APc = 0 and hence Ax(1) = b. In summary, given a feasible point x(0) we find a new feasible point x(1) using

where the choice of the step size α0 is discussed later in the section. The choice of x(1) above can be viewed as one iteration of a projected gradient algorithm, discussed in Section 23.3.
We now make the observation that the point x(0) should be chosen close to the center of the feasible set. Figure 18.1 illustrates this observation. Comparing the center and noncenter starting points in the figure, we can see that if we start at the center of the feasible set, we can take a larger step in the search direction. This larger step from the center point should yield a lower-cost value for the new point compared with the step originating from the noncenter point.

Figure 18.1 Results of projected gradient step from center and noncenter points.


Suppose that we are given a point x(0) that is feasible but is not a center point. We can transform the point to the center by applying what is called an affine scaling. For simplicity, suppose that A = [1,1,..., 1]/n and b = [1]. It is easy to see that the center of this feasible set is the point e = [1,..., 1]. To transform x(0) to e, we use the affine-scaling transformation

where D0 is a diagonal matrix whose diagonal entries are the components of the vector x(0):

Note that D0 is invertible because we assumed that x(0) is strictly interior. For general A and b we will still use the same affine-scaling transformation as above. In general, we may not be at precisely the center of the feasible set, but we hope that the transformed point will be "close" to the center. At least the point e is equidistant from the boundaries of the positive orthant {x : x ≥ 0}.
Once the starting point is at (or close to) the center of the feasible set after performing the affine-scaling transformation, we can proceed as described before. Because we have transformed the original vector x(0) via premultiplication by D−10, effectively changing the coordinate system, we also need to represent the original LP problem in the new coordinates. Specifically, the LP problem in the transformed coordinates takes the form

where

In the new () coordinate system we construct the orthogonal projector

and set (0) to be in the direction of the orthogonal projection of −0 onto the nullspace of 0:

Then, compute  using

where  = D−10x(0). TO obtain a point in the original coordinates, we perform the transformation

The procedure above takes a point x(0) and generates a new point x(1). This procedure can be represented as

where

We repeat the procedure iteratively to generate a sequence of points {x(k)}, where

with

At each stage of the algorithm, we have to ensure that the point x(k) is strictly interior. Note that the condition Ax(k) = b is satisfied automatically at each stage because of the way we select d(k). However, we also need to guarantee that x(k)i > 0 for i = 1,..., n. This can be done through appropriate choice of the step size αk, discussed next.
The main criterion for choosing αk is to make it as large as possible, but not so large that some components of xk+1 become nonpositive. That is, we select αk so that x(k+1)i = x(k)i + αkd(k)k > 0 for i = 1,..., n. To proceed, first define

The number rk represents the largest value of the step size αk such that all the components of x(k+1) are nonnegative. To ensure that x(k+1) is strictly interior, we use a step size of the form αk = αrk, where α  (0,1). Typical values of α for this method are α = 0.9 or 0.99 (see [96, p. 572]).
Unlike the simplex method, the affine scaling method will not reach the optimal solution in a finite number of steps. Therefore, we need a stopping criterion. For this, we can use any of the stopping criteria discussed in Section 8.2. For example, we can stop if

where ε > 0 is a prespecified threshold (see also [96, p. 572] for a similar stopping criterion, as well as an alternative criterion involving duality).
Two-Phase Method
To implement the affine scaling method described above, we need an initial feasible starting point that is strictly interior. We now describe a method to find such a starting point. After the starting point is found, we can then proceed to search for an optimal solution to the problem. This approach involves two phases: In phase I we find an initial strictly interior feasible point, and in phase II we use the result of phase I to initialize the affine scaling algorithm to find an optimal solution. This procedure is analogous to the two-phase simplex algorithm described in Section 16.6.
We now describe phase I of the two-phase affine scaling method. Let u be an arbitrary vector with positive components, and let

If v = 0, then u is a strictly interior feasible point. We can then set x(0) = u and proceed to phase II, where we apply the affine scaling method as described before. On the other hand, if v ≠ 0, we construct the following associated artificial problem:

The artificial problem above has an obvious strictly interior feasible point:

Using this point as the initial point, we can apply the affine scaling algorithm to the artificial problem. Because the objective function in the artificial problem is bounded below by 0, the affine scaling method will terminate with some optimal solution.
Proposition 18.1 The original LP problem has a feasible solution if and only if the associated artificial problem has an optimal feasible solution with objective function value zero.
Proof. ⇒: If the original problem has a feasible solution x, then the vector [x,0] is a feasible solution to the artificial problem. Clearly, this solution has an objective function value of zero. This solution is therefore optimal for the artificial problem, since there can be no feasible solution with negative objective function value.
⇐: Suppose that the artificial problem has an optimal feasible solution with objective function value zero. Then, this solution must have the form [x,0], where x ≥ 0. Hence, we have Ax = b, and x is a feasible solution to the original problem.
Suppose that the original LP problem has a feasible solution. By Proposition 18.1, if we apply the affine scaling method to the artificial problem (with initial point [u,1]), the algorithm will terminate with objective function value zero. The optimal solution will be of the form [x, 0]. We argue that x will in general be a strictly interior feasible point. It is easy to see that x ≥ 0. To convince ourselves that each component of x will be positive in general, note that the subset of optimal feasible solutions of the artificial problem in which one or more among the first n components are zero is a very small or thin subset of the set of all optimal feasible solutions. By small or thin we mean in the sense that a two-dimensional plane in 3 is small or thin. In particular, the volume of the two-dimensional plane in 3 is zero. Thus, it is very unlikely that the affine scaling algorithm will terminate with an optimal feasible solution in which one or more among the first n components are zero.
Having completed phase I as described above, we then use the first n components of the terminal optimal feasible solution for the artificial problem as our initial point for the affine scaling method applied to the original LP problem. This second application of the affine scaling algorithm constitutes phase II.
In theory, phase I generates a feasible point to initiate phase II. However, because of the finite precision of typical computer implementations, the solution obtained from phase I may not, in fact, be feasible. Moreover, even if the initial point in phase II is feasible, in practice the iterates may lose feasibility, owing to finite precision computations. Special procedures for dealing with such problems are available. For a discussion of numerical implementation of affine scaling algorithms, see [42, Section 7.1.2].
18.4 Karmarkar's Method
Basic Ideas
Like the affine scaling method, Karmarkar's method for solving LP problems differs fundamentally from the classical simplex method in various respects. First, Karmarkar's method is an interior-point method. Another difference between Karmarkar's method and the simplex method is that the latter stops when it finds an optimal solution. On the other hand, Karmarkar's method stops when it finds a solution that has an objective function value that is less than or equal to a prespecified fraction of the original guess. A third difference between the two methods is that the simplex method starts with LP problems in standard form, whereas Karmarkar's method starts with LP problems in a special canonical form, which we call Karmarkar's canonical form. We discuss this canonical form in the next subsection. While more recent interior-point methods are recognized to be superior to Karmarkar's original algorithm in efficiency and robustness, a study of Karmarkar's method provides an informative introduction to the study of more advanced interior-point methods.
Karmarkar's Canonical Form
To apply Karmarkar's algorithm to a given LP problem, we must first transform the given problem into a particular form, which we refer to as Karmarkar's canonical form. Karmarkar's canonical form is written as

where x = [x1,..., xn]. As in our discussion of Khachiyan's method, we assume without loss of generality that the entries of A and c are integers.
We now introduce some notation that allows convenient manipulation of the canonical form. First, let e = [1,..., 1] be the vector in n with each component equal to 1. Let Ω denote the nullspace of A, that is, the subspace

Define the simplex in n by

We denote the center of the simplex Δ by

Clearly, a0  Δ. With the notation above, Karmarkar's canonical form can be rewritten as

Note that the constraint set (or feasible set) Ω ∩ Δ can be represented as

Example 18.1 Consider the following LP problem, taken from [125]:

Clearly, this problem is already in Karmarkar's canonical form, with c = [5,4,8], and A = O. The feasible set for this example is illustrated in Figure 18.2.

Figure 18.2 Feasible set for Example 18.1.


Example 18.2 Consider the following LP problem, taken from [110]:

This problem is in Karmarkar's canonical form, with c = [3,3,-1] and A = [2,-3,1]. The feasible set for this example is illustrated in Figure 18.3 (adapted from [110]).

Figure 18.3 The feasible set for Example 18.2.


We show later that any LP problem can be converted into an equivalent problem in Karmarkar's canonical form.
Karmarkar's Restricted Problem
Karmarkar's algorithm solves LP problems in Karmarkar's canonical form, with the following assumptions:

A. The center a0 of the simplex Δ is a feasible point: a0  Ω.
B. The minimum value of the objective function over the feasible set is zero.
C. The (m + 1) x n matrix

has rank m + 1.
D. We are given a termination parameter q > 0, such that if we obtain a feasible point x satisfying

then we consider the problem solved.

Any LP problem that is in Karmarkar's canonical form and that also satisfies the four assumptions above is called a Karmarkar's restricted problem. In the following we discuss the assumptions and their interpretations.
We begin by looking at assumption A. We point out that this assumption is not restrictive, since any LP problem that has an optimal feasible solution can be converted into a problem in Karmarkar's canonical form that satisfies assumption A. We discuss this in the next subsection.
We next turn our attention to assumption B. Any LP problem in Karmarkar's canonical form can be converted into one that satisfies assumption B, provided that we know beforehand the minimum value of its objective function over the feasible set. Specifically, suppose that we are given an LP problem where the minimum value of the objective function is M. As in [110], consider the function f(x) = cx − M. Then, using the property that ex = 1 on the feasible set, we have that for any feasible x,

where  = c − Me. Notice that the objective function above has a minimum value of zero and is a linear function of x. We can replace the original objective function with the new objective function above, without altering the solution.
Example 18.3 Recall the LP problem in Example 18.1:

The problem satisfies assumption A (and assumption C) but not assumption B, since the minimum value of the objective function over the feasible set is 4. To convert the above into a problem that satisfies assumption B, we replace c = [5, 4, 8] by  = [1, 0, 4].
Example 18.4 The reader can easily verify that the LP problem in Example 18.2 satisfies assumptions A, B, and C.
Assumption C is a technical assumption that is required in the implementation of the algorithm. Its significance will be clear when we discuss the update equation in Karmarkar's algorithm.
Assumption D is the basis for the stopping criterion of Karmarkar's algorithm. In particular, we stop when we have found a feasible point satisfying c x/c a0 ≤ 2−q. Such a stopping criterion is inherent in any algorithm that uses finite-precision arithmetic. Observe that the stopping criterion above depends on the value of cτ a0. It will turn out that Karmarkar's algorithm uses a0 as the starting point. Therefore, we can see that the accuracy of the final solution in the algorithm is influenced by the starting point.
From General Form to Karmarkar's Canonical Form
We now show how any LP problem can be coverted into an equivalent problem in Karmarkar's canonical form. By equivalent we mean that the solution to one can be used to determine the solution to the other, and vice versa. To this end, recall that any LP problem can be transformed into an equivalent problem in standard form. Therefore, it suffices to show that any LP problem in standard form can be transformed into an equivalent problem in Karmarkar's canonical form. In fact, the transformation given below (taken from [71]) will also guarantee that assumption A of the preceding subsection is satisfied.
To proceed, consider a given LP problem in standard form:

We first present a simple way to convert this problem into Karmarkar's canonical form, ignoring the requirement to satisfy assumption A. For this, define a new variable z  n+1 by

Also define c′ = [c, 0] and A′ = [A, −b]. Using this notation, we can now rewrite the LP problem above as

We need one more step to transform the problem into one that includes the constraint that the decision variables sum to 1. For this, let y = [y1, ..., yn, yn+1]  n+1, where

This transformation from x to y is called a projective transformation. It can be shown that (see later)

Therefore, we have transformed the given LP problem in standard form into the following problem, which is in Karmarkar's canonical form:

The transformation technique above can be modified slightly to ensure that assumption A holds. We follow the treatment of [71]. We first assume that we are given a point a = [a1, ..., an] that is a strictly interior feasible point; that is, Aa = b and a > 0. We show later how this assumption can be enforced. Let P+ denote the positive orthant of n, given by P+ = {x  n : x ≥ 0}. Let Δ = {x  n+1 : e x = 1, x ≥ 0} be the simplex in n+1. Define the map T : P+ → Δ by

with

We call the map T a projective transformation of the positive orthant P+ into the simplex Δ (for an introduction to projective transformations, see [68]). The transformation T has several interesting properties (see Exercises 18.4, 18.5, and 18.6). In particular, we can find a vector c′  n+1 and a matrix A′  m x (n+1) such that for each x  n,

and

(see Exercises 18.5 and 18.6 for the forms of A′ and c′). Note that for each x  n, we have eT(x) = 1, which means that T(x)  Δ. Furthermore, note that for each x  n,

Taking this into account, consider the following LP problem (where y is the decision variable):

Note that this LP problem is in Karmarkar's canonical form. Furthermore, in light of the definitions of c′ and A′, the above LP problem is equivalent to the original LP problem in standard form. Hence, we have converted the LP problem in standard form into an equivalent problem in Karmarkar's canonical form. In addition, because a is a strictly interior feasible point, and a0 = T(a) is the center of the simplex Δ (see Exercise 18.4), the point a0 is a feasible point of the transformed problem. Hence, assumption A of the preceding subsection is satisfied for the problem above.
We started this discussion with the assumption that we are given a point a that is a strictly interior feasible point of the original LP problem in standard form. To see how this assumption can be made to hold, we now show that we can transform any given LP problem into an equivalent problem in standard form where such a point a is explicitly given. To this end, consider a given LP problem of the form

Note that any LP problem can be converted into an equivalent problem of the above form. To see this, recall that any LP problem can be transformed into an equivalent problem in standard form. But any problem in standard form can be represented as above, since the constraint Ax = b can be written as Ax ≥ b, −Ax ≥ −b. We next write the dual to the problem above:

As we did in our discussion of Khachiyan's algorithm, we now combine the primal and dual problems to get

As we pointed out in the earlier section on Khachiyan's algorithm, the original LP problem is solved if and only if we can find a pair (x, λ) that satisfies the set of relations above. This follows from Theorem 17.1. We now introduce slack and surplus variables u and v to get the following equivalent set of relations:

Let x0  n, λ0  m, u0  n, and v0  m be points that satisfy x0 > 0, λ0 > 0, u0 > 0, and v0 > 0. For example, we could choose x0 = [1, ..., 1], and likewise with λ0, u0, and v0. Consider the LP problem

We refer to the above as the Karmarkar's artificial problem, which can be represented in matrix notation as

where

(the subscripts above refer to the dimensions/sizes of the corresponding matrices/vectors). Observe that the following point is a strictly interior feasible point for the problem above:

Furthermore, the minimum value of the objective function for Karmarkar's artificial problem is zero if and only if the previous set of relations has a solution, that is, there exists x, λ, u, and v satisfying

Therefore, Karmarkar's artificial LP problem is equivalent to the original LP problem:

Note that the main difference between the original LP problem and Karmarkar's artificial problem is that we have an explicit strictly interior feasible point for Karmarkar's artificial problem, and hence we have satisfied the assumption that we imposed at the beginning of this subsection.
The Algorithm
We are now ready to describe Karmarkar's algorithm. Keep in mind that the LP problem we are solving is a Karmarkar's restricted problem, that is, a problem in Karmarkar's canonical form and satisfies assumptions A, B, C, and D. For convenience, we restate the problem:

where Ω = {x  n : Ax = 0} and Δ = {x  n : e x = 1, x ≥ 0}. Karmarkar's algorithm is an iterative algorithm that, given an initial point x(0) and parameter q, generates a sequence x(1), x(2), ..., x(N). Karmarkar's algorithm is described by the following steps:

1. Initialize: Set k : = 0; x(0) = a0 = e/n.
2. Update: Set x(k+1) = Ψ(x(k)), where Ψ is an update map described below.
3. Check the stopping criterion: If the condition c x(k)/c x(0) ≤ 2−q is satisfied, then stop.
4. Iterate: Set k := k + 1; go to step 2.

We describe the update map Ψ as follows. First, consider the first step in the algorithm: x(0) = a0. To compute x(1) we use the familiar update equation

where α is a step size and d(0) is an update direction. The step size α is chosen to be a value in (0, 1). Karmarkar recommends a value of 1/4 in his original paper [71]. The update direction d(0) is chosen as follows. First, note that the gradient of the objective function is c. Therefore, the direction of maximum rate of decrease of the objective function is −c. However, in general, we cannot simply update along this direction, since x(1) is required to lie in the constraint set

where B0  (m+1) x n is given by

Note that since x(0)  Ω ∩ Δ, then for x(1) = x(0) + αd(0) also to lie in Ω ∩ Δ, the vector d must be an element of the nullspace of B0. Hence, we choose d to be in the direction of the orthogonal projection of −c onto the nullspace of B0. This projection is accomplished by the matrix P0 given by

Note that B0B0 is nonsingular by assumption C. Specifically, we choose d(0) to be the vector d(0) = −r(0), where

and . The scalar r is incorporated into the update vector d(0) for the following reason. First, observe that r is the radius of the largest sphere inscribed in the simplex Δ (see Exercise 18.7). Therefore, the vector d(0) = r(0) points in the direction of the projection (0) of c onto the nullspace of B0 and x(1) = x(0) + αd(0) is guaranteed to lie in the constraint set Ω ∩ Δ. In fact, x(1) lies in the set Ω ∩ Δ ∩ {x : ||x − a0|| ≥ r}. Finally, we note that x(1) is a strictly interior point of Δ.
The general update step x(k+1) = Ψ(x(k)) is performed as follows. We first give a brief description of the basic idea, which is similar to the update from x(0) to x(1) described above. However, note that x(k) is, in general, not at the center of the simplex. Therefore, let us first transform this point to the center. To do this, let Dk be a diagonal matrix whose diagonal entries are the components of the vector x(k):

It turns out that because x(0) is a strictly interior point of Δ, x(k) is a strictly interior point of Δ for all k (see Exercise 18.10). Therefore, Dk is nonsingular and

Consider the mapping Uk : Δ → Δ given by Uk(x) = D−1k x/e D−1k x. Note that Uk(x(k)) = e/n = a0. We use Uk to change the variable from x to  = Uk(x). We do this so that x(k) is mapped into the center of the simplex, as indicated above. Note that Uk is an invertible mapping, with x = U−1k () = Dk/e Dk. Letting  = Uk(x(k)) = a0, we can now apply the procedure that we described before for getting x(1) from x(0) = a0. Specifically, we update (k) to obtain (k+1) using the update formula (k+1) =  + αd(k). To compute d(k), we need to state the original LP problem in the new variable :

The reader can easily verify that the LP problem above in the new variable  is equivalent to the original LP problem in the sense that x* is an optimal solution to the original problem if and only if Uk(x*) is an optimal solution to the transformed problem. To see this, simply note that  = Uk(x) = D−1k x/e D−1k x, and rewrite the objective function and constraints accordingly (see Exercise 18.8). As before, let

We choose d(k) = −r(k), where (k) is the normalized projection of −(c Dk) = −Dk c onto the nullspace of Bk, and  as before. To determine (k), we define the projector matrix Pk by

Note that BkBk is nonsingular (see Exercise 18.9). The vector (k) is therefore given by

The direction vector d(k) is then

The updated vector (k+1) =  + αd(k) is guaranteed to lie in the transformed feasible set { : ADk  = 0} ∩ Δ. The final step is to apply the inverse transformation U−1k to obtain x(k+1):

Note that x(k+1) lies in the set Ω ∩ Δ. Indeed, we have already seen that Uk and U−1k map Δ into Δ. To see that Ax(k+1) = 0, we simply premultiply the foregoing expression by A and use the fact that ADk(k+1) = 0.
We now summarize the update x(k+1) = Ψ (x(k)):

1. Compute the matrices:

2. Compute the orthogonal projector onto the nullspace of Bk:

3. Compute the normalized orthogonal projection of c onto the nullspace of Bk:

4. Compute the direction vector:

where 
5. Compute (k+1) using

where α is the prespecified step size, α  (0, 1).
6. Compute x(k+1) by applying the inverse transformation U−1k:


The matrix Pk in step 2 is needed solely for computing PkDkc in step 3. In fact, the two steps can be combined in an efficient way without having to compute Pk explicitly, as follows. We first solve a set of linear equations BkBk y = BkDkc (for the variable y), and then compute PkDkc using the expression PkDkc = Dkc − Bk y.
For more details on Karmarkar's algorithm, see [42], [55], [71], and [124]. For an informal introduction to the algorithm, see [110]. For further reading on other nonsimplex methods in linear programming, see [42], [55], [96], and [119]. A continuous gradient system for solving linear programming problems is discussed in [26]. An interesting three-article series on developments of the linear programming area before and after 1984 appeared in SIAM News, Vol. 22, No. 2, March 1989. The first article in this journal issue contains an account by Wright on recent progress and a history of linear programming from the early 1800s. The second article, by Anstreicher, focuses on interior-point algorithms developed since 1984. Finally in the third article in the series, Monma surveys computational implementations of interior-point methods.
EXERCISES

18.1 Write a simple MATLAB function to implement the affine scaling algorithm. The inputs are c, A, b, and x(0), where x(0) is a strictly feasible initial point. Test the function on the problem in Example 16.2; use x(0) = [2, 3, 2, 3, 3].
18.2 Write a MATLAB routine that implements the two-phase affine scaling method. It may be useful to use the MATLAB function of Exercise 18.1. Test the routine on the problem in Example 16.5.
18.3 For a given linear programming problem of the form

the associated Karmarkar's artificial problem can be solved directly using the affine scaling method. Write a simple MATLAB program to solve problems of the form above by using the affine scaling algorithm applied to the associated Karmarkar's artificial problem. It may be useful to use the MATLAB function of Exercise 18.1. Test your program on the problem in Example 15.15.
18.4 Let a  n, a > 0. Let T = [T1, ..., Tn+1] be the projective transformation of the positive orthant P+ of n into the simplex Δ in n+1, given by

Prove the following properties of T (see [71]):
1. T is a one-to-one mapping; that is, T(x) = T(y) implies that x = y.
2. T maps P+ onto Δ \ {x : xn+1 = 0}  {x  Δ : xn+1 > 0}; that is, for each y  {x  Δ : xn+1 > 0}, there exists x  P+ such that y = T(x).
3. The inverse transformation of T exists on {x  Δ : xn+1 > 0} and is given by T−1 = [T−11, ..., T−1n], with T−1i (y) = aiyi/yn+1.
4. T maps a to the center of the simplex Δ, that is, T(a) = e/(n + 1) = [1/(n + 1), ..., 1/(n + 1)]  n+1.
5. Suppose that x satisfies Ax = b, and y = T(x). Let x′ = [y1 a1, ..., yn an]. Then, Ax′ = byn+1.
18.5 Let T be the projective transformation in Exercise 18.4 and A  m x n be a given matrix. Prove that there exists a matrix A′  m x (n+1) such that Ax = b if and only if A'T(x) = 0.Hint: Let the ith column of A′ be given by ai times the ith column of A, i = 1, ..., n, and the (n + 1)th column of A′ be given by −b.
18.6 Let T be the projective transformation in Exercise 18.4 and c  n be a given vector. Prove that there exists a vector c′  n+1 such that c x = 0 if and only if c′ T(x) = 0.Hint: Use property 3 in Exercise 18.4, with the c′ = [c1', ..., cn+1'] given by ci′ = aici, i = 1, ..., n, and cc+1′ = 0.
18.7 Let Δ = {x  n : e x = 1, x ≥ 0} be the simplex in n, n > 1, and let a0 = e/n be its center. A sphere of radius r centered at a0 is the set {x  n : ||x − a0|| ≤ r}. The sphere is said to be inscribed in Δ if {x  n : ||x − a0|| = r, e x = 1} ⊂ Δ. Show that the largest such sphere has radius .
18.8 Consider the following Karmarkar's restricted problem:

Let x0  Δ be a strictly interior point of Δ, and D be a diagonal matrix whose diagonal entries are the components of x0. Define the map U : Δ → Δ by U(x) = D1 x/e D−1 x. Let  = U(x) represent a change of variable. Show that the following transformed LP problem in the variable ,

is equivalent to the original LP problem above in the sense that x* is an optimal solution to the original problem if and only if * = U(x*) is an optimal solution to the transformed problem.
18.9 Let A  m x n, m < n, and Ω = {x : Ax = 0}. Suppose that A satisfies

Let x0  Δ ∩ Δ Ω be a strictly interior point of Δ ⊂ n and D be a diagonal matrix whose diagonal entries are the components of x0. Consider the matrix B defined by

Show that rank B = m + 1, and hence BB is nonsingular.
18.10 Show that in Karmarkar's algorithm, x(k) is a strictly interior point of Δ.








CHAPTER 19
INTEGER LINEAR PROGRAMMING
19.1 Introduction
This chapter is devoted to linear programs with the additional constraint that the solution components be integers. Such problems are called integer linear programming (ILP) (or simply integer programming) problems, and arise naturally in many practical situations. For example, in Example 15.1, the decision variables represent production levels, which we allowed to take real values. If production levels correspond to actual numbers of products, then it is natural to impose the constraint that they be integer valued. If we expect solutions that are very large in magnitude, then ignoring the integer constraint might have little practical consequence. However, in cases where the solution is a relatively small integer (on the order of 10, say), then ignoring the integer constraint could lead to dramatically erroneous solutions.
Throughout this section, we use the notation  for the set of integers, n the set of vectors with n integer components, and m × n the set of m × n matrices with integer entries. Using this notation, we can express an ILP problem in following form:

19.2 Unimodular Matrices
There is a class of ILP problems that can be solved using standard linear programming methods. To proceed, we need some definitions and background results. The reader should recall the definition of a minor from Section 2.2.
Definition 19.1 An m × n integer matrix A  m × n, m ≤ n, is unimodular if all its nonzero mth-order minors are ±1 (i.e., either 1 or −1).
Unimodular matrices play a special role in the context of linear equations and integer basic solutions. Consider the linear equation Ax = b with A  m × n, m ≤ n. Let B be a corresponding basis matrix (an m × m matrix consisting of m linearly independent columns of A). Then, the unimodularity of A is equivalent to |det B| = 1 for any such B. The following lemma connects unimodularity with integer basic solutions.
Lemma 19.1 Consider the linear equation Ax = b where A  m × n, n ≤ n, is unimodular and b  m. Then, all basic solutions have integer components.
Proof. As usual, suppose that the first m columns of A constitute a basis, and that B is the invertible m × m matrix composed of these columns. Then the corresponding basic solution is

Because all the elements of A are integers, B is an integer matrix. Moreover, because A is unimodular, |det B| = 1. This implies that the inverse B−1 is also an integer matrix (see [62, p. 21]). Therefore, x* is an integer vector.
Corollary 19.1 Consider the LP constraint

where A is unimodular, A  m × n, m ≤ n, and b  m. Then, all basic feasible solutions have integer components.
Unimodularity allows us to solve ILP problems using the simplex method. Specifically, consider the ILP problem

where A  m × n, m ≤ n, is unimodular and b  m. Then, the corollary above tells us that if we consider the associated LP problem

the optimal basic feasible solution is an integer vector. This means that we can apply the simplex method to the LP problem above to obtain a solution to the original ILP problem.
Example 19.1 Consider the following ILP problem:

We can write this problem in matrix form with

Notice that b  3. Moreover, it is easy to check that A is unimodular. Hence, the ILP problem above can be solved by solving the LP problem

This was done in Example 16.2 using the simplex method, yielding optimal solution [2,6,2,0,0], which is an integer vector.
In general, when the matrix A is not unimodular, the simplex method applied to the associated LP problem yields a noninteger optimal solution. However, in some cases, even if A is not unimodular, the simplex method still produces an integer optimal basic feasible solution. To see this, suppose that we are given A  m × n, m ≤ n, and b  m. Note that as long as each m × m basis matrix B consisting of columns of A corresponding to a basic feasible solution has the property that |det B| = 1, we can use the argument in the proof of Lemma 19.1 to conclude that the basic feasible solution is an integer vector. Equivalently, we can draw this conclusion if each basis submatrix B of A such that |det B| ≠ 1 corresponds to a nonfeasible basic solution. We illustrate this in the following example.
Example 19.2 Consider the ILP problem

Can this ILP problem be solved using the simplex method? We can easily verify that the matrix

is not unimodular. Indeed, it has one (and only one) basis submatrix with determinant other than ±1, consisting of the first, fourth, and fifth columns of A. Indeed, if we write B = [a1,a4,a5], then det B = −2. However, a closer examination of this matrix and the vector b = [2,3,3] reveals that the corresponding basic solution is not feasible: B−1b = [−1,2,4] (which, coincidentally, happens to be an integer vector). Therefore, for this problem, applying the simplex method to the associated LP problem will produce an integer optimal basic feasible solution, which also solves the original ILP problem.
We begin by forming the first tableau,

We have r2 = −2. Therefore, we introduce a2 into the new basis. We calculate the ratios yi0/yi2, yi2 > 0, to determine the pivot element:

We will use y12 as the pivot. Performing elementary row operations, we obtain the second tableau,

We now have r1 = − 5 < 0. Therefore, we introduce a1 into the new basis. We next calculate the ratios yi0/yi2, yi2 > 0, to determine the pivot element:

We will use y21 as the pivot. Performing row elementary operations, we obtain the third tableau,

We have r3 = − 3 < 0. Therefore, we introduce a3 into the new basis. We next calculate the ratios yi0/yi2, yi2 > 0, to determine the pivot element,

We will use y33 as the pivot. Performing row elementary operations, we obtain the fourth tableau,

All reduced cost coefficients are now positive, which means that the current solution is optimal. This solution is [3,6,2,0,0].
Next, we consider ILP problems of the form

We have seen in Section 15.5 that we can transform the inequality constraint Ax ≤ b into standard form by introducing slack variables. Doing so would lead to a new problem in standard form for which the constraint has the form [A, I]y = b (where the vector y contains x and the slack variables). To deal with matrices of the form [A, I], we need another definition.
Definition 19.2 An m × n integer matrix A  m × n is totally unimodular if all its nonzero minors are ±1.
By minors here we mean pth-order minors for p ≤ min(m, n). Equivalently, a matrix A  m × n is totally unimodular if and only if all its square invertible submatrices have determinant ±1. By a submatrix of A we mean a matrix obtained by removing some columns and rows of A. It is easy to see from this definition that if an integer matrix is totally unimodular, then each entry is 0, 1, or −1. The next proposition relates the total unimodularity of A with the unimodularity of [A, I] (see also Exercise 19.3).
Proposition 19.1 If an m × n integer matrix A  m × n is totally unimodular, then the matrix [A, I] is unimodular.
Proof. Let A satisfy the assumptions of the proposition. We will show that any m × m invertible submatrix of [A, I] has determinant ±1. We first note that any m × m invertible submatrix of [A, I] that consists only of columns of A has determinant ±1 because A is totally unimodular. Moreover, the m × m submatrix I satisfies det I = 1.
Consider now an m × m invertible submatrix of [A, I] composed of k columns of A and m − k columns of I. Without loss of generality, suppose that this submatrix is composed of the last k columns of A and the first m − k columns of I; that is, the m × m invertible submatrix is

where ei is the ith column of the identity matrix. This choice of columns is without loss of generality because we can exchange rows and columns to arrive at this form, and each exchange only changes the sign of the determinant. Moreover, note that det B − ±det Bk,k (see also Exercises 19.4 and 2.4). Thus, Bk,k is invertible because B is invert ible. Moreover, because Bk,k is a submatrix of A and A is totally unimodular, det Bk,k = ±1. Hence, det B = ±1 also. Thus any m × m invertible submatrix of [A, I] has determinant ±1, which implies that [A, I] is unimodular.
Combining the result above with Lemma 19.1, we obtain the following corollary.
Corollary 19.2 Consider the LP constraint

where A  m × n is totally unimodular and b  m. Then, all basic feasible solutions have integer components.
Total total unimodularity of A allows us to solve ILP problems of the following form using the simplex method:

where b  m. Specifically, we first consider the associated LP problem

If A is totally unimodular, then the corollary above tells us that once we convert this problem into standard form by introducing a slack-variable vector z,

the optimal basic feasible solution is an integer vector. This means that we can apply the simplex method to the LP problem above to obtain a solution to the original ILP problem. Note that although we only needed the x part of the solution to be integer, the slack-variable vector z is automatically integer for any integer x, because both A and b only contain integers (see also Exercise 19.5).
Example 19.3 Consider the following ILP problem:

This problem can be written in the matrix form above with

It is easy to check that A is totally unimodular. Hence, the ILP problem above can be solved by solving the LP problem

as was done in Example 16.2.
As discussed before, even if [A, I] is not unimodular, the simplex algorithm might still yield a solution to the original ILP. In particular, even if A is not totally unimodular, the method above might still work, as illustrated in the following example.
Example 19.4 Consider the following ILP problem:

We first express the given problem in this equivalent form:

We next represent the problem above in standard form by introducing slack variables x3, x4, and x4 to obtain

This problem is now of the form in Example 19.2, where the simplex method was used. Recall that the solution is [3,6,2,0,0]. Thus, the solution to the original problem is x* = [3,6].
Note that the matrix

is not totally unimodular, because it has an entry (−2) not equal to 0, 1, or −1. Indeed, the matrix [A, I] is not unimodular. However, in this case, the simplex method still produced an optimal solution to the ILP, as explained in Example 19.2.
19.3 The Gomory Cutting-Plane Method
In 1958, Ralph E. Gomory [54] proposed a method where noninteger optimal solutions obtained using the simplex method are successively removed from the feasible set by adding constraints that exclude these noninteger solutions from the feasible set. The additional constraints, referred to as Gomory cuts, do not eliminate integer feasible solutions from the feasible set. The process is repeated until the optimal solution is an integer vector.
To describe Gomory cuts, we use the floor operator, defined next.
Definition 19.3 The floor of a real number, denoted , is the integer obtained by rounding x toward −∞.
For example, 3.4 = 3 and −3.4 = −4.
Consider the ILP problem

We begin by applying the simplex method to obtain an optimal basic feasible solution to the LP problem

As usual, suppose that the first m columns form the basis for the optimal basic feasible solution. The corresponding canonical augmented matrix is

Consider the ith component of the optimal basic feasible solution, yi0. Suppose that yi0 is not an integer. Note that any feasible vector x satisfies the equality constraint (taken from the ith row)

We use this equation to derive an additional constraint that would eliminate the current optimal noninteger solution from the feasible set without eliminating any integer feasible solution. To see how, consider the inequality constraint

Because yij ≤ yij, any x ≥ 0 that satisfies the first equality constraint above also satisfies this inequality constraint. Thus, any feasible x satisfies this inequality constraint. Moreover, for any integer feasible vector x, the left-hand side of the inequality constraint is an integer. Therefore, any integer feasible vector x also satisfies

Subtracting this inequality from the equation above, we deduce that any integer feasible vector satisfies

Next, notice that the optimal basic feasible solution above does not satisfy this inequality, because the left-hand side for the optimal basic feasible solution is 0, but the right-hand side is a positive number. Therefore, if we impose the additional inequality constraint above to the original LP problem, the new constraint set would be such that the current optimal basic feasible solution is no longer feasible, but yet every integer feasible vector remains feasible. This new constraint is called a Gomory cut.
To transform the new LP problem into standard form, we introduce the surplus variable xn+1 to obtain the equality constraint

For convenience, we will also call this equality constraint a Gomory cut. By augmenting this equation into A and b, or canonical versions of them (e.g., in the form of a simplex tableau), we obtain a new LP problem in standard form. We can then solve the new problem using the simplex method and examine the resulting optimal basic feasible solution. If the solution satisfies the integer constraints, then we are done—this vector gives an optimal solution to the original ILP problem by extracting the appropriate components. If the solution does not satisfy the integer constraints, we introduce another Gomory cut and repeat the process. We call this procedure the Gomory cutting-plane method.
Note that in applying the Gomory cutting-plane method, we only need to introduce enough cuts to satisfy the integer constraints for the original ILP problem. The additional variables introduced by slack variables or by the Gomory cuts are not constrained to be integers.
In the following two examples, we illustrate how the Gomory cutting-plane method can be implemented by incorporating Gomory cuts directly into the simplex tableau.
Example 19.5 Consider the following ILP problem1:

We first solve the problem graphically. The constraint set Ω for the associated LP problem (without integer constraints) can be found by calculating the extreme points:

In Figure 19.1, we show the feasible set Ω. In Figure 19.2, we show the feasible set for the ILP problem, which allows us to solve the problem graphically. The solution is obtained by finding the straight line f = 3x1 + 4x2 with largest f that passes through a feasible point with integer components. This can be accomplished by first drawing the line f = 3x1 + 4x2 for f = 0 and then gradually increasing the values of f, which corresponds to sliding across the feasible region until the straight line passes through the "last" integer feasible point yielding the largest value of the objective function. From Figure 19.2, we can see that the optimal solution to the ILP problem is [2,2].

Figure 19.1 Feasible set Ω for LP problem in Example 19.5.



Figure 19.2 Graphical solution for ILP problem in Example 19.5.


We now solve the problem using the Gomory cutting-plane method. First we represent the associated LP problem in standard form:

Note that we only need the first two components of the solution to be integers. We can start the simplex method because we have an obvious basic feasible solution. The first tableau is

We bring a2 into the basis and pivot about the element (1,2) to obtain

Next, we pivot about the element (2,1) to obtain

The corresponding optimal basic feasible solution is

which does not satisfy the integer constraints.
We start by introducing the Gomory cut corresponding to the first row of the tableau. We obtain

We add this constraint to our tableau:

Pivoting about the element (3,3) gives

The corresponding optimal basic feasible solution is [7/2,1,3/5,0,0], which still does not satisfy the integer constraint.
Next, we construct the Gomory cut for the second row of the tableau:

We add this constraint to our tableau to obtain

Pivoting about (4,4), we get

In this optimal basic feasible solution, the first two components are integers. Thus, we conclude that the solution to our ILP is [2,2], which agrees with the graphical solution in Figure 19.2.
In Example 19.5, the final solution to the LP problem after applying the Gomory cutting-plane method is not an integer vector. Only the first two components are integers, as these are the only two components in the original ILP problem. As pointed out earlier, the slack variables and variables introduced by the Gomory cuts are not constrained to be integers. However, if we are given an ILP problem with inequality constraints as in Example 19.5 but with only integer values in constraint data, then the slack variables and those introduced by the Gomory cuts are automatically integer valued (see also Exercise 19.9). We illustrate this in the following example.
Example 19.6 Consider the following ILP problem:

A graphical solution to this ILP problem is shown in Figure 19.3. As in Example 19.5, the solution is obtained by finding the straight line f = 3x1 + 4x2 with largest f that passes through a feasible point with integer components. This point is [5,4].

Figure 19.3 Graphical solution of the ILP problem in Example 19.6, where integer feasible solutions are marked with heavy dots.


We now solve the ILP problem above using the simplex method with Gomory cuts. We first represent the associated LP problem in standard form by introducing slack variables x3 and x4. The initial tableau has the form

In this case there is an obvious initial basic feasible solution available, which allows us to initialize the simplex method to solve the problem. After two iterations of the simplex algorithm, the final tableau is

with optimal solution

Both basic components are noninteger. Let us construct a Gomory cut for the first basic component x1* = 11/2. Prom the first row of the tableau, the associated constraint equation is

If we apply the floor operator to this equation as explained before, we get an inequality constraint

A graphical solution of the above problem after adding this inequality constraint to the original LP problem is shown in Figure 19.4. We can see that in this new problem, the first component of the optimal solution is an integer, but not the second. This means that a single Gomory cut will not suffice.

Figure 19.4 Graphical solution of the ILP in Example 19.6 after adding the constraint x1 ≤ 5 to the original constraints.


To continue with the Gomory procedure for the problem using the simplex method, we first write down the Gomory cut

We now obtain a new tableau by augmenting the previous tableau with the above constraint:

At this point, there is no obvious basic feasible solution. However, we can easily use the two-phase method. This yields

which has all nonnegative reduced cost coefficients. Hence, we obtain the optimal basic feasible solution

As expected, the second component does not satisfy the integer constraint.
Next, we write down the Gomory cut for the basic component x2* = 51/11 using the numbers in the second row of the tableau:

Updating the tableau gives

Again, there is no obvious basic feasible solution. Applying the two-phase method gives

The corresponding optimal basic feasible solution still does not satisfy the integer constraints; neither the first nor the second components are integer.
Next, we introduce the Gomory cut using the numbers in the second row of the previous tableau to obtain

Applying the two-phase method again gives

(Note that this basic feasible solution is degenerate—the corresponding basis is not unique.) The corresponding optimal basic feasible solution is

which satisfies the integer constraints. From this, we see that the integer optimal solution to the original ILP problem is [5,4], which agrees with our graphical solution in Figure 19.3.
In this example, we note that the final solution to LP problem after introducing slack variables and using the Gomory cutting-plane method is an integer vector. The reason for this, in contrast with Example 19.5, is that the original ILP inequality constraint data has only integers.
A linear programming problem in which not all of the components are required to be integers is called a mixed integer linear programming (MILP) problem. Gomory cuts are also relevant to solving MILP problems. In fact, Example 19.5 is an instance of an MILP problem, because the slack variables in the standard form of the problem are not constrained to be integers. Moreover, the cutting-plane idea also has been applied to nonsimplex methods and nonlinear programming algorithms.
For other methods for solving ILPs, see [119].
EXERCISES

19.1 Show that if A is totally unimodular, then so is any submatrix of it.
19.2 Show that if A is totally unimodular, then so is A.
19.3 Show that A is totally unimodular if and only [A, I] is totally unimodular. This result is stronger than Proposition 19.1.
19.4 Consider the matrix B in the proof of Proposition 19.1:

Show that det B = ±det Bk,k.
19.5 Consider the constraint

where A and b contain only integers. Suppose that we introduce the slack-variable vector z to obtain the equivalent constraint

Show that if z satisfies this constraint (with some x), then z is an integer vector.
19.6 Write a MATLAB program to generate Figures 19.1 and 19.2.
19.7 Consider the constraint in standard form Ax = b. Suppose that we augment this with a Gomory cut to obtain

Let xn+1 satisfy this constraint with an integer vector x. Show that xn+1 is an integer.
19.8 Consider the ILP problem in standard form

Show that if we use the Gomory cutting-plane method with the simplex algorithm, then the final optimal basic feasible solution, including the variables introduced by the Gomory method, is an integer vector. (Use Exercise 19.7.)
19.9 Consider the ILP problem

Suppose that we introduce slack variables to convert the problem into standard form, and we use the Gomory cutting-plane method with the simplex algorithm to solve the resulting problem. Show that the final optimal basic feasible solution, including the slack variables and the variables introduced by the Gomory method, is an integer vector. (Use Exercises 19.5 and 19.8.)
19.10 Use a graphical method to find an integer solution to the dual of the ILP problem in Example 19.5.

1 Thanks to David Schvartzman Cohenca for his solution to this problem.







PART IV
NONLINEAR CONSTRAINED OPTIMIZATION







CHAPTER 20
PROBLEMS WITH EQUALITY CONSTRAINTS
20.1 Introduction
In this part we discuss methods for solving a class of nonlinear constrained optimization problems that can be formulated as

where x  n, f : n → , hi : n → , gj : n → , and m ≥ n. In vector notation, the problem above can be represented in the following standard form:

where h : n → m and g : n → p. As usual, we adopt the following terminology.
Definition 20.1 Any point satisfying the constraints is called a feasible point. The set of all feasible points,

is called a feasible set.
Optimization problems of the above form are not new to us. Indeed, linear programming problems of the form

which we studied in Part III, are of this type.
As we remarked in Part II, there is no loss of generality by considering only minimization problems. For if we are confronted with a maximization problem, it can easily be transformed into the minimization problem by observing that

We illustrate the problems we study in this part by considering the following simple numerical example.
Example 20.1 Consider the following optimization problem:

This problem is already in the standard form given earlier, with f(x1, x2) = (x1 − 1)2 + x2 − 2, h(x1, x2) = x2 − x1 − 1, and g(x1, x2) = x1 + x2 − 2. This problem turns out to be simple enough to be solved graphically (see Figure 20.1). In the figure the set of points that satisfy the constraints (the feasible set) is marked by the heavy solid line. The inverted parabolas represent level sets of the objective function f—the lower the level set, the smaller the objective function value. Therefore, the solution can be obtained by finding the lowest-level set that intersects the feasible set. In this case, the minimizer lies on the level set with f = −1/4. The minimizer of the objective function is x* = [1/2,3/2].

Figure 20.1 Graphical solution to the problem in Example 20.1.


In the remainder of this chapter we discuss constrained optimization problems with only equality constraints. The general constrained optimization problem is discussed in the chapters to follow.
20.2 Problem Formulation
The class of optimization problems we analyze in this chapter is

where x  n, f : n → , h : n → m, h = [h1,..., hm], and m ≤ n. We assume that the function h is continuously differentiable, that is, h  ⊂1.
We introduce the following definition.
Definition 20.2 A point x* satisfying the constraints h1 (x*) = 0,..., hm(x*) = 0 is said to be a regular point of the constraints if the gradient vectors ∇h1(x*),..., ∇hm(x*) are linearly independent.
Let Dh(x*) be the Jacobian matrix of h = [h1,..., hm] at x*, given by

Then, x* is regular if and only if rank Dh(x*) = m (i.e., the Jacobian matrix is of full rank).
The set of equality constraints h1(x) = 0,..., hm(x) = 0, hi : n → , describes a surface

Assuming that the points in S are regular, the dimension of the surface S is n − m.
Example 20.2 Let n = 3 and m = 1 (i.e., we are operating in 3). Assuming that all points in S are regular, the set S is a two-dimensional surface. For example, let

Note that ∇h1(x) = [0,1, −2x3], and hence for any x  3, ∇h1(x) ≠ 0. In this case,

See Figure 20.2 for a graphical illustration.

Figure 20.2 Two-dimensional surface in 3.


Example 20.3 Let n = 3 and m = 2. Assuming regularity, the feasible set S is a one-dimensional object (i.e., a curve in 3). For example, let

In this case, ∇h1(x) = [1,0,0] and ∇h2(x) = [0, 1, −2x3]. Hence, the vectors ∇h1(x) and ∇h2(x) are linearly independent in 3. Thus,

See Figure 20.3 for a graphical illustration.

Figure 20.3 One-dimensional surface in 3.


20.3 Tangent and Normal Spaces
In this section we discuss the notion of a tangent space and normal space at a point on a surface. We begin by defining a curve on a surface S.
Definition 20.3 A curve C on a surface S is a set of points {x(t)  S : t  (a, b)}, continuously parameterized by t  (a, b); that is, x : (a, b) → S is a continuous function.
A graphical illustration of the definition of a curve is given in Figure 20.4. The definition of a curve implies that all the points on the curve satisfy the equation describing the surface. The curve C passes through a point x* if there exists t*  (a, b) such that x(t*) = x*.

Figure 20.4 Curve on a surface.


Intuitively, we can think of a curve C = {x(t) : t  (a, b)} as the path traversed by a point x traveling on the surface S. The position of the point at time t is given by x(t).
Definition 20.4 The curve C = {x(t) : t  (a, b)} is differentiable if

exists for all t  (a, b).
The curve C = {x(t) : t  (a, b)} is twice differentiable if

exists for all t  (a, b).
Note that both (t) and (t) are n-dimensional vectors. We can think of (t) and (t) as the velocity and acceleration, respectively, of a point traversing the curve C with position x(t) at time t. The vector (t) points in the direction of the instantaneous motion of x(t). Therefore, the vector (t*) is tangent to the curve C at x* (see Figure 20.5).

Figure 20.5 Geometric interpretation of the differentiability of a curve.


We are now ready to introduce the notions of a tangent space. For this recall the set

where h  ⊂1. We think of S as a surface in 1.
Definition 20.5 The tangent space at a point x* on the surface S = {x  n : h(x) = 0} is the set T(x*) = {y : Dh(x*)y = 0}.
Note that the tangent space T(x*) is the nullspace of the matrix Dh(x*):

The tangent space is therefore a subspace of n.
Assuming that x* is regular, the dimension of the tangent space is n − m, where m is the number of equality constraints hi(x*) = 0. Note that the tangent space passes through the origin. However, it is often convenient to picture the tangent space as a plane that passes through the point x*. For this, we define the tangent plane at x* to be the set

Figure 20.6 illustrates the notion of a tangent plane, and Figure 20.7, the relationship between the tangent plane and the tangent space.

Figure 20.6 Tangent plane to the surface S at the point x*.



Figure 20.7 Tangent spaces and planes in 2 and 3.


Example 20.4 Let

Then, S is the x3-axis in 3 (see Figure 20.8). We have

Because ∇h1 and ∇h2 are linearly independent when evaluated at any x  S, all the points of S are regular. The tangent space at an arbitrary point of S is

In this example, the tangent space T(x) at any point x  S is a one-dimensional subspace of 3.
Intuitively, we would expect the definition of the tangent space at a point on a surface to be the collection of all "tangent vectors" to the surface at that point. We have seen that the derivative of a curve on a surface at a point is a tangent vector to the curve, and hence to the surface. The intuition above agrees with our definition whenever x* is regular, as stated in the theorem below.
Theorem 20.1 Suppose that x*  S is a regular point and T(x*) is the tangent space at x*. Then, y  T(x*) if and only if there exists a differentiable curve in S passing through x* with derivative y at x*.
Proof. ⇐: Suppose that there exists a curve {x(t) : t  (a, b)} in S such that x(t*) = x* and x(t*) = y for some t*  (a, b). Then,


Figure 20.8 The surface S = {x  3 : x1 = 0, x1 − x2 = 0}.


for all t  (a, b). If we differentiate the function h(x(t)) with respect to t using the chain rule, we obtain

for all t  (a, b). Therefore, at t* we get

and hence y  T(x*).
⇒: To prove this, we need to use the implicit function theorem. We refer the reader to [88, p. 325].
We now introduce the notion of a normal space.
Definition 20.6 The normal space N(x*) at a point x* on the surface S = {x  n : h(x) = 0} is the set .
We can express the normal space N(x*) as

that is, the range of the matrix Dh(x*). Note that the normal space N(x*) is the subspace of n spanned by the vectors ∇h1(x*), ..., ∇hm(x*); that is,

Note that the normal space contains the zero vector. Assuming that x* is regular, the dimension of the normal space N(x*) is m. As in the case of the tangent space, it is often convenient to picture the normal space N(x*) as passing through the point x* (rather than through the origin of n). For this, we define the normal plane at x* as the set

Figure 20.9 illustrates the normal space and plane in 3 (i.e., n − 3 and m = 1).

Figure 20.9 Normal space in 3.


We now show that the tangent space and normal space are orthogonal complements of each other (see Section 3.3).
Lemma 20.1 We have T(x*) = N(x*)⊥ and T(x*)⊥ = N(x*).
Proof. By definition of T(x*), we may write

Hence, by definition of N(x*), we have T(x*) = N(x*)⊥. By Exercise 3.11 we also have T(x*)⊥ = N(x*).
By Lemma 20.1, we can write n as the direct sum decomposition (see Section 3.3):

that is, given any vector v  n, there are unique vectors   N(x*) and y  T(x*) such that

20.4 Lagrange Condition
In this section we present a first-order necessary condition for extremum problems with constraints. The result is the well-known Lagrange's theorem. To better understand the idea underlying this theorem, we first consider functions of two variables and only one equality constraint. Let h : 2 →  be the constraint function. Recall that at each point x of the domain, the gradient vector ∇h(x) is orthogonal to the level set that passes through that point. Indeed, let us choose a point  such that h(x*) = 0, and assume that ∇h(x*) ≠ 0. The level set through the point x* is the set {x : h(x) = 0}. We then parameterize this level set in a neighborhood of x* by a curve {x(t)}, that is, a continuously differentiate vector function x :  → 2 such that

We can now show that ∇h(x*) is orthogonal to (t*). Indeed, because h is constant on the curve {x(t) : t  (a, b)}, we have that for all t  (a, b),

Hence, for all t  (a, b),

Applying the chain rule, we get

Therefore, ∇h(x*) is orthogonal to (t*).
Now suppose that x* is a minimizer of f : 2 →  on the set {x : h(x) = 0}. We claim that ∇f(x*) is orthogonal to (t*). To see this, it is enough to observe that the composite function of t given by

achieves a minimum at t*. Consequently, the first-order necessary condition for the unconstrained extremum problem implies that

Applying the chain rule yields

Thus, ∇f(x*) is orthogonal to (t*). The fact that (t*) is tangent to the curve {x(t)} at x* means that ∇f(x*) is orthogonal to the curve at x* (see Figure 20.10).

Figure 20.10 The gradient ∇f(x*) is orthogonal to the curve {x(t)} at the point x* that is a minimizer of f on the curve.


Recall that ∇h(x*) is also orthogonal to (t*). Therefore, the vectors ∇h(x*) and ∇f(x*) are parallel; that is, ∇f(x*) is a scalar multiple of ∇f(x*). The observations above allow us now to formulate Lagrange's theorem for functions of two variables with one constraint.
Theorem 20.2 Lagrange's Theorem for n = 2, m = 1. Let the point x* be a minimizer of f : 2 →  subject to the constraint h(x) = 0, h : 2 → . Then, ∇f(x*) and ∇h(x*) are parallel. That is, if ∇h(x*) ≠ 0, then there existe a scalar λ* such that

In Theorem 20.2, we refer to λ* as the Lagrange multiplier. Note that the theorem also holds for maximizers. Figure 20.11 gives an illustration of Lagrange's theorem for the case where x* is a maximizer of f over the set {x : h(x) = 0}.

Figure 20.11 Lagrange's theorem for n = 2, m = 1.


Lagrange's theorem provides a first-order necessary condition for a point to be a local minimizer. This condition, which we call the Lagrange condition, consists of two equations:

Note that the Lagrange condition is necessary but not sufficient. In Figure 20.12 we illustrate a variety of points where the Lagrange condition is satisfied, including a case where the point is not an extremizer (neither a maximizer nor a minimizer).

Figure 20.12 Four examples where the Lagrange condition is satisfied: (a) maximizer, (b) minimizer, (c) minimizer, (d) not an extremizer.
(Adapted from [120].)


We now generalize Lagrange's theorem for the case when f : n →  and h : n → m, m ≤ n.
Theorem 20.3 Lagrange's Theorem. Let x* be a local minimizer (or maximizer) of f : n → , subject to h(x) = 0, h : n → m, m ≤ n. Assume that x* is a regular point. Then, there exists λ*  m such that

Proof. We need to prove that

for some λ*  m; that is, ∇f(x*)  (Dh(x*)) = N(x*). But by Lemma 20.1, N(x*) = T(x*)⊥. Therefore, it remains to show that ∇f(x*)  T(x*)⊥.
We proceed as follows. Suppose that

Then, by Theorem 20.1, there exists a differentiable curve {x(t) : t  (a, b)} such that for all t  (a, b),

and there exists t*  (a, b) satisfying

Now consider the composite function φ(t) = f(x(t)). Note that t* is a local minimizer of this function. By the first-order necessary condition for unconstrained local minimizers (see Theorem 6.1),

Applying the chain rule yields

So all y  T(x*) satisfy

that is,

This completes the proof.
Lagrange's theorem states that if x* is an extremizer, then the gradient of the objective function f can be expressed as a linear combination of the gradients of the constraints. We refer to the vector λ* in Theorem 20.3 as the Lagrange multiplier vector, and its components as Lagrange multipliers.
From the proof of Lagrange's theorem, we see that a compact way to write the necessary condition is ∇f(x*)  N(x*). If this condition fails, then x* cannot be an extremizer. This situation is illustrated in Figure 20.13.

Figure 20.13 Example where the Lagrange condition does not hold.


Notice that regularity is stated as an assumption in Lagrange's theorem. This assumption plays an essential role, as illustrated in the following example.
Example 20.5 Consider the following problem:

where f(x) = x and

The feasible set is evidently [0,1]. Clearly, x* = 0 is a local minimizer. However, f′(x*) = 1 and h′(x*) = 0. Therefore, x* does not satisfy the necessary condition in Lagrange's theorem. Note, however, that x* is not a regular point, which is why Lagrange's theorem does not apply here.
It is convenient to introduce the Lagrangian function l : n × m → , given by

The Lagrange condition for a local minimizer x* can be represented using the Lagrangian function as

for some λ*, where the derivative operation D is with respect to the entire argument [x, λ]. In other words, the necessary condition in Lagrange's theorem is equivalent to the first-order necessary condition for unconstrained optimization applied to the Lagrangian function.
To see the above, denote the derivative of l with respect to x as Dxl and the derivative of l with respect to λ as Dλl. Then,

Note that Dxl(x, λ) = Df(x) + λ Dh(x) and Dxl(x, λ) = h(x). Therefore, Lagrange's theorem for a local minimizer x* can be stated as

for some λ*, which is equivalent to

In other words, the Lagrange condition can be expressed as Dl(x*, λ*) = 0.
The Lagrange condition is used to find possible extremizers. This entails solving the equations

The above represents n + m equations in n + m unknowns. Keep in mind that the Lagrange condition is necessary but not sufficient; that is, a point x* satisfying the equations above need not be an extremizer.
Example 20.6 Given a fixed area of cardboard, we wish to construct a closed cardboard box with maximum volume. We can formulate and solve this problem using the Lagrange condition. Denote the dimensions of the box with maximum volume by x1, x2, and x3, and let the given fixed area of cardboard be A. The problem can then be formulated as

We denote f(x) = −x1x2x3 and h(x) = x1x2 + x2x3 + x3x1 − A/2. We have ∇f(x) = −[x2x3, x1x3, x1x2] and ∇h(x) = [x2 + x3, x1 + x3, x1 + x2]. Note that all feasible points are regular in this case. By the Lagrange condition, the dimensions of the box with maximum volume satisfies

where λ  .
We now solve these equations. First, we show that that x1, x2, x3, and λ are all nonzero. Suppose that x1 = 0. By the constraints, we have x2x3 = A/2. However, the second and third equations in the Lagrange condition yield λx2 = λx3 = 0, which together with the first equation implies that x2x3 = 0. This contradicts the constraints. A similar argument applies to x2 and x3.
Next, suppose that λ = 0. Then, the sum of the three Lagrange equations gives x2x3 + x1x3 + x1x2 = 0, which contradicts the constraints.
We now solve for x1, x2, and x3 in the Lagrange equations. First, multiply the first equation by x1 and the second by x2, and subtract one from the other. We arrive at x3λ(x1 − x2) = 0. Because neither x3 nor λ can be zero (by part b), we conclude that x1 − x2. We similarly deduce that x2 = x3. From the constraint equation, we obtain x1 = x2 − x3 = .
Notice that we have ignored the constraints that x1, x2, and x3 are positive so that we can solve the problem using Lagrange's theorem. However, there is only one solution to the Lagrange equations, and the solution is positive. Therefore, if a solution exists for the problem with positivity constraints on the variables x1, x2, and x3, then this solution must necessarily be equal to the solution above obtained by ignoring the positivity constraints.
Next we provide an example with a quadratic objective function and a quadratic constraint.
Example 20.7 Consider the problem of extremizing the objective function

on the ellipse

We have

Thus,

and

Setting Dxl(x, λ) = 0 and Dλl(x, λ) = 0, we obtain three equations in three unknowns

All feasible points in this problem are regular. Prom the first of the equations above, we get either x1 = 0 or λ = −1. For the case where x1 = 0, the second and third equations imply that λ = −1/2 and x2 = ±1/. For the case where λ = −1, the second and third equations imply that x1 = ±1 and x2 = 0. Thus, the points that satisfy the Lagrange condition for extrema are

Because

and

we conclude that if there are minimizers, then they are located at x(1) and x(2) and if there are maximizers, then they are located at x(3) and x(4). It turns out that, indeed, x(1) and x(2) are minimizers and x(3) and x(4) are maximizers. This problem can be solved graphically, as illustrated in Figure 20.14.

Figure 20.14 Graphical solution of the problem in Example 20.7.


In the example above, both the objective function f and the constraint function h are quadratic functions. In the next example we take a closer look at a class of problems where both the objective function f and the constraint h are quadratic functions of n variables.
Example 20.8 Consider the following problem:

where Q = Q ≥ 0 and P = P > 0. Note that if a point x = [x1, ..., xn] is a solution to the problem, then so is any nonzero scalar multiple of it,

Indeed,

Therefore, to avoid the multiplicity of solutions, we further impose the constraint

The optimization problem becomes

Let us write

Any feasible point for this problem is regular (see Exercise 20.13). We now apply Lagrange's method. We first form the Lagrangian function

Applying the Lagrange condition yields

The first of the equations above can be represented as

or

This representation is possible because P = P and Q = Q. By assumption P > 0, hence P−1 exists. Premultiplying (λP − Q)x − 0 by P−1, we obtain

or, equivalently,

Therefore, the solution, if it exists, is an eigenvector of P−1Q, and the Lagrange multiplier is the corresponding eigenvalue. As usual, let x* and λ* be the optimal solution. Because x*Px* = 1 and P−1 Qx* = λ*x*, we have

Hence, λ* is the maximum of the objective function, and therefore is, in fact, the maximal eigenvalue of P−1Q. It is also called the maximal generalized eigenvalue.
In the problems above, we are able to find points that are candidates for extremizers of the given objective function subject to equality constraints. These critical points are the only candidates because they are the only points that satisfy the Lagrange condition. To classify such critical points as minimizers, maximizers, or neither, we need a stronger condition—possibly a necessary and sufficient condition. In the next section we discuss a second-order necessary condition and a second-order sufficient condition for minimizers.
20.5 Second-Order Conditions
We assume that f : n →  and h : n → m are twice continuously differentiable: f, h  C2. Let

be the Lagrangian function. Let L(x, λ) be the Hessian matrix of l(x, λ) with respect to x:

where F(x) is the Hessian matrix of f at x and Hk(x) is the Hessian matrix of hk at x, k = 1, ..., m, given by

We introduce the notation [λH(x)]:

Using the notation above, we can write

Theorem 20.4 Second-Order Necessary Conditions. Let x* be a local minimizer of f : n →  subject to h(x) = 0, h : n → m, m ≤ n, and f, h  ⊂2. Suppose that x* is regular. Then, there exists λ*  m such that:

1. Df(x*) + λ* Dh(x*) = 0.
2. For all y  T(x*), we have y L(x*, λ*)y ≥ 0.

Proof. The existence of λ*  m such that Df(x*) + λ* Dh(x*) = 0 follows from Lagrange's theorem. It remains to prove the second part of the result. Suppose that y  T(x*); that is, y belongs to the tangent space to S = {x  n : h(x) = 0} at x*. Because h  ⊂2, following the argument of Theorem 20.1, there exists a twice-differentiable curve {x(t) :t  (a,b)}on S such that

for some t*  (a, b). Observe that by assumption, t* is a local minimizer of the function φ(t) = f(x(t)). From the second-order necessary condition for unconstrained minimization (see Theorem 6.2), we obtain

Using the formula

and applying the chain rule yields

Because h(x(t)) = 0 for all t  (a, b), we have

Thus, for all t  (a, b),

In particular, the above is true for t = t*; that is,

Adding this equation to the inequality

yields

But, by Lagrange's theorem, Df(x*) + λ* Dh(x*) = 0. Therefore,

which proves the result.
Observe that L(x, λ) plays a similar role as the Hessian matrix F(x) of the objective function f did in the unconstrained minimization case. However, we now require that L(x*, λ*) ≥ 0 only on T(x*) rather than on n.
The conditions above are necessary, but not sufficient, for a point to be a local minimizer. We now present, without a proof, sufficient conditions for a point to be a strict local minimizer.
Theorem 20.5 Second-Order Sufficient Conditions. Suppose that f, h  ⊂2 and there exists a point x*  n and λ*  m such that:

1. Df(x*) + λ* Dh(x*) = 0.
2. For all y  T(x*), y ≠ 0, we have y L(x*, λ*)y > 0.

Then, x* is a strict local minimizer of f subject to h(x) = 0.
Proof. The interested reader can consult [88, p. 334] for a proof of this result.
Theorem 20.5 states that if an x* satisfies the Lagrange condition, and L(x*, λ*) is positive definite on T(x*), then x* is a strict local minimizer. A similar result to Theorem 20.5 holds for a strict local maximizer, the only difference being that L(x*, λ*) be negative definite on T(x*). We illustrate this condition in the following example.
Example 20.9 Consider the following problem:

where

As pointed out earlier, we can represent this problem in the equivalent form

The Lagrangian function for the transformed problem is given by

The Lagrange condition yields

where

There are only two values of λ that satisfy (λI − P−1 Q)x = 0, namely, the eigenvalues of P−1 Q: λ1 = 2, λ2 = 1. We recall from our previous discussion of this problem that the Lagrange multiplier corresponding to the solution is the maximum eigenvalue of P−1 Q, namely, λ* = λ1 = 2. The corresponding eigenvector is the maximizer—the solution to the problem. The eigenvector corresponding to the eigenvalue λ* = 2 satisfying the constraint x Px = 1 is ±x*, where

At this point, all we have established is that the pairs (±x*, λ*) satisfy the Lagrange condition. We now show that the points ±x* are, in fact, strict local maximizers. We do this for the point x*. A similar procedure applies to −x*. We first compute the Hessian matrix of the Lagrangian function. We have

The tangent space T(x*) to {x : 1 − x Px = 0} is

Note that for each y  T(x*), y ≠ 0,

Hence, L(x*, λ*) < 0 on T(x*), and thus x* = [1/, 0] is a strict local maximizer. The same is true for the point −x*. Note that

which, as expected, is the value of the maximal eigenvalue of P−1 Q. Finally, we point out that any scalar multiple tx* of x*, t ≠, 0, is a solution to the original problem of maximizing x Qx/x Px.
20.6 Minimizing Quadratics Subject to Linear Constraints
Consider the problem

where Q > 0, A  mxn, m < n, rank A = m. This problem is a special case of what is called a quadratic programming problem (the general form of a quadratic programming problem includes the constraint x ≥ 0). Note that the constraint set contains an infinite number of points (see Section 2.3). We now show, using Lagrange's theorem, that there is a unique solution to the optimization problem above. Following that, we provide an example illustrating the application of this solution to an optimal control problem.
To solve the problem, we first form the Lagrangian function

The Lagrange condition yields

Rewriting, we get

Premultiplying both sides of the above by A gives

Using the fact that Ax* = b, and noting that AQ−1 A is invertible because Q > 0 and rank A = m, we can solve for λ* to obtain

Therefore, we obtain

The point x* is the only candidate for a minimizer. To establish that x* is indeed a minimizer, we verify that x* satisfies the second-order sufficient conditions. For this, we first find the Hessian matrix of the Lagrangian function at (x*, λ*). We have

which is positive definite. Thus, the point x* is a strict local minimizer. We will see in Chapter 22 that x* is, in fact, a global minimizer.
The special case where Q = In, the n × n identity matrix, reduces to the problem considered in Section 12.3. Specifically, the problem in Section 12.3 is to minimize the norm ||x|| subject to Ax = b. The objective function here is f(x) = ||x||, which is not differentiate at x = 0. This precludes the use of Lagrange's theorem because the theorem requires differentiability of the objective function. We can overcome this difficulty by considering an equivalent optimization problem:

The objective function ||x||2/2 has the same minimizer as the previous objective function ||x||. Indeed, if x* is such that for all x  n satisfying Ax = b, ||x*|| ≤ ||x||, then ||x*||2/2 ≤ ||x||2/2. The same is true for the converse. Because the problem of minimizing ||x||2/2 subject to Ax = b is simply the problem considered above with Q = In, we easily deduce the solution to be x* = A (AA)−1 b, which agrees with the solution in Section 12.3.
Example 20.10 Consider the discrete-time linear system model

with initial condition x0 given. We can think of {xk} as a discrete-time signal that is controlled by an external input signal {uk}. In the control literature, xk is called the state at time k. For a given x0, our goal is to choose the control signal {uk} so that the state remains "small" over a time interval [1, N], but at the same time the control signal is "not too large." To express the desire to keep the state {xk} small, we choose the control sequence to minimize

On the other hand, maintaining a control signal that is not too large, we minimize

The two objectives above are conflicting in the sense that they cannot, in general, be achieved simultaneously—minimizing the first may result in a large control effort, while minimizing the second may result in large states. This is clearly a problem that requires compromise. One way to approach the problem is to minimize a weighted sum of the two functions above. Specifically, we can formulate the problem as

where the parameters q and r reflect the relative importance of keeping the state small versus keeping the control effort not too large. This problem is an instance of the linear quadratic regulator (LQR) problem (see, e.g., [15], [20], [85], [86], or [99]). Combining the two conflicting objectives of keeping the state small while keeping the control effort small is an instance of the weighted sum approach (see Section 24.4).
To solve the problem above, we can rewrite it as a quadratic programming problem. Define

With these definitions, the problem reduces to the previously considered quadratic programming problem,

where Q is 2N × 2N, A is N × 2N, and b  N. The solution is

The first N components of z* represent the optimal state signal in the interval [1, N], whereas the second N components represent the optimal control signal.
In practice, computation of the matrix inverses in the formula for z above may be too costly. There are other ways to tackle the problem by exploiting its special structure. This is the study of optimal control (see, e.g., [15], [20], [85], [86], or [99]).
The following example illustrates an application of the above discussion.
Example 20.11 Credit-Card Holder Dilemma. Suppose that we currently have a credit-card debt of $10,000. Credit-card debts are subject to a monthly interest rate of 2%, and the account balance is increased by the interest amount every month. Each month we have the option of reducing the account balance by contributing a payment to the account. Over the next 10 months, we plan to contribute a payment every month in such a way as to minimize the overall debt level while minimizing the hardship of making monthly payments.
We solve our problem using the LQR framework described in Example 20.10. Let the current time be 0, xk the account balance at the end of month k, and uk our payment in month k. We have

that is, the account balance in a given month is equal to the account balance in the previous month plus the monthly interest on that balance minus our payment that month. Our optimization problem is then

which is an instance of the LQR problem. The parameters q and r reflect our priority in trading off between debt reduction and hardship in making payments. The more anxious we are to reduce our debt, the larger the value of q relative to r. On the other hand, the more reluctant we are to make payments, the larger the value of r relative to q.
The solution to the problem above is given by the formula derived in Example 20.10. In Figure 20.15 we plot the monthly account balances and payments over the next 10 months using q = 1 and r = 10. We can see here that our debt has been reduced to less than $1000 after 10 months, but with a first payment close to $3000. If we feel that a payment of $3000 is too high, then we can try to reduce this amount by increasing the value of r relative to q. However, going too far along these lines can lead to trouble. Indeed, if we use q = 1 and r = 300 (see Figure 20.16), although the monthly payments do not exceed $400, the account balance is never reduced by much below $10,000. In this case, the interest on the account balance eats up a significant portion of our monthly payments. In fact, our debt after 10 months will be higher than $10,000.

Figure 20.15 Plots for Example 20.11 with q = 1 and r = 10.



Figure 20.16 Plots for Example 20.11 with q = 1 and r = 300.


For a treatment of optimization problems with quadratic objective functions, subject to linear or quadratic constraints, arising in communication and signal processing, see [105] and [106].
EXERCISES

20.1 Consider the following constraints on 2:

Find the set of feasible points. Are the feasible points regular? Justify your answer.
20.2 Find local extremizers for the following optimization problems:
a. 
b. 
c. 
20.3 Find minimizers and maximizers of the function

subject to

where

20.4 Consider the problem

where f : 2 → , h : 2 → , and ∇f(x) = [x1, x1 + 4]. Suppose that x* is an optimal solution and ∇h(x*) = [1, 4]. Find ∇f(x*).
20.5 Consider the problem

where 
a. Find all points satisfying the Lagrange condition for the problem.
b. Using second-order conditions, determine whether or not each of the points in part a is a local minimizer.
20.6 We wish to construct a closed box with minimum surface area that encloses a volume of V cubic feet, where V > 0.
a. Let a, b, and c denote the dimensions of the box with minimum surface area (with volume V). Derive the Lagrange condition that must be satisfied by a, b, and c.
b. What does it mean for a point x* to be a regular point in this problem? Is the point x* = [a, b, c] a regular point?
c. Find a, b, and c.
d. Does the point x* = [a, b, c] found in part c satisfy the second-order sufficient condition?
20.7 Find local extremizers of
a.  = 16.
b.  = 140.
20.8 Consider the problem

a. Use Lagrange's theorem to find all possible local minimizers and maximizers.
b. Use the second-order sufficient conditions to specify which points are strict local minimizers and which are strict local maximizers.
c. Are the points in part b global minimizers or maximizers? Explain.
20.9 Find all maximizers of the function

20.10 Find all solutions to the problem

20.11 Consider a matrix A with the property that A A has eigenvalues ranging from 1 to 20 (i.e., the smallest eigenvalue is 1 and the largest is 20). Let x be a vector such that ||x|| = 1, and let y = Ax. Use Lagrange multiplier methods to find the range of values that ||y|| can take.
Hint: What is the largest value that ||y|| can take? What is the smallest value that ||y|| can take?
20.12 Consider a matrix A  mxn. Define the induced 2-norm of A, denoted ||A||2, to be the number

where the norm ||·|| on the right-hand side above is the usual Euclidean norm.
   Suppose that the eigenvalues of A A are λ1,..., λn (ordered from largest to smallest). Use Lagrange's theorem to express ||A||2 in terms of the eigenvalues above (cf. Theorem 3.8).
20.13 Let P = P be a positive definite matrix. Show that any point x satisfying 1 − x Px = 0 is a regular point.
20.14 Consider the problem

where a, b  . Show that if [1, 1] is a solution to the problem, then a = b.
20.15 Consider the problem

a. Apply Lagrange's theorem directly to the problem to show that if a solution exists, it must be either [1,1] or [-1, 1].
b. Use the second-order necessary conditions to show that [-1, 1] cannot possibly be the solution.
c. Use the second-order sufficient conditions to show that [1, 1] is a strict local minimizer.
20.16 Let A  mxn, m ≤ n, rank A = m, and x0  n. Let x* be the point on the nullspace of A that is closest to x0 (in the sense of Euclidean norm).
a. Show that x* is orthogonal to x* − x0.
b. Find a formula for x* in terms of A and x0.
20.17 Consider the problem

where A  mxn, m > n, C  pxn, p < n, and both A and C are of full rank. We wish to find an expression for the solution (in terms of A, b, C, and d).
a. Apply Lagrange's theorem to solve this problem.
b. As an alternative, rewrite the given optimization problem in the form of a quadratic programming problem and apply the formula in Section 20.6 to obtain the solution.
20.18 Consider the problem of minimizing a general quadratic function subject to a linear constraint:

where Q = Q > 0, A  mxn, m < n, rank A = m, and d is a constant. Derive a closed-form solution to the problem.
20.19 Let L be an n × n real symmetric matrix, and let  be a subspace of n with dimension m < n. Let {b1,..., bm} ⊂ n be a basis for , and let B be the n × m matrix with bi as the ith column. Let L be the m × m matrix defined by L = B LB. Show that L is positive semidefinite (definite) on  if and only if L is positive semidefinite (definite).
Note: This result is useful for checking that the Hessian of the Lagrangian function at a point is positive definite on the tangent space at that point.
20.20 Consider the sequence {xk}, xk  , generated by the recursion

where u0, u1, u2,... is a sequence of "control inputs," and the initial condition x0 ≠ 0 is given. The recursion above is also called a discrete-time linear system. We wish to find values of control inputs u0 and u1 such that x2 = 0, and the average input energy (u20 + u21)/2 is minimized. Denote the optimal inputs by u0* and u1*.
a. Find expressions for u0* and u1* in terms of a, b, and x0.
b. Use the second-order sufficient conditions to show that the point u* = [u0*, u1*] in part a is a strict local minimizer.
20.21 Consider the discrete-time linear system xk = 2xk-1 + uk, k ≥ 1, with x0 = 1. Find the values of the control inputs u1 and u2 to minimize

20.22 Consider the discrete-time linear system xk+1 = xk + 2uk, 0 ≤ k ≤ 2, with x0 = 3. Use the Lagrange multiplier approach to calculate the optimal control sequence {u0, u1, u3} that transfers the initial state x0 to x3 = 9 while minimizing









CHAPTER 21
PROBLEMS WITH INEQUALITY CONSTRAINTS
21.1 Karush-Kuhn-Tucker Condition
In Chapter 20 we analyzed constrained optimization problems involving only equality constraints. In this chapter we discuss extremum problems that also involve inequality constraints. The treatment in this chapter parallels that of Chapter 20. In particular, as we shall see, problems with inequality constraints can also be treated using Lagrange multipliers.
We consider the following problem:

where f : n →, h : n → m, m ≤ n, and g: n → p. For the general problem above, we adopt the following definitions.
Definition 21.1 An inequality constraint gj(x) ≤ 0 is said to be active at x* if gj(x*) = 0. It is inactive at x* if gj(x*) < 0.
By convention, we consider an equality constraint hi(x) = 0 to be always active.
Definition 21.2 Let x* satisfy h(x*) = 0, g(x*) ≤ 0, and let J(x*) be the index set of active inequality constraints:

Then, we say that x* is a regular point if the vectors

are linearly independent.
We now prove a first-order necessary condition for a point to be a local minimizer. We call this condition the Karush-Kuhn-Tucker (KKT) condition. In the literature, this condition is sometimes also called the Kuhn-Tucker condition.
Theorem 21.1 Karush-Kuhn-Tucker (KKT) Theorem. Let f, h, g  1. Let x* be a regular point and a local minimizer for the problem of minimizing f subject to h(x) = 0, g(x) ≤ 0. Then, there exist λ*  m and μ*  p such that:

1. μ* ≥ 0.
2. Df(x*) + λ* Dh(x*) + μ* Dg(x*) = 0.
3. μ* g(x*) = 0.

In Theorem 21.1, we refer to λ* as the Lagrange multiplier vector and μ* as the Karush-Kuhn-Tucker (KKT) multiplier vector. We refer to their components as Lagrange multipliers and Karush-Kuhn-Tucker (KKT) multipliers, respectively.
Before proving this theorem, let us first discuss its meaning. Observe that μj* ≥ 0 (by condition 1) and gj(x*) ≤ 0. Therefore, the condition

implies that if gj(x*) < 0, then μj* = 0; that is, for all j ∉ J(x*), we have μj*. = 0. In other words, the KKT multipliers μj* corresponding to inactive constraints are zero. The other KKT multipliers, μi*, i  J(x*), are nonnegative; they may or may not be equal to zero.
Example 21.1 A graphical illustration of the KKT theorem is given in Figure 21.1. In this two-dimensional example, we have only inequality constraints gj(x) ≤ 0 j = 1, 2, 3. Note that the point x* in the figure is indeed a minimizer. The constraint g3(x) ≤ 0 is inactive: g3(x*) < 0; hence μ3* = 0. By the KKT theorem, we have

Figure 21.1 Illustration of the Karush-Kuhn-Tucker (KKT) theorem.



or, equivalently,

where μ*1 > 0 and μ*2 > 0. It is easy to interpret the KKT condition graphically for this example. Specifically, we can see from Figure 21.1 that ∇ f(x*) must be a linear combination of the vectors − ∇g1(x*) and − ∇g2(x*) with positive coefficients. This is reflected exactly in the equation above, where the coefficients μ*1 and μ*2 are the KKT multipliers.
We apply the KKT condition in the same way that we apply any necessary condition. Specifically, we search for points satisfying the KKT condition and treat these points as candidate minimizers. To summarize, the KKT condition consists of five parts (three equations and two inequalities):

1. μ* ≥ 0.
2. Df(x*) + λ* Dh(x*) + μ* Dg(x*) = 0.
3. μ* g(x*) = 0.
4. h(x*) = 0.
5. g(x*) ≤ 0.

We now prove the KKT theorem.
Proof of the Karush-Kuhn-Tucker Theorem. Let x* be a regular local minimizer of f on the set {x : h(x) = 0, g(x) ≤ 0}. Then, x* is also a regular local minimizer of f on the set {x : h(x) = 0, gj(x) = 0, j  J(x*)} (see Exercise 21.16). Note that the latter constraint set involves only equality constraints. Therefore, from Lagrange's theorem, it follows that there exist vectors λ*  m and μ*  p such that

where for all j ∉ J(x*), we have μj* = 0. To complete the proof it remains to show that for all j  J(x*), we have μ*j ≥ 0 (and hence for all j = 1, ..., p, we have μ*j ≥ 0, i.e., μ* ≥ 0). We use a proof by contradiction. So suppose that there exists j  J(x*) such that μ*j < 0. Let  and (x*) be the surface and tangent space defined by all other active constraints at x*. Specifically,

and

We claim that by the regularity of x*, there exists y  (x*) such that

To see this, suppose that for all y  (x*), ∇gj(x*) y = Dgj(x*)y = 0. This implies that ∇gj(x*)  (x*). By Lemma 20.1, this, in turn, implies that

But this contradicts the fact that x* is a regular point, which proves our claim. Without loss of generality, we assume that we have y such that Dgj(x*)y < 0.
Consider the Lagrange condition, rewritten as

If we postmultiply the above by y and use the fact that y  (x*), we get

Because Dgj(x*)y < 0 and we have assumed that μ*j < 0, we have

Because y  (x*), by Theorem 20.1 we can find a differentiable curve {x(t) : t  (a, b)} on  such that there exists t*  (a, b) with x(t*) = x* and (t*) = y. Now,

The above means that there is a δ > 0 such that for all t  (t*, t* + δ], we have

On the other hand,

and for some ε > 0 and all t  [t*, t* + ε], we have that gj(x(t)) ≤ 0. Therefore, for all t  (t*, t* + min{δ, ε}], we have that gj(x (t)) ≤ 0 and f(x(t)) < f(x*). Because the points x(t), t  (t*, t* + min{δ, ε}], are in , they are feasible points with lower objective function values than x*. This contradicts the assumption that x* is a local minimizer, which completes the proof.
Example 21.2 Consider the circuit in Figure 21.2. Formulate and solve the KKT condition for the following problems.

Figure 21.2 Circuit in Example 21.2.



a. Find the value of the resistor R ≥ 0 such that the power absorbed by this resistor is maximized.
b. Find the value of the resistor R ≥ 0 such that the power delivered to the 10-Ω resistor is maximized.

Solution:

a. The power absorbed by the resistor R is p = i2R, where . The optimization problem can be represented as

The derivative of the objective function is

Thus, the KKT condition is

We consider two cases. In the first case, suppose that μ > 0. Then, R = 0. But this contradicts the first condition above. Now suppose that μ = 0. Then, by the first condition, we have R = 10. Therefore, the only solution to the KKT condition is R = 10, μ = 0.
b. The power absorbed by the 10-Ω resistor is p = i210, where i = 20/(10 + R). The optimization problem can be represented as

The derivative of the objective function is

Thus, the KKT condition is

As before, we consider two cases. In the first case, suppose that μ > 0. Then, R = 0, which is feasible. For the second case, suppose that μ = 0. But this contradicts the first condition. Therefore, the only solution to the KKT condition is R = 0, μ = 8.

In the case when the objective function is to be maximized, that is, when the optimization problem has the form

the KKT condition can be written as

1. μ* ≥ 0.
2. Df(x*) + λ*Dh(x*) + μ* Dg(x*) = 0.
3. μ*g(x*) = 0.
4. h(x*) = 0.
5. g(x*) ≤ 0.

The above is easily derived by converting the maximization problem above into a minimization problem, by multiplying the objective function by −1. It can be further rewritten as

1. μ* < 0.
2. Df(x*) + λ*Dh(x*) + μ* Dg(x*) = 0.
3. μ* g(x*) = 0.
4. h(x*) = 0.
5. g(x*) ≤ 0.

The form shown above is obtained from the preceding one by changing the signs of μ* and λ* and multiplying condition 2 by −1.
We can similarly derive the KKT condition for the case when the inequality constraint is of the form g(x) ≥ 0. Specifically, consider the problem

We multiply the inequality constraint function by −1 to obtain − g(x) ≤ 0. Thus, the KKT condition for this case is

1. μ* ≥ 0.
2. Df(x*) + λ* Dh(x*) − μ* Dg(x*) = 0.
3. μ* g(x*) = 0.
4. h(x*) = 0.
5. g(x*) ≥ 0.

Changing the sign of μ* as before, we obtain

1. μ* ≤ 0.
2. Df(x*) + λ* Dh(x*) + μ* Dg(x*) = 0.
3. μ* g(x*) = 0.
4. h(x*) = 0.
5. g(x*) ≥ 0.

For the problem

the KKT condition is exactly the same as in Theorem 21.1, except for the reversal of the inequality constraint.
Example 21.3 In Figure 21.3, the two points x1 and x2 are feasible points; that is, g(x1) ≥ 0 and g(x2) ≥ 0, and they satisfy the KKT condition.

Figure 21.3 Points satisfying the KKT condition (x1 is a maximizer and x2 is a minimizer).


The point x1 is a maximizer. The KKT condition for this point (with KKT multiplier μ1) is

1. μ1 ≥ 0.
2. ∇ f(x1) + μ1 ∇g(x1) = 0.
3. μ1g(x1) = 0.
4. g(x1) ≥ 0.

The point x2 is a minimizer of f. The KKT condition for this point (with KKT multiplier μ2) is

1. μ2 ≤ 0.
2. ∇f(x2) + μ2 ∇g(x2) = 0.
3. μ2g(x2) = 0.
4. g(x2) ≥ 0.

Example 21.4 Consider the problem

where

The KKT condition for this problem is

1. μ = [μ1, μ2] ≤ 0.
2. Df(x) + μ = 0.
3. μ x = 0.
4. x ≥ 0.

We have

This gives

We now have four variables, three equations, and the inequality constraints on each variable. To find a solution (x*, μ*), we first try

which gives

The above satisfies all the KKT and feasibility conditions. In a similar fashion, we can try

which gives

This point clearly violates the nonpositivity constraint on μ*1.
The feasible point above satisfying the KKT condition is only a candidate for a minimizer. However, there is no guarantee that the point is indeed a minimizer, because the KKT condition is, in general, only necessary. A sufficient condition for a point to be a minimizer is given in the next section.
Example 21.4 is a special case of a more general problem of the form

The KKT condition for this problem has the form

From the above, we can eliminate μ to obtain

Some possible points in 2 that satisfy these conditions are depicted in Figure 21.4.

Figure 21.4 Some possible points satisfying the KKT condition for problems with positive constraints.
(Adapted from [13].)


For further results related to the KKT condition, we refer the reader to [90, Chapter 7].
21.2 Second-Order Conditions
As in the case of extremum problems with equality constraints, we can also give second-order necessary and sufficient conditions for extremum problems involving inequality constraints. For this, we need to define the following matrix:

where F(x) is the Hessian matrix of f at x, and the notation [λH(x)] represents

as before. Similarly, the notation [μG(x)] represents

where Gk(x) is the Hessian of gk at x, given by

In the following theorem, we use

that is, the tangent space to the surface defined by active constraints.
Theorem 21.2 Second-Order Necessary Conditions. Let x* be a local minimizer of f : n →  subject to h(x) = 0, g(x) ≤ 0, h : n → m, m ≤ n, g : n → p, and f, h, g  2. Suppose that x* is regular. Then, there exist λ*  m and μ*  p such that:

1. μ* ≥ 0, Df(x*) + λ* Dh(x*) + μ* Dg(x*) = 0, μ* g(x*) = 0.
2. For all y  T(x*) we have y L(x*, λ*, μ*)y ≥ 0.

Proof. Part 1 is simply a result of the KKT theorem. To prove part 2, we note that because the point x* is a local minimizer over {x : h(x) = 0, g(x) ≤ 0}, it is also a local minimizer over {x : h(x) = 0, gj(x) = 0, j  J(x*)}; that is, the point x* is a local minimizer with active constraints taken as equality constraints (see Exercise 21.16). Hence, the second-order necessary conditions for equality constraints (Theorem 20.4) are applicable here, which completes the proof.
We now state the second-order sufficient conditions for extremum problems involving inequality constraints. In the formulation of the result, we use the following set:

where (x*, μ*) = {i : gi(x*) = 0, μi* > 0}. Note that (x*, μ*) is a subset of J(x*): (x*, μ*) ⊂ J(x*). This, in turn, implies that T(x*) is a subset of (x*, μ*): T(x*) ⊂ (x*, μ*).
Theorem 21.3 Second-Order Sufficient Conditions. Suppose that f, g, h  2 and there exist a feasible point x*  n and vectors λ*  m and μ*  p such that:

1. μ* ≥ 0, Df(x*) + λ* Dh(x*) + μ* Dg(x*) = 0, μ* g(x*) = 0.
2. For all y  (x*, μ*), y ≠ 0, we have y L(x*, λ*, μ*)y > 0.

Then, x* is a strict local minimizer of f subject to h(x) = 0, g(x) ≤ 0.
Proof For a proof of this theorem, we refer the reader to [88, p. 345].
A result similar to Theorem 21.3 holds for a strict local maximizer, the only difference being that we need μ* ≤ 0 and that L(x*, λ*) be negative definite on (x*, μ*).
Example 21.5 Consider the following problem:


a. Write down the KKT condition for this problem.
b. Find all points (and KKT multipliers) satisfying the KKT condition. In each case, determine if the point is regular.
c. Find all points in part b that also satisfy the SONC.
d. Find all points in part c that also satisfy the SOSC.
e. Find all points in part c that are local minimizers.

Solution:

a. Write f(x) = x1x2, g1(x) = 2 − x1 − x2, and g2(x) = x1 − x2. The KKT condition is

b. It is easy to check that μ1 ≠ 0 and μ2  0. This leaves us with only one solution to the KKT condition: x*1 = x*2 = 1, μ*1 = 1, μ*2 = 0. For this point, we have Dg1(x*) = [−1, −1] and Dg2(x*) = [1, −1]. Hence, x* is regular.
c. Both constraints are active. Hence, because x* is regular, T(x*) = {0}. This implies that the SONC is satisfied.
d. Now,

Moreover, (x*, μ*) = {y : [−1, −1]y = 0} = {y : y1 = −y2}. Pick y = [1, −1]  (x*, μ*). We have y L(x*, μ*)y = −2 < 0, which means that the SOSC fails.
e. In fact, the point x* is not a local minimizer. To see this, draw a picture of the constraint set and level sets of the objective function. Moving in the feasible direction [1, 1], the objective function increases; but moving in the feasible direction [−1, 1], the objective function decreases.

We now solve analytically the problem in Example 20.1 that we solved graphically earlier.
Example 21.6 We wish to minimize f(x) = (x1 − 1)2 + x2 − 2 subject to

For all x:  2, we have

Thus, ∇h(x) and ∇g(x) are linearly independent and hence all feasible points are regular. We first write the KKT condition. Because Df(x) = [2x1 − 2, 1], we have

To find points that satisfy the conditions above, we first try μ > 0, which implies that x1 + x2 − 2 = 0. Thus, we are faced with a system of four linear equations

Solving the system of equations above, we obtain

However, the above is not a legitimate solution to the KKT condition, because we obtained μ = 0, which contradicts the assumption that μ = 0.
In the second try, we assume that μ = 0. Thus, we have to solve the system of equations

and the solutions must satisfy

Solving the equations above, we obtain

Note that x* = [1/2,3/2] satisfies the constraint g(x*) ≤ 0. The point x* satisfying the KKT necessary condition is therefore the candidate for being a minimizer.
We now verify if x* = [l/2,3/2], λ* = −1, μ* = 0, satisfy the second-order sufficient conditions. For this, we form the matrix

We then find the subspace

Note that because μ* = 0, the active constraint g(x*) = 0 does not enter the computation of (x*, μ*). Note also that in this case, (x*) = {0}. We have

We then check for positive definiteness of L(x*, λ*, μ*) on (x*, μ*). We have

Thus, L(x*, λ*, μ*) is positive definite on (x*, μ*). Observe that L(x*, λ*, μ*) is, in fact, only positive semidefinite on 2.
By the second-order sufficient conditions, we conclude that x* = [1/2,3/2] is a strict local minimizer.
EXERCISES

21.1 Consider the optimization problem

a. Find all the points that satisfy the KKT conditions.
b. Apply the SOSC to determine the nature of the critical points from the previous part.
21.2 Find local extremizers for:
a. x21 + x22 − 2x1 − 10x2 + 26 subject to x2 − x21 ≤ 0, 5x1 + x2 ≤ 5.
b. x21 + x22 subject to x1 ≥ 0, x2 ≥ 0, x1 + x ≥ 5.
c. x21 + 6x1x2 − 4x1 − 2x2 subject to x21 + 2x2 ≤ 1, 2x1 − 2x2 ≤ 1.
21.3 Find local minimizers for x21 + x22 subject to x21 + 2x1x2 + x22 = 1, x21 − x2 ≤ 0.
21.4 Write down the Karush-Kuhn-Tucker condition for the optimization problem in Exercise 15.8.
21.5 Consider the problem

where x1 and x2 are real variables. Answer each of the following questions, making sure that you give complete reasoning for your answers.
a. Write down the KKT condition for the problem, and find all points that satisfy the condition. Check whether or not each point is regular.
b. Determine whether or not the point(s) in part a satisfy the second-order necessary condition.
c. Determine whether or not the point(s) in part b satisfy the second-order sufficient condition.
21.6 Consider the problem

a. Find all points satisfying the KKT condition for the problem.
b. For each point x* in part a, find T(x*), N(x*), and (x*).
c. Find the subset of points from part a that satisfy the second-order necessary condition.
21.7 Consider the problem of optimizing (either minimizing or maximizing) (x1 − 2)2 + (x2 − 1)2 subject to

The point x* = 0 satisfies the KKT conditions.
a. Does x* satisfy the FONC for minimization or maximization? What are the KKT multipliers?
b. Does x* satisfy the SOSC? Carefully justify your answer.
21.8 Consider the optimization problem

where f(x) = x1x22, where x = [x1, x2], and Ω = {x  2 : x1 = x2, x1 ≥ 0}.
a. Find all points satisfying the KKT condition.
b. Do each of the points found in part a satisfy the second-order necessary condition?
c. Do each of the points found in part a satisfy the second-order sufficient condition?
21.9 Consider the problem

a. Write down the KKT condition for the problem.
b. Define what it means for a feasible point x* to be regular in this particular problem. Are there any feasible points in this problem that are not regular? If yes, find them. If not, explain why not.
21.10 Let g : n →  and x0  n be given, where g(x0) > 0. Consider the problem

Suppose that x* is a solution to the problem and g  1. Use the KKT theorem to decide which of the following equations/inequalities hold:

21.11 Consider a square room with corners located at [0,0], [0,2], [2,0], and [2,2] (in 2). We wish to find the point in the room that is closest to the point [3,4].
a. Guess which point in the room is the closest point in the room to the point [3,4].
b. Use the second-order sufficient conditions to prove that the point you have guessed is a strict local minimizer.
Hint: Minimizing the distance is the same as minimizing the square distance.
21.12 Consider the quadratic programming problem

where Q = Q > 0, A  m x n, and b ≥ 0. Find all points satisfying the KKT condition.
21.13 Consider the linear programming problem

where a, b, c, d, e   are all nonzero constants. Suppose that x* is an optimal basic feasible solution to the problem.
a. Write down the Karush-Kuhn-Tucker condition involving x* (specifying clearly the number of Lagrange and KKT multipliers).
b. Is x* regular? Explain.
c. Find the tangent space T(x*) (defined by the active constraints) for this problem.
d. Assume that the relative cost coefficients of all nonbasic variables are strictly positive. Does x* satisfy the second-order sufficient condition? Explain.
21.14 Consider the problem

where A  m x n, m < n, is of full rank. Use the KKT theorem to show that if there exists a solution, then the optimal objective function value is 0.
21.15 Consider a linear programming problem in standard form (see Chapter 15).
a. Write down the Karush-Kuhn-Tucker condition for the problem.
b. Use part a to show that if there exists an optimal feasible solution to the linear program, then there exists a feasible solution to the corresponding dual problem that achieves an objective function value that is the same as the optimal value of the primal (compare this with Theorem 17.1).
c. Use parts a and b to prove that if x* is an optimal feasible solutions of the primal, then there exists a feasible solution λ* to the dual such that (c − λ*A)x* = 0 (compare this with Theorem 17.3).
21.16 Consider the constraint set S = {x : h(x) = 0,g(x) ≤ 0}. Let x*  S be a regular local minimizer of f over S and J(x*) the index set of active inequality constraints. Show that x* is also a regular local minimizer of f over the set S′ = {x : h(x) = 0, gj(x) = 0, j  J(x*)}.
21.17 Solve the following optimization problem using the second-order sufficient conditions:

See Figure 22.1 for a graphical illustration of the problem.
21.18 Solve the following optimization problem using the second-order sufficient conditions:

See Figure 22.2 for a graphical illustration of the problem.
21.19 Consider the problem

Figure 22.3 gives a graphical illustration of the problem. Deduce from the figure that the problem has two strict local minimizers, and use the second-order sufficient conditions to verify the graphical solutions.
21.20 Consider the following optimization problem with an inequality constraint:

a. Does the point x* = [2,0] satisfy the KKT (first-order necessary) condition?
b. Does the point x* = [2,0] satisfy the second-order necessary condition (for problems with inequality constraints)?
c. Is the point x* = [2,0] a local minimizer?
(See Exercise 6.15 for a similar problem treated using set-constrained methods.)
21.21 Consider the problem

where a  n, a ≥ 0, and b  , b > 0. Show that if a solution to the problem exists, then it is unique, and find an expression for it in terms of a and b.
21.22 Consider the problem

where a, b   are given constants satisfying a2 + b2 ≥ 1.
a. Let x* = [x*, x*2] be a solution to the problem. Use the first-order necessary conditions for unconstrained optimization to show that (x*1)2 + (x*2) = 1
b. Use the KKT theorem to show that the solution x* = [x*1, x*2] is unique and has the form x*1 = αa, x*2 = αb, where α   is a positive constant.
c. Find an expression for α (from part b) in terms of a and b.
21.23 Consider the problem

[exp(x) = ex is the exponential of x]. Let x* = [x*1, x*2] be the solution to the problem.
a. Write down the KKT condition that must be satisfied by x*.
b. Prove that x*2 = exp(x*1).
c. Prove that −2 < x*1 < 0.
21.24 Consider the problem

where c  n, c ≠ 0. Suppose that x* = αe is a solution to the problem, where α   and e = [1,..., 1], and the corresponding objective value is 4.
a. Show that ||x*||2 = 2.
b. Find α and c (they may depend on n).
21.25 Consider the problem with equality constraint

We can convert the above into the equivalent optimization problem

Write down the KKT condition for the equivalent problem (with inequality constraint) and explain why the KKT theorem cannot be applied in this case.








CHAPTER 22
CONVEX OPTIMIZATION PROBLEMS
22.1 Introduction
The optimization problems posed at the beginning of this part are, in general, very difficult to solve. The source of these difficulties may be in the objective function or the constraints. Even if the objective function is simple and "well-behaved," the nature of the constraints may make the problem difficult to solve. We illustrate some of these difficulties in the following examples.
Example 22.1 Consider the optimization problem

The problem is depicted in Figure 22.1, where, as we can see, the constrained minimizer is the same as the unconstrained minimizer. At the minimizer, all the constraints are inactive. If we had only known this fact, we could have approached this problem as an unconstrained optimization problem using techniques from Part II.

Figure 22.1 Situation where the constrained and the unconstrained minimizers are the same.


Example 22.2 Consider the optimization problem

The problem is depicted in Figure 22.2. At the solution, only one constraint is active. If we had only known about this we could have handled this problem as a constrained optimization problem using the Lagrange multiplier method.

Figure 22.2 Situation where only one constraint is active.


Example 22.3 Consider the optimization problem

The problem is depicted in Figure 22.3. This example illustrates the situation where the constraints introduce local minimizers, even though the objective function itself has only one unconstrained global minimizer.

Figure 22.3 Situation where the constraints introduce local minimizers.


Some of the difficulties illustrated in the examples above can be eliminated if we restrict our problems to convex feasible regions. Admittedly, some important real-life problems do not fit into this framework. On the other hand, it is possible to give results of a global nature for this class of optimization problems. In the next section, we introduce the notion of a convex function, which plays an important role in our subsequent treatment of such problems.
22.2 Convex Functions
We begin with a definition of the graph of a real-valued function.
Definition 22.1 The graph of f : Ω → , Ω ⊂ n, is the set of points in Ω ×  ⊂ n+1 given by

We can visualize the graph of f as simply the set of points on a "plot" of f(x) versus x (see Figure 22.4). We next define the epigraph of a real-valued function.

Figure 22.4 Graph and epigraph of a function f :  → .


Definition 22.2 The epigraph of a function f : Ω → , Ω ⊂ n, denoted epi(f), is the set of points in Ω ×  given by

The epigraph epi(f) of a function f is simply the set of points in Ω ×  on or above the graph of f (see Figure 22.4). We can also think of epi(f) as a subset of n+1.
Recall that a set Ω ⊂ n is convex if for every x1, x2  Ω and α  (0,1), αx1 + (1 − α)x2  Ω (see Section 4.3). We now introduce the notion of a convex function.
Definition 22.3 A function f : Ω → , Ω ⊂ n, is convex on Ω if its epigraph is a convex set.
Theorem 22.1 If a function f : Ω → , Ω ⊂ n, is convex on Ω, then Ω is a convex set.
Proof. We prove this theorem by contraposition. Suppose that Ω is not a convex set. Then, there exist two points y1 and y2 such that for some α  (0,1),

Let

Then, the pairs

belong to the graph of f, and hence also the epigraph of f. Let

We have

But note that w  epi(f), because z  Ω. Therefore, epi(f) is not convex, and hence f is not a convex function.
The next theorem gives a very useful characterization of convex functions. This characterization is often used as a definition for a convex function.
Theorem 22.2 A function f : Ω →  defined on a convex set Ω ⊂ n is convex if and only if for all x,y  Ω and all α  (0,1), we have

Proof. ⇐: Assume that for all x, y  Ω and α  (0,1),

Let [x,a] and [y, b] be two points in epi(f), where a, b  . From the definition of epi(f) it follows that

Therefore, using the first inequality above, we have

Because Ω is convex, αx + (1 − α)y  Ω. Hence,

which implies that epi(f) is a convex set, and hence f is a convex function.
⇒: Assume that f : Ω →  is a convex function. Let x, y  Ω and

Thus,

Because f is a convex function, its epigraph is a convex subset of n+1. Therefore, for all α  (0,1), we have

The above implies that for all α  (0,1),

This completes the proof.
A geometric interpretation of Theorem 22.2 is given in Figure 22.5. The theorem states that if f : Ω →  is a convex function over a convex set Ω, then for all x, y  Ω, the points on the line segment in n+1 connecting [x, f(x)] and [y, f(y)] must lie on or above the graph of f.

Figure 22.5 Geometric interpretation of Theorem 22.2.


Using Theorem 22.2, it is straightforward to show that any nonnegative scaling of a convex function is convex, and that the sum of convex functions is convex.
Theorem 22.3 Suppose that f, f1, and f2 are convex functions. Then, for any a ≥ 0, the function a f is convex. Moreover, f1 + f2 is convex.
Proof. Let x, y  Ω and α  (0,1). Fix a ≥ 0. For convenience, write  = af. We have

which implies that  is convex.
Next, write f3 = f1 + f2. We have

which implies that f3 is convex.
Theorem 22.3 implies that for any given collection of convex functions f1, ..., f and nonnegative numbers c1, ..., c, the function c1 f2 + ··· + c f is convex. Using a method of proof similar to that used in Theorem 22.3, it is similarly straightforward to show that the function max{f1, ..., f} is convex (see Exercise 22.6).
We now define the notion of strict convexity.
Definition 22.4 A function f : Ω →  on a convex set Ω ⊂ n is strictly convex if for all x, y  Ω, x ≠ y, and α  (0, 1), we have

From this definition, we see that for a strictly convex function, all points on the open line segment connecting the points [x, f(x)] and [y, f(y)] lie (strictly) above the graph of f.
Definition 22.5 A function f : Ω →  on a convex set Ω ⊂ n is (strictly) concave if −f is (strictly) convex.
Note that the graph of a strictly concave function always lies above the line segment connecting any two points on its graph.
To show that a function is not convex, we need only produce a pair of points x, y  Ω and an α  (0, 1) such that the inequality in Theorem 22.2 is violated.
Example 22.4 Let f(x) = x1x2. Is f convex over Ω = {x : x1 ≥ 0, x2 ≥ 0}?
The answer is no. Take, for example, x = [1,2]  Ω and y = [2, 1]  Ω. Then,

Hence,

and

If, for example, α = 1/2  (0, 1), then

which shows that f is not convex over Ω.
Example 22.4 is an illustration of the following general result.
Proposition 22.1 A quadratic form f : Ω → , Ω ⊂ n, given by f(x) = x Qx, Q  n×n, Q = Q, is convex on Ω if and only if for all x, y  Ω, (x − y) Q(x−y) ≥ 0.
Proof. The result follows from Theorem 22.2. Indeed, the function f(x) = x Qx is convex if and only if for every α  (0,1), and every x, y  n, we have

or, equivalently,

Substituting for f into the left-hand side of this equation yields

Therefore, f is convex if and only if

which proves the result.
Example 22.5 In Example 22.4, f(x) = x1x2, which can be written as f(x) = xQx, where

Let Ω = {x : x ≥ 0}, and x = [2, 2]  Ω, y = [1, 3]  Ω. We have

and

Hence, by Proposition 22.1, f is not convex on Ω.
Differentiate convex functions can be characterized using the following theorem.
Theorem 22.4 Let f : Ω → , f  ⊂1, be defined on an open convex set Ω  n. Then, f is convex on Ω if and only if for all x, y  Ω,

Proof. ⇒ Suppose that f : Ω →  is differentiate and convex. Then, by Theorem 22.2, for any y, x  Ω and α  (0, 1) we have

Rearranging terms yields

Upon dividing both sides of this inequality by α, we get

If we now take the limit as α → 0 and apply the definition of the directional derivative of f at x in the direction y - x (see Section 6.2), we get

or

⇐: Assume that Ω is convex, f : Ω →  is differentiable, and for all x, y  Ω,

Let u, v  Ω and α  (0,1). Because Ω is convex,

We also have

and

Multiplying the first of this inequalities by α and the second by (1 - α) and then adding them together yields

But

Hence,

Hence, by Theorem 22.2, f is a convex function.
In Theorem 22.4, the assumption that Ω be open is not necessary, as long as f  ⊂1 on some open set that contains Ω (e.g., f  ⊂1 on n).
A geometric interpretation of Theorem 22.4 is given in Figure 22.6. To explain the interpretation, let x0  Ω. The function (x) = f(x0) + D f(x0)(x − x0) is the linear approximation to f at x0. The theorem says that the graph of f always lies above its linear approximation at any point. In other words, the linear approximation to a convex function f at any point of its domain lies below epi(f).

Figure 22.6 Geometric interpretation of Theorem 22.4.


This geometric idea leads to a generalization of the gradient to the case where f is not differentiable. Let f : Ω, →  be denned on an open convex set Ω ⊂ n. A vector g  n is said to be a subgradient of f at a point x  Ω if for all y  Ω,

As in the case of the standard gradient, if g is a subgradient, then for a given x0  Ω, the function (x) = f(x0) + g (x − x0) lies below epi(f).
For functions that are twice continuously differentiable, the following theorem gives another possible characterization of convexity.
Theorem 22.5 Let f : Ω → , f  ⊂2, be defined on an open convex set Ω ⊂ n. Then, f is convex on Ω if and only if for each x  Ω, the Hessian F(x) of f at x is a positive semidefinite matrix.
Proof. ⇐ Let x, y  Ω. Because f  ⊂2, by Taylor's theorem there exists α  (0, 1) such that

Because F(x + α(y - x)) is positive semidefinite,

Therefore, we have

which implies that f is convex, by Theorem 22.4.
⇒ We use contraposition. Assume that there exists x  Ω such that F(x) is not positive semidefinite. Therefore, there exists d  n such that dF(x)d < 0. By assumption, Ω is open; thus, the point x is an interior point. By the continuity of the Hessian matrix, there exists a nonzero s   such that x + sd  Ω, and if we write y = x + sd, then for all points z on the line segment joining x and y, we have dF(z)d < 0. By Taylor's theorem there exists α  (0, 1) such that

Because α  (0,1), the point x + αsd is on the line segment joining x and y, and therefore

Because s ≠ 0, we have s2 > 0, and hence

Therefore, by Theorem 22.4, f is not a convex function.
Theorem 22.5 can be strengthened to include nonopen sets by modifying the condition to be (y − x) F(x)(y − x) ≥ 0 for all x, y  Ω (and assuming that f  ⊂2 on some open set that contains Ω; for example, f  ⊂2 on n). A proof similar to that above can be used in this case.
Note that by definition of concavity, a function f : Ω → , f  ⊂2, is concave over the convex set Ω ⊂ n if and only if for all x  Ω, the Hessian F(x) of f is negative semidefinite.
Example 22.6 Determine whether the following functions are convex, concave, or neither:

1. f :  → , f(x) = −8x2.
2. f : 3 → , f(x) = 4x21 + 3x22 + 5x23 + 6x1x2 + x1x3 − 3x1 − 2x2 + 15.
3. f : 2 → , f(x) = 2x1x2 −x21 − x22.

Solution:

1. We use Theorem 22.5. We first compute the Hessian, which in this case is just the second derivative: (d2f/dx2)(x) = −16 < 0 for all x  . Hence, f is concave over .
2. The Hessian matrix of f is

The leading principal minors of F(x) are

Hence, F(x) is positive definite for all x  3. Therefore, f is a convex function over 3.
3. The Hessian of f is

which is negative semidefinite for all x  2. Hence, f is concave on 2.

22.3 Convex Optimization Problems
In this section we consider optimization problems where the objective function is a convex function and the constraint set is a convex set. We refer to such problems as convex optimization problems or convex programming problems. Optimization problems that can be classified as convex programming problems include linear programs and optimization problems with quadratic objective function and linear constraints. Convex programming problems are interesting for several reasons. Specifically, as we shall see, local minimizers are global for such problems. Furthermore, first-order necessary conditions become sufficient conditions for minimization.
Our first theorem below states that in convex programming problems, local minimizers are also global.
Theorem 22.6 Let f : Ω, →  be a convex function defined on a Convex set Ω ⊂ n. Then, a point is a global minimizer of f over Ω if and only if it is a local minimizer of f.
Proof. ⇒ This is obvious.
⇐ We prove this by contraposition. Suppose that x* is not a global minimizer of f over Ω. Then, for some y  Ω, we have f(y) < f(x*). By assumption, the function f is convex, and hence for all α  (0, 1),

Because f(y) < f(x*), we have

Thus, for all α  (0, 1),

Hence, there exist points that are arbitrarily close to x* and have lower objective function value. For example, the sequence {yn} of points given by

converges to x*, and f(yn) < f(x*). Hence, x* is not a local minimizer, which completes the proof.
We now show that the set of global optimizers is convex. For this, we need the following lemma.
Lemma 22.1 Let g : Ω, →  be a convex function defined on a convex set Ω ⊂ n. Then, for each c  , the set

is a convex set.
Proof. Let x, y  Γc. Then, g(x) ≤ c and g(y) ≤ c. Because g is convex, for all α  (0, 1),

Hence, αx + (1 − α)y  Γc, which implies that Γc is convex.
Corollary 22.1 Let f : Ω, →  be a convex function defined on a convex set Ω n. Then, the set of all global minimizers of f over Ω is a convex set.
Proof. The result follows immediately from Lemma 22.1 by setting

We now show that if the objective function is continuously differentiable and convex, then the first-order necessary condition (see Theorem 6.1) for a point to be a minimizer is also sufficient. We use the following lemma.
Lemma 22.2 Let f : Ω →  be a convex function defined on the convex set Ω ⊂ n and f  ⊂1 on an open convex set containing Ω. Suppose that the point x*  Ω is such that for all x  Ω, x ≠ x*, we have

Then, x* is a global minimizer of f over Ω.
Proof. Because the function f is convex, by Theorem 22.4, for all x  Ω, we have

Hence, the condition Df(x*)(x − x*) ≥ 0 implies that f(x) ≥ f(x*).
Observe that for any x  Ω, the vector x − x* can be interpreted as a feasible direction at x* (see Definition 6.2). Using Lemma 22.2, we have the following theorem (cf. Theorem 6.1).
Theorem 22.7 Let f : Ω →  be a convex function defined on the convex set Ω ⊂ n, and f  ⊂1 on an open convex set containing Ω. Suppose that the point x*  Ω is such that for any feasible direction d at x*, we have

Then, x* is a global minimizer of f over Ω.
Proof. Let x  Ω, x ≠ x*. By convexity of Ω,

for all α  (0, 1). Hence, the vector d = x − x* is a feasible direction at x* (see Definition 6.2). By assumption,

Hence, by Lemma 22.2, x* is a global minimizer of f over Ω.
From Theorem 22.7, we easily deduce the following corollary (compare this with Corollary 6.1).
Corollary 22.2 Let f : Ω → , f  ⊂1, be a convex function defined on the convex set Ω ⊂ n. Suppose that the point x*  Ω is such that

Then, x* is a global minimizer of f over Ω.
We now consider the constrained optimization problem

We assume that the feasible set is convex. An example where this is the case is when

The following theorem states that provided the feasible set is convex, the Lagrange condition is sufficient for a point to be a minimizer.
Theorem 22.8 Let f : n → , f  ⊂1, be a convex function on the set of feasible points

where h : n → m, h  ⊂1, and Ω is convex. Suppose that there exist x*  Ω and λ*  m such that

Then, x* is a global minimizer of f over Ω.
Proof. By Theorem 22.4, for all x  Ω, we have

Substituting D f(x*) = −λ* D h(x*) into the inequality above yields

Because Ω is convex, (1 - α)x* + αx  Ω for all α  (0, 1). Thus,

for all α  (0, 1). Premultiplying by λ*, subtracting λ*h(x*) = 0, and dividing by α, we get

for all α  (0, 1). If we now take the limit as α → 0 and apply the definition of the directional derivative of λ* h at x* in the direction x - x* (see Section 6.2), we get

Hence,

which implies that x* is a global minimizer of f over Ω.
Consider the general constrained optimization problem

As before, we assume that the feasible set is convex. This is the case if, for example, the two sets {x : h(x) = 0} and {x : g(x) ≤ 0} are convex, because the feasible set is the intersection of these two sets (see also Theorem 4.1). We have already seen an example where the set {x : h(x) = 0} is convex. On the other hand, an example where the set {x : g(x) ≤ 0} is convex is when the components of g = [g1, ..., gp] are all convex functions. Indeed, the set {x : g(x) ≤ 0} is the intersection of the sets {x : gi(x) ≤ 0}, i = 1, ..., p. Because each of these sets is convex (by Lemma 22.1), their intersection is also convex.
We now prove that the Karush-Kuhn-Tucker (KKT) condition is sufficient for a point to be a minimizer to the problem above.
Theorem 22.9 Let f : n → , f  ⊂1, be a convex function on the set of feasible points

where h : n → m, g : n → p, h, g  ⊂1, and Ω is convex. Suppose that there exist x*  Ω, λ*  m, and μ*  p, such that

1. μ* ≥ 0.
2. Df(x*) + λ* Dh(x*) + μ* Dg(x*) = 0.
3. μ* g(x*) = 0.

Then, x* is a global minimizer of f over Ω.
Proof. Suppose that x  Ω. By convexity of f and Theorem 22.4,

Using condition 2, we get

As in the proof of Theorem 22.8, we can show that λ* Dh(x*)(x − x*) = 0. We now claim that μ* Dg(x*)(x − x*) ≤ 0. To see this, note that because Ω is convex, (1 − α)x* + αx  Ω for all α  (0, 1). Thus,

for all α  (0,1). Premultiplying by μ* ≥ 0 (by condition 1), subtracting μ* g(x*) = 0 (by condition 3), and dividing by α, we get

We now take the limit as α → 0 to obtain μ* Dg(x*)(x − x*) ≤ 0.
From the above, we have

for all x  Ω, which completes the proof.
Example 22.7 A bank account starts out with 0 dollars. At the beginning of each month, we deposit some money into the bank account. Denote by xk the amount deposited in the kth month, k = 1, 2, .... Suppose that the monthly interest rate is r > 0 and the interest is paid into the account at the end of each month (and compounded). We wish to maximize the total amount of money accumulated at the end of n months, such that the total money deposited during the n months does not exceed D dollars (where D > 0).
To solve this problem we first show that the problem can be posed as a linear program, and is therefore a convex optimization problem. Let yk be the total amount in the bank at the end of the kth month. Then, yk = (1 + r)(yk−1 + xk), k ≥ 1, with y0 = 0. Therefore, we want to maximize yn subject to the constraint that xk ≥ 0, k = 1, ..., n, and x1 + ··· + xn ≤ D. It is easy to deduce that

Let c = [(1 + r)n, (1 + r)n−1, ..., (1 + r)], e = [1, ..., 1], and x = [x1, ..., xn]. Then, we can write the problem as

which is a linear program.
It is intuitively clear that the optimal strategy is to deposit D dollars in the first month. To show that this strategy is indeed optimal, we use Theorem 22.9. Let x* = [D, 0, ..., 0]  n. Because the problem is a convex programming problem, it suffices to show that x* satisfies the KKT condition (see Theorem 22.9). The KKT condition for this problem is

where μ(1)   and μ(2)  n. Let μ(1) = (1 + r)n and μ(2) = (1 + r)ne − c. Then, it is clear that the KKT condition is satisfied. Therefore, x* is a global minimizer.
An entire book devoted to the vast topic of convexity and optimization is [7]. For extensions of the theory of convex optimization, we refer the reader to [136, Chapter 10]. The study of convex programming problems also serves as a prerequisite to nondifferentiable optimization (see, e.g., [38]).
22.4 Semidefinite Programming
Semidefinite programming is a subfield of convex optimization concerned with minimizing a linear objective function subject to a linear matrix inequality. The linear matrix inequality constraint defines a convex feasible set over which the linear objective function is to be minimized. Semidefinite programming can be viewed as an extension of linear programming, where the componentwise inequalities on vectors are replaced by matrix inequalities (see Exercise 22.20). For further reading on the subject of semidefinite programming, we recommend an excellent survey paper by Vandenberghe and Boyd [128].
Linear Matrix Inequalities and Their Properties
Consider n + 1 real symmetric matrices

and a vector

Then,

is an affine function of x, because F(x) is composed of a linear term  and a constant term F0.
Consider now an inequality constraint of the form

The inequality constraint above is to be interpreted as the set of vectors x such that

that is, F(x) is positive semidefinite [or, in the usual notation, F(x) ≥ 0]. Recall that the terms Fi represent constant matrices, x is unknown, and F(x) = F(x) is an affine function x. The expression F(x) = F0 + x1F1 + ··· + xnFn ≥ 0 is referred to in the literature as a linear matrix inequality (LMI), although the term affine matrix inequality would seem to be more appropriate. It is easy to verify that the set {x : F(x) ≥ 0} is convex (see Exercise 22.20).
We can speak similarly of LMIs of the form F(x) > 0, where the requirement is for F(x) to be positive definite (rather than just positive semidefinite). It is again easy to see that the set {x : F(x) > 0} is convex.
A system of LMIs

can be represented as one single LMI:

As an example, a linear inequality involving an m × n real constant matrix A of the form

can be represented as m LMIs:

where ai is the ith row of the matrix A. We can view each scalar inequality as an LMI. We then represent m LMIs as one LMI:

With the foregoing facts as background, we can now give an example of semidefinite programming:

The matrix property that we discuss next is useful when converting certain LMIs or nonlinear matrix inequalities into equivalent LMIs. We start with a simple observation. Let P be a nonsingular n × n matrix and let x = Mz, where M  n×n such that det M ≠ 0. Then, we have

that is,

Similarly,

Suppose that we have a square matrix

Then, by the observation above,

where I is an identity matrix of appropriate dimension. In other words,

We now introduce the notion of the Schur complement, useful in studying LMIs. Consider a square matrix of the form

where A11 and A22 are square submatrices. Suppose that the matrix A11 is invertible. Then, we have

Let

which is called the Schur complement of A11. For the case when A12 = A21, we have

where

Hence,

that is,

Given

we can similarly define the Schur complement of A22, assuming that A22 is invertible. We have

where Δ22 = A11 − A12A−122A21 is the Schur complement of A22. So, for the case where A12 = A21,

Many problems of optimization, control design, and signal processing can be formulated in terms of LMIs. To determine whether or not there exists a point x such that F(x) > 0 is called a feasibility problem. We say that the LMI is nonfeasible if no such solution exists.
Example 22.8 We now present a simple example illustrating the LMI feasibility problem. Let A  m×m be a given real constant square matrix. Suppose that we wish to determine if A has all its eigenvalues in the open left half-complex plane. It is well known that this condition is true if and only if there exists a real symmetric positive definite matrix P such that

or, equivalently,

(also called the Lyapunov inequality; see [16]). Thus, the location of all eigenvalues of A being in the open left half-complex plane is equivalent to feasibility of the following matrix inequality:

that is, the existence of P = P > 0 such that AP + PA < 0.
We now show that finding P = P > 0 such that A P + PA < 0 is indeed an LMI. For this, let

where

We next define the following matrices:

Note that Pi has only nonzero elements corresponding to xi in P. Let

We can then write

Let

Then,

if and only if

Note that this LMI involves a strict inequality. Most numerical solvers do not handle strict inequalities. Such solvers simply treat a strict inequality (>) as a non-strict inequality (≥).
LMI Solvers
The inequality F(x) = F0 + x1F1 + ··· + xnFn ≥ 0 is called the canonical representation of an LMI. Numerical LMI solvers do not deal directly with LMIs in canonical form because of various inefficiencies. Instead, LMI solvers use a structured representation of LMIs.
We can use MATLAB's LMI toolbox to solve LMIs efficiently. This toolbox has three types of LMI solvers, which we discuss next.
Finding a Feasible Solution Under LMI Constraints
First, we discuss MATLAB's LMI solver for solving the feasibility problem defined by a given system of LMI constraints. Using this solver, we can solve any system of LMIs of the form

where X1,...,Xk are matrix variables, N is the left outer factor, M is the right outer factor, (X1,..., Xk) is the left inner factor, and (X1,..., Xk) is the right inner factor. The matrices (·) and (·) are, in general, symmetric block matrices. We note that the term left-hand side refers to what is on the "smaller" side of the inequality 0 ≤ X. Thus in X ≥ 0, the matrix X is still on the right-hand side because it is on the "larger" side of the inequality.
We now provide a description of an approach that can be used to solve the given LMI system feasibility problem. To initialize the LMI system description, we type setlmis([]). Then we declare matrix variables using the command lmivar. The command lmiterm allows us to specify LMIs that constitute the LMI system under consideration. Next, we need to obtain an internal representation using the command getlmis. We next compute a feasible solution to the LMI system using the command feasp. After that, we extract matrix variable values with the command dec2mat. In summary, a general structure of a MATLAB program for finding a feasible solution to the set of LMIs could have the form
setlmis([])
lmivar
lmiterm
.
.
.
lmiterm
getlmis
feasp
dec2mat
We now analyze these commands in some detail so that the reader can write simple MATLAB programs for solving LMIs after completing this section.
First, to create a new matrix-valued variable, say, X, in the given LMI system, we use the command
X = lmivar(type,structure)
The input type specifies the structure of the variable X. There may be three structures of matrix variables. When type=1, we have a symmetric block diagonal matrix variable. The input type=2 refers to a full rectangular matrix variable. Finally, type=3 refers to other cases. The second input structure gives additional information on the structure of the matrix variable X. For example, the matrix variable X could have the form

where each Di is a square symmetric matrix. For the example above we would use type=l. The matrix variable above has r blocks. The input structure is then an r × 2 matrix whose ith row describes the ith block, where the first component of each row gives the corresponding block size, while the second element of each row specifies the block type. For example,
X = lmivar(1,[3 1])
specifies a full symmetric 3 × 3 matrix variable. On the other hand,
X = lmivar(2,[2 3])
specifies a rectangular 2 × 3 matrix variable. Finally, a matrix variable S of the form

can be declared as follows:
S = lmivar(1,[2 0;2 1])
Note above that the second component of the first row of the second input has the value of zero; that is, structure(1,2)=0. This describes a scalar block matrix of the form

Note that the second block is a 2 × 2 symmetric full block.
We next take a closer look at a command whose purpose is to specify the terms of the LMI system of interest. This command has the form
lmiterm(termid,A,B,flag)
We briefly describe each of the four inputs of this command. The first input, termid, is a row with four elements that specify the terms of each LMI of the LMI system. We have termid(1)=n to specify the left-hand side of the nth LMI. We use termid(1)=-n to specify the right-hand side of the nth LMI. The middle two elements of the input termid specify the block location. Thus termid(2,3) = [i j] refers to the term that belongs to the (i,j) block of the LMI specified by the first component. Finally, termid(4)=0 for the constant term, termid(4)=X for the variable term in the form AXB, while termid(4)=-X for the variable term in the form AXB. The second and third inputs of the command lmiterm give the values of the left and right outer factors; that is, A and B give the values of the constant outer factors in the variable terms AXB and AXB. Finally, the fourth input to lmiterm serves as a compact way to specify the expression

Thus, flag='s' can be used to denote a symmetrized expression. We now illustrate the command above on the following LMI:

We have one LMI with two terms. We could use the following description of this single LMI:
lmiterm ([1 1 1 P],1,A)
lmiterm([1 1 1 −P],A',1)
On the other hand, we can describe this LMI compactly using the flag as follows:
lmiterm([1 1 1 P],1,A,'s')
Now, to solve the feasibility problem we could have typed
[tmin,xfeas] = feas(lmis)
In general, for a given LMI feasibility problem of the form

the command feasp solves the auxiliary convex problem

The system of LMIs is feasible if the minimal t is negative. We add that the current value of t is displayed by feasp at each iteration.
Finally, we convert the output of the LMI solver into matrix variables using the command
P = dec2mat(lmis,xfeas,P).
Example 22.9 Let

We use the commands of the LMI Control Toolbox discussed above to write a program that finds P such that P > 0.5I2 and

The program is as follows:
A_1 = [−1 0;0 −1];
A_2 = [−2 0;1 −1];
setlmis([])
P = lmivar(1,[2,1])
lmiterm([1 1 1 P],A_1',1,'s')
lmiterm([2 1 1 P],A_2',1,'s')
lmiterm([3 1 1 0],.5)
lmiterm([−3 1 1 P],1,1)
lmis=getlmis;
[tmin,xfeas] = feasp(lmis);
P = dec2mat(lmis,xfeas,P)
Minimizing a Linear Objective Under LMI Constraints
The next solver we discuss solves the convex optimization problem

The notation A(x) ≤ B(x) is shorthand notation for a general structured LMI system.
This solver is invoked using the function mincx. Thus, to solve a mincx problem, in addition to specifying the LMI constraints as in the feasp problem, we also declare the linear objective function. Then we invoke the function mincx. We illustrate and contrast the feasp and mincx solvers in the following example.
Example 22.10 Consider the optimization problem

where

We first solve the feasibility problem; that is, we find an x such that Ax ≤ b, using the feasp solver. After that, we solve the minimization problem above using the mincx solver. A simple MATLAB code accomplishing these tasks is shown below.
% Enter problem data
A = [1 1;1 3;2 1];
b = [8 18 14]';
c = [−4 −5]';
setlmis([]);
X = lmivar(2,[2 1]);
lmiterm([1 1 1 X],A(1,:),1);
lmiterm([1 1 1 0],−b(1));
lmiterm([1 2 2 X],A(2,:),1);
lmiterm([1 2 2 0],−b(2));
lmiterm([1 3 3 X],A(3,:),1);
lmiterm([1 3 3 0],−b(3));
lmis = getlmis;
%---------------------------------
disp('------------feasp result-------------')
[tmin,xfeas] = feasp(lmis);
x_feasp = dec2mat(lmis,xfeas,X)
disp('---------mincx result----------')
[objective,x_mincx] = mincx(lmis,c,[0.0001 1000 0 0 1])
The feasp function produces

The mincx function produces

In the next example, we discuss the function defcx, which we can use to construct the vector c used by the LMI solver mincx.
Example 22.11 Suppose that we wish to solve the optimization problem

where trace(P) is the sum of the diagonal elements of P. We can use the function mincx to solve this problem. However, to use mincx, we need a vector c such that

After specifying the LMIs and obtaining their internal representation using, for example, the command lmisys=getlmis, we can obtain the desired c with the following MATLAB code,
q = decnbr(lmisys);
c = zeros(q,1);
for j = 1:q
    Pj = defcx(lmisys,j,P);
    c(j) = trace(Pj);
end

Having obtained the vector c, we can use the function mincx to solve the optimization problem.
Minimizing a Generalized Eigenvalue Under LMI Constraints
This problem can be stated as

Here, we need to distinguish between standard LMI constraints of the form C(x) ≤ D(x) and linear-fractional LMIs of the form A(x) ≤ λB(x), which are concerned with the generalized eigenvalue λ. The generalized eigenvalue minimization problem under LMI constraints can be solved using the solver gevp. The basic structure of the gevp solver has the form
[lopt,xopt] = gep{lmisys,nflc}
which returns lopt, the global minimum of the generalized eigenvalue, and xopt, the optimal decision vector variable. The argument lmisys is the system of LMIs, C(x) ≤ D(x), C(x) ≤ D(x), and A(x) ≤ λB(x) for λ = 1. As in the previous solvers, the corresponding optimal values of the matrix variables are obtained using dec2mat. The number of linear-fractional constraints is specified with nflc. There are other inputs to gevp but they are optional. For more information on this type of the LMI solver, we refer the reader to the LMI Lab in MATLAB's Robust Control Toolbox user's guide.
Example 22.12 Consider the problem of finding the smallest α such that where

This problem is related to finding the decay rate of the stable linear differential equation  = Ax. Finding α that solves the optimization problem above can be accomplished using the following LMIs:
A = [−1.1853 0.9134 0.2785
    0.9058 −1.3676 0.5469
    0.1270 0.0975 −3.0000];
setlmis([]);
P = lmivar(1,[3 1])
lmiterm([−1 1 1 P], 1,1) % P
lmiterm([1 1 1 0],.01)    % P ≥= 0.01*I
lmiterm([2 1 1 P],1,A,'s') % linear fractional constraint—LHS
lmiterm([−2 1 1 P], 1,1) % linear fractional constraint—RHS
lmis = getlmis;
[gamma,P_opt] = gevp(lmis,1);
P = dec2mat(lmis,P_opt,P)
alpha = −gamma
The result is

Notice that we used P ≥ 0.01I in place of P > 0.
More examples of linear matrix inequalities in system and control theory can be found in the book by Boyd et al. [16].
A quick introduction to MATLAB's LMI toolbox is the tutorial that can be accessed with the command lmidem within MATLAB. In addition to the MATLAB's LMI toolbox, there is another toolbox for solving LMIs called LMITOOL, a built-in software package in Scilab toolbox, developed at INRIA in Prance. Scilab offers free software for numerical optimization. There is a version of LMITOOL for MATLAB that can be obtained from the website of the Scilab Consortium.
Yet Another LMI Package, YALMIP, for solving LMIs was developed in Switzerland in the Automatic Control Laboratory at ETH. YALMIP is an "intuitive and flexible modelling language for solving optimization problems in MATLAB."
LMIs are tools of modern optimization. The following quote on numerical linear algebra from Gill, Murray, and Wright [52, p. 2] applies as well to the contents of this chapter: "At the heart of modern optimization methods are techniques associated with linear algebra. Numerical linear algebra applies not simply in optimization, but in all fields of scientific computation, including approximation, ordinary differential equations, and partial differential equations. The importance of numerical linear algebra to modern scientific computing cannot be overstated. Without fast and reliable linear algebraic building blocks, it is impossible to develop effective optimization methods; without some knowledge of the fundamental issues in linear algebra, it is impossible to understand what happens during the transition from equations in a textbook to actual computation."
EXERCISES

22.1 Find the range of values of the parameter α for which the function

is concave.
22.2 Consider the function

where Q = Q > 0 and x,b  n. Define the function ϕ :  →  by ϕ(α) = f(x + αd), where x, d  n are fixed vectors and d ≠ 0. Show that ϕ(α) is a strictly convex quadratic function of α.
22.3 Show that f(x) = x1x2 is a convex function on Ω = {[a, ma] : a  }, where m is any given nonnegative constant.
22.4 Suppose that the set Ω = {x : h(x) = c} is convex, where h : n →  and c  . Show that h is convex and concave over Ω.
22.5 Find all subgradients of

at x = 0 and at x = 1.
22.6 Let Ω ⊂ n be a convex set, and fi : Ω → , i = 1,..., be convex functions. Show that max{f1,..., f} is a convex function.
Note: The notation max{f1,..., f} denotes a function from Ω to  such that for each x  Ω, its value is the largest among the numbers fi(x), i = 1,..., .
22.7 Let Ω ⊂ n be an open convex set. Show that a symmetric matrix Q  n is positive semidefmite if and only if for each x, y  Ω, (x-y)Q(x-y) ≥ 0. Show that a similar result for positive definiteness holds if we replace the "≥" by ">" in the inequality above.
22.8 Consider the problem

(see also Exercise 21.9). Is the problem a convex optimization problem? If yes, give a complete proof. If no, explain why not, giving complete explanations.
22.9 Consider the optimization problem

where f(x) = x1x22, where x = [x1,x2], and Ω = {x  2 : x1 = x2, x1 ≥ 0}. (See also Exercise 21.8.) Show that the problem is a convex optimization problem.
22.10 Consider the convex optimization problem

Suppose that the points y  Ω and z  Ω are local minimizers. Determine the largest set of points G ⊂ Ω for which you can be sure that every point in G is a global minimizer.
22.11 Suppose that we have a convex optimization problem on 3.
a. Consider the following three feasible points: [1,0,0], [0,1,0], [0,0,1]. Suppose that all three have objective function value 1. What can you say about the objective function value of the point (1/3) [1,1,1]? Explain fully.
b. Suppose we know that the three points in part a are global minimizers. What can you say about the point (1/3)[1,1,1]? Explain fully.
22.12 Consider the optimization problem

where Q  n×n, Q = Q > 0, A  m×n, and rank A = m.
a. Find all points satisfying the Lagrange condition for the problem (in terms of Q, A, and b).
b. Are the points (or point) global minimizers for this problem?
22.13 Let f : n → , f  ⊂1, be a convex function on the set of feasible points

where a1,..., ap  n, and b1,..., bp  . Suppose that there exist x*  S, and μ*  p, μ* ≤ 0, such that

where J(x*) = {i : aix* + bi = 0}. Show that x* is a global minimizer of f over Ω.
22.14 Consider the problem: minimize ||x||2 (x  n) subject to ax ≥ b, where a  n is a nonzero vector and b  , b > 0. Suppose that x* is a solution to the problem.
a. Show that the constraint set is convex.
b. Use the KKT theorem to show that ax* = b.
c. Show that x* is unique, and find an expression for x* in terms of a and b.
22.15 Consider the problem

For this problem we have the following theorem (see also Exercise 17.16).
Theorem: A solution to this problem exists if and only if c ≥ 0. Moreover, if a solution exists, 0 is a solution.
a. Show that the problem is a convex programming problem.
b. Use the first-order necessary condition (for set constraints) to prove the theorem.
c. Use the KKT condition to prove the above theorem.
22.16 Consider a linear programming problem in standard form.
a. Derive the KKT condition for the problem.
b. Explain precisely why the KKT condition is sufficient for optimality in this case.
c. Write down the dual to the standard form primal problem (see Chapter 17).
d. Suppose that x* and λ* are feasible solutions to the primal and dual, respectively. Use the KKT condition to prove that if the complementary slackness condition (c − λ* A)x* = 0 holds, then x* is an optimal solution to the primal problem. Compare the above with Exercise 21.15.
22.17 Consider two real-valued discrete-time signals, s(1) and s(2) defined over the time interval [1, n]. Let s(1)i and s(2)i be the values at time i of the signals s(1) and s(2), respectively. Assume that the energies of the two signals are 1 [i.e., (s(1)1)2 + ··· + (s(1)n)2 = 1 and (s(2)1)2 + ··· + (s(2)n)2 = 1].
   Let Sa be the set of all signals that are linear combinations of s(1) and s(2) with the property that for each signal in Sa, the value of the signal over all time is no smaller than a  . For each s  Sa, if s = x1s(1) + x2s(2), we call x1 and x2 the coefficients of s.
   We wish to find a signal in Sa such that the sum of the squares of its coefficients is minimized.
a. Formulate the problem as an optimization problem.
b. Derive the Karush-Kuhn-Tucker conditions for the problem.
c. Suppose that you have found a point satisfying the Karush-Kuhn-Tucker conditions. Does this point satisfy the second-order sufficient condition?
d. Is this problem a convex optimization problem?
22.18 Let a probability vector be any vector p  n satisfying pi > 0, i = 1,..., n, and p1 + ··· + pn = 1.
   Let p  n and q  n be two probability vectors. Define

where "log" is the natural logarithm function.
a. Let Ω be the set of all probability vectors (with fixed n). Show that Ω is convex.
b. Show that for each fixed p, the function f defined by f(q) = D(p, q) is convex over Ω.
c. Show the following: D(p,q) ≥ 0 for any probability vectors p and q. Moreover, D(p, q) = 0 if and only if p = q.
d. Describe an application of the result of part c.
22.19 Let Ω ⊂ n be a nonempty closed convex set and z  n be a given point such that z  Ω. Consider the optimization problem

Does this problem have an optimal solution? If so, is it unique? Whatever your assertion, prove it.
Hint: (i) If x1 and x2 are optimal solutions, what can you say about x3 = (x1 + x2)/2? (ii) The triangle inequality states that ||x + y||≤||x||+||y||, with equality holding if and only if x = αy for some α ≥ 0 (or x = 0 or y = 0).
22.20 This exercise is about semidefinite programming.
a. Show that if A  n × n and B  n × n are symmetric and A ≥ 0, B ≥ 0, then for any α  (0,1), we have αA + (1 − α)B ≥ 0. As usual, the notation "≥ 0" denotes positive semidefiniteness.
b. Consider the following semidefinite programming problem, that is, an optimization problem with linear objective function and linear matrix inequality constraints:

where x = [x1,..., xn]  n is the decision variable, c  n, and F0, F1,..., Fn  m × m are symmetric.
Show that this problem is a convex optimization problem.
c. Consider the linear programming problem

where A  m × n, b  m, and the inequality Ax ≥ b has the usual elementwise interpretation. Show that this linear programming problem can be converted to the problem in part b.
Hint: First consider diagonal Fj.
22.21 Suppose that you have a cake and you need to divide it among n different children. Suppose that the ith child receives a fraction xi of the cake. We will call the vector x = [x1,..., xn] an allocation. We require that every child receives at least some share of the cake, and that the entire cake is completely used up in the allocation. We also impose the additional condition that the first child (i = 1) is allocated a share that is at least twice that of any other child. We say that the allocation is feasible if it meets all these requirements.
   A feasible allocation x is said to be proportionally fair if for any other allocation y,

a. Let Ω be the set of all feasible allocations. Show that Ω is convex.
b. Show that a feasible allocation is proportionally fair if and only if it solves the following optimization problem:

22.22 Let Ui :  → , Ui  ⊂1, i = 1,..., n, be a set of concave increasing functions. Consider the optimization problem

where C > 0 is a given constant.
a. Show that the optimization problem above is a convex optimization problem.
b. Show that x* = [x1*,..., xn*] is an optimal solution to the optimization problem if and only if there exists a scalar μ* ≥ 0 such that xi* = arg maxx(Ui(x) − μ*x). [The quantity Ui(x) has the interpretation of the "utility" of x, whereas μ* has the interpretation of a "price" per unit of x.]
c. Show that .
22.23 Give an example of a function f : 2 → , a set Ω = {x : g(x) ≥ 0}, and a regular point x*  Ω, such that the following all hold simultaneously:
1. x* satisfies the FONC for set constraint Ω (Theorem 6.1).
2. x* satisfies the KKT condition for inequality constraint g(x) ≤ 0 (Theorem 21.1).
3. x* satisfies the SONC for set constraint Ω, (Theorem 6.2).
4. x* does not satisfy the SONC for inequality constraint g(x) ≤ 0 (Theorem 21.2).
Be sure to show carefully that your choice of f, Ω = {x : g(x) ≤ 0}, and x* satisfies all the conditions above simultaneously.
22.24 This question is on duality theory for nonlinear programming problems, analogous to the theory for linear programming (Chapter 17). (A version for quadratic programming is considered in Exercise 17.24.)
   Consider the following optimization problem:

where f : n →  is convex, each component of g : n → m is convex, and f, g  ⊂1. Let us call this problem the primal problem.
   Define the dual of the problem above as

where q is denned by

with l(x, μ) = f(x) + μg(x) the Lagrangian at x, μ.
   Prove the following results:
a. If x0 and μ0 are feasible points in the primal and dual, respectively, then f(x0) ≥ q(μ0). This is the weak duality lemma for nonlinear programming, analogous to Lemma 17.1.
b. If x0 and μ0 are feasible points in the primal and dual, and f(x0) = q(μ0), then x0 and μ0 are optimal solutions to the primal and dual, respectively.
c. If the primal has an optimal (feasible) solution, then so does the dual, and their objective function values are equal. (You may assume regularity.) This is the duality theorem for nonlinear programming, analogous to Theorem 17.2.
22.25 Consider the matrix

where γ is a parameter.
a. Find the Schur complement of M (1,1);
b. Find the Schur complement of M(2:3, 2:3) (the bottom-right 2 × 2 submatrix of M, using MATLAB notation).
22.26 Represent the Lyapunov inequality

where

as a canonical LMI.
22.27 Let A, B, and R be given matrices such that R = R > 0. Suppose that we wish to find a symmetric positive definite matrix P satisfying the following quadratic inequality:

Represent this inequality in the form of LMIs. (This inequality should not be confused with the algebraic Riccati inequality, which has a negative sign in front of the third term.)
22.28 Let

Write a MATLAB program that finds a matrix P satisfying 0.1 I3 ≤ P ≤ I3 and









CHAPTER 23
ALGORITHMS FOR CONSTRAINED OPTIMIZATION
23.1 Introduction
In Part II we discussed algorithms for solving unconstrained optimization problems. In this chapter we present some simple algorithms for solving special constrained optimization problems. The methods here build on those of Part II.
We begin our presentation in the next section with a discussion of projected methods, including a treatment of projected gradient methods for problems with linear equality constraints. We then consider Lagrangian methods. Finally, we consider penalty methods. This chapter is intended as an introduction to ideas underlying methods for solving constrained optimization problems. For an in-depth coverage of the subject, we refer the reader to [11].
23.2 Projections
The optimization algorithms considered in Part II have the general form

where d(k) is typically a function of ∇f(x(k)). The value of x(k) is not constrained to lie inside any particular set. Such an algorithm is not immediately applicable to solving constrained optimization problems in which the decision variable is required to lie within a prespecified constraint set.
Consider the optimization problem

If we use the algorithm above to solve this constrained problem, the iterates x(k) may not satisfy the constraints. Therefore, we need to modify the algorithms to take into account the presence of the constraints. A simple modification involves the introduction of a projection. The idea is as follows. If x(k) + αkd(k) is in Ω, then we set x(k+1) = x(k) + αkd(k) as usual. If, on the other hand, x(k) + αkd(k) is not in Ω, then we "project" it back into Ω before setting x(k+1)
To illustrate the projection method, consider the case where the constraint set Ω ⊂ n is given by

In this case, Ω is a "box" in n; for this reason, this form of Ω is called a box constraint. Given a point x  n, define y = II[x]  n by

The point II[x] is called the projection of x onto Ω. Note that II[x] is actually the "closest" point in Ω to x. Using the projection operator II, we can modify the previous unconstrained algorithm as follows:

Note that the iterates x(k) now all lie inside Ω. We call the algorithm above a projected algorithm.
In the more general case, we can define the projection onto Ω:

In this case, II[x] is again the "closest" point in Ω to x. This projection operator is well-defined only for certain types of constraint sets: for example, closed convex sets (see Exercise 22.19). For some sets Ω, the "argmin" above is not well-defined. If the projection II is well-defined, we can similarly apply the projected algorithm

In some cases, there is a formula for computing II[x], For example, if Ω represents a box constraint as described above, then the formula given previously can be used. Another example is where Ω is a linear variety, which is discussed in the next section. In general, even if the projection II is well-defined, computation of II[x] for a given x may not be easy. Often, the projection II[x] may have to be computed numerically. However, the numerical computation of II[x] itself entails solving an optimization algorithm. Indeed, the computation of II[x] may be as difficult as the original optimization problem, as is the case in the following example:

Note that the solution to the problem in this case can be written as II[0]. Therefore, if 0  Ω, the computation of a projection is equivalent to solving the given optimization problem.
As an example, consider the projection method applied specifically to the gradient algorithm (see Chapter 8). Recall that the vector − ∇f(x) points in the direction of maximum rate of decrease of f at x. This was the basis for gradient methods for unconstrained optimization, which have the form x(k+1) = x(k) − αk∇f(x(k)), where αk is the step size. The choice of the step size αk depends on the particular gradient algorithm. For example, recall that in the steepest descent algorithm, αk = arg minα≥0 f(x(k) − α∇ f(x(k))).
The projected version of the gradient algorithm has the form

We refer to the above as the projected gradient algorithm.
Example 23.1 Consider the problem

where Q = Q > 0. Suppose that we apply a fixed-step-size projected gradient algorithm to this problem.

a. Derive a formula for the update equation for the algorithm (i.e., write down an explicit formula for xk+1 as a function of x(k), Q, and the fixed step size α). You may assume that the argument in the projection operator to obtain x(k) is never zero.
b. Is it possible for the algorithm not to converge to an optimal solution even if the step size α > 0 is taken to be arbitrarily small?
c. Show that for 0 < α < 1/λmax (where λmax is the largest eigenvalue of Q), the fixed-step-size projected gradient algorithm (with step size α) converges to an optimal solution, provided that x(0) is not orthogonal to the eigenvectors of Q corresponding to the smallest eigenvalue. (Assume that the eigenvalues are distinct.)

Solution:

a. The projection operator in this case simply maps any vector to the closest point on the unit circle. Therefore, the projection operator is given by II[x] = x/||x||, provided that x ≠ 0. The update equation is

where βk = 1/||(I − αQ)x(k)|| (i.e., it is whatever constant scaling is needed to make x(k+1) have unit norm).
b. If we start with x(0) being an eigenvector of Q, then x(k) = x(0) for all k. Therefore, if the corresponding eigenvalue is not the smallest, then clearly the algorithm is stuck at a point that is not optimal.
c. We have

But (I − αQ)vi = (1 − αλi)vi, where λi is the eigenvalue corresponding to vi. Hence,

which means that . In other words, y(k)i = β(k)y(0)i (1 − αλi)k, where . We rewrite x(k) as

Assuming that y(0)1 ≠ 0, we obtain

Using the fact that (1 − αλi)/(1 − αλ1) < 1 (because the λi > λ1 for i > 1 and α < 1/λmax), we deduce that

which implies that x(k) → v1, as required.

23.3 Projected Gradient Methods with Linear Constraints
In this section we consider optimization problems of the form

where f : n → , A  m × n, m < n, rank A = m, b  m. We assume throughout that f  ⊂1. In the problem above, the constraint set is Ω = {x : Ax = b}. The specific structure of the constraint set allows us to compute the projection operator II using the orthogonal projector (see Section 3.3). Specifically, II[x] can be defined using the orthogonal projector matrix P given by

(see Example 12.5). Two important properties of the orthogonal projector P that we use in this section are (see Theorem 3.5):
Another property of the orthogonal projector that we need in our discussion is given in the following lemma.
Lemma 23.1 Let v  n. Then, Pv = 0 if and only if v  (A). In other words, (P) = (A). Moreover, Av = 0 if and only if v  (P); that is, (A) = (P).
Proof. ⇒: We have

If Pv = 0, then

and hence v  (A).
⇐: Suppose that there exists u  m such that v = A u. Then,

Hence, we have proved that (P) = (A).
Using an argument similar to that above, we can show that (A) = (P).
Recall that in unconstrained optimization, the first-order necessary condition for a point x* to be a local minimizer is ∇f(x*) = 0 (see Section 6.2). In optimization problems with equality constraints, the Lagrange condition plays the role of the first-order necessary condition (see Section 20.4). When the constraint set takes the form {x : Ax = b}, the Lagrange condition can be written as P∇f(x*) = 0, as stated in the following proposition.
Proposition 23.1 Let x*  n be a feasible point Then, P∇f(x*) = 0 if and only if x* satisfies the Lagrange condition.
Proof. By Lemma 23.1, P∇f(x*) = 0 if and only if we have ∇f(x*)  (A). This is equivalent to the condition that there exists λ*  m such that ∇f(x*) + Aλ* = 0, which together with the feasibility equation Ax = b, constitutes the Lagrange condition.
Recall that the projected gradient algorithm has the form

For the case where the constraints are linear, it turns out that we can express the projection II in terms of the matrix P as follows:

assuming that x(k)  Ω. Although the formula above can be derived algebraically (see Exercise 23.4), it is more insightful to derive the formula using a geometric argument, as follows. In our constrained optimization problem, the vector −∇f(x) is not necessarily a feasible direction. In other words, if x(k) is a feasible point and we apply the algorithm x(k+1) = x(k) − αk∇f(x(k)), then x(k+1) need not be feasible. This problem can be overcome by replacing −∇f(x(k)) by a vector that points in a feasible direction. Note that the set of feasible directions is simply the nullspace (A) of the matrix A. Therefore, we should first project the vector − ∇f(x) onto (A). This projection is equivalent to multiplication by the matrix P. In summary, in the projection gradient algorithm, we update x(k) according to the equation

The projected gradient algorithm has the following property.
Proposition 23.2 In a projected gradient algorithm, if x(0) is feasible, then each x(k) is feasible; that is, for each k ≥ 0, Ax(k) = b.
Proof. We proceed by induction. The result holds for k = 0 by assumption. Suppose now that Ax(k) = b. We now show that Ax(k+1) = b. To show this, first observe that P∇f(x(k))   (A). Therefore,

which completes the proof.
The projected gradient algorithm updates x(k) in the direction of −P∇f(x(k)). This vector points in the direction of maximum rate of decrease of f at x(k) along the surface defined by Ax = b, as described in the following argument. Let x be any feasible point and d a feasible direction such that ||d|| = 1. The rate of increase of f at x in the direction d is . Next, we note that because d is a feasible direction, it lies in (A) and hence by Lemma 23.1, we have d  (P) = (P). So, there exists v such that d = Pv. Hence,

By the Cauchy-Schwarz inequality,

with equality if and only if the direction of v is parallel with the direction of P∇f(x). Therefore, the vector −P∇f(x) points in the direction of maximum rate of decrease of f at x among all feasible directions.
Following the discussion in Chapter 8 for gradient methods in unconstrained optimization, we suggest the following gradient method for our constrained problem. Suppose that we have a starting point x(0), which we assume is feasible; that is, Ax(0) = b. Consider the point x = x(0) − αP∇f(x(0)), where α  . As usual, the scalar α is called the step size. By the discussion above, x is also a feasible point. Using a Taylor series expansion of f about x(0) and the fact that P = P2 = PP, we get

Thus, if P∇f(x(0)) ≠ 0, that is, x(0) does not satisfy the Lagrange condition, then we can choose an α sufficiently small such that f(x) < f(x(0)), which means that x = x(0) − αP∇f(x(0)) is an improvement over x(0). This is the basis for the projected gradient algorithm x(k+1) = x(k) − αkP∇f(x(k)), where the initial point x(0) satisfies Ax(0) = b and αk is some step size. As for unconstrained gradient methods, the choice of αk determines the behavior of the algorithm. For small step sizes, the algorithm progresses slowly, while large step sizes may result in a zigzagging path. A well-known variant of the projected gradient algorithm is the projected steepest descent algorithm, where αk is given by

The following theorem states that the projected steepest descent algorithm is a descent algorithm, in the sense that at each step the value of the objective function decreases.
Theorem 23.1 If {x(k)} is the sequence of points generated by the projected steepest descent algorithm and if P∇f(x(k)) ≠ 0, then f(x(k+1)) < f(x(k)).
Proof First, recall that

where αk ≥ 0 is the minimizer of

over all α ≥ 0. Thus, for α ≥ 0, we have

By the chain rule,

Using the fact that P = P2 = PP, we get

because P∇f(x(k)) ≠ 0 by assumption. Thus, there exists  > 0 such that ϕk(0) > φk(α) for all α  (0, ]. Hence,

which completes the proof of the theorem.
In Theorem 23.1 we needed the assumption that P∇f(x(k)) ≠ 0 to prove that the algorithm possesses the descent property. If for some k, we have P∇f(x(k)) = 0, then by Proposition 23.1 the point x(k) satisfies the Lagrange condition. This condition can be used as a stopping criterion for the algorithm. Note that in this case, x(k+1) = x(k). For the case where f is a convex function, the condition P∇f(x(k)) = 0 is, in fact, equivalent to x(k) being a global minimizer of f over the constraint set {x : Ax = b}. We show this in the following proposition.
Proposition 23.3 The point x*  n is a global minimizer of a convex function f over {x : Ax = b} if and only if P∇f(x*) = 0.
Proof. We first write h(x) = Ax − b. Then, the constraints can be written as h(x) = 0, and the problem is of the form considered in earlier chapters. Note that Dh(x) = A. Hence, x*  n is a global minimizer of f if and only if the Lagrange condition holds (see Theorem 22.8). By Proposition 23.1, this is true if and only if P∇f(x*) = 0, and this completes the proof.
For an application of the projected steepest descent algorithm to minimum fuel and minimum amplitude control problems in linear discrete systems, see [78].
23.4 Lagrangian Algorithms
In this section we consider an optimization method based on the Lagrangian function (see Section 20.4). The basic idea is to use gradient algorithms to update simultaneously the decision variable and Lagrange multiplier vector. We consider first the case with equality constraints, followed by inequality constraints.
Lagrangian Algorithm for Equality Constraints
Consider the following optimization problem with equality constraints:

where h : n → m. Recall that for this problem the Lagrangian function is given by

Assume that f, h  C2; as usual, denote the Hessian of the Lagrangian by L(x, λ).
The Lagrangian algorithm for this problem is given by

Notice that the update equation for x(k) is a gradient algorithm for minimizing the Lagrangian with respect to its x argument, and the update equation for λ(k) is a gradient algorithm for maximizing the Lagrangian with respect to its λ argument. Because only the gradient is used, the method is also called the first-order Lagrangian algorithm.
The following lemma establishes that if the algorithm converges, the limit must satisfy the Lagrange condition. More specifically, the lemma states that any fixed point of the algorithm must satisfy the Lagrange condition. A fixed point of an update algorithm is simply a point with the property that when updated using the algorithm, the resulting point is equal to the given point. For the case of the Lagrangian algorithm, which updates both x(k) and λ(k) vectors, a fixed point is a pair of vectors. If the Lagrangian algorithm converges, the limit must be a fixed point. We omit the proof of the lemma because it follows easily by inspection.
Lemma 23.2 For the Lagrangian algorithm for updating x(k) and λ(k), the pair (x*, λ*) is a fixed point if and only if it satisfies the Lagrange condition.
Below, we use (x*, λ*) to denote a pair satisfying the Lagrange condition. Assume that L(x*, λ*) > 0. Also assume that x* is a regular point. With these assumptions, we are now ready to state and prove that the algorithm is locally convergent. For simplicity, we will take αk and βk to be fixed constants (not depending on k), denoted α and β, respectively.
Theorem 23.2 For the Lagrangian algorithm for updating x(k) and λ(k), provided that α and β are sufficiently small, there is a neighborhood of (x*, λ*) such that if the pair (x(0), λ(0)) is in this neighborhood, then the the algorithm converges to (x*, λ*) with at least a linear order of convergence.
Proof. We can rescale x and λ by appropriate constants (so that the assumptions are preserved) and effectively change the relative values of the step sizes for the update equations. Therefore, without loss of generality, we can take β = α.
We will set up our proof by introducing some convenient notation. Given a pair (x, λ), let w = [x, λ] be the (n + m)-vector constructed by concatenating x and λ. Similarly define w(k) = [x(k), λ(k)] and w* = [x*, λ*]. Define the map U : n×m → n×m by

Then, the Lagrangian algorithm can be rewritten as

We now write ||w(k+1) − w*|| in terms of ||w(k) − w*||, where || · || denotes the usual Euclidean norm. By Lemma 23.3, w* = [x*, μ*] is a fixed point of w(k+1) = U(w(k)). Therefore,

Let DU be the (matrix) derivative of U:

By the mean value theorem (see Theorem 5.9),

where G(w(k)) is a matrix whose rows are the rows of DU evaluated at points that lie on the line segment joining w(k) and w* (these points may differ from row to row). Taking norms of both sides of the equation above,

Finally, combining the above, we have

We now claim that for sufficiently small α > 0, ||DU(w*)|| < 1. Our argument here follows [11, Section 4.4]. Let

so that DU(w*) = I + αM. Hence, to prove the claim, it suffices to show that the eigenvalues of M all lie in the open left-half complex plane.
For any complex vector y, let yH represent its complex conjugate transpose (or Hermitian) and (y) its real part. Let λ be an eigenvalue of M and w = [x, λ] ≠ 0 be a corresponding eigenvector. Now,  (wH Mw) = (λ)||w||2. However, from the structure of M, we can readily see that

By the assumption that L(x*, λ*) > 0, we know that (xHL(x*, λ*)x) > 0 if x ≠ 0. Therefore, comparing the two equations above, we deduce that (λ) < 0, as required, provided that x is nonzero, as we now demonstrate.
Now, suppose that x = 0. Because w is an eigenvector of M, we have Mw = λw. Extracting the first n components, we have Dh(x*)λ = 0. By the regularity assumption, we deduce that λ = 0. This contradicts the assumption that w  0. Hence we conclude that x  0, which completes the proof of our claim that for sufficiently small α > 0, ||DU(w*)|| < 1.
The result of the foregoing claim allows us to pick constants η > 0 and  < 1 such that for all w = [x, λ] satisfying ||w − w*|| ≤ η, we have ||G(w)|| ≤  (this follows from the continuity of DU and norms).
To complete the proof, suppose that ||w(0) − w* ≤ η. We will show by induction that for all k ≥ 0, ||w(k) − w*|| ≤ η and ||w(k+1) − w*|| ≤ ||w(k) − w*||, from which we conclude that w(k) converges to w* with at least linear order of convergence. For k = 0, the result follows because ||w(0) − w*|| ≤ η by assumption, and

which follows from ||w(0) − w*|| ≤ η. So suppose that the result holds for k. This implies that ||G(w(k))|| ≤ . To show the k + 1 case, we write

This means that ||G(w(k+1))|| ≤ , from which we can write

This completes the proof.
Lagrangian Algorithm for Inequality Constraints
Consider the following optimization problem with inequality constraints:

where g : n → p. Recall that for this problem the Lagrangian function is given by

As before, assume that f, g  C2; as usual, denote the Hessian of the Lagrangian by L(x, μ).
The Lagrangian algorithm for this problem is given by

where [·]+ = max{·, 0} (applied componentwise). Notice that, as before, the update equation for x(k) is a gradient algorithm for minimizing the Lagrangian with respect to its x argument. The update equation for μ(k) is a projected gradient algorithm for maximizing the Lagrangian with respect to its μ argument. The reason for the projection is that the KKT multiplier vector is required to be nonnegative to satisfy the KKT condition.
The following lemma establishes that if the algorithm converges, the limit must satisfy the KKT condition. As before, we use the notion of a fixed point to state the result formally. The proof is omitted because the result follows easily by inspection.
Lemma 23.3 For the Lagrangian algorithm for updating x(k) and μ(k) the pair (x*, μ*) is a fixed point if and only if it satisfies the KKT condition.
As before, we use the notation (x*, μ*) to denote a pair satisfying the KKT condition. Assume that L(x*, μ*) > 0. Also assume that x* is a regular point. With these assumptions, we are now ready to state and prove that the algorithm is locally convergent. As before, we will take αk and βk to be fixed constants (not depending on k), denoted α and β, respectively. Our analysis examines the behavior of the algorithm in two phases. In the first phase, the "nonactive" multipliers decrease to zero in finite time and remain at zero thereafter. In the second phase, the x(k) iterates and the "active" multipliers converge jointly to their respective solutions, with at least a linear order of convergence.
Theorem 23.3 For the Lagrangian algorithm for updating x(k) and μ(k), provided that α and β are sufficiently small, there is a neighborhood (x*, μ*) such that if the pair (x(0), μ(0)) is in this neighborhood, then (1) the nonactive multipliers reduce to zero in finite time and remain at zero thereafter and (2) the algorithm converges to (x*, μ*) with at least a linear order of convergence.
Proof. As in the proof of Theorem 23.2, we can rescale x and μ by appropriate constants (so that the assumptions are preserved) and effectively change the relative values of the step sizes for the update equations. Therefore, without loss of generality, we can take β = α.
We set up our proof using the same vector notation as before. Given a pair (x, μ), let w = [x, μ] be the (n + p)-vector constructed by concatenating x and μ. Similarly define w(k) = [x(k), μ(k)] and w* = [x*, μ*]. Define the map U as

Also, define the map II by

Then, the update equations can be rewritten as

Because II is a projection onto the convex set {w = [x, α] : α ≤ 0}, it is a nonexpansive map (see [12, Proposition 3.2]), which means that ||II[v] − II[w] || ≤ || v − w||.
We now write ||w(k+1) − w*|| in terms of ||w(k) − w*||, where || · || denotes the usual Euclidean norm. By Lemma 23.3, w* = [x*, μ*] is a fixed point of w(k+1) = U(w(k)). Therefore,

by the nonexpansiveness of II Let DU be the (matrix) derivative of U:

By the mean value theorem,

where G(w(k)) is a matrix whose rows are the rows of DU evaluated at points that lie on the line segment joining w(k) and w* (these points may differ from row to row). Taking norms of both sides of the equation above yields

Finally, combining the above, we obtain

Let gA represent those rows of g corresponding to active constraints (at x*) and  represent the remaining rows of g. [Recall that by regularity, DgA(x*) has full rank.] Given a vector μ, we divide it into two subvectors μA and , according to active and nonactive components, respectively. (Note that , the zero vector.) Next, write wA = [x, μA] and

so that

Finally, let GA be such that UA(w(k)A) − UA (wA*) = GA(w(k)A)(w(k)A − w*A) (by the mean value theorem as before).
We organize the remainder of our proof into four claims.
Claim 1: For sufficiently small α > 0, ||DUA(wA*)|| < 1.
The argument here parallels that of the proof of Theorem 23.2. So for the sake of brevity we omit the details.
The result of claim 1 allows us to pick constants η > 0, δ > 0, and A < 1 such that for all w = [x, μ] satisfying ||w − w*|| ≤ , ||GA(wA)|| ≤ A, and , where e is the vector with all components equal to 1. The first inequality follows from claim 1 and the continuity of DUA(·) and || · ||. The second follows from the fact that the components of  are negative.
Let  = max{||G(w)|| : ||w − w*|| ≤ η}, which we assume to be at least 1; otherwise, set  = 1. Now pick ε > 0 such that εε/(αδ) ≤ η. We can do this because the left side of this inequality goes to 0 as ε → 0. Assume for convenience that k0 = ε/(αδ) is an integer; otherwise, replace all instances of ε/(αδ) by the smallest integer that exceeds it (i.e., round it up to the closest integer).
For the remainder of this proof, let w(0) satisfy ||w(0) − w*|| ≤ ε.
Claim 2: For k = 0,..., k0, ||w(k) − w*|| ≤ η.
To prove the claim, we show by induction that ||w(k) − w*|| ≤ εk (which is bounded above by η provided that k ≤ k0). For k = 0, by assumption ||w(0) − w*|| ≤ ε = ε0, as required. For the inductive step, suppose that . Now, using  and the fact that ||w(k) − w*|| ≤ η,

and the result now follows by induction.
Claim 3: For k = 0,..., k0,  is monotonically nonincreasing in k, and  (which is equal to .
By claim 2, g(x(k)) ≤ −δe for all k = 0,..., k0. Hence, for k < k0,

which establishes nonincreasing monotonicity.
To show that , suppose that for some nonactive component l, we have . By the monotonicity above, μ(k)l > 0 for k = 0,..., k0. Hence,

But by claim 2, gl(x(k)) ≤ −δ for all k = 0,..., k0 − 1. Hence, , which is a contradiction.
Finally, we will state and prove claim 4, which completes the proof of the theorem.
Claim 4: For k ≥ k0, we have  and .
We use induction. For k = k0, we have ||w(k0) − w*|| ≤ η by claim 2,  by claim 3. Hence,

Because , it is, similarly, also true that . Thus,

where  because ||w(k0) − w*|| ≤ η. Hence, , as required.
For the inductive step, suppose that the result holds for k ≥ k0. Now,  and

which implies that . It follows that

and now using the same argument as in the case of k = k0 above we get . Finally,

Because A < 1, claim 4 implies that w(k) converges to w*, with at least a linear order of convergence.
An application of Lagrangian algorithms to a problem in decentralized rate control for sensor networks appears in [24], [25], and [93]. The proof above is based on [25].
23.5 Penalty Methods
Consider a general constrained optimization problem

We now discuss a method for solving this problem using techniques from unconstrained optimization. Specifically, we approximate the constrained optimization problem above by the unconstrained optimization problem

where  is a positive constant and P : n →  is a given function. We then solve the associated unconstrained optimization problem and use the solution as an approximation to the minimizer of the original problem. The constant γ is called the penalty parameter, and the function P is called the penalty function. Formally, we define a penalty function as follows.
Definition 23.1 A function P : n →  is called a penalty function for the constrained optimization problem above if it satisfies the following three conditions:

1. P is continuous.
2. P(x) ≥ 0 for all .
3. P(x) = 0 if and only if x is feasible (i.e., x  Ω).

Clearly, for the unconstrained problem above to be a good approximation to the original problem, the penalty function P must be chosen appropriately. The role of the penalty function is to "penalize" points that are outside the feasible set.
To illustrate how we choose penalty functions, consider a constrained optimization problem of the form

where . Considering only inequality constraints is not restrictive, because an equality constraint of the form h(x) = 0 is equivalent to the inequality constraint ||h(x)||2 ≤ 0 (however, see Exercise 21.25 for a caveat). For the constrained problem above, it is natural that the penalty function be defined in terms of the constraint functions g1,..., gp. A possible choice for P is

where

We refer to this penalty function as the absolute value penalty function, because it is equal to Σ|gi(x)|, where the summation is taken over all constraints that are violated at x. We illustrate this penalty function in the following example.
Example 23.2 Let g1, g2 :  →  be defined by g1(x) = x − 2, g2(x) = − (x + 1)3. The feasible set denned by  : g1(x) ≤ 0, g2(x) ≤ 0} is simply the interval [-1, 2]. In this example, we have

and

Figure 23.1 provides a graphical illustration of g+ for this example.

Figure 23.1 g+ for Example 23.2.


The absolute value penalty function may not be differentiate at points x where gi(x) = 0, as is the case at the point x = 2 in Example 23.2 (notice, though, that in Example 23.2, P is differentiate at x = −1). Therefore, in such cases we cannot use techniques for optimization that involve derivatives. A form of the penalty function that is guaranteed to be differentiable is the Courant-Beltrami penalty function, given by

In the following discussion we do not assume any particular form of the penalty function P. We only assume that P satisfies conditions 1 to 3 given in Definition 23.1.
The penalty function method for solving constrained optimization problems involves constructing and solving an associated unconstrained optimization problem and using the solution to the unconstrained problem as the solution to the original constrained problem. Of course, the solution to the unconstrained problem (the approximated solution) may not be exactly equal to the solution to the constrained problem (the true solution). Whether or not the solution to the unconstrained problem is a good approximation to the true solution depends on the penalty parameter γ and the penalty function P. We would expect that the larger the value of the penalty parameter γ, the closer the approximated solution will be to the true solution, because points that violate the constraints are penalized more heavily. Ideally, in the limit as γ → ∞, the penalty method should yield the true solution to the constrained problem. In the remainder of this section, we analyze this property of the penalty function method.
Example 23.3 Consider the problem

where Q = Q > 0.

a. Using the penalty function P(x) = (||x||2 − 1)2 and penalty parameter γ, write down an unconstrained optimization problem whose solution xγ approximates the solution to this problem.
b. Show that for any γ, xγ is an eigenvector of Q.
c. Show that ||xγ||2 − 1 = O(1/γ) as γ → ∞.

Solution:

a. The unconstrained problem based on the given penalty function is

b. By the FONC, xγ satisfies

Rearranging, we obtain

where λγ is a scalar. Hence, xγ is an eigenvector of Q. (This agrees with Example 20.8.)
c. Now, λγ = 2γ(1 − ||xγ||2) ≤ λmax, where λmax is the largest eigenvalue of Q. Hence, ||xγ||2 − 1 = −λmax/(2γ) = O(1/γ) as γ → ∞.

We now analyze the penalty method in a more general setting. In our analysis, we adopt the following notation. Denote by x* a solution (global minimizer) to the problem. Let P be a penalty function for the problem. For each k = 1, 2,..., let γk   be a given positive constant. Define an associated function q(γk, ·) : n →  by

For each k, we can write the following associated unconstrained optimization problem:

Denote by x(k) a minimizer of q(γk, x). The following technical lemma describes certain useful relationships between the constrained problem and the associated unconstrained problems.
Lemma 23.4 Suppose that {γk} is a nondecreasing sequence; that is, for each k, we have γk ≤ γk+1. Then, for each k we have

1. 
2. 
3. 
4. 

Proof. We first prove part 1. From the definition of q and the fact that {γk} is an increasing sequence, we have

Now, because x(k) is a minimizer of q(γk, x),

Combining the above, we get part 1.
We next prove part 2. Because x(k) and x(k+1) minimize q(γk, x) and q(γk+1, x), respectively, we can write

Adding the inequalities above yields

Rearranging, we get

We know by assumption that γk+1 ≥ γk. If γk+1 > γk, then we get P(x(k+1)) ≤ P(x(k)). If, on the other hand, γk+1 = γk, then clearly x(k+1) = x(k) and so P(x(k+1)) = P(x(k)). Therefore, in either case, we arrive at part 2.
We now prove part 3. Because x(k) is a minimizer of q(γk x), we obtain

Therefore,

From part 2 we have P(x(k)) − P(x(k+1)) ≥ 0, and γk > 0 by assumption; therefore, we get

Finally, we now prove part 4. Because x(k) is a minimizer of q(γk, x), we get

Because x* is a minimizer for the constrained optimization problem, we have P(x*) = 0. Therefore,

Because P(x(k)) ≥ 0 and γk ≥ 0,

which completes the proof.
With the above lemma, we are now ready to prove the following theorem.
Theorem 23.4 Suppose that the objective function f is continuous and γk → ∞ as k → ∞. Then, the limit of any convergent subsequence of the sequence {x(k)} is a solution to the constrained optimization problem.
Proof. Suppose that {x(mk)} is a convergent subsequence of the sequence {x(k)}. (See Section 5.1 for a discussion of sequences and subsequences.) Let  be the limit of {x(mk)}. By Lemma 23.4, the sequence {q(γk, x(k))} is nondecreasing and bounded above by f(x*). Therefore, the sequence {q(γk, x(k))} has a limit  such that q* ≤ f(x*) (see Theorem 5.3). Because the function f is continuous and f(x(mk)) ≤ f(x*) by Lemma 23.4, we have

Because the sequences {f(x(mk))} and {q(γmk, x(mk))} both converge, the sequence  also converges, with

By Lemma 23.4, the sequence {P(x(k))} is nonincreasing and bounded from below by 0. Therefore, {P(x(k))} converges (again see Theorem 5.3), and hence so does {P(x(mk))}. Because γmk → ∞ we conclude that

By continuity of P, we have

and hence  is a feasible point. Because f(x*) ≥ f() from above, we conclude that  must be a solution to the constrained optimization problem.
If we perform an infinite number of minimization runs, with the penalty parameter γk → ∞, then Theorem 23.4 ensures that the limit of any convergent subsequence is a minimizer x* to the original constrained optimization problem. There is clearly a practical limitation in applying this theorem. It is certainly desirable to find a minimizer to the original constrained optimization problem using a single minimization run for the unconstrained problem that approximates the original problem using a penalty function. In other words, we desire an exact solution to the original constrained problem by solving the associated unconstrained problem [minimize f(x) + γP(x)] with a finite γ > 0. It turns out that indeed this can be accomplished, in which case we say that the penalty function is exact. However, it is necessary that exact penalty functions be nondifferentiable, as shown in [10], and illustrated in the following example.
Example 23.4 Consider the problem

where f(x) = 5 − 3x. Clearly, the solution is x* = 1.
Suppose that we use the penalty method to solve the problem, with a penalty function P that is differentiate at x* = 1. Then, P′(x*) = 0, because P(x) = 0 for all x  [0, 1]. Hence, if we let g = f + γP, then g′ (x*) = f′ (x*) + γP′ (x*) ≠ 0 for all finite γ > 0. Hence, x* = 1 does not satisfy the first-order necessary condition to be a local minimizer of g. Thus, P is not an exact penalty function.
Here, we prove a result on the necessity of nondifferentiability of exact penalty functions for a special class of problems.
Proposition 23.4 Consider the problem

with Ω ⊂ n convex. Suppose that the minimizer x* lies on the boundary of Ω and there exists a feasible direction d at x* such that d ∇f(x) ≥ 0. If P is an exact penalty function, then P is not differentiable at x*.
Proof. We use contraposition. Suppose that P is differentiable at x*. Then, , because P(x) = 0 for all x  Ω. Hence, if we let g = f + γP, then  g(x*) > 0 for all finite γ > 0, which implies that  g(x*) ≠ 0. Hence, x* is not a local minimizer of g, and thus P is not an exact penalty function.
Note that the result of Proposition 23.4 does not hold if we remove the assumption that d ∇ f(x*) > 0. Indeed, consider a convex problem where ∇ f(x*) = 0. Choose P to be differentiable. Clearly, in this case we have ∇g(x*) = ∇ f(x*) + γ ∇P(x*) = 0. The function P is therefore an exact penalty function, although differentiable.
For further reading on the subject of optimization of nondifferentiable functions, see, for example, [38]. References [11] and [96] provide further discussions on the penalty method, including nondifferentiable exact penalty functions. These references also discuss exact penalty methods involving differentiable functions; these methods go beyond the elementary type of penalty method introduced in this chapter.
EXERCISES

23.1 Consider the constrained optimization problem

where f(x) = xQx and Q = Q. We wish to apply a fixed-step-size projected gradient algorithm to this problem:

where α > 0 and Π is the usual projection operator defined by Π[x] = arg minzΩ ||z − x|| and Ω is the constraint set.
a. Find a simple formula for Π[x] in this problem (an explicit expression in terms of x), assuming that x ≠ 0.
b. For the remainder of the question, suppose that

Find the solution(s) to this optimization problem.
c. Let . Derive an expression for y(k+1) in terms of y(k) and α.
d. Assuming that x(0)2 ≠ 0, use parts b and c to show that for any α > 0, x(k) converges to a solution to the optimization problem (i.e., the algorithm works).
e. In part d, what if x(0)2 = 0?
23.2 Consider the problem

where f(x) = cx and c  n is a given nonzero vector. (Linear programming is a special case of this problem.) We wish to apply a fixed-step-size projected gradient algorithm

where, as usual, Π is the projection operator onto Ω (assume that for any y, Π[y] = arg minxΩ ||y − x||2 is unique).
a. Suppose that for some k, x(k) is a global minimizer of the problem. Is it necessarily the case that x(k+1) = x(k)? Explain fully.
b. Suppose that for some k, x(k+1) = x(k). Is it necessarily the case that x(k) is a local minimizer of the problem? Explain fully.
23.3 Consider the optimization problem

where g : 2 → , f  ⊂1 and Ω = [−1, 1]2 = {x : −1 ≤ xi ≤ 1, i = 1, 2}. Consider the projected steepest descent algorithm applied to this problem:

where Π represents the projection operator with respect to Ω and αk = arg minα≥0 f(x(k) − α∇f(x(k))). Our goal is to prove the following statement:
x(k + 1) = x(k) if and only if x(k) satisfies the first-order necessary condition.
We will do this in two parts.
a. Prove the statement above for the case where x(k) is an interior point of Ω.
b. Prove the statement for the case where x(k) is a boundary point of Ω.
Hint: Consider two further subcases: (i) x(k) is a corner point, and (ii) x(k) is not a corner point. For subcase (i) it suffices to take x(k) = [1, 1]. For subcase (ii) it suffices to take x(k)  {x : x1 = 1, −1 < x2 < 1}.
23.4 Let A  m × n, m < n, rank A = m, and b  m. Define Ω = {x : Ax = b} and let x0  Ω. Show that for any y  n,

where P = I − A (AA)−1 A.
Hint: Use Exercise 6.7 and Example 12.5.
23.5 Let f : n →  be given by f(x) = x Qx − xc, where Q = Q > 0. We wish to minimize f over {x : Ax = b}, where A  m × n, m < n, and rank A = m. Show that the projected steepest descent algorithm for this case takes the form

where

and P = In − A (AA)−1 A.
23.6 Consider the problem

where A  m × n, m < n, and rank A − m. Show that if x(0)  {x : Ax = b}, then the projected steepest descent algorithm converges to the solution in one step.
23.7 Show that in the projected steepest descent algorithm, we have that for each k:
a. g(k+1) Pg(k) = 0.
b. The vector x(k+1) − x(k) is orthogonal to the vector x(k+2) − x(k+1).
23.8 Consider the optimization problem

where Ω ⊂ n. Suppose that we apply the penalty method to this problem, which involves solving an associated unconstrained optimization problem with penalty function P and penalty parameter γ > 0.
a. Write down the unconstrained problem associated with penalty function P and penalty parameter γ.
b. Let x* be a global minimizer of the given constrained problem, and let xγ be a global minimizer of the associated unconstrained optimization problem (in part a) with penalty parameter γ. Show that if xγ  Ω, then f(xγ) < f(x*).
23.9 Use the penalty method to solve the following problem:

Hint: Use the penalty function P(x) = (x1 + x2 − 3)2. The solution you find must be exact, not approximate.
23.10 Consider the simple optimization problem

where a  . Suppose that we use the penalty method to solve this problem, with penalty function

(the Courant-Beltrami penalty function). Given a number ε > 0, find the smallest value of the penalty parameter γ such that the solution obtained using the penalty method is no further than ε from the true solution to the given problem. (Think of ε as the desired accuracy.)
23.11 Consider the problem

where A  m × n, b  m, m ≤ n, and rank A = m. Let x* be the solution. Suppose that we solve the problem using the penalty method, with the penalty function

Let xγ* be the solution to the associated unconstrained problem with the penalty parameter γ > 0; that is, xγ* is the solution to

a. Suppose that

Verify that xγ* converges to the solution x* of the original constrained problem as γ → ∞.
b. Prove that xγ* → x* as γ → ∞ holds in general.
Hint: Use the following result: There exist orthogonal matrices U  m × m and V  n × n such that

where

is a diagonal matrix with diagonal elements that are the square roots of the eigenvalues of AA.
The result above is called the singular value decomposition (see, e.g., [62, p. 411]).








CHAPTER 24
MULTIOBJECTIVE OPTIMIZATION
24.1 Introduction
When an optimization problem involves only one objective function, it is a single-objective optimization. Most engineering problems require the designer to optimize a number of conflicting objectives. The objectives are in conflict with each other if an improvement in one objective leads to deterioration in another. Multiobjective problems in which there is competition between objectives may have no single, unique optimal solution. Multiobjective optimization problems are also referred to as multicriteria or vector optimization problems. We can formulate a multiobjective optimization problem as follows: Find a decision variable that satisfies the given constraints and optimizes a vector function whose components are objective functions. Formally, the multiobjective optimization problem is stated as follows:

where f : n →  and Ω ⊂ n. For example, the constraint set Ω can have the form

where

In general, we may have three different types of multiobjective optimization problems:

 Minimize all the objective functions.
 Maximize all the objective functions.
 Minimize some and maximize others.

However, as usual, any of these can be converted into an equivalent minimization problem.
24.2 Pareto Solutions
Note that multiobjective function assigns to each decision variable a multi-objective vector function value in the objective function space. A graphical illustration of this statement is illustrated in Figures 24.1 and 24.2. In Figure 24.1 the decision variable is a point x  2 while the vector of objective functions is given by f : 2 → 2. In Figure 24.2 the decision variable is a point x  2 while the vector of objective functions is f : 2 → 3. In single-objective optimization problems our goal is to find a single solution, where we focus mainly on the decision variable space. On the other hand, in multiobjective problems we are usually more interested in the objective space. As pointed out by Miettinen [92, p. 11], multiobjective problems are in a sense ill-defined because there is no natural ordering in the objective space. Miettinen [92] illustrates this statement with the following simple example. One can say that [1, 1] is less than [3, 3]. But how do we compare [1, 3] and [3, 1]? In general, in multiobjective optimization problems our goal is to find good compromises. Roughly speaking, in a multiobjective optimization problem, a solution is optimal if there exists no other solution, within the feasible set, that gives improved performance with regard to all the objectives. A formal definition of an optimal point for a multiobjective optimization problem was proposed by Francis Y. Edgeworth in 1881 and generalized by Vilfredo Pareto in 1896. It is customary now to refer to an optimal point of a multiobjective optimization problem as the Pareto minimizer, whose formal definition is given next.

Figure 24.1 Two-dimensional illustration of a multiobjective vector function assigning to each decision variable a multiobjective vector function value.



Figure 24.2 Three-dimensional illustration of a multiobjective vector function assigning to each decision variable a multiobjective vector function value.


Definition 24.1 Let f : n →  and x  Ω be given. For the optimization problem

a point x*  Ω is called a Pareto minimizer if there exists no x  Ω such that for i = 1, 2,..., ,

and for at least one i,

In other words, the point x* is a Pareto minimizer, or a nondominated solution, if there exists no other feasible decision variable x that would decrease some objectives without causing simultaneous increase in at least one other variable.
The set of Pareto minimizers (optimizers) is called the Pareto front, as illustrated in Figure 24.3. Most multiobjective optimization algorithms use the concept of domination. A solution is said to be nondominated if it is Pareto optimal.

Figure 24.3 The Pareto front is marked with a heavy line.


In Figure 24.4 we show different combinations of two-objective optimization and the corresponding Pareto fronts. In particular, in the upper left, we show the Pareto front for the case when we are minimizing both components of the objective function vector, which we represent by "min-min." Similarly, "min-max" represents the case when we are minimizing the first objective function and maximizing the second; and so forth.

Figure 24.4 Pareto fronts for four possible cases of two-objective optimization.


24.3 Computing the Pareto Front
When computing the Paret front, two solutions are compared and the dominated solution is eliminated from the set of candidates of Pareto optimizers. Thus, the Pareto front consists of nondominated solutions.
To proceed, we need some notation. Let

be the rth candidate Pareto optimal solution, r = 1, 2,..., R, where R is the number of current candidate Pareto solutions. Let

be the corresponding value of the objective function vector. For any new solution candidate xj, we evaluate the objective function vector f(xj). We then compare the new solution candidate with the existing Pareto solutions. We need to consider three cases:

 xj dominates at least one candidate solution.
 xj does not dominate any existing candidate solutions.
 xj is dominated by a candidate solution.

If xj dominates at least one candidate solution, we delete the dominated solutions from the set and add the new solution xj to the set of candidates. In the second case, when the new candidate solution xj does not dominate any of the existing candidate Pareto solutions, add this new Pareto solution to the set of candidate Pareto solutions. Finally, in the third case, when the new candidate solution is dominated by at least one of the existing candidate Pareto solutions, we do not change the set of the existing candidate Pareto solutions.
Example 24.1 Consider the two-objective minimization problem whose data are as follows:



x(i)
f(x(i))




[5, 6]
[30, 45]


[4, 5]
[22, 29]


[3, 7]
[19, 53]


[6, 8]
[41, 75]


[1, 4]
[13, 45]


[6, 7]
[42, 55]


[2, 5]
[37, 46]


[3, 6]
[28, 37]


[2, 7]
[12, 51]


[4, 7]
[41, 67]



Suppose that we wish to find nondominated pairs for this problem. Recall that a point x* is a nondominated point if for all i and all x,

and at least for one component j of the objective vector, we have

To find the Pareto front, we start with the first pair as a candidate Pareto optimal solution and then compare the other pairs against this first pair, replacing the first pair as necessary. We then continue with the other pairs, building up a set of candidate Pareto solutions and modifying this set when appropriate. The result of the search gives the following Pareto optimal set:



x(i)
f(x(i))




[4, 5]
[22, 29]


[1, 4]
[13, 45]


[2, 7]
[12, 51]



We now discuss an algorithm for generating the Pareto front that implements the foregoing ideas. This algorithm is a minor modification of the algorithm of Osyczka [98, pp. 100-101]. We use the following notation. Let J be the number of candidate solutions to be checked for optimality, while R is the number of current candidate Pareto solutions. Recall that  is the number of objective functions, the dimension of the objective function vector, and n is the dimension of the decision space, that is, the number of components of x. The algorithm consists of eight steps.
Algorithm for Generating a Pareto Front

1. Generate an initial solution x1 and evaluate f*1 = f(x1). This first solution generated is taken as a candidate Pareto solution. Set initial indices R := 1 and j := 1.
2. Set j := j + 1. If j ≤ J, then generate solution xj and go to step 3. Otherwise, stop, because all the candidate solutions have already been considered.
3. Set r := 1 and q := 0 (q represents the number of eliminated solutions from the existing set of Pareto solutions).
4. If for all i = 1, 2,..., ,

then set q := q + 1, f*R := f(xj), remember the solution that should be eliminated, and go to step 6.
5. If for all i = 1, 2,..., ,

then go to step 2.
6. Set r := r + 1. If r ≤ R, go to step 4.
7. If q ≠ 0, remove from the candidate Pareto set the solutions that are eliminated in step 4, add solution xj as a new candidate Pareto solution, and go to step 2.
8. Set R := R + 1, x*R := xj, f*R := f(xj), and go to step 2.

Example 24.2 We apply the algorithm above to generate the Pareto front for the multiobjective optimization problem

We performed 100 iterations. At each iteration we randomly generated 50 feasible points. Then we applied the algorithm above to extract from this set of feasible points candidate Pareto optimal solutions. In Figure 24.5 we show Pareto optimal points obtained after 100 iterations of the algorithm. We also show level sets of the objective functions in the (x1, x2)-space. In Figure 24.6 we show the Pareto front in the objective function space after 100 iterations of the algorithm. The Pareto optimal points are marked with x's. The points marked with ·'s are the candidate solutions generated randomly at the beginning of the last iteration of the algorithm.

Figure 24.5 Pareto optimal points in the decision space along with the level sets of the objective functions f1 and f2.



Figure 24.6 Pareto front for the problem of Example 24.2. Also marked are the objective vector values for the remaining candidate points generated in the last iteration.


We have described a simple approach to computing the Pareto front. Alternative methods include those that apply genetic algorithms to solving multiobjective optimization problems, as discussed in Deb [37], Coello Coello et al. [31], and Osyczka [98].
24.4 From Multiobjective to Single-Objective Optimization
In some cases it is possible to deal with a multiobjective optimization problem by converting the problem into a single-objective optimization problem, so that standard optimization methods can be brought to bear. Here, we discuss four techniques to convert a multiobjective problem to a single-objective problem. We assume throughout that an objective function vector f(x) = [f1(x),..., f(x)] is given.
The first method is to form a single objective function by taking a linear combination, with positive coefficients, of the components of the objective function vector. Equivalently, we form a convex combination of the components of the objective function vector. In other words, we use

as the (single) objective function, where c is a vector of positive components. This method is also called the weighted-sum method, where the coefficients of the linear combination (i.e., the components of c) are called weights. These weights reflect the relative importance of the individual components in the objective vector. Of course, it might be difficult to determine suitable weight values.
A second method is to form a single objective function by taking the maximum of the components of the objective vector:

In other words, we convert the multiobjective minimization problem into one of minimizing the maximum of the components. For this reason, it is also called the minimax method. Note that this method applies to situations where the components of the objective vector are comparable or compatible, in the sense that they are in the same units (e.g., they are all lengths measured in meters, or masses in kilograms). A limitation of this method is that the resulting single objective function might not be differentiable, thereby precluding the use of optimization methods that rely on differentiability (e.g., gradient algorithms). However, as we show in the following, a minimax problem with linear objective vector components and linear constraints can be reduced to a linear programming problem.
Example 24.3 Given vectors v1,..., vp  n and scalars u1,..., up, consider the minimax problem

where A  m×n and b  m. Call this problem P1.

a. Consider the optimization problem

where the decision variable is the vector [x, y]. Call this problem P2. Show that x* solves P1 if and only if [x*, y*] with  solves P2.
Hint: y ≥ max{a, b, c} if and only if y ≥ a, y ≥ b, and y ≥ c.
b. Use part a to derive a linear programming problem

that is equivalent to P1 (by "equivalent" we mean that the solution to one gives us the solution to the other). Explain how a solution to the linear programming problem above gives a solution to P1.

Solution:

a. First suppose that x* is optimal in P1. Let . Then, [x*, y*] is feasible in P2. Let [x, y] be any feasible point in P2. Then (by the hint)

Moreover, x is feasible in P1, and hence

Hence, [x*, y*] is optimal in the linear programming problem.
To prove the converse, suppose that x* is not optimal in P1. Then, there is some x′ that is feasible in P1 such that

But [x′, y′] is evidently feasible in P2, and has objective function value (y′) that is lower than that of [x*, y*]. Hence, [x*, y*] is not optimal in P2.
b. Define

Then the equivalent problem can be written as

By part a, if we obtain a solution to this linear programming problem, then the first n components form a solution to the original minimax problem.

A third method to convert a multiobjective problem to a single-objective problem, assuming that the components of the objective vector are nonnegative, is to form a single objective function by taking the p-norm of the objective vector:

The minimax method can be viewed as a special case of this method, with p = ∞. The weighted-sum method with uniform weights can be viewed as this method with p = 1. To make the objective function differentiable in the case where p is finite (so that we can apply gradient methods, for example), we replace it by its pth power:

A fourth method is to minimize one of the components of the objective vector subject to constraints on the other components. For example, given f, we solve

where b2,..., b are given constants that reflect satisfactory values for the objectives f2,..., f, respectively. Of course, this approach is suitable only in situations where these satisfactory values can be determined.
24.5 Uncertain Linear Programming Problems
In this section we show how multiobjective optimization methods can be used to solve linear programming problems with uncertain coefficients, including uncertain constraints and uncertain objective functions.
Uncertain Constraints
We first consider a generalization of linear programming to problems with uncertain constraints. Our exposition is based on a discussion of fuzzy linear programming by Wang [131, Chapter 30]. We consider the following general linear programming problem:

We can represent the constraints in the form

Suppose that the constraints' bounds are uncertain in the sense that they can vary within given tolerance values and can be represented as

where θ  [0, 1] and ti > 0, i = 1, 2,..., m.
We now discuss a method to solve the problem above. First, solve the following two linear programming problems:

and

Denote the solution to the two programs as x(1) and x(0), respectively, and let z1 = c x(1) and z0 = c x(0). Using these definitions, we construct a function that characterizes the "degree of the penalty" associated with the uncertain constraints in the linear programming problem

A plot of this function is given in Figure 24.7. Note that when c x ≤ z0, then μ0(x) = 0, which represents minimum degree of penalty. On the other hand, when c x ≥ z1, then μ0(x) = 1, and we have a maximum degree of penalty. When z0 ≤ c x ≤ z1, the degree of penalty varies from 0 to 1.

Figure 24.7 Plot of the function μ0(x).


Next, we introduce a function that describes the degree of penalty for violating the ith constraint:

A plot of this function is shown in Figure 24.8.

Figure 24.8 Plot of the function μi(x).


Using the definitions above we can reformulate the original linear programming problem as a multiobjective optimization problem, with the goal of minimizing the functions that penalize constraint violations:

We can employ the minimax method to solve the multiobjective optimization problem as a single-objective problem

As shown in Example 24.3, the problem above can be stated equivalently as

Using now the definitions of μ0 and μi, i = 1,..., m, we restate the optimization problem above as

Example 24.4 Consider the following linear programming problem:

where the tolerances are t1 = 2 and t2 = 1.

a. Solve the two linear programming problems to obtain x(1) and x(0) using the data above. Then find z1 and z0.
b. Construct the equivalent optimization problem (involving θ) using the data above.
c. Express the optimization problem as a linear programming problem in standard form.

Solution:

a. We can solve these problems graphically to obtain

Hence,

b. The optimization problem has the form

where

c. We have

Using our data, we obtain

Write x3 = θ. Then, the above problem can be represented as

The above linear program expressed in the form of a linear programming problem in standard form is


Uncertain Objective Function Coefficients
We now consider a linear programming problem with uncertain objective function coefficients. We assume that uncertainties of the objective coefficients are modeled by the following triangular function:

A plot of this function for a = 1, b = 2, and c = 6 is shown in Figure 24.9. In other words, the uncertain objective coefficients will be represented by the triangular functions of the form given above. Following Wang [131, p. 386], we use the notation  to denote the uncertain coefficient ci represented by the triangular function . Then the linear programming problem

Figure 24.9 Plot of the triangular function μ(x; a, b, c) for a = 1, b = 2, and c = 6.



becomes

where

This is a multiobjective optimization problem. Wang [131] suggests that instead of minimizing the three values c−x, c0x, and c+x simultaneously, the center, c0x, be minimized; the left leg, (c0 − c−)x, be maximized; and the right leg, (c+ − c0) x, be minimized. This results in pushing the triangular functions to the left in the minimization process. Thus, the multiobjective optimization problem above can be changed to the following multiobjective optimization problem:

Uncertain Constraint Coefficients
We may be faced with solving a linear programming problem with uncertain constraint coefficients. In this case the coefficients of the constraint matrix A would be represented by triangular functions of the form given in the preceding section. That is, the coefficient aij of the constraint matrix A would be modeled by the function . Then, the linear programming problem with uncertain constraint coefficients would take the form

where A− = [a−ij], A0 = [a0ij], and A+ = [a+ij].
General Uncertainties
Finally, we may be faced with solving an uncertain linear programming problem that is a combination of the basic uncertain linear programming problems discussed above. For example, suppose that we are asked to solve the following quite general uncertain linear programming problem:

where the tilde symbols refer to the uncertain data; that is, we have

We can represent the uncertain linear programming problem above as a multiobjective optimization problem of the form

EXERCISES

24.1 Write a MATLAB program that implements the algorithm for generating a Pareto front, and test it on the problem in Example 24.1.
24.2 Consider the multiobjective problem

where f : n → .
a. Suppose that we solve the single-objective problem

where c  n, c > 0 (i.e., we use the weighted-sum approach). Show that if x* is a global minimizer for the single-objective problem above, then x* is a Pareto minimizer for the given multiobjective problem. Then show that it is not necessarily the case that if x* is a Pareto minimizer for the multiobjective problem, then there exists a c > 0 such that x* is a global minimizer for the single-objective (weighted-sum) problem.
b. Assuming that for all x  Ω, f(x) ≥ 0, suppose that we solve the single-objective problem

where p  , p > 0 (i.e., we use the minimum-norm approach). Show that if x* is a global minimizer for the single-objective problem above, then x* is a Pareto minimizer for the given multiobjective problem. Then show that it is not necessarily the case that if x* is a Pareto minimizer for the multiobjective problem, then there exists a p > 0 such that x* is a global minimizer for the single-objective (minimum-norm) problem.
c. Suppose that we solve the single-objective problem

(i.e., we use the minimax approach). Show that it is not necessarily the case that if x* is a Pareto minimizer for the given multiobjective problem, then x* is a global minimizer for the single-objective (minimax) problem. Then show that it also is not necessarily the case that if x* is a global minimizer for the single-objective problem, then x* is a Pareto minimizer for the multiobjective problem.
24.3 Let f : n →  be given. Consider the following multiobjective problem with equality constraints:

Suppose that f  1, all the components of f are convex, and Ω is convex. Suppose that there exists x* and c* > 0 such that for any feasible direction d at x*, we have

Show that x* is a Pareto minimizer.
24.4 Let f : n →  and h : n → m be given. Consider the following multiobjective problem with equality constraints:

Suppose that f, h  1, all the components of f are convex, and the constraint set is convex. Show that if there exists x*, c* > 0, and λ* such that

then x* is a Pareto minimizer. We can think of the above as a Lagrange condition for the constrained multiobjective function.
24.5 Let f : n →  and g : n → p be given. Consider the following multiobjective problem with inequality constraints:

Suppose that f, g  1, all the components of f are convex, and the constraint set is convex. Show that if there exists x*, c* > 0, and μ* such that

then x* is a Pareto minimizer. We can think of the above as a KKT condition for the constrained multiobjective function.
24.6 Let f : n → , h : n → m, and g : n → p be given. Consider the general constrained multiobjective problem

Suppose that f,g,h  1, all the components of f are convex, and the constraint set is convex. Show that if there exists x*, c* > 0, λ*, and μ* such that

then x* is a Pareto minimizer.
24.7 Let f1 : n →  and f2 : n → , f1, f2  1. Consider the minimax problem

Show that if x* is a local minimizer, then there exist μ*1, μ*2   such that

and μi* = 0 if fi(x*) < max{f1(x*), f2(x*)}.
Hint: Consider the following problem: minimizes subject to z ≥ fi(x), i = 1,2.








REFERENCES
1. J. S. Arora, Introduction to Optimum Design. New York: McGraw-Hill Book Co., 1989.
2. R. G. Bartle, The Elements of Real Analysis, 2nd ed. New York: Wiley, 1976.
3. M. S. Bazaraa, H. D. Sherali, and C. M. Shetty, Nonlinear Programming: Theory and Algorithms, 2nd ed. New York: Wiley, 1993.
4. A. Bhaya and E. Kaszkurewicz, Control Perspectives on Numerical Algorithms and Matrix Problems. Philadelphia: Society for Industrial and Applied Mathematics, 2006.
5. B. Beliczynski, A. Dzielinski, M. Iwanowski, and B. Ribeiro, Eds., Adaptive and Natural Computing Algorithms, vol. 4431 of Lecture Notes in Computer Science. Berlin: Springer, 2007.
6. A. Ben-Israel and T. N. E. Greville, Generalized Inverses: Theory and Applications. New York: Wiley-Interscience, 1974.
7. L. D. Berkovitz, Convexity and Optimization in n. Hoboken, NJ: Wiley, 2002.
8. C. C. Berresford, A. M. Rockett, and J. C. Stevenson, "Khachiyan's algorithm, Part 1: A new solution to linear programming problems," Byte, vol. 5, no. 8, pp. 198-208, Aug. 1980.
9. C. C. Berresford, A. M. Rockett, and J. C. Stevenson, "Khachiyan's algorithm, Part 2: Problems with the algorithm," Byte, vol. 5, no. 9, pp. 242-255, Sept. 1980.
10. D. P. Bertsekas, "Necessary and sufficient conditions for a penalty method to be exact," Mathematical Programming, vol. 9, no. 1, pp. 87-99, Aug. 1975.
11. D. P. Bertsekas, Nonlinear Programming: 2nd ed. Belmont, MA: Athena Scientific, 1999.
12. D. P. Bertsekas and J. N. Tsitsiklis, Parallel and Distributed Computation: Numerical Methods. Belmont, MA: Athena Scientific, 1997.
13. K. G. Binmore, Calculus. Cambridge, England: Cambridge University Press, 1986.
14. R. G. Bland, D. Goldfarb, and M. J. Todd, "The ellipsoid method: A survey," Operations Research, vol. 29, pp. 1039-1091, 1981.
15. V. G. Boltyanskii, Mathematical Methods of Optimal Control. New York: Holt, Rinehart and Winston, 1971.
16. S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan, Linear Matrix Inequalities in System and Control Theory. Philadelphia, PA: SIAM, 1994.
17. R. P. Brent, Algorithms for Minimization without Derivatives. Englewood Cliffs, NJ: Prentice Hall, 1973.
18. L. Brickman, Mathematical Introduction to Linear Programming and Game Theory. New York: Springer-Verlag, 1989.
19. C. G. Broyden, "Quasi-Newton methods," in Optimization Methods in Electronics and Communications (K. W. Cattermole and J. J. O'Reilly, Eds.), vol. 1 of Mathematical Topics in Telecommunications. New York: Wiley, 1984, pp. 105-110,
20. A. E. Bryson and Y.-C. Ho, Applied Optimal Control: Optimization, Estimation, and Control, rev. print. Washington, DC: Hemisphere Publishing Corporation, 1975.
21. B. D. Bunday, Basic Optimization Methods. London: Edward Arnold, 1984.
22. J. Campbell, The Improbable Machine. New York: Simon and Schuster, 1989.
23. S. L. Campbell and C. D. Meyer, Jr., Generalized Inverses of Linear Transformations. New York: Dover Publications, 1991.
24. E. K. P. Chong and B. E. Brewington, "Distributed communications resource management for tracking and surveillance networks," in Proceedings of the Conference on Signal and Data Processing of Small Targets 2005 (SPIE Vol. 5913), part of the SPIE Symposium on Optics & Photonics, San Diego, California, July 31-Aug. 4, 2005, pp. 280-291.
25. E. K. P. Chong and B. E. Brewington, "Decentralized rate control for tracking and surveillance networks," Ad Hoc Networks, special issue on Recent Advances in Wireless Sensor Networks, vol. 5, no. 6, pp. 910-928, Aug. 2007.
26. E. K. P. Chong, S. Hui, and S. H. ak, "An analysis of a class of neural networks for solving linear programming problems," IEEE Transactions on Automatic Control, special section on Neural Networks in Control, Identification, and Decision Making, vol. 44, no. 11, pp. 1995-2006, Nov. 1999.
27. E. K. P. Chong and S. H. ak, "Single-dimensional search methods," in Wiley Encyclopedia of Operations Research and Management Science, 2011, ISBN: 978-0-470-40063-0.
28. A. Cichocki and R. Unbehauen, Neural Networks for Optimization and Signal Processing. Chichester, England: Wiley, 1993.
29. M. Clerc, "The swarm and the queen: Towards a deterministic and adaptive particle swarm optimization," in Proceedings of the Congress of Evolutionary Computation, Washington, DC, July 1999, pp. 1951-1957.
30. M. Clerc and J. Kennedy, "The particle swarm: Explosion, stability and convergence in a multidimensional complex space," IEEE Transactions on Evolutionary Computation, vol. 6, pp. 58-73, Feb. 2002.
31. C. A. Coello Coello, D. A. Van Veldhuizen, and G. B. Lamont, Evolutionary Algorithms for Solving Multi-Objective Problems. New York: Kluwer Academic/Plenum Publishers, 2002.
32. S. D. Conte and C. de Boor, Elementary Numerical Analysis: An Algorithmic Approach, 3rd ed. New York: McGraw-Hill Book Co., 1980.
33. M. A. Dahleh and I. J. Diaz-Bobillo, Control of Uncertain Systems: A Linear Programming Approach. Upper Saddle River, NJ: Prentice Hall, 1995.
34. G. B. Dantzig, Linear Programming and Extensions. Princeton, NJ: Princeton University Press, 1963.
35. G. B. Dantzig and M. N. Thapa, Linear Programming, vol. 1, Introduction. New York: Springer-Verlag, 1997.
36. L. Davis, Ed., Genetic Algorithms and Simulated Annealing, Research Notes in Artificial Intelligence. London: Pitman, 1987.
37. K. Deb, Multi-objective Optimization Using Evolutionary Algorithms. Chichester, England: Wiley, 2001.
38. V. F. Dem'yanov and L. V. Vasil'ev, Nondifferentiable Optimization. New York: Optimization Software, Inc., Publications Division, 1985.
39. J. E. Dennis, Jr. and R. B. Schnabel, Numerical Methods for Unconstrained Optimization and Nonlinear Equations. Englewood Cliffs, NJ: Prentice Hall, 1983.
40. J. Dongarra and F. Sullivan, "The top 10 algorithms," Computing in Science and Engineering, pp. 22-23, Jan./Feb. 2000.
41. V. N. Faddeeva, Computational Methods of Linear Algebra. New York: Dover Publications, 1959.
42. S.-C. Fang and S. Puthenpura, Linear Optimization and Extensions: Theory and Algorithms. Englewood Cliffs, NJ: Prentice Hall, 1993.
43. R. Fletcher, Practical Methods of Optimization, 2nd ed. Chichester, England: Wiley, 1987.
44. F. R. Gantmacher, The Theory of Matrices, vol. 1. New York: Chelsea Publishing Co., 1959.
45. F. R. Gantmacher, The Theory of Matrices, 2nd ed. Moscow: Nauka, revised 1966. In Russian.
46. S. I. Gass, An Illustrated Guide to Linear Programming. New York: McGraw-Hill Book Co., 1970.
47. I. M. Gel'fand, Lectures on Linear Algebra. New York: Interscience Publishers, 1961.
48. S. Geman and D. Geman, "Stochastic relaxation, Gibbs distribution, and the Bayesian restoration of images," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 6, pp. 721-741, 1984.
49. P. E. Gill and W. Murray, "Safeguarded steplength algorithms for optimization using descent methods," Tech. Rep. NPL NAC 37, National Physical Laboratory, Division of Numerical Analysis and Computing, Teddington, England, Aug. 1974.
50. P. E. Gill, W. Murray, M. A. Saunders, and M. H. Wright, "Two step-length algorithms for numerical optimization," Tech. Rep. SOL 79-25, Systems Optimization Laboratory, Department of Operations Research, Stanford University, Stanford, CA, Dec. 1979.
51. P. E. Gill, W. Murray, and M. H. Wright, Practical Optimization. London: Academic Press, 1981.
52. P. E. Gill, W. Murray, and M. H. Wright, Numerical Linear Algebra and Optimization. Redwood City, CA: Addison-Wesley, 1991.
53. G. H. Golub and C. F. Van Loan, Matrix Computations, 3rd ed.. Baltimore, MD: The Johns Hopkins University Press, 1983.
54. R. E. Gomory, "Outline of an algorithm for integer solutions to linear programs," Bulletin of the American Mathematical Society, vol. 64, no. 5, pp. 275-278, Sep. 1958.
55. C. C. Gonzaga, "Path-following methods for linear programming," SIAM Review, vol. 34, no. 2, pp. 167-224, June 1992.
56. B. Hajek, "Cooling schedules for optimal annealing," Mathematics of Operations Research, vol. 13, no. 2, pp. 311-329, 1988.
57. J. Hannig, E. K. P. Chong, and S. R. Kulkarni, "Relative frequencies of generalized simulated annealing," Mathematics of Operations Research, vol. 31, no. 1, pp. 199-216, Feb. 2006.
58. R. L. Harvey, Neural Network Principles. Englewood Cliffs, NJ: Prentice Hall, 1994.
59. S. Haykin, Neural Networks: A Comprehensive Foundation, 2nd ed. Upper Saddle River, NJ: Prentice Hall, 1999.
60. J. Hertz, A. Krogh, and R. G. Palmer, Introduction to the Theory of Neural Computation, vol. 1 of Santa Fe Institute Studies in the Sciences of Complexity. Redwood City, CA: Addison-Wesley, 1991.
61. J. H. Holland, Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence. Cambridge, MA: MIT Press, 1992.
62. R. A. Horn and C. R. Johnson, Matrix Analysis. Cambridge, England: Cambridge University Press, 1985.
63. A. S. Householder, The Theory of Matrices in Numerical Analysis. New York: Dover Publications, 1975.
64. S. Hui and S. H. ak, "The Widrow-Hoff algorithm for McCulloch-Pitts type neurons," IEEE Transactions on Neural Networks, vol. 5, no. 6, pp. 924-929, Nov. 1994.
65. D. R. Hush and B. G. Horne, "Progress in supervised neural networks: What's new since Lippmann," IEEE Signal Processing Magazine, pp. 8-39, Jan. 1993.
66. S. Isaak and M. N. Manougian, Basic Concepts of Linear Algebra. New York: W. W. Norton & Co., 1976.
67. J.-S. R. Jang, C.-T. Sun, and E. Mizutani, Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and Machine Intelligence. Upper Saddle River, NJ: Prentice Hall, 1997.
68. W. E. Jenner, Rudiments of Algebraic Geometry. New York: Oxford University Press, 1963.
69. E. M. Johansson, F. U. Dowla, and D. M. Goodman, "Backpropagation learning for multi-layer feed-forward neural networks using the conjugate gradient method," International Journal of Neural Systems, vol. 2, no. 4, pp. 291-301, 1992.
70. S. Kaczmarz, "Approximate solution of systems of linear equations," International Journal of Control, vol. 57, no. 6, pp. 1269-1271, 1993. A reprint of the original paper: S. Kaczmarz, "Angenäherte Auflösung von Systemen linearer Gleichunger," Bulletin International de l'Academic Polonaise des Sciences et des Lettres, Serie A, pp. 355-357, 1937.
71. N. Karmarkar, "A new polynomial-time algorithm for linear programming," Combinatorica, vol. 4, no. 4, pp. 373-395, 1984.
72. M. F. Kelly, P. A. Parker, and R. N. Scott, "The application of neural networks to myoelectric signal analysis: A preliminary study," IEEE Transactions on Biomedical Engineering, vol. 37, no. 3, pp. 221-230, Mar. 1990.
73. J. Kennedy and R. C. Eberhart, with Y. Shi, Swarm Intelligence. San Francisco: Morgan Kaufmann, 2001.
74. L. G. Khachiyan, "A polynomial algorithm in linear programming," Soviet Mathematics Doklady, vol. 20, no. 1, pp. 191-194, 1979.
75. S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi, "Optimization by simulated annealing," Science, vol. 220, no. 4598, pp. 671-680, 1983.
76. V. Klee and G. J. Minty, "How good is the simplex algorithm?" in Inequalities-III (O. Shisha, Ed.), New York: Academic Press, 1972, pp. 159-175.
77. D. E. Knuth, The Art of Computer Programming, vol. 1, Fundamental Algorithms, 2nd ed. Reading, MA: Addison-Wesley, 1973.
78. L. Kolev, "Iterative algorithm for the minimum fuel and minimum amplitude problems for linear discrete systems," International Journal of Control, vol. 21, no. 5, pp. 779-784, 1975.
79. J. R. Koza, Genetic Programming: On the Programming of Computers by Means of Natural Selection. Cambridge, MA: MIT Press, 1992.
80. T. Kozek, T. Roska, and L. O. Chua, "Genetic algorithm for CNN template learning," IEEE Transactions on Circuits and Systems, I: Fundamental Theory and Applications, vol. 40, no. 6, pp. 392-402, June 1993.
81. K. Kuratowski, Introduction to Calculus, 2nd ed., vol. 17 of International Series of Monographs in Pure and Applied Mathematics. Warsaw, Poland: Pergamon Press, 1969.
82. J. C. Lagarias, J. A. Reeds, M. H. Wright, and P. E. Wright, "Convergence properties of the Nelder-Mead simplex method in low dimensions," SIAM Journal on Optimization, vol. 9, no. 1, pp. 112-147, 1998.
83. S. Lang, Calculus of Several Variables, 3rd ed. New York: Springer-Verlag, 1987.
84. J. M. Layton, Multivariable Control Theory. Stevenage, England: Peter Peregrinus on behalf of the Institution of Electrical Engineers, 1976.
85. E. B. Lee and L. Markus, Foundations of Optimal Control Theory. Malabar, FL: Robert E. Krieger Publishing Company, 1986.
86. G. Leitmann, The Calculus of Variations and Optimal Control: An Introduction. New York: Plenum Press, 1981.
87. D. G. Luenberger, Optimization by Vector Space Methods. New York: Wiley, 1969.
88. D. G. Luenberger and Y. Ye, Linear and Nonlinear Programming, 3rd ed. New York, NY: Springer Science + Business Media, 2008.
89. I. J. Maddox, Elements of Functional Analysis, 2nd ed. Cambridge, England: Cambridge University Press, 1988.
90. O. L. Mangasarian, Nonlinear Programming. New York: McGraw-Hill Book Co., 1969.
91. N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, H. Teller, and E. Teller, "Equation of state calculations by fast computing machines," Journal of Chemical Physics, vol. 21, no. 6, pp. 1087-1092, 1953.
92. K. M. Miettinen, Nonlinear Multiobjective Optimization. Norwell, MA: Kluwer Academic Publishers, 1998.
93. S. A. Miller and E. K. P. Chong, "Flow-rate control for managing communications in tracking and surveillance networks," in Proceedings of the Conference on Signal and Data Processing of Small Targets 2007 (SPIE Vol. 6699), part of the SPIE Symposium on Optics & Photonics, San Diego, California, Aug. 26-30, 2007.
94. M. Mitchell, An Introduction to Genetic Algorithms. Cambridge, MA: MIT Press, 1996.
95. A. Mostowski and M. Stark, Elements of Higher Algebra. Warsaw, Poland: PWN—Polish Scientific Publishers, 1958.
96. S. G. Nash and A. Sofer, Linear and Nonlinear Programming. New York: McGraw-Hill Book Co., 1996.
97. J. A. Nelder and R. Mead, "A simplex method for function minimization," Computer Journal, vol. 7, no. 4, pp. 308-313, 1965.
98. A. Osyczka, Evolutionary Algorithms for Single and Multicriteria Design Optimization. Heidelberg, Germany: Physica-Verlag, 2002.
99. D. H. Owens, Multivariable and Optimal Systems. London: Academic Press, 1981.
100. T. M. Ozan, Applied Mathematical Programming for Production and Engineering Management. Englewood Cliffs, NJ: Prentice Hall, 1986.
101. C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization: Algorithms and Complexity. Englewood Cliffs, NJ: Prentice Hall, 1982.
102. P. C. Parks, "S. Kaczmarz (1895-1939)," International Journal of Control, vol. 57, no. 6, pp. 1263-1267, 1993.
103. R. J. Patton and G. P. Liu, "Robust control design via eigenstructure assignment, genetic algorithms and gradient-based optimisation," IEE Proceedings on Control Theory and Applications, vol. 141, no. 3, pp. 202-208, May 1994.
104. A. L. Peressini, F. E. Sullivan, and J. J. Uhl, Jr., The Mathematics of Nonlinear Programming. New York: Springer-Verlag, 1988.
105. A. Pezeshki, L. L. Scharf, M. Lundberg, and E. K. P. Chong, "Constrained quadratic minimizations for signal processing and communications," in Proceedings of the Joint 44th IEEE Conference on Decision and Control and European Control Conference (CDC-ECC'05), Seville, Spain, Dec. 12-15, 2005, pp. 7949-7953.
106. A. Pezeshki, L. L. Scharf, and E. K. P. Chong, "The geometry of linearly and quadratically constrained optimization problems for signal processing and communications," Journal of the Franklin Institute, special issue on Modelling and Simulation in Advanced Communications, vol. 347, no. 5, pp. 818-835, June 2010.
107. M. J. D. Powell, "Convergence properties of algorithms for nonlinear optimization," SIAM Review, vol. 28, no. 4, pp. 487-500, Dec. 1986.
108. S. S. Rangwala and D. A. Dornfeld, "Learning and optimization of machining operations using computing abilities of neural networks," IEEE Transactions on Systems, Man and Cybernetics, vol. 19, no. 2, pp. 299-314, Mar./Apr. 1989.
109. G. V. Reklaitis, A. Ravindran, and K. M. Ragsdell, Engineering Optimization: Methods and Applications. New York: Wiley-Interscience, 1983.
110. A. M. Rockett and J. C. Stevenson, "Karmarkar's algorithm: A method for solving large linear programming problems," Byte, vol. 12, no. 10, pp. 146-160, Sept. 1987.
111. H. L. Royden, Real Analysis, 3rd ed. New York: Macmillan Company, 1988.
112. W. Rudin, Principles of Mathematical Analysis, 3rd ed. New York: McGraw-Hill Book Co., 1976.
113. D. E. Rumelhart, J. L. McClelland, and the PDP Research Group, Parallel Distributed Processing: Explorations in the Micro structure of Cognition, vol. 1, Foundations. Cambridge, MA: MIT Press, 1986.
114. D. Russell, Optimization Theory. New York: W. A. Benjamin, 1970.
115. S. L. Salas and E. Hille, Calculus: One and Several Variables, 4th ed. New York: Wiley, 1982.
116. L. L. Scharf, L. T. McWhorter, E. K. P. Chong, J. S. Goldstein, and M. D. Zoltowski, "Algebraic equivalence of conjugate direction and multistage Wiener filters," in Proceedings of the Eleventh Annual Workshop on Adaptive Sensor Array Processing (ASAP), Lexington, Massachusetts, Mar. 11-13, 2003.
117. L. L. Scharf, E. K. P. Chong, and Z. Zhang, "Algebraic equivalence of matrix conjugate direction and matrix multistage filters for estimating random vectors," in Proceedings of the 43rd IEEE Conference on Decision and Control (CDC'04), Atlantis Resort, Paradise Island, Bahamas, Dec. 14-17, 2004, pp. 4175-4179.
118. L. L. Scharf, E. K. P. Chong, M. D. Zoltowski, J. S. Goldstein, and I. S. Reed, "Subspace expansion and the equivalence of conjugate direction and multistage Wiener filters," IEEE Transactions on Signal Processing, vol. 56, no. 10, pp. 5013-5019, Oct. 2008.
119. A. Schrijver, Theory of Linear and Integer Programming. New York: Wiley, 1986.
120. R. T. Seeley, Calculus of Several Variables: An Introduction. Glenview, IL: Scott, Foresman and Co., 1970.
121. J. R. Silvester, "Determinants of block matrices," The Mathematical Gazette, vol. 48, no. 51, pp. 460-467, Nov. 2000.
122. W. Spendley, G. R. Hext, and F. R. Himsworth, "Sequential application of simplex designs in optimization and evolutionary operation," Technometrics, vol. 4, pp. 441-461, 1962.
123. W. A. Spivey, Linear Programming: An Introduction. New York: Macmillan Company, 1963.
124. R. E. Stone and C. A. Tovey, "The simplex and projective scaling algorithms as iteratively reweighted least squares methods," SIAM Review, vol. 33, no. 2, pp. 220-237, June 1991.
125. G. Strang, Introduction to Applied Mathematics. Wellesley, MA: Wellesley-Cambridge Press, 1986.
126. G. Strang, Linear Algebra and Its Applications. New York: Academic Press, 1980.
127. T. W. Then and E. K. P. Chong, "Genetic algorithms in noisy environments," in Proceedings of the 9th IEEE Symposium on Intelligent Control, pp. 225-230, Aug. 1994.
128. L. Vandenberghe and S. Boyd, "Semidefinite programming," SIAM Review, vol. 38, no. 1, pp. 49-95, Mar. 1996.
129. P. P. Varaiya, Notes on Optimization. New York: Van Nostrand Reinhold Co., 1972.
130. D. J. Velleman, How To Prove It: A Structured Approach. Cambridge, England: Cambridge University Press, 1994.
131. L.-X. Wang, A Course in Fuzzy Systems and Control. Upper Saddle River, NJ: Prentice Hall, 1999.
132. B. Widrow and M. A. Lehr, "30 years of adaptive neural networks: Perceptron, madaline, and backpropagation," Proceedings of the IEEE, vol. 78, no. 9, pp. 1415-1442, Sept. 1990.
133. D. J. Wilde, Optimum Seeking Methods. Englewood Cliffs, NJ: Prentice Hall, 1964.
134. R. E. Williamson and H. F. Trotter, Multivariable Mathematics, 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1979.
135. W. I. Zangwill, Nonlinear Programming: A Unified Approach. Englewood Cliffs, NJ: Prentice Hall, 1969.
136. G. Zoutendijk, Mathematical Programming Methods. Amsterdam, The Netherlands: North-Holland, 1976.
137. J. M. Zurada, Introduction to Artificial Neural Systems. St. Paul, MN: West Publishing Co., 1992.







Index
Absolute value
Absolute value penalty function
Activation function
Active constraint
Adaline
Adaptive linear element
Additivity
Affine function
Affine matrix inequality
Affine scaling
Affine scaling method
artificial problem
stopping criterion
strictly interior feasible point
Algebraic Riccati inequality
Algorithm
affine scaling
backpropagation
BFGS
Broyden-Fletcher-Goldfarb-Shanno, see BFGS algorithm
complexity of
conjugate gradient, see Conjugate gradient algorithm
convergence of, see Convergence
Davidon-Fletcher-Powell, see DFP algorithm
DFP
ellipsoid, see Khachiyan's method
exponential complexity
fixed step size
for constrained optimization
genetic
globally monotone
gradient
Gram-Schmidt
interior-point
iterative, See also Search methods
Kaczmarz's
Karmarkar's, see Karmarkar's method
Khachiyan's
Lagrangian
naive random search
Nelder-Mead
particle swarm optimization
polynomial complexity
probabilistic search
projected
projected gradient
projected steepest descent
quasi-Newton, see Quasi-Newton methods
randomized search
rank one
rank two
RLS
secant method
simplex, see Simplex method
simulated annealing
single-rank symmetric
SRS
steepest descent
symmetric Huang family
variable metric
Widrow-Hoff
zero finding
Allocation
Alphabet in genetic algorithm
Argmin
Armijo backtracking algorithm
Armijo condition
Armijo-Goldstein condition
Artificial neural networks, see Feedforward neural networks
Artificial problem
in affine scaling method
in Karmarkar's method
in simplex method
Associative
Asymmetric duality
Augmented matrix

Backpropagation algorithm
as a gradient algorithm
forward pass
reverse pass
Ball
Banana (Rosenbrock's) function
Basic columns
Basic feasible solution
Basic solutions
Basic variables
Basis
definition of
entering
in linear equations
leaving
natural
orthogonal
Beltrami
Best-so-far
BFGS algorithm
Big-oh notation
Bisection method
Bland's rule
Boltzmann
Bolzano-Weierstrass theorem
Boundary
Boundary point
Bounded above
Bounded below
Bounded sequence
Bounded set
Box constraint
Bracketing
Brent's method
Broyden
Broyden-Fletcher-Goldfarb-Shanno algorithm, see BFGS algorithm

Canonical augmented matrix
Canonical form
Canonical representation
Canonical representation of LMI
Canonical tableau
Carrier of polyhedron
Cauchy-Schwarz inequality
Center of gravity
Centroid
Chain rule
Characteristic equation
Characteristic polynomial
Chromosome in genetic algorithm
Circuit
Citation style
Clairaut's theorem
Closed set
Column vector
Combinatorial optimization
Commutative
Compact set
Compatible matrix norm
Complementarity
Complementary slackness
Complex inner product
Complex vector space
Complexity of algorithm
exponential
polynomial
Component of vector
Composite function
Concave function, see Convex function
Condition number
Conjugate direction methods
Conjugate gradient algorithm
Fletcher-Reeves formula
Hestenes-Stiefel formula
nonquadratic problems
Polak-Ribière formula
Powell formula
quadratic problems
stopping criterion
Consistent linear inequalities
Constrained optimization
Constraint
active
box
convex
equality
functional
inactive
inequality
set
Constraint set, See also Feasible set
Continuity
Continuous function
Continuously differentiable function
Contradiction, proof
Contraposition, proof
Contrapositive
Control system
Convergence
fixed-step-size gradient algorithm
globally convergent
gradient algorithms
Kaczmarz's algorithm
linear
locally convergent
Newton's method
of sequence of matrices
order of
penalty method
quadratic (second-order)
rate of
ratio
steepest descent algorithm
sublinear
superlinear
Convergent sequence
Convex combination
Convex constraint
Convex function
definition of
differentiable
equivalent definition of
minimizers of
optimization of
quadratic
strict
twice differentiable
Convex optimization
Convex programming, see Convex optimization
Convex set
definition of
extreme point
in definition of convex function
polyhedron
polytope
properties of
supporting hyperplane
Cooling schedule
Coordinates
Cost function
Courant-Beltrami penalty function
Cramer's rule
Crossing site
Crossover in genetic algorithm
crossing site
multiple-point crossover
one-point crossover
Cubic fit
Curve
Cutting-plane method
Cycling in simplex method

Dantzig
Davidon
Davidon-Fletcher-Powell algorithm, see DFP algorithm
Decision variable
Decomposition
direct sum
orthogonal
Decreasing sequence
Degenerate basic feasible solution
DeMorgan's law
Derivative
partial
Derivative descent search
Derivative matrix
Descent property
Determinant
DFP algorithm
Diagonal matrix
Diet problem
Differentiable curve
Differentiable function
Dimension
Direct sum decomposition
Directional derivative
Discrete Fourier series
Discrete-time linear system
Distributive
Domination
Dual linear program
Dual nonlinear program
Dual quadratic program
Duality
asymmetric
dual nonlinear program
dual problem
dual quadratic program
dual vector
duality theorem
in quasi-Newton methods
Karush-Kuhn-Tucker conditions
linear programming
nonlinear programming
primal nonlinear program
primal problem
primal quadratic program
quadratic programming
symmetric
weak duality lemma
Duality theorem
Dyadic product

Eberhart, Russell
Edge of polyhedron
Eigenvalue
definition of
maximal
minimal
of symmetric matrix
Eigenvector
definition of
of symmetric matrix
orthogonal
relation to Q-conjugacy
Electric circuit
Elementary matrix
elementary row operation
first kind
second kind
third kind
Elementary row operation
Elitism in genetic algorithm
Ellipsoid
Ellipsoid algorithm, see Khachiyan's method
Encoding in genetic algorithm
Entry of matrix
Epigraph
Equality constraint
Estimation
Euclidean inner product
Euclidean norm
Evolution in genetic algorithm
Exact penalty function
Exclusive OR, see XOR
Expanding subspace theorem
Exponential complexity
Extreme point
Extremizer

Face of polyhedron
Farkas's transposition theorem
Feasibility problem
Feasible direction
Feasible point
Feasible set
Feedforward neural networks
activation function
Adaline
backpropagation algorithm
function approximation
hidden layer
input layer
learning
neuron
output layer
single-neuron training
supervised learning
training
training set
unsupervised learning
weights
Fibonacci method
Fibonacci sequence
First-order Lagrangian algorithm
First-order necessary condition equality constraint (Lagrange)
in convex optimization
inequality constraint (KKT)
interior case
set constraint
Fitness in genetic algorithm
Fitting straight line
Fixed point
Fixed step size
Fletcher
Fletcher-Reeves formula
Floor
FONC, see First-order necessary condition
Fourier series
Frobenius norm
Full-rank factorization
Function
affine
banana
composite
concave, see Convex function
continuous
continuously differentiable
convex
cost
derivative matrix of
derivative of
differentiable
directional derivative of
gradient of
graph of
Jacobian matrix of
Lagrangian
linear, see Linear transformation
matrix-valued
maximum rate of decrease
maximum rate of increase
notation
objective
partial derivative of
penalty
Powell
Rosenbrock's
sigmoid
twice continuously differentiable
twice differentiable
uniformly continuous
unimodal
utility
Function approximation
Functional constraint
Fundamental theorem of algebra
Fundamental theorem of linear algebra
Fundamental theorem of LP
Fuzzy linear programming

Gale's transposition theorem
Gauss-Newton method
Generalized eigenvalue
Generalized inverse
Genetic algorithm
alphabet
analysis of
best-so-far chromosome
chromosome
crossover
elitism
encoding
evolution
fitness
initial population
length of schema
mating pool
mutation
offspring
order of schema
parents
population size
real-number
representation scheme
roulette-wheel scheme
schema
selection
stopping criterion
tournament scheme
Gibbs
Global minimizer
Globally convergent
Globally monotone algorithm
Golden section
Golden section search
Goldfarb
Goldstein condition
Gomory cut
Gomory cutting-plane method
Gordan's transposition theorem
Gradient
Gradient descent algorithm, see Algorithm, gradient
Gradient methods
backpropagation algorithm
constrained optimization, see Projected gradient method
convergence of
convergence rate of
descent property
equality constraints, see Lagrangian algorithms
fixed step size
inequality constraints, see Lagrangian algorithms
Lagrangian
order of convergence
projected
stopping criterion
Gram matrix
Gram-Schmidt
Grammian
Graph
Greatest lower bound

Haijan, see Khachiyan
Hadamard product
Hajek
Half-space
negative
positive
Hessian
Hessian matrix
Hestenes, Magnus
Hestenes-Stiefel formula
Hidden layer in neural network
Hoff
Holland, John
Homogeneity
Huang family
Hyperplane
definition of
supporting
tangent to graph

Identity matrix
ILP, see Integer linear programming
Image of matrix, see Range of matrix
Implicit function theorem
Impulse response
Inactive constraint
Inconsistent system of equations
Increasing sequence
Indefinite matrix
Induced matrix norm
Induction, principle of
Inequality constraint
Infimum, see Greatest lower bound
Inner product
complex
Euclidean
properties of
Innovation
Input layer in neural network
Integer linear programming
Integer programming, see Integer linear programming
Interior
Interior point
Interior-point method
Inverse
continuity of
matrix
Inverse Hessian
Inverse parabolic interpolation
Invertible matrix, see Nonsingular matrix
Iterative algorithm, see Search methods

Jacobian matrix
Jordan form

Kaczmarz's algorithm
Kantorovich
Karmarkar
Karmarkar's method
artificial problem
complexity
Karmarkar's canonical form
Karmarkar's restricted problem
projective transformation
simplex
stopping criterion
strictly interior feasible point
Karush-Kuhn-Tucker condition, see KKT condition
Karush-Kuhn-Tucker multiplier, see KKT multiplier
Karush-Kuhn-Tucker theorem
Kennedy, James
Kernel of matrix, see Nullspace of matrix
Khachiyan
Khachiyan's method
KKT condition
KKT multiplier
KKT theorem
Klee-Minty problem
Koopmans
Krylov subspace
Kuhn-Tucker condition, see KKT condition

Lagrange condition
Lagrange multiplier
Lagrange's theorem
Lagrangian algorithms
Lagrangian function
Lanczos, Cornelius
Leading principal minor
Learning in neural network
Least squares
nonlinear
Least upper bound
Left pseudoinverse
Level set
Levenberg-Marquardt algorithm
Levenberg-Marquardt modification
Limit of sequence
Line fitting
Line search
Line segment
Linear combination
Linear convergence
Linear dynamical system, see Discrete-time linear system
Linear equations
augmented matrix
basic solution
basis
canonical augmented matrix
canonical form
degenerate basic solutions
existence of solution
inconsistent
Kaczmarz's algorithm
least-squares solution
minimum-norm solution
overdetermined
particular solution
pivot
solving in general
solving using row operations
Linear function, see Linear transformation
Linear inequalities
consistent
in linear programming
Linear least squares
Linear matrix inequality
Linear programming
affine scaling method
artificial problem in affine scaling method
artificial problem in Karmarkar's method
artificial problem in simplex method
artificial variables in simplex method
as constrained problem
asymmetric duality
basic columns
basic feasible solution
basic solutions
basic variables
Bland's rule
brief history of LP
canonical augmented matrix
canonical tableau
complementary slackness
cycling
degenerate basic feasible solution
dual problem
duality, see Duality
duality theorem
examples of
extreme point
feasible solution
fundamental theorem of LP
fuzzy
geometric view of
integer linear programming
interior-point method
Karmarkar's method, see Karmarkar's method
Karush-Kuhn-Tucker condition
Khachiyan's method
Klee-Minty problem
optimal basic feasible solution
optimal feasible solution
primal problem
reduced cost coefficient
revised simplex method
sensitivity
simplex method
slack variable
standard form
surplus variable
symmetric duality
tableau
two-dimensional
two-phase affine scaling method
two-phase simplex method
uncertain
weak duality lemma
Linear quadratic regulator
Linear regression, see Line fitting
Linear space, see Vector space
Linear transformation
Linear variety
Linear-fractional LMIs
Linearly dependent
Linearly independent
Little-oh notation
LMI, see Linear matrix inequality
LMI solvers
LMI toolbox for MATLAB
LMITOOL
Local minimizer
Locally convergent
Location parameter
Lower bound
LP, see Linear programming
LQR
Lyapunov inequality

MacDuffee
Markov chain
Mating pool in genetic algorithm
MATLAB, xiii
LMI toolbox
Matrix
affine matrix inequality
compatible norm
condition number
continuous
convergence of sequence
definition of
derivative
determinant
diagonal
eigenvalue of, see Eigenvalue
eigenvector of, see Eigenvector
elementary, see Elementary matrix
entry of
full-rank factorization
function, matrix-valued
game theory
generalized inverse
Gram
Hadamard product
Hessian
identity
image of, see Range of matrix
indefinite
induced norm
inverse
invertible, see Nonsingular matrix
Jacobian
Jordan form
kernel of, see Nullspace of matrix
leading principal minor of
left pseudoinverse
linear matrix inequality
minor of
Moore-Penrose inverse
negative definite
negative semidefinite
nonsingular
notation
nullspace of
orthogonal
orthogonal projector
Penrose generalized inverse
positive definite
positive semidefinite
principal minor of
pseudoinverse
range of
rank of
representation of linear transformation
right pseudoinverse
Schur complement
Schur product
sequence of
series of
similar
square
stochastic
submatrix of
Sylvester's criterion
symmetric
totally unimodular
trace
transformation
transpose of
unimodular
Matrix norm
Matrix-valued function
Max
Maximizer
Mean value theorem
MILP, see Mixed integer linear programming
Min
Minimax
Minimizer
description of
global
local
Pareto
strict global
strict local
Minimum norm
Minor
definition of
leading principal
principal
Minty
Mixed integer linear programming
Monotone sequence
Moore-Penrose inverse
Morrison
Multicriteria optimization
Multiobjective optimization
Mutation in genetic algorithm

Naive random search
Natural basis
Negative definite
matrix
quadratic form
Negative half-space
Negative semidefinite
matrix
quadratic form
Neighborhood
Nelder-Mead algorithm
centroid
contraction
expansion
Neural networks, see Feedforward neural networks
Neuron
Newton's method
convergence of
descent direction
descent property
for nonlinear least squares
Gauss-Newton method
general
Levenberg-Marquardt modification of
modification of
of tangents
one-dimensional
order of convergence
Newton-Raphson method, see Newton's method
Non-strict inequality
Nondecreasing sequence
Nondifferentiable optimization
Nondifferentiable penalty function
Nonincreasing sequence
Nonlinear least squares
Nonsingular matrix
Norm
compatible
Euclidean
Probenius
general vector norm
induced
matrix
p-norm
properties of
Normal
Normal plane
Normal space
Notation
Nullspace of matrix

Objective function
Offspring in genetic algorithm
One-dimensional search methods
Open set
Optimal basic feasible solution
Optimal control
Optimal feasible solution in LP
Optimization
combinatorial
constrained
convex
linear, see Linear programming
multicriteria
multiobjective
nondifferentiable
semidefinite
unconstrained, see Unconstrained optimization
vector
with equality constraints
with inequality constraints
with set constraint
Optimization algorithm, see Search methods
Order of convergence
Order symbol
Orthant
Orthogonal
Orthogonal basis
Orthogonal complement
Orthogonal decomposition
Orthogonal matrix
Orthogonal projection
Orthogonal projector
Orthogonal vectors
Outer product
Output layer in neural network
Overdetermined system of equations

Parents in genetic algorithm
Pareto front
Pareto minimizer
Partial derivative
Particle swarm optimization
Particular solution
Penalty function
Penalty method
absolute value penalty function
convergence
Courant-Beltrami penalty function
exact penalty function
nondifferentiable penalty function
penalty function
penalty parameter
Penalty parameter
Penrose, see Moore-Penrose inverse
Penrose generalized inverse
Perp, see Orthogonal complement
Pivot
Polak-Ribière formula
Polyhedron
carrier of
definition of
edge of
face of
in linear programming
vertex of
Polynomial, characteristic
Polynomial complexity
Polytope
definition of
in linear programming
Population in genetic algorithm
Positive definite
matrix
quadratic form
relation to eigenvalues
Sylvester's criterion
Positive half-space
Positive orthant
Positive semidefinite
matrix
quadratic form
relation to eigenvalues
relation to principal minors
Positivity
Powell
Powell formula
Powell function
Primal linear program
Primal nonlinear program
Primal quadratic program
Primal-dual method
Principal minor
Principle of induction
Probabilistic search
Probability vector
Product
dyadic
inner
outer
Product rule
Projected algorithm
Projected gradient method
stopping criterion
Projected steepest descent algorithm
Projection, see Orthogonal projection
Projective transformation
Proof
contradiction (reductio ad absurdum)
contraposition
direct method
methods of
principle of induction
Proportional fairness
Pseudoinverse
Pythagorean theorem

Q-conjugate
definition of
linear independence
relation to eigenvectors
relation to orthogonality

Quadratic convergence
Quadratic fit
Quadratic form
convex
definition of
maximizing
negative definite
negative semidefinite
positive definite
positive semidefinite
Sylvester's criterion
Quadratic programming
Quasi-Newton methods
approximating inverse Hessian
BFGS algorithm
complementarity
conjugate direction property
descent property
DFP algorithm
duality
rank one formula
rank two update
single-rank symmetric
symmetric Huang family
variable metric algorithm

Randomized search
Range of matrix
Rank of matrix
Rank one formula
Rank two update
Rate of convergence
Ratio of convergence
Rayleigh's inequality
Real vector space
Recursive least-squares, see RLS algorithm
Reduced cost coefficient
Reductio ad absurdum
Reeves
Regular point
Relative cost coefficient, see Reduced cost coefficient
Representation scheme in genetic algorithm
Revised simplex method
Revised tableau
Ribière
Riccati inequality
Right pseudoinverse
RLS algorithm
Rosenbrock's function
Roulette-wheel scheme
Row operations
Row vector

Scalar
Scale parameter
Schema in genetic algorithm
length of
order of
Schmidt, see Gram-Schmidt
Schur complement
Schur product
Schwarz, see Cauchy-Schwarz inequality
Schwarz's theorem
Scilab Consortium
Search direction
Search methods
bisection method
conjugate direction methods
conjugate gradient algorithm
constrained optimization
derivative descent search
Fibonacci
general algorithm
genetic algorithm
Golden section
gradient methods
Kaczmarz's algorithm
Lagrangian
line search
naive random search
Nelder-Mead algorithm
neural network training
Newton's method
Newton-Raphson method, see Newton's method
one-dimensional
particle swarm optimization
penalty method
probabilistic
projected
projected gradient methods
quasi-Newton methods
randomized
secant method
simulated annealing algorithm
steepest descent method
Secant method
Second-order necessary condition
equality constraints
inequality constraints
interior case
set constraint
Second-order sufficient condition
equality constraints
inequality constraints
interior case
set constraint
Selection in genetic algorithm
Semidefinite programming
Sensitivity
Sequence
bounded
bounded above
bounded below
convergent
decreasing
Fibonacci
greatest lower bound
increasing
least upper bound
limit
lower bound
monotone
nondecreasing
nonincreasing
of matrices
of real numbers
order of convergence
subsequence of
upper bound
Set
boundary of
bounded
closed
compact
constraint, see Feasible set
convex, see Convex set
feasible
interior of
minus
notation
open
simplex
subset of
Set constraint
Shanno
Sherman-Morrison formula
Sherman-Morrison-Woodbury formula
Shift parameter
Sigmoid
Signal-to-interference ratio
Similar matrices
Simplex
Simplex algorithm, see Simplex method
Simplex method
algorithm
artificial problem
artificial variables
Bland's rule
canonical augmented matrix
canonical tableau
complexity
cycling
exponential complexity
integer linear programming
matrix form
reduced cost coefficient
revised simplex method
revised tableau
row operations
stopping criterion
tableau
two-phase
updating augmented matrix
updating canonical tableau
Simulating annealing algorithm
Simultaneous equations, see Linear equations
Single-rank symmetric algorithm
Singular value decomposition
Slack variable
SONC, see Second-order necessary condition
SOSC, see Second-order sufficient condition
Span
Sphere
Square matrix
SRS algorithm
Standard form linear program
Statement
biconditional
conditional
Steepest ascent
Steepest ascent method, see Steepest descent method
Steepest descent
order of convergence
Steepest descent method
for constrained optimization
for quadratic
projected
Step response
Step size
Stiefel, Eduard
Stochastic matrix
Stopping criterion
affine scaling method
conjugate gradient method
genetic algorithm
gradient method
Karmarkar's method
line search
projected gradient method
simplex method
Strict inequality
Strictly interior feasible point
Strong Wolfe condition
Structured representation of LMI
Subgradient
Sublinear convergence
Submatrix
Subsequence
Subset
Subspace
Superlinear convergence
Supervised learning
Supporting hyperplane
Supremum, see Least upper bound
Surface
Surplus variable
SVD, see Singular value decomposition
Sylvester's criterion
Symmetric duality
Symmetric Huang family
Symmetric matrix
Symmetry

Tableau in linear programming
Tangent line
Tangent plane
Tangent space
Tangent vector
Taylor series, See also Taylor's theorem
Taylor's formula, See also Taylor's theorem
Taylor's theorem
Temperature schedule
Termination criterion, see Stopping criterion
Third-order necessary condition
Third-order sufficient condition
Threshold
Totally unimodular
Tournament scheme
Trace
Training of neural network
Training set
Transformation
affine scaling
linear
matrix
matrix representation of
projective
Transportation problem
Transpose
matrix
vector
Transposition theorems
Traveling salesperson problem
Triangle inequality
Truth table
Tucker, see KKT condition
Twice continuously differentiable function
Twice differentiable function
Two-dimensional linear program
Two-phase affine scaling method
Two-phase simplex method

Uncertainty range
Unconstrained optimization
basics of
conditions for
Uniform continuity
Uniformly continuous function
Unimodal
Unimodular
Unimodular, totally
Unsupervised learning
Upper bound
Utility function

Variable metric algorithm
Variety, linear
Vector
column
complex
component of
convex combination
definition of
difference
field
linear combination
linearly dependent
linearly independent
normal
orthogonal
probability
row
tangent
transpose of
zero vector
Vector field
Vector optimization
Vector space
basis for
complex
definition of
dimension of
real
subspace of
Vertex

Weak duality lemma
Weierstrass theorem
Weighted sum
Weights in neural network
Widrow
Widrow-Hoff algorithm
Wiener filter
Wolfe condition
Woodbury

XOR

YALMIP
Yet Another LMI Package (YALMIP)

Zero finding
Zero matrix
Zero vector



