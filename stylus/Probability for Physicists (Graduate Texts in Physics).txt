



Part IFundamentals of Probability and Probability Distributions










© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_1




1. Basic Terminology




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
The concepts of random experiment, outcomes, sample space and events are introduced, and basic combinatorics (variations, permutations, combinations) is reviewed, leading to the exposition of fundamental properties of probability. A discussion of conditional probability is offered, followed by the definition of the independence of events and the derivation of the total probability and Bayes formulas.




1.1 Random Experiments and Events
A physics experiment can be envisioned as a process that maps the initial state (input) into the final state (output). Of course we wish such an experiment to be non-random: during the measurement we strive to control all external conditions—input data, the measurement process itself, as well as the analysis of output data—and justly expect that each repetition of the experiment with an identical initial state and in equal circumstances will yield the same
	


                  
                  
                 result.
In a random experiment, on the other hand, it may happen that multiple repeats of the experiment with the same input and under equal external conditions will end up in different outputs. The main feature of a random experiment is therefore our inability to uniquely predict the precise final state based on input data. We rather ask ourselves about the frequency of occurrence of a specific final state with respect to the number of trials. That is why this number should be as large as possible: we shall assume that, in principle, a random experiment can be repeated infinitely many times.
A specific output of a random experiments is called an outcome. An example of an outcome is the number of photons measured by a detector, e.g. 12. The set of all possible outcomes of a random experiment is called the sample space, S. In the detector example, the sample space is the set . Any subset of the sample space is called an event. Individual outcomes are elementary events. Elementary events can be joined in compound events: for example, the detector sees more than 10 photons (11 or 12 or 13, and so on) or sees 10 photons and less than 20 neutrons simultaneously.
The events, elementary
	


                  
                  
                

                  
                  
                

                  
                  
                

                  
                  
                

	
	 or compound, are denoted by letters  The event that occurs in all repetitions of the experiment—or can be assumed to occur in all future tries—is called a certain or universal event and is denoted by U. The event that does not occur in any repetition of the experiment is called an impossible event, denoted by  or . The relations between events can be expressed in the language of set theory. Take two events A and B and consider the possibility that at least one of them occurs: this eventuality is called the sum of events and is denoted by

Summing events is commutative and associative: we have  and . The sum of two events can be generalized: the event that at least one of the events  occurs, isThe event that both A and B occur simultaneously, is called the product of events
A and B. It is written
	
 asor simplyFor each event A one obviously has  and . The product of events is also commutative and associative; it holds that  and . The compound event that all events  occur simultaneously, isThe addition and multiplication are related by the distributive rule . The event that A occurs but B does not, is called the difference of events and is denoted by

(In general .) The events A and B are exclusive or incompatible if they can not occur simultaneously, that is, if
	


                  
                  
                
The events A are B
complementary if in each repetition of the experiment precisely one of them occurs: this implies
	

An event complementary to event A is denoted by . Hence, for any event A,Sums of events in which individual pairs of terms are mutually exclusive, are particularly appealing. Such sums are denoted by a special sign:Event sums can be expressed as sums of incompatible terms: (1.1)The set of events

 (1.2)is called the complete set of events, if in each repetition of the experiment precisely one of the events contained in it occurs. The events from a complete set are all possible (), pair-wise incompatible ( for ), and their sum is a certain event: , where n may be infinite.

Example
There are six possible outcomes in throwing a die: the sample space is . The event A of throwing an odd number—the compound event consisting of outcomes ,  or —corresponds to , while for even numbers . The sum of A and B exhausts the whole sample space;  implies a certain event. The event of throwing a 7 is impossible: it is not contained in the sample space at all. 



Example
A coin is tossed twice, yielding either head (h) or tail (t) in each toss. The sample space of this random experiment is . Let A represent the event that in two tosses we get at least one head, , and let B represent the event that the second toss results in a tail, thus . The event that at least one of A and B occurs (i.e. A or B or both) isWe got  but that does not hold in general: if, for example, one would demand event B to yield two heads, , one would obtain . The event of A and B occurring simultaneously isThis implies that A and B are not exclusive, otherwise we would have obtained . The event that A occurs but B does not occur isThe complementary event to A is . 


The sample spaces in the above examples are discrete. An illustration of a continuous one can be found in thermal implantation of ions into quartz () in the fabrication of chips. The motion of ions in the crystal is diffusive and the ions penetrate to different depths: the sample space for the depths over which a certain concentration profile builds up is, say, the interval .


1.2 Basic Combinatorics

1.2.1 Variations and Permutations
We perform
	
	
                    
                    m
                   experiments, of which the first has  possible outcomes, the second has  outcomes for each outcome of the first, the third has  outcomes for each outcome of the first two, and so on. The number of possible outcomes of all m experiments isIf  for all i, the number of all possible outcomes is simply


Example
A questionnaire contains five questions with three possible answers each, and ten questions with five possible answers each. In how many ways the questionnaire can be filled out if exactly one answer is allowed for each question? By the above formulas, in no less than  ways. 


What if we have n different objects and are interested in how many ways (that is, variations) m objects from this set can be reshuffled, paying attention to their ordering? The first object can be chosen in n ways. Now, the second one can only be chosen from the reduced set of  objects,  , and the last object from the remaining . The number of variations is then (1.3)The symbol on the right is known as the Pochammer
	
	
                    
                    
                   symbol.

Example
The letters A, B, C and D () can be assembled in groups of two () in  ways: . Note that in this procedure, ordering is crucial: AB does not equal BA. 


A special case of (1.3) is  when variations are called permutations: the number of permutations of n objects isSpeaking in reverse, n! is the number of all permutations of n objects, while (1.3) is the number of ordered sub-sequences of length m from these n objects.

Example
We would like to arrange ten books (four physics, three mathematics, two chemistry books and a dictionary) on a shelf such that the books from the same field remain together. For each possible arrangement of the fields we have  options, while the fields themselves can be arranged in 4! ways, hence there are a total of  possibilities. 


We are often interested in the permutations of n objects,  of which are of one kind and indistinguishable,  of another kind ,  of the mth kind, while . From all n! permutations the indistinguishable ones  must be removed, hence the required number of permutations is  and is

	
	

 denoted by the multinomial symbol:
 (1.4)



1.2.2 Combinations Without Repetition
In how many
	 ways can we arrange n objects into different groups of m objects if the ordering is irrelevant? (For example, the letters A, B, C, D and E in groups of three.) Based on previous considerations leading to (1.3) we would expect  variations. But in doing this, equal groups would be counted multiple (m!) times: the letters A, B and C, for example, would form  groups ABC, ACB, BAC, BCA, CAB and CBA, in which the letters are just mixed. Thus the desired number of arrangements—in this case called combinations of mth order among n elements without repetition—is

	
	
                    
                    
                  
	 (1.5)The symbol at the extreme right is called the binomial symbol. It can not hurt to recall its parade discipline, the binomial formula
 (1.6)



1.2.3 Combinations with Repetition
In combinations
with repetition
	 we allow the elements to appear multiple times, for example, in combining four letters (A, B, C and D) into groups of three, where not only triplets with different elements like ABC or ABD, but also the options AAA, AAB and so on should be counted. The following combinations are allowed:In general the number of combinations of mth order among n elements with repetition is (1.7)In the example above (, ) one indeed has .



1.3 Properties of Probability
A random experiment always leaves us in doubt whether an event will occur or not. A measure of probability with which an event may be expected to occur is its relative frequency. It can be calculated by applying "common sense", i.e. by dividing the number of chosen ("good") events A to occur, by the number of all encountered events: in throwing a die there are six possible outcomes, three of which yield odd numbers, so the relative frequency of the event  "odd number of points" should be . One may also proceed pragmatically: throw the die a thousand times and count, say, 513 odd and 487 even outcomes. The empirical relative frequency of the odd result is therefore . Of course this value will fluctuate if a die is thrown a thousand times again, and yet again—to 0.505, 0.477, 0.498 and so on. But we have reason to believe that after many, many trials the value will stabilize at the previously established value of 0.5.
We therefore define
	 the probability P(A) of event A in a random experiment as the value at which the relative frequency of A usually stabilizes after the experiment has been repeated many times1 (see also Appendix A). ObviouslyThe probability of a certain event is one, . For any event A we havehence also : the probability of an impossible event is zero. For arbitrary events A and B the following relation holds: (1.8)For exclusive events,  and the equation above reduces towhich can be generalized for pair-wise exclusive events asTo generalize (1.8) to multiple events one only needs to throw a glance at (1.1): for example, with three events A, B and C we read offtherefore also (1.9)


Example
(Adapted from [3].) In the semiconductor wafer production impurities populate the upper layers of the substrate. In the analysis of 1000 samples one finds a large concentration of impurities in 113 wafers that were near the ion source during the process, and in 294 wafers that were at a greater distance from it. A low concentration is found in 520 samples from near the source and 73 samples that were farther away. What is the probability that a randomly selected wafer was near the source during the production (event N), or that it contains a large concentration of impurities (event L), or both?
We can answer the question by carefully counting the measurements satisfying the condition: . Of course, (1.8) leads to the same conclusion: the probability of N is , the probability of L is , while the probability of N and L occurring simultaneously—they are not exclusive!—is , henceIgnoring the last term, P(NL), is a frequent mistake which, however, is easily caught as it leads to probability being greater than one. 



Fig. 1.1Detector of cosmic rays. [Left] Sub-detectors wired in a nine-fold coincidence. [Right] Triplets of sub-detectors wired in a three-fold coincidence


Example
(Adapted from [4].) A detector of cosmic rays consists of nine smaller independent sub-detectors all pointing in the same direction of the sky. Suppose that the probability for the detection of a cosmic ray shower (event E) by the individual sub-detector—the so-called detection efficiency—is . If we require that the shower is seen by all sub-detectors simultaneously (nine-fold coincidence, Fig. 1.1 (left)), the probability to detect the shower (event X) is
	
	
                    
                    
                  
	The sub-detectors can also be wired in three triplets, where a favorable outcome is defined by at least one sub-detector in the triplet observing the shower. Only then a triple coincidence is formed from the three resulting signals (Fig. 1.1 (right)). In this case the total shower detection probability iswhere we have used (1.9). 




1.4 Conditional Probability
Let A be an
	


	 event in a random experiment (call it 'first') running under a certain set of conditions, and P(A) its probability. Imagine another event B that may occur in this or another experiment. What is the probability  of event A if B is interpreted as an additional condition for the first experiment? Because event B modifies the set of conditions, we are now actually performing a new experiment differing from the first one, thus we generally expect . The probability  is called the conditional probability of event A
under the condition B or given event B, and we appropriately denote it by P(A|B). This probability is easy to compute: in n repetitions of the experiment with the augmented set of conditions B occurs  times, while  occurs  times, thereforeThe conditional probability for A given B () is therefore computed by dividing the probability of the simultaneous event, , by P(B). Obviously, the reverse is also true:Both relations can be merged into a single statement known as the theorem on the probability of the product of events or simply the product formula:
 (1.10)The first part of the equation can be verbalized as follows: the probability that A and B occur simultaneously equals the product of probabilities that A occurs first, and the probability that B occurs, given that A has already occurred. (The second part proceeds analogously.)
The theorem can be generalized to multiple events. Let  be arbitrary events and let . Then (1.11)Perhaps the essence becomes even clearer if we reverse the ordering of the factors and digest the formula from right to left:


Example
What is the probability that throwing a die yields a number of spots which is less than four given that the number is odd? Let A mean "odd number of spots" (), and B "the number of spots less than four" (). If A and B occur simultaneously, the probability of the compound event can be inferred from the intersection of sets A and B in Fig. 1.2 (left): it issince only elements  inhabit the intersection, while the complete sample space is . But this is not yet the answer to our question! We are interested in the probability of B once A ("the condition") has already occurred: this implies that the sample space has shrunk to  as shown in Fig. 1.2 (right). From this reduced space we need to pick the elements that fulfill the requirement B: they are  and thereforeEquation (1.10) says the same: . We can imagine that the unconditional probability  has increased to  by the additional information that the throw yields an odd number. 



Fig. 1.2The conditional probability in throwing a die. [Left] The probability of events A and B occurring simultaneously corresponds to the intersection of the sets  and  within the complete sample space S. [Right] The condition A first isolates the set  from the complete S. The conditional probability of B given A corresponds to the fraction of the elements in this set that also fulfill the requirement B



Example
A box in our cellar holds 32 bottles of wine, eight of which are spoiled. We randomly select four bottles from the box for today's dinner. What is the probability that not a single one will be spoiled?
This can be solved in two ways. The first method is to apply the product formula by considering that with each new bottle fetched from the box, both the total number of bottles and the number of spoiled bottles in it are reduced by one. Let  denote the event that the ith chosen bottle is good, and A the event that all four bottles are fine. The probability of the first bottle being good is . This leaves 31 bottles in the box, 23 of which are good, hence the probability of the second bottle being intact is . Analogously  and  for the third and fourth bottle, respectively. Formula (1.11) then givesThe second option is to count the number of ways in which 24 good bottles can be arranged in four places: it is equal to  (see (1.5)). But this number must be divided by the number of all possible combinations of bottles in four places, which is . The probability of four bottles being good is then




Example
An electric circuit has five independent elements with various degrees of reliability—probabilities that an element functions—shown in the figure.






What is the probability that the circuit works (transmits signals from input to output) and the probability that A does not work, given that the circuit works?
Let us denote the event "element A works" by A (and analogously for the elements B, C, D and E). The circuit works (event V) when the elements A and B work or the elements C, D and E work or all five of them work, hencewhere we have used (1.8). The probability that A has failed (event ), given that the circuit works, is obtained by the following consideration, noting that . We first calculate the probability that A does not work while the circuit as a whole works. If A has failed, then the bottom branch of the circuit must work. But even if A is inoperational, two options remain for B: it either works or it does not. Thus . Thus the conditional probability we have been looking for iswhere we have used , since . 


1.4.1 Independent Events
If events 

A
	 and B are independent, the probability that A occurs (or does not occur) is independent of whether we have any information on B (and vice-versa), henceAccording to (1.10), the probability that such events occur simultaneously equals the product of probabilities of them occurring individually: (1.12)When more than two events are involved, independence must be defined more carefully. The events in the setare mutually or completely independent if, for every combination  of kth order without repetition () among the numbers , it holds that (1.13)When  this system of equations has the formwhich is a special case of (1.11); when , the leftover of (1.13) is simplyIf (1.12) applies to any pair of events in , we say that such events are pair-wise independent, but this is still a far cry from mutual (complete) independence! There are  combinations without repetition among n elements (see (1.6) with ). One of them corresponds to the empty set, while there are n combinations of the first order, as we learn from (1.5). The system above therefore imposes  conditions that must be fulfilled by the events from  in order for them to be mutually independent. In the special case  there are four such conditions:This important distinction between pair-wise and mutual independence is discussed in the following Example.






Example
The spin in a quantum system can have two projections:  (spin "up", ) or  (spin "down", ). The orientation of the spin is measured twice in a row. We make the following event assignments: event A means "spin  in the first measurement", event B is "spin  in the second measurement", and event C is "both measurements show the same projection". The sample space for the measured pairs of orientations is , while the chosen three events correspond to its subsets ,  and  shown in the Figure. We immediately obtain the probabilitiesas well asSinceevents A, B and C are pair-wise independent. On the other hand,so the events are not
mutually
	
 independent. 




1.4.2 Bayes Formula
When an event
	
	
                    
                    
                    A
                   occurs under different, mutually exclusive conditions, and we know the conditional probabilities of A given all these conditions, we can also calculate the unconditional probability of A. The two-condition case is illustrated by the following classic insurance-company example.

Example
An insurance company classifies the drivers into those deemed less () and those more accident-prone (). These are two mutually exclusive 'conditions'—call them B and —that exhaust all options, as there is no third class, thus , . On average, a first-class driver causes a crash every 10 years, and the second-class driver once in 5 years. Let A denote the event of an accident, regardless of its cause. The probability for a first-tier driver to cause a crash within a year is , while it is  for the second-tier driver. What is the probability that a new customer will cause an accident within the first year? Since for any A and B, , we also havewhile from (1.10) it follows that (1.14)Statistically, the company may therefore expect the probability offor a newly insured driver to cause an accident within a year. 


Equation (1.14) is a sort of weighted average over both driver classes, where the weights depend on conditions B and . Suppose that there are more such mutually exclusive conditions: we then prefer to call them assumptions or hypotheses and denote them by : we have  or  ... or , exhausting all possibilities. The set of all  constitutes a complete set defined by (1.2), henceApplying the left-hand side of (1.10) to each term separately yields the so-called total probability formula
 (1.15)illustrated in Fig. 1.3.Fig. 1.3Illustration of the total probability formula. Mutually exclusive conditions or hypotheses  are disjoint sets that partition the sample space S and therefore also an arbitrary event A


Let us recall (1.10) once more, this time in its second part, whence one reads off  orThe denominator of this expression is given by (1.15) and the final result is the famous Bayes formula [5] (1.16)A random experiment may repeatedly yield events A, but the events  conditioning A—with corresponding probabilities —occurred prior to
A. The quantities  are therefore called prior probabilities since they are, in principle, known in advance. In contrast, the left side of the Bayes formula gives the probability that the hypothesis  is valid with respect to the later outcome A.
                    
                    
                  
	

 The conditional probability  is called posterior, since it uses the present outcome A to specify the probability of  occurring prior to A. This is why the Bayes formula is also known as the theorem on probability of hypotheses.

Example
A company decides to manufacture cell-phones by using processor chips of different suppliers. The first type of chip is built into , the second into , and the third into  of devices. A randomly chosen device contains chip i (event ) with probability , where ,  and : these are the known prior probabilities. Some chips are unreliable, causing the devices to malfunction. The probability that a cell-phone breaks down (event A), given that it contains chip i, is . The manufacturer establishes ,  and .
We go to a store and buy a cell-phone made by this company. It breaks down immediately (event A at this very moment). What is the probability that it was manufactured (event  in the past) in the factory installing type-i chips ()? We are looking for the posterior probabilities  given by the Bayes formula. Its denominator contains , which is common to all three cases—and this is the probability that the cell-phone breaks down. This leads toOf course
	


	 we also have . 





1.5 Problems

1.5.1 Boltzmann, Bose-Einstein and Fermi-Dirac Distributions
(Adapted from

	
	
                    
                    
                  
	
                    
                    
                   [1].) Imagine a system of n particles in which the state of each particle is described by p values (components of the position vector or linear momentum, spin quantum number, and so on). Each particle state can be represented by such a p-plet, which is a point in p-dimensional space. The state of the whole system is uniquely specified by a n-plet of such points.
Let us divide the phase space into N () cells. The state of the system is described by specifying the distribution of states among the cells. We are interested in the probability that a given cell is occupied by the prescribed number of particles. Consider three options:  The particles are distinguishable, each cell can be occupied by an arbitrary number of particles, and all such distributions are equally probable. We then say that the particles "obey" Boltzmann statistics: an example of such a system are gas molecules.  The particles are indistinguishable, but the cells may still be occupied by arbitrary many particles and all such distributions are equally probable. This is the foundation of Bose-Einstein statistics obeyed by particles with integer spins (bosons), e.g. photons.  The particles are indistinguishable, each cell may accommodate at most one particle due to the Pauli principle [6]. All distributions are equally probable. This case refers to the Fermi-Dirac statistics applicable to particles with half-integer spins (fermions), e.g. electrons, protons and neutrons.

 Let  be the event that there are precisely k particles () in a certain cell, regardless of their distribution in other cells.  Each of the n particles can be put into any of the N cells, even if other particles are already sitting there. All particles can therefore be arranged in  ways and this is the number of all possible outcomes. How many correspond to event ? Into the chosen cell one can pour k particles in  ways, while the remaining  particles can be arranged into the other  cells in  ways. Event  therefore accommodates  outcomes, thus
 Since particles are indistinguishable and each cell is allowed to swallow an arbitrary number of particles, the number of all possible distributions equals the number of combinations of nth order among N elements with repetition (1.7), i.e. . How many are acceptable for ? Event  occurs precisely when k particles are selected for a given cell—since they are indistinguishable, this can be accomplished in one way only—while the remaining  are distributed among  cells, which amounts to combinations of order  among  elements with repetition, i.e. . It follows that
 Since at most one particle is allowed to occupy any single cell, all possible distributions can be counted by choosing n cells out of N and putting one particle into every one of them: this can be accomplished in  ways. How many of them correspond to event ? For  there are none, while for  or  there are as many ways as one can arrange  particles over  cells, which is . Thereforewhile  for . Figure 1.4 (left) shows the probabilities  for all three distributions in the case , , while Fig. 1.4 (right) shows the Boltzmann and the Bose-Einstein distribution in the case , .Fig. 1.4The probability of finding k particles in any chosen cell of a N-cell phase space flooded with n particles, in the case of Boltzmann (B), Bose-Einstein (BE) and Fermi-Dirac (FD) statistics. [Left] , . The sum of all probabilities within a given distribution of course equals 1, as it is obvious e.g. in the case of the Fermi-Dirac distribution: , . [Right] , 




1.5.2 Blood Types
The fractions

	
	
 of blood types O, A, B and AB in the whole population are
 Two persons are picked at random from the population. What is the probability of their having the same blood type, and what is the probability that their types differ?  We pick four people from the same population. What is the probability that precisely k () blood types will be found among them?

 Let us replace the letter notation  by indices 1, 2, 3, 4, and let  denote the probability that a person has blood type i ().  All possible pairs are , , each having probability , therefore . The complementary event has probability  which, in a more arduous manner, can be computed as:
 Let P(k) denote the probability that precisely k blood types will be found in the chosen four. For  the quartets are , , hence . For  we use (1.4) to obtain the number of possible combinations in samples of the form  (), which is , and the number of combinations in samples of the form  (), which is . We getThe calculation for  is tedious and is best avoided by calculating the probability for , which is , and accumulating all previously computed P(k) into the complementary event: .


1.5.3 Independence of Events in Particle Detection
Two detectors are

	 used to detect charged particles with different parities (mirror symmetries of their wave-functions): pions ( and ) and kaons ( and ), all possessing negative parity, as well as protons (p), deuterons (d) and  and  nuclei, all of which have positive parities. Assume that all particles appear with equal frequencies and assign indices  to types . Let A denote the event that the first detector has seen a negative-parity particle. Let B denote the event that the second detector has detected a positive-parity particle, and suppose thatLet C denote the event that both detectors observe particles with equal parities. Are events A, B and C (pair-wise or mutually) independent?

 There are 64 equally probable outcomes (i, j) in an experiment where the first and second detector detect particles i and j, respectively; 16 of them are pion-kaon combinations fulfilling condition C:as well as 16 combinations of atomic nuclei,thus . Suppose that the first detector has seen a negative-parity particle and has thereby imposed condition A: then C occurs if the second detector also reports a negative-parity particle (probability 1 / 2), implying . Analogously we conclude , and finallyWe conclude that A, B and C are pair-wise but not mutually independent since  does not hold true. Our calculation shows that , while  is an impossible event: if there is a negative-parity particle in the first detector and a positive-parity particle in the second one, we can not have the same parity in both detectors, thus .
How do these considerations change if the detectors are inefficient in detecting heavier nuclei ( and )? Do events A, B and C remain independent? How does the result change in physically more sensible circumstances in which the number of pions exceeds the number of kaons by a factor of 100?


1.5.4 Searching for the Lost Plane
The authorities
	 believe that an airliner has been lost in one of the three regions  () in which the crash has occurred with equal probability, . Let  denote the probability that the plane search in region i will locate the plane that actually does lie in i. Calculate the conditional probability that the plane crashed in region i, given that the search in region 1 was unsuccessful!

 Let  () denote the event that the plane went down in region i, and N the event that the search in region 1 was unsuccessful. Bayes formula for  giveswhile for  and  one getswhere we have exploited the fact that the search in region 1 must be unsuccessful if the plane lies in region 2 or 3, hence . For example, if , the probability that the plane is in region 1—given that it has not been found in it—is . Note that .


1.5.5 The Monty Hall Problem 

In the Monty

	 Hall TV show with three boxes (adapted from [7, 8]) one box contains the car keys while the remaining boxes are empty. When the contestant picks one of the boxes (e.g. box 1), Monty Hall (MH) tells her: "I'll make you a favor and open one of the remaining boxes that does not contain the keys (e.g. 2). Thus the keys are either in your chosen box or in box 3, so the probability of your winning the car has increased from 1 / 3 to 1 / 2." The contestant (C) responds: "I've changed my mind. I prefer to pick box 3 instead of box 1."

 Is Monty's claim correct? What is the probability of the contestant winning the car if she changes her mind following Monty's disclosure, and what is her chance of winning if she insists on her initial choice?  Suppose that the contestant has been playing this game for a long time and knows that different boxes have different probabilities of containing the keys, e.g. 50, 40 and  for boxes 1, 2 and 3. What is the most promising strategy in this case?

 Two observations are crucial: MH knows which box contains the keys and obviously does not wish to reveal it; he opens one of the two remaining boxes at random and with equal probability. The answer to  can then be obtained by simple counting of possible outcomes shown in Table 1.1: 'W' means that the contestant 'wins', 'L' means 'loses'. (All information is contained in the first three rows since the rest consists just of cyclic permutations.) The probability of C winning the car when insisting on the initial choice is 1 / 3. The probability of winning the car after having changed her mind is 2 / 3. Consequently, Monty's claim is false.Table 1.1Possible outcomes in the Monty Hall contestKeys are inC picksMH opensOutcomeC switchesOutcome112 or 3W1 for 3 or 2L123L2 for 1W132L3 for 1W213L1 for 2W221 or 3W2 for 3 or 1L231L3 for 2W312L1 for 3W321L2 for 3W331 or 2W3 for 2 or 1LBoth contestant's strategies are shown: "C picks" means the one and only choice of the box, while "C switches" means that the contestant selects a different box after Monty's disclosure

The problem can be approached from another, more intuitive viewpoint [9]. Suppose C decides to always switch. If she chooses an empty box, she can not lose: MH is then obliged to open the other empty box, so, by switching, C gets the only remaining box—the one containing the keys. C loses only if she initially chooses the box with the keys. Whether this strategy of "perpetual switching" works depends only on the initial choice of the empty box (probability 2 / 3) or the box containing the keys (probability 1 / 3).
Conditional probability offers yet another vantage point. Suppose that C chooses box 1 while the keys are in box 2 (event A, ). MH opens box 3 (event B). The graph





then tells us that  and , hence, by Bayes formula,In two thirds of the cases the keys are in the remaining box, so C doubles her 1 / 3 chance of winning by switching. The same conclusion can be reached by analyzing the sample space in which the events are not equally probable. Denote all possible outcomes by (i, j), where i is the box containing the keys, j is the box opened by MH, and let  denote the corresponding probability for such an outcome. When we shall later become familiar with the concept of random variables, all these values will be merged into the expression (1.17)which we shall read as: "The discrete variable X is distributed such that the probability of outcome (1, 3) is , the probability of outcome (2, 3) is , and so on." In a compact manner, however, we can write down the sample space with the probability values attached as subscripts:In this notation,  and . Since , we haveMonty's tempting offer to increase the contestant's probability of winning to 1 / 2 is based on the wrong assumption that the remaining two possible events (1, 3) and (2, 3) are equally probable—i.e. that the sample space after the condition A has been imposed is —leading to the wrong result .
The best strategy for  is: C should choose the least probable box (box 3); when MH reveals an empty box, C should switch. In this case C will win  of the time. The limiting case that box 3 never holds the keys is also covered: MH reveals the other empty box so, by switching, C always wins.


1.5.6 Bayes Formula in Medical Diagnostics
We have fallen
	
	
                    
                    
                  
	
                    
                    
                  
	
                    
                    
                  
	
                    
                  
	
 ill with fever and visit a doctor. Recently he has read some news on the west Nile virus that, on average, infects one person per million. He draws a blood sample for a test that has a positive outcome in  of the cases where the disease is actually present (the so-called sensitivity of the test), and a negative outcome in  of the cases where the disease is not present (the so-called specificity of the test). The test of our blood comes out positive.  What is the probability that we are actually infected by the virus?  Analyze the more general case of a disease probed by a larger number of tests or exhibiting multiple symptoms.

 Let us denote the positive outcome of the test by H ("high titer") and negative by L ("low titer") and write the corresponding conditional probabilities in Table 1.2. Now just read it carefully.  The probability that the test is positive and the disease (D) is in fact present, is indeed . But the probability that we are actually infected by the virus, given the test was positive, is P(D|H), and can be computed by using the Bayes formula (1.16):where we have used  and thus, due to the complementarity of H and L, . But the numerator also contains the prior probability that, as a random member of the population, we catch the disease at all, which is . This results in a very small probability
Table 1.2Conditional probabilities in a diagnostic test that can be negative when the disease is absent (specificity ), negative in spite of the disease (false negative), positive with no disease (false positive) or positive with the disease present (sensitivity ) Disease absentDisease presentNegative test



Positive test






 When a disease manifests itself in several symptoms or tests (), the posterior probability for the disease is still given by the Bayes formulabut it becomes useless in practical cases. Namely, for a specific disease  from a set of n diseases and a single symptom S one would have the expressionwhich becomes much more complex by adding new symptoms. With each new symptom  added to the previous set of symptoms , one would have to computeFor a diagnostic system incorporating, say, 50 diseases and 500 symptoms that may occur individually or collectively in any of these diseases, we would require the data on  conditional probabilities. In the so-called naive Bayes approach one therefore frequently assumes that the symptoms are independent in the sense thatThe first equation states that the probability for  to appear in a part of the population that also exhibits symptom  is equal to the probability of  appearing in the whole population. The second approximation says that the probability of  appearing in a part of the population that has the disease D and some other symptom , is equal to the probability of  appearing in all persons having the disease D. These simplifications allow us to operate with far fewer conditional probabilities —only  in the example above—expressing the probability of  given the presence of the disease  [10]:Inevitably, the assumption of symptom independence is quite coarse: given the presence of the disease, the probability of two symptoms appearing simultaneously is larger than the product of probabilities of individual symptoms. (If we have a headache and know it was caused by the flu, we will most likely develop a sore throat as well.)


1.5.7 One-Dimensional Random Walk 

(Adapted from [1].) A particle

	 moves along the real axis, starting at the origin (). Consecutive random collisions uniformly spaced in time send it one step to the left () or to the right () with probabilities 1 / 2 either way.  What is the probability that after 2n collisions the particle will return to  without ever meandering into the  region? Five random walks are shown for illustration in Fig. 1.5 (left). For example, walk number 3 that has always remained at  and has terminated at  after 100 collisions is "acceptable".  Verify your result by a computer simulation. (Random walks will be discussed more generally in Sects. 6.​7 and 6.​8.)Fig. 1.5[Left] Five one-dimensional random walks with 100 time steps. We are looking for the fraction of the walks that terminate at the origin (event A) and never blunder to  (condition B), as in walk number 3 shown here. [Right] The ratio between the simulated and theoretical expectation value for event AB



 Each random walk is a consequence of 2n collisions. Each collision shifts the particle to the left () or to the right (), thus the number of all possible walks is . Let A be the event that the particle returns to the origin after 2n collisions, and B the event that the particle does not wander to  during 2n collisions. We are looking for the probability P(AB), where .

 Let us first determine P(A). From  possible and equally probable walks only those are acceptable for event A that end up at (2n, 0), like the walk in Fig. 1.6 denoted by the full line. In all of them the particle has experienced n unit kicks to the left and n unit kicks to the right. The number of all such walks can be calculated by counting all possible ways of choosing n collisions that result in a left (or right) shift, from the total 2n collisions. There are  such ways, thereforeFrom the walks ending up at (2n, 0) and thereby fulfilling condition A, we should disregard those that fluctuate to  if we wish to satisfy condition B. How do we count such occurrences? For each such walk (from the very moment it has crossed the boundary and reached the point ) we imagine a new walk, which is the mirror image of the remainder of the previous walk across the  axis (dashed line in Fig. 1.6). The new walk certainly terminates at  and is therefore composed of  right and  left shifts. Hence, under condition A,  do not fulfill B, while  do. This implies thatThe probability we have been looking for is therefore (1.18)
Fig. 1.6A random walk that enters the region  after a certain time—still returning to the origin after 2n steps—and its mirror image from that moment on


 You do not trust this calculation? Let us try to check it by a simple computer simulation. For each n chosen in advance, start with a particle at the origin, then randomly add  or  to its current position and write down its final coordinate after 2n steps. A walk that ends up at  and has never erred into  is counted as "good". If for each n we repeat N walks, we may expect that the ratio of the good walks and all attempted walks will approach the calculated probability (1.18) in the limit . Let us denote this simulated probability by . Figure 1.5 (right) shows the ratio between  and the theoretical P(AB) as a function of the walk duration n for three different numbers N of how many times the simulation was re-run. Apparently our calculation was correct: with increasing N the ratio does stabilize near 1. The thick line in the figure still looks wiggly? It is! Recall that for  there are  all possible walks, while we have performed only a million of them at each n.



References


1.
R. Jamnik, Verjetnostni račun (Mladinska knjiga, Ljubljana, 1971)MATH


2.
P. Gregory, Bayesian Logical Data Analysis for the Physical Sciences (Cambridge University Press, Cambridge, 2005)CrossRefMATH


3.
D.C. Montgomery, G.C. Runger, Applied Statistics and Probability for Engineers, 5th edn. (Wiley, New York, 2010)MATH


4.
A.G. Frodesen, O. Skjeggestad, H. Tøfte, Probability and Statistics in Particle Physics (Universitetsforlaget, Bergen, 1979)


5.
T. Bayes, An essay toward solving a problem in the doctrine of chances. Philos. Trans. 53, 370 (1763)CrossRefMATH


6.
J.J. Brehm, W.J. Mullin, Introduction to the Structure of Matter (Wiley, New York, 1989)


7.
S. Selvin, A problem in probability. Am. Stat. 29, 67 (1975)CrossRef


8.
S. Selvin, On the Monty Hall problem. Am. Stat. 29, 134 (1975)CrossRef


9.
M.A. Carlton, Pedigrees, prizes, and prisoners: the misuse of conditional probability. J. Stat. Educ. 13(2) (2005)


10.
S. Schwartz, J. Baron, J. Clarke, A casual Bayesian model for the diagnosis of appendicitis, Conference on Uncertainty in Artificial Intelligence (UAI-86) (Elsevier Science Publishers, Amsterdam, 1986), p. 423




Footnotes


1


This is the so-called frequentist approach to probability, in contrast to the Bayesian approach: an introduction to the latter is offered by [2].

 













© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_2




2. Probability Distributions




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
Starting with the examples of distributions in general, the Dirac delta and the Heaviside unit functions are presented, followed by the definition of continuous and discrete random variables and their corresponding probability distributions. Probability functions, probability densities and (cumulative) distribution functions are introduced. Transformations of random variables are discussed, with particular attention given to the cases where the inverse of the mapping is not unique. Two-dimensional cases are treated separately, defining joint and marginal distributions, as well as explaining the variable transformation rules in multiple dimensions.



Having become acquainted with the basic properties of probability, we shall devote this chapter to the question of how probability can be related to the all-pervading concept of distribution. We introduce two general-purpose tools, the so-called Dirac delta "function" and the Heaviside step function, then move on to random variables and their discrete and continuous probability distributions.

2.1 Dirac Delta
The value of a real function f of a real variable x at  can be calculated
          
        , of course, by evaluating f(0). But we would like to possess a mathematical tool—denote it by —that supplies f(0) as the result of integrating f over the whole real axis, (2.1)Physicists call this tool the "Dirac delta". It is a sort of functional, since it maps from a function space to the range of f—for purposes of our discussion, let this be simply . It seems to operate as a multiplication of f by a very narrow spike (Fig. 2.1 (left)), resulting in the value of f at the origin. This is the reason one often identifies this tool as a genuine  "function". Due to its property (2.2)which is nothing but (2.1) in the special case , one often hears even the—completely nonsensical—claim that the  "function" is normalized. From the strict mathematical point of view, the Dirac delta is neither a function nor a functional, but a measure (see [1] and Appendix A).Fig. 2.1[Left] Illustration of definition (2.1). The integral of the product of a continuous function f and the spike "function"  yields the value of f at the origin. [Right] A narrow and deep square potential V(x), for which only the product of its linear dimension and depth is relevant, can be approximated by 


Obviously the  "function" must possess a unit inverse to the unit of x, since f(x) and f(0) must have the same units. If x measures distance (unit [m]), then  must have unit []. In atomic physics, for example, a very narrow and deep square-well potential V(x) with depth  (in [eV]) and width a (in [nm]) around the origin at  (Fig. 2.1 (right)) can be written asBy writing V in this manner we wish to say that in the limits  and , such that the product  [] remains constant, the precise shape of V(x) is irrelevant: in computing the expectation values with such a potential only the value of the integrand f at the origin matters. Obviously we have acquired a tool that allows us to represent any point, point-like or very compact quantity in physics, for example, a point electric charge or a tiny mass.
The box-like picture does not appear fancy enough? Two very popular ways to convey the essence of the Dirac delta are the limit (in the sense of functions) of the Gaussian, which we will discuss later, and the Fourier integral representation:We proceed analogously in multiple dimensions. The one-dimensional Dirac delta can be generalized to  asIn this case  must have units of [] if x, y and z are in .

2.1.1 Composition of the Dirac Delta with a Function
How does the Dirac delta behave when its argument is a function, as in ? If the function  has precisely one zero at , i.e. , and satisfies the condition , then [1] (2.3)The simplest case is , hence  and . It follows from (2.3) that by an additive change of the variable, , the Dirac delta yields the functional value corresponding to a translation along the abscissa:We imagine that the Dirac delta "combs" the real axis and thereby "samples" the function f at . When g has several simple zeros , (2.3) must be considered in the vicinity of each zero separately: (2.4)In such a case (2.3) is replaced byThe zeros  must be simple (single). The formulas listed above may not be used in the case of multiple zeros, . One can use the same tool to deal with the case , , , corresponding to  and . Rescaling the argument x by a non-zero a then yields , which we write symbolically as (2.5)or, in three dimensions, as . Let us generalize this to the case that the vector  is rescaled by a matrix A instead of the scalar a! We havewhere  is the appropriate volume element. Formula (2.5) is then replaced by


Example
To calculate the effect of the Dirac delta when its argument is the function  possessing two real simple zeros,  and , (2.4) must be applied with . We obtainSuch a form can be used, for instance, to describe two very narrow and very high potential layers (upward-facing square-well potentials) centered at  and .    


It is worth mentioning that the Dirac delta is part of the tool used primarily in quantum mechanics to evaluate the integrals of the form (2.6)where E and T denote energy and time, respectively. This tool is based on the Sokhotsky-Plemelj theorem dealing with a particular family of Cauchy integrals along closed curves C in the complex plane, providing the limit values of the integral from both sides of C. The version of the theorem on the real axis states that
          

                    
                    
                  
Here  denotes the principal (generalized) value of the integral, so this compact notation must actually be read as (2.7)where . We add an infinitesimally small negative real term to the purely imaginary argument of the exponential function in (2.6), integrate over time, and apply (2.7):




2.2 Heaviside Function
The Heaviside function H is defined as (2.8)From  to just slightly below  the integral yields zero; but as soon as we cross , the value of the integral jumps to 1 according to (2.2) and stays there until . The function H is therefore also known as the step function (Fig. 2.2 (left)). Two handy
          
        

          
           analytic approximations of H (Fig. 2.2 (right)) are (2.9)
 (2.10)In literature one occasionally encounters a non-standard definition of the step function, where , , and . Its symmetry about the origin does establish a neat resemblance to these analytic approximations, but one should use it carefully.Fig. 2.2[Left] Heaviside function, known as the unit step. We define it to be continuous from the right. (Alternative definition with continuity from the left is also possible.) [Right] Analytic approximations (2.9) and (2.10) of the Heaviside function



2.3 Discrete and Continuous Distributions
Before we try to understand probability distributions, consider a distribution of some well-known physical quantity like, for example, mass. What is the spatial distribution of mass in the globular cluster NGC 7006 shown in Fig. 2.3?Fig. 2.3The globular cluster NGC 7006 at a distance of approximately 135, 000 light years from the Earth contains hundreds of thousands of stars. [Left] Photograph taken by the Hubble Space Telescope. [Right] The spatial distribution of mass density within the cluster (and in any set of point masses) can be described by (2.11)

From the viewpoint of gravity individual stars can be treated as point bodies, since the stars in the cluster do not overlap and gravity acts as if they were compacted to single points, their respective centers of mass. The spatial dependence of mass density within such a cluster—and in any set of point masses—can be described by the formula (2.11)where  is the mass of the individual body and  is its position vector. Only at distances smaller than the star radii this description becomes inadequate and forces us to abandon the discrete picture and switch to the continuum. Within an individual star, of course, the distribution of mass is given by the densitywhich makes physical sense in the limit . But even this limit must be taken with a grain of salt: descending the order-of-magnitude ladder to ever smaller volumes and into the realm of molecules and atoms, the continuous description again becomes inappropriate and must be replaced by discrete distributions.


2.4 Random Variables
          
        

                  
                  
                  
                

                  
                

                  
                

                  
                

Each outcome of a random experiment is specified by the value of one or more random or stochastic variables. Random variables are functions, defined on the sample space S. Their role is to assign a number to each possible outcome in S; in addition, the frequency with which a certain number occurs, is associated with the corresponding probability. For example, if throwing a die is considered to be a random process with sample space (2.12)the value of a random variable X "communicates" the outcome:We denote random variables by upper-case and their values by lower-case letters. An individual outcome is called the realization of a random variable or draw. The probability for any outcome in (2.12) is 1 / 6, hence we can write, as in (1.​17):



2.5 One-Dimensional Discrete Distributions
          
        

                  
                

                  
                  
                  
                

A discrete random variable is a random variable that can assume a finite number of different values  (). Let the points  on the real axis be arranged such that . The probability that in a particular repetition of the experiment X acquires the value , is written as (2.13)The function
           
         is called the probability [mass] function that corresponds to a discrete probability distribution. Probability is a non-negative quantity, thereforeThe outcomes of an experiment () constitute a complete set of events (1.​2), hence the sum of the probabilities of individual outcomes is one: (2.14)This equation states that the probability distribution is normalized.
It makes sense to define the probability that X assumes a value smaller than or equal to some value x. For example, in throwing a die we are interested in the probability that the observed number of spots (random variable X) is less than four () or that the number of spots is ,  or . The sum of probabilities must therefore collect ("accumulate") the values ,  and . In other words, the sum in (2.14) should not be pulled all the way
          
        

                  
                

                  
                  
                

                  
                  
                  
                

                  
                  
                

          
           to n
         but only up to . This sum is given by the [cumulative] distribution function
Since  are non-negative,  is a non-decreasing function.The definition domain of  formally ranges from  to , so  certainly vanishes from the left extreme of the real axis until just below the point , while it is equal to one from the point  upwards, since by that point all possible  have been collected in the sum:When moving along the x axis, "continuity from the right" applies to :Hence, for any value  encountered while combing x in the positive sense,  jumps to a value which is  higher than the previous one.

Example
A fair die is thrown twice. What is the expected distribution of the sum of spots from both throws after many trials? Let the variable X measure the sum of spots, which can be , . There are  possible outcomes, all equally probable (1 / 36). But different sums are not equally probable. The sum of 2 can be obtained in a single way, namely by one spot appearing on the first die and one on the second, hence . The sum of 3 can be realized by  or , thus . The sum of 4 appears in three cases: ,  or , thus , and so on, up to . Hence X can be assigned the probability distribution shown in Fig. 2.4 (top). It is non-zero only at eleven points  (values denoted by circles), and zero elsewhere.
The distribution function  is shown in Fig. 2.4 (bottom). It vanishes from  to , where it jumps to the value . With increasing x, each  adds a value of  to , where it remains until it bumps into the next point, . When the last point () has been accounted for, we have exhausted all possibilities: henceforth, up to , the value  stays fixed.
Let us calculate the probability that the sum of spots is at most :What is the probability that it is more than 6? We should not plunge blindly into the calculation. Indeed , but one also seesThe probability that we encounter a sum of spots less than 1 is zero, of course: . By common sense or by looking at the figure we also realize that  and .    



Fig. 2.4Discrete probability distribution of individual outcomes (sum of spots) in two throws of a die. [Top] The probability distribution of the sum of spots, given by the values of the probability function . [Bottom] Cumulative distribution function 




2.6 One-Dimensional Continuous Distributions
In continuous
          
        

                  
                

          
          
           probability distributions we can never speak of "a probability that a continuous variable X assumes a value x". This would be just as inappropriate as claiming that a certain point along a thin wire with linear mass density  has a finite mass. A point is a mathematical abstraction with dimension zero and can not contain a finite mass. We can only refer to probabilities that " of a thin wire has a mass of ", "a random variable X has a value between a and b", "value between x and ", and so on. Analogously to the mass distribution a continuous probability distribution can be assigned a probability density function, which is non-negative and normalized
          
        :An example of a continuous probability density is shown in Fig. 2.5. The corresponding distribution function  can be obtained by integrating the density  from  (or the extreme left of its definition domain) up to the current x. From "tiny bits of probability"  one obtains the probability
          
        

                  
                

                  
                  
                

                  
                  
                  
                

                  
                  
                

          
           that 
        :If these relations are valid, we say that "variable X is distributed according to the distribution " and denote this as  or . (The same convention applies for discrete distributions.) If the variables X and Y are distributed according to the same distribution, we write . The obvious fact thatwill serve us well later. Just as before, the cumulative distribution is a non-decreasing function: from  up to the leftmost edge of the interval where  ( in Fig. 2.5),  vanishes. With increasing x, an ever larger portion of probability is integrated into the cumulative distribution from this point upwards, until the rightmost edge of the domain of  is reached (). Here  becomes equal to 1 and remains so all the way to .Fig. 2.5An example of a normalized probability density  (thin curve, left ordinate) for a continuous probability distribution which differs from zero only on the interval [0.5, 2.5] (arrows), and the corresponding distribution function  (thick curve, right ordinate). The probability  is equal to the area of the shaded region (integral of  from  to ) and also equal to 


The probability that X assumes a value on the interval  is given by the definite integral of the probability density over this range,The integral can be written as a difference of integralstherefore alsoThe shaded region in Fig. 2.5 shows the area under the graph of  on the interval , which equals the probability . The same result is obtained by subtracting .

Example
Let X be distributed according to the density , where C is a constant and  (Cauchy distribution). What is the probability that the value of  lies between  and 1? We first determine C:whence . The condition  is fulfilled on two intervals, as one can have either  or . These "events" are mutually exclusive, so the corresponding probabilities should be summed:The distribution function isOf course  and , as one expects of a distribution function, as well as .    




2.7 Transformation of Random Variables
          
        

                  
                

In this section
          
           we learn how to determine the distribution of a random variable calculated from another random variable with a known distribution. Let the random variable X be distributed according to the density . We are interested in the distribution of the variable Y which is some given function of X,where  is differentiable and monotonous on D (increasing or decreasing everywhere): this means that  implies , i.e. its inverse is unique (bijective mapping from one interval to another). Suppose that h increases on D (the contrary case is derived analogously). Thenwhere we have transformed the independent variable, , and the upper integration boundary, . If  is continuous at , then  is differentiable at y, hence the desired result is (2.15)


Example
Let us commence with an example from a nuclear physicist's daily lab routine: a planar problem of a point radioactive source and a linear detector (e.g. an electrode in a particle-tracking wire chamber) at a distance d from the source at its nearest point (Fig. 2.6).Fig. 2.6A planar problem with a point radioactive source and infinitely long thin detector. The isotropic radiation from the source (uniform distribution over angles ) is distributed according to the Cauchy distribution along the detector (y coordinate)

The source radiates isotropically (from  to  in the whole plane or from  to  in the lower half-plane), and the detector has a constant sensitivity. The random variable  (this used to be X in (2.15)), which measures the emission angle  of the radiated particle, is therefore uniformly distributed,But what is the distribution of radiation along the wire? We must convert the distribution over  to a distribution over x. From the figure we infer (2.16)therefore . According to (2.15) we obtainwhich is a correctly normalized Cauchy distribution, since . If we read the above equation in reverse, we learn something else: the values of y, randomly distributed according to the Cauchy distribution, can be obtained by randomly picking numbers , uniformly distributed between  and , and calculating y by using (2.16).Fig. 2.7[Left] Path lengths of particles flying from the source to the detector at angle  in the setup of Fig. 2.6. One interval on the ordinate corresponds to two intervals on the abscissa: the inverse of  is not unique. [Right] Path-length distribution

What, then, is the distribution of flight path lengths of particles flying from the source to the detector? (The question is relevant because different flight paths imply different energy losses, meaning that the particles will be detected with different energies along the wire.) The flight path length is : now the functional form of h is different, see Figs. 2.6 and 2.7 (left). Thus , and the same rule as above yields (2.17)The variable s is defined on . When we wish to check whether the distribution with the density  is normalized, a surprise is in store:What went wrong? When the variable  runs through its definition domain, the variable s runs through its respective domain twice. For a correct normalization we should therefore multiply the density (2.17) by 2. In other words: the inverse of the function  is not unique, since an arbitrary interval of s corresponds to two equally long intervals of , as shown in Fig. 2.7 (left). How this discrepancy is handled will be discussed in the following.    



2.7.1 What If the Inverse of  Is Not Unique?
When the function  is not bijective, an interval on the ordinate corresponds to two or more intervals on the abscissa (see Fig. 2.8). Suppose that for each  there is a finite set . Let  for some , and let h be differentiable, except in a countable number of points. By the inverse function theorem, there exists an open interval  including x and an open interval  including y, such that h (restricted to ) is bijective and its inverse  exists and is differentiable. In other words, for each  there exists a function  such that  for each  in the neighborhood of y and  for each  in the neighborhood of . If needed, the interval  containing the values y for which all inverses  are defined, can be made small enough to render all  distinct. Assume , where . ThusLet , , differentiate both sides of the equation with respect to  and finally let . It follows that (2.18)
Fig. 2.8An example of the mapping  whose inverse is not unique: an interval on the ordinate corresponds to three intervals on the abscissa; they must be accounted for separately when transforming the probability densities according to (2.18)


Example
A random variable X is distributed according to the density , where  is a normalization constant. We would like to calculate the distribution  of the random variable , where (2.19)This function is shown in Fig. 2.8. Let us restrict ourselves to  which also dictates the normalization of : the rightmost branch of h reaches the value  at , where , so  andHence the correctly normalized density isAny subinterval on the y-axis () corresponds to three distinct intervals on the x-axis, lying in the separate definition domains of (2.19). In the leftmost domain we have , so the inverse function there is . Similar results for the remaining two domains are readily obtained:We use (2.18) to calculateWe also computehence the distribution with density  is also correctly
              
              
             normalized.    





2.8 Two-Dimensional Discrete Distributions
          
        

                  
                

It is not hard to generalize our discussion to two-dimensional probability distributions: first we are dealing with two discrete random variables X and Y, for which we define a joint probability [mass] function,with the properties  and . Suppose that X assumes the values  and Y assumes the values . By analogy to (2.13) the probability that  and  equals
Fig. 2.9[Left] Discrete two-dimensional probability distribution with a joint probability function . [Right] Continuous two-dimensional distribution with a joint density 


An example of a two-dimensional discrete distribution with the probability function , where , , and  is a normalization factor, is shown in Fig. 2.9 (left). The probability that  (regardless of Y) is obtained by summing the contributions of all , while the probability for  (regardless of X) is calculated by summing the contributions of all :As usual, the symbols  and  denote the projections of the two-dimensional distribution  to the corresponding distributions pertaining to the variables X and Y alone. Such one-dimensional projections are called marginal distributions. One must ensure the overall normalizationhence also(Compute the normalization factor  introduced above as an exercise!) We define the two-dimensional (joint) cumulative distribution function as the sum of all contributions to probability for which  and , i.e.If the events  and  are independent for all x and y, it holds that
          
        

                  
                
Such random variables X and Y are called independent. In that case we also have (2.20)



2.9 Two-Dimensional Continuous Distributions
By now a generalization
          
        

                  
                

                  
                

          
          
           of continuous probability distributions to two dimensions should not prove a tough nut to crack. One introduces a joint probability density [function], which is non-negative throughout the definition domain,and normalized,An example of a probability distribution with such probability density on the domain  is shown in Fig. 2.9 (right)—calculate the appropriate normalization factor ! The probability that a continuous random variable X takes a value between a and b and a continuous random variable Y takes a value between c and d, is equal to the integralindicated in the figure by the rectangular cut-out  and . The probability in this example is the volume of the column under the cut-out of the  graph. The corresponding joint distribution function
          
        

                  
                

                  
                  
                 isAnalogously to the discrete case we obtain the probabilities for  by integrating the joint density over the whole domain of Y, and vice-versa: (2.21)
 (2.22)Hence, the marginal probability densities are
          
          
        

                  
                
 (2.23)In the continuous
          
        

           case we call the variables X and Y independent if the events  and  are independent for all x and y, i.e.which is equivalent to (2.24)It is important that precisely in this case we also have (2.25)Let us complete the story on joint densities by adding the concept of conditional probability. We inquire about the probability that event A occurred (e.g. ), with the additional information ("condition") that B also occurred (e.g. ), glancing at (1.​10). With continuous distributions, however, it is meaningless to speak of the probability that a random variable assumed some precise value; the statement must therefore be understood in the density sense: (2.26)and (2.27)For example, the probability that , given , is(This can also be interpreted as the definition of the conditional density .) 


Example
Let the continuous random variables X and Y possess the joint probability densityshown in the figure. Note that the domain is only the shaded part of ! What are the marginal probability densities  and , and the conditional densities  and ? We first check the normalization of :The marginal density  is obtained by integrating  over all possible values of Y, which is from  to  (vertical dark-shaded band),while the marginal density  is calculated by integrating  over all values of X, i.e. from  to  (horizontal band):The conditional probability densities are (2.28)where x is a variable and y a parameter, and (2.29)here y is a variable and x is a parameter. If our calculation was right, all densities should be correctly normalized, as we have only been tailoring the integration to the desired density. By elementary integration we indeed find outFinal question: are X and Y independent? The form of the function  might mislead us into believing that a factorization like, for example, , already implies that X and Y are independent. But for independence we have required (2.25), which certainly does not apply here, sinceThe culprit, of course, is the narrowing of the domain  to the triangle: the  restriction prevents the variables X and Y from grazing freely.    




2.10 Transformation of Variables in Two and More Dimensions
          
          
        

In Sect. 2.7 we learned how a probability distribution of a single random variable can be transformed into a distribution of another variable which is a function of the former. We would like to generalize the result of (2.18)—disregarding the issue of uniqueness, already (2.15)—to several dimensions. Instead of the scalar function of a scalar variable, , we are now dealing with vector quantities:  is a vector of n independent variables, distributed according to the probability density , and  is a vector-valued function, which uniquely maps  to a corresponding vector , so that for the values  and  we haveThe n
        -dimensional
        

          
           generalization of the derivative of h is the Jacobi total derivative matrix:
 (2.30)Comparing this to (2.15) it is easy to see that the probability density  of the variable  is given by (2.31)whereIt is also useful to note that (2.32)


Example
Let X and Y be independent random variables with the joint density (2.33)The function  maps a pair of variables (X, Y) into a pair , such that for their values, arranged as vectors  and , it holds that (2.34)where the  function is sensitive to the quadrant of the pair (x, y). The inverse of  isand the corresponding  Jacobi matrix (2.30) isIts determinant is (2.35)From (2.31) it then follows thatThis means that the variable D is exponentially distributed (as we learn later, "with parameter 1 / 2"), while  is uniformly distributed, with values on the interval . Besides, D are  independent. (Explain why!)
The example can also be read in reverse. Start with an exponentially distributed variable D (with parameter 1 / 2) and a uniformly distributed, independent variable , and combine them asThen , where the function  is already known from (2.34). The Jacobi matrix corresponding to the inverse function  isand has determinant 2, as one can see from (2.35) and (2.32) without even computing the matrix, since . Hencewhich is precisely (2.33). We have learned in passing how one can form a pair of independent, normally distributed (Gaussian) variables: pick a value of D from the exponential distribution with parameter 1 / 2 and a value of  from the interval , then calculate . See also Sect. C.2.5.    



Long example (Retold after [2].) Continuous random variables X and Y are distributed according to the joint probability densitywhich is already normalized, since . Find the probability density corresponding to the linear combination ! If we wish to reap the fruits of our previous lessons, we must assign to the pair of variables X and Y another pair  and  such that the mapping will be unique. LetThe first choice is motivated by the problem statement itself while the second choice will soon become clear: in short, it allows us to keep the integration bounds as simple as possible and ensure a non-zero Jacobi determinant. We solve both equations for x and y—in other words, we find the inverse functions  and :From here we can infer that with respect to the original domains of X and Y,the variables U and V span the rangesdenoted by the shaded area in Fig. 2.10 (left).Fig. 2.10Finding the probability density of the variable . [Left] Calculation in the domain of the transformed variables U and V. [Right] Calculation in the domain of the original variables X and Y


Just as in the previous example the new density  is calculated by evaluating the old density  with transformed arguments and multiplying the result by the absolute value of the Jacobi determinant:thereforeIf this two-dimensional probability density (corresponding to random variables U and V) is integrated over v, we obtain the one-dimensional density corresponding to the variable . In doing so we must pay attention to correct integration boundaries of the areas denoted by I, II and III in Fig. 2.10:
Alternative Method

There is another path leading to the same goal. We first calculate the distribution function of the variable U, i.e. the probability for "event" ,where  and , and differentiate the resulting function with respect to u. The integration boundaries must again be tailored carefully: we are now integrating over a rectangular domain of the original variables x and y, but the condition  slices it into three distinct sub-domains shown in Fig. 2.10 (right). In domain I we obtainwhere we have first integrated over y and then over x. Reversing the order of integration would yield the same result, but the upper integration boundaries must be adjusted accordingly:Finally (2.36)which is exactly the same expression as before. We exploit the same machinery to handle the contributions from regions II and III. In the end, we should also check the normalization: we find , as expected.

Swiss Army Knife Approach

We can perhaps shed a different light on the direct integration of the joint density by resorting to the Dirac delta tool. All cuts through the definition domain in Fig. 2.10 (right) are straight lines of the form . By inserting the Dirac delta with the argument  in the integrand this straight-line constraint is enforced, while the integral over x becomes trivial. We getwe just need to pay attention to the intervals for the integration over y. These depend on the values of u, but in such a way that x never leaves the interval [0, 4] and y never leaves [1, 5]. As before, this results in three domains,on which the final integral over y should be evaluated. In the first domain, for example, we obtain
        which
          
          
         is identical to (2.36).    



2.11 Problems

2.11.1 Black-Body Radiation
          

The distribution of energy spectral density of black-body radiation with respect to wavelengths  at temperature T is given by the Planck formula
          

                    
                    
                  
where h is the Planck and  is the Boltzmann constant (Fig. 2.11 (left)). Calculate the distribution over the frequencies  (Fig. 2.11 (right)) and show that the maxima of the two distributions are not at the same location, i.e. !Fig. 2.11Planck law of black-body radiation. [Left] Temperature dependence of the energy spectral density in terms of wavelengths. [Right] Temperature dependence of the density in terms of frequencies. Wien curves connecting the maxima of both distributions are also shown [3]


 Only one independent variable is involved, so the frequency distribution is obtained by the chain rule for derivativesThe value on the abscissa which corresponds to the maximum of the distributions can be obtained by solving the equation for the local extremum. In the case of the wavelength spectrum one needs to solve the equation , whencewhile in the case of the frequency distribution we need to solve , which becomesThese dimensionless equations have analytic solutions (see [4], p. 94), but they can be harnessed numerically by iteration. The solution of the first equation is . At the temperature on the surface of the Sun () this means , i.e. blue light (visible part of the spectrum). The solution of the second equation is . At the same temperature T, , which is in the infra-red.Fig. 2.12[Left] Planar detector with plates at a distance h. The radioactive source is at the origin. The ratio  is given. [Right] The expected distribution of particle energy losses



2.11.2 Energy Losses of Particles in a Planar Detector
            
          

(Adapted from [5].) Consider a planar detector consisting of two parallel infinite plates spaced apart by h (Fig. 2.12 (left)). A radioactive source attached to the bottom plate radiates  particles with energy . The space between the plates is filled with gas in which particles lose energy. A particle flying unhindered loses all its energy along a distance called the range (R), which scales roughly as , where k is a known constant. The electric signals picked up on the plates are proportional to the energy loss E of particles in the gas. We are interested in the distribution of the pulse heights.

 In radioactive decay of nuclei at rest no direction of the sky is privileged; the  particles are emitted isotropically into the solid angle , hence . This implies  and  (upper hemisphere). The remaining energy of a particle emitted under the angle  and hitting the top plate after flying over a distance of  is . If the plate were not there, the particle could have flown an additional distance . Let us introduce dimensionless quantities  and , so that the equation for the range becomes . We read off  from the figure, therefore . Our task is to express the original distribution over  by the distribution over x, which we accomplish by the derivative chain rule: (2.37)In Fig. 2.12 (right) this probability density is shown by the curve on the interval from  to . The lower edge of the interval corresponds to the smallest possible energy loss of the particle in the gas: it occurs if the particle flies vertically upwards from the source, so that  and therefore .
What about the particles that are emitted under large enough angles to lose all their energy (meaning ) and never reach the top plate? From the geometry we deduce that the fraction of such particles is , and they contribute to the energy loss distribution with an additional term (2.38)Only then the sum of (2.37) and (2.38) is correctly normalized, so that .


2.11.3 Computing Marginal Probability Densities from a Joint Density
Continuous random variables X and Y are distributed according to the joint probability densitywhere C is the normalization constant.  Determine C and calculate the probabilities ,  and .  Compute the distribution functions  and , then differentiate them with respect to x and y to obtain the marginal probability densities  and .

  The normalization constant is determined by integrating the density  over the whole definition domain:hence . The required probabilities are
 The cumulative distributions can be computed by resorting to (2.21) and (2.22):and thencewhile one has  outside of the specified regions. Of course we can reach the same conclusion more easily, through (2.23).


2.11.4 Independence of Random Variables in Two Dimensions
Suppose that the coordinate q and the velocity p of some body are random, that they always lie on the intervals  and , and that by measuring them a distribution of points in phase space (q, p) is observed (top figure). What is the fraction of points in the phase space corresponding to a  deviation from the linear relationwhere  and ? The relation is denoted by the dashed line, while the conditionis indicated by the shaded area. In dimensionless variables  and  the condition becomes  (see bottom panel).





Of course the events in the indicated region can be simply counted, but let us try to envision a simple model. The gradual increase of the density of points from  towards  and from  to  suggests that perhaps the mechanism behind the observed pattern can be described by a distribution of continuous variables X and Y with a joint densitywhere C is a constant. Normalize the distribution, then calculate  , i.e. the probability that the values of X and Y are restricted to the shaded band.  Calculate the one-dimensional probability densities  and . Are the variables X and Y independent? (Do not forget that this is only a model!)

 First we normalize the joint density:
 The required probability is obtained by integrating the density over two regions: the dark-shaded region defined by  and , and the light-shaded region defined by  and :
 The variables X and Y are independent:therefore we indeed observe .


2.11.5 Transformation of Variables in Two Dimensions
Let two independent continuous variables X and Y be described by the joint probability densityCalculate the joint probability density of random variables U and V, where  ,  and  , !

 In the case  the system of equations relating x and y to u and v has a unique solutionThe Jacobi determinant of this system isand its absolute value is . From (2.31) we obtainLet us check the normalization of the density ! Suitable definition domains of the transformed variables must be considered,  and . Then indeedIn the case  the system has two solutions: (2.39)where . The Jacobi determinant for the first solution isand its absolute value is . Equation (2.31) then yieldsLet us again check the normalization:What have we missed? In the case  the mapping from (u, v) to (x, y) is not unique, so the problem must be split into such domain segments that on each of them the inverses  and  are unique—precisely in the same spirit as in the one-dimensional problem of Sect. 2.7.1. We must evaluate (2.31) on each of these segments and sum the contributions. Since |J| is the same for both solutions of (2.39), all that is missing for the correct probability density is a factor of 2, thus



2.11.6 Distribution of Maximal and Minimal Values
Let  be independent and identically distributed continuous random variables.  What is the distribution
            
          

                    
                    
                  

            
             of their maximal value(One can inquire about the distribution of U because U itself is a random variable.) Derive the general expression and apply it in the case that all  are described by the exponential probability density of the form , where , .  What is the distribution of the minimum of such variables,
 Problem  can be solved by using distribution functions. If the maximal value of all  should be smaller than some x, all  simultaneously should be smaller than x, henceFor an individual exponentially distributed X it holds thatthereforeTo obtain the probability density corresponding to U, we only need to compute the derivative of ,Problem  can be solved analogously. If all  are simultaneously larger than some x, the minimal  is certainly also larger than x:For exponentially distributed X we havewhenceThe probability density corresponding to variable V is thenIn other words, if each of the n independent variables  is exponentially distributed (with parameter ), their minimal value is also exponentially
            
          

                    
                    
                  

                    
                    
                   distributed, but with parameter .



References


1.
J.C. Ferreira, Introduction to the theory of distributions, Pitman Monographs and Surveys in Pure and Applied Mathematics (Addison Wesley Longman Ltd., Harlow, 1997)


2.
M.R. Spiegel, J. Schiller, R.A. Srinivasan, Theory and Problems of Probability and Statistics, 4th edn. (McGraw-Hill, New York, 2012)


3.
J.J. Brehm, W.J. Mullin, Introduction to the Structure of Matter (Wiley, New York, 1989)


4.
S. Širca, M. Horvat, Computational Methods for Physicists (Springer, Berlin, 2012)MATH


5.
I. Kuščer, A. Kodre, Mathematik in Physik und Technik (Springer, Berlin, 1993)MATH














© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_3




3. Special Continuous Probability Distributions




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
Particular continuous distributions encountered on a daily basis are discussed: the simplest uniform distribution, the exponential distribution characterizing the decay of unstable atoms and nuclei, the ubiquitous normal (Gauss) distribution in both its general and standardized form, the Maxwell velocity distribution in its vector and scalar form, the Pareto (power-law) distribution, and the Cauchy (Lorentz, Breit-Wigner) distribution suitable for describing spectral line shapes and resonances. Three further distributions are introduced (-, Student's t- and F-distributions), predominantly used in problems of statistical inference based on samples. Generalizations of the exponential law to hypo- and hyper-exponential distributions are presented.



In this chapter we become acquainted with the most frequently used continuous probability distributions that physicists typically deal with on a daily basis.

3.1 Uniform Distribution
	

	
Its name says it all: the uniform distribution describes outcomes of random experiments—a set of measured values of a random variable—where all values between the lowest (a) and the highest possible (b) are equally probable. A bus that runs on a 15-min schedule, will turn up at our stop anywhere between  and  from now: our waiting time X is a continuous random variable distributed uniformly between a and b, which one denotes asThe probability density corresponding to the uniform distribution U(a, b) is (3.1)(Fig. 3.1 (left)) and its distribution function isIf we show up at the bus stop at a random instant, the probability that our waiting time will not exceed , is
Fig. 3.1[Left] The probability density of the uniform distribution U(a, b). [Right] The probability density of the exponential distribution with parameter 



Example
On a hot day, a house-fly mostly sits still, but occasionally takes off to stretch its legs. Suppose that the time T of its buzzing around is uniformly distributed between 0 and , i.e. . What is the probability that it will fly for more than  (event A) given that it flies for more than  (condition B)? Due to the additional information B the probability density is no longer  but -, hence


The same result can be obtained by using the original density  and direct application of the conditional probability formula:No matter how trivial the example is, do not forget that computing a conditional probability imposes a restriction on the sample space!    




3.2 Exponential Distribution


                  
                  
                
	
The exponential distribution is used to describe processes in which the probability of a certain event per unit time is constant: the classical example is the time-dependence of the radioactive decay of nuclei, but it is also used in modeling the distribution of waiting times in queues or durations of fault-free operation (lifetimes) of devices like light bulbs or computer disks.
The decay
	 of an unstable atomic nucleus is a random process par excellence (see also Sects. C.3 and 3.2.1). For a single nucleus, it is impossible to predict the precise moment of its decay; the probability for it to decay in some time interval depends only on the length of this interval, , not on the age of the nucleus. We say that the nuclei "do not age" and that radioactive decay is a "memory-less" process: suppose that we have been waiting in vain for time t for the nucleus to decay; the probability that the decay finally occurs after , is independent of t, (3.2)If the interval  is short enough, we can assume that the decay probability is proportional to , and then the only choice becomeswhere  is the decay probability per unit time [, also called the decay constant, while  is the characteristic or decay time. The probability
	


                  
                  
                
that
	 a nucleus has not decayed yet after  is . The probability that it has not decayed after a longer time , meaning that it will decay at some time , is therefore (3.3)Since , we can immediately calculate the corresponding probability density, (3.4)shown in Fig. 3.1 (right). (As an exercise, check the validity of (3.2)!) Let us think in a complementary way: the probability that the nucleus has not decayed until time t must equal the probability that it will decay at some instant from t until , i.e. the corresponding integral of the density we have just derived. IndeedIt is incredible how many wrong interpretations of these considerations can be heard, so let us reiterate: Equation (3.3) gives the probability that until time t the nucleus has not decayed. At time zero this probability equals 1 and exponentially drops to zero henceforth: every unstable nucleus will decay at some time. The rate of change of the number of nuclei—nuclei still available for decay—is given by the differential equation  with the initial condition , and its solution is (3.5)The decay constant  is determined experimentally by counting the number of decays R(t) until time t. Since , it follows from above that , thereforeBy fitting this functional dependence to the measured data we extract .

Mini-example Two counters in a bank are busy serving a single customer each: the first person has just arrived, while the other has been there for 10 min. Which counter should we choose in order to be served as quickly as possible? If the waiting times are exponentially distributed, it does not matter.    


Example
You do not believe the Mini-example? Let the variable T measure the time between consecutive particle hits in a Geiger-Müller counter, where T is exponentially distributed, with a characteristic time of  [1]. The probability that we detect a particle  after the counter has been switched on, is (3.6)Now imagine that we switch on the detector and three minutes () elapse without a single particle being detected. What is the probability to detect a particle within the next ? Intuitively we expect that after three minutes the next particle is "long over-due". But we need the conditional probabilityHereand , thus , which is the same as (3.6). The fact that we have waited 3 minutes without detecting a particle, has no influence whatsoever on the probability of detection within the next .    



Example
Customers A and B arrive simultaneously at two bank counters. Their service time is an exponentially distributed random variable with parameters  and , respectively. What is the probability that B leaves before A?
Let  and  be random variables measuring the actual service time. The probability that A has not been served until  is . The corresponding probability for customer B is . Since the waiting processes are independent, their joint probability density is the product of individual probability densities:Therefore the required probability isThe limits are also sensible: if the clerk serving B is very slow (), then , while in the opposite case .    


The conviction that exponential distributions are encountered only in random processes involving time in some manner, is quite false. Imagine a box containing many balls with diameter d. The fraction of black and white balls is p and , respectively [2]. We draw the balls from the box and arrange them in a line, one touching the other. Suppose we have just drawn a black ball. What is the probability that the distance x between its center and the center of the next black ball is exactly iD, ()? We are observing the sequences of drawn balls or "events" of the formso the required probability is obviouslySince these events are exclusive, the corresponding probability function is a sum of all probabilities for individual sequences:Abbreviating  and  this can be written assince . Suppose we take the limits  and  (i.e. there are very few black balls in the box and they have very small diameters), such that  and x remain unchanged: then , and the corresponding density is , which is indeed the same as (3.4).

3.2.1 Is the Decay of Unstable States Truly Exponential?
The exponential distribution offers an excellent phenomenological description of the time dependence of the decay of nuclei and other unstable quantum-mechanical states, but its theoretical justification implies many approximations and assumptions, some of which might be questionable in the extremes  and . Further reading can be found in [3] and the classic textbooks [4-6].



3.3 Normal (Gauss) Distribution


                  
                  
                
	
It is impossible
	


	
	 to resist the temptation of beginning this Section by quoting the famous passage from Poincaré's Probability calculus published in 1912 [7]:

 [The law of the distribution of errors] does not follow from strict deduction; many seemingly correct derivations are poorly argued, among them the one resting on the assumption that the probability of deviation is proportional to the deviation. Everyone trusts this law, as I have recently been told by Mr. Lippmann, since the experimentalists believe it is a mathematical theorem, while the theorists think it is an experimental fact.1


The normal (Gauss) distribution describes—at least approximately—countless quantities from any sphere of human existence and Nature, for example, diameters of screws being produced in their thousands on a lathe, body masses of people, exam grades and velocities of molecules. A partial explanation and justification for this ubiquity of the Gaussian awaits us in Sect. 6.​3 and in particular in Chap. 11. For now let us simply become acquainted with the bell-shaped curve of its two-parameter probability density (3.7)shown in Fig. 3.2 (top).Fig. 3.2[Top] Normal distribution  with average (mean)  and positive parameter  determining the peak width. Regardless of  the area under the curve equals one. [Bottom] Standardized normal distribution N(0, 1)

The definition domain itself makes it clear why the normal distribution is just an approximation in many cases: body masses can not be negative and exam grades can not be infinite
. The distribution is symmetric around the value of , while the width of its peak is driven by the standard deviation
; at  the function  has an inflection. The commonly accepted "abbreviation" for the normal distribution is . In Chap. 4 we will see that  is its average or mean and  is its variance.
The cumulative distribution function corresponding to density (3.7) is
where
	


                  
                
 (3.8)is the so-called error function which is tabulated (see Tables D.1 and D.2 and the text below). The probability that a continuous random variable, distributed according to the density (3.7), takes a value between a and b, is (3.9)


3.3.1 Standardized Normal Distribution
When handling normally distributed data it makes sense to eliminate the dependence on the origin and the width by subtracting  from the variable X and divide out , thereby forming a new, standardized random variableThe distribution of Z is then called standardized normal and is denoted by N(0, 1) (zero mean, unit variance). It corresponds to the probability density (3.10)while the distribution function is (3.11)The values of definite integrals of the standardized normal distribution (3.12)for z between 0 and 5 in steps of 0.01, which is sufficient for everyday use, are listed in Table D.1. The abscissas  or  () are particularly important. The areas under the curve  on these intervals,are equal to (3.13)(see Fig. 3.2 (bottom)) and tell us what fraction of the data (diameters, masses, exam grades, velocities) is within these—completely arbitrary—intervals and what fraction is outside. For example, if we establish a normal mass distribution of a large sample of massless particles (smeared around zero due to measurement errors), while a few counts lie above , one may say: "The probability that the particle actually has a non-zero mass, is ." But if the distribution of measurement error is indeed Gaussian, then even the extreme  events in the distribution tail may be genuine! However, by increasing the upper bound to , ,... we can be more and more confident that the deviation is not just a statistical fluctuation. In modern nuclear and particle physics the discovery of a new particle, state or process the mass difference or the signal-to-noise ratio must typically be larger than .






Example
(Adapted from [1].) The diameter of the computer disk axes is described by a normally distributed random variable  with average  and standard deviation , as shown in the figure. The required specification (shaded area) is . Let us calculate the fraction of the axes that fulfill this criterion: it is equal to the probability , which can be computed by converting to the standardized variables , corresponding to the lower specified bound, and , which corresponds to the upper one. Hence the probability is  and can be computed by using the values from Table D.1 (see also Fig. D.1):If the machining tool is modified so as to produce the axes with the required diameter of , but with the same uncertainty as before, , the standardized variables become -, thusThe fraction of useful axes is thereby increased by about .



3.3.2 Measure of Peak Separation
	
	
                    
                    
                  

A practical quantity referring to the normal distribution is its full width at half-maximum (FWHM), see double-headed arrow in Fig. 3.2 (top). It can be obtained by simple calculation:  or , hence . The  is just twice this number,
 offers a measure of how well two Gaussian peaks in a physical spectrum can be separated. By convention we can distinguish neighboring peaks with equal amplitudes and equal  if their centers are at least FWHM apart (Fig. 3.3).Fig. 3.3Illustration of the measure of peak separation. The centers of the fourth and fifth peak from the left are 0.3 apart, which is just slightly above the value of  for individual peaks, so they can still be separated. The three leftmost peaks can also be separated. The structure at the right consists of two peaks which are too close to each other to be cleanly separated. In practice, similar decisions are almost always complicated by the presence of noise




3.4 Maxwell Distribution
	


                  
                  
                

                  
                

                  
                  
                
	
The Maxwell distribution describes the velocities of molecules in thermal motion in thermodynamic equilibrium. In such motion the velocity components of each molecule, , are stochastically independent, and the average velocity (as a vector) is zero. The directions x, y and z correspond to kinetic energies ,  and , and the probability density in velocity space at given temperature T decreases exponentially with energy. The probability density for  is the product of three one-dimensional Gaussian densities: (3.14)where  and . The distribution over  is spherically symmetric, so the appropriate distribution in magnitudes  is obtained by evaluating  in a thin spherical shell with volume , thus (3.15)An example of such distribution for nitrogen molecules at temperatures 193 and  is shown in Fig. 3.4 (left).Fig. 3.4[Left] Maxwell distribution of velocities of nitrogen molecules at  and . See also Fig. 4.​1 (right) and Problem 3.10.4. [Right] Pareto distribution with parameters  (minimum value on the abscissa) and a (shape parameter)



3.5 Pareto Distribution
	


                  
                  
                
	
Probability distributions of many quantities that can be interpreted as random variables have relatively narrow ranges of values. The height of an average adult, for example, is , but nobody is 50 or  tall. The data acquired by the WHO [8] show that the body mass index (ratio of the mass in kilograms to the square of the height in meters) is restricted to a range between 15 and 50.
But one also frequently encounters quantities that span many orders of magnitude, for example, the number of inhabitants of human settlements (ranging from a few tens in a village to tens of millions in modern city conglomerates). Similar "processes" with a large probability for small values and small probability for large values are: frequency of specific given names, size of computer files, number of citations of scientific papers, number of web-page accesses and the quantities of sold merchandise (see Example on p. 97), but also quantities measured in natural phenomena, like step lengths in random walks (anomalous diffusion), magnitudes of earthquakes, diameters of lunar craters or the intensities of solar X-ray bursts [9-11]. A useful approximation for the description of such quantities is the Pareto (power law) distribution with the probability density (3.16)where b is the minimal allowed x (Fig. 3.4 (right)), and a is a parameter which determines the relation between the prominence of the peak near the origin and the strength of the tail at large x. It is this flexibility in parameters that renders the Pareto distribution so useful in modeling the processes and phenomena enumerated above. As an example, Fig. 3.5 (left) shows the distribution of the lunar craters in terms of their diameter, and Fig. 3.5 (right) shows the distribution of solar X-ray bursts in terms of their intensity.Fig. 3.5[Left] Distribution of lunar craters with respect to their diameter, as determined by researchers of the Lunar Orbiter Laser Altimeter (LOLA) project [12, 13] up to 2011. [Right] The distribution of hard X-rays in terms of their intensity, measured by the Hard X-Ray Burst Spectrometer (HXRBS) between 1980 and 1989 [14]. The straight lines represent the approximate power-law dependencies, also drawn in the shaded areas, although the Pareto distributions commence only at their right edges ()

The Pareto distribution


	 is normalized on the interval  and frequently one does not use its distribution function  but rather its complement, (3.17)as it is easier to normalize and compare it to the data: the ordinate simply specifies the number of data points (measurements, events) that were larger than the chosen value on the abscissa. By plotting the data in this way, one avoids histogramming in bins, which is not unique. The values  should not be set to the left edge of the interval on which measurements are available (e.g.  in LOLA measurements), but to the value above which the description in terms of a power-law appears reasonable (50 ). The parameter a can be determined by fitting the power function to the data, but in favor of better stability [9] we recommend the formulawhich we derive later ((8.​11)).

Hint If we wish to plot the cumulative distribution for the data , we can use the popular graphing tool Gnuplot. We first sort the data, so that  are arranged in increasing order (two-column file data). The cumulative distribution can then be plotted by the command


3.5.1 Estimating the Maximum  in the Sample
Having at our disposal a sample of n measurements presumably originating from a power-law distribution with known parameters a and b, a simple consideration allows us to estimate the value of the largest expected observation [9]. Since we are dealing with a continuous distribution, we should refer to the probability that its value falls in the interval . The probability that a data point is larger than x, is given by (3.17), while the probability for the opposite event is . The probability that a particular measurement will be in  and that all others will be smaller is therefore . Because the largest measurement can be chosen in n ways, the total probability is
The expected value of the largest measurement—such quantities will be discussed in the next chapter—is obtained by integrating x, weighted by the total probability, over the whole definition domain:where B(p, q) is the beta function. We have substituted  in the intermediate step. For the sample in Fig. 3.5 (left), which contains  data points,  and , we obtain . If the sample were ten times as large, we would anticipate .



3.6 Cauchy Distribution
	


                  
                  
                

                  
                  
                

                  
                
	
The Cauchy distribution with probability density (3.18)is already familiar to us from the Example on p. 41. In fact, we should have discussed it along with the exponential, as the Fourier transform of the exponential function in the time scale is the Cauchy function in the energy scale: (3.19)In other words, the energy distribution of the states decaying exponentially in time is given by the Cauchy distribution. It is therefore suitable for the description of spectral line shapes in electromagnetic transitions of atoms and molecules (Fig. 3.6 (left)) or for modeling the energy dependence of cross-sections for the formation of resonances in hadronic physics (Fig. 3.6 (right)). With this in mind, it makes sense to furnish it with the option of being shifted by  and with a parameter s specifying its width: (3.20)
Fig. 3.6[Left] A spectral line in the emission spectrum of silicon (centered at ) at a temperature of  and particle density  [15], along with the Cauchy (Lorentz) approximation. Why the agreement with the measured values is imperfect and how it can be improved will be revealed in Problem 6.​9.​2. [Right] Energy dependence of the cross-section for scattering of charged pions on protons. In this process a resonance state is formed whose energy distribution in the vicinity of the maximum can also be described by the Cauchy (Breit-Wigner) distribution

In spectroscopy the Cauchy distribution is also known
	


                  
                  
                

                  
                

                  
                  
                

	 as the Lorentz curve, while in the studies of narrow, isolated resonant states in nuclear and particle physics it is called the Breit-Wigner distribution: in this case it is written aswhere  is the resonance energy and  is the resonance width.


3.7 The  distribution
The  distribution, a one-parameter probability distribution
	
 with the density (3.21)will play its role in the our discussion on statistics (Chaps. 7-10). The parameter  is called the number of degrees of freedom. The probability density of the  distribution for four values of  is shown in Fig. 3.7. The corresponding distribution function isIn practical work one usually does not need this definite integral but rather the answer to the opposite question, the cut-off value x at given P. These values are tabulated: see Fig. D.1 (top right) and Table D.3.Fig. 3.7The density of the  distribution for four different parameters (degrees of freedom) . The maximum of the function  for  is located at . For large  the  density converges to the density of the normal distribution with average  and variance . The thin curve just next to  denotes the density of the N(8, 10) distribution



3.8 Student's Distribution
	


                  
                
	
The Student's distribution (or the t distribution)2 is also a one-parameter probability distribution that we shall encounter in subsequent chapters devoted to statistics. Its density is (3.22)where  is the number of degrees of freedom and B is the beta function. The graphs of its density for ,  and  are shown in Fig. 3.8. In the limit  the Student's distribution tends to the standardized normal distribution.Fig. 3.8The density of the Student's (t) distribution with ,  and  degrees of freedom. The distribution is symmetric about the origin and approaches the standardized normal distribution N(0, 1) with increasing  (thin curve), from which it is hardly discernible beyond 




3.9 

                distribution
	

              
The F distribution is a two-parameter distribution with the probability density (3.23)where  is the number of degrees of freedom "in the numerator" and  is the number of degrees of freedom "in the denominator". Why this distinction is necessary will become clear in Sect. 7.​2.​3: there we shall compare ratios of particular random variables, distributed according to (3.23). The probability densities of the F distribution are shown in Fig. 3.9 for several typical  pairs.Fig. 3.9[Left] The probability density of the F distribution for  degrees of freedom (numerator) and three different degrees of freedom  (denominator). [Right] The density of the F distribution for  and three different values of 




3.10 Problems

3.10.1 In-Flight Decay of Neutral Pions

	
	
                    
                    
                  

A complicated transformation of a uniform distribution may still turn out to be a uniform distribution, as we learn by solving the classical problem in relativistic kinematics, the in-flight neutral pion decay to two photons, . Calculate the energy distribution of the decay photons, !

 Let the  meson fly in the laboratory frame along the z-axis with velocity . The decay in the  rest frame is isotropic. Due to azimuthal symmetry () this implies a uniform distribution over the cosine of the angle  (see Sect. C.2.2):where  is the emission angle of the first photon in the rest frame, as shown in the figure:





The energy distribution of the photons is obtained by the derivative chain-rule: (3.24)We therefore need to establish a relation between  and , and it is offered by the Lorentz transformation from the  rest frame to the laboratory frame. Of course, the energies of the photons in the rest frame are equal, , and their four-vectors areThe Lorentz transformation that gives us their energies in the laboratory frame iswhere  and . It follows thati.e.When this is inserted in (3.24), we obtain the required energy distribution, which is indeed uniform:namely on the interval between the minimal and maximal valuesLet us check our findings by a simple simulation, observing the decay of pions with a velocity of 0.7c (). We use a computer to generate 100000 uniformly distributed values  (Fig. 3.10 (left)), and then use each of these values to calculate the photon energies in the laboratory frame,  and . A uniform distribution over  on the interval between  and  should appear. It can be seen in Fig. 3.10 (right) that we were not mistaken.Fig. 3.10The  decay. [Left] Uniform distribution of events over  in the  rest frame. [Right] Uniform energy distribution of the decay pions in the laboratory frame



3.10.2 Product of Uniformly Distributed Variables

	

(Adapted from [17].) Let two continuous random variables X and Y be described by a known probability density .  Calculate the probability density  of the product random variable  in the most general case and in the case that X and Y are independent.  Discuss the special case of independent variables X and Y, both of which are uniformly distributed on the interval (0, 1).






 Define the domain  (shown for positive z as the shaded region in the figure) which determines the distribution function of the variable Z:To facilitate the determination of integration boundaries, the intervals of four integrations in this equation—read from left to right—are denoted by numbers 1 to 4 in the figure. (The derivation for negative z proceeds analogously.)  The corresponding probability density is then obtained by differentiation:If X and Y are independent, possessing probability densities  and , one has , thus (3.25)
 The product of uniformly distributed variables X and Y is always positive and less than 1, hence the probability density  of the variable  may be non-zero only on the interval (0, 1). On this interval it can be determined by using (3.25), in which only the first term survives due to this very requirement, and even here the integrand is positive only if  and , i.e. when . It follows thatwhile  elsewhere.


3.10.3 Joint Distribution of Exponential Variables

	
	
                    
                  
	
                    
                    
                  

Let X and Y be independent random variables distributed exponentially with parameters  and ,Imagine a square region .  Calculate the value of a, for which the probability that a randomly drawn (x, y) pair falls into S, equals 1 / 2.  Calculate the conditional joint probability density of the variables X and Y, given that  and .

 The variables X and Y are independent, hence their joint probability density isThe probability that a random pair of values (x, y) finds itself in S, equals
 We are looking for a such that . This equation is best solved by Newton's method, in spite of its known pitfalls: with the function  (plot it!) and its derivative  we start the iteration ,   With the initial approximation  just a few iteration steps lead to .

 We first form the conditional distribution functionwhere we have taken into account that X and Y are independent. The probability density can then be calculated by differentiating  with respect to x and y:We should also check the normalization which must be fulfilled—as for any probability density—also for the calculated conditional density. Indeed we findwhere  is the definition domain of the conditional joint probability density.


3.10.4 Integral of Maxwell Distribution over Finite Range
	
	
                    
                    
                  

What fraction of nitrogen () molecules at temperature  have velocities between  and , if the velocity distribution is of the Maxwell type (see Fig. 3.4 (left))?

 Let us rewrite (3.15) in a slightly more compact formThe required fraction of molecules is equal to the definite integral of the probability density from  to ,Such integrals are typically handled by resorting to integration by parts, in which the power of the variable x in the integrand is gradually reduced:In our case we only need the integral with , thereforeFrom Table D.2 we read off  and , and all that is needed is to merge the expressions to(The result by computing the  functions accurately is 0.5066.)


3.10.5 Decay of Unstable States and the Hyper-exponential Distribution

	
	
                    
                  

Organic scintillator is a material in which charged particles promote electrons to excited states, which get rid of the excess energy by photon emission. The time dependence of the intensity of emitted light can be approximated by a sum of two independent excitation mechanisms (occurring almost instantaneously) and de-excitations proceeding with two different decay times, as shown in Fig. 3.11.  Write down the corresponding probability density and the functional form of the decay curve.  Generalize the expressions to multiple time components. Does the same physical picture apply to a mixture of radioactive isotopes, if each of them has only a single decay mode?Fig. 3.11Typical time dependence of a light pulse emanating from an organic scintillator: in this case, the total intensity consists of a fast relaxation component with decay time  (frequency ) and a slow one with decay time  ()


 The mechanisms of light generation in scintillators are poorly understood, but the predominant opinion seems to be that the type of relaxation (fast or slow) is determined already during excitation.  We are thus dealing with exclusive (incompatible) events, hence the probability density isThe time dependence of the light curve is then given by the distribution function:
 Obviously one can generalize this to multiple (k) time components by writing (3.26)The distribution with such probability density is known as the k-phase hyper-exponential distribution. It can be used to model the superposition of k independent events, e.g. the response time of a system of k parallel computer servers, in which the ith server is assigned with probability  to handle our request, and the distribution of its service time is exponential with parameter  (Fig. 3.12 (left)). Such a distribution also describes the lifetime of a product manufactured on several parallel assembly lines or in factories with different levels of manufacturing quality.Fig. 3.12[Left] A set of k parallel independent processes ("phases") with a single output, described by the hyper-exponential distribution. [Right] An illustration of the decay modes of a sample of unstable particles

At first sight, radioactive
 decay in a sample containing various isotopes (for example, a mixture of ,  and ) resembles such a k-phase process. But the key difference is that the decays of individual isotopes are not mutually exclusive: in a chosen time interval  we might detect the decay of a single isotope, two, or all three. In this case the hyper-exponential distribution is not justified.
Similar conclusions can be drawn for the decay of unstable particles with multiple decay modes, each occurring with a distinct probability. Suppose that particle  decays into the final state  consisting of two or more lighter particles. The usual decay law (3.5) applies:If multiple final states  are allowed, we must sum over all contributions: the time derivative of the number of particles still available for decay at time t isThe extinction of N is therefore driven by a single time constant,  ! Just prior to the decay, Nature does not think about the type of the final state, but rather just chooses the time of the decay by exponential law with parameter ,where  is the average decay time. Instead of  we sometimes prefer to specify the conjugated variable in the Heisenberg sense (time and energy, position and linear momentum, angle and angular

	
	
                    
                   momentum), known as the total decay width:
The total width  is a sum of the partial widths
, ,   It is only at the very moment of decay that the particle randomly "picks" a certain final state. The probabilities for the transitions to specific final states can be expressed by branching ratios or branching fractions: for individual decay modes we have (3.27)Conservation of probability (a particle must decay into some final state after all) of course ensuresAs an example, Table 3.1 shows the partial widths and branching fractions in the decay of the  bosons produced in collisions of electrons and positrons at invariant energies around ; see Fig. 3.12 (right). From the total decay width we compute the average decay time . The energy dependence of the  resonance is described by the Breit-Wigner distribution (Fig. 3.6 (right)) with the center

	
	
                    
                    
                  
	
 at approximately  and a width of about .Table 3.1The dominant decay modes of the  boson, the corresponding partial decay widths and the branching fractionsDecay modeWidth (MeV)Branching fraction (%)

































100



3.10.6 Nuclear Decay Chains and the Hypo-exponential Distribution

	
	
                    
                  

In nuclear decay chains an unstable nucleus decays with characteristic time  to a lighter nucleus, which in turn decays with characteristic time  to an even lighter nucleus, and so on. Such decay chains with consecutive emissions (mostly  particles or electrons) are typical of heavy nuclei. Figure 3.13 (left) shows a segment of the uranium decay chain where each subsequent isotope has a single decay mode, but with a different characteristic time. Find the probability distribution to describe such processes!Fig. 3.13[Left] A segment of the uranium decay chain where only one type of decay is allowed at each stage. [Center] Depiction of k serial processes with a single output, described by the hypo-exponential distribution. [Right] Illustration of a nuclear decay chain; compare it to Fig. 3.12 (right)


 Suppose that the decay chain is initiated by type 1 isotopes with no daughter nuclei present at time zero, and that no other isotope decays into this type. The time evolution of the decay chain is then governed by the set of differential equationswith initial conditionsWe already know the solution of the first line:The next component of the chain is obtained by first multiplying the second line of the system by  and exploiting the previously calculated solution for ,We move the first term on the right to the left,and integrate to getThe constant C is dictated by the condition , whence  andThe same trick can be used to obtain the remaining elements of the chain: in the ith line of the system we always multiply  by , carry over  to the left where it can be joined with its neighbor into a derivative of a product, grab the result from the previous step, and integrate. For the third element of the chain, for example, we obtainIt is obvious that this can be generalized to (3.28)except that we must replace  in the numerator of all fractions. Such a distribution, which in general describes a sum of independent

	, exponentially distributed variables, each with its own parameter , is called hypo-exponential.



References


1.
D.C. Montgomery, G.C. Runger, Applied Statistics and Probability for Engineers, 5th edn (John Wiley & Sons, New York, 2010)


2.
G.H. Jowett, The exponential distribution and its applications. Inco. Stat. 8, 89 (1958)MATH


3.
P.J. Aston, Is radioactive decay really exponential?, Europhys. Lett. 97 (2012) 52001. See also the reply C. A. Nicolaides, Comment on Is radioactive decay really exponential?. Europhys. Lett. 101, 42001 (2013)


4.
R.G. Newton, Scattering Theory of Waves and Particles, 2nd edn (Springer-Verlag, Berlin, 1982)


5.
E. Merzbacher, Quantum Mechanics, 3rd edn (Wiley & Sons Inc, New York, 1998)


6.
M.L. Goldberger, K.M. Watson, Collision Theory, (John Wiley & Sons, New York 1964) (Chapter 8)


7.
H. Poincaré, Calcul des Probabilités (Gauthier-Villars, Paris, 1912)MATH


8.
WHO Global InfoBase team, The SuRF Report 2. Country-level data and comparable estimates. (World Health Organization, Geneva, Surveillance of chronic disease Risk Factors, 2005)


9.
M.E.J. Newman, Power laws, Pareto distributions and Zipf's law. Contemp. Phys. 46, 323 (2005)


10.
A. Clauset, C.R. Shalizi, M.E.J. Newman, Power-law distributions in empirical data. SIAM Rev. 51, 661 (2009)ADSMathSciNetCrossRefMATH


11.
M. Schroeder, Fractals, Chaos, Power Laws: Minutes from an Infinite Paradise (W.H. Freeman, New York, 1991)MATH


12.
J.W. Head, C.I. Fassett, S.J. Kadish, D.E. Smith, M.T. Zuber, G.A. Neumann, E. Mazarico, Global distribution of large lunar craters: implications for resurfacing and impactor populations. Science 329, 1504 (2010)ADSCrossRef


13.
S.J. Kadish, C.I. Fassett, J.W. Head, D.E. Smith, M.T. Zuber, G.A. Neumann, E. Mazarico, A Global Catalog of Large Lunar Crater () from the Lunar Orbiter Laser Altimeter. Lunar Plan. Sci. Conf., XLII, abstract 1006 (2011)


14.
B.R. Dennis, L.E. Orwig, G.S. Kennard, G.J. Labow, R.A. Schwartz, A.R. Shaver, A.K. Tolbert, The Complete Hard X-ray Burst Spectrometer Event List, 1980-1989, NASA Technical Memorandum 4332 (NASA, 1991)


15.
S. Bukvić, S. Djeniže, A. Srećković, Line broadening in the Si I, Si II, Si III, and Si IV spectra in the helium plasma, Astron. Astrophys. 508, 491 (2009)


16.
Student [W.S. Gosset], The probable error of a mean. Biometrika 6, 1 (1908)


17.
R. Jamnik, Verjetnostni račun (Mladinska knjiga, Ljubljana, 1971)MATH




Footnotes


1


In the original: "Elle ne s'obtient pas par des déductions rigoureuses; plus d'une démonstration qu'on a voulu en donner est grossière, entre autres celle qui s'appuie sur l'affirmation que la probabilité des écarts est proportionelle aux écarts. Tout le monde y croit cependant, me disait un jour M. Lippmann, car les expérimentateurs s'imaginent que c'est un théorème de mathématiques, et les mathématiciens que c'est un fait expérimental."

 



2


The Student's distribution acquired its first peculiar name from a paper [16] that an English statistician W.S. Gosset published under the pseudonym Student, and the second one from a specific random variable (see formula (7.​18)).

 













© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_4




4. Expected Values




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
Finding expected values of distributions is one of the main tasks of any probabilistic analysis. The expected value in the narrower sense of the average (mean), which is a measure of distribution location, is introduced first, followed by the related concepts of the median and distribution quantiles. Expected values of functions of random variables are presented, as well as the variance as the primary measure of the distribution scale. The discussion is extended to moments of distributions (skewness, kurtosis), as well as to two- and d-dimensional generalizations. Finally, propagation of errors is analyzed.



In this chapter we discuss quantities that one may anticipate for
	 individual random variables or their functions—with respect to the probability distributions of these variables—after multiple repetitions of random experiments: they are known as expected values or expectations of random variables. The most important such quantity is the average value, which is the expected value in the basic, narrowest sense of the word; further below we also discuss other expected values in the broader sense.

4.1 Expected (Average, Mean) Value
The expected value of


	 a discrete
 random variable X, which can assume the values  (), is computed by weighting (multiplying) each of these values by the probability  that in a large number of trials this particular value turns up (see (2.​13)), then sum all such products: (4.1)The average is denoted by E or by a line across the random variable (or its function) being averaged. Both E[X] and , as well as the frequently used symbol  imply the "averaging operation" performed on the variable X. (We emphasize this because we occasionally also use the slightly misleading expression "expected value of a distribution": what usually changes in random processes is the value of a variable, not its distribution!) In Chaps. 4-6 the symbols (4.2)signify the one and the same thing, while in Chaps. 7-10 the symbols  and  will denote the average value of a sample and  will be used strictly as expected value. The only symbol that really would not make any sense, is E[x].
It can not hurt to recall the formula to compute the center of mass of a one-dimensional system of point-like masses with a total mass :If all probabilities in (4.1) are equal, we get a simple expression for the usual arithmetic averageThe expected value of a continuous random variable X is obtained by replacing the sum by the integral and integrating the product of the variable value x and the corresponding probability density over the whole definition domain, (4.3)(Beware: this expected value may not exist for certain types of densities .) The analogy from mechanics is again the center of mass of a three-dimensional inhomogeneous body, which is calculated by integrating the product of the position vector with the position-dependent density over the whole volume:


Example
In a casino we indulge in a game of dice with the following rules for each throw: 2 spots—win ; 4 spots—win ; 6 spots—lose ; 1 spot, 3 spots or 5 spots—neither win nor lose. Any number of spots  is equally probable, , so the expected value of our earnings isIf the casino wishes to profit from this game, the participation fee should be at least this much.    




4.2 Median
The median of a random variable X (discrete or continuous) is
 the value , for which (4.4)For a continuous variable X the inequalities become equalities,as it is always possible to find the value of x that splits the area under the probability density curve in two halves: the probabilities that X assumes a value above or below the median, respectively, are exactly .
The median of a discrete variable X sometimes can not be determined uniquely, since the discrete nature of its distribution may cause the inequalities in (4.4) to be fulfilled simultaneously, but for many different x. For example, consider a discrete distribution with probability function , where   We see that  holds for any value . In such cases the median is defined as the central point of the interval on which the assignment is ambiguous—in the present example we therefore set it to .Fig. 4.1[Left] Probability density  (see (4.5)) with its average, median and mode (maximum). [Right] Maxwell distribution with its mode ("most probable velocity"), average velocity and the root-mean-square velocity. See also Fig. 3.​4 (left)


Example
A continuous random variable has the probability density (4.5)shown in Fig. 4.1 (left). Find the mode (location of maximum), median and the average (mean) of this distribution!
The mode is obtained by differentiating and setting the result to zero:The median  must split the area under the curve of  in two parts of 1 / 2 each, thusThis results in the quadratic equation  with two solutions, . Only the solution with the negative sign is acceptable as it is the only one that falls within the [0, 3] domain:The average is calculated by using the definition (4.3),All three values are shown in Fig. 4.1 (left).    




4.3 Quantiles
The value of a random variable, below which a certain fraction of
 all events are found after numerous trials, is called the quantile of its distribution (lat. quantum, "how much"). For a continuous probability distribution this means that the integral of the probability density from  to  equals  (Fig. 4.2). For example, the 0.50th quantile of the standardized normal distribution is , while its 0.9985th quantile is , see (3.​13).Fig. 4.2Definition of the quantile of a continuous distribution. The integral of the density  from  (or the lowest edge of its domain) to  equals . The figure shows the density , , the corresponding distribution function, and the 90th percentile (), which is 

Fig. 4.3[Left] Daily sales of fiction books as a function of sales rank. [Right] Daily earnings as a function of sales rank. In the book segment the online giant earns  by selling books with sales ranks above , while the average sales rank is 


To express the th quantile all values 
are


                  
                

                  
                

                  
                 allowed, but several brethren terms are in wide use for specific values of : integer values (in percent) express percentiles, the tenths of the whole range of  are delimited by deciles and the fourths by quartiles:
 defines the 20th percentile or the second decile of a distribution,  and  set the limits of its first and third quartile. Hence,  carries no less than five names: it is the 0.50th quantile, the 50th percentile, the second quartile, the fifth decile and—the median. The difference  is called the inter-quartile range (IQR). The interval  contains half of all values; a quarter of them reside to its left and a quarter to its right.

Example
Fig. 4.3 (left) shows the daily sales of fiction books from the 1000 bestseller list (sales rank r) of the Amazon online bookstore in a certain time period. (Note the log-log scale: in linear scale the distribution has a sharp peak at  and a rapidly dropping tail, so it mostly occupies the region around the origin.)
To study the sales dynamics such discrete distributions are often approximated by continuous Pareto distributions (3.​16). For

	
	
                    
                    
                  
	
                    
                    
                  
	

 many markets in the past, the "Pareto 80/20 principle" seemed to apply, stating that a relatively small fraction () of products (in our case best-selling books) brings the most () profit. Figure 4.3 (right) shows the daily earnings as a function of sales rank, as well as the median, average rank, and the sales rank up to which Amazon earns  of the money: the latter is 234 (of 1000), neatly corresponding with the Pareto "principle". Still, it is obvious from the graph that the Pareto distribution under-estimates the actual sales at high ranks r. Analyses show [1, 2] that the distribution n(r) has become flatter over the years, meaning that more and more profit is being squeezed from the ever increasing tail; see also [3].    




4.4 Expected Values of Functions of Random Variables
The simplest functions of random variables are the sum  of two variables and the linear combination , where a and b are arbitrary real constants. Since the expected value of a continuous random variable, E[X], is defined by an integral, the expected values of  and  inherit all properties of the integral, in particular linearity. (A similar conclusion follows in the discrete case where we are dealing with sums.) Therefore, for both continuous and discrete random variables it holds that (4.6)as well asandOne needs to be slightly more careful in computing the expected values of more general functions of random variables. Suppose that X is a discrete random variable with probability distribution (probability function) . Then  is also a random variable and its probability function isIf X takes the values  and Y takes the values  (), we havehence (4.7)If X is a continuous random variable, we just need to replace the sum by the integral and the probability function by the probability density: (4.8)This is a good spot to comment on a very popular approximation that can be an ugly mistake or a good short-cut to a solution: it is the approximation (4.9)The trick works well if the density  of X is a sharp, strongly peaked function, and not so well otherwise. Regardless of this, however, for any convex1 function g, Jensen's inequality holds true: (4.10)that is,


4.4.1 Probability Densities in Quantum Mechanics
As physicists, we ceaselessly calculate expected values of the form (4.8) in any field related to statistical or
	
	
                    
                    
                  
	


 quantum mechanics. We say: the expected value of an operator  in a certain state of a quantum-mechanical system (for example, ground state of the hydrogen atom) described by the wave-function , isThe operator  acts on the right part of the integrand, , then the result is multiplied from the left by its complex conjugate , and integrated over the whole domain. If  is multiplicative, for example —in this case we obtain the expectation value of the third Cartesian component of the electron's position vector in the hydrogen atom—we are computing just (4.11)which is the integral of a product of two scalar functions, the second of which, , is nothing but the probability density of (4.8).

Example
An electron moving in the electric field of a lead nucleus is described by the functionwhere . The nucleus may be imagined as a positively charged sphere with radius . How much time does the electron "spend" in the nucleus, i. e. what is the probability that it resides within a sphere of radius R? All we are looking for is the expected value of the operator  in (4.11); due to angular symmetry the volume element is simply , thusAn almost identical result is obtained by assuming that  is practically constant on the interval [0, R], which is reasonable, since . In this case we obtain .    





4.5 Variance and Effective Deviation
Computing the expected value of a random variable X tells us something about where within its domain its values will approximately land


                  
                

	 after many repetitions of the corresponding random experiment. Now we are also interested in the variation (scattering) of the values around their average . A measure of this scattering is the variance, defined asA large variance means a large scatter around the average and vice-versa. The positive square root of the variance,is known as effective or standard deviation—in particular with the normal distribution on our minds. In the following we shall also make use of the relation (4.12)(Prove it as an exercise.) If X is a discrete random variable, which takes the values  and has the probability function , its variance is (4.13)In the case that all probabilities are equal, , the variance is (4.14)Note the factor 1 / n—not , as one often encounters—as it will acquire an important role in random samples in Chap. 7.
If X is a continuous random variable with the probability density , its variance is (4.15)It can be shown that, regardless of the distribution obeyed by
	


                  
                

                  
                  
                

	 any (continuous or discrete) random variable X, it holds thatfor any constant , which is known as the Chebyshev inequality. It can also be formulated in terms of the slightly tighter Cantelli's constraints (4.16)We may resort to this tool if we know only the expected value of the random variable, , and its variance, , but not the functional form of its distribution. In such cases we can still calculate the upper limits for probabilities of the form (4.16).

Example
Suppose that the measured noise voltage at the output of a circuit has an average of  and variance . The probability that the noise exceeds  (i. e. raises more than  above the average), can be bounded from above as .    




4.6 Complex Random Variables
A particular linear combination of real random variables X and Y is the


                  
                  
                  
                

                  
                  
                

                  
                  complex random variable
Its distribution function at  is defined aswhere  is the distribution function of the pair—more precisely, the random vector (X, Y). The expected value of the variable Z is defined asComputing the expectation values of complex random variables is an additive and homogeneous operation: for arbitrary  and  it holds thatwhile for an arbitrary complex constant  we haveThe variance of a complex random variable is defined asA short calculation—do it!—shows that it is equal to the sum of the variances of its components,The complex random variables  and  are mutually independent if random vectors2
 and  are independent. (A generalization is at hand: complex random variables  ( are mutually independent if the same applies to random vectors .) If  and  are independent and possess expected values, their product also possesses it, and it holds that



4.7 Moments
The average (mean) and the variance are two special cases of expected values in the broader sense called moments: the pth raw or algebraic moment
 of a random variable X is defined as the expected value of its pth power, that is, : (4.17)Frequently we also require central moments, defined with respect to the corresponding average value of the variable, that is, :From here we read off  (normalization of probability distribution),  and . The following relations (check them as an exercise) also hold:In addition to the first (average) and second moment (variance) only the third and fourth central moment are in everyday use. The
 third central moment, divided by the third power of its effective deviation, (4.18)is called the coefficient of skewness or simply skewness. The coefficient  measures the asymmetry of the distribution around its average:  means that the distribution has a relatively longer tail to the left of the average value (Fig. 4.4 (left)), while  implies a more pronounced tail to its right (Fig. 4.4 (center)).Fig. 4.4[Left] A distribution with negative skewness: the tail protruding to the left of the average value is more pronounced than the one sticking to the right. [Center] A distribution with positive skewness. [Right] Examples of distributions with positive (thick full curve) and negative excess kurtosis (thick dashed curve) with respect to the normal distribution (thin full curve)
Table 4.1Properties of select continuous distributions: average (mean) value, median, mode, variance, skewness () and kurtosis ()DistributionAverageMedianModeVariance




U(a, b)(3.​1)



 / 

0



(3.​4)



0

29

(3.​7)







03

(3.​20) / 



 /  /  / 

(3.​21)













(3.​22)

00







(3.​16)




b







Notes
 approximate dependence for large  |  for  |  undefined for 

 undefined for  |  undefined for  |  undefined for 

 defined for , otherwise  |  defined for , while  for 


 for 


 for 



The
 fourth central moment, divided by the square of the variance,is known as kurtosis and tells us something about the "sharpness" or "bluntness" of the distribution. For the normal distribution we have , so we sometimes prefer to specify the quantity (4.19)called the excess kurtosis:
 indicates that the distribution is "sharper" than the normal (more prominent peak, faster falling tails), while  implies a "blunter" distribution (less pronounced peak, stronger tails), see Fig. 4.4 (right).

The


	
	 properties of the most important continuous distributions—average value, median, mode (location of maximum), variance, skewness () and kurtosis ()—are listed in Table 4.1. See also Appendices B.2 and B.3, where we shall learn how to "automate" the calculation of moments by using generating and characteristic functions.

Example
We are
	
	

 interested in the mode ("most probable velocity"), average velocity and the average velocity squared of  gas molecules (molar mass , mass of single molecule ) at temperature . The velocity distribution of the molecules is given by the Maxwell distribution (3.​15), whose maximum (mode) is determined by , henceThe average value and the square root of the average velocity squared ("root-mean-square velocity") are computed from (4.3) and (4.17) with :where we have used  and . These three famous quantities are shown in Fig. 4.1 (right).    



4.7.1 Moments of the Cauchy Distribution
The Cauchy distribution  drops off so slowly at
	
	
                    
                    
                    
                   that its moments (average, variance, and so on) do not exist. For this reason its domain is frequently restricted to a narrower interval :This is particularly popular in nuclear physics where the Breit-Wigner description of the shape of the resonance peak in its tails—see Fig. 3.​6 (right)—is no longer adequate due to the presence of neighboring resonances or background. With the truncated density  both the average and the variance are well defined:Narrowing down the domain is a special case of a larger class of "distortions" of probability distributions used to describe, for example, non-ideal outcomes of a process or imperfect efficiencies for analyzing particles in a detector. If individual events are detected under different conditions, the ideal probability density, , must be weighted by the detection efficiency:
where y is an auxiliary variable over which the averaging is being performed, and  is the probability density for the event being detected near  and . An introduction to such weighted averaging procedures can be found in Sect. 8.5 of [4].



4.8 Two- and d-dimensional Generalizations

Let the continuous random variables X and Y be distributed according to the joint probability density . In this case the expected values of the individual variable can be calculated by the obvious generalization of (4.3) to two dimensions. The density  is weighted by the variable whose expected value we are about to compute, while the other is left untouched:In the discrete case the extension to two variables requires a generalization of (4.1):By analogy to (4.15) and (4.13) we also compute the variances of variables in the continuous case,and the variances in the discrete case,Henceforth we only give equations pertaining to continuous variables. The corresponding expressions for discrete variables are obtained, as usual, by replacing the probability densities  by the probability [mass] functions , and integrals by sums.
Since now two variables are at hand, we can define yet a third version of the double integral (or the double sum) in which the variables enter
 bilinearly—the so-called mixed moment known as the covariance of X and Y:One immediately sees thatfor arbitrary constants a and b, as well asTherefore, if X and Y are mutually independent, then by definition (2.​25) one also has , and then(The covariance of independent variables equals zero.) For a later discussion of measurement uncertainties the following relation between the variance and covariance of two variables is important: (4.20)In other words,By using the covariance and both effective deviations we define the Pearson's coefficient of linear correlation (also linear correlation coefficient
)
	


                  
                  
                

                  
                  
                

                  
                  
                
 (4.21)It is easy to confirm the allowed range of  given above. Because of its power two the expression  is non-negative for any . Let us expand it:The left side of this inequality is a real polynomial of second degree  with coefficients , , , which is non-negative everywhere, so it can have at most one real zero. This implies that its discriminant can not be positive, so . This tells us that  or , which is precisely (4.21).
The generalization of (4.20) to the sum of (not necessarily independent) random variables  isIf the variables  are mutually independent, this expression reduces to (4.22)


Example
Many sticks with length 1 are broken at two random locations. What is the average length of the central pieces? At each hit, the stick breaks at  and , where the values  and  are uniformly distributed over the interval [0, 1], but one can have either  or . What we are seeking, then, is the expected value of the variable  (with values l) with respect to the probability density :  

How would the result change if the probability that the stick breaks linearly increases from 0 at the origin to 1 at the opposite edge?    



Example
Let the continuous random variables X and Y both be normally distributed, with averages  and  and variances  and . What is their joint probability density if X and Y are independent, and what are their joint

	
	

 and conditional densities in the dependent case, with correlation coefficient ?
If X and Y are independent, their joint probability density—by (2.​25)—is simply the product of the corresponding one-dimensional densities:The curves of constant values of  in the (x, y) plane are untilted ellipses in general (), and circles in the special case . At any rate  for such a distribution. A two-dimensional normal distribution of dependent (and therefore correlated) variables is described by the probability densitywhere we have denoted  and . This distribution can not be factorized as , and its curves of constant values are tilted ellipses; for parameters , ,  and  they are shown in Fig. 4.5 (left).Fig. 4.5[Left] Joint probability density of two dependent, normally distributed random variables X and Y with averages  and , variances  and linear correlation coefficient . [Right] Conditional probability density 


Conditional probability densities  and  can be computed by using (2.​26) and (2.​27). Let us treat the first case, the other one is obtained by simply replacing ,  and  at appropriate locations:This conditional probability density is shown in Fig. 4.5 (right). By comparing it to definition (3.​7) we infer that the random variable X|Y is distributed asa feature also seen in the plot: the width of the band does not depend on y.    



4.8.1 Multivariate Normal Distribution
This appears to be a good place to generalize the normal distribution of

	
	
                    
                    
                  
	


 two variables (the so-called binormal or bivariate normal distribution) to d dimensions. We are dealing with a vector random variableand its averageWe construct the 
covariance matrix
 with the matrix elementsThe covariance matrix is symmetric and at least positive semi-definite. It

	
	
                    
                    
                  
	
 can even be strictly positive definite if none of the variables  is a linear combination of the others. The probability density of the multivariate normal distribution (compare it to its one-dimensional counterpart (3.​10)) is then (4.23)If  as in the previous Example, we have simply  and , while the covariance matrix is



4.8.2 Correlation Does Not Imply Causality
A vanishing

	
	
                    
                    
                  
	
                    
                    
                  
	
 correlation coefficient of X and Y does not mean that these variables are stochastically independent: for each density  that is an even function of the deviations  and , one has . In other words,  is just a necessary, but not sufficient condition for independence: see bottom part of Fig. 7.​8 which illustrates the correlation in the case of finite samples.
Even though one observes a correlation in a pair of variables (sets of values, measurements, phenomena) this does not necessarily mean that there is a direct causal relation between them: correlation does not imply causality. When we observe an apparent dependence between two correlated quantities, often a third factor is involved, common to both X and Y. Example: the sales of ice-cream and the number of shark attacks at the beach are certainly correlated, but there is no causal relation between the two. (Does your purchase of three scoops of ice-cream instead of one triple your chances of being bitten by a shark?) The common factor of tempting scoops and aggressiveness of sharks is a hot summer day, when people wish to cool off in the water and sharks prefer to dwell near the shore.
Besides, one should be aware that correlation and causality are concepts originating in completely different worlds: the former is a statement on the basis of probability theory, while the latter signifies a strictly physical phenomenon, whose background is time and the causal connection between the present and past events.



4.9 Propagation of Errors
If we knew how to generalize (4.20) to an arbitrary function of an
 arbitrary number of variables, we would be able to answer the important question of error propagation. But what do we mean by "error of random variable"? In the introductory chapters we learned that each measurement of a quantity represents a single realization of a random variable whose value fluctuates statistically. Such a random deviation from its expected value is called the statistical uncertainty or "error". By studying the propagation of errors we wish to find out how the uncertainties of a given set of variables translate into the uncertainty of a function of these variables. A typical example is the determination of the thermal power released on a resistor from the corresponding voltage drop: if the uncertainty of the voltage measurement is  and the resistance R is known to an accuracy of no more than , what is the uncertainty of the calculated power ?
Let  be real random variables with expected values , which we arrange as vectorsandjust as in Sect. 4.8.1. Let  be an arbitrary function of these variables which, of course, is also a random variable. Assume that the covariances of all  pairs are known. We would like to estimate the variance of the variable Y. In the vicinity of  we expand Y in a Taylor series in  up to the linear term,and resort to the approximation  (see (4.9) and (4.10)) to compute the variance. It follows that (4.24)
where
	


                  
                  
                
is the covariance matrix of the variables : its diagonal terms are the variances of the individual variables, , while the non-diagonal ones () are the covariances . Formula (4.24) is what we have been looking for: it tells us—within the specified approximations—how the "errors" in  map to the "errors" in Y. If  are mutually independent, we have  for  and the formula simplifies to (4.25)


Example
Let  and  be independent continuous random variables with the mean values  and  and variances  and . We are interested in the variance  of their ratio . Since  and  are independent, we may apply formula (4.25). We need the derivativesThereforeorwhere .    



Example
Let X and Y be independent random variables with the expected values  and  and variances  and  (with respective "uncertainties of measurements"  and ). What is the variance  of the product of their powers,(This is a generalization of the function from the previous example to arbitrary powers m and n.) By formula (4.25) we again obtainThuswhere we have denoted .    



4.9.1 Multiple Functions and Transformation of the Covariance Matrix
Let us now discuss the case of multiple scalar functions , which all depend on variables ,We arrange the function values in the vector  and retrace the steps from the beginning of this section. We neglect all higher order terms in the Taylor expansionand take into account that . Instead of (4.24) we now obtain a relation between the covariance matrix of variable  and the covariance matrix of the variables ,
	This

	
	

 relation becomes even more transparent if we write the Taylor expansion aswhere  and  are n- and m-dimensional vectors, respectively, while D is an  matrix embodying the linear part of the expansion, namely (4.26)Henceor, in brief, (4.27)The propagation of errors in higher dimensions can therefore be seen as a transformation of the covariance matrix. The variances  of the variables  are the diagonal matrix elements of . In general they pick up terms from all elements of , even the non-diagonal ones, since (4.28)But if the variables  are mutually independent, only diagonal elements of  contribute to the right-hand side of the above equation, yielding (4.29)Equations (4.28) and (4.29) are multi-dimensional equivalents of (4.24) and (4.25). Note that the non-diagonal elements of  may be non-zero even though  are mutually independent! You can find an example of how to use these equations in the case of a measurement of
	
	the
	 momentum of a particle in Problem 4.10.6.



4.10 Problems

4.10.1 Expected Device Failure Time
A computer disk is controlled by five circuits (). The time until
	 an irreparable failure in each circuit is exponentially distributed, with individual time constants . The disk as a whole works if circuits 1, 2 and 3, circuits 3, 4 and 5, or, obviously, all five circuits work simultaneously. What is the expected time of disk failure?

 The probability that the ith element is not broken until time t (the probability that the failure time is larger than t) is exponentially decreasing and equals . For the disk to fail, three key events are responsible:The disk operates as long as . The probability that the disk still operates after time t, is thereforeThis is not yet our answer, since the expression still contains time! We are looking for the expected value of failure time, where we should recall that the appropriate probability density is  (see (3.​4)), hence



4.10.2 Covariance of Continuous Random Variables
(Adopted from [5], Example 4.56.) Calculate the linear correlation coefficient of continuous random variables X and Y distributed according to the joint probability densitywhere H is the Heaviside function (see (2.​8)).

 The linear correlation coefficient  of variables X and Y (see (4.21)) is equal to the ratio of covariance  to the product of their effective deviations  and . First we need to calculate the expected value of the product XY,then the expected values of X, Y,  and ,It follows thathenceand



4.10.3 Conditional Expected Values of Two-Dimensional Distributions
Let us return to the Example on p. 49 involving two random variables, distributed according to the joint probability densityFind  the conditional expected value of the variable Y, given , and  the conditional expected value of the variable X, given !

 We have already calculated the conditional densities  and  in (2.​28) and (2.​29), so the conditional expected value  equalsand the conditional expected value  is



4.10.4 Expected Values of Hyper- and Hypo-exponential Variables
Calculate the expected value, the second moment and the variance of

	
	

 continuous random variables, distributed according to the  hyper-exponential (see (3.​26)) and  hypo-exponential distribution (see (3.​28)).

 The hyper-exponential distribution, which describes a mixture (superposition) of k independent phases of a parallel process, whose ith phase proceeds with probability  and time constant , is defined by the probability density (4.30)where  and . The expected value of a hyper-exponentially distributed variable X is (4.31)and its second moment isIts variance is therefore (4.32)While  holds true for the usual single-exponential distribution, its hyper-exponential generalization always has , except when all  are equal: this inequality is the origin of the root "hyper" in its name.

 The hypo-exponential distribution describes the distribution of the sum of k () independent continuous random variables , in which each term separately is distributed exponentially with parameter . The sum variable  has the probability density (4.33)whereBy comparing (4.33) to (4.30) one might conclude that the coefficients  represent the probabilities  for the realization of the ith random variable, but we are dealing with a serial process here: all indices i come into play—see Fig. 3.​13! On the other hand, one can exploit the analytic structure of expressions (4.31) and (4.32), one simply needs to replace all  by . By a slightly tedious calculation (or by exploiting the linearity of  and using formula (4.20)) we obtain very simple expressions for the average and variance:It is easy to see—Pythagoras's theorem comes in handy—that one always has . The root "hypo" in the name of the distribution expresses precisely this property.


4.10.5 Gaussian Noise in an Electric Circuit
The noise in electric circuits is frequently of Gaussian nature. Assume that
	 the noise (random variable X) is normally distributed, with average  and variance .  Calculate the probability that the noise exceeds the value  and the probability that its value is on the interval between  and !  What is the probability that the noise exceeds , given that it is positive?  Calculate the expected value of |X|.

 It is worthwhile to convert the variable  to the standardized formso that . The required probabilities  are thenandwhere the probability density  is given by (3.​10). We have read off the numerical values of the integrals from Table D.1.

 The required conditional probability is
 Since , we also have , so we need to computeand revert to the old variable, hence .


4.10.6 Error Propagation in a Measurement of the Momentum Vector 

We are measuring the time t in which a non-relativistic particle of mass m and momentum p traverses a distance L (that is, ), and the spherical angles  and 
	of
	 the vector  relative to the z-axis. Suppose that we have measured the average values ,  and , but all measurements contain one-percent uncertainties ,  and , which are uncorrelated. Determine the uncertainties of the quantities
 In the notation of Sect. 4.9 we are dealing with the variableswith the averages ,  and . The corresponding covariance matrix (omitting the units for clarity) isWe need to calculate the covariance matrix of the variablesand we need the derivatives (4.26) to do that:When these expressions are arranged in the  matrix D, (4.27) immediately yieldsThe uncertainties of ,  and  then becomeThe propagation of the one-percent errors on the variables 1 / p,  and  has therefore resulted in more than one-percent errors on the variables ,  and :The error of  and  has increased dramatically. A feeling for why this happens in  can be acquired by simple differentiation  orThe average value of  is not very far from , where  and . Any error in  in this neighborhood, no matter how small, is amplified by the large factor  that even diverges as .
In addition, the covariances ,  and  are all non-zero, and the corresponding correlation coefficients are




References


1.
E. Brynjolfsson, Y.J. Hu, D. Simester, Goodbye Pareto principle, hello long tail: the effect of search costs on the concentration of product sales. Manage. Sci. 57, 1373 (2011)CrossRef


2.
E. Brynjolfsson, Y.J. Hu, M.D. Smith, The longer tail: the changing shape of Amazon's sales distribution curve. http://​dx.​doi.​org/​10.​2139/​ssrn.​1679991. 20 Sep 2010


3.
C. Anderson, The Long Tail: Why the Future of Business is Selling Less of More (Hyperion, New York, 2006)


4.
F. James, Statistical Methods in Experimental Physics, 2nd edn. (World Scientific, Singapore, 2010)


5.
Y. Viniotis, Probability and Random Processes for Electrical Engineers (WCB McGraw-Hill, Singapore, 1998)




Footnotes


1


A function is defined to be convex if the line segment between any two points on the graph of the function lies above the graph.

 



2


A random vector  with a distribution function  and a random vector  with a distribution function  are mutually independent if . This is an obvious generalization of (2.​20) and (2.​24).

 













© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_5




5. Special Discrete Probability Distributions




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
The binomial (Bernoulli), multinomial, negative binomial (Pascal), and Poisson distributions are presented as the most frequently occurring discrete probability distributions in practice. The normal approximation of the binomial distribution is introduced as an example of the Laplace limit theorem, and the Poisson distribution is shown to represent a special limiting case of the binomial.



In this chapter we discuss distributions of discrete random variables, of which the binomial and the Poisson distributions are the most important.

5.1 Binomial Distribution
          
        

                  
                  
                

We are dealing with the binomial (Bernoulli) distribution whenever many random, mutually independent ("Bernoulli") trials yield only two kinds of outcomes—something occurs (probability p) or does not occur (probability ). Tossing a coin results in heads or tails; a girl or a boy is born; the weather prediction for tomorrow is rainy or dry. The probability that in N trials we encounter n outcomes of "type p" and  outcomes of "type q" counted by the random variable X, is given by a two-parameter distribution (5.1)with parameters N and . The structure  is obvious: since the "p" events and "q" events are mutually independent, we simply multiply the probability of "p" occurring n-times and "q" occurring -times. We just need to figure out the number of ways such a combination can be accomplished: it is given by the binomial symbol (1.​5). Of course, the distribution is normalized, (5.2)where we have used the binomial expansion (1.​6). The
          
        

          
           examples of the binomial distribution with parameters ,  and ,  are shown in Fig. 5.1. The distribution with  is symmetric about its average value; the trend of its values (filled circles) vaguely reminds us of the normal distribution; this will be exploited later on (Sect. 5.4).Fig. 5.1Binomial (Bernoulli) distribution for n outcomes of "type p" with  trials. [Left] The distribution with parameter  is symmetric around N / 2. [Right] A distribution with parameter  (in this case ) is squeezed towards the origin; for  it is pushed towards N




Example

A six-sided fair die is thrown five times. What is the probability of obtaining 3 dots exactly twice () in these five trials ()? The probability of the outcome "3 dots" in each throw is , while the probability for any other outcome is . Hence the probability isWhat is the probability that three dots appear at most once ()? There are only two possibilities: they do not appear at all () or precisely once (). These events are mutually exclusive, soWhat is the probability of having three dots at least twice ()? The strenuous path to answering this iswhile an easy one (complementarity of events!) is . 



Mini-example The unstable meson  can decay in a variety of decay modes:  (mode 1),  (mode 2),  (mode 3),  with branching fractions (see definition (3.​27)) , , ,  Suppose we observe N decays, x of which are of type 2: then X is a binomially distributed random variable—as we are going to obtain a different x at fixed N in any
          
        

                  
                 experiment—with parameters N and . 



Example

The waiting time in a cafeteria is an exponentially distributed random variable with the average of 4 min. What is the probability that the student will be served in less than 3 min on at least four of the following six days?
The probability that the student (on any day) has not been served in 4 min decays exponentially: , where . The probability of him being served in less than 3 min is . We must consider all possibilities on consecutive days, which is accounted for by the binomial distribution. On each day there are only two options: he is served in less than 3 min (probability p) or later than that (probability ). Thus the probability we are looking for isCalculate the probability that the student is served quickly (in less than 3 min) on at least one day out of six and the probability of him being served quickly precisely on day six! How does the latter result change when  is modified? (Hint: expand the exponential up to the linear term.) 




Example

The reliability of an airplane engine (the probability of it functioning flawlessly) is p. The airplane is able to fly if at least half of its engines operate. For which values of p a four-engine airplane is safer than a two-engine airplane?
A two-engine airplane can fly if at least one engine is operational, i.e. with probabilityA four-engine airplane can fly if at least two engines operate, i.e. with probabilityWe are seeking values of p satisfying the inequality . After rearranging the terms we obtainSince  only the second factor is relevant: thus four engines are safer than two if . A consolation for potentially frightened passengers: if , which is a poor engine by modern engineering standards, one still has  and . 



5.1.1 Expected Value and Variance
The expected value (average) and the variance of a binomially distributed random variable X can be calculated by substituting  in (5.2), computing the first and second derivative with respect to , and finally resetting . Thusof which the first derivative with respect to  yields (5.3)and the second derivative gives (5.4)When  is restored, the left-hand side of (5.3) is precisely the expression for the expected value of X, while the left-hand side of (5.4) is the expected value of its function . The first equation therefore giveswhile the second equation yields(According to the convention (4.​2) we denote the expected values by a line over the corresponding random quantity.) Finally, both results can be combined to calculate the variance:Let us summarize: (5.5)If we interpret  as the uncertainty of the measured number of events—do not confuse it with the  parameter of the normal distribution!—we have (5.6)What does that mean for the empirical determination of probabilities in Bernoulli trials? If in N trials we observe  "good" and  "bad" outcomes, the ratios  and  at large enough N become good approximations to the unknown probabilities p and q. In this case we may writeand use (5.6) to express p: (5.7)A method to calculate arbitrary moments of discrete random variables directly by means of probability generating functions is discussed in Appendix B.1.


Example

(Adapted from [1].) Initially we have  radioactive nuclei,  of which remain "alive" after . How accurately can one determine the half-time () based on this information? We use (5.7) to calculate the probability p (and its uncertainty) of having n undecayed nuclei at time t:It follows thatwhere we have used the small-x expansion of the function
              
            

                      
                      
                    

                      
                      
                      
                     and inserted  and . 





5.2 Multinomial Distribution
          
        

                  
                  
                

The binomial distribution can be generalized by considering not just two kinds of outcomes with probabilities p and  in N trials, where , but having k types of outcomes with probabilities  and requiringThe probability that in N trials we obtain precisely  outcomes of type 1,  outcomes of type 2 and so on, is given by the multinomial distribution
where the combinatorial factor at the right is the multinomial symbol (1.​4). Let us assume
          
        

          
           that the ith outcome is a "good" event while all other outcomes are "bad". This means that every random variable  by itself (with values ) is distributed binomially with parameters N and . By (5.5) the expected value and variance of individual 's arewhile the covariances of the  pairs are
Mini-example We measure the velocity distribution of molecules (we expect a result similar to Fig. 4.​1 (right)) and arrange the data in a histogram with  equidistant bins , , and so on, up to . Individual bins contain  molecules; all bins are mutually independent. In total we count  molecules. Such a histogram—it will change at each new measurement—represents a multinomial distribution. 



5.3 Negative Binomial (Pascal) Distribution
          
        

                  
                  
                

Suppose we observe a sequence of independent Bernoulli trials with probability p for a type-A outcome (e.g. an electronic circuit says "success") and probability  for a type-B outcome (circuit reports "failure"), as shown below:How long must we wait for r failures to occur? The probability to count n successes ( in the above sequence) before accumulating r failures (,  above) is given by the negative binomial
1 random variable with the distribution (5.8)What is the probability of having n outcomes of any kind (A or B, variable Y), before encountering r failures? Because the sum of "good" and "bad" events, , is constant, we just need to replace  in the definition, thusBoth forms of the distribution are normalized, which one can check by using the formula


5.3.1 Negative Binomial Distribution
            
            
          

             of Order k

Here is a tougher nut to crack: how long must we wait for k consecutive type-B outcomes or, more generally still, how long must we wait for r appearances of k consecutive type-B outcomes? One may imagine a device exposed to strong radiation that causes errors in its memory. The device is able to recover from these errors (B) and remains operational (A) until the radiation damage is so large that k consecutive errors occur. For example, in the sequencewe have had  consecutive failures (B) after  outcomes of type A or B, while in the sequencewe have had two () occurrences of a four-fold () consecutive error after  outcomes. The probability for arbitrary k and r is given by the negative binomial distribution of order k [2]:where  and where we sum over all non-negative integers , such that . An example of how this distribution is used can be found in Sect. 5.6.3.



5.4 Normal Approximation of the Binomial Distribution
          
          
        

                  
                  
                

If N is large and neither p nor q are too close to zero, the binomial distribution can be approximated by the normal distribution, although this appears to be something preposterous as the former is discrete, while the latter is continuous! This approximation is embodied in the Laplace limit theorem
 (5.9)proven in Appendix B.3.1. In other words, the standardized binomial variableis asymptotically normal. In practice, this applies already when .


Example

A die is thrown 120 times. What is the probability of observing 4 dots no more than 18 times? The distribution of all  events is binomial, with probabilities  and . The exact answer—requiring us to calculate 19 terms and an overwhelming amount of factorials—is (5.10)By resorting to (5.9) we can obtain an approximate answer with much less effort. The formula requires us to calculate the average value and the effective deviationand then calculate the standardized variables corresponding to the original (binomial) variables, i.e. the lower () and upper () summation index (see Fig. 5.2 (left)). One usually takes an 0.5 smaller lower value and an 0.5 larger upper value (see Fig. 5.2 (right)): this is the easiest way to approximate any discrete value  by the area under the curve of the probability density  on the interval —and ensure that even by approximating a single point of a discrete distribution one obtains a non-zero result.
The boundary values of the standardized variables are  and . By using Table D.1 we calculatewhere  is the distribution function of the standardized normal distribution. Compared to (5.10), this approximate probability is off by less than . 



Fig. 5.2[Left] Approximating the binomial distribution by a normal distribution in the case ,  (, ).] [Right] Same figure in logarithmic scale showing the boundary values (0 and 18) of the binomial distribution and the corresponding integration boundaries of the normal distribution



5.5 Poisson Distribution
          
        

The Poisson distribution is the limit of the binomial in which the probability p of an individual outcome becomes very small () and the number of trials very large (), such that the average  remains unchanged. In each term of (5.1) we therefore write  and :In the limit , the last factor is justtherefore . We have obtained a single-parameter distribution with parameter , its expected value. It is traditionally denoted by , so this important distribution, illustrated in Fig. 5.3, is usually written as (5.11)
Fig. 5.3Three examples of the Poisson distribution with , 3.7 and 9.5. For comparison, the density of the normal distribution  is also shown

No approximations have been made by taking the  and  limits, just one parameter has evaporated. At its heart, then, the Poisson distribution is still "binomial", whence it also inherits the expressions for its average and variance; but  implies , thus (5.12)Instead of (5.6) we may therefore write



Example

A total of  people are vaccinated. The probability p for side effects is small, , therefore, on average, only  people will experience them. What is the probability that the number of people experiencing unwanted effects will be greater than two? The probability that precisely n people experience a side effect, isThe probability we are seeking is therefore . The calculation of these 1998 values can be avoided by considering the complementary
            
           event: . 



Classical example We count X nuclear decays in time t; let  (at least a few times ten). The estimate of the true source activity (decays per unit time) a is . The measured X fluctuates about  by . But the true value  is unknown, hence we approximate  and writeDividing the second equation by t we obtain the relation between the true activity a and the measured value :Therefore, if we wish to measure the source activity to a precision of , we must count  decays. This obstacle awaits us in all experiments where anything is being "counted". A k-fold reduction in statistical uncertainty requires us to count  times
           more events, i.e. measure  times as long. 



Example

(Adapted from [1].) On an average day the surface of the Earth (radius ) is hit by 25 meteorites. What is the probability that in 10 years at least one of its  inhabitants will be hit by a meteorite?
The probability
           of an individual being hit is proportional to the ratio of surface areas , where  is the average surface area of a human being and  is the surface area of the Earth. In ten years the Earth receives  meteorites, thus the expected number of hit people in this period of time is . The probability that a meteorite hits at least one person, is therefore . 




Example

Let us stay with the dangers from the sky! When London was bombarded with German "flying bombs" during World War II, some people thought that hits tend to form clusters, looking as if the probabilities of certain areas being hit were relatively higher [3]. Can this assumption be justified?
There were 537 hits on the surface area of , divided into  quadrants with an area of  each, so the average number of hits in any quadrant was . If the points of impact were completely random, the probability that a chosen quadrant has been hit  times, is given by the Poisson distributionThe expected number of quadrants with precisely n hits should therefore be , for example,  quadrants with no hits at all. The expected numbers of quadrants with n hits and the corresponding observed numbers are shown in the Table 5.1.Table 5.1The distribution of hits in World War II bombing raids over LondonNumber of bombs in quadrantExpected number of quadrantsObserved number of quadrants0226.742291211.392112



3



4











If the
           projectiles "preferred" specific quadrants, one should be able to see this primarily as a decrease of the number of quadrants with no hits and an increase in the middle portion of the distribution. But the excellent agreement of the expected and observed numbers proves that the distribution of hits is consistent with a random—Poisson—distribution. We shall put this statement on a more quantitative footing in Sect. 10.​3. 




Example

(Adapted from [1].) A gas mixture contains  molecules of  endowed with the radioactive  isotope. A sample of  is taken for analysis. What is the probability that the concentration of the radioactive admixture in the sample will exceed its average concentration by more than ?
On average
            
          , a sample will contain  radioactive molecules. The probability we wish to compute is the sum of probabilities that the sample contains  or  or  molecules, and so on, thuswhere . How do we determine ? Assume that the mixture is at standard conditions, where a mole of gas ( molecules) has a volume of . Therefore, a sample with volume V contains  molecules, which by far exceeds the number of radioactive molecules in it, so we can safely set . For such high n we can approximate the Poisson distribution by the normal (see Sect. 5.4) and replace the sum of millions of billions of Poissonian contributions by an integral of the normal density with average  and variance  with the integration boundaries  and , i.e. the standardized normal distribution with the boundariesBy using (3.​9), (3.​12) and Table D.2 we obtain . A direct calculation of the sum by Mathematica yields . 




5.6 
                Problems
          
        
              

5.6.1 Detection Efficiency
            
          

                    
                    
                  

                    
                  

                    
                    
                  

(Adapted from [4].) Galactic sources of gamma radiation are measured by specially designed gamma-ray spectrometers. Assume that for a certain region of the sky we have used two different detectors (different electronics, varying atmospheric conditions and so on) and determined the following numbers of sources:  Calculate the total detection efficiency—the ratio between the number of observed rays and the number of all incident rays—and its uncertainty! Note that the measurement has a binomial nature: any source is either detected (probability p) or missed (probability ).

 The efficiencies (which are the estimates of the true values based on repeated samples) for detection by a single spectrometer and for simultaneous detection arewhere N is the true number of the sources. Of course, because the values ,  and  fluctuate statistically, one can only obtain an estimate  for N. The measurements with two spectrometers are mutually independent, , and thereforeBut the measurements with two devices ("event 1" and "event 2") are not mutually exclusive, implying , thus the total efficiency isThe random variable X that "counts" good events (detected sources) is binomially distributed, the minimum and maximum numbers of detected sources being 0 and N, respectively. The relative number of the detected sources X / N therefore has the variance(recall (4.​12)). The variance of the efficiency of an individual detector is thenThe variance of the total efficiency  is calculated by using (4.​25):The total efficiency  as a function of  and  is shown in Fig. 5.4 (left), while its uncertainty, multiplied by , is shown in Fig. 5.4 (right).Fig. 5.4[Left] The total efficiency  for the detection of gamma-rays with two spectrometers as a function of individual detector efficiencies  and . [Right] The uncertainty of the total efficiency, multiplied by . The uncertainty is smallest in the limit , but also when , although in that corner, one also has .



5.6.2 The Newsboy Problem 

A newsboy purchases
             his newspapers at a price of  and sells them at . While he must purchase all copies from his supplier at once, his actual sales depend on the fluctuating daily demand. The long-term average of the daily demand is 100 copies, and he buys just as many from the supplier.  Calculate the newsboy's daily profit by assuming that the demand is described by a random variable with a Poisson distribution with average .  How many copies should he purchase in order to maximize his profit?

 If the variable X represents the demand and n is the number of copies purchased by the newsboy, the variable  describes the number of copies sold to the customers: if the demand is below the number of copies he has purchased, he sells X, while in the opposite case he sells n (since he has none left).  His profit with  sold copies (fluctuating by the day) is therefore measured by the random variableand its expected value is . It holds thatThe second term in the final expression is zero, since  (the newsboy's daily purchase equals the average demand), thereforeor .Fig. 5.5The profit of the newsboy purchasing n newspaper copies  from the supplier and selling them at  to customers whose demand is modeled by a Poisson distribution with average 100


 As shown in Fig. 5.5, the newsboy's daily purchase of 100 copies is not optimal: he could count on a maximum profit of  by purchasing 95 or 96 copies per day (the maximum of  is at ); in all other cases his profit will be smaller than that
            
          

                    
                    
                  . If he buys more than 150 copies per day, he will even lose money.


5.6.3 Time to Critical Error
            
          

A computer memory constantly reviews its "sanity": it may detect flawless operation (A) or error (B). The memory operates until encountering four consecutive errors (BBBB, "critical error"). The probability of a single error is . Calculate the probability that after  sanity checks the memory still operates. Use  your knowledge of combinatorics and  the negative binomial distribution
            
             of order k.

  In a random sequence of outcomes A and B we await the subsequence BBBB, which may occur at the very beginning, or the subsequence ABBBB that, however, should not be preceded by the critical BBBB quartet. The options are:

The probability of the critical error occurring already in the first  steps (immediate BBBB combination), is . The probability of it occurring in  steps (subsequence ABBBB) is , while in  steps (combinations AABBBB or BABBBB), it is . From the table above we further see that  and , but to conclude  would be a mistake: indeed there would be  possibilities before the critical part ABBBB of the whole sequence, but one of them has the form , which should be discarded as it already contains the terminating sequence BBBB at the very beginning, which brings us to the  case considered before. For , we therefore have . Analogously, the subsequent P(N) arewhere  is the number of quartets of the form BBBB that precede the terminal quintet ABBBB in the sequence and should therefore not be considered. This number becomes increasingly difficult to determine at high N: for example, we have  for ,  for ,  for , and so on.
There is a more elegant solution. Define the probability P(i) that the critical error occurs precisely at the ith place in the sequence. The first few values (from  to ) are known from the previous discussion:Now define the probability  of the critical error occurring anywhere up to (including) the nth place,From  onwards we therefore simply await the ABBBB subsequence which occurs with probability , while no critical error should occur before that, leading to the recurrenceThe solution of the problem—the sum up to  can be evaluated by some symbolic computation program, e.g. Mathematica—is therefore
 Undoubtedly the solution by using the negative binomial distribution of order k is the simplest (we can use Mathematica again), but it is bereft of any insight into the heart of the problem:



5.6.4 Counting Events with an Inefficient Detector
            
          

                    
                    
                  

                    
                    
                  

                    
                  

                    
                    
                  

(Adapted from [4].) Charged particles are counted by a detector with a non-ideal efficiency: the probability to detect a particle is . Assume that the number X of particles traversing the detector in fixed time t is Poisson-distributed with average . What is the probability of detecting precisely r particles in time t?

 If we are supposed to count r particles, at least r particles should actually fly through the detector. The desired probability is therefore the sum of probabilities of detecting r particles while , , ,  particles have actually flown through. For given n the number r of actually detected particles is given by the binomial distribution; from the total probability formula (1.​15) it follows thatorwhich is nothing but the Poisson distribution with the expected value . (In the theory of Poisson processes this  effect is suggestively called thinning.)


5.6.5 Influence of Primary Ionization on Spatial Resolution 


          Charged
          

             particles flying through gas ionize its atoms and molecules. In this (so-called primary) ionization, a few times ten electron-ion pairs are generated per centimeter of the particle's path length through the gas at normal conditions, depending on the average atomic number of the gas, . The average number of ionizations on distance x is , where  [5].  Determine the distribution of locations where the electron-ion pairs are formed!  How does the discrete nature of the primary ionization (see Fig. 5.6) influence the spatial resolution of such an elementary detector?Fig. 5.6A charged particle passing near the anode wire in a gas ionization detector. The jth electron-ion pair is formed at . The electrons are attracted by the anode, and their drift time towards it is a measure of the impact distance of the original particle


 Ionizations are rare, independent events, so they obey the Poisson distribution: if  is the average number of ionizations on distance L, the probability for n ionizations is
 For these n ionizations the distribution of each (jth) pair () along x isThe function , for example, describes the point of creation of the third pair if seven pairs were created in total. (To understand the structure of this expression, plot , then ,  and their sum—it is 2—then , ,  and their sum—it is 3—and so on!) The distribution for the point of creation of the jth pair, if  pairs per unit length were created, is thenThese distributions are shown in Fig. 5.7 (left) for , which approximately corresponds to neon at a pressure of . Each function is normalized on .

 The first factor, relevant for the spatial resolution of an ionization detector, is the non-uniformity of primary ionizations. The jth ionization occurs on distance  from the origin (see Fig. 5.6), but in both directions (negative or positive x), which can be absorbed in computing the moments of  by replacing . The average distance of the jth ionization and the average of its square are thereforeso the corresponding variance isFrom Fig. 5.6 we see that , thus, by error-propagation (4.​25), we findFigure 5.7 (right) shows the uncertainties  for various j with  ( mixture of argon and isobutane at normal conditions). The spatial resolution therefore deteriorates with
            
           increasing number of primary ionizations.Fig. 5.7[Left] The distribution of points of creation of the jth electron-ion pair. [Right] The influence of primary statistics on the spatial resolution




References


1.
I. Kuščer, A. Kodre, Mathematik in Physik und Technik (Springer, Berlin, 1993)MATH


2.
A.N. Philippou, The negative binomial distribution of order  and some of its properties. Biom. J. 26, 789 (1984)MathSciNetCrossRefMATH


3.
R.D. Clarke, An application of the Poisson distribution. J. Inst. Actuaries 72, 481 (1946)


4.
A.G. Frodesen, O. Skjeggestad, H. Tøfte, Probability and Statistics in Particle Physics (Universitetsforlaget, Bergen, 1979)


5.
F. Sauli, Principles of Operation of Multiwire Proportional and Drift Chambers (CERN Reprint 77-09, Geneva, 1977)




Footnotes


1


The 'negative' attribute in the name of the distribution originates in the property.

 













© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_6




6. Stable Distributions and Random Walks




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
Stable distributions are special types of probability distributions whose origin is a particular limit regime of other types of distributions. They are closely related to the simple convolution process, which is introduced first for continuous and then for discrete random variables. This leads to the central limit theorem as one of the most important results of probability theory, as well as to its generalized version which is useful in the analysis of random walks. Extreme-value distributions are also presented, as they possess a limit theorem of their own (Fisher-Tippett-Gnedenko). The last part is devoted to the discussion of discrete-time and continuous-time random walks, together with their characteristic diffusion properties.



In this chapter we sum independent random variables  and discuss what happens to the distribution of their sum, . We shall see that the distribution of Y is given by the convolution of distributions of individual 's, and that in the case —under certain conditions—the distributions of Y tend to stable distributions, relevant for the processes of random walks.

6.1 Convolution of Continuous Distributions


                  
                  
                
	
What is the distribution of  if continuous random variables X and Y correspond to densities  and ? We are interested in the probability that the sum  falls within the interval , where x and y are arbitrary within their own definition domains. All points fulfilling this requirement are represented by the oblique shaded area in the figure.
  

One must add up all contributions to the probability within this band. The infinitesimal area  (shaded rhomboid) corresponds to the infinitesimal probability . By integrating over x we obtain the probability . Let us write only its density and insert : (6.1)This operation is called the convolution of distributions and we denote it by the symbol . If you do not trust this geometric argument, one can also reason as follows:whence (6.1) follows immediately. Convolution is a symmetric operation:A convolution of three probability distributions is calculated as follows:and generalizations of higher order are obvious.Fig. 6.1Twofold consecutive convolution of  with itself



Example

What do we obtain after two consecutive convolutions of a symmetric uniform distribution , corresponding to the "box" probability density f shown in Fig. 6.1? The first convolution yieldswhich is a triangular distribution ( in the figure). The second convolution giveswhere . This density is denoted by  in the figure. Try to proceed yet another step and calculate ! (You shall see in an instant where this is leading.)     




Example

What about the convolution of an asymmetric distribution? For instance, what is the distribution of the variable  if all  are uniformly distributed on [0, 1], i.e. ?
The density of the variable Y for arbitrary  is (6.2)and is shown in Fig. 6.2 for  (original distribution),  (single convolution), ,  and . As in the previous example, the density after several convolutions reminds one of something "bell-shaped", one could suspect, the normal distribution. Besides, the distribution of the sum variable creeps away from the origin: this is a cue for the following subsection.     



Fig. 6.2Multiple convolutions of the U(0, 1) distribution with itself


6.1.1 The Effect of Convolution on Distribution Moments
First consider what happens to the average of the sum of two random variables:thusor , which we already know from (4.​6). Now let us also calculate the variance of Z! We must average the expressionBecause X and Y are independent, the expected value of the second term is zero, so we are left with onlyor . We know that too, namely, from (4.​20), in a slightly different garb also from (4.​25) if one sets . As an exercise, check what happens to the third and fourth moment upon convolution: you will find out that , so the third moments of distributions are additive. By taking into account the definition of skewness  (see (4.​18)) this can also be written asThe fourth moments are not additive, since , but by using (4.​19) this can be simplified to



Example

It appears as if even the most "weird" distribution evolves into something "bell-shaped" when it is convoluted with itself a couple of times. Figure 6.3 shows an example in which upon just two convolutions a rather irregular density (fulfilling all requirements for a probability density) turns into a density closely resembling the standardized normal distribution.     



Fig. 6.3As few as two convolutions may be needed to turn a relatively irregular distribution into a distribution that looks almost like the standardized normal. (We have subtracted the expected value of the distribution obtained at each step and rescaled the variance)



Example

Still, convolution does not perform miracles. Let us calculate the n-fold convolution of the Cauchy distribution with itself! We obtain (6.3)Certainly  does not approach the density of the normal distribution; rather, it remains faithful to its ancestry. Consecutive convolutions yield just further Cauchy distributions! We say that the Cauchy distribution is stable with respect to convolution. The reasons for this behaviour will be discussed below.     





6.2 Convolution of Discrete Distributions
	


                  
                  
                
	
The discrete analog of the continuous convolution formula (6.1) for the summation of independent discrete random variables X and Y is at hand: if X takes the value i, then Y must be  if their sum is to be n. Since X and Y are independent, the probabilities for such an "event" should be multiplied, thus (6.4)or



Example

Let us demonstrate that the convolution of two Poisson distributions is still a Poisson distribution! Let  and  be mutually independent Poisson variables with parameters  and . For their sum  one then hasthus indeed . A more elegant solution of this problem will be given by the Example on p. 369 in Appendix B.1.     




Example

Let us compute the probability distribution of the sum  of independent discrete random variables X and Y, distributed according toThe distributions are shown in Fig. 6.4 (left) [1].Fig. 6.4Discrete convolution in the case when the distributions have different supports. [Left] Distributions f and g. [Right] Convolution of f and g


In principle we are supposed to find all values , so we must compute the convolution sum  for each n separately:To make the point, let us just calculate the probability that . We needWhen n and j indices are combed through, many  terms vanish (crossed-out terms above); only the underlined bilinears survive. Such a procedure must be repeated for each n: a direct calculation of convolutions may become somewhat tedious. The problem can also be solved by using generating functions, as demonstrated by the Example on p. 374 in Appendix B.2. 
                    
                    
                       




6.3 Central Limit Theorem
	

	
Let  be real, independent and identically distributed random variables with probability density , whose expected value  and variance  are bounded. Define the sum of random variables . By (4.​6) and (4.​22), the expected value and variance of  are  and , respectively. The probability density  of the sum variable  is given by the n-fold convolution of the densities of the 's,The example in Fig. 6.2 has revealed that the average of the probability density, calculated by consecutive convolutions of the original density, kept on increasing: in that case, the average in the limit  even diverges! One sees that the variance keeps on growing as well. Both problems can be avoided by defining a rescaled variableThis ensures that upon subsequent convolutions, the average of the currently obtained density is subtracted and its variance is rescaled: see Fig. 6.3. In the limit  the distribution function of the variable  then converges to the distribution function of the standardized normal distribution N(0, 1),or, in the language of probability densities,In other words, the dimensionless probability density  converges to the standardized normal probability density in the limit , which is known as the central limit theorem (CLT).

6.3.1 Proof of the Central Limit Theorem
The central limit theorem can be proven in many ways: one way is to exploit our knowledge on momentum-generating functions from Appendix B.2. Suppose that the momentum-generating function of the variables  exists and is finite for all t in some neighborhood of . Then for each standardized variable , for which  and  (thus also ), there exists a corresponding momentum-generating functionwhich is the same for all . Its Taylor expansion in the vicinity of  is (6.5)Let us introduce the standardized variableIts momentum-generating function is . Since the variables  are mutually independent, this also applies to the rescaled variables , therefore, by formula (B.16), we getorBy using the expansion of , truncated at second order, we getHenceWe know from (B.13) that this is precisely the momentum-generating function corresponding to the normal distribution N(0, 1), so indeedwhich we set out to prove. A direct proof (avoiding the use of generating functions) can be found in [2]; it proceeds along the same lines as the proof of the Laplace limit theorem in Appendix B.3.1.
The speed of convergence to the standardized normal distribution N(0, 1) with the distribution function  is quantified by the Berry-Esséen theorem [2]. If the third moment of  is bounded (), it holds that

	
	where  [3]. Now we also realize why consecutive convolutions in (6.3) have not led us to the normal distribution: no moments exist for the Cauchy distribution (see Sect. 4.​7.​1), so the condition  is not fulfilled. Moreover, one should not truncate the Taylor expansion (6.5).
The central limit theorem and the form of the bound on the speed of convergence remain valid when summing variables  distributed according to different (non-identical) probability distributions, if the variables are not too "dispersed" (Lindeberg criterion, see [2]). An excellent (and critical) commentary on "why normal distributions are normal" is also given by [4].


Example

Let us revisit the convolution of the uniform distribution in Fig. 6.2. We sum twelve mutually independent variables  and subtract 6, (6.6)What are we supposed to get? The averages of all  are 1 / 2, , while their variances are  (see Table 4.​1). Hence, Y should also have an average of zero and a variance of . By the central limit theorem, Y should be almost normally distributed, if we believe that . How well this holds is shown in Fig. 6.5.Fig. 6.5Histogram of  values Y, randomly generated according to (6.6), compared to the density of the standardized normal distribution (3.​10). In effect, the figure also shows the deviation of (6.2) from the normal density. The sharp cut-offs at 
 and 4.7 are random: by drawing a larger number of values the histogram would fill the whole interval 


We have thus created a primitive "convolution" generator of approximately normally distributed numbers, but with its tails cut off since Y can never exceed 6 and can never drop below . It is a practical generator—which does not mean that it is good. How a "decent" generator of normally distributed random numbers can be devised will be discussed in Sect. C.2.5.     




Example

(Adapted from [5].) The mass M of granules of a pharmaceutical ingredient is a random variable, distributed according to the probability density (6.7)To analyze the granulate, we acquire a sample of 30 granules. What is the probability that the total mass of the granules in the sample exceeds its average value by more than ?
The average mass of a single granule and its variance areThe probability density  of the total sample mass X, which is also a random variable, is a convolution of thirty densities of the form (6.7); this number is large enough to invoke the central limit theorem, so the density  is almost normal, with average  and variance :The desired probability is thenwhere we have used Table D.2. 
                      
                      
                         





6.4 Stable Distributions 

The normal distribution as the limit distribution of the sum of independent random variables can be generalized by the concept of stable distributions [6, 7].
Suppose we are dealing with independent random variables ,  and  with the same distribution over the sample space . We say that such a distribution is stable, if for each pair of numbers a and b there exists a pair c and d such that the distribution of the linear combination  is equal to the distribution of , that is,Such random variables are also called 'stable'; a superposition of stable random variables is a linear function of a stable random variable with the same distribution.
Stable distributions are most commonly described by their characteristic functions (see Appendix B.3). Among many possible notations we follow [6]. We say that a random variable X has a stable distribution , if the logarithm of its characteristic function (B.17) has the formwhereThe parameter  is the stability index or characteristic exponent, the parameter  describes the skewness of the distribution, and two further parameters  and  correspond to the distribution scale and location, respectively. For  the expected value exists and is equal to . For general  there exist moments , where .Fig. 6.6Stable distributions . [Top left and right] Dependence on parameter  at  and 1.0. [Bottom left and right] Dependence on parameter  at  and 1.0. At  the independent variable is shifted by 


It is convenient to express X by another random variable Z,Namely, the characteristic function of Z is somewhat simpler,as it depends only on two parameters,  and . The probability density  of the variable Z is calculated by the inverse Fourier transformation of the characteristic function :where . The values of  and  can be computed by using integrators tailored to rapidly oscillating integrands: see [8], p. 660; a modest software support for stable distributions can also be found in [9]. With respect to  and , the definition domains of  areThe dependence of  ( or  with appropriate scaling) on the parameter  is shown in Fig. 6.6 (top left and right), while the dependence on  is shown in the same figure at bottom left and right. 
                  
                  
                
	
By a suitable choice of parameters such a general formulation allows for all possible stable distributions. The most relevant ones areStable distributions with  have a characteristic behaviour of probability densities known as power or fat tails. The cumulative probabilities satisfy the asymptotic relations
	


                  
                
 (6.8)where . For  such asymptotic behaviour is valid in both limits, . Note that the probability density has the asymptotics  if the cumulative probability goes as 

                  
                

                  
                  
                .


6.5 Generalized Central Limit Theorem 

Having introduced stable distributions (Sect. 6.4) one can formulate the generalized central (or Lévy's) limit theorem, elaborated more closely in [2]. Here we just convey its essence.
Suppose we have a sequence of independent, identically distributed random variables , from which we form the partial sumAssume that their distribution has power tails, so that for  the following limits exist:and . Then real coefficients  and  exist such that the rescaled partial sumin the limit  is stable, and its probability density is . Its skewness is given by , while  and  arewhere H is the Heaviside function. The constant  is defined next to (6.8). The coefficient  for  diverges with increasing n as .
The generalized central limit theorem is useful in analyzing the process of random walk, which is analogous to extending the partial sum of random numbers . Such processes are discussed in Sects. 6.7 and 6.8. The convergence to the stable distribution when  is becoming more and more "capricious" when  decreases.


6.6 Extreme-Value Distributions 

In Sects. 6.3 and 6.4 we have discussed the distributions of values obtained in summing independent, identically distributed random variables . Now we are interested in statistical properties of their maximal and minimal values, i.e. the behaviour of the quantitieswhen . We thereby learn something about the probability of extreme events, as exceptionally strong earthquakes, unusual extent of floods or inconceivably large amounts of precipitation: "It rained for four years, eleven months, and two days." (See [10], p. 315.) The variables  are the values of the process, usually recorded at constant time intervals—for example,  daily temperature averages on Mt. Blanc—while  is the corresponding annual maximum. We are interested in, say, the probability that on top of Mt. Blanc, the temperature of  will be exceeded on any one day in the next ten years.
In principle, we have already answered these questions—about both the maximal and minimal value—in Problem 2.​11.​6: if  is the distribution function of the individual 's, the maximal values  are distributed according to (6.9)and the minimal asBut this does not help much, as  is usually not known! A statistical analysis of the observations may result in an approximate functional form of , but even small errors in  (particularly in its tails) may imply large deviations in . We therefore accept the fact that  is unknown and try to find families of functions , by which extreme data can be modeled directly [11, 12].
  

There is another problem. Define  as the smallest value x, for which . Then for any  we get , when , so that the distribution function of  degenerates into a "step" at . The figure above shows this in the case of uniformly distributed variables  with probability density  () and distribution function  (). When , the distribution function  tends to the step (Heaviside) function at , while its derivative (probability density) resembles the delta "function" at the same point. Our goal is to find a non-degenerate distribution function. We will show that this can be accomplished by a rescaling of the variable , (6.10)where  and  are constants. Illustrations of a suitable choice of these constants or of their calculation are given by the following Example and Exercise in Sect. 6.9.5. A general method to determine these constants is discussed in [13, 14].


Example

Let  be a sequence of independent, exponentially distributed variables, thus . Let  and . Thenwhen . By a suitable choice of  and  we have therefore stabilized the location and scale of the distributions of  in the limit .
Let us repeat this calculation for independent variables with the distribution function  and for uniformly distributed variables, ! In the first case we set  and , and get  (). In the second case a good choice is  and , yielding  () in the limit . Plot all three functions  of this Example and elaborate why one or the other are more or less sensible for the actual physical description of extreme phenomena!     



6.6.1 Fisher-Tippett-Gnedenko Theorem

	

Apparently the choice of constants  and  is crucial if we wish the distribution of  in the limit  to be non-trivial (not degenerated into a point); the basic formalism for a correct determination of these constants is discussed e.g. in [14]. In the following we assume that such constants can be found; one can then namely invoke the Fisher-Tippett-Gnedenko theorem [15, 16], which is the extreme-value analog of the central limit theorem of Sect. 6.3: if there exist sequences of constants  and  such that in the limit  we havefor a non-degenerate distribution function G, then G belongs to the family (6.11)defined on the set of points , where ,  and . Formula (6.11) defines the family of generalized extreme-value distributions (GEV). An individual distribution is described by the location parameter  (a sort of average of extreme values), the scale parameter  (their dispersion), and the shape parameter . The value of  characterizes three sub-families of the GEV set—Fréchet (), Gumbel () and Weibull ()—differing by the location of the point  and asymptotics. 
                    
                    
                  
	
                    
                    
                  
	

 The Gumbel-type distributions must be understood as the  limit of (6.11): (6.12)The corresponding probability density in the case  is (6.13)while for  it isThe predictive power of the Fisher-Tippett-Gnedenko theorem does not lag behind the one of the central limit theorem: if one is able to find suitable  and , the limiting extreme-value distribution is always of the type (6.11), regardless of the parent distribution
 that generated these extreme values in the first place! Different choices of  and  lead to GEV-type distributions with different  and , but with the same shape parameter , which is the essential parameter of the distribution.


Example

Figure 6.7 (left) shows the annual rainfall maxima, measured over 151 years (1864-2014) in the Swiss town of Engelberg [17]. Each data point represents the extreme one-day total (the wettest day in the year): we are therefore already looking at the extreme values and we are interested in their distribution, not the distribution of all non-zero daily quantities: that is most likely normal! 
                      
                      
                    
Fig. 6.7Rainfall in Engelberg (1864-2014). [Left] Time series of annual extreme values. [Right] Histogram of extremes, the corresponding probability density g (dashed curve) and the GEV distribution function (full curve). The optimal parameters ,  and  have been determined by fitting g to the histogram
Fig. 6.8Return values for extreme rainfall in Engelberg (period 1864-2014). The full curve is the model prediction with parameters from Fig. 6.7, and the dashed curve is the model with parameters obtained by the maximum likelihood method

Figure 6.7 (right) shows the histogram of 151 extreme one-day totals, normalized such that the sum over all bins is equal to one. It can therefore be directly fitted by the density (6.13) (dashed curve), resulting in the distribution parameters , ,  (Fréchet family). The corresponding distribution function is shown by the full curve.     




6.6.2 Return Values and Return Periods

	
	
                    
                    
                  

The extreme-value distribution and its asymptotic behaviour can be nicely illustrated by a return-level plot. Suppose that we have measured  daily rainfall amounts  over a period of N consecutive years, so that their annual maxima are also available:The quantiles of the annual extremes distribution are obtained by inverting (6.11):where . We call  the return level corresponding to the return period
. One may namely expect that the value  will be exceeded once every 1 / p years or that the annual maximum will exceed the value  in any year with a probability of . From these definitions it follows that (6.14)The model dependence of  on T in the case of Engelberg rainfall is shown in Fig. 6.8 by the full curve. On the abscissa one usually uses a logarithmic scale; one thereby shrinks the region of "extreme extreme" values and obtains a clearer picture of the asymptotics in terms of . We must also plot the actually measured extreme observations . In general, these are not sorted, so—in the spirit of (6.14)—individual extremes  are mapped to their return periods:The points  are denoted by circles in the figure. The maximum one-day total of , recorded in 2005, has an expected return period of 31 years, while the deluge witnessed in 1874 may reoccur every 150 years on the average.
The fitting of the probability density to the data as in the previous Example depends on the number of bins in the histogram (see Sect. 9.​2), so this is not the best way to pin down the optimal parameters. In Problem 8.​8.​3 the parameters of the GEV distribution and their uncertainties will be determined for the same data set by the method of maximum likelihood. In Fig. 6.7 (right) this distribution is shown by the dashed line.


6.6.3 Asymptotics of Minimal Values
So far we have only discussed the distributions of maximal values, most frequently occurring in practice. On the other hand, the distributions of extremely small values, i.e. the asymptotic behaviour of the quantitieswhen , are also important, in particular in modeling critical errors in systems, where the lifetime of the whole system, , is equal to the minimal lifetime of one of its components .
There is no need to derive new formulas for minimal values; we can simply use the maximal-value results. Define  for , so that small values of  correspond to large values of . Thus if  and , we also haveIn the limit  we therefore obtain (6.15)on , where . This means that a minimal-value distribution can be modeled either by directly fitting (6.15) to the observations or by using (6.11) and considering the symmetry exposed above: if, for example, we wish to model the data  by a minimal-value distribution (parameters ), this is equivalent to modeling the data  by a maximal-value distribution with the same  and , but with . 
                    
                    
                  
	
                    
                    
                  
	
                    
                    
                  
	
                    
                  




6.7 Discrete-Time Random Walks 

Random walks are non-stationary random processes used to model a variety of physical processes. A random or stochastic process is a generalization of the concept of a random variable: instead of drawing a single value, one "draws" a whole time series (signal), representing one possible realization of the random process or its sample path. The non-stationarity of the process means that its statistical properties change with time. (A detailed classification of random process can be found in [8].) In this subsection we discuss discrete-time random walks [2, 18, 19], while the next subsection is devoted to their continuous-time counterparts [18-21]. See also Chap. 12.
Imagine a discrete-time random process X, observed as a sequence of random variables . The partial sums of this sequence are (6.16)and represent a new discrete-time random process Y, i.e. a sequence of random variables . The process Y is called a random walk, whose individual step is the process X(t). Let the sample space  of X and Y be continuous. We are interested in the time evolution of the probability density  of the random variable Y if the initial density  is known.
If we assume that Y is a process in which the state of each point depends only on the state of the previous point, the time evolution of  is determined bywhere  is the conditional probability density that Y goes from value x at time  to value y at time t. Let us also assume that the process X is independent of the previous states, so that . By considering (6.16) and substituting  we getBy using this formula  can be expressed as a convolution of the initial distribution  and the distribution of the sum of steps until time t, :The time evolution  is most easily realized in Fourier space, where it is given by the product of Fourier transforms  of the probability densities,One often assumes that at time zero the value of the process Y is zero and that . This assumption is useful in particular when one is interested in the qualitative behaviour of  at long times.

6.7.1 Asymptotics
To understand the time asymptotics of the distribution  is is sufficient to study one-dimensional random walks. Assume that the steps are identically distributed, with the density , and . The distribution corresponding to the process Y is therefore determined by the formulafor all times t. The behaviour of  in the limit  is determined by the central limit theorem (Sect. 6.3) and its generalization (Sect. 6.5). The theorems tell us that at large t,  converges to the limiting (or asymptotic) distribution which can be expressed by one of the stable distributions , such thatwith suitably chosen functions L and . The function L represents the effective width of the central part of the distribution , where the bulk of the probability is concentrated, and is called the characteristic spatial scale of the distribution. The function  has the role of the distribution average. 
                    
                    
                  

Furthermore, if the distribution of steps, , has a bounded variance, , the central limit theorem tells us that  tends to the normal distribution with a standard deviation ofSuch asymptotic dependence of the spatial scale on time defines normal diffusion, and this regime of random walks is named accordingly (Fig. 6.9
	(left))

	
	
                    
                    
                  
	
                    
                  
	
                    
                  
	
                    
                  
	
                    
                    
                  
	
                    
                  
	
                    
                    
                  
	
                    
                    
                  
	
                    
                  .Fig. 6.9Dependence of the characteristic spatial scale L on time t. [Left] Discrete-time random walks. [Right] Continuous-time random walks

If the probability density  asymptotically behaves aswhere  are constants, we say that the distribution has power or fat tails, a concept familiar from Sect. 6.4. For , the second moment of the distribution no longer exists, and  at large t tends to a distribution with scaleBecause in this case the characteristic scale changes faster than in the case of normal diffusion, we are referring to super-diffusion. The dynamics of the process Y in this regime is known as Lévy flights. The diffusion with  is called ballistic: particles propagate with no restrictions with given velocities, so thatNear  we have , a regime called log-normal diffusion.
These properties can be easily generalized to multiple space dimensions. We observe the projection of the walk, , along the direction defined by the unit vector , and its probability density . For each  we apply the central limit theorem or its generalization and determine the scale . A random walk possesses a particular direction  along which the scale is largest or increases fastest with time. We may take  to be the characteristic scale of the distribution . An example of a simulation of a two-dimensional random walk where the steps in x and y directions are independent, is shown in Fig. 6.10.Fig. 6.10Examples of random walks  with  steps, generated according to  and , where X and Y are independent random variables, uniformly distributed on . [Left] . [Right] . The circles denote the initial position of the walks, 


If the densities  have power tails,  also has them. This applies regardless of the central limit theorem or its generalization. Suppose that in the limit  we have . When the walk "generates" the density , the tails add up, so  when . This means that the probability of extreme events in Y(t) increases with time, sinceTo estimate the variance of such processes we therefore apply methods of robust statistics (Sect. 7.​4). Instead of calculating the standard deviation  in sub-diffusive random walks, for example, one is better off using MAD (7.​23).



6.8 Continuous-Time Random Walks 

In continuous-time random walks [18-21] the number of steps N(t) taken until time t becomes a continuous random variable. The definition of a discrete-time random walk (6.16) should therefore be rewritten asThe expression for Y(t) can not be cast in iterative form  as in (6.16). The number of steps N(t) has a probability distribution . Suppose that N(t) and X(i) are independent processes—which is not always true, as it is not possible to take arbitrary many steps within given time [22, 23]. If X(i) at different times are independent and correspond to probability densities , the probability density of the random variable Y(t) iswhereIn the interpretation of such random walks and the choice of distribution  we follow [20]. A walk is envisioned as a sequence of steps whose lengths X(i) and waiting time
T(i) between the steps are randomly drawn. After N steps the walk makes it to the point  and the elapsed time is , so thatWithin given time, a specific point can be reached in different numbers of steps N. If the step lengths X(i) and waiting times T(i) are independent, the number of steps N(t) taken until time t is determined by the process of drawing the waiting times. Let us introduce the probability that the ith step does not occur before time t,where  is the probability density corresponding to the distribution of waiting times. The probability of making n steps within the time interval [0, t] is thenwhereThe distribution  can be calculated by using the Laplace transformation in time domain and the Fourier transformation in spatial domain: this allows one to avoid convolutions and operate with products of functions in transform spaces. The procedure, which we can not discuss in detail, leads to the Montroll-Weiss equation [20], helping us to identify four parameter regions corresponding to distributions of step lengths (density ) and waiting times (density ) with different dependencies of the scale L on time t, which determine the diffusion properties of the random walk. These regions are shown in Fig. 6.9 (right) and quantified below. We assume that the distributions of step lengths and waiting times do not change during the walk, so that  and . 
                  
                  
                

                  
                  
                

                  
                  
                

                  
                

                  
                
	

Normal diffusion with spatial scale  is obtained when , .

Sub-diffusion with scale  is obtained with ,  and distribution of waiting times
Super-diffusion with  is obtained with ,  and distribution of step lengthsWhen  and , and whenthe scale is . The walks are super-diffusive if  and sub-diffusive otherwise. Processes for which  are deeply non-Markovian: this means that the values of the process at given time depend on its whole history, not just on the immediately preceding state. Further reading can be found in [19, 21]. 
                  
                  
                
	


6.9 Problems

6.9.1 Convolutions with the Normal Distribution

	

Calculate the convolution of the normal distribution with the  uniform,  normal and  exponential distributions!

 The convolution of the uniform distribution with the density  (see (3.​1)) and the normal distribution with the density  (Definition (3.​7)) isThis function is shown in Fig. 6.11 (left).Fig. 6.11[Left] Convolution of the uniform distribution  and the standardized normal distribution N(0, 1). [Right] Convolution of the exponential distribution with parameter  and the standardized normal distribution

Part  is easily solved by using characteristic functions (B.20) and property (B.22):From this it is clear that the convolution of two normal distributions with means  and  and variances  and  is also a normal distribution, with mean  and variance .
Problem  requires us to convolute the distribution with the probability density  (see (3.​4)) and the normal distribution, where we set :Upon rearranging the exponent,it follows thatThis function is shown in Fig. 6.11 (right).


6.9.2 Spectral Line Width
	
	
                    
                    
                  

The lines in emission spectra of atoms and molecules have finite widths [24]. Line broadening has three contributions: the natural width (N), collisional broadening (C) due to inelastic collisions of radiating particles, and Doppler broadening (D). Calculate a realistic spectral line profile by convoluting these three distributions.

 The natural width of the line in spontaneous emission—usually the smallest contribution to broadening—has a Lorentz (Cauchy) profile with a width of ,As noted in the discussion of (3.​19), such a distribution embodies a Fourier transformation of the exponential time dependence of the decays into Fourier space.
The broadening due to inelastic collisions depends on pressure and temperature—approximately one has —and has a Cauchy profile as well: (6.17)A convolution of two Cauchy distributions is again a Cauchy distribution,where we have shifted the origin of  before integrating by setting  in (6.17). If we had failed to do that, the peak of the convoluted distribution  would shift from  to —see Sect. 6.1.1!
The Doppler effect is proportional to the velocity of the radiating objects, which is normally distributed (see (3.​14) for a single velocity component), hence the corresponding contribution to the line profile has the formThe final spectral line shape is calculated by the convolution of the distributions  and , where again the origin must be shifted. We obtainwhereThis is called the Voigt distribution. The natural width is usually neglected because . How well  describes an actual line shape (as compared to the Cauchy and Gaussian profiles) is shown in Fig. 6.12. 
                    
                    
                  
	Fig. 6.12Description of the Si (III) emission line at the wave-length of  (compare to Fig. 3.​6) by a Gaussian (normal), Cauchy (Lorentz) and Voigt distribution with added constant background



6.9.3 Random Structure of Polymer Molecules
	

(Adapted from [5].) A polymer molecule can be envisioned as a chain consisting of a large number of equal, rigid, thin segments of length L. Starting at the origin, a molecule grows by attaching to the current terminal point further and further segments in arbitrary directions in space.  What is the probability distribution for the position of the terminal point?  Calculate the expected distance  between the initial and terminal point of the chain and !

 When a new segment is attached to the chain, it "chooses" its orientation at random: the directional distribution is therefore isotropic, . For a projection of a single segment onto an arbitrary direction (e.g. x) we have (6.18)The X-coordinate of the terminal point of an N-segment chain is the sum of independent and identically distributed random variables  so, by the central limit theorem, it is nearly normally distributed at large N, with expected value  and variance . The corresponding probability density isThe x, y and z projections are not independent when a single segment is attached, but they are independent on average (after many attachments), so the same reasoning applies to Y and Z coordinates. Since , the probability density corresponding to the radial distribution of the terminal point of the chain is (6.19)
 This can be used to calculate the expected values of R and :The latter can also be derived by recalling (6.18), sinceThere is yet another path to the same result. Each segment () is defined by a vector . We are interested in the average square of the sum vector,Averaging the second sum yields zero due to random orientations, , hence



6.9.4 Scattering of Thermal Neutrons in Lead

	

(Adapted from [5].) A neutron moves with velocity v in lead and scatters elastically off lead nuclei. The average time between collisions is , corresponding to the mean free path . The times between consecutive collisions are mutually independent, and each scattering is isotropic.  What is the (spatial) probability distribution of neutrons at long times? Calculate the average distance  of neutrons from the origin and !  Demonstrate that  is proportional to time, so the process has the usual diffusive nature! The diffusion coefficient D is defined by the relation . How does D depend on  and v?

 Isotropic scattering implies . But we must also take into account the times between collisions or the distances l traversed by the neutron between collisions, , thuswhere . The joint probability density of the linear and angular variable, relevant to each collision, is thereforeThe expected value of the projection of the neutron trajectory between two collisions onto the x-axis and the corresponding variance areHence, as in Sect. 6.9.3,  after N scatterings, while . Therefore the probability density for the distribution of R (distance from the origin to the current collision point) has the same functional form as in (6.19),one only needs to insert the variance  instead of . It follows that
 The elapsed time after N collisions is , so that indeed  is proportional to time, . From the definition  it follows that



6.9.5 Distribution of Extreme Values of Normal Variables 

Let continuous random variables  () be normally distributed, , with the corresponding distribution function  and probability density  for each variable:What is the distribution function  of the values ? This Problem [13] is a continuation of the Example on p. 157 and represents a method to determine the parameters  and  for the scaling formula (6.10) such that the limiting distribution (6.9) is non-degenerate.

 Let  and let  be a sequence of real numbers such that (6.20)By definition of the exponential function by a series we getwhen . The leading dependence of the distribution function, , follows without explicit reference to the parent distribution  being normal! A motivation for a specific form of  can then be found in the asymptotic property of the normal distribution (6.21)Let . The reason for this choice, fully consistent with (6.20), will become clear in the following: this is the only way to obtain in the final expression a linear dependence on x in the rescaled argument of the distribution function. By comparing (6.20) to (6.21) we obtainTaking the logarithm we get  or (6.22)For fixed x in the limit  one therefore has , so that taking the logarithm again yields  orInserting this in (6.22), we get , henceand finally, after taking the square root,This expression has the formwhence we read off the normalization constants  and : (6.23)These constants imply , that is,The distribution of extreme values of normally distributed variables is therefore of the Gumbel type (6.12) with normalization constants (6.23).



References


1.
D.L. Evans, L.M. Leemis, Algorithms for computing the distributions of sums of discrete random variables. Math. Comp. Model. 40, 1429 (2004)MathSciNetCrossRefMATH


2.
W. Feller, An Introduction to Probability Theory and Its Applications, vol. 2, 2nd edn. (Wiley, New York, 1971)MATH


3.
I.S. Tyurin, On the accuracy of the Gaussian approximation. Dokl. Math. 80, 840 (2009)MathSciNetCrossRefMATH


4.
A. Lyon, Why are normal distributions normal? Brit. J. Phil. Sci. 65, 621 (2014)MathSciNetCrossRefMATH


5.
I. Kuščer, A. Kodre, Mathematik in Physik und Technik (Springer, Berlin, 1993)MATH


6.
J.P. Nolan, Stable Distributions—Models for Heavy Tailed Data (Birkhäuser, Boston, 2010)


7.
S. Borak, W. Härdle, R. Weron, Stable Distributions, SFB 649 Discussion Paper 2005-008 (Humboldt University Berlin, Berlin, 2005)


8.
S. Širca, M. Horvat, Computational Methods for Physicists (Springer, Berlin, 2012)MATH


9.
GSL (GNU Scientific Library), http://​www.​gnu.​org/​software/​gsl



10.
G.G. Márquez, One Hundred Years of Solitude (HarperCollins, New York, 2006)


11.
E.J. Gumbel, Statistics of Extremes (Columbia University Press, New York, 1958)MATH


12.
S. Coles, An Introduction to Statistical Modeling of Extreme Values (Springer, Berlin, 2001)CrossRefMATH


13.
M.R. Leadbetter, G. Lindgren, H. Rootzén, Extremes and Related Properties of Random Sequences and Processes (Springer, New York, 1983)CrossRefMATH


14.
S.I. Resnick, Extreme Values, Regular Variation, and Point Processes (Springer, New York, 1987)CrossRefMATH


15.
R.A. Fisher, L.H.C. Tippett, On the estimation of the frequency distribution of the largest or smallest member of a sample. Proc. Camb. Phil. Soc. 24, 180 (1928)ADSCrossRefMATH


16.
B.V. Gnedenko, Sur la distribution limite du terme maximum d'une série aléatoire. Ann. Math. 44, 423 (1943)MathSciNetCrossRefMATH


17.
MeteoSwiss IDAWEB, http://​gate.​meteoswiss.​ch/​idaweb/​



18.
B.D. Hughes, Random Walks and Random Environments: Vol. 1: Random Walks (Oxford University Press, New York, 1995)MATH


19.
M. Bazant, Random walks and diffusion, MIT OpenCourseWare, Course 18.366, http://​ocw.​mit.​edu/​courses/​mathematics/​



20.
E.W. Montroll, G.H. Weiss, Random walks on lattices II. J. Math. Phys. 6, 167 (1965)ADSMathSciNetCrossRef


21.
R. Metzler, J. Klafter, The random walk's guide to anomalous diffusion: a fractional dynamics approach. Phys. Rep. 339, 1 (2000)ADSMathSciNetCrossRefMATH


22.
M.F. Shlesinger, B.J. West, J. Klafter, Lévy dynamics of enhanced diffusion: application to turbulence. Phys. Rev. Lett. 58, 1100 (1987)ADSMathSciNetCrossRef


23.
V. Tejedor, R. Metzler, Anomalous diffusion in correlated continuous time random walks. J. Phys. A: Math. Theor. 43, 082002 (2010)ADSMathSciNetCrossRefMATH


24.
J.J. Brehm, W.J. Mullin, Introduction to the Structure of Matter (Wiley, New York, 1989)











Part IIDetermination of Distribution Parameters










© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_7




7. Statistical Inference from Samples




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
Any kind of empirical determination of probability distributions and their parameters amounts to statistical inference procedures based on samples randomly drawn from a population. The concepts of the statistic and the estimator are introduced, paying attention to their consistency and bias. Sample mean and sample variance are defined, and three most relevant sample distributions are investigated: distribution of sums and differences, distribution of variances, and distribution of variance ratios. Confidence intervals for the sample mean and sample variance are discussed. The problem of outliers is elucidated in the context of robust measures, and linear (Pearson) and non-parametric (Spearman) correlations are presented.



Chapters 7-10 are devoted to basics of statistics. The main task of statistics is the empirical determination of probability distributions and their
	
	
                
               parameters.
We start by introducing the concepts of population and sample. A population is a finite or infinite set of elements from which we acquire samples. By using statistical methods we strive to determine the properties of the entire population by analyzing only its sample, even though the sample may be much smaller than the population. If a quantity represented by a random variable X is measured (counted, realized, recorded) n-times, we obtain a set of values , burdened with some error or uncertainty. Part of this uncertainty has random (statistical) nature: the values  are scattered because, in general, a sample contains new elements each time it is acquired. This part of the uncertainty can be reduced by increasing the sample size. The other part of the uncertainty has a systematic origin and can not be removed by augmenting the sample.

Example
From a population of  we acquire a sample of  people and measure their heights. We would like to use the measured n values to determine the average height and its variance, and offer some kind of a statement on what these numbers mean in the context of the whole population. If we measure the height of 1000 randomly selected people today and 1000 randomly selected people tomorrow, we shall in general obtain two different averages and variances (statistical uncertainty). If we use a faulty instrument that constantly gives a height  too short, we will obtain wrong heights regardless of the sample size and regardless of whether sampling is repeated multiple times (systematic uncertainty). 


A population can be finite or infinite (). Tossing a coin many times, for example, yields an estimate of the probability of observing head or tail (which will be approximately 1/2, see Sect. 1.​3), but in this case the population consists of the set of all possible tosses, which is infinite.

7.1 Statistics and Estimators

Our


                  
                

                  
                

                  
                  
                

	
	 vantage point is the random sample  from a population characterized by an unknown parameter . We would like to estimate this parameter from the sample. The function (7.1)of random variables  with values , used to obtain the estimate for , is called the estimator of parameter . Conventionally we use the same notation for the estimator as a prescription for the variables, e.g. , as for its concrete value or estimate, e.g. . Any function of the form (7.1) is called a sample statistic, while probability distributions of such statistics are called 
	sample

	
	
                    
                    
                  
	
                    
                    
                   distributions.
Of course, the functional form of a statistic is not arbitrary: above all, one wishes to devise the statistic  so that it is consistent. This means that the estimate  converges to the true value  when the number of observations n is increased: if a sample of size n results in an estimate , then for any positive  and  there should exist m such that  holds true for each . This is approximately equivalent to the statement that the variance of an estimator goes to zero for infinite samples: (7.2)Moreover, it is usually desirable that the estimator is unbiased. This means that for samples of arbitrary size n, not only infinite ones, the expected value of  is equal to the true parameter, (7.3)If, on the contrary, (7.4)where , we say that the estimator is biased. For sensible estimators one expects  and, say,  when .

7.1.1 Sample Mean and Sample Variance
The crucial parameters of interest for any distribution of a random variable are its mean and variance. Their values inferred from a given sample are generally different from their values for the whole

	
	
                    
                    
                  
	
                    
                    
                  
	
                    
                    
                   population.
Suppose we acquire a sample of n values from a population so that any value can occur multiple times. In the case of body heights this means that a person is chosen at random, her height is measured—this is the value of X—and "returned" to the population, whence she can be randomly "drawn" again. We say that the sample has been obtained by replacement: if , we should not have any second thoughts about that. If all values in the sample have equal weights, the estimator (7.5)gives the sample mean of body heights according to (4.​1). Note that the line above the symbol now represents the sample mean, while in previous sections it has been used as an alternative notation for the expected value. In the following we use the notationwhile expected values will be strictly denoted by . (As usual, lower-case letters imply concrete values of the corresponding statistics upon each sampling.) Our estimate for the unknown population mean
, which we wish to infer based on the concrete sample , is thereforeThe estimator (7.5) is clearly unbiased since, according to (7.3), its expected value is equal to the population mean, (7.6)It is also consistent, since its variance approaches zero when : (7.7)This looks nice, but just as we do not know the true population mean , the true population variance  is also unknown. At best, we can resort to (4.​14) to devise a formula for the sample variance, (7.8)which undoubtedly is a kind of estimator for , but is it unbiased? If it were unbiased, its expected value should be equal to the population variance,Does this hold true? Let us focus on the first term in the sum and write it asthen square it,The variables  and  () are mutually independent, so  and the mixed terms do not contribute to the expected value:The sum (7.8) contains n such terms, and there is a factor 1/n up front, thus (7.9)Therefore  is a biased estimator for the population variance: an unbiased estimator is obtained if the right-hand side of (7.8) is multiplied by , (7.10)Sometimes both (7.8) and (7.10) are invoked as formulas for sample variance, although by analogy to definitions (4.​14) and (4.​15) only the first form is correct. With respect to bias, the formulas are not equivalent, except for  when the difference is negligible. An illustration of two samples with different means  and almost equal variances  is in Fig. 7.1. See also Problem 7.6.1.Fig. 7.1Samples of size  with different means and almost equal effective deviations ,  (left) and ,  (right), taken from a population with mean  and effective deviation . Full circles with error bars denote the sample means  and their uncertainties 


When formula (7.7) is applied to the estimator (7.10), one obtains the estimator of the variance of the sample mean
Taking the square root yields the uncertainty or "error" of the mean
This result offers an important lesson whose importance can not be over-emphasized. Each sample from a population has a different sample mean and different sample variance. The observations will usually be scattered by approximately  (shaded areas in Fig. 7.1), but that does not mean that the average will also be scattered by —its uncertainty will only be  ! We therefore simply write (7.11)which is about the same. The expression on the left implies that the sample average approximately equals the true (population) average, and its uncertainty is . The formula on the right says that  is a good approximation for , the error is, perhaps, . Regardless of the interpretation these formulas dictate the sample size needed for the desired precision: if, for example, we wish to determine  to a precision of , we need  observations.
Now let us assume that individual  are normally distributed. How can we estimate the scattering of the sample variance  about the population variance ? At large n, where the distinction between formulas (7.8) and (7.10) is immaterial, this "variance of variance" equalsSquaring the expression in square brackets first yields n quartic terms of the form , , where . The excess of a normally distributed continuous variable is zero, thus by (4.​19) its fourth central moment is , contributing  to the final expression. Secondly, with the approximation  the factors in the products , , are independent, resulting in  terms of the formTheir total contribution is . Thirdly, we are left with n mixed terms of the form  and the lone , thus at lastWe have obtainedwhich in the case  impliesTo determine  to a precision of 1, one therefore needs  observations.

Example
(Adapted from [1].) The complete population consists of  values . The mean and variance of its elements arethus . From this population we draw all possible samples of size  with replacement. There are  such samples:For each of these samples one can compute 25 sample means, (2.0, 2.5, 4.0, 5.0, 6.5, 2.5, 3.0, , 11.0), denoted by  (). Their expected value—i.e. the mean of the sample distribution of means—iswhich is nothing but the true population mean, as expected according to (7.6). The variance of the sample distribution of means iswhich we also obtain from (7.7): (7.12)or .
If samples of size  are drawn without replacement, one can form only  such samples:The sample means  are now . The expected value of the sample distribution of means is still , while the variance of the sample distribution of means is . We have obtained a different result as in (7.12) because formula (7.7) is not applicable in the case of sampling without replacement. Instead, one should useThen indeedWe have used small N and n to convey the general idea, otherwise a set of five elements could hardly be identified with a "large" population () suitable for "proper" statistical analysis. 





7.2 Three Important Sample Distributions

7.2.1 Sample Distribution of Sums and


	 Differences
Suppose we are dealing with two infinite populations with means  and  and variances  and . We draw a sample of size  from the first population and a sample of size  from the second, and compute the sample means  and . Referring to our previous findings—see in particular (4.​6), (4.​20) and (7.7)—we can write (7.13)and (7.14)If , the random variable (7.15)is distributed according to the standardized normal distribution to a very good approximation. The statement remains valid in the case of drawing with replacement from finite populations, as one can, in principle, draw an infinite sample from a population to which elements are restored after being drawn.

Example
The lifetime of a circuit of type A is normally distributed, with mean  and standard deviation . The circuits of type B have the mean lifetime  with the standard deviation . We test  of type-A circuits and  of type-B circuits. What is the probability that A circuits will operate a year longer than B circuits?
By using (7.13) and (7.14) we obtainWe are interested in the probability that the difference of mean lifetimes is larger than one year, . The corresponding standardized variable (7.15) for the limit value  isand is normally distributed. So the probability we want iswhere we have used Table D.1. 




7.2.2 Sample Distribution of


	 Variances
Sample distributions of variances are obtained when one acquires all possible random samples of size n from the population and calculates the variance of each sample. From the population variance, , and the sample variance  (in biased form (7.8)) we construct the random variable (7.16)If random samples of size n are drawn from a normally distributed population, the statistic (7.16) is distributed according to the  distribution (3.​21) with  degrees of freedom.

Example
Let us revisit the Example on p. 182, where we have drawn  samples of size  from a population of  elements. What are the mean and the variance of the corresponding sample variances, and what is the expected number of samples whose variance exceeds 7.15?
We first compute k () sample variances  for each of the 25 samples. We obtain the variances (7.17)The mean of this sample distribution of variances isThis has been expected by (7.9), since . The scattering of the sample variances is calculated by the usual formula:The random variable  is given by (7.16). With the population variance, , and the prescribed sample variance, ,  takes the valuewhere the subscript 1 indicates that the variable is -distributed (see (3.​21)) with  degree of freedom. The probability that the sample variance  exceeds the prescribed variance , is therefore equal to the probability that the value of  according to this distribution is larger than the critical value . From the first line of Table D.3 (column for  or ) and Fig. 7.2 (right) we find this probability to beThere are  samples,  of which are expected to have a variance exceeding the prescribed one. In our Example there are indeed six: they correspond to the underlined elements in (7.17). 



Fig. 7.2The probability density of the  distribution for [Left]  and [Right]  degrees of freedom. The dashed vertical line indicates the distribution mean



7.2.3 Sample Distribution of Variance


	 Ratios
From two normally distributed populations with variances  and  we draw two independent samples (one from each population) of sizes  and , respectively. Let the sample variances (in biased form (7.8)) be  and . Then the statisticis distributed according to the F distribution (defined in (3.​23)) with  degrees of freedom.

Example
From two normally distributed populations with variances  and  we draw two samples of size  and  from the first and second population, respectively. What is the probability that the variance of the first sample is at least twice the variance of the second sample? (A very small probability may be anticipated, as the variance of the first population is roughly twice smaller than the variance of the second, thus it seems highly unlikely that the situation would be nearly opposite in the samples from these populations.) We calculate the statisticThe Problem statement requires  orThe F statistic is distributed according to the F distribution (3.​23) with  degrees of freedom (numerator) and  (denominator). From Tables D.5 and D.6 we read off the 95 and 99 quantiles  and , implying that the sought-after probability (that  and ) is larger than 1 and smaller than 5 (Fig. 7.3 (left)). For a more precise answer, we integrate the density up to the specified bound (see Appendix D.1):
Fig. 7.3Probability density of the F distribution for [Left] ,  and [Right] ,  degrees of freedom

What is in the numerator and what is in the denominator of the F ratio is irrelevant, one just needs consistent book-keeping. Let us switch the roles of the populations, so that , ,  and . Seeking the probability that the sample variances satisfy the inequality  now meansAs shown in Fig. 7.3 (right), this probability is also obtained by integrating the density of the F distribution, but with its degrees of freedom swapped:which is the same as before. 





7.3 
Confidence
	
 Intervals
Next to (7.11) we wrote: "The error is, perhaps, ." What exactly does that mean? The full circles in Fig. 7.1 denoting the sample averages are displaced by more than their uncertainty, i.e. by more than  ! Obviously we need a more quantitative measure for "perhaps". It is offered by a criterion called the confidence interval.

7.3.1 Confidence Interval for Sample


	 Mean
Let  and  be the mean and variance of the sample distribution of some statistic T, e.g.  or . If the sample statistic is approximately normal—which applies to many statistics if the sample size is at least a few times 10—we expect that the value of T will be on the interval  approximately 68.3 of the time, on  about 95.5 of the time, on  roughly 99.7 of the time, and so on (see (3.​13)). We say: with confidence level (CL) 68.3 we may be confident (we trust, believe, anticipate), that T will be found on the interval , and analogously for the others. Such an interval is called the confidence interval.
Suppose we have a sample  or n independent observations for which we have already determined the sample mean  and variance  in unbiased form (7.10). To determine how well  estimates the true population mean, , we first form the statistic (7.18)If  are normally distributed as , the T statistic is distributed according to the Student distribution (3.​22) with  degrees of freedom. The integral of the density  determines the boundaries of the interval  on which the values of t or the corresponding mean  are expected with the pre-specified probability (confidence level) , while there is a probability (risk level)  that t will be outside of it: see

	
	
                    
                   Fig. 7.4.Fig. 7.4The relation between the confidence level  and critical values  for the determination of the confidence interval  for the sample mean. (Example with .)

Thenor (7.19)meaning: "The true mean of a large population, from which a sample  has been obtained, is estimated as , and the confidence interval (7.19) contains  with probability ." For large samples () the Student distribution is practically identical to the standardized normal, and the corresponding bounds  are simply the bounds in the Gauss curve (Table 7.1). Understandably,  increases with increasing confidence level: a broader interval implies less "risk".Table 7.1Critical values  for the Student distribution with , 20 and 30 degrees of freedom for a few commonly used confidence levels 


50
68.26
90
95
95.45
99
99.73

 ()0.7001.0531.8122.2282.2903.1693.892
 ()0.6871.0261.7242.0862.1392.8453.376
 ()0.6831.0181.6972.0422.0922.7503.230

0.6751.0001.6451.9602.0002.5763.000The last line contains the critical values for the normal distribution, 



Example
An eleven-fold () measurement of a particle's mass yielded a mean of  and an unbiased estimate for the standard deviation . What is the confidence interval on which the true mass of the particle  may be expected with a confidence level of ?
If the observations are normally distributed, the variable  is Student-distributed, with  degrees of freedom. Table 7.1, first row, , gives the critical value , shown also in Fig. 7.4. The requested confidence interval for  is thereforeBecause the Student distribution is symmetric about the origin, the equationdefines the same  as the equationso  can also be determined by using the table of quantiles of the Student distribution. For purposes of this Problem () we need the 95. quantile, located in the tenth row of Table D.4 in the  column, whence we again read off .  


Example
The closing time x of safety valves is measured by an imprecise device reporting values with a standard deviation  which we know is near the value of . How many valves should we test, at confidence level 
, in order to determine the mean closing time  to a precision of ?
Let us assume that a large sample () will be required, so that the Student distribution can be replaced by the normal (i.e. Student in the  limit). From the last row of Table D.4, in the  column, we get , corresponding to the confidence interval , hence the population mean is determined to a precision given byThe problem is asking for , whenceWe see that the normal approximation is justified.  



7.3.2 Confidence Interval for Sample


	 Variance
From Sect. 7.2.2 (see (7.16)) we know that the variable  is -distributed, with  degrees of freedom, so we can immediately write down the confidence interval for the sample variance. Take a confidence level of , for example, so that the critical values of  are  and . So the variable  is bounded as . The population effective deviation  can therefore be bounded by the sample effective deviation  as (7.20)Figure 7.2 (right) shows the  case, with critical values  and  (eighth row of Table D.3). Note that the bounds are not symmetric with respect to the distribution average! See also Problem 7.6.4.


7.3.3 Confidence Region for Sample Mean and

	 Variance
Suppose we were to use the sample  to simultaneously locate, with chosen confidence level (probability ) both the true mean 
and the true variance . By doing this, we would identify something we call a confidence region. If the population is normally distributed like , the variables  and  are independent, which can be proven by using characteristic functions. A confidence region at  is then obtained by simultaneous requirementsandwhere  are the symmetric bounds in the density of the t distribution, while  and  are the lower and upper bounds in the density of the  distribution. The confidence region is then defined by the equation , that is, (7.21)An example of such a region is shown in Fig. 7.5. (We could have chosen a different sharing of  between the mean and the variance; the shaded area would be correspondingly narrower and taller or wider and shorter.)Fig. 7.5Joint confidence region for sample mean and variance of a normally distributed population, defined by parameters ,  and  in (7.21)




7.4 Outliers and Robust Measures of Mean and


                  
                  
                 Variance
Occasionally a sample contains values which obviously differ from the bulk of the sample. They are called outliers. Outliers may hint at an error in the experiment or may represent genuine measurements that happen to strongly deviate from the majority of observations. The ozone hole over Antarctica, for example, has been indicated by peculiar recordings by the Nimbus 7 satellite, but they were wrongly attributed to instrumental errors
 [2].
To determine the parameters that characterize the samples with relatively small shares of outliers, we use so-called robust measures and robust statistics [3]. Among other things, "robustness" implies a small sensitivity of estimates of mean and variance to the inclusion or exclusion of individual or all outliers from the sample.

Example
The classical motivational case for the application of robust methods is the set of 24 measurements of copper content in bread flour [4]:shown in Fig. 7.6. The arithmetic average of the whole sample is  and the standard deviation is . If the value  is excluded from the sample, we get  and . Clearly a single outlier may strongly modify both  and , so neither  nor  are suitable as robust estimators of population properties. 



Fig. 7.6A sample (24 values) of copper content in flour, in units of . The value  (and potentially also ) is an outlier. The median is much less sensitive to the exclusion of the rightmost outlier  than the arithmetic average

A much more robust measure of the "center" of the sample  is the median.
	
	 It is defined as the value (see (4.​4)) which splits an ordered sample () such that half of the observations are to its left and half are to its right, (7.22)The scattering of the data about the median can be quantified by the 


                  
                  
                  median absolute deviation
                 (MAD) (7.23)where  is a sequence of ones with length n. To be able to compare this to the standard deviation, one frequently uses the quantitywhere the factor 1.4826 is chosen such that for a normal  distribution one has . The values for the whole flour sample are  and , while if  is excluded, one gets 3.37 and 0.504. Both values change only insignificantly when the outlier is excluded (see Fig. 7.6).

7.4.1 
	Chasing
	
	
                    
                   Outliers
A straightforward way to exclude outliers is the ""-rule. By following it we may decide to eliminate all observations deviating from the sample mean  by more than , or simply assign them the values . The method has several flaws. Among others, it forces us to needlessly remove, on average, three observations from an immaculate, normally distributed sample of size , since the interval  for large n contains 99.7 of the data. Besides, the calculation of the mean and variance itself is highly sensitive to outliers. It is therefore preferable to use the
	
	
                    
                   criterionA simple method to visually identify candidates for outliers is to draw a box diagram. One first calculates the sample median and, by a generalization of (7.22), its first and third quartile,  and : the sample is then divided into four compartments with a quarter of observations in each. One then calculates the inter-quartile range () and the bounds  and , beyond which outliers are expected to appear,This method identifies the values  and  in the flour sample as outliers (see Fig. 7.7). The interval  for large n and normal distribution contains 99.3 of observations, making the method roughly equivalent to the ""-rule.Fig. 7.7Box diagram used to identify outlier candidates. Outliers may be expected outside of the interval 




7.4.2 Distribution of Sample Median (and Sample

	 Quantiles)
There is a theorem on the distribution of sample quantiles, whose special case is the median. Let X be a continuous random variable with the probability density  and distribution function . Let  denote the pth quantile of X, so that , and  the sample quantile determined from the sample . In the limit of large samples () it holds that [5]Hence the sample median () is asymptotically normally distributed with mean  and variance . The variance depends on the density ! If the population is normally distributed according to , we have 
	and


	. Therefore (7.24)




7.5 
Sample
	
 Correlation
In this section we introduce measures of correlation between data sets. The correlation strength is measured by correlation coefficients. Suitable statistics are used to determine whether observed correlations are statistically significant.

7.5.1 Linear (Pearson)

	
	
                    
                    
                  
	
                    
                    
                   Correlation
The basic measure for the strength of correlation between two data sets is the linear correlation coefficient . Correlations in two-dimensional data sets can often be simply "seen": characteristic patterns in the  plane for correlation coefficients  (almost complete positive correlation),  (nearly total negative correlation or anti-correlation) and  (roughly uncorrelated observations) are shown in Fig. 7.8.Fig. 7.8Sets of observations  that one can describe by a two-dimensional distribution of X and Y, and corresponding estimates of the sample correlation coefficient . [Top, right to left] Almost completely correlated (), uncorrelated () and nearly anti-correlated data (). [Bottom] Three cases of realizations of uncorrelated variables which are not statistically independent. The requirement  is just a necessary condition for statistical independence

The linear correlation coefficient between the data sets  and  (the estimate of the true 
	)

	 is (7.25)The coefficient (7.25) is suitable to estimate the true correlation strength once we have figured out that a correlation exists and has a certain statistical significance. As in the case of the sample mean and average, the sample correlation coefficient  can also be endowed with confidence intervals. To do this, one uses the Fisher transformation of , namely, the

	
	
                    
                   statisticand assume that the observations  and  obey the joint binormal (two-dimensional normal) distribution. For sample sizes n of at least a few times 10 the Z statistic is approximately normally distributed, i.e.where  is the true correlation coefficient. (It turns out that correlation coefficients  of samples drawn from normally distributed populations are smaller than their population counterparts , hence biased. The  helps to approximately cancel that bias.) The best estimate for  is then , while the risk level (significance) at which one may claim that the measured  differs from , is given byTo determine whether the observations of  and  under conditions "1" and "2" exhibit different correlations, one compares the correlation coefficients  and . The statistical significance of the observed difference between  and  isThe reverse question is: to what confidence interval  can the correlation coefficient be restricted, given a confidence level of ? For the commonly used 
 the values  and  are given


	 by



7.5.2 Non-parametric (Spearman)
	
	
                    
                    
                  
	
                    
                   Correlation
The linear correlation coefficient formula (7.25) contains the sample means  and , which are strongly sensitive to outliers (see Sect. 7.4). A more robust tool is called for, and one option is to define the correlation by referring to the positions (ranks)  and  that individual  and  occupy in the ordered samples  and . The ranks are counted from 1 upwards. When several (e.g. m) equal values share m positions, they are all assigned the average rank which they would have if they differed by an infinitesimal amount. One also computes the average ranks 
	and


	.

Example
Let us determine the rank of the sample . We first order the sample, obtaining . The values  share ranks 2 to 4, so their average rank is . The values  share ranks 7 and 8, so their rank is 7.5. Therefore  and the average rank is . 


By using the ranks  and  as well as the average ranks  and  we define the

rank correlation coefficient
 (7.26)To calculate  one refers only to the mutual position of the observations, so this kind of correlation estimation is called non-parametric. The distribution of ranked observations is uniform, and if there are just a few duplicates, the estimate (7.26) is much more robust than (7.25). The statistical significance of the measured coefficient  is determined by the t-test (details in Chap. 10). We form the statisticwhich is distributed approximately according to the Student distribution with  degrees of freedom. The confidence level  (statistical significance ), at which one can reject the hypothesis that the measured correlation coefficient  equals the true coefficient , is calculated fromwhere  is the probability density of the t distribution (see (3.​22)), while B(a, b) and  are the complete and incomplete beta functions.



7.6 Problems

7.6.1 Estimator of Third

	 Moment
Find an unbiased estimator of the third distribution moment, , based on the sample ! This Problem offers a parallel to the biased and unbiased estimators of the population variance (formulas (7.8) and (7.10)).

 We splitand calculate the expected values term by term:ThereforeThe weighted sum at the left—surely the first form to cross one's mind—obviously results in a biased estimator. An unbiased estimator of the third moment is



7.6.2 Unbiasedness of Poisson Variable

	 Estimators
In this example [6] we realize that unbiasedness is not the holy grail to which one should strive at all costs. (See also [7].) Let the variable X be Poisson-distributed, with parameter , thus . Find an unbiased estimator for !

 Any unbiased estimator T(X) for the given quantity must satisfy the equationBut the only option is , since then . What does that mean? If, for example, a single observation yields , we can reasonably conclude that the true  is virtually zero, while the unbiasedness requirement forces us to accept the value  as the estimate of . On the other hand, if, for instance, we observe , the unbiased estimate is supposed to be , which is a negative value for a random quantity whose values are always on (0, 1]. A better estimator for  is certainly , even though it is biased.


7.6.3 Concentration of Mercury in Fish
(Adapted from [8]). The following average concentrations of mercury ( per  of body weight) in fish from  Florida lakes are available:The histogram of the observations is shown in Fig. 7.9 (left). Based on this sample, find the 95 confidence interval for the average concentration  in the whole fish population!Fig. 7.9[Left] Average mercury concentrations in fish from  Florida lakes. [Right] Graph of standardized variables  as functions of sorted observations , the so called "Q-Q plot" showing the quantiles of two distributions on the respective axes



	Does perhaps the sample itself indicate "normality"? A good tool to answer this question is a graph containing the ordered observations  on the abscissa and the variable  corresponding to the th quantile  of the standardized normal distribution on the ordinate axis. How is the graph constructed? When observations are sorted, the smallest observation is , hence  and , where  is the distribution function of the standardized normal distribution. Then . The pair  is the point at the extreme bottom left in Fig. 7.9 (right). The remaining points , ,  are calculated in the same manner: we plotIf the sample is normally distributed, the  vs.  graph is a straight line.
In our case the sample exhibits a distribution which does not appear to be normal: its distribution function increases faster than the normal distribution function at small , and slower at large . This indicates that the underlying distribution is positively skewed (see Fig. 4.​4), which one can also infer from the histogram in Fig. 7.9 (left).
The confidence interval for the population mean could be calculated by (7.19), but it only applies to normally distributed . However, due to the central limit theorem the mean  is approximately normally distributed at large n
regardless of the distribution of
, with mean  and variance . In our case , so (7.19) may be used nonetheless. From the sample we compute  and , then use the last row of Table 7.1 at 
 to obtain the critical . Therefore,  can be bounded aswhich amounts to .


7.6.4 Dosage of Active
	 Ingredient
The mass of the active pharmaceutical ingredient in pills is distributed about the known average value: in a sample of  pills taken for analysis we find a variance of  (). Find the 80 confidence interval for the true (population) standard deviation of the active ingredient mass (a 10 chance of it being too small and a 10 chance of it being too large)!

 The confidence interval for the population variance is given by formula (7.20). From Table D.3 for  we read off the critical values of the  distribution,  and , so  can be bounded asor . Note that  does not lie in the middle of this interval, as we already know from Sect. 7.3.2 (Fig. 7.2).



References


1.
M.R. Spiegel, J. Schiller, R.A. Srinivasan, Theory and Problems of Probability and Statistics, 4th edn. (McGraw-Hill, New York, 2012)


2.
R. Kandel, Our Changing Climate (McGraw-Hill, New York, 1991)


3.
L. Davies, U. Gather, Robust statistics, Chap. III.9, in Handbook of computational statistics. Concepts and methods, ed. by J.E. Gentle, W. Härdle, Y. Mori (Springer, Berlin, 2004), pp. 655-695


4.
Analytical Methods Committee. Robust statistics - how not to reject outliers, Part 1: basic concepts. Analyst 114, 1693 (1989), Part 2: Inter-laboratory trials. Analyst 114, 1699 (1989)


5.
A.M. Walker, A note on the asymptotic distribution of sample quantiles. J. R. Stat. Soc. B 30, 570 (1968)MathSciNetMATH


6.
J.P. Romano, A.F. Siegel, Counterexamples in Probability and Statistics (Wadsworth & Brooks/Cole, Monterey, 1986)MATH


7.
M. Hardy, An illuminating counterexample. Am. Math. Mon. 110, 234 (2003)MathSciNetCrossRefMATH


8.
T.R. Lange, H.E. Royals, L.L. Connor, Influence of water chemistry on mercury concentration in largemouth bass from Florida lakes. Trans. Am. Fish. Soc. 122, 74 (1993)CrossRef














© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_8




8. Maximum-Likelihood Method




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
The maximum-likelihood method offers a possibility to devise estimators of unknown population parameters by circumventing the calculation of expected values like average, variance and higher moments. The likelihood function is defined and its role in formulating the principle of maximum likelihood is elucidated. The variance and efficiency of maximum-likelihood estimators is discussed, in particular in the light of its information content and possible minimum variance bound. Likelihood intervals are introduced by analogy to the confidence intervals used in standard sample-based inference. The method is extended to the case when several parameters are determined simultaneously, and to likelihood regions as generalizations of likelihood intervals.



In this chapter
        
       we discuss the possibility to devise an estimator for the unknown population parameter  without resorting to the calculation of expected values like average, variance
        
      

         and higher moments (Chap. 7).

8.1 Likelihood Function
When a continuous or discrete random variable X is measured n-times (or a sample of size n is drawn from an infinite population) one obtains a set of values . Assume that X is distributed according to the probability density or probability function , where (8.1)are unknown parameters. Let us consider continuous variables only; the discussion of discrete variables follows the same pattern. The probability that, at given parameters , just the values  on intervals  have been observed, is . The product of probability densities in this expression is called the likelihood function:where the | sign is a warning that a joint conditional density (or probability function) is implied, since the values  have been observed given the "condition" . Note that the likelihood function depends on the sample and is therefore itself a random variable. We also define its logarithm, the log-likelihood function
There are two good reasons for taking the log. Multiplying many  may result in floating-point underflow, which is avoided by turning the product into a sum. In addition, having a sum is convenient as we shall be taking the derivative of the likelihood function with respect to the parameters . Note also that the log is a monotonously increasing function with a singularity at the origin: this may be a source of numerical problems in seeking the global maximum of .


8.2 Principle of Maximum Likelihood
          
        

The principle of maximum likelihood states that the optimal value of the parameter  is found by maximizing the likelihood function (or its logarithm) with respect to : such a measurement of  is then seen to be "most likely". We therefore wish to find such  that for all possible  it holds that . Assume that the function  is twice differentiable with respect to . The value  is obtained by setting its first derivative to zero, (8.2)where  denotes the derivative with respect to . This formula is known as the likelihood equation. The condition
          
         that we have indeed found the maximum, is (8.3)Should we wish to determine several parameters (8.1) simultaneously, the likelihood equation needs to be solved for each parameter separately, (8.4)By analogy to (8.3) we also identify the sufficient condition that the absolute maximum of  has been found: the square matrix A with the elements (8.5)must be positive definite.

Example
We have been measuring the same quantity with several devices with different uncertainties . The observations  are scattered about the true value . Suppose that the fluctuations about the mean are normally distributed. The probability that, given the value of parameter , the observation  is on the interval , is thenThe corresponding likelihood function isand its logarithm is (8.6)By solving the likelihood equationwe obtain the estimate  for the parameter : (8.7)which is the familiar formula for the weighted average of  with normally distributed errors . The second derivative of  with respect to  reveals that  indeed corresponds to the maximum , since .    



Example
Six vertical lines in Fig. 8.1 represent the observations , assumed to originate in a Cauchy-distributed population with the width parameter  and unknown mean  (see (3.​20)). What is the maximum-likelihood estimate for the mean, , based on this sample?Fig. 8.1Sample of six values from a Cauchy-distributed population with the width parameter  and unknown mean . (The sample was in fact generated by a Cauchy random generator with  and .) Also shown is the log-likelihood function with four local maxima () and the global maximum () at 


The log-likelihood function for a sample of size n isand the likelihood equation isThis can be written as , where p is a polynomial of degree . Thus, in general, the likelihood equation has  solutions, some of which correspond to local maxima of . The optimal  corresponds to the global maximum, which one usually finds numerically and tends to be near the sample median. In our case there are four local maxima and a global maximum at .    




8.3 Variance of Estimator
Consistency and unbiasedness (see (7.​2)-(7.​4)) are not the only desirable properties of a statistical estimator. One would also like it to have as small a variance as possible. In general, different estimators of the same quantity have different variances: for example, both the sample mean and the sample median are consistent and unbiased estimators of the "center" of a population with known variance. Yet—as we shall see—the variance of the mean is smaller than the variance of the median, so the sample median is a "better" estimator.

8.3.1 Limit of Large Samples
For large samples () one expects the estimate  to be not very different from the true value of the parameter . We may therefore divide the likelihood equation (8.2) by n, expand it in a Taylor series and keep just the first two terms:or, briefly, (8.8)The quantities  and  have a random nature, as they depend on the current sample . What are their expected values? For large n the sums can be replaced by integrals: the role of the sum weights 1 / n are taken over by the probability densities. Then one can writewhere we have considered the fact that the probability density  is normalized regardless of the value of its parameter, , and the derivative of a constant is zero. We play the same game with :The first term vanishes for the same reason as in , while the second term is the (negative) expected value of the quantity , so (8.9)for all non-degenerate cases. Then we see from (8.8) that  at large n approaches the true value , since . Therefore, the estimate for the parameter  isWhat is the variance of this estimate? We compute it as the expected valueThe denominator has already been calculated in (8.9), and the numerator isIndividual random variables  are mutually independent, so all mixed terms may be discarded, only the n quadratic terms survive:So the variance we are looking for isDenoting  and taking into account the mutual independence of individual , the following relations also hold true:The variance can thus be computed in at least four equivalent ways; two require the log-likelihood function, and two require the probability density: (8.10)


Example
Let us determine the parameter a in the Pareto distribution (3.​16) based on the measured sample  and the assumption that the other parameter, b, is known. The likelihood function isand its logarithm isThe likelihood equation  yields , whence the estimate (8.11)Formula (8.10) then gives its variance:hence .    





8.4 Efficiency of Estimator
There is a relation between the bias and the variance of an estimator, based on the information contained in the sample. (The concept of information will be discussed in detail in Chap. 11.) The equivalent
          
         quantities in (8.10) can be interpreted as the information of the sample with respect to parameter ,One can prove that the variance of estimates, obtained with a specific estimator, is bounded from below [1, 2]. The lower bound of the variance of estimator T with bias b is given
          
        

           by the Cramér-Rao or information inequality
If b does not depend on —or if the estimator is unbiased ()—the minimum variance bound is given by the inequality
          
        

                  
                  
                
 (8.12)with a very clear interpretation: by increasing the information (sample size) it is possible to reduce the variance of the estimate obtained from the sample with estimator T. It can be shown [3] that the lower bound is attained precisely when (8.13)Here A is an arbitrary quantity independent of , but it may depend on the parameter . Integrating the above relation we get , whence, by inverting the log, (8.14)where D and d do not depend on . Hence T is a minimum variance estimator if the likelihood function L has the particular form (8.14). If, in addition, such an estimator is unbiased, it also follows from (8.12) thator (8.15)The quality of an estimator is expressed by its efficiency, defined as the ratio of the minimal and actual variance of the estimator
          
        

                  
                  
                ,In principle high efficiency is desirable, although it does not say much about other qualities of an estimator, for example, its robustness.

Example
We have acquired an integer sample , assumed to stem from a Poisson-distributed population, corresponding to the probability function  and unknown parameter . We wish to determine this parameter. The log-likelihood function isBy comparing its derivative
            
          
and formula (8.13) we see that the arithmetic mean  is an unbiased estimator for the parameter  with variance . How this works in practice is shown in Fig. 8.2.    



Fig. 8.2Samples of size  taken from a Poisson-distributed population with parameter . [Left] Sample with arithmetic mean . [Right] Sample with . The expected variance of the sample mean is  and the effective deviation is , consistent with the values shown


Example
Let us examine the variances of the sample mean  and the sample median (definition (7.​22)) as two possible estimators for the true population mean , assuming that the population is normally distributed, with known variance . The derivative of the log-likelihood function with n observations isThis has the form (8.13), where , ,  and . By (8.15) thereforeWe know this already, for example, from (7.​7). What
             about the median? The sample median of large samples () is normally distributed according to (7.​24), soTherefore the sample median is a less efficient estimator for the population mean than the sample mean, since its efficiency is onlyIn plain English: after many samplings, an equally good estimate for the true mean  of a normally distributed population can be obtained from a sample mean of 637 observations as from the median of 1000 observations.    




8.5 Likelihood Intervals
          
        


Likelihood intervals are analogs of confidence intervals discussed in Sect. 7.​3 with a slightly different interpretation. A confidence interval  expresses the probability that the true mean  will be on this interval, namely . On the other hand, a likelihood interval (8.16)where it is assumed that the true  is known, and the corresponding probability (8.17)measure our "belief" that the observations  were generated by a random process with parameter  from the interval (8.16).
How can a likelihood interval be determined? The distribution of  is generally unknown, so we also do not know how to compute the probability (8.17). However, if we resort to the large-sample limit (), all maximum-likelihood estimates attain the minimum variance bound [4]. In addition, in the asymptotic regime  the likelihood function becomes independent of the sample  and tends to the normal distribution in , with mean  and variance : (8.18)Here the true  is to be seen as "dancing" about the parameter . In its vicinity, the log-likelihood function has a parabolic shape (8.19)shown in Fig. 8.3.Fig. 8.3The parabolic shape of the log-likelihood function near  with the corresponding likelihood intervals for ,  and 


An arbitrary likelihood interval  is then defined by the formula (8.20)where  is the distribution function of the standard normal distribution (3.​11). We are mostly interested in symmetric intervals  with probability content p and probabilities  to the left and right of them: (8.21)The intervals correspond to line segments between the intersections of parabolas with horizontal lines at a below , where , as shown in Fig. 8.3. Likelihood intervals with , 95.4 and  (, 2 and 3) correspond to , 2.0 and 4.5, respectively. The method approximately works even in the asymmetric case, as shown in the following Example
        

                  
                  
                

                  
                  
                .

Example
Based on measured decay instances  we wish to determine the decay time of radioactive nuclei. The sample  may not be large: Fig. 8.4 (left) shows  decays, and Fig. 8.4 (right) shows  decays.
The probability for the nucleus to decay in the time interval  is given by the exponential distribution, so that . The likelihood function for the sample  with parameter  iswhile its log is . Comparing the likelihood equation for ,to (8.13) we deduce that  is an unbiased estimator for the mean decay time, with variance , so that the uncertainty of the parameter  is . At  one has  orFor small n this does not have the parabolic shape (8.19) in , so one can not determine a symmetric likelihood interval by using (8.21). But one can still define an asymmetric interval , where  and . For, say, , this interval is defined byand is shown in the upper part of Fig. 8.4 (left). When ,  becomes more and more parabolic and the corresponding likelihood interval more and more symmetric (see Fig. 8.4 (right)). Ultimately, in the limit,  we finally obtain a symmetric interval .    



Fig. 8.4Determination of the mean decay time of nuclei from the sample of [Left]  and [Right]  measured decay instances. Also shown are the graphs of log-likelihood functions. The true decay time, used to generate the events, is 




8.6 Simultaneous Determination of Multiple Parameters
          
        

Let us revisit the issue of determining multiple parameters , whose values we wish to infer from the sample . The likelihood equations with corresponding log-likelihood functions used to obtain the estimates for individual  have already been written down: see (8.4). What complicates the matter is the determination of uncertainties (variances) of these parameters and their correlations.

8.6.1 General Method for Arbitrary (Small or Large) Samples
If estimates can be written as explicit functions of the variables , that is, in the form , the covariances of  and  can be defined as (8.22)The variances of individual  are obtained when this formula is applied at ,The multiple integral should be calculated on the whole definition domain of the random variables , corresponding to sample values . This method is applicable to samples of any size, small or large.

Example
By using (8.22) let us show that the variance of the estimator  for the mean decay time
              
            

              
               is indeed , as shown in Example on p. 213: (8.23)Of course, one can also be very brief: . We shall revisit the decay time determination in Problem 8.8.1.    




8.6.2 Asymptotic Method (Large Samples)
For large samples () the dependence (8.19) can be generalized to multiple parameters aswhere A is the matrix of negative second derivatives of  with respect to  as in (8.5). Its expected value  is a symmetric matrix with the elements (8.24)and the likelihood function has the form of a p-dimensional normal density (4.​23),where  is the covariance matrix of the parameters . Its elements are (8.25)while the correlation coefficient of an arbitrary parameter pair is


Example
A sample  presumably stems from a normally distributed population. We are interested in the estimates for its mean  and effective deviation  as well as their uncertainties. We already know the log-likelihood function (8.6), except that now all effective deviations are the same. Since two parameters are involved,  and , there are two likelihood equations:The usual formulas for sample mean and variance follow:To calculate their uncertainties, we need the second derivatives:whose expected values areInverting the matrix B yields the covariance matrix:soThe estimates  and —in this particular case—are uncorrelated. Repeat the calculation for  instead of ! What is the difference?    





8.7 Likelihood Regions
          
        

When the maximum-likelihood method is used to determine multiple parameters  and one would like to specify their uncertainties, likelihood intervals are replaced by likelihood regions. Similar to (8.20) one is usually interested in the probabilities that parameters  and  simultaneously lie on their corresponding intervals,Let us restrict the discussion to two, generally correlated parameters by using large samples (). By analogy to the one-dimensional case (8.18) the likelihood function near the optimal values  and  has the form of a binormal density in  and  (see Example on p. 108), with possible correlations:orwhere R is a random variable (8.26)The curves of constant likelihood are ellipses centered at , defining the corresponding likelihood region. The limiting value  defines the covariance ellipse, an example of which is shown in Fig. 8.5 (left).Fig. 8.5[Left] Covariance ellipse as the boundary of the likelihood region () for parameters  and . The arrows with lengths  and  denote the usual likelihood intervals () for an individual parameter regardless of the other. [Right] A rectangle circumscribing the ellipse as an alternative likelihood region

It turns out [4] that R—regardless of , , ,  and —is distributed according to the  distribution with , so that with a chosen probability p one has (8.27)where  is given by (3.​21). The probability that  is equal to the probability that  and  are simultaneously within the ellipse defined by the equation . One first chooses a probability p with which one would like to jointly "capture"  and  in the elliptic region: the corresponding ellipses are then the intersections of the surface  with parallel planes , where —just like in the one-dimensional case in Fig. 8.3. Solving (8.27) for ,then yields the equation of the ellipse (8.26) with . Some typical pairs of p and  are listed in the table below. 
p
0.3930.5000.6830.8650.9540.9890.997

11.392.3046.16911.62


8.7.1 Alternative Likelihood Regions
Likelihood regions may be defined by any prescription that algebraically or geometrically maps the parameter uncertainties to the chosen probability p in a unique way. Instead of an elliptic region, for example, one can define a rectangular one, such that its sides correspond to the parameter rangesThe probability p that  and  are simultaneously within the rectangle depends on the correlation parameter . It is given by the formula [4]where  is the distribution function of the standard normal distribution (3.​11). The integral is calculated numerically. If a rectangle circumscribes the covariance ellipse as shown in Fig. 8.5 (right), one obtains a likelihood region with , on which  and  are found with probabilitySome typical pairs of  and  are shown in the table below. 

0.00.20.40.50.60.70.80.91.0

0.4660.4710.4860.4980.5140.5340.5610.5960.683

The column with  corresponds to uncorrelated parameters, for which




8.8 
                Problems
          
        
              

8.8.1 Lifetime of Particles in Finite Detector
            
          

                    
                    
                  

A detector is used to measure the lifetime of unstable particles, yielding the sample . The times , where  is the Lorentz factor, are calculated for each particle from the measured length  of its trajectory and its velocity . Use the sample  to obtain the maximum-likelihood estimate of the mean decay time and its variance! Discuss the cases that the detector is  infinitely large or  finite.

  Decay times are exponentially distributed with density , where  is the true decay time. In an infinite detector one may have  and the density  is correctly normalized. The likelihood function for n observations iswhere  is the usual sample mean. The log-likelihood function is , and the corresponding likelihood equation isThe estimate  for the true decay time  is therefore the sample mean, . Its variance is given by formula (8.23).

 The finite-detector case is more interesting. Here the decays can be described by the probability densitywhere T is the potential decay time. Namely, for the ith particle the time can only be measured on a finite interval . The log-likelihood function is nowwhile the likelihood equation isMultiplying by  yields an implicit equation for ,which can be solved iteratively with the initial condition obtained in Problem . With  we also calculate its variance by using the formula



8.8.2 Device Failure Due to Corrosion
          

Figure 8.6 (left) shows  device lifetimes  (times until failure) in dependence of the corrosion level of its components [5]. Assume that the lifetime is an exponentially distributed random variable T with the probability density , where x is the corrosion level, while a and b are unknown parameters. Determine a and b and their variances by using the maximum-likelihood method!Fig. 8.6[Left] Device lifetimes as functions of corrosion levels present in its components. The curve represents the model with parameters determined by the maximum-likelihood method. [Right] Measured distribution of glass fibers with respect to their tensile strength. Both data sets can be found on the book's website


 The sample consists of  pairs . The likelihood function and its logarithm areThe likelihood equations areThese equations can not be solved analytically for a and b, so one can either seek a numerical solution or directly maximize . Either way, (8.28)so that . Since the variable T is exponentially distributed, its expected value is . This curve for optimal parameters (8.28) is shown in Fig. 8.6 (left). To compute the variances we also need the second derivativesWe arrange these expressions in matrix B by formula (8.24). Its inverse is the covariance matrix of the optimal parameters:By using (8.25) we finally obtain  and .


8.8.3 Distribution of Extreme Rainfall
Let us revisit
            
          

            
             the Example from p. 159. By fitting the probability density (6.​13) to the histogram in Fig. 6.​7 (right) we obtained the parameter values listed in that Figure (, , ). Use the maximum-likelihood method to determine ,  and  as well as their uncertainties!

 The distribution of the measured  extreme values  is modeled by the probability density of the form (6.​13). The appropriate log-likelihood function is (8.29)To compute the estimates ,  and  we need to solve the likelihood equations ,  and , but clearly this can be rather annoying. Such cases call for a numerical tool like Mathematica to directly maximize . We already know the approximate parameter values: if, on the other hand, we have not the slightest idea of what they should be, we can simply plot : see Fig. 8.7.Fig. 8.7The values of the log-likelihood function as a function of parameters ,  and  pertaining to the distribution of extreme rainfall in Engelberg. The black symbols denote the maximal value  attained by the parameter set (8.30)

Inspecting the plot allows us to narrow down the search region:We get (8.30)where . These values are denoted by black dots in Fig. 8.7.
Without the use of modern computational tools the calculation of variances of optimal parameters is even more strenuous. One needs second derivativesto construct the matrix B by formula (8.24) and the covariance matrix ,With more and more parameters, this is no longer manageable by hand. In Mathematica, on the contrary, one simply defines the parameter array  and the  matrix B, which one fills with negative second derivatives:The matrix elements, calculated symbolically, need to be evaluated with the optimal parameters (8.30). The only remaining task is to compute the covariance matrix (use ):so the parameter uncertainties are



8.8.4 Tensile Strength of Glass Fibers
            
          


          
            In modeling the tension of glass fibers one can imagine that each fiber consists of many smaller fibers, so that the whole breaks when the weakest link in the chain breaks. Figure 8.6 (right) shows the measured distribution of fibers with respect to their tensile strength [6]. The measured strengths are therefore a kind of minimal extreme values; describe them with an appropriate extreme distribution of the type (6.​15), and determine its parameters and their variances by using the maximum-likelihood method!

 As explained in Sect. 6.​6.​3, the same problem can be solved if one negates the data () and finds the distribution of maximal values with the sign of the mean parameter reversed, . Then the optimal parameters and their variances can be calculated precisely by the procedure outlined in Problem 8.8.3. The log-likelihood function is given by formula (8.29), and maximizing it gives the estimatesat which the value of the log-likelihood function is . The parameter covariance matrix isand the parameter uncertainties are
            
          

                    
                    
                  





References


1.
H. Cramér, Mathematical Methods of Statistics (Princeton University Press, Princeton, 1946)MATH


2.
C.R. Rao, Information and the accuracy attainable in the estimation of statistical parameters. Bull. Calcutta Math. Soc. 37, 81 (1945)MathSciNetMATH


3.
S. Brandt, Data Analysis, 4th edn. (Springer, Berlin, 2014)CrossRef


4.
A.G. Frodesen, O. Skjeggestad, H. Tøfte, Probability and Statistics in Particle Physics (Universitetsforlaget, Bergen, 1979)


5.
S. Coles, An Introduction to Statistical Modeling of Extreme Values (Springer, Berlin, 2001)CrossRefMATH


6.
R.L. Smith, J.C. Naylor, A comparison of maximum likelihood and Bayesian estimators for the three-parameter Weibull distribution. Appl. Stat. 36, 358 (1987)MathSciNetCrossRef














© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_9




9. Method of Least Squares




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
The method of least squares is the basic tool of developing and verifying models by fitting theoretical curves to data. Fitting functions that linearly depend on model parameters (linear regression) is treated first, discussing the distinct cases of known and unknown experimental uncertainties, finding confidence intervals for the optimal parameters, and estimating the quality of the fit. Regression with standard and orthogonal polynomials, straight-line fitting and fitting a constant are analyzed separately. Linear regression for binned data, linear regression with constraints, general linear regression by using singular-value decomposition, and robust linear regression are presented, followed by a discussion of non-linear regression.



Almost on a daily basis one encounters the problem of fitting a chosen function
        
       to the pairs , i.e. the values  measured at points , arranged in vectors (9.1)The observations  may be correlated. These correlations—to be precise, the estimates of correlations—can of course only be determined by multiple (M-fold) measurement of the whole set  at the same . The obtained variances and covariances can be stored in the sample covariance matrix
where . If the measurements are independent, the covariance matrix is diagonal, (9.2)where  is the uncertainty of the individual . The function f being used to fit the data contains a certain set of model parameters. The final estimates for their values should be sensitive to the precision or uncertainty of the data. Searching for the appropriate model function f is called regression.

9.1 Linear Regression
Linear regression means
          
        

          
           that the model function linearly depends on the parameters . This kind of regression is used when we seek a polynomial (9.3)that best fits the data (9.1) or, say, a function of the form (9.4)We assume that each value  is a realization of a random variable, distributed about the unknown true value with the uncertainty . For the parameter set  one therefore generally writesor, in matrix form,Here  is a vector of dimension n and A is a  matrix with elements  that in general are functions of x. In (9.4), for example, we have ,  and  for each i.
The main idea of fitting is to minimize the sum of squares  with respect to the uncertainties . This is the core of the method of least squares. (Regression, however, is not a uniquely solvable problem, as many other measures of deviation of  from  exist; least squares just happen to be by far the most popular.) One therefore tries to find the parameters  minimizing the quadratic form (9.5)or, in the case of uncorrelated uncertainties (9.2), (9.6)The deviation of  from the model value  is "punished" inversely proportional to the absolute error of . The measure of deviation  is minimized when its minimum is found by requiringor, in vector form, . This yields the so-called normal system of linear equations for the parameters ,
                  
                  
                
 (9.7)Its solution is the vector of optimal parametersWhat are the uncertainties (variances) and covariances of components of ? In other words, what is the connection between the  covariance matrix of parameters  and the  covariance matrix of the values ? We use  in the error-propagation formula (4.​2.​7) to derive (9.8)Therefore (9.9)where . The estimate  is unbiased, . Because the relation between  and  is linear, the Gauss-Markov theorem [1] also tells us that it has the smallest
           possible variance.
Note that the dependence of the measure of deviation on parameters  has the general formIn the case of a single parameter  and a constant covariance matrix of observations  one sees that  has a parabolic shape: (9.10)In the following we shall describe the fitting of functions to data in a pedagogically rather non-orthodox sequence: polynomials, orthogonal polynomials, straight line, constant. We assume throughout that the observations  are independent, so that their covariance matrix is given by (9.2).

9.1.1 Fitting a Polynomial, Known Uncertainties
When (9.6) is minimized
            
           with respect to  () with model (9.3), the system (9.7) becomesHere V is an  Vandermonde matrix with the elements (9.11)while  is the weight matrix. Denoting  ( matrix, ) and  (p-dimensional vector) the system can be rewritten as (9.12)or (9.13)where . The solution of (9.12) is the vector of optimal parameters, (9.14)while their variances and covariances are (9.15)


Example
An experiment results in the angular distribution of scattered particles, shown in the Table below and in Fig. 9.1. The independent variables  are the cosines of the scattering angles and are "almost exactly known": a measurement at  involves angles on the interval around  which is much smaller than the distance between the neighboring points, . The dependent variables  are the numbers of detected particles at given angle. The uncertainties  change with the angle and increase in the backward direction.

  











0.10.30.50.70.9

1412908881534501352218355278482

32020518016012010390888076  
Let us fit the data by a parabola (). Formulas (9.13) yieldand solving the system (9.14) gives the optimal parametersTheir covariance matrix (see (9.15)) isThe uncertainties of parameters , i.e. the effective deviations , can be read off from its diagonal elements:
Fig. 9.1Distribution of particles with respect to scattering angles (angular distribution), together with the best-fit parabola. The shading indicates the area corresponding to the variation of the parabola's leading coefficient by one effective deviation

The parabola with these parameters that fits the data optimally is shown in Fig. 9.1. The correlations between the calculated parameters are most clearly identified in the correlation matrix  with the elementshenceThe diagonal elements (auto-correlations of parameters ) are equal to 1. The parameter cross-correlations are given by the off-diagonal terms: a smaller absolute value implies a smaller correlation. A positive sign means proportionality (correlation), a negative sign implies anti-correlation.    



9.1.2 Fitting Observations with Unknown Uncertainties
So far we have assumed that the uncertainties of  were known, providing each observation  with an error  having zero mean, that is,where  is the covariance matrix. If the uncertainties are unknown, one usually assumes that the variance is constant for all observations, , and that the observations are independent. Then the covariance matrix is simply , where  is an  unit matrix. In this case the optimal parameters  can be calculated without even knowing  since it follows from (9.7) that (9.16)How can one nevertheless still estimate  and the uncertainties of ? We compute the sum of squared residuals (SSR)
          
measuring the deviation of the model  (evaluated with optimal parameters) from the observations . The unbiased estimator for the unknown variance  is then (9.17)while the covariance matrix of the optimal parameters is (9.18)which can be compared to (9.8) when the uncertainties were known. Variances, covariances and correlation of optimal parameters are then obtained from (9.9).

Long example Fig. 9.2 (top left) shows the deviation of the global average temperature of the Earth surface from the average value obtained between 1951 and 1980, the so-called temperature anomaly [2]. The circles denote the average annual anomalies as a function of time (135 observations between 1880 and 2014). The uncertainties of  are
           unknown.Fig. 9.2[Left top and bottom] Time dependence of the temperature anomaly (Earth surface) and best-fit polynomials of odd degrees. [Right] Deviations (residuals) , calculated with the optimal parameter values  in the case [Top]  and [Bottom] . Notice how the residuals have shrunk and become more random

It is useful to shift the origin, , so that one can work with smaller numbers on the interval [0, 134]. The data are fitted by polynomials (9.3) of various odd degrees (, 4, 6 and 8). The matrix A in formula (9.16) is of the Vandermonde form (9.11). The calculated optimal parameters minimizing the measure of deviation  are listed in Table 9.1. The corresponding minimal values  are given in the second column of Table 9.2.
How can we judge whether the chosen model function is "good"? The basic diagnostic tool are the differences between  and  once the minimization of  has been done, that is, the residuals
The distribution of residuals should be as random as possible. The residuals of the linear fit () are shown in Fig. 9.2 (top right). This is clearly unsatisfactory as the residuals are large and even exhibit some sort of oscillations. When the fit degree is increased, the residuals shrink and tend to become more and more random, while  keeps on dropping. However, one should not perpetuate this as it is wise to describe the observations with as few parameters as possible: any data  can be even exactly interpolated by a polynomial of degree , but such fits invariably become too "wild": signs of such behaviour can be glimpsed already at  near the edges of Fig. 9.2 (bottom left).Table 9.1Values of optimal parameters  (units of  are ) in regression analysis of the temperature anomaly with polynomials of various degrees
p
















2

9.311      4

15.02

0.135    6



1.424

2.626

  8

8.354

3.072

5.875

0.195See also Table 9.2

Table 9.2Minimal values , the coefficient of determination R and the estimate of variance  (9.17) in polynomial regression of the temperature anomaly
p



R


23.6880.8280.027842.0950.9020.016061.8590.9130.014481.7920.9170.0141

Even if  drops when p is increased that does not necessarily mean that a high-degree polynomial is better than a lower-degree polynomial. Indeed such a function does a better job in describing the variation in the data, but it is unclear how much of it can be assigned to the uncertainties of observations and how much to the model having too many parameters. A good measure of the variance that can be assigned solely to the model is the coefficient of determination
where the denominator is the total variance of the observations about their mean. A better regression results in the values of R being closer to 1. The values of R for our Example are given in the third column of Table 9.2. A seventh-degree polynomial (), for instance, describes  of the data variation.
Two more tools are at hand to evaluate the relevance of individual parameters  for the description of data. The first tool are their correlations, obtained by the usual formula (9.9). Fitting with , say, gives the correlation matrixindicating strong correlations, in particular between  and  as well as  and . The other tool are the ratios of absolute parameter values and their variances—a kind of "signal-to-noise" ratios—where the variances are obtained from the diagonal elements of the covariance matrix (9.18). If a parameter  is thought to be statistically relevant, the value  should be much larger than 1. In fitting with  we get , ,  and , while for  we get , , , ,  and , indicating that parameter  might be superfluous.    



9.1.3 Confidence Intervals for Optimal Parameters
Confidence intervals
             for parameters  are calculated by analogy to the confidence intervals for the sample mean (Sect. 7.​3.​1). If the uncertainties  of  are unknown—yet assumed to be independent and identically distributed—we first compute the covariance matrix (9.18) and extract the variancesAt chosen confidence level , parameter  then has the confidence interval (9.19)on which the unknown true value  lies. The critical value  can be read off from Table 7.​1, taking into account  degrees of freedom. In other words,If , the t distribution turns into the standardized normal distribution and  can be replaced by  from the last row of Table 7.​1.

Example
In the previous Example we have seen that in fitting a fifth-degree polynomial () the least reliable parameter was , for which we gotLet us choose a confidence level of . The sample is large, , so the limit the of normal distribution is justified and we can simply take . The confidence interval (9.19) for  is thenthus .    




9.1.4 How "Good" Is the Fit?
If the uncertainties of the n observations are mutually independent and normally distributed, and if the p regression parameters are also independent, the minimized sum of squared residuals is -distributed (3.​21) with  degrees of freedom,Thereforewhere  is the distribution function. This formula is used to quantify the goodness of fit. A small  or large G indicate a "good fit", large  and small G imply that a fit is "bad". See also
            
           Sect. 10.​3.


9.1.5 Regression with Orthogonal Polynomials 

The matrix
             B
           in (9.12) tends to be ill-conditioned: its condition number exponentially grows with dimension p, so , see Sect. 3.2.5 in [3]. At desired precision  of parameters  the procedure described above can accommodate polynomials of degree , where  is the machine precision. In double precision this usually means .
One can at least partly avoid these stability problems if the points  coincide with the definition domain of some system of orthogonal polynomials. Most suitable for regression purposes are the discrete-variable orthogonal polynomials  which are mutually independent and orthogonal on a discrete set of points  in the sensewith some weight . A model function may be devised as a linear combinationwhere the expansion coefficients  are unknown parameters. They are again determined such that the measure of deviation (9.6) is minimal. We getThe condition for a minimum, , gives  or


Example
The most popular system of discrete-variable orthogonal polynomials are the Chebyshev polynomials of the first kind, : see Sect. 4.3 in [3]. A linear combination  of these polynomials minimizes the measure  with the nodes  and coefficientswhich is known as the Chebyshev approximation. The problem is well defined for . In the limiting case  the function f interpolates the data  and the measure of deviation is .    




9.1.6 Fitting a Straight Line
Seeking a straight
            
           line  fitting the data  with known uncertainties  is the most common two-parameter linear regression. When the measure  (9.6) is minimized with respect to  and , we obtain an analytically solvable systemwhere we have denotedThe matrix B from (9.12) is immediately recognizable, as well as its inverse, (9.20)where we assume that . The coefficients  and  that minimize  areA sample fit is shown in Fig. 9.3 (left).Fig. 9.3Fitting a straight line to the data  with errors  at  by using the method of least squares. [Left] The straight line that minimizes the measure  (, ). [Center] Covariance ellipse with the center at  and uncertainties  and . Parameters within the ellipse correspond to straight lines contained in the shaded area of the [Right] panel. The straight line corresponding to the true  and  has a probability  of lying within this area

For constant uncertainties () the parameters of the straight line arewhere  and  are the arithmetic means of the observations, whileare their sample variance and covariance. We see that the straight line goes through the "center of gravity"  of the data. From (9.15) and (9.20) we also obtain the variances of the parameters  and , which do not depend on the position of the points along the y-axis: (9.21)The off-diagonal elements of the covariance matrix  for estimates  and  areso that the linear correlation coefficient between  and  is equal toLet us denote(Do not confuse these with the uncertainties  of the observations .) The estimates  and , their uncertainties  and , together with the correlation coefficient , define a covariance ellipse centered at  with semi-axes  and , rotated by angle  in the -plane. The ellipse parameters are given by the formulas [4]An example of a covariance ellipse is shown in Fig. 9.3 (center). The points within the covariance ellipse define a bundle of straight lines, indicated by the shaded area in Fig. 9.3 (right).
Let us show how the uncertainties  and  change with increasing number of points n. Suppose that the points  are uniformly distributed on , so that  with , and that each observation has an error of . We calculate ,  and . From (9.21) we then extract the asymptotic behaviour in the  limit,Therefore, with increasing n, the straight-line parameters  and  gain in precision just as any other statistical average, i.e. .


9.1.7 Fitting a Straight Line with Uncertainties in both Coordinates
In linear
            
            
          

            
            
             regression with a straight line , where the values in both  and  possess uncertainties, we strive to minimize the measureThe determination of optimal  and  requires us to simultaneously fulfill the conditions  and . Because  enters non-linearly, the problem is non-trivial; a reliable algorithm is
            
           given in [3], p. 233.


9.1.8 Fitting a Constant
In zero-degree polynomial
            
           regression the observations  are fitted by a constant: we minimize (9.6) with . The condition  yields (9.22)which is precisely the weighted average (8.​7). It can be used in place of the arithmetic average whenever measurements of the same quantity have different errors. The asymptotic behaviour is  when .
We shall see in Chap. 10 that the measure  helps us quantify the assumption of the normal distribution of uncertainties . At chosen significance (risk level) , say, , we determine  from the equationwhere  is the density (3.​21). We compare the obtained  with the value of  calculated from the data. If , the optimal value  and its uncertainty may be considered to be consistent with the data.Fig. 9.4Fitting a constant to the data , . [Left] Weighted average in the presence of two outliers ( and ) and without them. [Right] Weighted average of data with an unexplained systematic error, resulting in an unreasonably small uncertainty of the parameter , as well as the average involving rescaled errors (9.23)


Example
This apparent simplicity conceals many pitfalls. Figure 9.4 (left) shows  observations, which we fit by a constant c. The procedure outlined above yields  and . With a chosen significance  and , we use Fig. 10.​6 to read off . Since , the basic premise of normal errors may be rejected. (In other words, a constant probability density is inconsistent with the measured sample with a probability much higher than .) But if the outliers  and  are omitted, we get  and , while . Now the fit appears to be consistent with the data.

A different problem is revealed in Fig. 9.4 (right). By using the method of Sect. 7.​3.​1 we can show that individual observations are outside of the confidence interval for the sample mean. In this case we obtain  and again . But now outliers can not be blamed for a large value of , as the measurements obviously include an unknown, underestimated systematic error. In such cases the measurement uncertainties may be rescaled: (9.23)The weighted average to compute the desired parameter  can still be formed by (9.22), but its uncertainty now becomesThis yields a more sensible result and, by construction, .    



9.1.9 Are We Allowed to Simply Discard Some Data?
We certainly are! Among various reasons a specific measurement or an individual observation may be simply removed, the Particle Data Group [5] lists the following:  the measurement is superseded by or included in later results;  its error is not given;  it involves questionable assumptions;  it has a poor signal-to-noise ratio, low statistical significance, or is otherwise of poorer quality than other data available;  it is clearly inconsistent with other results that appear to be more reliable;  it is not independent of other
            
            
          

                    
                   results. The figure shows the mean values of the neutron decay time, as known over the years 1960-2015. (There were several independent experiments each year; shown are the annual averages.) Think about which of the values shown in the plot should be trusted today, based on the above criteria!








9.2 Linear Regression for Binned Data
Often the
           data are histogrammed or binned in the x variable. This means that n observations  are classified into N mutually exclusive classes or bins. The ith bin then contains  observations. Figure 9.5 (left) shows an example: the first bin  has  counts, the second bin  has , and so on.Fig. 9.5[Left] Histogram of  counts arranged in  bins. [Right] The same data set, but with specified uncertainties in the x variable which partially overlap (the classes are not mutually exclusive). These two arrangements are not equivalent!

Let the probability of a certain value landing in the ith bin be . Here  is the parameter set that determines the model distribution of the observations. The expected number of counts in the ith bin is therefore . The histogram contains all observations, hence (9.24)In Sect. 5.​2 we have demonstrated that the distribution of  in N bins is multinomial, with the covariance matrixDue to normalization (9.24) the matrix  is singular (), but the least-squares problem can still be formulated. Namely, one of the bins—say, the Nth—can be eliminated, since one always has . This results in an  non-singular matrix  which we use to minimize the measure of deviationwhere  and , and the matrix elements are , , so that , where . By invoking the Sherman-Morrison formula we getwhere we have used . The expression for  can therefore be rewritten as (9.25)Minimizing (9.25) is not a trivial exercise, as the parameters  enter non-linearly. However, if the number of bins N is large enough, individual  are so small the all off-diagonal elements of the matrix  may be neglected. Consequently,  and the parameters  can be obtained by solving the system of equations (9.26)Two simplifications are possible—none of which eliminates non-linearity. The first option is at hand: imagine that the denominators in (9.25) are independent of . This is equivalent to the system (9.26) without the quadratic term. Besides, we can replace  by  in the denominators, as these values can not be that different! In this simplified approach, one only needs to solve the system (9.27)


Example
Let us revisit Fig. 9.5 (left). A total of  counts have been classified into  equidistant bins  of width , where , . There are , 147, 153, 136, 95, 74, 54, 59, 79, 90 counts in individual bins. Assume that the observations are described by the probability density (9.28)where  is an unknown parameter, and that the uncertainties of  are Poissonian. The corresponding  can be calculated by integrating f over each bin:


The estimate for  is obtained by (9.27), where we use :We obtainWe are dealing with a linear problem, so  near  has a parabolic shape (9.10), which we write as , whenceBeing a density, the best-fit function (9.28) now only needs to be normalized. As we have  counts in  bins, it obviously has to be multiplied by , so the final form is . It is shown by
          
         the dashed line in Fig. 9.5 (left).    



9.3 Linear Regression with Constraints
The true quantities
          
          
         being measured—call them —are often algebraically related through some kind of constraints.
                  
                

          
           The actual observations  have uncertainties , so they do not necessarily satisfy the constraints which, however, should be satisfied by the estimates . The following classic example outlines two general approaches to including constraints in the method of least squares.

Example
A measurement of the interior angles of a triangle yields ,  and  with errors . The measured values add up to , not the required . If the true angles are considered as unknown parameters (), they can be estimated by the method of least squares. We minimizebesides, we require that (9.29)This constraint can be used to eliminate one variable from , say, :


We calculate the derivative of this measure with respect to  and  and set it to zero, obtaining two equations for two unknowns. With their solution we exploit (9.29) to obtain the remaining third angle, thus finallyso that the requirement  is fulfilled by construction. It can be seen that the method has uniformly distributed the missing  from  to the correct value  among the three
        

                  
                 observations.
The problem can also be solved by using Lagrange multipliers. The constraint equation (9.29) is taken into account by endowing it with a weight representing a new unknown parameter with respect to which the measure of deviation needs to be minimized:By solving the equations  we get (9.30)whence , yielding the same ,  and  as before.    

It is worthwhile to outline the method of least squares with linear constraints in a more general way. If we must determine p parameters  satisfying q constraints with the appropriate multipliers , we must minimizeHere B is a  matrix and  is a q-dimensional vector, while the observations vector , their covariance matrix  and the regression matrix A have their usual, well-known roles. The measure  is minimized by setting its derivatives with respect to each of the  parameters to zero:By denoting (9.31)this can be written asThe solution of this system [1] are , , and their variances and covariances: (9.32)
 (9.33)
 (9.34)


Exercise
Let us revisit the triangle angles problem. The observations vector is , their covariance matrix is , and the regression matrix is . The constraint equation  is embodied by the  "matrix" B and the 1-dimensional "vector" :The quantities from the definition (9.31) areUsing (9.32) and (9.33) immediately leads to (9.30), and formula (9.34) gives the variances and covariances of the parameter estimates:We see that the estimated "optimal" values of the angles have a smaller effective deviation than the measured ones—namely instead of —but they
            
             have become correlated (non-zero off-diagonal elements).    




9.4 General Linear Regression by Singular-Value Decomposition 

A more
        

                  
                  
                

          
           general form of linear regression,where  are basis functions, can be used to fit the model function f to a large data set with fewer parameters. Such problems are over-determined, but the data are often not rich enough to nail down a unique linear combination of the basis functions: one may obtain many functions that minimize the measure (9.35)almost equally well. (From a strict mathematical point of view, of course, the least-square solution of an over-determined problem is unique.)
A better control over the importance of the parameters  can be obtained by using singular-value decomposition (see Sects. 3.3.2 and 3.3.3 in [3]), which excels also in terms of numerical stability. By denotingwhere  and , we see that (9.35) has the typical least-squares form . We then perform the singular-value decomposition of A: , where . The matrix  has n-dimensional columns , the matrix  has p-dimensional columns , and the diagonal matrix  contains the singular values . The vector of optimal parameters is obtained by the sumThe variances and covariances of  are (9.36)We must be alert to the singular values  for which , where  is the machine precision. Such values, appearing in the denominators of (9.36), increase the uncertainties of parameters  and indicate that including further parameters would be pointless. They also contribute insignificantly to the minimization of , so they can be eliminated. This is done by setting  (for further explanations see e.g. the comment to the Fitsvd algorithm in [6]). One may also discard those singular values for which the ratio  is larger than , at least until  starts to increase significantly.


9.5 Robust Linear Regression
As all estimates
          
        

          
           of distribution location and scale, regression methods are sensitive to outliers (see Sect. 7.​4). A telling example is shown in Fig. 9.6 (left). A set of  data of the form , where , ,  and  are realizations of  and , and a relatively large set of  presumed outliers , realizations of  and , are fitted by a straight line. The standard least-squares method (LS) yields a result that does not describe the bulk of the data.Fig. 9.6Robust linear regression on data containing a significant fraction of outliers. [Left] The standard least-squares method (LS) yields a straight line that goes through both data clusters, but does not describe their main part correctly. The LMS method delivers a reasonable description. [Right] The function being minimized in the LMS method.

A simple method exists where we minimize the median of the squares of residuals  called least median of squares (LMS). We seek parameters  and  that minimize the measure of deviation
          
        

                  
                
 (9.37)The main issue with the LMS method is precisely the minimization of (9.37). The function being minimized with respect to  has  local minima, where n is the number of points  and p is the degree of the regression polynomial. The example in the figure has  and  (straight line), so there are  local minima, among which the global one needs to be found, as shown in Fig. 9.6 (right). For introductory reading on robust regression
          
        

          
           see [7].


9.6 Non-linear Regression
In non-linear
          
         regression the dependence of the model function on regression parameters is non-linear, for example(Compare this to (9.4)!) As usual, the observations  at  are arranged in vectors  and , and the components of the regression function in , where . By analogy to (9.5) the measure of deviation is defined asIf the measurement errors are uncorrelated, the covariance matrix is diagonal, , and the above expression can be simplified toThis is where the problems start. Minimization of  now implies solving a system of p (in general non-linear) equations  (). Such problems are therefore solved iteratively: we ride on the sequence (9.38)where  is obtained by solving a linear problem, to approach the optimal parameter set. In the th step the update can be calculated by minimizingwhere we have suppressed the dependence of  on . If  is small,  can be expanded as , where J is the Jacobi matrix with the elementsLet us denote  and . We must minimize the expressionFrom the requirement  it follows thatThis system of linear equations must be solved in order to obtain the update  in the th iteration (9.38). If the data uncertainties are uncorrelated, the procedure can be summarized as (9.39)
 (9.40)whereWe are still missing the uncertainties of . Near the optimum  is approximately parabolic, so the covariance matrix of the regression parameters can be computed as in (9.8),or simply , if  are uncorrelated. As usual the diagonal elements of this matrix are the parameter variances, and the off-diagonal are the covariances, see (9.9). If the uncertainties of  are unknown, the parameter covariance matrix can be calculated by analogy to (9.18), (9.41)
Fig. 9.7Global release of . [Left] Exponential model, calculated by non-linear regression (full curve), and the curve obtained by transforming the straight-line regression of the logarithmic data (dashed). [Right] Linear regression of logarithmic data


Example
The circles in
           Fig. 9.7 (left) show  values of annual global release of  from fossil fuels over 1880-2011, measured in millions of tons [8]. We would like to model the data by a two-parameter () function of the form (9.42)where  (origin shift as in Fig. 9.2). The data errors are unknown, which amounts to  for all i. The iterative method requires the derivativesto form the  matrix G,where , as well as the right-hand side of (9.39):Iteration (9.40) is started by the initial approximation . After less than ten iterations we obtain the final valueswhere the uncertainties have been computed by formula (9.41). The result of non-linear regression is shown by the full curve in Fig. 9.7 (left). However, do think about it: a standard polynomial regression of such strongly scattered data would yield an equally likable result.

There is another way to proceed. If we take the logarithm of the values , non-linear regression becomes linear, since (9.43)Now the regression parameters are  (not ) and . By the recipe of Sect. 9.1.6 we compute their optimal valuescorresponding to the straight line in Fig. 9.7 (right). But if the obtained  and  are reinserted in (9.42), one gets a different description of the non-logarithmic data, namely the dashed curve in Fig. 9.7 (left). The reason for this disagreement is clear. Even though the models (9.42) and (9.43) are mathematically identical, the chosen statistical models  and  are not equivalent because the uncertainties  and  in each case have different distributions. Besides, the method of least squares by itself is not "invariant" with respect to such transformations.    



9.7 Problems
The following Problems refer to standard data sets of NIST, suitable for studying non-linear regression and testing computer programs [9]. The data files can be found on the website of the book.Fig. 9.8[Left] Two Gaussians on exponential background (model function (9.44)). The dashed line is the initial approximation with parameters (9.45), and the full curve is the final result of the fit. [Right] Time dependence of the pressure gradient during the "El Niño Southern Oscillation" (ENSO) phenomenon


9.7.1 Two Gaussians on Exponential Background
A frequent problem in all areas of physics is describing the data by a model involving several normal (Gaussian) distributions with different means and widths, superposed on exponential background. An example is shown in Fig. 9.8 (left). Fit the data  at  points  by a eight-parameter () function (9.44)by using the method of non-linear regression described in Sect. 9.6! Use the initial approximation (9.45)corresponding to the dashed line.

 The final (rounded) set of parameters and their uncertainties isIt corresponds to the full curve in Fig. 9.8 (left).


9.7.2 Time Dependence of the Pressure Gradient
Figure 9.8 (right) shows
          

                    
                  

                    
                    
                   monthly averages of pressure differences between Easter Island in the Pacific and the Australian city of Darwin [10]. This pressure gradient is responsible for the trade winds in the southern hemisphere. The Fourier analysis of the data (the signal is indicated by the dashed line connecting the points) reveals peaks at three frequencies. The most prominent one corresponds to the annual cycle (12-month period), but one can detect two further components with 26 and 44-month periods, characteristic for the so-called El Niño Southern Oscillation (ENSO) phenomenon. Fit the data by a nine-parameter () functionby the method of non-linear regression (Sect. 9.6) with the initial approximation
 The final set of parameters and their uncertainties isIt corresponds to the full curve in Fig. 9.8 (right).


9.7.3 Thermal Expansion of Copper
Figure 9.9 (left) shows the measured thermal expansion coefficient of copper as a function of temperature [11]. Describe the data () by the rational function (9.46)depending on  parameters . Use the initial sets (9.47)
 (9.48)and make precisely 20 steps of (9.39). Does the iteration converge in both cases? Plot the solutions corresponding to both parameter sets.

 Choosing good initial parameters is crucial. The iteration started with (9.47) does not converge: the solution after 20 iterations is the dashed curve in the figure. The iteration initialized with (9.48) does converge (full curve); the final parameter set is
Fig. 9.9[Left] Fitting the function (9.46) to the measured thermal expansion coefficient of copper after 20 iterations: the dashed curve corresponds to the initial conditions (9.47), while the full curve corresponds to (9.48). [Right] Regression analysis of the data on electron mobility in silicon as a function of donor concentration



9.7.4 Electron Mobility in Semiconductor
Figure 9.9 (right) shows the
           measured electron mobilities in silicon as a function of the (log)-concentration of donor admixtures at a certain temperature [12]. Fit the data () by a model of the form (9.46)! Make 20 steps of the iteration (9.39) with two initial regression parameter sets:
 The method started with the first initial set does not converge. Using the second set does lead to convergence and the final parameters areThe solution with these parameters is shown by the full curve in the figure.


9.7.5 Quantum Defects in Iodine Atoms
Figure 9.10 (left) shows the data from a study of quantum
           defects in iodine atoms [9], with the excited-state energies on the abscissa and the number of defects on the ordinate axis. Fit the  values by a four-parameter () function (9.49)by using the method of non-linear least squares, described in Sect. 9.6! Initialize the algorithm by the regression parameters
Fig. 9.10[Left] Modeling the number of quantum defects in iodine atoms by the function (9.49). [Right] Fitting the function (9.50) to the observed values of the magnetic field strength in a superconductor as a function of time


 The final values of the parameters are



9.7.6 Magnetization in Superconductor
The circles in
           Fig. 9.10 (right) represent the  observed magnetic field strengths as a function of time from a study of magnetization in superconductors [9]. Model the data by the function (9.50)Use the iterative method of non-linear least squares described in Sect. 9.6. The initial parameter set is 


 The final parameter
            
          

            
             set when iteration (9.39) terminates is ,  and .



References


1.
A.G. Frodesen, O. Skjeggestad, H. Tøfte, Probability and Statistics in Particle Physics (Universitetsforlaget, Bergen, 1979)


2.
GISS/NASA Analysis, based on: J. Hansen, R. Ruedy, M. Sato, K. Lo, Global surface temperature change, Rev. Geophys. 48 (2010) RG4004


3.
S. Širca, M. Horvat, Computational Methods for Physicists (Springer, Berlin, 2012)MATH


4.
S. Brandt, Data Analysis, 4th edn. (Springer, Berlin, 2014)CrossRef


5.
K.A. Olive et al. (Particle Data Group), The review of particle physics. Chin. Phys. C 38, 090001 (2014)


6.
W.H. Press, B.P. Flannery, S.A. Teukolsky, W.T. Vetterling, Numerical Recipes: The Art of Scientific Computing, 3rd edn. (Cambridge University Press, Cambridge, 2007)MATH


7.
R.A. Maronna, R.D. Martin, V.J. Yohai, Robust statistics (John Wiley & Sons, Chichester, Theory and methods, 2006)


8.
T.A. Boden, G. Marland, R.J. Andres, Global, regional, and national fossil-fuel  emissions, Carbon Dioxide Information Analysis Center, Oak Ridge National Laboratory, doi:10.​3334/​CDIAC/​00001_​V2015



9.
NIST Statistical Reference Datasets (2003). The data files and explanations are accessible at http://​www.​itl.​nist.​gov/​div898/​strd/​



10.
D. Kahaner, C. Moler, S. Nash, Numerical Methods and Software (Prentice-Hall, Englewood Cliffs, 1989)MATH


11.
T.A. Hahn, Thermal expansion of copper from  to —standard reference material 736. J. Appl. Phys. 41, 5096 (1970)ADSCrossRef


12.
S.S. Li, W.R. Thurber, The dopant density and temperature dependence of electron mobility and resistivity in -type silicon. Solid State Electron. 20, 609 (1977)ADSCrossRef














© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_10




10. Statistical Tests: Verifying Hypotheses




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
Statistical tests based on hypotheses are used to statistically verify or disprove, at a certain level of significance, models of populations and their probability distributions. The null and alternative hypothesis are the corner-stones of each such verification, and go hand-in-hand with the possibility of inference errors; these are defined first, followed by the exposition of standard parametric tests for normally distributed variables (tests of mean, variance, comparison of means, variances). Pearson's -test is introduced as a means to ascertain the quality of regression (goodness of fit) in the case of binned data. The Kolmogorov-Smirnov test with which binned data can be compared to a continuous distribution function or two binned data sets can be compared to each other, is discussed as a distribution-free alternative.



Chapters 7-9 were dealing with methods by which random samples were used to make
        
      

        
         inferences about populations and to estimate the parameters of their distributions. This chapter introduces tools used to verify—from the statistical viewpoint—whether a population model is acceptable or not [1].

10.1 Basic Concepts
To test the validity of a model we use hypotheses about the properties of a population or its probability distribution, for example, "the coin is fair", implying a probability of  for heads or tails. The basic hypothesis being tested is called the null hypothesis and is denoted by , for instance
        

                  
                  
                
According to the result of a statistical test the null hypothesis may be accepted or rejected—although "hypothesis accepted" should always be interpreted as "from the statistical perspective the available data is insufficient to reject it"; in the following we should keep in mind this subtle difference. Strictly speaking, one never tests the null hypothesis by itself, but always against its alternative hypothesis denoted by , for instance
          
        ,Namely, hypotheses need not be exclusive: one can test, for example, the hypothesis  with respect to .
Testing hypothesis also brings about the question of inference errors. Imagine a blood test used to determine the presence of a disease in two populations, healthy and sick (see also Problem 1.​5.​6). Most often it happens that based on the test (values of x) the populations can not be clearly separated (Fig. 10.1 (left)): no matter what  we choose, for  a fraction of the sick population will be identified as sick (true positives, TP), and its remainder as healthy (false negatives, FN), while with  a part of the healthy population will be seen as healthy (true negatives, TN), and the rest as sick
         (false positives, FP).Fig. 10.1[Left] Distribution with respect to blood test results in healthy and sick populations. One can have true positive (TP), false negative (FN), true negative (TN) and false positive (FP) outcomes. [Right] Comparison of significance () and sensitivity () of the test as a measure of its reliability—known as the Receiver Operating Characteristic (ROC) curve. See also Example on p. 262

Let us discuss only a continuous random variable X, whose distribution is specified by a single-parameter probability density . Suppose that the experiment yields a value , for which we wish to ascertain whether it is consistent with one or another hypothesis. Let the null and alternative hypotheses correspond to the distributions with parameters  and , respectively:as shown in Fig. 10.2. If the observed x exceeds the critical value , we reject , otherwise it may be accepted. (Recall our initial warning.) The interval  in Fig. 10.2 (left) is therefore called the rejection region, while  is
        

           the acceptance region
        . Setting the value of  is our primary job—we choose in advance the probability  such that the observed x falls in the rejection region. This probability is called the statistical significance of the test
        

                  
                  
                

                  
                  
                , (10.1)If  is rejected with significance , this means that with probability  we have made a wrong conclusion, as the observed x can also take values above : we have rejected a hypothesis when in fact it should have been accepted. We say that we have rejected it at confidence level . In the language of blood tests this value is also called specificity, so that
          
        

                  
                  
                

                  
                

                  
                  
                

Fig. 10.2Probability densities corresponding to [Left] Null hypothesis with parameter  and [Right] Alternative hypothesis with parameter . Shading indicates the rejection regions for significances  and . The critical point is denoted by 


We can also make a different error, namely, accept  as true although it is, in fact, false and  should have been accepted instead. The probability for this type of error depends on  (Fig. 10.2 (right)) and is denoted by :The power of a statistical test or its sensitivity is defined as the probability that a hypothesis is rejected when it is indeed false. The power of testing the null hypothesis  against the alternative hypothesis  is
        

                  
                  
                

                  
                  
                

                  
                  
                
 (10.2)therefore, in the blood-test example,How does a test attain large power? The integral (10.2) can be written aswhere . Hence the power is large if  is large or—in the case of a sequence of observations —where the ratio (10.3)exceeds a prescribed constant that depends on . Usually  rapidly increases with  and is close to 1 above  (see Fig. 10.1 (right)). The more the curve approaches the top left corner, the larger the predictive power of the test; in the extreme case (point ) the populations are completely separated. Conventionally one chooses . How this works in practice is demonstrated by the following Example and Problem 10.5.1.

Example
Photo-disintegration of  nuclei below the pion production threshold involves two decay channels, two-body (2bbu) and three-body (3bbu) breakup:



Fig. 10.3Normalized distribution of events with respect to the missing energy in the processes  (two-body breakup, 2bbu) and  (three-body breakup, 3bbu). [Left] Measured spectrum. [Right] Theoretical spectrum

Experimentally they can be distinguished by calculating the so-called missing energy, i.e. the difference between the photon energy and the kinetic energy of the final-state proton, , where both  and  have some measurement uncertainties. An example of a measured spectrum is shown in Fig. 10.3 (left). The peak at  corresponds to the separation energy of the proton in , while the bump above  is a witness to the additional  required to split the remaining deuteron to a proton and a neutron.
How do we choose the critical  to test the hypothesis that a detected proton comes from the two-body process? The higher we set it, the more certain we can be that the sample will contain all "true" protons from 2bbu, but at the same time more an more "false" protons from 3bbu will be identified as belonging to 2bbu. If  is set very low, we may reduce the contamination of the 2bbu sample by protons from 3bbu, but at the same time we discard a significant fraction of true 2bbu events. Ideally we wish to minimize the probability  for the rejection of  when it is actually correct, and minimize the probability  for the acceptance of , when it is actually false.
How can this be accomplished? Suppose we have a theoretical model of the breakup processes that fits the data well (Fig. 10.3 (right)). The probability densities  and  describe the protons from 2bbu and 3bbu, respectively. (Both densities can be normalized to the number of counts, or vice-versa.) For the chosen statistical significance of the test, , we first establish  for which


With this  we calculate the power of the testThe model in Fig. 10.3 (right) is a sum of normal distributions  for 2bbu and  for 3bbu, with parameters , ,  and . The critical value  at chosen  is therefore nothing but the corresponding quantile of the normal distribution, and the integral for  is its definite integral. In both cases we may use formula (3.​9) and Tables D.1 and D.2. The dependence of  on —the ROC curve—is
        

          
           shown in the above figure.       



10.2 Parametric Tests for Normal Variables

10.2.1 Test of Sample Mean
Let  be a random sample drawn
            
          

            
             from a normally distributed population (). "Testing the mean" implies that we shall use the sample mean  to provide some sort of statement on the true (population) mean , which is unknown. Two cases must be distinguished: the population variance  is known or unknown. If  is known, testing the hypothesisagainst the alternative hypothesis  can be accomplished by using the statisticwhich is distributed according to the standardized normal distribution. If  is unknown, we can replace it by the unbiased sample variance (7.​10) and use the statisticwhich is distributed according to the Student's t distribution with  degrees of freedom. Of course, if the sample variance is used in the biased form (7.​8), one should replace  by . See also Sect. 7.​3.​1.

Example
By using twelve () identical thermometers we have measured the temperaturesMay we claim, with statistical significance , that the true temperature during the measurement was higher than ?
The null hypothesis is , the alternative hypothesis is . We wish to reject  if the sample mean  exceeds . From the data we calculate  and . The value of the statistic T is


Table D.4, row , reveals that  lies between , corresponding to , and , corresponding to . The required  corresponds to the critical . Since , we may reject  and accept the hypothesis  that the actual temperature exceeded , with significance  (confidence level ).    



10.2.2 Test of Sample Variance

          Testing
            
           the variance based on the sample  from a normally distributed population () again requires us to distinguish two cases: that the true mean  is known or unknown. If  is known, the hypothesisagainst  can be tested by using the statisticwhich is distributed according to the  distribution with n degrees of freedom. (Here  is taken in its biased form (7.​8).) If  is unknown, it must be replaced by the sample mean  in the above formula. Then the statistic  is also -distributed, but with  degrees of freedom. See also Sect. 7.​2.​2.

Example
To construct a detector we need many wire electrodes of a specific length. The largest allowed length tolerance is . A precise measurement of the length is very demanding, so we can only afford a small sample of  electrodes, for which we establish a variance of . Given a statistical significance of , does the wire length in the unexplored "population" fluctuate exceedingly?
The null hypothesis is , while the alternative hypothesis is . We may reject  if the sample variance exceeds the critical variance (at given ). The value of the test statistic is . The critical  can be read off from Table D.3, row for , column for : it is . Since , we have no reason (at confidence level ) to reject . We may conclude that the variance of all electrodes is within the prescribed limits.    




10.2.3 Comparison of Two Sample Means, 


          Assume
            
           we have two samples,  and , drawn from normally distributed populations with different means and equal, but unknown variances , that is,  and . A situation like this occurs, for instance, when we apply the same technique to measure a quantity that might have changed during the measurements. By comparing the samples  and  we test the hypothesis  that they stem from populations with a specific difference between the true means,against the alternative . The suitable statistic iswhere  and  are the sample means, while  and  are biased sample variances (7.​8). The statistic T is t-distributed, with  degrees of freedom.

Example
A laboratory uses two chemicals to determine the concentration of a compound in blood. Presently a reliable, but expensive chemical is used, and it has yielded a sample of  concentrations(in some units). With the cheaper chemical we obtain a sample of  valuesMay we claim that there is a statistically significant difference between the average concentrations (with significance )?

The null hypothesis is  (equal concentrations), and the alternative is . From the sample averages  and  and sample variances  and  we calculate , then the value of the test statistic . Table D.4, row , tells us that  lies between , corresponding to , and , corresponding to . The required —we must perform a two-sided test, as the alternative hypothesis means either  or —corresponds to the critical value . Since , there is no reason to reject the null hypothesis: both chemicals are equally effective, so in order to reduce costs, we may purchase the cheaper one.    



10.2.4 Comparison of Two Sample Means, 

A similar
            
           test can be performed for samples  and  of sizes  and , presumably stemming from normal populations with different (and unknown) variances. The test statistic iswhere  and  are the sample means, and  and  are the unbiased variances (7.​10). If  is true, T is normally distributed (N(0, 1)) when .


10.2.5 Comparison of Two Sample Variances

          Comparing
            
          

            
             the variances of two samples  and  is another classic: we thereby test the hypothesis whether the corresponding population variances are equal,The test statistic is the ratio of unbiased sample variances,If the null hypothesis is valid, this ratio is distributed according to the F distribution with  degrees of freedom. Given the significance  three alternative hypotheses  can be formulated:Here , ,  and  are the F-distribution quantiles (Tables D.5 and D.6).

Example
In sputtering of thin metal layers on semiconductor substrate wafers we strive to minimize the variance of the layer thickness. The variance in the sample of  layers fabricated with oven X is , while the variance in the second sample of  layers made with oven Y is found to be . Can we claim that any of the two ovens makes layers whose thickness is more precise? Let the significance of the test be .
The null hypothesis  may be rejected if the sample f (value of statistic F) satisfies  or . From Table D.5 and from the symmetry of the F distribution (see Fig. 7.​3) we getThe samples give . Since neither  nor  apply,  can not be rejected at confidence level . The ovens are equally fine. (However, the samples are small, thus the conclusion is a bit risky.)    



Example
The F test can also be applied to determine the maximum sensible degree of the polynomial used to fit the data , given a confidence level . The test can be used to distinguish among so-called nested models, in which a richer function inherits all parameters of the subordinate one and adds its
              
             own: the model , for instance, is nested within . In general the more modest ansatz has q, while the richer has p parameters, . Is the former sufficient to describe the data () or should new terms be included (: at least one of the enumerated parameters )? For both models we calculate the sum of squared residuals  and  by using (9.​6) and form the statisticwhich is distributed according to . If the calculated F exceeds the critical value ,  can be rejected; "further parameters are needed". In polynomial regression, , the statistic (10.4)therefore "measures" whether the inclusion of a new (pth) parameter—hence the next, th polynomial degree—is justified or not [2].


Fig. 10.4Using the F test to determine the maximum degree of the regression polynomial. [Left] Data (see website of the book). [Right] Dependence of  and the statistic F on the number of parameters p (polynomial of degree )

Figure 10.4 (left) shows the data () and polynomial fits of various orders, while Fig. 10.4 (right) shows  and the statistic F (10.4). Let us choose . Since ,  for all shown p (Table D.5). At  the calculated F falls below  for the first time. Hence, from  upwards—even though  keeps on dropping—the data do not offer sufficient statistical support to keep on "inflating" the model.    




10.3 Pearson's  Test

        In
          
        

                  
                  
                

                  
                  
                 Sect. 9.​2 we have seen how the observations , classified in N mutually exclusive bins, can be fitted by a chosen function. Now we are interested in the goodness of this fit. If  are the values of a random variable with the probability density f, ascertaining the quality of the fit amounts to testing the hypothesiswhere  is the chosen density. The same applies in the discrete case, where f and  are probability functions. In the ith bin one expects  counts, while in fact  are observed. The null hypothesis is , and the test statistic is already familiar from (9.​25): (10.5)If  is true,  is distributed according to the  distribution with  degrees of freedom, where p is the number of parameters estimated from the sample and taken into account in the formulation of the hypothesis. This is an important detail; namely, if (10.5) is written such that the ith bin corresponds to a theoretical probability , while it actually contains  counts, the null hypothesis is , where  or, equivalently, , where . The appropriate test statistic is then


Example
Let us revisit the bombing of London during World War II (Example on pp. 133-134). All 576 quadrants with 0, 1, 2, 3, 4 or  hits in each were classified in  bins . From 537 hits we calculated the expected number of hits in any quadrant, . If the hit distribution were Poissonian, with average , we would expect  quadrants with 0, 1, 2, 3, 4 or  hits, respectively. Let us test the hypothesis  with confidence level !

          In the formulation of the hypothesis two parameters have been fixed: the average  and the normalization , hence the statistic (10.5) is -distributed, with  degrees of freedom. We may reject  if the observed  exceeds the critical value  (see Table D.3 for  and ). From the data we get . Hence  can not be rejected: the observed distribution is consistent with the Poisson distribution.    



Example
In an experiment we measure the distribution of events with respect to x that takes the values on the interval . Due to instrumental restrictions we are able to measure only on a restricted range, . A total of  events are classified in  bins, as shown in Fig. 10.5. Is the measured distribution consistent with a uniform or, rather, normal distribution? Consider  and .


Fig. 10.5Pearson's  test for checking the consistency of the binned data with an assumed theoretical model. [Left] Comparison of data to the uniform distribution. [Right] Comparison of data to the normal distribution

There are  counts in all bins. Let us first check the consistency of the data with the uniform distribution, where each bin is expected to contain  counts. By using (10.5) we get . Dividing this by the number of degrees of freedom  yields the so-called reduced value . This should be compared to the critical  at chosen  (Fig. 10.6 or Table D.3). For  we read off , while for  we see that . In either case , indicating that the measured distribution does not match the uniform distribution.
We have narrowed down the acceptance region in Fig. 10.5, as this often happens in practice, forcing us to see the data as nothing but "a constant". What do we obtain with  corresponding to the normal distribution? Assume that only its standard deviation has been determined from the data, so . Now we obtain  or , which is less than  and less than . We can therefore claim, with confidence level at least , that the measured and normal distribution are mutually consistent.    
Fig. 10.6Reduced value of  for Pearson's test as a function of the number of degrees of freedom  at chosen significance . The  symbols at  denote the critical points  and  for the Example in Fig. 10.5, while  indicate the calculated values of  and  corresponding to the comparison of the data to the uniform and normal distribution, respectively


10.3.1 Comparing Two Sets of Binned Data

          The
            
           same test can be used to ascertain the mutual consistency of two sets of histogrammed data of size m and n in N bins. The appropriate test statistic is (10.6)In general . The -test with the chosen significance  is performed as before, by using the  distribution with  degrees of freedom. The values  indicate that the observations  and  do not come from the same distribution law. An example is given
            
           in Problem 10.5.2.



10.4 Kolmogorov-Smirnov Test
Kolmogorov-Smirnov (KS) test [3, 4] is a non-parametric
          
         test used to establish the probability that the observed data (sample) stems from a population distributed according to the chosen continuous theoretical distribution, or that one sample comes from the same population as the other. Of course both the data and the model distribution can be binned and compared by Pearson's test, but a direct comparison has several advantages.

        First
          
          
         we sort the sample  so that . For this sorted set we define the empirical distribution function
This is a monotonously increasing function that jumps upwards by 1 / n at each point . For the data (10.7)it is shown by the "staircase" curve in Fig. 10.7 (left).Fig. 10.7Kolmogorov-Smirnov test. [Left] Data sample (), the corresponding empirical distribution function , model distribution function F and the greatest distance between them, . [Right] Critical values  as a function of n for various statistical significances . The curves correspond to the asymptotic formulas (10.9)

In the basic version of the KS test the empirical distribution  is compared to the model distribution F, shown by the smooth curve in the figure. The null hypothesis isThe test statistic is the maximum distance between these distributions, (10.8)The smaller the distance  (value of statistic ), the better the agreement between  and F, pointing to the acceptance of the null hypothesis. If, however, the calculated  is larger than the critical value  at chosen ,  may be rejected. The critical values are tabulated; a method (and a Matlab code) to compute them can be found in [5]. The symbols in Fig. 10.7 (right) represent  for a few typical . The figure also contains the asymptotic curves (10.9)It is a most charming property of the KS test that the distribution of the statistic  is known and, moreover, does not depend on the distribution F. In the  limit it holds that (10.10)which is suitable for the calculation at large z, while the formis preferable for small z. In either case (10.11)Everyday work is made simpler by the approximation (10.12)which works well already for  and has the correct asymptotics. This can be exploited for the calculation of the critical  for arbitrary, even small n, if tables are not at hand. Namely, one can insert  in (10.10) to obtainWith given n we must figure out  such that the sum on the right equals the chosen  on the left. When we succeed, we have found . (Do this as an exercise! Replace guesswork by bisection.)

Example
At significance  we wish to test the null hypothesis that the sample (10.7) has been drawn from a standard normal population with distribution function F (see (3.​11), ). By using (10.8) we obtain  indicated in Fig. 10.7. The sample size is , and the exact critical value is . By formula (10.9), not expected to apply at such low n, one gets . In either case , the hypothesis can not be rejected. The data are consistent with the normal distribution.    


The advantage that the distribution of  is independent of the model distribution F can be exploited for the determination of confidence regions for the true distribution function of the population, . Namely, realizing thatat chosen significance , this also means thatTherefore, at any x the value of the true distribution function  lies between  and  with probability . In other words, if we use the calculated  to create a band about the empirical distribution, the curve of  lies within this band with probability .

10.4.1 Comparison of Two Samples
Instead of comparing sample and model distributions the KS test can also be applied to two samples, which may even have different sizes [6]. Let the samples of size m and n have the empirical distribution functions  and , respectively. The null hypothesis is that the samples originate in the population with the same distribution function. In this case the test statistic isThe critical values  that the value of the statistic  should exceed in order for the null hypothesis to be rejected at chosen , are tabulated for small m and n. Fortunately, it turns out that (10.11) can be replaced byso the critical values for the two-sample test in the limit of large m and n are just suitably rescaled critical values of the one-sample test: (10.13)For small m and n one may again use the empirical parameterization (10.12) with the replacement .

Example
We wish to examine the null hypothesis that sample (10.7) and sample (10.14)originate in the population with the same distribution function. The samples have equal sizes, . Their histograms are shown in Fig. 10.8 (left), and their empirical distribution functions are shown in Fig. 10.8 (right).
The maximum distance between  and  is . The critical values computed by formula (10.13) are , ,  (the exact ones are 0.350, 0.400, 0.500). Hence the null hypothesis can be rejected at significance  between 1 and 5%.    



Fig. 10.8Comparison of two samples by the Kolmogorov-Smirnov test. [Left] Histogrammed data (10.7) and (10.14). [Right] Corresponding empirical distribution functions and the maximum distance between them



10.4.2 Other Tests Based on Empirical Distribution Functions

          The
            
           KS test is perhaps the most popular test based on empirical distributions, but others exist [7]. Let us name some general considerations regarding their use. Instead of the distance between  and F, for example, one could also measure the average square of the deviation of  from F by calculating the integralwhere w(x) is a weight function. By choosing different w(x) greater emphasis can be given to certain portions of the definition domain of a distribution or its specific aspects. Setting
            
          
for instance, yields the Anderson-Darling (AD) test [8, 9], which is more sensitive to the distribution tails, where F(x) and  are small. The corresponding statistic iswhere  must be sorted, . The values of the statistic in this and other tests is not hard to calculate, but two essential questions always remain: how is this statistic distributed and how can we compute the critical values for a given distribution function F. For the AD test, critical values are available for the uniform, normal, log-normal, exponential and Pareto distributions [10-14].
A final
           warning: the KS test is strictly applicable only if the model distribution F being compared to the empirical distribution  is independent of the observations. This means that the sample being tested should not be used to estimate the parameters of F, say, its expected value or variance. In this case the test works, but both the cumulative distributions (10.10) and the critical values are modified. A possible solution of this problem is the so-called bootstrap resampling: the measured sample is used to generate a large set of new samples, and the test is performed with the entire ensemble. Introductory reading on
            
          

            
             bootstrap methods is offered by [15].



10.5 Problems

10.5.1 Test of Mean Decay Time
(Adapted from [16].) By
            
          

            
             a well-established theory (hypothesis ) a certain quantum-mechanical state should decay with decay time , while according to a competing theory (hypothesis ) it should have decay time ,The actually observed decay times are  and their average is . Find the region where the power of rejecting the null hypothesis with variable  at  is largest, for   and  ! Assume that the density has the form  for both hypotheses.

 The power of the test of  against  is large where the ratio (10.3) is larger than some constant,orThe best rejection region is the interval of values  satisfying this inequality, where  is a constant depending on . The statistic  (with values ) is thus an appropriate statistic to test the true mean , but we must also know its own distribution.

 In the case  the probability density for  is simplyso by (10.1) the rejection region is defined asThe critical value is , and the power of the test of  against  isTherefore, by a single observation, , one may reject  if . Conversely, the probability that  is accepted when actually  is true, is .

 In the case  the distribution of the test statistic  can be approximated by the normal distribution with average  and variance ,The critical value  of the rejection region for  at given  is then defined bywhere  is the distribution function of the standardized normal distribution. At chosen  this means  (see Table D.1) orand the power of the test of  against  isIncreasing the sample size dramatically increases the power of the test: with , for instance, we get  and .


10.5.2 Pearson's Test for Two Histogrammed Samples

          The
            
           same quantity is measured in two laboratories. We have obtained a sample of  observations from laboratory A and  data points from laboratory B, classified in a histogram with  bins shown in Fig. 10.9. The numbers of counts in each bin are represented by the samplesAre these two samples mutually consistent from the perspective of the Pearson's  test with statistical significance ?Fig. 10.9Histograms of  and  observations obtained in two laboratories, compared to each other by the Pearson's  test


 The statistic (10.6) is -distributed, with  degrees of freedom. At chosen  we need its 90. percentile, available in Table D.3: . By using (10.6) with the observed data we get . Since , we can not reject the hypothesis that the observed distributions are consistent. What if we are a bit less demanding and assume ? In this case we obtain , which is a tad below the observed : at confidence level  we can just claim that the distributions are mutually inconsistent.


10.5.3 Flu Medicine
A classical pharmaceutical problem is the test of a drug intended to shorten the duration of flu symptoms. In untreated patients their average duration is , with standard deviation . The medicine is given to  random patients when they develop first symptoms. In this sample the average symptom duration is . Is this outcome statistically significant at
           significance ?

 We are testing the null hypothesis  (medicine has no statistically significant effect) against  (medicine shortens the duration of symptoms). Assume that the distribution of  is normal; by (7.​6) and (7.​7) it can be assigned the mean  and variance . Since the alternative hypothesis has the form , the rejection region is at the lower end of the real axis, strictly speaking , but practically , as the duration of symptoms can not be negative. We therefore seek  such that  if  is true. In terms of the standardized variable  this meanswhere  is the distribution function of the standardized normal distribution. HenceBecause , the hypothesis  (i.e. that a relatively small average duration has only been observed by chance) can be rejected. The efficacy of the medicine is indeed statistically significant.


10.5.4 Exam Grades

          The
           eternal professor (not student) question: are exam grades normally distributed?  The grades of a small sample of  students (in ) are . Is this result compatible with a normal distribution with mean  and standard deviation , at significance ?  From a large population we take a sample of  exams of students majoring in A (grades ) and  exams of students majoring in B (grades ). Is there a statistically significant difference between the A and B students, at ?

 We use the Kolmogorov-Smirnov test.  The null hypothesis is , where F is the distribution function of . The maximum distance between  and F is . The exact critical value is , while the asymptotic formula (10.9) gives 0.387. Since ,  can not be rejected, which speaks in favor of the normal distribution of grades.

 The null hypothesis is that the samples stem from the same population, thus within statistical fluctuations their empirical distribution functions should also be the same, . The maximum distance between  and  is . The exact (tabulated) critical value is , while the asymptotic formula (10.13) gives 0.879. Since ,  can not be rejected.



References


1.
W.J. Conover, Practical Nonparametric Statistics, 3rd edn. (Wiley, New York, 1999)


2.
F. James, Statistical Methods in Experimental Physics, 2nd edn. (World Scientific, Singapore, 2006)MATH


3.
A. Kolmogorov, Sulla determinazione empirica di una legge di distribuzione, Giornalo dell'Istituto Italiano degli Attuari 4, 461 (1933). Translated in A.N. Shiryayev (Ed.), Selected works of A.N. Kolmogorov, Vol. II (Springer Science+Business Media, Dordrecht, 1992) p. 139


4.
N. Smirnov, Sur les écarts de la courbe de distribution empirique. Rec. Math. 6, 3 (1939)MATH


5.
S. Facchinetti, A procedure to find exact critical values of Kolmogorov-Smirnov test, Statistica Applicata—Ital. J. Appl. Stat. 21, 337 (2009)


6.
J.W. Pratt, J.D. Gibbons, Concepts of Nonparametric Theory (Springer, New York, 1981)CrossRefMATH


7.
M.A. Stephens, Tests based on EDF statistics, in Goodness of Fit Techniques, ed. by R.B. D'Agostino, M.A. Stephens (Marcel Dekker, New York, 1986), pp. 97-194


8.
T.W. Anderson, D.A. Darling, Asymptotic theory of certain "goodness of fit" criteria based on stochastic processes. Ann. Math. Statist. 23, 193 (1952)MathSciNetCrossRefMATH


9.
T.W. Anderson, D.A. Darling, A test of goodness of fit. J. Am. Stat. Assoc. 49, 765 (1954)MathSciNetCrossRefMATH


10.
M.A. Stephens, EDF statistics for goodness of fit and some comparisons. J. Am. Stat. Assoc. 69, 730 (1974)CrossRef


11.
M.A. Stephens, Asymptotic results for goodness-of-fit statistics with unknown parameters. Annals Stat. 4, 357 (1976)MathSciNetCrossRefMATH


12.
M.A. Stephens, Goodness of fit for the extreme value distribution. Biometrika 64, 583 (1977)MathSciNetCrossRefMATH


13.
M.A. Stephens, Goodness of fit with special reference to tests for exponentiality, Technical Report No. 262, Department of Statistics, Stanford University, Stanford, 1977


14.
M.A. Stephens, Tests of fit for the logistic distribution based on the empirical distribution function. Biometrika 66, 591 (1979)MathSciNetCrossRefMATH


15.
A.C. Davison, D.V. Hinkley, Bootstrap Methods and their Application (Cambridge University Press, Cambridge, 1997)CrossRefMATH


16.
A.G. Frodesen, O. Skjeggestad, H. Tøfte, Probability and statistics in particle physics (Universitetsforlaget, Bergen, 1979)











Part IIISpecial Applications of Probability










© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_11




11. Entropy and Information 





Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
Entropy is introduced as a concept that quantifies the amount of information contained in a signal or in its corresponding probability distribution. It is defined for discrete and continuous distributions, along with its relative counterpart, the Kullback-Leibler divergence that measures the "distance" between two distributions. The principle of maximum entropy is stated, paving the way to the derivation of several discrete maximum-entropy distributions by means of Lagrange multiplier formalism: the Maxwell-Boltzmann, Bose-Einstein and Fermi-Dirac distributions. The relation between information and thermodynamic entropy is elucidated. A brief discussion of continuous maximum-entropy distributions is followed by presenting the method of maximum-entropy spectral analysis.




11.1 Measures of Information and Entropy
One of the possible
         paths to the definition of entropy
          
         leads through the concept of information. This connection can be elucidated by studying a discrete random variable X that can take
          
         finitely many values  with probabilities , where  and . Imagine that each outcome of the experiment with this variable, say, the event () occurring with probability , brings some information I(p). The value x can be seen as a "signal" or "message" carrying information I(p).
How can its quantity be measured? Intuitively it is clear that any measure of information must have logarithmic nature [1, 2]. If events with probabilities  and  occur independently (probability ), the information of such a combined outcome should equal the information supplied by single outcomes: the sentences "it snows" and "it is Friday" together carry as much information as "it snows and it is Friday". Hence, a measure of information should be additive,Besides, we wish the function I(p) to be non-negative, , monotonous, , and continuous: small changes in p imply small changes in I(p). An obvious candidate is the functionand in fact it can be shown that it is the only possible [3]. A measure defined in this way has the sensible properties  (a certain event carries no information) and  (a highly improbable event brings lots of information). The arbitrary real constant C can be hidden in the base of the logarithm by using , and is therefore irrelevant. If we adopt  and , information is measured in bits. If we choose  and , it is measured in nats, differing from bits only by the factor .
There is only one step from information to information entropy. If individual values  occur with probabilities , , the average quantity of received or "created" information is (11.1)This "weighting scale" of information is called the entropy of a finite probability distribution due to Shannon [3]. How can it be interpreted?
The essence of any random process is uncertainty. The outcomes are not predictable, but each received signal (a single value ) reduces the uncertainty we had prior to receiving it. The expression (11.1) can therefore be understood as a measure of such uncertainty. We must realize that H measures information entropy that should not be confused with the thermodynamic entropy S. In the following 'entropy' means information entropy.1

The measure (11.1) has many convenient properties. The uncertainty of a certain event is zero, . The uncertainty of an impossible event is also zero, , as nothing is unclear in an event that never occurs, besides, formally . The value of the entropy depends only on the probability distribution  and no other properties that might be assigned to the signal. It is independent of the permutations among  and does not change if n events are augmented by an impossible event, . Entropy is maximal when we are "maximally uncertain", i.e. when all outcomes are equally likely:  (uniform distribution). For any other distribution or under any condition imposed on  the entropy decreases (see Example on p. 288 and [5]).

Example
In tossing a fair coin heads and tails are equally probable: .  The entropy of the random variable with such distribution isHence, tossing a coin supplies one bit of information on average. If the coin is unfair such that, for instance,  and , we getThe entropy has decreased as the coin prefers a specific side, reducing the uncertainty. The dependence of H on  is shown in the above figure.



Example
Throwing a die has the probability distribution , . (We "know" that. How exactly this follows from the principle of maximum entropy is discussed in Sect. 11.2.) The entropy of this uniform distribution isTherefore, throwing a die yields about 2.585 bits of information on average. But if someone tells us, say, that the number of dots is odd, the sample space shrinks since there are only three possible outcomes, hence  andThe entropy has diminished as three outcomes instead of six imply less "uncertainty", less "indefiniteness". The restriction to odd number of points on average means  bit of acquired information.



11.1.1 Entropy of Infinite Discrete Probability Distribution
If the partial sums  converge when , then (11.2)represents the entropy of an infinite
            
           discrete probability distribution.

Double example For the geometric distribution  () the sum (11.2) is not difficult to calculate: . So its entropy is 2 bits.
With , where   and , we are not that fortunate: even though  and the distribution is normalized, , we realize that . The entropy of such a distribution does not exist (or we say that it has infinite entropy). 



11.1.2 Entropy of a Continuous Probability Distribution
The entropy of a continuous probability distribution
            
           is defined by analogy to the discrete formulation (11.1). If X is a continuous random variable with the probability density f, the entropy of its distribution is (11.3)Note that we are using the notation H(X) instead of, say, H(f). Information and entropy may be assigned both to the random variable itself or to its probability distribution. There is no general consensus about that, so we will adopt the notations I(X) and  as well as H(X) and  in the discrete case—just as H(X) and H(f) in the continuous case—as equivalent.

Example
The entropy of the uniform distribution U(a, b) or the uniformly distributed continuous random variable  is (11.4)The result clearly depends only on the difference . This means that all uniform distributions with the same spacing have the same entropy.



Example
The distribution with the probability densityis normalized, , but . Its entropy is infinite (or: does not exist).




11.1.3 Kullback-Leibler Distance
Imagine a time series
          

                    
                    
                   (signal, sequence) with values distributed according to a discrete probability distribution . The recorded signal contains a certain information. We measure another sequence with values corresponding to the distribution . Has the new sample brought any additional information with respect to the original data set? In other words, how "distant" to each other—in the entropy sense—are the two distributions? A measure of this "remoteness" is the Kullback-Leibler distance or divergence, sometimes also called relative entropy [6]. For discrete distributions  and  it is defined aswhile for continuous distributions with densities p and q it is formulated asThe concept of distance refers to the property , where the equality sign applies precisely when . Moreover, .Fig. 11.1[Left] Distributions of vertical seismic velocities (histograms) and the corresponding normal distributions as best fits to the data (brighter curves). [Right] The Kullback-Leibler distance between the measured distributions and the normal distributions from the left panel, , 



Example
A seismological study
             [7] resulted in the distribution of vertical components of seismic velocities. The signals (348 time series lasting 68 hours each) were frequency-filtered, so that they corresponded to oscillations with periods (5- and (10- on the time scale. The obtained distributions  and  shown in Fig. 11.1 (left) were fitted by normal distributions  and . For all 348 samples, relative entropies  and  were calculated. As shown in Fig. 11.1 (right), the "distance" of the second distribution from the normal is much larger, i.e. it is "much less Gaussian".





11.2 Principle of Maximum Entropy
We have seen that in a random
           process the "uncertainty"— i.e. its information
          
         entropy—is largest when all of its outcomes are equally
          
         probable. This realization has been succinctly expressed already by Laplace in his principle of insufficient reason or principle of indifference: in the absence of a specific reason to distinguish between two or more outcomes, the best strategy is to treat them as equally probable.
The principle of maximum entropy builds upon and upgrades this guideline: in any circumstance where incomplete information is available—for instance, in a sample of observations—we strive to quantitatively describe the data by a probability distribution that is consistent with all known information, yet at the same time as "nonrestrictive" as possible, "uncertain", "free" with respect to the unknown information.2 Laplace's argument offers only the negative lever-arm "in the absence of ...", while the principle of maximum entropy offers clearly defined, positive tool in the sense of determining the distribution that is "as nonrestrictive as possible". It is precisely this aspect that removes the flavor of arbitrariness from Laplace's principle [8, 9].
We are using the words like "lever-arm", "aspect", "flavor"—all loose, non-mathematical concepts! The principle of maximum entropy can not be strictly "proven", yet de facto practically all known probability distributions follow from it in a very natural manner. In contrast to Laplace, it has the important property that each outcome not absolutely excluded by a known piece of information is assigned a non-zero contribution. Initial reading on maximum entropy is offered by the review article [10]; for a very mathematically tinted discussion see [11].

Example
According to Laplace, throwing a fair die corresponds to the uniform distribution , . One can reach the same conclusion by invoking the principle of maximum entropy. We maximize  with the condition . This can be done by the classical method of Lagrange multipliers. We calculate the first derivative of the Lagrange functionwith respect to  and set it to zero. It follows that  and . Hence  do not depend on i (that is, they are all equal) and their sum must be 1. Therefore , see Fig. 11.2 (left). In the following this Example will be expanded into a more general tool based on Lagrange multipliers. 



Fig. 11.2Discrete distribution of values in throwing a fair die and the principle of maximum entropy. [Left] Normalized distribution with no restrictions (constraints). [Right] Normalized distribution with constraints (11.11) and (11.12)



11.3 Discrete Distributions with Maximum Entropy

11.3.1 Lagrange Formalism for Discrete Distributions
On any discrete probability
            
            
           distribution we may
            
           impose additional
           conditions or constraints. The constraint
           we indeed must impose is the normalization requirement . But we may also include conditions of the form
          
 (11.5)An example of such constraint is the requirement that the average number of dots in throwing a die is 4, which is expressed as . The distribution that maximizes the entropy at given constraints can be calculated by the general method of Lagrange multipliers. To each of the  constraints (normalization plus m conditions of the form (11.5)) we assign its multiplier and minimize (11.6)In the second term we have subtracted 1 from  in order to cancel the 1 from the derivative of the first sum. No harm has been done:  is just as good a multiplier as . We calculate the derivative of  with respect to  and set it to zero: (11.7)It follows that (11.8)We insert this in  and extract from it the factor  called the phase sum or the partition function:
                    
                  

                    
                    
                  

                    
                  
 (11.9)We take the logarithm of the phase sum,calculate its derivative with respect to  and, finally, multiply the expressions in the numerator and denominator of the obtained fraction by :This is a system of m equations for m unknowns ,  that needs to be solved for better or worse. The calculated multipliers yield the final formula for individual probabilities corresponding to the maximum-entropy distribution: (11.10)


Example
A die has been tweaked such that the probability of obtaining three dots is twice the probability of getting two, and the probability of observing four dots is twice the probability of finding five. Calculate, with these restrictions, the probability distribution of the number of dots  that is consistent with the maximum-entropy assumption!
Normalization and the two specified conditions introduce the constraints (11.11)
 (11.12)We see that ,  and . The appropriate Lagrange function isbut there is no need to calculate its derivative (11.7) again, since (11.9) immediately gives us the partition function (11.13)and thence a system of two equations for  and :Its solution is , so that (11.13) yields . The final result then follows from (11.10):See Fig. 11.2 (right) and think: we certainly have not anticipated the distribution  () after all this rattle; but why is the answer not simply ? Why did the probabilities  and  change from their "Laplacian" values of 1 / 6 even though the constraints (11.11) and (11.12) do not address them at all? 




11.3.2 Distribution with Prescribed Mean and Maximum Entropy
Among all finite discrete distributions with probabilities  () and prescribed arithmetic mean  () the one with the maximum entropy is the power distribution. One can see that if one maximizes the entropy  with the constraints (11.14)i.e. the Lagrange functionFrom  it follows that , where . When this is inserted in (11.14), two equations for the unknowns a and b follow:The system is solved numerically. Taking  and , 10, 15.5 yields , (0.09181, 0.9229), (0.03333, 1.0000), respectively. The obtained power distributions with calculated parameters are shown in Fig. 11.3.Fig. 11.3Finite discrete distributions () with prescribed arithmetic means , 10 and 15.5 and maximum entropy. In the last case the power distribution has degenerated into the uniform distribution



11.3.3 Maxwell-Boltzmann Distribution
Assume that a physical
             system possesses energy levels with single-particle energies  that particles occupy with probabilities . Let the expected value of the energy, , be prescribed. What is the probability distribution of particles that is consistent with the assumption of maximum entropy? We must maximize the entropy with the constraints (11.15)that is, the Lagrange functionThis means  or , where . When this is inserted in (11.15), it follows that (11.16)If we set , where  is the Boltzmann constant, these expressions specify the Maxwell-Boltzmann distribution (see also Sect. 11.3.4).

Example
We discuss a system
             with three () discrete energy levels ,  and , shown in Fig. 11.4 (left). We are interested in the level occupation probabilities  at three inverse values of the  parameter, say, ,  and . By using (11.16) we obtainThe probabilities  are shown in Fig. 11.4 (right). The average energy  in the case of , lying just slightly below , is denoted by the dashed line in the level scheme (left part of Figure).


Fig. 11.4Maxwell-Boltzmann statistics in a three-level system. [Left] The circles approximately denote the distribution of  particles corresponding to  (, , ), and the dashed line indicates the average energy . [Right] The maximum-entropy probability distribution for various values of . At high T the distribution approaches the uniform distribution!

The reverse task is also interesting: to what temperature must the system be heated that the average energy will equal a specific value? If, for example, we wish to attain , we must set , i.e. crank up the heater to , where . 



11.3.4 Relation Between Information and Thermodynamic Entropy
The mathematical form
            
           of the theory of information entropy is identical to the formulas for entropy obtained in the framework of statistical mechanics. In other words: the rules of statistical mechanics are principles of statistical inference in physical garb. Let  be the energy levels of a physical system with parameters  specifying quantities like volume, external electro-magnetic field, gravitational potential, and so on. At given mean energy  the probabilities  for the occupation of levels  are given by a special form of (11.8), readily identified as the Maxwell-Boltzmann distribution if one identifies . Similar arguments [8, 9] can be used to accommodate free energy
          

                    
                    
                  
 (11.17)in the framework of statistical inference, as well as thermodynamic entropy,formally differing from the information entropy only by the Boltzmann constant providing the appropriate units.
From (11.17) it also becomes clear why in the three-level system in Fig. 11.4 at high temperatures particles fail to accumulate at the highest level as one might intuitively expect, but rather their distribution approaches the uniform distribution (,  and  all tend to 1 / 3). Minimizing the free energy  at  means minimizing the internal energy U, so at  indeed all particles occupy the lowest level. But at high T one has , so in this limit minimizing F implies maximizing S—thus the uniformity of the
          

                    
                  

                    
                    
                  

                    
                    
                   distribution.

Example
A pair of elementary magnetic dipoles (e.g. electrons with magnetic moments  treated classically) is exposed to a homogeneous external field . Each dipole can only be oriented along  or opposite to it, so four configurations are possible, shown in Fig. 11.5 together with their magnetic energies . At what temperature the average energy of this system (at given magnetic field density ) is equal to  and the entropy maximal?
                      
                    
Fig. 11.5Configurations of a pair of magnetic dipoles (with individual magnetic moments ) in an external field and the corresponding magnetic energies

From (11.16) we obtain  orSolving this equation for  we get  or . At this temperature the expected occupation probabilities are ,  and . 




11.3.5 Bose-Einstein Distribution
Let us discuss a more
             complex problem of N particles that may occupy n energy levels with energies , . Let  be the conditional probability that the ith level contains j particles,   (the number of particles on the individual level is not restricted). The condition—namely that the system is in the ith state—is given by the prior probability , so that . If the probabilities  are unknown, we may recall Laplace and simply set . The distribution of particles , consistent with the requirement of maximum entropy, is then found by maximizing the entropy with the constraints (11.18)where  is the prescribed average system energy. When the system is in the ith state, its entropy is , so the total entropy isThis can also be seen if the expression for entropy is rewritten aswhere we have used , so the first term plays no role in taking the derivative with respect to . The Lagrange function to be minimized is then (11.19)where  and  are additional Lagrange multipliers. Taking the derivative with respect to  we get . From here we express  and insert it in (11.18). A brief calculation [5] then leads to the occupation probabilities (11.20)as well as to the formulas for the number of particles and system energy, (11.21)where . We also set , where  is the chemical potential. The expected number of particles on the ith level is (11.22)where  and  must be determined from (11.21) at known N and . One also hasThe obtained distribution is suitable for the description of particles with integer spin (bosons), e.g. photons, atoms with even numbers of electrons, and nuclei with even numbers of nucleons.
If we are dealing with bosons whose number is not conserved (virtual photons, phonons, magnons), there is no restriction on the particle number N, hence the third constraint in (11.18) and the third term in (11.19) are superfluous. In this case  and therefore the chemical potential also vanishes: .


11.3.6 Fermi-Dirac Distribution
The Fermi-Dirac distribution
            
           describes fermionic systems, i.e. systems of particles with non-integer spins . For such particles Fermi's rule says that the same energy level can not be occupied by two (or more) particles: the level can be vacant or inhabited by precisely one particle. To determine the corresponding maximum-entropy distribution we can exploit our previous derivation, where we restrict  in (11.18). The expression for occupation probabilities still has the form (11.20), while (11.21) is replaced byand (11.22) by




11.4 Continuous Distributions with Maximum Entropy
Maximum-entropy continuous distributions
          
         with imposed additional constraints can also be handled
          
          
         by the Lagrange multiplier method. We discuss a single
          
          
         paradigmatic case: a distribution whose variance  is prescribed and has maximum entropy. One must maximize (11.3) with the constraints  and , where  is a free parameter. By analogy to the discrete case (11.6) the Lagrange function is The variation of the first term isBy the variation of the whole Lagrange function, which is set to zero,we get  or . We insert this function into the constraint equations:It follows that , then the first equation gives . Hence the desired density f corresponds to the normal distribution (3.​7).
Two further interesting results refer to case when the mean is prescribed and the case that the definition domain of X is a finite interval. The following theorems on maximum-entropy distributions applicable to continuous random variables X with density f and entropy H(X) are given without proof:1.If , then H(X) exists and  holds true, where the equality applies only if —in this case the mean can be anything, as only an offset along the x-axis is involved. 2.If X is a non-negative random variable ( for ) with finite mean , then H(X) exists and  holds true, where the equality applies only if . 3.If the variable X is restricted to the interval [a, b], i.e.  for  and , then H(X) exists and  holds true, where the equality applies only if —see (11.4). 


Example
Let us fix the variance of a continuous distribution, , and check that the exponential and uniform distributions with such variance have lower entropy than the normal with the same variance. By Theorem 1 the normal distribution has entropy . The exponential distribution has  (see (3.​4) and Table 4.​1 and set ). By Theorem 2 its entropy is . The uniform distribution on [a, b] has  (Table 4.​1), so by Theorem 3 its entropy is . 




11.5 Maximum-Entropy Spectral Analysis
The maximum-entropy principle
          
         also
          
         leads to a powerful
          
         tool for spectral
         analysis of time
          
         series. Suppose we have a set of (generally complex) observations  at times . We are interested in the probability distribution of these values, f, that minimizes the entropy . From the data  (values of ) we first form our fundamental observables, the temporal auto-correlations
        
Their expected valuesmay be understood as continuous analogues of discrete constraints (11.5). If  are complex,  are also complex in general, but it is readily noticed thatAs we shall see, this symmetry is essential if we wish that certain quantities—for instance, the frequency spectrum of the measured signal—is purely real. We must maximize the Lagrange functionwhere  and  are unknown Lagrange multipliers. The second sum runs from , since the auto-correlation  is purely real, hence . Thereforewhere we have denoted  and , so that . We know how to solve the problem of maximizing such a Lagrange function from the discrete case: see formulas (11.9) and (11.10). The probability density and the partition function have the formwhile the constraint equations are (11.23)The sum  in the arguments of the exponentials can be written aswhere B is a banded (-diagonal),  Toeplitz matrix with the elements
          
        
 (11.24)The properly
          
         normalized probability
          
         density we are seeking is
          
          
         therefore (11.25)which is the density of the multivariate normal distribution (4.​23). Note that B is Hermitian, . It must also be positive definite, otherwise f does not exist.

11.5.1 Calculating the Lagrange Multipliers
The entropy corresponding to the obtained probability density is (11.26)Here we have used the relation  which is easy to prove.3 The phase sum is nothing but the multi-dimensional Gauss integralso thatUsing the relation between the determinants  or (11.27)(11.23) can be written asBy Szegő's theorem [12] at fixed m we have
            
          
 (11.28)whereFor  this leads to the approximationThis is the key formula connecting the auto-correlations  to the Lagrange multipliers  contained in . Let us denote  and write (11.29)where . Calculating  requires an integration along the unit circle in the complex plane: (11.30)It turns out [13] that g(z) can be factorized as (11.31)The first factor, G(z), has zeros only within the unit circle, and the second factor only outside of it; the function g(z) has m zeros within the unit circle, m zeros outside and one on the circle. By the convolution of  and  we getThis can be recast as the Yule-Walker system of equations
            
          
orwhere  for all matrix elements. The last step is to use the obtained  to calculate the Lagrange multiplierswhere  and , so that  and . The matrix B from the definition (11.24) is thereby uniquely determined, and with it the probability density (11.25).


11.5.2 Estimating the Spectrum
The power spectral density (PSD)
            
           of a signal is defined aswhere  and  are the auto-correlations of an infinite signal. In the true world we usually only know its finite sample, so the obtained formulas will offer just an estimate of the true frequency spectrum. With the calculated  (), the solutions of the Yule-Walker system, we define the functionFrom (11.31) it follows thatOn the other hand, (11.30) can be used to writeThe power spectral density can therefore be estimated asThis formula (up to a multiplicative constant) is usually seen in the form (11.32)where  (),  and . In signal-processing theory—see, for instance, [14]—the parameter  represents the variance of the Gaussian noise generated by the signal through the feedback
          

            
             loop with filter .
The described spectral estimation tool is called the auto-regression model; if the process (the signal) has a Gaussian nature, it is also known as Maximum-Entropy Spectral Analysis (MESA) [15]. How MESA works in practice is demonstrated by
          

             the following Example.

Example
We use the auto-regression method to analyze the signal (11.33)where  and , thus . The time series contains three frequency components with frequencies , 0.2 and 0.3 with amplitudes 1, 2 and 3, respectively. Besides, it is very noisy (last term in (11.33)). The sample of the first 500 values of the signal is shown in Fig. 11.6 (left).Fig. 11.6[Left] The first 500 values of the signal (11.33). [Right] The estimate of the spectrum by using the auto-regression method for two different m


The estimate of the spectrum, calculated by (11.32) for two different m, is shown in Fig. 11.6 (right). With increasing m the spikes become sharper, but spurious peaks start to appear that are not expected to be present in the spectrum and represent noise. 


By using the entropy expression and the relation between the determinants of B and C one can derive an interesting formula relating entropy to power spectral density. At large enough T Szegő's theorem (11.28) can be understood aswhere we have used the relation (11.29) between p and . We insert this in (11.26) and consider (11.27); see also definition (11.24). It follows thatSince , this also meanswhere  is the change of entropy per unit time (entropy rate). This is the average "speed" of acquiring information in the measurement of a spectrum.



References


1.
H. Nyquist, Certain factors affecting telegraph speed. Bell Syst. Tech. J. 3, 324 (1924)CrossRef


2.
R.V.L. Hartley, Transmission of information. Bell Syst. Tech. J. 7, 535 (1928)CrossRef


3.
C.E. Shannon, A mathematical theory of communication, Bell Syst. Tech. J. 27 (1948) 379 (Part I) and 623 (Part II)


4.
M. Tribus, Thirty years of information theory, in Maximum Entropy Formalism, ed. by R.D. Levine, M. Tribus (MIT Press, Cambridge, 1979), pp. 1-19


5.
J.N. Kapur, Maximum Entropy Models in Science and Engineering (Wiley Eastern Ltd., New Delhi, 1989)MATH


6.
S. Kullback, R.A. Leibler, On information and sufficiency. Ann. Math. Stat. 22, 79 (1951)MathSciNetCrossRefMATH


7.
S.M. Hanasoge, M. Branicki, Interpreting cross-correlations of one-bit filtered seismic noise. Geophys. J. Int. 195, 1811 (2013)ADSCrossRef


8.
E.T. Jaynes, Information theory and statistical mechanics. Phys. Rev. 106, 620 (1957)ADSMathSciNetCrossRefMATH


9.
E.T. Jaynes, Information theory and statistical mechanics. II. Phys. Rev. 108, 171 (1957)ADSMathSciNetCrossRefMATH


10.
S. Pressé, K. Ghosh, J. Lee, K.A. Dill, Principles of maximum entropy and maximum caliber in statistical physics. Rev. Mod. Phys. 85, 1115 (2013)ADSCrossRef


11.
P. Harremoës, F. Topsøe, Maximum entropy fundamentals. Entropy 3, 191 (2001)ADSMathSciNetCrossRefMATH


12.
G. Szegő, Ein Grenzwertsatz über die Toeplitzschen Determinanten einer reellen positiven Funktion. Math. Ann. 76, 490 (1915)MathSciNetCrossRefMATH


13.
E.T. Jaynes, On the rationale of maximum-entropy methods. IEEE Proc. 70, 939 (1982)ADSCrossRef


14.
P. Stoica, R. Moses, Spectral Analysis of Signals (Prentice-Hall Inc, New Jersey, 2005)


15.
J.P. Burg, Maximum entropy spectral analysis, lecture at the 37th annual international meeting, Soc. Explor. Geophys., Oklahoma City, Oklahoma, 31 Oct 1967




Footnotes


1


Shannon had second thoughts on introducing the concept of entropy to information theory. It is said that his decision was stimulated by the mathematician John Neumann who said [4]: "Firstly, you have got the same expression  as is used for entropy in thermodynamics and, secondly and more importantly, since even after one hundred years, nobody understands what entropy is, if you use the word entropy you will always win in an argument!" See also Sect. 11.3.4.

 



2


As shown below, the simplest example is the uniform distribution: if no additional condition is imposed on the distribution apart from normalization, the distribution with the maximum entropy is precisely the uniform distribution. (This applies in both the discrete and continuous cases.)

 



3


Let  be a d-dimensional vector of complex random variables with mean  and covariance matrix . Then any constant symmetric matrix M satisfies . In our case (see (11.25)) B in the density  is Hermitian, but by decomposing  and transforming  we can write , so that . This is the density of the multivariate normal distribution with mean  and covariance matrix . It follows that .

 













© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_12




12. Markov Processes 





Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
Markov processes are introduced as memoryless stochastic processes and classified in four classes based on whether the time parameter is continuous or discrete and whether the sample space is continuous or discrete. Two of them are treated in more detail: discrete-time ("classical") Markov chains and continuous-time, continuous-state Markov processes. Long-time behavior of the chains is discussed, establishing the conditions for the formation of equilibrium distributions. In the continuous case, the Markov propagator is defined along with a discussion of moment functions, characterizing functions, and time evolution of the moments. Two particular Markov processes, the Wiener and the Ornstein-Uhlenbeck process, are given special attention due to their relevance for the study of diffusion.




      Imagine
        
       a sequence of random
        
       variables X(t) describing the state of a dynamical system at times t. In Sect. 6.​7 such a sequence was called a random or stochastic process [1]. A special class of random processes consists of processes in which the state of the system at current t depends only on its state just prior to t, while all earlier states are irrelevant for its
       time evolution: the process is memoryless. Such processes are called Markov processes after the
        
       Russian mathematician A.A. Markov (1856-1922).
Markov harnessed statistical methods to analyze letter sequences in Pushkin's poem Eugene Onegin: he was seeking probabilities of a vowel preceding a consonant, a vowel appearing after the consonant, and so on, as well as the answer to the question whether such estimates change with the length of the analyzed text and whether Pushkin's "statistical profile" is perhaps unique. In 1913 he presented his findings to the Imperial Academy of Sciences in St. Petersburg [2] and thereby initiated a completely novel field of research [3, 4].
Markov processes are divided in four families based on whether the time parameter is continuous or discrete and whether the values of X are continuous or discrete. In the following we shall discuss two combinations: discrete variables with discrete time steps—such processes are known as discrete-time Markov chains—and continuous variables with a continuous time evolution.

12.1 Discrete-Time (Classical) Markov Chains

        A
          
         classical Markov chain
          
          
         is a random process X(t) in a finite discrete state space  with discrete time  For simplicity we denote . The "memoryless" feature of the process is expressed by the relationIn plain words: the probability of arriving to the state-space point  at time  is independent of all previous points except  in which the system dwelled at time t. Abbreviating  and , the right-hand side
         expresses the conditional probability for the transition from  to  in a single time step, the so-called single-step transition probability,
                  
                  
                
 (12.1)which is at the heart of any Markov chain. The basic properties of probability demand . In the following we shall only discuss time-homogeneous chains, in which the transition probabilities do not depend on time,Analogously one defines the probability for the transition from state i to state j after n time steps, known as the n-step transition probability,
        For
           different n
         these are related by the Chapman-Kolmogorov equation [5] (12.2)How can it be elucidated? The transition from state i to state j in  steps occurs in n steps from the initial state i to the intermediate state k with probability , and thence in m steps to the final state j with probability . The events "go from i to k in n steps" and "go from k to j in m steps" are independent. The probability for the whole transition is then obtained by the total probability formula (1.​15) by summing over all intermediate states k.
According to (12.1) the transition probabilities can be organized in the so-called stochastic matrix , while the distribution of states which the system occupies
          
         at time t, can be summarized by the row-vector
          
        
Sincethe distribution  at time  can be calculated from  at time  by simply multiplying . Equation (12.2) then also tells us that the mapping between  and the distribution  at an arbitrary later time is as simple as it gets, namely (12.3)where t is the power of the matrix . Therefore the dynamics of the probability distribution of the chain is completely determined by the probability distribution of the initial state  and the one-step transition probabilities .

12.1.1 Long-Time Characteristics of Markov Chains

          If
            
            
           there is a
            
           non-zero probability
            
            
           of arriving to any
            
            
           state in  from any other
            
           state in  we say that the Markov chain is irreducible. It is also important whether one can return to the initial state or not. A state is periodic if it can be revisited by paths with the numbers of steps whose greatest common divisors are greater than 1. In the opposite case, the state is aperiodic. A state is reproducible if we certainly return to it in finite time. If the chain is irreducible on the whole  and all states are aperiodic and reproducible, we call it ergodic.
The bombardment with all these definitions has more than just academic purpose, as it leads to the important concept of equilibrium distributions; see also Sect. 6.​4. The equilibrium distribution is defined by  or (12.4)Ergodic chains possess a limit distribution which is equal to the equilibrium distribution: (12.5)No condition is imposed on the initial distribution (index i), so the attribute "equilibrium" is justified. In finite  all states are reproducible,1 hence eigenvectors and eigenvalues of  can be found: the vector  representing the equilibrium distribution is the left eigenvector of  with the largest possible eigenvalue . Each initial distribution  converges to the equilibrium as , , where  () is the second largest eigenvalue of .

          In
            
           finite spaces  the Perron-Frobenius theorem [6] guarantees that for
            
           irreducible chains there exists a vector  with components (12.6)representing the equilibrium distribution regardless of the initial state i. For finite  the methods (12.4) and (12.6) to compute the equilibrium distribution are equivalent.

Example
Imagine a binary
             communication channel shown in the figure.  Each node receives a signal (bit) and passes it on to the next node with some probability, or there may be an error in the process so that the opposite bit is forwarded. What happens at each node depends on its state: if the node receives bit 0, it is forwarded correctly with probability , while the probability of forwarding the wrong bit, 1, is . If it receives bit 1, the probability of correct transmittal is , while the probability of passing on the wrong value, 0, is . (Topologically it all looks like Pushkin's vowels and consonants!) Such error-prone communication can be modeled by a discrete-time Markov chain on the state-space . The variable  represents the bits 0 or 1 leaving the t-th node of the channel. Let us choose  and , so the stochastic matrix isLet  be the initial state—the channel input is a symmetric mixture of bits 0 and 1. We are interested in the behavior of the chain at large "times", i.e. the distribution of states 0 and 1 at the output of the channel containing very many nodes. We could use formula (12.3),but it requires us to compute the t-th power of . Fortunately, our linear algebra professor tells us that  can be written as , whereand ,  are the eigenvalues of . They can be calculated by solving the secular equation , whence  and . ThereforeBy using the specified parameters a and b we getHence the distribution of states at the tth node isAll we need to do now is , amounting to (12.7)It is no wonder that the chain "drifts" to a regime where the probability for output 0 is larger than the probability for 1, since . We could have even guessed the values (12.7) by reasoning that  and .    



Example
(Adapted from [7].) Can
              
             the long-time analysis of a Markov chain be used to predict weather? Imagine a simple model that knows only three weather conditions: sunny (s), cloudy (c) and rainy (r), so the state space is . Let the time step be one day. Assume that the probabilities that tomorrow will be sunny, cloudy or rainy if the weather today is sunny, are 0.6, 0.3 and 0.1; the probabilities that the next day will be sunny, cloudy or rainy if the weather today is cloudy, are 0.2, 0.3 and 0.5; the probabilities of having sun, clouds or rain tomorrow if it is raining today, are 0.4, 0.1 and 0.5, as shown by the graph. We arrange all nine conditional probabilities in the stochastic matrix where the lines and rows correspond to today's and tomorrow's weather conditions, respectively: (12.8)  

What is the probability of having rain in two days if it is cloudy today? It is given by the Chapman-Kolmogorov
              
             equation (12.2):And what is the probability of rain three days, five days   from now if it is cloudy today? The answer always sits at the same spot: in the matrix element at the second-row, third-column crossing of the matrices , , and so on:
Fig. 12.1Time evolution of three components of the probability distribution  at two different initial distributions,  and . In both cases the Markov chain with the stochastic matrix (12.8) converges to the stable distribution (12.9) in just a few steps

We see that the columns of ever higher powers of  become more and more constant (independent of the rows): at long times we approach the equilibrium distribution regardless of the initial state (Fig. 12.1). But formula (12.6) is impractical as it requires us to compute high powers of . Besides, the series has a slow convergence. It is therefore preferable to solve the system (12.4):together with . (The normalization condition is needed since only two of these equations are independent. Thus we get three equations for three unknowns.) The solution of the system is the equilibrium distribution (12.9)Of course the probabilities ,  and  also express the probabilities for sunny, cloudy or rainy weather over a longer time period. If the values (12.9) were in fact measured, say, over a period of one year, they could even be used to calibrate the model—i.e. the elements of the stochastic matrix—so that it would always converge to the desired end configuration.    





12.2 Continuous-Time Markov Processes

        In
          
          
         continuous-time Markov
          
          
         processes the transitions between the states X(t) in a dynamical system
          
         do not occur in discrete time jumps but rather in a continuous, smooth time evolution. In the following—the notation mostly follows [8]—we discuss continuous-time processes in which also the states themselves can be represented on the whole real axis, i.e. by a random variable , .
As we are dealing with continuous random variables, the set of variables  can be assigned a joint probability densitywhere the subscript n and the superscript (1) on f indicate that the n values  at times  on the left of the | sign depend on one value  at time  at its right. This applies to general time evolution; but Markov processes are "memoryless" and thereforeIn general, a state  at time  may depend on i states  at all previous times , while in Markov processes only the state immediately preceding it is relevant, i.e.  at time . Hence the joint probability density can be written as a product of densities for individual transitions:It follows that for arbitrary time  on the interval , Chapman-Kolmogorov equation
          
         applies:In continuous language this equation conveys the same message as its discrete analogue (12.2): the probability  for the transition from the state  at time  to some state on the interval  at time  is the sum of probabilities that this transition occurred through a state on any interval  at intermediate time .

12.2.1 Markov Propagator and Its Moments

          The
            
           key quantity embodying
            
            
           the actual
            
            
           step between
            
           two states in a very short time  is the Markov propagator
 (12.10)The propagator tells us the state of the process at time , if at time t it was in state x: the new state will be . The propagator depends on three real parameters x, t and , but conventionally the latter is given the most prominent spot: namely,  may be independent of x and t, but it must depend on . Since X(t) is a random variable, the propagator is also
            
           a random variable, so it can be assigned its own propagator density function  with
            
            
           the definitionThe density  allows
            
           us to define
            
           the propagator moment functions
where   The moment functions  and the density f are related by [8] (12.11)This is the Kramers-Moyal partial differential equation of infinite order describing the time evolution of  at fixed  and , for which all moments  and the initial condition  must be known.
If we choose a short enough , the propagator  can be composed of n propagators  with which the process proceeds in time from state  to states  in steps of length . These, however, can be made so small that during a step the value of x remains almost constant: (12.12)In this approximation all these steps become mutually independent, therefore  and . It is easy to show2 that in this case the expected value and variance of the Markov propagator must be proportional to the length of the time step:where the functions A and D do not depend on . The key consideration follows. In the mentioned approximation the right-hand side of (12.12) is a sum
           of independent and identically
            
           distributed random variables, so by
           the central limit theorem the variable  on its left is
            
           normally
            
           distributed
            
          , (12.13)The functions A and D are the characterizing functions of the Markov process. Do not confuse them with the characteristic functions of Sect. B.3! Due to obvious reasons A is called the drift function and D is known as the diffusion function. The propagator density of a continuous Markov process is thereforeFrom here and from (12.13) we see3 that A and D are equal to the moment functions  and , respectively, while it turns out that higher moments vanish:What is left of (12.11), then, is just
            
           the Fokker-Planck equation
which is a partial differential equation of the second order that needs to be solved with the initial condition .


12.2.2 Time Evolution of the Moments
The time evolution of the variable X is determined by the Markov propagator. But how do its expected value and variance evolve? And what is the time evolution of the expected value and variance of the random variable (12.14)which is called the integral of the Markov process? The answers to both questions for any continuous-time Markov process are given by ordinary differential equations, which we list without proof: for their derivation see e.g. [8]. The time evolution of the expected value and variance of X(t) is given by the equations (12.15)
 (12.16)with initial conditions  and . The time evolution of the corresponding moments of S(t) for  is given by (12.17)
 (12.18)where the integrand in (12.18) is the solution of the auxiliary equationwith the initial condition .


12.2.3 Wiener Process

          The
            
           Wiener process is a Markov process in which the drift and diffusion functions are constant, i.e. independent of x and t:  and . In this case, not much is left of (12.15) and (12.16); their solutions are (12.19)
 (12.20)while from (12.17) and (12.18) we obtain (12.21)
 (12.22)The Fokker-Planck equation also simplifies significantly
            
          ,It is solved by the initial condition , and its solution is (12.23)The Wiener process is therefore a Markov process described by a normally distributed variable with mean  and variance , as shown in Fig. 12.2. The corresponding phenomenon in nature is self-diffusion, in which a particle with mass M diffuses among a large ensemble of equally heavy
            
           particles (see also Sect. 12.2.4 and
           Example on p. 320).
Let us check our understanding of the Wiener process by a simple computer simulation of the Fokker-Planck equation, where the exact functional form of A(x, t) and D(x, t) can be freely chosen. We set ,  and , and choose a small enough . Then we repeat until desired:1.Draw a value . 2.

 3.

 4.

 5.Write t, x(t) and s(t), and return to 1. 

An example of such a simulation until  is shown in Fig. 12.3.Fig. 12.2Time evolution of the probability density of a continuous-time Markov process with constant drift and diffusion functions,  and . The initial condition, the  "function", gradually spreads into an ever broader normal distribution which "drifts" along the straight line given by 

Fig. 12.3Simulation of the Wiener process with initial value , time step length , drift function  and diffusion function . [Left] Some realizations of the random process that does not "drift" anywhere on average, as  by (12.19). The thin and thick curves denote the boundaries of one and two standard deviations, as dictated by (12.20). [Right] Some integrals of the process with mean zero (see (12.21)) and cubic increase of variance (12.22)



12.2.4 Ornstein-Uhlenbeck Process

          The
            
           Ornstein-Uhlenbeck process is a special case of a continuous Markov process where the drift function has the form , , while the diffusion function is independent of x and t, . The evolution equation (12.15) therefore has the form  with the initial condition . Its solution is (12.24)Equation (12.16) for the variance of X isand the initial condition is , thus (12.25)In this case the Fokker-Planck equation has the form
            
          
and needs to be solved with the initial condition . The probability density  that solves this equation corresponds to a normally distributed random variable(You can check this, with some effort, by writing the density of the normal distribution with specified mean and variance as in (12.23) and insert it in the Fokker-Planck equation.) In the large-t limit this meansin other words, convergence to a stable distribution:We are witnessing a process X(t) with probability density  whose expected value exponentially approaches zero, while its variance in the limit  stabilizes at the value D / (2k), as shown in Fig. 12.4.Fig. 12.4Time evolution of the probability density of a continuous Markov process with the drift function , , and the diffusion function . The "center of gravity" of the initial condition  gradually slips towards zero, yet the width of the distribution no longer increases: the last time slice is already 


By the same token we solve (12.17) for the expected value of the integral of the Markov process, S(t), as well as (12.18) for its variance. For  we get (12.26)
 (12.27)Let us again try to understand what (12.24-12.27) are saying by a computer simulation, based on the algorithm from p. 317. We choose ,  and . Sample results are shown in Fig. 12.5.Fig. 12.5Computer simulation of the Ornstein-Uhlenbeck process with initial value , time step , drift function ,  and diffusion function . [Left] Some realizations of the process that (on average) exponentially "drifts" towards zero, as dictated by (12.24). The thin and thick dashed lines indicate the boundaries of one and two standard deviations according to (12.25). At large t the standard deviation settles at  (stable distribution). [Right] Some integrals of the process with a mean that attains the value  in the limit  according to (12.26), and the variance that changes according to (12.27)


Long example Wanderings of a heavy macroscopic particle in a gas (fluid) of smaller, lighter particles—Brownian motion—can be approximately treated as a continuous Markov process. In order to see this, we
           first reinterpret the propagator (12.10) as
           a small changeBy (12.13),  is a normally distributed random variable with mean  and variance . We use the relation  which is valid for arbitrary real constants a and b and can be proven by using methods of Sect. 2.​7. Identifying  and  leads to , therefore
            
          
 (12.28)where . We have derived a specific form of the Langevin equation, which is easy to code as it explicitly expresses the random variable  in terms of the variables X(t) and .
The analysis of Brownian motion, discovered in the early 19th century, has a long history. Einstein's approach was to treat the coordinate of the particles as a Wiener process [9], while Langevin [10] placed his bet on their velocity as the key quantity. A spherical particle with radius R and mass M, moving with velocity v in a fluid with viscosity , obeys Newton's law  orwhere  is the linear drag coefficient. (In both Langevin equation (12.28) and Netwon's law the time interval  is assumed to be infinitesimally small.) The "driving" term  on the right represents the average linear momentum transferred to the wandering particle by the particles of the fluid (average force over ). But in general this momentum transfer fluctuates about its average. With this in mind we augment Newton's law by a term that provides the process with this kind of jitter: (12.29)where  and c is a positive constant which needs to be determined. We have denoted the velocity by an upper-case letter as we are dealing with a random variable. By comparing (12.29) and (12.28) we realize that Brownian motion can be understood as a continuous Markov process in which the role of the generic random variable X(t) is played by the physical velocity, V(t). The process has the Ornstein-Uhlenbeck form with the drift and diffusion functionsWith the initial condition , (12.24) and (12.25) immediately give us the expected value of the velocity and its variance for : (12.30)
 (12.31)This is precisely what we observe in Fig. 12.5 (left), where on the ordinate axis one should imagine the velocity V(t) instead of the generic variable X(t): a particle that starts moving in the fluid with velocity  at time zero, on average slows down exponentially according to (12.30), but the velocity distribution settles into a stable form with the variance (12.31).
The path (coordinate) of the particle is the integral of its velocity over time, , so at the level of random variables we may resort to (12.14), where X(t) is replaced by V(t), and formulas (12.26) and (12.27): (12.32)
 (12.33)The content of these equations is expressed by Fig. 12.5 (right): the path traveled by the particle with non-zero initial velocity  keeps increasing according to (12.32) at short times, but on average it attains the terminal value , about which is straggles with variance (12.33).
Asymptotically we have (12.34)which aids us in determining the constant c. After a long time the particle is in thermodynamic equilibrium with the fluid at temperature T. We can then expect—that was Langevin's key assumption—that  is a normally distributed random variable with mean zero and variance , sinceAccording to (12.34) we may therefore equateor . It follows thatWe have obtained the famous result that the variance of the particle's position at long times linearly increases with time and that it depends on the temperature and viscosity of the fluid. The main deficiency of this approach, of course, is the treatment of collisions, in which velocity actually changes very rapidly, as continuous ("smooth") processes. This is why in the described approximation the effective deviation depends neither on the mass of the Brownian particle, M, nor on the mass of the fluid particles, m. A much improved calculation, in which Brownian motion is analyzed as a discrete-time Markov process with continuous states (see, for example, Sect. 4.5 in [8]) reveals these delicate dependencies as well. One then obtainswhere  is the average particle density of the gas. The dependence of the variance on m is particularly intriguing. The velocity of the gas particles has a Maxwell distribution and, as we have seen in the Example on p. 105, all its characteristic velocities (mode, expected value and effective deviation) exhibit the  dependence: the lighter
            
            
           the gas particles, the more efficient
            
           they are in "kicking" the heavier particle and
           in dispersing
            
           its position.     




References


1.
E. Parzer, Stochastic Processes (Holden-Day, San Francisco, 1962)


2.
A.A. Markov, An example of statistical investigation of the text "Eugene Onegin" concerning the connection of samples in chains. Sci. Context 19, 591 (2006). English translation from original RussianCrossRefMATH


3.
S. Meyn, R.L. Tweedie, Markov Chains and Stochastic Stability (Springer, Berlin, 1995)MATH


4.
D.W. Stroock, An Introduction to Markov Processes, 2nd edn. (Springer, Berlin, 2014)CrossRefMATH


5.
A. Papoulis, Probability, Random Variables and Stochastic Processes, 3rd edn. (McGraw-Hill, New York, 1991)MATH


6.
C. Meyer, Matrix Analysis and Applied Linear Algebra (SIAM, Philadelphia, 2000)CrossRef


7.
B. Hayes, First links in the Markov chain. Am. Sci. 101, 92 (2013)


8.
D.T. Gillespie, Markov Processes. An Introduction for Physical Scientists (Academic Press Inc, Boston, 1992)MATH


9.
A. Einstein, On the movement of small particles suspended in stationary liquids required by the molecular-kinetic theory of heat. Ann. Phys. 17, 549 (1905)CrossRef


10.
P. Langevin, Sur la théorie du mouvement brownien, C. R. Acad. Sci. (Paris) 146, 530 (1908). See also the English translation D.S. Lemons, A. Gythiel, Paul Langevin's 1908 paper "On the theory of Brownian motion". Am. J. Phys. 65, 1079 (1997)




Footnotes


1


If the chain is irreducible and the states are reproducible (in finite  they always are), the equilibrium distribution does not exist. If the chain is irreducible and its states are periodic, the limit (12.5) may not exist or it may depend on i: an example is the matrix  with the equilibrium distribution , as , but  does not exist.

 



2


We are referring to a simple lemma: is g(z) is a smooth function of z satisfying  for any positive integer n, it holds that , where C does not depend on z.

 



3


The expected value is , whence . The variance is given by , therefore .

 













© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_13




13. The Monte-Carlo Method




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
The Monte-Carlo method is introduced as a generic tool for the solution of mathematical physics models by means of computer simulation. These problems range from simple one-dimensional integration to sophisticated multi-dimensional models involving elaborate geometries and complex system states. A historical introduction and an exposition of the basic idea are followed by a basic treatment of numerical integration and discussing methods of variance reduction like importance sampling and use of quasi-random sequences. Markov-chain Monte Carlo is presented as a powerful method to generate random numbers according to arbitrary, even extremely complicated distributions. A specific implementation in the form of the Metropolis-Hastings algorithm is offered.



The Monte Carlo (MC) method or simulation is a generic name for any procedure in which drawing random numbers and statistical samples allows us to approximately evaluate some mathematical quantity or expression, for example, a definite integral or a system of equations, but it can also be applied to much more general problems of mathematical physics [1]. The emphasis is on the word 'approximately': the quality of the solution depends on the sample size one can afford. Yet from the viewpoint of feasibility and precision as compared to standard numerical methods—in particular in multi-dimensional integration with complicated integration boundaries and in handling complex mathematical models—the Monte-Carlo method offers the only reasonable approach
        
      .

13.1 Historical Introduction and Basic Idea
          
        

                  
                

The French naturalist Georges-Louis Leclerc, count de Buffon (1707-1788), has shown how throwing a needle onto a mesh of uniformly spaced parallel lines allows one to estimate . Let the needle length be L and the line spacing be . Take y to be the shortest distance from the needle center to the closest line and  the acute angle of the needle with respect to the lines. At each throw any distance  and any angle  are equally probable, which corresponds to the uniform probability densities 2 / D and  and the joint density . The probability of a needle crossing a line is thenLet us check this result by a simple program, which is already our first Monte-Carlo simulation! We draw a random number  with a uniform distribution between 0 and D determining the ordinate of one end of the needle (Fig. 13.1 (left)). The ordinate  of the other end is obtained by drawing an angle  from  and calculating . Then we check whether the needle crosses a line ( or ) or not. If the whole procedure is repeated N-times and we count n crossings, it holds thatThe estimate of  is thenThe relative error  depends on the numbers of drawn and accepted events and, of course, on the ratio D / L. Its dependence on N for  is shown in Fig. 13.2 (left) indicating that in order to determine  to six-digit precision one needs approximately  throws. The importance of this dependence on N will become evident shortly.Fig. 13.1Determining  by the Monte-Carlo method. [Left] Simulation of the Buffon's needle experiment (see text for explanation). [Right] Determination of  by drawing points uniformly distributed on the square  and checking whether the points fall within the inscribed unit circle. Shown are  points,  of which lie within the circle, hence 


Let us try another way to "calculate" . We draw N pairs of random numbers , uniformly distributed on . By testing the conditionwe check whether a pair is within the inscribed unit circle, which we take to be a "good" outcome (Fig. 13.1 (right)). The ratio between the number of good outcomes, n, and the number of all drawn pairs, N, is an approximation for the ratio between the area of the circle and the area of the square: , thusThe error  as a function of N is also shown in Fig. 13.2 (left).Fig. 13.2Statistical error of the Monte-Carlo method in dependence of the number of drawn points, N. [Left] Relative error of the approximation for when calculating the ratio of the areas of the square and inscribed circle, and in the simulation of Buffon's needle experiment (). [Right] Relative error of the approximation for the volume of a hypersphere with dimension 4 or 10


        One
        

           can look at the problem from the opposite perspective. Assume we already know , but are interested in the volume of a d-dimensional hypersphere with radius R, "inscribed" in the corresponding hypercube with side 2R. The exact volumes areLet us simply set  and make N draws of d random numbers , distributed according to . At each draw we check the validity ofThe ratio of the number of draws n for which the condition is fulfilled, to the number of all draws, N, is equal to the ratio of the volumes of the hypersphere and the hypercube, , so we can estimate (13.1)Fig. 13.2 (right) shows how the statistical error of this calculation depends on N if  and . As in the circle-square case we notice the characteristic inverse-square-root decrease of the error.


13.2 Numerical Integration
          
          
        

Numerical integration is among the most important problems that can be solved by the Monte-Carlo method. We wish to calculate a definite integral of the form (13.2)where f is some probability density ( and ). We independently draw N values  of the random variable X, distributed according to the density f. (How this can be done for an arbitrary distribution, is discussed in Sect. C.2.) The value  is then estimated as (13.3)This estimator is unbiased, since its expected value isYet to control the integration we are interested in its variance, as it determines the quality of the numerical approximation: (13.4)In other words: the estimate for the value of the integral is , and the statistical error of this estimate depends on the specific form of g, but, as usual, it decreases inversely proportional to the square root of N,In the special case  and  one has , hence (13.2)-(13.4) can be merged to (13.5)where  and  are the means of the functions g and  on the interval [a, b]. An analogous formula can be written for multi-dimensional integration:


Example
Use the Monte-Carlo method to calculate the definite integral


 (13.6)whose integrand is shown by the full curve in Fig. 13.3 (left)! The exact value is .Fig. 13.3Calculation of definite integrals by the Monte-Carlo method. [Left] The integrand of the integral (13.6) and examples of probability densities used to generate random . [Right] A geometric body whose mass and center of gravity can be computed by the Monte-Carlo method

The MC integral can be performed in two ways. First the integrand is rewritten by including a probability density f corresponding to the uniform distribution on ,We draw the values  () of a uniformly distributed random variable , and calculate the sums  and . (Of course each such draw sequence yields different values.) The estimate for the integral by (13.5) is then (13.7)On the other hand, the integrand can also be split like this:In this case  must be drawn according to the probability density f corresponding to a triangular distribution on  (fifth row of Table C.1). Taking  again we get  and , and thenceThe error of the integral is smaller than that obtained from (13.7) by using the first method. The lesson is that by a different choice of the distribution used to generate the integration variable the variance of the MC estimate can be influenced. This will be the topic of Sect. 13.3. 


Example
A homogeneous sphere with density  and radius  is carved out by a cylinder with radius R / 2 whose longitudinal symmetry axis is parallel to the z-axis and goes through the point . The resulting geometrical body is shown in Fig. 13.3 (right). What are its mass and center of gravity?

The mass of a body in three-dimensional space is , where  is the volume element of the integration domain . The spherical coordinate system in which  is the most convenient. We also know how to uniformly generate random points in it (formula (C.4)), so for constant density the integrand is nothing but  and . But integration boundaries are crucial: a cylinder carves out the region defined by (13.8)The MC estimate for the mass of the body is therefore simply (13.9)where  and where , if condition (13.8) is fulfilled or  if it is not. The points  are therefore drawn only to check the validity of (13.8)! With  we get, for instance,With some geometric effort the mass can be, in fact, calculated exactly. The body is split to the untouched "left" (L) half and the whittled "right" (R) half which, due to its symmetry, consists of four equal parts. Their masses areso the total mass of the body is . What about the center of gravity, ? Symmetry clearly dictates , while (13.10)Even this can still be handled analytically:By (13.10) it follows that . How can we calculate  by using the MC method? Again we must calculate the sum as in (13.9), where we now set . We independently draw the values ,  and  according to a uniform distribution within the sphere, and finally divide out the total mass; the approximation for the abscissa of the center of gravity is then (13.11)With  we get .
We are also interested in the mass and center of gravity of the body if the sphere is inhomogeneous, for example, with the radial dependence of the density . In this case the recipes (13.9) and (13.11) becomeIf we wish to deal with this analytically, we must again calculate four integrals for , ,  and . This is an increasingly annoying procedure, especially if one imagines a complex carved-out sculpture for which a clear overview of the integration boundaries is lost. On the other hand, the MC method (right side of above equations) only requires us to change a few powers and rerun the program. Which avenue one should pursue depends on the compromise between the desired precision and computing time—yours or computer's. 


13.2.1 Advantage of Monte-Carlo Methods over Quadrature Formulas
The statistical error  of the integral  by the Monte-Carlo method decreases with the square root of the sample size: . One must therefore draw  points in order to determine the value of the integral to a precision of . Of course an integral of the type (13.2) could also be computed by using some classical numerical method, say, a quadrature formulaHere  are the weights depending on the method and  are the quadrature points that suitably fill the integration domain—e.g. the interval [a, b] or a d-dimensional hypercube. The discrete nature of this formula implies an error, too; usually it is estimated as , where h is a measure of the distance between the points of the domain, e.g.  on interval [a, b]. The error constant C and the power k (quadrature order) depend on the method.
Let  and  denote the times needed to compute the integral by using quadrature and the MC method, respectively. Clearly  grows linearly with the number of points:  where d is the space dimension. From  it follows that  or . The  is the product of the number of drawn points and time  needed for an individual sample, , thus the ratio of computing times at given  is (13.12)The ratio  decreases with space dimension d and increases with order of quadrature k. Indeed fancy quadrature formulas with high k exist, but the larger the d, the harder it is to find a formula that still ensures  and thus . Therefore, at large d the MC method is much more efficient than classical quadrature. In practice this applies already at .



13.3 Variance Reduction
          
          
        

Procedures exist which allow us to reduce the variance of MC estimates; for details see [2]. The simplest one is to analytically split the integration domain. Suppose we are seeking the value of the integral  and we can separate  such that . The decompositionis useful if the integral can be solved exactly on  while the MC method is called for in the remaining domain . A separation like this has been done in the Example on page 330: there  was the untouched hemisphere that could be handled analytically, while  was the carved-out piece where the MC method was applicable. However, one must ensure that there is no statistical correlation between  and , where  and .
An obvious simplification is also the splitting the integrand, :This seemingly trivial intervention is very effective if the integral of  is relatively easy to compute and g and —in the sense of their "wildness" within the integration domain—are very similar. Then the MC method is only applied to the integral of the "smooth" residual function .

13.3.1 Importance Sampling
          

The most effective way to reduce the variance of values of integrals by the MC method is importance sampling. In the Example on page 329 we realized that the variance can be influenced by the choice of probability density f in the integral (13.2). When the density of the uniform distribution, , has been used (Fig. 13.3 (left)), the whole interval  has been sampled uniformly, although it is obvious that the points near  and  make the dominant contribution to the integral. If, however, sampling with respect to the triangular distribution with density  has been used, the relevant left part of the interval has been sampled more often that the less important right part, thereby reducing the variance
          

                    
                    
                  .
Instead of using f, therefore, the values  can be drawn according to some other distribution with density p called the importance function [2, 3]. With it we compute the integralwhere ,  and . What do we achieve by doing this? The MC estimate based on the sample  drawn according to the distribution with density p is thenThe variance of this estimator iswhere all expected values are to be taken with respect to the distribution of X with density p—this is crucial! We wish to find p that minimizes this variance. The second term is simply , so the key to success is hidden in the first term. Jensen's inequality (4.​10) dictates its lower bound:The bound is reached whenAlas, we do not know the exact value of the integral in the denominator, otherwise we would not be computing it! In practice we therefore seek a function p(x) which is as similar as possible to the function |g(x)|f(x), i.e. such p(x) that the ratio |g(x)|f(x) / p(x) is approximately constant throughout the integration domain.

Example
(Adapted from [3].) Let us calculate the definite integral


which is of the form (13.2) with ,  (Fig. 13.4 (left)) and . At first we ignore the importance function and do it the old way: with values  of the uniformly distributed variable  we compute (13.13)The variance  of the estimate  at large N can even be calculated exactly: (13.14)The obtained value  can be seen in the upper graph of Fig. 13.4 (right), showing the approximation for the variance as a function of N.Fig. 13.4Weighting the integrand g by the importance function p. [Left] Graphs of functions  and . [Right] Variance of the plain MC estimate (upper graph) and by using the importance function p (lower graph)

Now choose an importance function p which is "as similar as possible" to g, say, . This function is non-negative and normalized to 1 on [0, 1], so it satisfies the requirements for a probability density. Since we are now computing the integralthe values  in the sums (13.15)must be drawn according to the density p! (Random values with such distribution can be generated by using some method of Sect. C.2.) A pleasant surprise is in store:By a fortunate choice of p the variance has been reduced by two orders of magnitude compared to the plain estimate (13.14); see the lower graph in Fig. 13.4 (right) which stabilizes at  for large N. 


Example
(Adapted from [3].) In the case of singular functions or functions whose certain moments do not exist, scaling the integrand by an importance function is unavoidable. For instance, let us calculate the integral


 (13.16)The integrand  is singular at  and  (Fig. 13.5 (left)), hence the plain MC estimate has infinite variance: with increasing N the sums (13.13), where the values  are drawn according to the uniform distribution U(0, 1), as well as the variance, keep increasing. This divergent behavior is demonstrated by the upper graph of Fig. 13.5 (right).Fig. 13.5[Left] Choice of importance function p for singular integrands g(x). All singularities of g should be included in p, so that the ratio g / p is regular at the problematic points and approximately constant throughout the domain. [Right] The variance of the MC estimate for Example (13.16) without the importance function and by using the g / p integrand

In such case it is prudent to choose an importance function that has the singularities at the same points and of the same order as g, for example (13.17)Random numbers according to this distribution can be drawn by using the tools of Sect. C.2, like the inverse method: it corresponds to a very simple algorithm1.Draw . 2.If , set , otherwise . (For explanation see also Sect. 3.4.4 in [3].) The weighted integrand g(x) / p(x) is plotted by the thick curve in Fig. 13.5 (left). The sums (13.15), where the values  are drawn according to the density (13.17), now yield a finite variance. Its dependence on N is shown by the lower graph in Fig. 13.5 (right). 



13.3.2 The Monte-Carlo Method with Quasi-Random Sequences
          

                    
                  

                    
                    
                  

                    
                    
                  

By a special kind of "drawing" the values  the convergence of MC estimates can be accelerated. Instead of the typical  behavior (see Fig. 13.2) trends like  or even  can be achieved, which, from the viewpoint of (13.12) is an argument in favor of the MC method. The word "drawing" actually implies a deterministic calculation of special sequences of d-pletswhich we use to sample the d-dimensional integration region at N points. The essence of the method is precisely the manner of this sampling: it is devised such that the points in d-dimensional space, forming the so-called quasi-sequence, "maximally avoid each other". An illustration of such quasi-sequence for , where each value in the pair  corresponds to a normally distributed variable , is offered by Fig. 13.6.Fig. 13.6Points of the Sobol quasi-random sequence on the domain . [Left] The first 128 points. [Center] The first 256 points. [Right] The first 1024 points

The full circles in the left Figure denote the first  points. The center Figure shows  points of the same sequence: the previous and the new 128 points are denoted by empty and full circles, respectively. The right Figure contains  points, of which the 256 old points are again denoted by empty circles and the 768 new ones with full circles. Apparently space is being filled by an almost regular pattern, yet the straggling of the points is more random and more uniform across the whole space than in drawing the values by pseudorandom algorithms (see Appendix C.1). Quasi-random sequences therefore truly start to excel only at very large N and high dimensions d.
Several brands of quasi-sequences and methods of their generation exist. Among the most popular is the Sobol sequence [4, 5], which has been used to generate the points in Fig. 13.6. The basic version is available in [6]; improvements for higher dimensions and larger periods are discussed in [7, 8].

Example
Let us redo the calculation of the volume of the four-dimensional hypersphere by using the MC method (see (13.1) and Fig. 13.2 (right)), but now we draw the points  in four-dimensional space as elements of the corresponding  Sobol sequence. The statistical error of the calculated volume estimate as a function of N is shown in Fig. 13.7
            
                      
                    

                      
                      
                      
                    

              
              
              . 



Fig. 13.7Statistical error of the MC approximation for the volume of a four-dimensional hypersphere. (Compare to Fig. 13.2). Shown is the dependence of the error on the number of points in the Sobol quasi-sequence () as compared to the usual generation of pseudo-random numbers ()




13.4 Markov-Chain Monte Carlo 

Classical Markov chains (see Sect. 12.​1) can be used to devise an effective method to generate random numbers according to arbitrary, even very complicated distributions, known as Markov-chain Monte Carlo (MCMC) [9]. The essence of the method is that the generated values form the states of a Markov chain whose equilibrium distribution is precisely the required probability distribution
          
        .
The key property of the chain that we exploit is reversibility. An irreducible Markov chain is reversible if the equilibrium probabilities  (see (12.​4) and (12.​6)) satisfy the requirement of detailed balance
 (13.18)Recalling  we see at once that detailed balance also meansin plain words: the probability for a transition between given states forward in time equals the probability for a transition between them backward in time. Assume that the distribution  satisfying (13.18) is unique (see Sect. 12.​1.​1). In the following we use this arsenal to formulate the core procedure of the MCMC method, the so-called Metropolis-Hastings algorithm.

13.4.1 Metropolis-Hastings Algorithm
Let  be the state space of the Markov chain possessing the equilibrium distribution , where . We shall attempt a quite general description of the MCMC method, where  may be discrete or continuous; the value x may even represent some exceedingly complex entity, say, the state of a two-dimensional spin lattice on which (at given temperature) some spins are oriented parallel to the magnetic field and some opposite to it—the formalism remains essentially the same.
                    
                    
                  

                    
                    
                  

If, for example, we wish to use the MCMC method to estimate the value of a one-dimensional integral of the form (13.2) by the sum (13.3), the random numbers must be drawn according to the desired distribution . The equilibrium distribution is therefore also known as the target distribution. Why don't we simply generate the values with the target distribution  by using some method of Sect. C.2, say, the rejection method (Sect. C.2.6)? In one dimension this is sensible, but in multiple dimensions the fraction of rejected points increases to the level of the method being useless. Similar problems are encountered in importance sampling (Sect. 13.3.1).
The main task is then: draw a sequence of values  of the Markov chain such that its equilibrium distribution is precisely . For this purpose we first introduce the candidate distribution
which is used to place a weight on the transitions between the states x and . (In previous notation  is just .) If the present state is x, the state  is a "candidate" for the next state, with probability . At this point reversibility and detailed balance (13.18) enter. To illustrate the main idea behind the Metropolis-Hastings algorithm, imagine for a moment that we can only jump between the states x and , as shown in Fig. 13.8, with probabilities ,  and , corresponding to the stochastic matrix (13.19)
Fig. 13.8A Markov chain in which only transitions between the states x and  are allowed. [Left] Chain with equilibrium distribution (13.20). [Right] Chain with equilibrium distribution (13.21)

If q satisfies the detailed-balance condition , we are done, since then  is already the equilibrium distribution of the chain. In the case in Fig. 13.8 (left) the condition is fulfilled: the equilibrium distribution satisfying  is (13.20)soBut what if (13.20) is not our desired (target) distribution and we actually wish to attain the equilibrium distribution (13.21)The matrix (13.19) can not do it, since . Besides, reversibility is gone: (13.22)The inequality reveals that the  transitions are too frequent with respect to  for the chain to be in equilibrium. Equilibrium is restored if the left-hand side if multiplied by a suitable factor, . Then instead of  the transition probability is , and one must also fix . The new equilibrated chain with the stochastic matrixis shown in Fig. 13.8 (right). For the chain with such matrix the desired distribution (13.21) is indeed stationary, .
                    
                  

This kind of tweaking of non-diagonal transition probabilities is the foundation of the Metropolis-Hastings algorithm [10-12]. The guesswork of finding the correct scaling factor for both matrix elements is replaced by a weight  with which a "good" state or configuration is accepted:If we wish to accept x while the chain tends to move to , we should be very generous in accepting the  transitions, so we set , while the  transitions should be stifled with probability  which, by the above equation, isIf the balance (13.22) tips in the opposite direction, in favor of the  transitions, we simply exchange the roles of x and . The same reasoning applies to a chain with many states, not just two. For any two states x and y we then define the acceptance probability of y with respect to x:
                    
                    
                  
 (13.23)The Metropolis-Hastings (MH) algorithm that generates a Markov chain  with states  given the initial state , the desired target distribution  and the chosen candidate distribution q, is therefore exceedingly simple:1.Draw a value  of the random variable . 2.For the next state of the chain take 
 3.Assign  and go to 1. 


Independent Metropolis-Hastings Algorithm

If the candidate distribution does not depend on the present state of the chain, that is, , the algorithm is even simpler:1.Draw a value  of the random variable . 2.For the next state of the chain take 
 3.Assign  and go to 1. 

Both versions of the algorithm generate the equilibrium distribution  even if its normalization constant is unknown, as it cancels in the ratio . Moreover, it is fascinating that it is generated regardless of the form of the function q! We must only ensure that  and q have the same definition domains. However, from the perspective of efficiency and precision of the algorithm it does matter what kind of q is chosen: we learn this in the following Example. For additional details see [13].

Example
We wish to generate random numbers according to a continuous distribution corresponding to a mixture of two normal densities of the form (3.​7),


 (13.24)with weights ,  and parameters , , . The rotated graphs of  are shown in the two small rectangles in Fig. 13.9 (right).Fig. 13.9Illustrating the MH algorithm as a generator of sequences of states in a Markov chain. [Top left] The states  of the chain generated by the algorithm with  (poor mixing). [Top right] Normalized histogram of states after  steps compared to the target probability density (13.24). [Bottom] Same as the panels above, but with  (good mixing)

Such a density enters, for instance, in a numerical evaluation of the integral (13.25)where g is some function. Let the candidate function used to draw the new state y in the MH algorithm also be a normal density with its mean equal to the previous state x: (13.26)while  is a free parameter. Choose , initial state , and run the algorithm for  steps. The obtained T states of the chain are shown in Fig. 13.9 (top left), while the normalized histogram of these states compared to the target density (13.24) is shown in Fig. 13.9 (top right).
The Figure tells us that the algorithm has spent about 2500 steps in sampling the first region of the target density centered at , switched to the other region around  after approximately 3000 steps, then changed its mind and quickly returned to the first region, sampling it for the next 3000 steps, and spent most of its remaining time in the second region. The histogram of the generated states  poorly matches the desired target density, because the algorithm dwelled at rather restricted portions of the definition domain for too long. The culprit is the parameter  being too small, much smaller than  and . The density (13.26) used to randomly explore the neighborhood of the old value x in order to come up with the new value y, is too "sharp" and leaves very limited freedom to the acceptance probability (13.23). The algorithm spends too much time in the same place: we say the states are poorly mixed
          
                    
                  .
If  is chosen more prudently, setting e.g. , which is comparable to , we obtain something like Fig. 13.9 (bottom left and right). Now the algorithm is very jittery and keeps on jumping between the two main prominences of the target distributions; the smearing of the candidate function is just about right that the algorithm can comb through all relevant parts of the domain. We have used the same number of steps, but with a carefully tailored function q the agreement between the generated and target distributions has greatly improved. A simple criterion for a basic tune of the candidate function is the fraction of steps in which the new state is accepted: it should hover around 0.5.Fig. 13.10Numerical integration with the MCMC method (MH algorithm). [Left] The graph of the integrand . [Right] Convergence of the estimate (13.28) to the exact value (13.27). As for any other statistical average, the relative error of integration by using the MCMC method and the Metropolis-Hastings algorithm has the typical inverse-square-root dependence on the number of steps

We know how to generate random numbers according to (13.24); now let us calculate some nasty integral of the form (13.25), for example, with the functionThe graph of  is in Fig. 13.10 (left), and the exact value of the integral is (13.27)With random numbers  generated by the MH algorithm we calculate the estimates of the integral (partial sums) (13.28)for various T. The relative error between the estimates  at each T and the exact value  is shown in Fig. 13.10 (right). 

With this Example we have barely scratched the surface of Markov-chain Monte Carlo methods. Approaches of the MCMC type truly blossom in multiple dimensions, where the classical methods of generating probability distributions become inefficient or—in the case of more general state spaces —completely useless. Further reading is offered by [14, 15].
                    
                  

                    
                    
                    
                  

                    
                    
                    
                  

                    
                    
                  

                    
                    
                  




References


1.
N. Metropolis, S. Ulam, The Monte Carlo method. J. Am. Stat. Assoc. 44, 335 (1949)MathSciNetCrossRefMATH


2.
J.E. Gentle, Random Number Generation and Monte Carlo Methods, 2nd edn. (Springer, Berlin, 2003)


3.
M.H. Kalos, P.A. Whitlock, Monte Carlo Methods, 2nd edn. (Wiley, Weinheim, 2008)


4.
I.M. Sobol', On the distribution of points in a cube and the approximate evaluation of integrals. USSR Comput. Maths. Math. Phys. 7, 86 (1967)MathSciNetCrossRefMATH


5.
I.A. Antonov, V.M. Saleev, An economic method of computing  sequences. USSR Comput. Math. Math. Phys. 19, 252 (1979)MathSciNetCrossRefMATH


6.
W.H. Press, B.P. Flannery, S.A. Teukolsky, W.T. Vetterling, Numerical Recipes: The Art of Scientific Computing, 3 edn. (Cambridge University Press, Cambridge, 2007)


7.
S.H. Paskov, J.F. Traub, Faster valuation of financial derivatives. J. Portf. Manag. 22, 113 (1995)CrossRef


8.
J.F. Traub, S.H. Paskov, I.F. Vanderhoof, A. Papageorgiou, Portfolio Structuring Using Low-discrepancy Deterministic Sequences, U.S. Patent 6,058,377, http://​www.​google.​com/​patents/​US6058377



9.
S. Chib, in Handbook of Computational Statistics, ed. by J.E. Gentle et al. Markov Chain Monte Carlo Technology, (Springer, Berlin, 2012), p. 73


10.
N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, A.H. Teller, E. Teller, Equation of state calculations by fast computing machines. J. Chem. Phys. 21, 1087 (1953)ADSCrossRef


11.
S. Chib, E. Greenberg, Understanding the Metropolis-Hastings algorithm. Am. Stat. 49, 327 (1995)


12.
I. Beichl, F. Sullivan, The Metropolis algorithm. Comput. Sci. Eng. 2(1), 65 (2000)


13.
C.P. Robert, G. Casella, Introducing Monte Carlo Methods with R (Springer, Berlin, 2010)CrossRefMATH


14.
W.R. Gilks, S. Richardson, D. Spiegelhalter (eds.), Markov-Chain Monte Carlo in Practice (Chapman & Hall / CRC, New York, 1996)MATH


15.
S. Brooks, A. Gelman, G. Jones, X.-Li. Meng (eds.), Handbook of Markov-Chain Monte Carlo (Chapman & Hall / CRC, New York, 2011)














© Springer International Publishing Switzerland 2016


Simon Sirca



Probability for Physicists


Graduate Texts in Physics

10.1007/978-3-319-31611-6_14




14. Stochastic Population Modeling




Simon Širca
1  




(1)
Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia

 



 

Simon Širca


Email: 
simon.sirca@fmf.uni-lj.si





Abstract
One way to analyze the time evolution of discrete populations is to develop models of birth, death and other mechanisms that influence the size of the population, as well as interactions between two or more populations. Modeling of births and deaths is introduced, followed by a discussion of a combined birth-death model representable in matrix form. The existence of equilibrium states is questioned and the time evolution of population distribution moments is presented.



This Chapter is devoted to population dynamics, i.e. modeling of birth, death and other processes experienced by "individuals" in discrete populations. (Such phenomena can be modeled as discrete-state Markov processes—see Chap. 12—but we discuss
        
       them separately here in a slightly simpler form.) "Individuals" may be single cells that die or successfully divide, people getting sick due to an infectious disease to which they succumb or become immune, subatomic particles being born or decaying in cosmic ray showers, or photons and electrons in resonant cavities of multi-level lasers [1-4].
Let X(t) be the size of the population at time t. Births, deaths, emigration, immigration and other mechanism that in any way modify the population size are treated stochastically, so X(t) is a random variable. The probability that X at time t has value n, is denoted by


14.1 Modeling Births
Let us begin
          
         with the simple case of cells dividing at a constant rate . The probability of one cell dividing in two in the interval  is . The probability that the whole population with X(t) cells at time t increases in size by precisely one cell in the interval , is therefore . Assume that at time  the population contains n cells. If  is small enough, multiple divisions may be neglected, so the population could achieve this state by two ways only: from a state with n cells at time t and no division in ; or from the state with  cells at time t and precisely one division in ,Assume that the population at time zero contains N cells, . Of course, it can only grow, thus one always has . For very small  it holds that . In the  limit the difference equation therefore becomes a system of differential equations: (14.1)The first equation is simpler, as the probability for the state with N cells can not be nourished by the state with  cells, but can only diminish, therefore . The initial conditions are (14.2)The solution of the system (14.1) with initial condition (14.2) for general n is [3]which corresponds to the negative binomial distribution (5.​8) with probability  for a "good" outcome. The expected value of the population size at time t iswhich, of course, implies unhindered growth, and its variance is



14.2 Modeling Deaths
By analogy to
          
         the birth-only model it is easy to establish the time evolution of a population that only experiences deaths. Let us stay with simple cellular division; the probability that a single cell dies in the interval  is , where  is the mortality rate. The probability that in the whole population with size X(t) at time t a single cell dies in the interval , is . The dynamics of the population is therefore described by the system of differential equations (14.3)If the size of the population at time zero is N, the initial conditions are (14.4)The solution of the system (14.3) with initial condition (14.4) for arbitrary n is [3]This is the usual binomial distribution with , so the expected value and variance of the variable X at time t are at hand: (14.5)


Example
A dying population can be modeled by a simple computer simulation. The key realization is that death is a Poisson process with a known mortality rate (Poisson parameter) . In the whole population with size X(t) at time t, on average  cells die during the interval —while in an actual "experiment" we may record zero, one, two,... deaths. The change of the population size at each time step is then , where  denotes a discrete random number, distributed according to the Poisson distribution with mean . Poisson generators are available in standard libraries [5]. We begin the simulation with  and subtract


 (14.6)until the population size drops to zero. Five such random death "paths" with ,  and  are shown in Fig. 14.1.Fig. 14.1Dying out of a population with Poisson-distributed number of deaths in each time interval. [Left] Depiction on linear scale. [Right] Depiction on logarithmic scale, together with the expected exponential dependence (14.5)

The exponential decay
          
        

          
           is not surprising: due to the randomness of the process the extinction times are somewhat scattered, but death of the entire population is unavoidable asA more relevant question is when on average the population dies out: we are interested in the distribution of extinction times. We follow many random death "paths" and note the times at which the population size drops to zero. A few such distributions with the same N and  as above are shown in Fig. 14.2 (left).Fig. 14.2Dying out of populations with different mortality rates. [Left] The distribution of extinction times , generated by following 100, 000 different random paths from the initial state with  to the final with . [Right] Dependence of the average extinction time  on the step length 


It is also interesting to know whether the calculated extinction time and the distribution of its averages depend on . The time step should not be too large, otherwise one can mow down the entire population in a single . For small enough steps, on the other hand, the results should be approximately independent of : namely, if  are Poisson-distributed random variables with parameters , their sum  is also a Poisson-distributed variable with parameter  (convolution; see also Example on page 148). In other words, in drawing random numbers we rely on the approximationThe quality of this approximation can be judged from Fig. 14.2 (right). 



14.3 Modeling Births and Deaths
The dynamics of
          
         births and deaths can be merged in a unified model that can even be endowed by a more general ansatz for natality and mortality rates in a population with size n. Let us denote them by  and . So far we have assumed that they are proportional to the population size, i.e.  and , but in general they can have a richer functional form which, however, must always satisfy the requirement : in a population with size  nothing can be born, and such a population can not "die further".
Birth and death are independent Poisson processes; the probability that a population of size n faces b births and d deaths in the interval  is therefore the product of individual probabilities,If  is small enough, only the terms with  and  may be considered. Let us useto denote the change in population size (increase or decrease) in the time interval . We discuss a population model with the propertieswhose dynamics is determined by the system of differential equationswhere  The probabilities  can be arranged in the vector (14.7)Its dimension depends on the expected population dynamics. In the simple death model with a population of initial size N one needs a ()-dimensional vector to accommodate all possible population sizes between 0 and N. In the simple birth model one needs an infinite-dimensional vector in principle, but in a computer implementation it is whittled down according to the sizes we wish to monitor. The coefficients of the system are stored in the matrixso that it can be expressed in matrix formand precisely the way it has been written it is also solved (given the specific initial condition)—see Example on page 356.
By analogy to the simple birth and death processes, the  limit yields the corresponding (tridiagonal) system of linear differential equations (14.8)where  The system is solved with the initial conditions (14.9)or with a more general condition .

14.3.1 Equilibrium State
Does an equilibrium state exist for the system (14.8), i.e. a stationary (stable) distribution with the property ? This question can be answered by setting all  to zero and noticing that :and so on. The only equilibrium state is therefore the state of total extinction, , as all  must sum to 1 at any t.


14.3.2 General Solution in the Case , , 

In the case that natality and mortality rates are proportional to the number of individuals in the population and different from each other,the solution of the system (14.8) with initial condition (14.9) can be written in closed form, depending on the initial population size N [3]. If the process begins with a single individual, , corresponding to , the solution iswhereFor  the general solution is (14.10)where  and .


14.3.3 General Solution in the Case , , 

If natality and mortality rates are equal, , the corresponding formulas for  and  are obtained by taking the  limit in the above expressions. (The rule of l'Hôpital comes to the rescue.)


14.3.4 Extinction Probability
The probability that
            
          

            
             a population dies out after time t (the extinction probability) is coded in the zeroth element of vector :ThereforeEven if natality and mortality rates are equal, the population certainly dies out!


14.3.5 Moments of the Distribution  in the Case , 

Apart from the dynamics with initial condition  corresponding to an exact initial population size N, we would like to understand the time evolution of a population whose size at time t (possibly ) has a more general distribution, e.g. . For this we need the ith moment of the variable X(t) with distribution ,We insist on the form ,  and calculate the time derivative of the first moment, . This is done by rewriting (14.8) and considering , (14.11)We have obtained the equation  with the solutionand the message: if , the mean of the population distribution (its "center of gravity") exponentially diverges; if , it exponentially decreases to zero; if , the average does not change—which does not mean that X(t) does not change! With a sharp initial condition , which means precisely , the same realization can be written as (14.12)as shown in Fig. 14.3 (left).Fig. 14.3Moments of the distribution  or random variable X(t) with initial condition  in the case of dominating births (), dominating deaths () and (quasi) equilibrium (). [Left] Expected value. [Right] Variance

A similar calculation yields the second moment. By analogy to (14.11) we obtain the differential equation  with the solutionHence the variance iswhere . If the initial size of the population is sharply defined, this expression can be further simplified, since then the initial variance is zero, , while the mean is : (14.13)The time evolution of the variance in three typical dynamical regimes (,  and ) is shown in Fig. 14.3 (right).

Example
Consider the example of stochastic analysis of a population with natality rate  and mortality rate . We are interested in the time evolution of the size of the population with initial size  on the interval . All we need is the recipe


 (14.14)where  denotes a random number generated according to the Poisson distribution with the specified mean parameter (compare to (14.6)).Fig. 14.4[Left] Simulation of 100 random paths generated according to (14.14) with , ,  and  until time . Compare to Figs. 14.1 and 1.​5 (left). [Right] The distribution of final states for all 100 paths
Fig. 14.5Components of  expressing the state of the population with exact initial size  after time t, i.e. the fraction of paths ending in . The arrow indicates the initial distribution. For  the
                    
                   analytic solution (14.10) is also plotted

First we calculate the "paths" traced by the population (compare to Fig. 14.1). Figure 14.4 (left) shows 100 such paths. Mortality exceeds natality, thus on average exponential decay (14.12) is observed. Yet in spite of  a few paths even meander beyond  at short times. The path "fan" spreads out, as predicted by (14.13), although at time  it should begin to shrink according to the lower curve in Fig. 14.3 (right). The distribution of the final states X(t) at  for all paths is shown in Fig. 14.4 (right).
The more paths we simulate, the smoother the histogram, which is nothing but a snapshot of the vector  (14.7) at arbitrary time. Let us compute a larger set (10, 000 instead of 100) random paths, all started with , i.e. . The components of the vector  at times , 3, 7 and 16 are shown in Fig. 14.5. The population dies out, and of the distribution  in the  limit only the  component survives. 




14.4 Concluding Example: Rabbits and Foxes
The time
         evolution of a population becomes truly interesting when we consider immigration and emigration, external factors like finite amounts of nutrients or energy, and in particular when we treat several populations and their mutual interactions. So far we have only learned the alphabet: thence the vast expanse of population dynamics opens which is beyond the scope of this chapter and this textbook.
Nevertheless, let us discuss a simple problem of two populations in order to gain at least some insight into the richness of possible states and their inter-dependence. It is the classical conflict of rabbits and foxes. If food is abundant and there are no threats, the rabbits multiply with natality rate  and die (due to old age or disease) with mortality rate . The corresponding parameters for the foxes are  and . But the crucial ingredient is the interaction between the two: in order for a fox to catch the rabbit, they must meet, so the probability of their meeting is proportional to the product of probabilities that the rabbit (R) and the fox (F) happen to be at the same place at the same time. We can then imagine the random variables R(t) and F(t) to be some sort of time-dependent "concentrations" of rabbits and foxes, and the product R(t)F(t) a kind of measure for the success of the hunt. With this guideline we write the differential equationswhere  and  are interaction parameters. The last term in the first equation is negative, since the foxes are killing the rabbits. The last term in the second equation is positive, as the fox population is being strengthened.
Assume that , so in truth only two parameters are genuinely free, since we can write , ,  and . Suppose that the rabbit and fox populations are in equilibrium with  rabbits and  foxes. Equilibrium means , thusThe equilibrium condition allows us to compute the interaction parameters,so the original system of differential equations can be written asThis system with initial conditions  and  can be solved deterministically, i.e. by a suitable program for integration of differential equations. Our question about the state of the populations at a later time t will be given a unique answer. But we can also solve it stochastically, so that in each term we draw a Poisson probability for the increase or decrease of the population with the argument which is a product of the growth or decay parameter, the current population size, and the step length . In short, we repeat the recipe (14.14), except that we now have two interacting populations. We therefore initialize the populations with R(0) and F(0) and enter the loopwhich is repeated until one of the populations dies out. Two examples of random population "paths" generated in this manner are shown in Fig. 14.6 (top). The two panels at bottom show the corresponding phase portraits.Fig. 14.6Modeling Poissonian population dynamics of rabbits and foxes with parameters , , ,  and . [Top left and right] Sample time evolutions of R(t) and F(t). [Bottom left and right] Phase portraits

We see in Fig. 14.6 (top left) that at  an approximately constant number of rabbits are available, which benefits the foxes. At  the rabbits become a rare commodity, so the fox population dwindles soon thereafter. This is swiftly exploited by the rabbits which happily multiply after ; this again aids the foxes, and the rabbits are mercilessly devoured up to . But this also implies that the food becomes scarce for the foxes as well, so they, too, almost perish.
Figure 14.6 (top right) shows a more interesting case of nearly periodic exchange of predator and prey resurrections: the fox population recovers shortly after the rabbit population culminates. Intervals with negative time derivative of R(t) correspond to intervals with positive derivatives of F(t). As an exercise, repeat the simulation many times and plot the distribution with respect to extinction times of rabbits and foxes as in Fig. 14.2! In the meanwhile, pause to ponder upon miraculous Nature that has managed to sustain such periodicity by using its own Monte-Carlo
        

          
           method for eons!


References


1.
J.H. Matis, T.R. Kiffe, Stochastic Population Models. A Compartmental Perspective. Lecture Notes in statistics, vol. 145 (Springer, Berlin, 2000)


2.
L.J.S. Allen, Stochastic Population and Epidemic Models (Springer, Cham, 2015)


3.
L.M. Ricciardi, in: Biomathematics Mathematical Ecology, eds. by T.G. Hallam, S.A. Levin. Stochastic Population Theory: Birth and Death Processes, vol 17 (Springer, Berlin, 1986) p. 155


4.
L.M. Ricciardi, in: Biomathematics Mathematical Ecology, eds. by T.G. Hallam, S.A. Levin. Stochastic Population Theory: Birth and Death Processes, vol 17 (Springer, Berlin, 1986) p. 191


5.

gsl_ran_poisson in GSL library, http://​www.​gnu.​org/​software/​gsl/​, or poidev in Numerical Recipes, eds. W.H. Press, B.P. Flannery, S.A. Teukolsky, W.T. Vetterling. Numerical Recipes: The Art of Scientific Computing, 3rd edn. (Cambridge University Press, Cambridge, 2007)
















Appendix A




                Probability as Measure
                


Abstract
                   Probability is defined in a mathematically strict manner as a measure. The Dirac delta is defined.
              

                Here we give a mathematically more strict definition of probability and the Dirac delta "function" (phenomenologically introduced in (
                2.​1
                )) as
                measures
                .
              


-Algebra


                
                  Let
                  
                
                 
                X
                be a non-empty set. A family of subsets
                
                of
                X
                is called a
                
-algebra on
X
                if it has the following properties:
                1.

                        ;
                       2.
                        for each subset
                        
                        also
                        
                        ;
                       3.
                        for each countable family
                        
                        of elements from
                        
                        , the union
                        
                        also belongs to
                        
                        .
                       


                The elements of the family
                
                are called
                measurable sets
                , and the set
                X
                , furnished with
                
                , is called a
                measurable space
                . A
                
                  measurable
                  
                  
                
                
                  
                  space
                
                is the pair
                
                .
              

Positive Measure


                Examples of positive measures are: length of subset (interval)
                
                  in
                  
                  
                
                , area of planar geometric shapes, volume of bodies in space. To generalize these special cases to arbitrary measurable spaces, one defines a
                positive measure
                (or simply
                measure
                ) on a measurable space
                
                as the function
                
                satisfying the conditions
                1.

                        and
                       2.

 
                for each countable family of disjoint subsets
                
                and
                
                . A positive measure
                
                on a measurable space
                
                is called a
                finite measure
                if
                
                .
              

Probability as a Positive Measure


                For random experiments with sample space
                S
                we define the
                
                  event
                  
                  
                
                
                  
                  
                
                
                  
                  
                  space
                  
                
                , which is the power set of
                S
                , i.e. the set of all subsets of
                S
                , including the empty set and
                S
                itself. The mapping
                
                is called a
                probability measure
                on measurable space
                
                if the following holds true:
                1.

                        for each
                        
                        ;
                       2.

                        ;
                       3.
                        if
                        
                        are mutually exclusive events in
                        
                        , then
                        
 


                In simple cases, e.g. in throwing a die,
                
                may indeed be identified with the power set of
                S
                , but often we restrict ourselves to a much smaller set: for example, it turns out [1] that there does not exist a probability measure
                P
                that would be defined on
                all
                subsets of the interval [0, 1] and would satisfy the requirement
                
                for each
                
                .
              

Dirac Measure


                Let
                X
                be an arbitrary non-empty set,
                
                its power set,
                
                  and
                  
                  
                
                
                its
                
                  arbitrary
                  
                  
                
                element. Then the prescription
                
                defines a positive finite measure on measurable space
                
                called the
                Dirac measure at x
                and denoted by
                
                . For each function
                
                the integration with respect to the Dirac delta represents the evaluation of the function at
                x
                ,
                
                In the special case
                
                we have
                
                for each measurable function
                
                , where
                
                and
                
                is the Dirac delta "function".
              

Reference
1.
                        G. Grimmett, D. Welsh,
                        Probability. An introduction
                        , 2nd edn. (Oxford University Press, Oxford, 2014)
                       






Appendix B




                Generating and Characteristic Functions
                


Abstract
                   Generating and characteristic functions are introduced as transformations of random variables that facilitate the calculation of certain distribution properties, in particular its moments and their convolutions. The problems of inverting probability-generating and characteristic functions, as well as of the existence of generating functions are presented.
              
Generating and characteristic functions are transformations of probability functions. As such they are not as easy to interpret as the distributions themselves, but in certain cases they offer immense benefits in terms of elegant calculation of distribution properties—for example, their moments—or quantities relating the distribution to each other, in particular their convolutions.

B.1 Probability-Generating Functions

                  Generating functions are applicable to random variables
                  
                    whose
                    
                    
                  
                  
                    
                    
                  
                  
                    
                    
                    possible
                  
                  values are non-negative integers or their subsets. Such variables are called
                  non-negative integer random variables
                  . Let
                  X
                  be such a variable with the probability function
                   (B.1)
                  Then the function of a real variable
                  
                  is the
                  [probability]-generating function
                  of the random variable
                  X
                  , distributed according to (
                  B.1
                  ). The coefficients in this power expansion are probabilities with values between 0 and 1. Since they are bounded, the series is absolutely convergent for any
                  
                  , and due to
                  
                  the series converges at least on
                  
                  . By comparison to (
                  4.​7
                  ) we also see that the generating function is equal to the expected value of the random variable
                  
                  ,
                   (B.2)
                  The generating function
                  
                  uniquely determines the probability function of
                  X
                  . This can be seen if we take the derivative of the series with respect to
                  z
                  :
                  
                  Namely, by setting
                  
                  we obtain
                   (B.3)
                  so indeed by taking consecutive derivatives the complete distribution is determined, as all its components
                  
                  are combed through. Why does this matter? Frequently only the generating function of
                  X
                  is available, while its probability function is not explicitly known. In such cases its components can be calculated by using (
                  B.3
                  ): see Sect. 
                  B.1.2
                  . Besides, taking the derivatives of the generating function is an easy way to produce the moments of
                  X
                  . For instance, by taking the first and second derivative we get
                  
                  On the other hand,
                   (B.4)
                  therefore
                   (B.5)
                  Individual moments can be calculated without such detours by using the formula
                  


Example

                    The generating
                    
                      function
                      
                      
                    
                    of the binomial distribution (Definition (
                    5.​1
                    )) with the probability function
                    
                    is
                    
                    Its first derivative is
                    
                    , and the second derivative is
                    
                    , thus
                    
                    and
                    
                    . From (
                    B.4
                    ) and (
                    B.5
                    ) it follows that
                    
                    which are familiar expressions (
                    5.​5
                    ).
                    



Example

                    The generating function of
                    
                      the
                      
                      
                    
                    Poisson distribution (
                    5.​11
                    ) is
                    
                    Differentiation gives
                    
                    and
                    
                    , hence
                    
                    and
                    
                    . From (
                    B.4
                    ) and (
                    B.5
                    ) it follows that
                    
                    which, again, we know from (
                    5.​12
                    ).
                    



B.1.1 Generating Functions and Convolution

                    Let us discuss mutually independent integer random variables
                    X
                    
                      and
                      
                      
                    
                    
                      
                      Y
                    
                    with the probability functions
                    
                    Their sum
                    
                    is also an integer random variable with the corresponding probability function
                    
                    The sum
                    Z
                    has value
                    n
                    when the variables
                    X
                    and
                    Y
                    have values
                    
                    or
                    
                    or
                    
                    , and so on. Since
                    X
                    and
                    Y
                    are independent, the probabilities of these simultaneous events are
                    
                    or
                    
                    or
                    
                    , and so on. In other words,
                    
                    We are looking at a discrete convolution of the sequences
                    
                    and
                    
                    , which we denote as
                    
                    This a discrete analogue of the Definition (
                    6.​1
                    ) or
                    
                    Where do generating functions come into play? Let
                    
                    be generating functions of
                    X
                    and
                    Y
                    . The generating function of their sum is then
                    
                    The series on the right is just the product of the series
                    
                    and
                    
                    , so
                     (B.6)
                    The generating function of the sum of independent integer variables is therefore equal to the product of the generating functions of the two terms. An even faster way to this result would be to consider (
                    B.2
                    ): if
                    X
                    and
                    Y
                    are independent, the variables
                    
                    and
                    
                    are independent, too; since for independent variables
                    U
                    and
                    V
                    one has
                    
                    , this also means
                     (B.7)
                    whence (
                    B.6
                    ) follows. This should not be read in the opposite direction: having
                    
                    does not necessarily mean that
                    X
                    and
                    Y
                    are independent. But the relation
                    can
                    be generalized to several independent variables: if
                    
                    are mutually independent random variables with generating functions
                    
                    and
                    Z
                    is their sum with the generating function
                    
                    , then
                     (B.8)
                    Multiplying generating functions is a much simpler operation than computing convolution sums, so convolution of independent integer random variables is most easily performed by using (
                    B.6
                    ) and (
                    B.8
                    ).
                  

Example

                      We demonstrate that the convolution of two Poisson distributions
                      
                        is
                        
                        
                        a
                      
                      Poisson distribution. In the Example on p. 148 we have derived this result by a direct calculation of the convolution sum. But if one calls generating functions
                      
                      and
                      
                      to the rescue, the effort is minimal:
                      
                      Clearly the variable
                      Z
                      has the generating function of the Poisson distribution with parameter
                      
                      , so indeed
                      
                      .
                      




B.1.2 Inverting the Probability-Generating Function

                    The functional form
                    
                    in the preceding Example immediately allowed us to conclude that
                    Z
                    is Poissonian, as we already knew the relation between the generating function and its inverse beforehand. The same procedure can be used for more complicated generating functions, as long as they can be split into sums of terms whose inverses are known.
                  

                    But how do we compute the inverse of an arbitrary generating function? Formula (
                    B.3
                    ) can be used for simple explicit functions, but analytic differentiation may be strenuous and is numerically unstable. The solution—in particular when the generating function is only known at discrete points—is offered by the Cauchy integral formula
                    
                    where
                    
                    is a subset completely contained in the definition domain of
                    
                    (neighborhood of
                    
                    ),
                    
                    is its boundary and
                    a
                    is any point in the interior of
                    D
                    . For the
                    n
                    th derivative of
                    
                    it holds that
                    
                    so the components
                    
                    of the probability distribution of
                    X
                    —use of (
                    B.3
                    ) requires derivatives of
                    
                    at
                    
                    —are given by the integral
                    
                    The closed curve
                    C
                    is a circle in the complex plane. By the substitution
                    
                    , where
                    R
                    must be such that
                    
                    is analytic on
                    D
                    , we get
                    
                    The integral can be evaluated by using the trapezoidal rule, resulting in the following approximation for the true distribution
                    
                    (
                    
                    ):
                     (B.9)
                    which is the inverse discrete Fourier transformation scaled by
                    R
                    . Due to the discrete nature of the approximation, discretization and aliasing errors are thereby introduced (see, for example, [1], page 166), which can be controlled by the parameter
                    R
                    . For details see [2-4].
                  

Example

                      Let us pretend that we do
                      
                        not
                        
                        
                      
                      know the probability function of the Poisson distribution
                      
                      , but only its generating function
                      
                      . Take
                      
                      , for instance: the exact values
                      
                      up to
                      
                      are shown in Fig. 
                      B.1
                      (left). We compute the approximations for
                      
                      by inverting the generating function via (
                      B.9
                      ) with different
                      R
                      , say,
                      
                      ,
                      
                      and
                      
                      . The absolute errors of these reconstructed probability functions are shown in Fig. 
                      B.1
                      (right). Note the absence of the value at
                      
                      : due to the periodicity of the Fourier transform one has
                      
                      .
                      



Fig. B.1
                            [Left] Poisson distribution with parameter
                            
                            in logarithmic scale. [Right] Difference between the exact values
                            
                            and their approximations, calculated by inverting the probability-generating function by the discrete Fourier transformation (
                            B.9
                            ), at several values of the parameter 
                            R

Fig. B.2
                            [Left] Poisson distribution with parameter
                            
                            , binomial distribution with
                            
                            ,
                            
                            , and their convolution. [Right] The difference between the exact probabilities
                            
                            and their approximations
                            
                            , calculated by inverting the generating function and by using the discrete Fourier transformation with various values of
                            R



Example

                      It is instructive to compare the convolution of discrete distributions, calculated by the basic formula (
                      6.​4
                      ), and by multiplying generating functions according to (
                      B.6
                      ). Take, for instance, the Poisson distribution
                      
                      where
                      
                      , and the binomial distribution
                      
                      where
                      
                      and
                      
                      . These distributions are shown in Fig. 
                      B.2
                      (left) at its left edge. By the definition of convolution we obtain the distribution
                       (B.10)
                      indicated by full circles in the figure. We should expect the same result by multiplying the generating functions of both distributions and computing the inverse Fourier transformation of the product. Thus we compute
                      
                      and then use this function in formula (
                      B.9
                      ):
                      
                      (Think about it: what
                      
                      should one take in the above equation and what is the range of
                      n
                      in (
                      B.10
                      ), considering that the definition domains of the distributions differ?) We thereby obtain the probabilities
                      
                      that should be equal to
                      
                      . How well this holds is shown
                      
                        in
                        
                        
                      
                      
                        
                        
                      
                      
                        
                        
                      
                      Fig. 
                      B.2
                      (right).
                      





B.2 Moment-Generating Functions

                  Probability-generating functions have been defined for
                  
                    random
                    
                    
                  
                  
                    
                    
                    variables
                  
                  with non-negative integer values. The concept can be extended to random variables with arbitrary real values, if
                  
                  (see (
                  B.2
                  )) is replaced by
                  
                  . If this expected value is finite for
                  t
                  on the interval
                  
                  for some
                  
                  , we may define the
                  moment-generating function
 (B.11)
                  which is nothing but the continuous Laplace transform. In the case of a discrete probability distribution of
                  X
                  , which we shall not discuss separately from now on, the corresponding definition is
                  
                  The 'moment-generating' attribute is easy to explain if one expands
                  
                  in a power series and exchanges the order of summation and taking the expected values:
                  
                  Namely, individual distribution moments can be obtained by taking consecutive derivatives
                   (B.12)
                  thus
                  
                  ,
                  
                  , and so on. Compare (
                  B.3
                  ) and (
                  B.12
                  )!
                

Example

                    Let us calculate the moment-generating function of a random variable distributed according to the Cauchy
                    
                      distribution
                      
                      
                    
                    (
                    3.​18
                    ):
                    
                    Now we see why Definition (
                    B.11
                    ) had to be formulated so carefully: the expected value
                    
                    for arbitrary
                    t
                    may not even exist! This obstacle will be circumvented in Sect. 
                    B.3
                    .
                    



Example

                    What about the moment-generating function of a random
                    
                      variable
                      
                      
                    
                    
                      
                      
                      distributed
                    
                    according to the standardized normal distribution (
                    3.​10
                    )? By elementary integration
                    1
                    we immediately obtain
                     (B.13)
                    The main dish follows: we expand the exponential in a power series
                    
                    and compare the terms with equal powers of
                    t
                    on both sides of the last equality. This gives us the odd moments
                    
                    while the even ones are
                    
                    thus
                    
                    ,
                    
                    ,
                    
                    ,
                    
                    , and so on. The first two values should already be familiar from Sect. 
                    4.​7
                    , while the others were derived elegantly, with minimal effort.
                    



                  Let
                  X
                  and
                  Y
                  be random variables with moment-generating functions
                  
                  and
                  
                  . If
                  X
                  and
                  Y
                  are mutually independent, the same reasoning that brought us to (
                  B.6
                  ) implies also
                   (B.14)
                  For random variables
                  X
                  and
                  Y
                  related through
                  
                  , it holds that
                   (B.15)
                  So the obvious generalization of (
                  B.14
                  ) to a sum of several variables is at hand: if
                  
                  are mutually independent random variables and
                  
                  is their linear combination with real coefficients
                  
                  , the moment-generating function of
                  Y
                  is equal to the product of moment-generating functions of individual variables
                  
                  :
                   (B.16)
                  Just as in (
                  B.7
                  ) these recipes may not be read in reverse:
                  
                  does not necessarily mean that
                  X
                  and
                  Y
                  are independent.
                

Example

                    The convolution problem from the Example on p. 148 can also be solved by generating functions. The moment-generating functions of
                    X
                    and
                    Y
                    are
                    
                    that is,
                    
                    Since
                    X
                    and
                    Y
                    are mutually independent, the moment-generating function
                    
                    of their sum
                    
                    is the product of the individual moment-generating functions
                    
                    and
                    
                    :
                    
                    All we need, then, is to read off the coefficient in front of
                    
                    , which is
                    
                    , and
                    
                      analogously
                      
                      
                    
                    
                      
                      
                      for
                    
                    any other
                    
                    .
                    




B.3 Characteristic Functions

                  Let
                  X
                  be a real (discrete or continuous) random variable and
                  t
                  
                    
                    
                  
                  
                    
                    
                    a
                  
                  non-random real variable. The quantity
                   (B.17)
                  is called the
                  characteristic function
                  of the random variable
                  X
                  [5, 6]. In contrast to the moment-generating function a characteristic function exists regardless of the distribution of
                  X
                  , and its definition domain is the whole real axis. Any characteristic function satisfies
                  
                  Besides, one has
                  
                  and
                  
                  , if
                  
                  exists. If the distribution of
                  X
                  is discrete, with probability function
                  
                  , where
                  
                  , it has the characteristic function
                   (B.18)
                  If the distribution is continuous, with probability density
                  
                  , it corresponds to
                   (B.19)
                  which is the usual Fourier transformation of
                  
                  .
                

Example

                    The Poisson distribution with the
                    
                      probability
                      
                      
                    
                    function
                    
                    has the characteristic function
                    
                    Calculate also the corresponding moment-generating function!
                    



Example

                    The standard normal distribution (
                    3.​10
                    )
                    
                      has
                      
                      
                    
                    
                      
                      
                      the
                    
                    characteristic function
                    
                    while the equivalent for the non-standardized normal distribution (
                    3.​7
                    ) is
                     (B.20)
                    where we have used the formula in (see footnote 1).
                    



                  The following important properties of characteristic functions are given without proof. If
                  a
                  and
                  b
                  are constants and
                  
                  , it holds that
                   (B.21)
                  which is also seen from (
                  B.15
                  ). If random variables
                  
                  are mutually independent and
                  
                  is their linear combination, then
                   (B.22)
                  This theorem also can not be reversed: having
                  
                  does not necessarily mean that
                  X
                  and
                  Y
                  are independent.
                

                  As the moment-generating functions, the characteristic functions, too, can be used to derive the statistical moments
                  
                  ,
                  
                  Namely, if
                  
                  is at least
                  p
                  -times continuously differentiable at the origin, it holds that
                  
                  There is a one-to-one correspondence between the characteristic function and the probability distribution: any two random variables
                  X
                  and
                  Y
                  have the same probability distribution precisely when
                  
                  , therefore
                  


Example

                    Let us calculate the characteristic function of
                    
                      the
                      
                      
                    
                    binomial distribution
                    
                    Imagine a Bernoulli (binomial) sequence of trials. To the
                    j
                    th trial in this sequence we assign a random variable
                    
                    with value 1 for a "good" event
                    A
                    (probability
                    p
                    ), or value 0 for the complementary event
                    
                    (probability
                    
                    ). Since the trials in the sequence are mutually independent, the same applies to the random variables
                    
                    . The variable
                    X
                    takes the value
                    n
                    if there were
                    n
                    occurrences of
                    A
                    in
                    N
                    trials: in this case precisely
                    n
                    variables
                    
                    have value 1, while the others are zero, hence
                    
                    . For an individual
                    
                    we then use (
                    B.18
                    ) to calculate
                    
                    By (
                    B.22
                    ) we then obtain the characteristic function of the binomial distribution
                     (B.23)
                    which we shall use in the following.
                    



B.3.1 Proof of Laplace's Limit Theorem

                    Characteristic functions allow us to show
                    
                      why
                      
                      
                    
                    the
                    discrete
                    binomial distribution at large
                    N
                    can be approximated by the
                    continuous
                    normal distribution, as claimed in Sect. 
                    5.​4
                    . One starts with a sequence of binomially distributed random variables
                    
                    (
                    
                    ) with the probability functions
                    
                    By (
                    B.23
                    ) each such distribution possesses the characteristic function
                    
                    We introduce standardized random variables
                    
                    and denote the characteristic function of each of them by
                    
                    . By (
                    B.21
                    ) we get
                     (B.24)
                    The terms in the brackets can be expanded in a power series:
                    
                    Here for each
                    t
                    one has
                    
                    . When this is inserted in (
                    B.24
                    ), we get
                     (B.25)
                    The limit of the sequence of characteristic functions
                    
                    is thus a continuous function, which is just the characteristic function of the standardized normal distribution. The aid to the final result is the theorem (given without proof): "
                    If the sequence of characteristic functions

at any real
t
converges to the function

and if

is continuous on an arbitrary small interval

                    ,
                    the sequence

of corresponding distribution functions converges to the distribution function
F
                    (
                    x
                    ),
                    whose characteristic function is precisely

                    ." This means that for any
                    x
                    one has
                    
                    so, at large
                    N
                    (and any
                    x
                    ) also
                    
                    Put in a more practical form: if the experiment outcome
                    A
                    has a probability
                    p
                    (
                    
                    ,
                    
                    ) of occurring and
                    X
                    is its frequency in
                    N
                    trials of this experiment, then for arbitrary real numbers
                    a
                    and
                    b
                    (
                    
                    ) it holds that
                    
                    Now we understand why, at large
                    N
                    , the binomial distribution could be approximated by the normal distribution with the same mean and variance as possessed by the given binomial distribution. This realization is known as the
                    Laplace's limit theorem
                    (in its integral form).
                  

                    By the same token the general central limit theorem can be derived that applies to any probability distribution, as long as its first and second moments exist. The tool is always the same: we power-expand the characteristic function and analyze its behaviour in the
                    
                    limit, which always has the form (
                    B.25
                    ).
                  


B.3.2 Inverting the Characteristic Function and Uniqueness of the Density

                    The characteristic function—as well as its closest relative, the moment-generating function—uniquely determine the probability distribution. In other words, the probability distribution and the characteristic functions offer equivalent description of statistical properties of a random variable. Both worlds are linked by the Fourier transformation: the inverse of (
                    B.18
                    ) is
                    
                    while the inverse of (
                    B.19
                    ) is
                    
                    But one must realize that the distribution is
                    not
                    necessarily uniquely determined if all its moments are known. A well-known case [7] are the probability densities
                     (B.26)
                    which have very different functional dependencies (see Fig. 
                    B.3
                    ) yet identical moments, namely
                    
                    This is
                    
                      the
                      
                      
                    
                    
                      
                      
                    
                    so-called
                    indeterminate moment problem
                    , briefly outlined below.
                    Fig. B.3
                            Example of different probability densities with identical moments. The function
                            
                            is the probability density of the
                            log-normal distribution:
                            is
                            Y
                            is a normally distributed continuous random variable and
                            
                            , then
                            X
                            is log-normally distributed
                          


                    
                      If
                      
                      
                      the
                    
                    variables
                    X
                    and
                    Y
                    have identical moments, their characteristic functions
                    
                    and
                    
                    have identical expansions near the origin of the real axis. But having equal expansions does not say much about the equality of
                    
                    and
                    
                    , as the expansion may not converge at all—the terms are calculable in principle, but they can not be summed: in the case just mentioned the convergence of the Taylor series of the characteristic function corresponding to the log-normal density (
                    B.26
                    ),
                    
                    is zero:
                    
                    However, in specific cases the convergence
                    is
                    guaranteed (Theorem 9.6.2 in [8]): if one can find
                    
                    such that near the origin,
                    
                    , the expected value of
                    
                    is finite, i. e. 
                    
                    , then
                    
                    is absolutely convergent for
                    
                    . Then one may conclude
                    
                    or
                    
                    and under these conditions the equality of the
                    
                      moments
                      
                      
                    
                    
                      
                      
                      of
                      X
                    
                    and
                    Y
                    implies the equality of their probability distributions.
                  

References
1.
                            S. Širca, M. Horvat,
                            Computational Methods for Physicists
                            (Springer, Berlin, 2012)
                           2.
                            J.K. Cavers, On the fast Fourier transform inversion of probability generating functions. J. Inst. Math. Appl.
                            22
                            , 275 (1978)
                           3.
                            J. Abate, W. Whitt, Numerical inversion of probability generating functions. Oper. Res. Lett.
                            12
                            , 245 (1992)
                           4.
                            J. Abate, W. Whitt, The Fourier-series method for inverting transforms of probability distributions. Queueing Syst.
                            10
                            , 5 (1992)
                           5.
                            S. Kullback, An application of characteristic functions to the distribution problem of statistics. Ann. Math. Stat.
                            5
                            , 263 (1934)
                           6.
                            E. Lukacs, Applications of characteristic functions in statistics. Indian J. Stat. A
                            25
                            , 175 (1963)
                           7.
                            C. Berg, Indeterminate moment problem and the theory of entire functions. J. Comput. Appl. Math.
                            65
                            , 27 (1995).
                           8.
                            T. Kawata,
                            Fourier Analysis in Probability Theory
                            (Academic Press, New York, 1972)
                           








Appendix C



Random Number Generators

Abstract
                   Methods of generating almost random numbers by means of computer algorithms are presented, starting from integer-based linear and non-linear congruential generators of uniformly distributed random numbers. They are followed by a discussion of methods to draw random numbers from arbitrary continuous distribution, and a brief mention of the ways to generate truly random numbers.
              

                Statistical methods and numerical procedures often require us to
                
                  use
                  
                
                random samples or some kind of "source" of numbers that are as random as possible, that is,
                pseudo-random
                . The computer namely can not do anything "by chance", so in order to generate pseudo-random numbers we rely on
                deterministic
                processes of computing particular sequences that are only
                seemingly
                random [1]. Generating
                
                  pseudo-random
                  
                
                numbers—labeled simply 'random' in the following—is called
                drawing
                .
              

C.1 Uniformly Distributed Pseudo-Random Numbers

                  In order to generate uniformly distributed pseudo-random numbers
                  
                    one
                    
                    
                  
                  
                    
                    
                  
                  
                    
                    
                  
                  
                    
                    
                    uses
                    uniform generators
                  
                  . They are supposed to deliver uniform numbers
                  
                  , distributed according to (
                  3.​1
                  ).
                

                  The sequences
                  
                  generated by a good uniform generator are expected to be
                  uncorrelated:
                  this means that the vectors of sub-sequences
                  
                  are as weakly correlated as possible, for each
                  k
                  separately. One also wishes for the sequence to possess a
                  long period:
                  it should not repeat itself too quickly. Besides, one would like the sequence
                  
                  to be
                  uniform and unbiased
                  , meaning that the same number of generated points fall in the same volume of space. An important request is a good uniformity of the distribution of the points
                  
                  in a
                  k
                  -dimensional hypercube, with
                  k
                  as large as possible: this is known as the
                  serial uniformity of the sequence
                  .
                

                  Most uniform generators are devised in integer arithmetic. Such generators return numbers with equal probabilities on the interval
                  
                  , where
                  
                  or
                  
                  . Uniform generators are standard components of general libraries and tools, e.g. 
                  rand()
                  in
                  Matlab
                  and C/C++,
                  gsl_rng_rand
                  in
                  GSL
                  or
                  Random[]
                  in
                  Mathematica
                  . Random integers
                  
                  generated by an integer generator can be converted to uniformly distributed real numbers
                  
                  by using the transformations
                  
                  If one uses floating-point arithmetic (precision
                  
                  , mantissa length
                  n
                  ), the numbers generated in this way have
                  
                  random most significant bits, which is often not enough, and certainly less than
                  n
                  . An approximation of a real number
                  
                  on the interval [0, 1) with all bits random is obtained by independently drawing integers
                  
                  and using the formula
                  
                  , where
                  
                  .
                

C.1.1 Linear Congruential Generators

                    Classical random generators are based on the
                    
                      relation
                      
                      
                    
                    of congruence.
                    2
                    Congruential generators of numbers
                    
                    , where
                    
                    , are defined by the
                    transition function

                    and
                    
                      the
                      
                    
                    
                      
                      
                    
                    
                      
                    
                    
                      
                    
                    relation
                    
                    where
                    k
                    is the generator
                    order
                    . Thus
                    
                    is restricted to
                    
                    by the congruence relation modulo
                    m
                    . The initial state of the generator
                    
                    is a unique function of the number called the
                    seed
                    by which the sequence is completely determined: a generator initialized by the same seed always delivers the same sequence of numbers. If
                    
                    is a linear function of parameters, one refers to
                    linear
                    generators, otherwise they are
                    non-linear
                    .
                  

                    The simplest
                    linear congruential generator
                    
                      (LCG)
                      
                      
                    
                    
                      
                    
                    
                      
                      has
                    
                    the form
                     (C.1)
                    where
                    a
                    is the
                    multiplier
                    and
                    c
                    is the
                    carry
                    parameter, while
                    
                    is the seed. Since
                    
                    are determined by
                    
                    and can take only
                    m
                    different values, the period of a LCG is at most
                    m
                    for
                    
                    and at most
                    
                    for
                    
                    .
                  

Example

                      Take a LCG with
                      
                      ,
                      
                      ,
                      
                      and
                      
                      . Run the recurrence (
                      C.1
                      ) a hundred times: we get
                      
                      , then again
                      
                      , and so on. The period of the generator is therefore only 30, but this is not the only problem. If subsequent pairs
                      
                      are plotted on a graph—do it!—we realize that all points lie on straight lines with slope 3. That certainly does not appear to be random!
                    

                      Is one better off by increasing
                      m
                      and
                      a
                      , and changing
                      c
                      ? Take, for instance,
                      
                      ,
                      
                      and
                      
                      : this corresponds to the default generator in the 32-bit
                      glibc
                      library. Figure 
                      C.1
                      (left) shows the distribution of subsequent triplets
                      
                      . Obviously the points are arranged in planes and this deficiency of LCG persists at larger
                      k
                      as well: in general the points
                      
                      do not fill the entire
                      k
                      -dimensional hypercube, but rather lie on at most
                      
                      hyperplanes. Besides, the least significant bits are less random that the rest (Fig. 
                      C.1
                      (right)). A good generator ought to produce points on many hyperplanes and make all their bits random. In applications where such deficiencies are irrelevant, LCG-type generators are nevertheless put to good use, as they are supported by all programming languages, simple and fast.
                      



Fig. C.1
                            [Left] Zoom-in of the phase space
                            
                            of points
                            
                            of the sequence
                            
                            obtained by the standard random generator from the
                            glibc
                            library with
                            
                            . [Right] The bits
                            b
                            of the random numbers
                            
                            (
                            black
                             
                            
                            ,
                            white
                             
                            
                            )
                          


                    Further representatives of the LCG family are the generators of the Add-with-Carry (AWC), Subtract-with-Borrow (SWB) and Multiply-with-Carry (MWC) type:
                    
                    The SWB algorithm is the basis of the
                    RANLUX
                    generator from the
                    GSL
                    library.
                    Multiple recursive generators
                    (MRG) are also in wide-spread use:
                     (C.2)
                    where
                    
                    are constants. The MR generators usually exhibit much larger periods than simple LC generators. If
                    m
                    is a prime, the maximal period may be as high as
                    
                    . An example of such a generator of the fifth order is
                    



C.1.2 Non-linear Congruential Generators

                    In general, non-linear generators are more random than linear ones,
                    
                      
                      
                    
                    
                      
                      
                      but
                    
                    they are also slower. Their main representatives are the
                    inversive congruential generators
                    (ICG) defined by the recurrence
                    
                    where
                    
                    , and the
                    explicit inversive congruential generators
                    (EICG) based on the relation
                    
                    For prime modules
                    m
                    the generators of IC and EIC types generate points that avoid accumulation in planes, a behavior so typical of the LC generators, yet modular inversion is a numerically intensive procedure, while the filling of space tends to be slightly less uniform.
                  


C.1.3 Generators Based on Bit Shifts

                    A completely different approach to generating random numbers is offered by
                    feedback shift register
                    generators. If the numbers
                    
                    are written as
                    n
                    -plets of bits, the relation (
                    C.2
                    ) can be written as
                     (C.3)
                    where all variables can take only values 0 or 1. It turns out that the recurrence (
                    C.3
                    ) can be performed by shifting bits: an example is shown in Fig. 
                    C.2
                    .
                    Fig. C.2Example of bit shifts in a FSR-type generator. The pushed-out bit 1 at the extreme left and the bit 1 deeper in the register are combined by an exclusive "or" (XOR). The result 0 replaces the missing bit at the extreme right


                    The evicted bit is then combined by the pattern of bits on its right by using a variety of logical operations. The recurrence (
                    C.3
                    ) often has the form
                    
                    or
                    
                    , where
                    
                    is the exclusive "or" (adding 0 and 1 modulo 2). For
                    n
                    -tuples
                    
                    this means
                    
                    , where the operation
                    
                    is performed bit-wise. This game can be continued: if
                    
                    are interpreted as
                    n
                    -dimensional vectors, they can be multiplied by
                    
                    matrices:
                    
                    Where is all this heading? The matrix
                    A
                    can be used to
                    twist
                    the bit
                    n
                    -tuples prior to being logically combined, thereby increasing the randomness of the generated
                    
                    . Such "kneading" of bit samples is at the heart of the
                    Mersenne twister
                    generator [2] (algorithm MT19937), which we recommend for serious applications. It is implemented in 32-bit integer arithmetic, has been theoretically well explored and is accessible in standard packages and libraries. Its period is
                    
                    and is serially uniform for dimensions
                    
                    . Its weakness is a somewhat lower randomness of subsequent
                    
                      bits
                      
                      
                    
                    between consecutive generated numbers.
                  


C.1.4 Some Hints for Use of Random Generators
Any random number generator, no matter how sophisticated, has some deficiency, which is usually very specific. If we, as non-specialists, need a generator to be invoked many times in our code, we may consider the following guidelines.

                    Only choose a generator devised and tested by experts. The code should be as terse as possible and based on integer arithmetic in favor of greater speed. Use generators with long periods and high serial uniformity in as many dimensions as possible. If the generator is accessible in source code, incorporate it into the program, as modern compilers can link the code segments in the form of
                    inline
                    functions. Before using a generator, study its statistical properties and ascertain whether its deficiencies may jeopardize the correctness of the result. Perform each calculation by using different generators and different seeds.
                  



C.2 Drawing from Arbitrary Continuous Distributions

                  A generator of random numbers with arbitrary distribution is obtained by transforming the numbers returned by a uniform generator. An exhaustive overview of the area is offered by the classical monograph [3]; here we present some cases of transformations to continuous distributions most commonly encountered in physics. For transformation to discrete distributions see Sect. 
                  C.2
                  in [4].
                

C.2.1 Uniform Distribution Within a Circle or Sphere

                    How do we draw points that are homogeneously distributed within
                    
                      a
                      
                      
                      
                    
                    
                      
                      
                      
                    
                    circle? Homogeneity means: the ratio of a tiny probability
                    
                    that the drawn point falls in a tiny surface element, to its area,
                    
                    , is equal to the ratio of probability 1 that the point falls in the whole circle, to its area,
                    
                    :
                    
                    Therefore we must draw
                    uniformly
                    in
                    
                    from 0 to 1 (not
                    r
                    from 0 to 1!) and in
                    
                    from 0 to
                    
                    . We need two random numbers
                    
                    and compute
                    
                    In the three-dimensional case the circle area
                    
                    needs to be replaced by the sphere volume
                    
                    , and the area element
                    
                    by the volume element
                    
                    . Thus
                    
                    Hence we must draw uniformly in
                    
                    from 0 to 1, uniformly in
                    
                    from
                    
                    to 1 (not
                    
                    from 0 to
                    
                    !) and uniformly in
                    
                    from 0 to
                    
                    . The three numbers
                    
                    drawn according to these distributions define the point
                     (C.4)




                    C.2.2 Uniform Distribution with Respect to Directions in
                    
                    and
                    


                    A uniform distribution over
                    directions
                    in space
                    
                      (usually
                      
                      
                      
                    
                    
                      
                      
                      
                    
                    ) is called isotropic. Isotropy means that the ratio between the number of points
                    
                    on the small surface
                    
                    on the unit sphere to an infinitesimal solid angle
                    
                    , is equal to the ratio of the number of points
                    N
                    on the whole surface to the full solid angle
                    
                    . A frequent beginner's mistake is to uniformly draw the angles
                    
                    and
                    
                    according to
                    
                    and
                    
                    , respectively, and compute
                    
                    . But this generates points that prefer to accumulate near the poles, as shown in Fig. 
                    C.3
                    (left). The correct way to draw is by recipe (
                    C.4
                    ), where the radial coordinate is simply ignored. This results in a homogeneous surface distribution, as shown in Fig. 
                    C.3
                    (right).
                    Fig. C.3
                            Generating an isotropic distribution in
                            
                            . [Left] Incorrect drawing by using
                            
                            ,
                            
                            . [Right] Correct drawing by using
                            



                    The points
                    
                    , uniformly distributed over the
                    
                    -dimensional sphere
                    
                    , can be generated by independently drawing the components of the vector
                    
                    with probability density
                    N
                    (0, 1) and normalizing it:
                    
                    , where
                    
                    .
                  


C.2.3 Uniform Distribution Over a Hyperplane

                    The points
                    
                    ,
                    
                    , uniformly distributed over a hyperplane defined by the
                    
                      equation
                      
                      
                    
                     
                    
                    (
                    
                    ,
                    
                    ), are generated by independently drawing
                    d
                    components of the vector
                    
                    with exponential density
                    
                    (see Table 
                    C.1
                    ) and calculating [5]
                    



C.2.4 Transformation (Inverse) Method

                    Our knowledge of variable transformations
                    
                      from
                      
                      
                    
                    Sects. 
                    2.​7
                    and
                    2.​10
                    can be used to generate random numbers according to an arbitrary continuous distribution. We know how uniform numbers
                    
                    can be generated; but as for arbitrary probability densities
                    
                    and
                    
                    one has
                    
                    , this means that
                    
                    since
                    
                    . The solution of this equation is
                    
                    , where
                    
                    is the distribution function of
                    X
                    . In other words,
                    
                    where
                    
                    is the
                    inverse
                    function of
                    
                    (not its reciprocal value). Clearly we have obtained a tool to generate random variables distributed according to
                    
                    (see Fig. 
                    C.4
                    (left)).
                    Fig. C.4Generating random numbers according to arbitrary continuous distributions. [Left] Transformation (inverse of distribution function) method. [Right] Rejection method


                    The transformation method is useful if the inverse
                    
                    is relatively easy to compute. The collection of such functions is quickly exhausted; some common examples are listed in Table 
                    C.1
                    .
                    Table C.1Generating random numbers according to chosen probability distributions by the transformation methodDistribution






















































                          Note that drawing
                          Y
                          by the uniform distribution
                          U
                          (0, 1) is equivalent to drawing by
                          
                          . For the normal distribution see also Sect. 
                          C.2.5



Example

                      Let us
                      
                        construct
                        
                        a
                      
                      generator of dipole electro-magnetic radiation! The distribution of radiated power with respect to the solid angle is
                      
                      ,
                      
                      where the normalization constant has been determined by
                      
                      . (The radiation is uniform in
                      
                      .) The corresponding distribution function is
                      
                      The desired distribution in
                      
                      is obtained by drawing the values
                      x
                      according to
                      U
                      (0, 1) and calculating
                      
                      . The inverse of
                      
                      is annoying but can be done. By substituting
                      
                      the problem amounts to solving the cubic equation
                      
                      , for which explicit formulas exist. Alternatively, one can seek the solution of the equation
                      
                      .
                      




C.2.5 Normally Distributed Random Numbers

                    If
                    
                    and
                    
                    are
                    
                      independent
                      
                    
                    
                      
                      
                      random
                    
                    variables, distributed
                    
                      as
                      
                      
                    
                    
                      
                      
                      
                    
                    and
                    
                    , their Box-Muller transformation [6]
                    
                    yields independent random variables
                    
                    and
                    
                    , distributed according to the standard normal distribution
                    N
                    (0, 1). The variables
                    
                    and
                    
                    define the length
                    
                    and the directional angle
                    
                    of a planar vector
                    
                    . The numerically intensive calculation of trigonometric functions can be avoided by using Marsaglia's implementation (see [7], Chap. 7, Algorithm P):
                    


                    The drawn vector
                    
                    on average uniformly covers the unit circle, while approximately
                    
                    generated points are rejected, so that for one pair
                    
                    one needs to draw
                    
                    uniform numbers.
                  

                    Values of the random vector
                    
                    , distributed according to the multivariate probability density (
                    4.​23
                    ) with mean
                    
                    and correlation matrix
                    
                    are generated by independently drawing
                    d
                    components of the vector
                    
                    by the standardized normal distribution
                    N
                    (0, 1) and computing
                    
                    where
                    L
                    is the lower-triangular
                    
                    matrix from
                    
                      the
                      
                      
                    
                    
                      
                      
                    
                    
                      
                      
                      
                      Cholesky
                    
                    decomposition of the correlation matrix,
                    
                    .
                  


C.2.6 Rejection Method

                    Suppose we wish to draw random numbers according to some
                    
                      complicated
                      
                      
                    
                    density
                    f
                    , while some very efficient way is at hand to generate the numbers according to another, simpler density
                    g
                    . We first try to find
                    
                    such that
                    f
                    is bounded by
                    Cg
                    from above as tightly as possible (Fig. 
                    C.4
                    (right)), that is, to ensure
                    
                    for all
                    x
                    with
                    C
                    as close to 1 as possible. Then the random numbers
                    Y
                    distributed according to
                    f
                    can be generated by the procedure:
                    1.
                            Generate the value
                            x
                            of random variable
                            X
                            according to density
                            g
                            .
                           2.
                            Generate the value
                            u
                            of random variable
                            U
                            according to
                            U
                            (0, 1).
                           3.
                            If
                            
                            , assign
                            
                            (
                            x
                            is "accepted"), otherwise return to step 1 (
                            x
                            is "rejected").
                           


                    Does this recipe really do what it is supposed to do? Let us define the event
                    
                    . From the given recipe and the Figure it is clear that
                    
                    hence
                    
                    Now define the event
                    
                    . We must prove that the conditional distribution function for
                    X
                    , given condition
                    B
                    , is indeed
                    F
                    , that is, we must check
                    
                    For this purpose we first calculate
                    P
                    (
                    B
                    |
                    A
                    ), where we exploit the definition of conditional probability (
                    1.​10
                    ) in the form
                    
                    ,
                    
                    and then invoke the product formula (
                    1.​10
                    ) for the final result
                    


Example

                      For the Cauchy distribution with
                      
                        probability
                        
                      
                      
                        
                      
                      
                        
                        
                      
                      
                        
                        
                      
                      density (
                      3.​18
                      ) the distribution function and its inverse are easy to calculate:
                       (C.5)
                      To generate the values of a Cauchy-distributed variable
                      X
                      one could therefore resort to the transformation method by using in (
                      C.5
                      ) a random variable
                      U
                      , uniformly distributed over
                      
                      —or, due to symmetry, over [0, 1]—and calculating
                      
                      (third row of Table 
                      C.1
                      ). But since computing the tangent is slow, it is better to seek the values of
                      X
                      as the ratios between the projections of the points within the circle onto
                      x
                      and
                      y
                      axes. These points are uniformly distributed with respect to the angles. We use the algorithm
                      


                      Note that the fraction of rejected points is
                      
                      and
                      
                        that
                        
                        the
                      
                      accepted points
                      
                      lie in the upper half of the unit circle. (Check this!)
                      






                  C.3 Generating
                  Truly
                  Random Numbers
                

                  If we wish to cast off the burden of the 'pseudo' attribute in our discussion and generate truly random numbers, we must also reach for a genuinely random process. An example of such process is the radioactive decay of atomic nuclei, which is exploited by the
                  HotBits
                  generator of random bit sequences [8]. The laboratory maintains a sample of radioactive cesium, decaying to an excited state of barium, electron and anti-neutrino with a decay time of
                  
                  :
                  
                  The decay instant is defined by the detected electron. The time of the decay of any nucleus in the source is completely random, so the time difference between subsequent decays is also completely random. The apparatus measures the time differences between two
                  pairs
                  of decays,
                  
                  and
                  
                  , as shown in the figure.
                






                  If
                  
                  (within instrumental resolution), the measurement is discarded. If
                  
                  , the value 0 is recorded, and if
                  
                  , the value 1 is recorded. The sense of comparing
                  
                  to
                  
                  is reversed with each subsequent pair in order to avoid systematic errors in the apparatus or in the measurement that could bias one outcome against the other. The final result is a random bit sequence like
                

1111011100100001101110100010110001001100110110011100111100000001


0100001010011111111001011101111001101001101110000100010110001111
                   ...
                
The speed of generation depends on the activity of the radioactive source.

Example

                    Imagine a descent along a binary tree (Fig. 
                    C.5
                    ) where each branch point represents a random step to the left (
                    
                    ) with probability
                    p
                    or to the right (
                    
                    ) with probability
                    
                    . (The left-right decision can be made, for example, by "asking" the radioactive source discussed above.) The values
                    
                    corresponding to the traversed branches are arranged in a
                    k
                    -digit binary number
                    
                    and suitably normalized,
                    
                    so that we ultimately end up with
                    
                    . What is the expected value of
                    
                    in the decimal system (base 10)? The individual digits
                    
                    take the values 0 or 1 with probabilities
                    
                    . Obviously
                    
                    , hence
                    
Fig. C.5
                            Binary tree used to generate a random
                            k
                            -digit binary number
                          


                    The variance of
                    
                    is
                    
                    We have thus devised a generator of
                    truly random
                    numbers, distributed according to
                    U
                    [0, 1). In particular, for
                    
                    one indeed has
                    
                    , while
                    
                    , as expected of a uniform distribution.
                    



References
1.
                          P. L'Ecuyer, Uniform random number generators, in
                          Non-uniform random variate generation
                          ,
                          International Encyclopedia of Statistical Science
                          , ed. by M. Lovric (Springer, Berlin, 2011)
                         2.
                          M. Matsumoto, T. Nishimura, Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator. ACM Trans. Model. Comput. Simul.
                          8
                          , 3 (1998)
                         3.
                          L. Devroye,
                          Non-uniform Random Variate Generation
                          (Springer, Berlin, 1986)
                         4.
                          S. Širca, M. Horvat,
                          Computational Methods for Physicists
                          (Springer, Berlin, 2012)
                         5.
                          M. Horvat, The ensemble of random Markov matrices. J. Stat. Mech.
                          2009
                          , P07005 (2009)
                         6.
                          G.E.P. Box, M.E. Muller, A note on the generation of random normal deviates. Ann. Math. Stat.
                          29
                          , 610 (1958)
                         7.
                          D. Knuth,
                          The Art of Computer Programming, Volume 2: Seminumerical Algorithms
                          , 3rd edn. (Addison-Wesley Professional, Reading 1998)
                         8.
                          J. Walker, Hotbits; see
                          http://​www.​fourmilab.​ch/​hotbits
 







Appendix D



Tables of Distribution Quantiles

Abstract
                   Definite integrals of the normal distribution are given in tabular form, along with the most frequently used quantiles of the
                
                ,
                t
                and
                F
                distributions.
              

                Definite integrals of some distributions have awkward
                
                  analytic
                  
                
                expressions, so one
                
                  may
                  
                
                prefer to read them off from tables. Table 
                D.1
                lists the integrals of the standardized normal distribution (Fig. 
                D.1
                (top left)), Table 
                D.2
                contains the values of the
                
                function, and Table 
                D.3
                has the quantiles
                
                of the
                
                distribution with
                
                degrees of freedom (Fig. 
                D.1
                (top right)). Table 
                D.4
                lists the quantiles
                
                of the Student's
                t
                distribution with
                
                degrees of freedom (Fig. 
                D.1
                (bottom left)). Tables 
                D.5
                and
                D.6
                contain the 95. percentile (
                
                ) and 99. percentile (
                
                ), respectively, of the
                F
                distribution with
                
                and
                
                degrees of freedom in the numerator and denominator, respectively (Fig. 
                D.1
                (bottom right)).
                Fig. D.1
                        [Top left] Definite integral of the standard normal distribution (
                        3.​10
                        ) and (
                        3.​12
                        ) from 0 to
                        z
                        . [Top right] Definition of the
                        p
                        th quantile of the
                        
                        distribution (
                        3.​21
                        ). [Bottom left] Definition of the
                        p
                        th quantile of the
                        t
                        distribution (
                        3.​22
                        ). [Bottom right] Definition of the
                        p
                        th quantile of the
                        F
                        distribution (
                        3.​23
                        )
                      
Table D.1
                        Integral of the standardized normal distribution (
                        3.​10
                        ) and (
                        3.​12
                        ) from 0 to
                        z
                        in steps of 0.01
                      
z
01234567890.00.00000.00400.00800.01200.01600.01990.02390.02790.03190.03590.10.03980.04380.04780.05170.05570.05960.06360.06750.07140.07540.20.07930.08320.08710.09100.09480.09870.10260.10640.11030.11410.30.11790.12170.12550.12930.13310.13680.14060.14430.14800.15170.40.15540.15910.16280.16640.17000.17360.17720.18080.18440.18790.50.19150.19500.19850.20190.20540.20880.21230.21570.21900.22240.60.22570.22910.23240.23570.23890.24220.24540.24860.25170.25490.70.25800.26110.26420.26730.27040.27340.27640.27940.28230.28520.80.28810.29100.29390.29670.29950.30230.30510.30780.31060.31330.90.31590.31860.32120.32380.32640.32890.33150.33400.33650.33891.00.34130.34380.34610.34850.35080.35310.35540.35770.35990.36211.10.36430.36650.36860.37080.37290.37490.37700.37900.38100.38301.20.38490.38690.38880.39070.39250.39440.39620.39800.39970.40151.30.40320.40490.40660.40820.40990.41150.41310.41470.41620.41771.40.41920.42070.42220.42360.42510.42650.42790.42920.43060.43191.50.43320.43450.43570.43700.43820.43940.44060.44180.44290.44411.60.44520.44630.44740.44840.44950.45050.45150.45250.45350.45451.70.45540.45640.45730.45820.45910.45990.46080.46160.46250.46331.80.46410.46490.46560.46640.46710.46780.46860.46930.46990.47061.90.47130.47190.47260.47320.47380.47440.47500.47560.47610.47672.00.47720.47780.47830.47880.47930.47980.48030.48080.48120.48172.10.48210.48260.48300.48340.48380.48420.48460.48500.48540.48572.20.48610.48640.48680.48710.48750.48780.48810.48840.48870.48902.30.48930.48960.48980.49010.49040.49060.49090.49110.49130.49162.40.49180.49200.49220.49250.49270.49290.49310.49320.49340.49362.50.49380.49400.49410.49430.49450.49460.49480.49490.49510.49522.60.49530.49550.49560.49570.49590.49600.49610.49620.49630.49642.70.49650.49660.49670.49680.49690.49700.49710.49720.49730.49742.80.49740.49750.49760.49770.49770.49780.49790.49790.49800.49812.90.49810.49820.49820.49830.49840.49840.49850.49850.49860.49863.00.49870.49870.49870.49880.49880.49890.49890.49890.49900.49903.10.49900.49910.49910.49910.49920.49920.49920.49920.49930.49933.20.49930.49930.49940.49940.49940.49940.49940.49950.49950.49953.30.49950.49950.49950.49960.49960.49960.49960.49960.49960.49973.40.49970.49970.49970.49970.49970.49970.49970.49970.49970.49983.50.49980.49980.49980.49980.49980.49980.49980.49980.49980.49983.60.49980.49980.49990.49990.49990.49990.49990.49990.49990.49993.70.49990.49990.49990.49990.49990.49990.49990.49990.49990.49993.80.49990.49990.49990.49990.49990.49990.49990.49990.49990.49993.90.50000.50000.50000.50000.50000.50000.50000.50000.50000.5000
Table D.2
                        Values of the
                        
                        function (
                        3.​8
                        ) from 0 to
                        z
                        in steps of 0.01
                      
z
01234567890.00.00000.01130.02260.03380.04510.05640.06760.07890.09010.10130.10.11250.12360.13480.14590.15690.16800.17900.19000.20090.21180.20.22270.23350.24430.25500.26570.27630.28690.29740.30790.31830.30.32860.33890.34910.35930.36940.37940.38930.39920.40900.41870.40.42840.43800.44750.45690.46620.47550.48470.49370.50270.51170.50.52050.52920.53790.54650.55490.56330.57160.57980.58790.59590.60.60390.61170.61940.62700.63460.64200.64940.65660.66380.67080.70.67780.68470.69140.69810.70470.71120.71750.72380.73000.73610.80.74210.74800.75380.75950.76510.77070.77610.78140.78670.79180.90.79690.80190.80680.81160.81630.82090.82540.82990.83420.83851.00.84270.84680.85080.85480.85860.86240.86610.86980.87330.87681.10.88020.88350.88680.89000.89310.89610.89910.90200.90480.90761.20.91030.91300.91550.91810.92050.92290.92520.92750.92970.93191.30.93400.93610.93810.94000.94190.94380.94560.94730.94900.95071.40.95230.95390.95540.95690.95830.95970.96110.96240.96370.96491.50.96610.96730.96840.96950.97060.97160.97260.97360.97450.97551.60.97630.97720.97800.97880.97960.98040.98110.98180.98250.98321.70.98380.98440.98500.98560.98610.98670.98720.98770.98820.98861.80.98910.98950.98990.99030.99070.99110.99150.99180.99220.99251.90.99280.99310.99340.99370.99390.99420.99440.99470.99490.99512.00.99530.99550.99570.99590.99610.99630.99640.99660.99670.99692.10.99700.99720.99730.99740.99750.99760.99770.99790.99800.99802.20.99810.99820.99830.99840.99850.99850.99860.99870.99870.99882.30.99890.99890.99900.99900.99910.99910.99920.99920.99920.99932.40.99930.99930.99940.99940.99940.99950.99950.99950.99950.99962.50.99960.99960.99960.99970.99970.99970.99970.99970.99970.99982.60.99980.99980.99980.99980.99980.99980.99980.99980.99980.99992.70.99990.99990.99990.99990.99990.99990.99990.99990.99990.99992.80.99990.99990.99990.99990.99990.99990.99991.0001.0001.0002.91.0001.0001.0001.0001.0001.0001.0001.0001.0001.0003.01.0001.0001.0001.0001.0001.0001.0001.0001.0001.000


                Note that the integral of the standardized normal distribution (Table 
                D.1
                ) and the value of the
                
                function (Table 
                D.2
                ) are related by
                
                The distribution function of the standardized normal distribution is
                
Table D.3
                        Quantiles
                        
                        of the
                        
                        distribution (
                        3.​21
                        ) with
                        
                        degrees of freedom for some typical (most commonly used) values of
                        p
                        from 0.005 to 0.999
                      





























10.0000.00020.00100.00390.01580.1020.4551.322.713.845.026.637.8810.820.0100.02010.05060.1030.2110.5751.392.774.615.997.389.2110.613.830.0720.1150.2160.3520.5841.212.374.116.257.819.3511.312.816.340.2070.2970.4840.7111.061.923.365.397.789.4911.113.314.918.550.4120.5540.8311.151.612.674.356.639.2411.112.815.116.720.560.6760.8721.241.642.203.455.357.8410.612.614.416.818.522.570.9891.241.692.172.834.256.359.0412.014.116.018.520.324.381.341.652.182.733.495.077.3410.213.415.517.520.122.026.191.732.092.703.334.175.908.3411.414.716.919.021.723.627.9102.162.563.253.944.876.749.3412.516.018.320.523.225.229.6112.603.053.824.575.587.5810.313.717.319.721.924.726.831.3123.073.574.405.236.308.4411.314.818.521.023.326.228.332.9133.574.115.015.897.049.3012.316.019.822.424.727.729.834.5144.074.665.636.577.7910.213.317.121.123.726.129.131.336.1154.605.236.267.268.5511.014.318.222.325.027.530.632.837.7165.145.816.917.969.3111.915.319.423.526.328.832.034.339.3175.706.417.568.6710.112.816.320.524.827.630.233.435.740.8186.267.018.239.3910.913.717.321.626.028.931.534.837.242.3196.847.638.9110.111.714.618.322.727.230.132.936.238.643.8207.438.269.5910.912.415.519.323.828.431.434.237.640.045.3218.038.9010.311.613.216.320.324.929.632.735.538.941.446.8228.649.5411.012.314.017.221.326.030.833.936.840.342.848.3239.2610.211.713.114.818.122.327.132.035.238.141.644.249.7249.8910.912.413.815.719.023.328.233.236.439.443.045.651.22510.511.513.114.616.519.924.329.334.437.740.644.346.952.62611.212.213.815.417.320.825.330.435.638.941.945.648.354.12711.812.914.616.218.121.726.331.536.740.143.247.049.655.52812.513.615.316.918.922.727.332.637.941.344.548.351.056.92913.114.316.017.719.823.628.333.739.142.645.749.652.358.33013.815.016.818.520.624.529.334.840.343.847.050.953.759.74020.722.224.426.529.133.739.345.651.855.859.363.766.873.45028.029.732.434.837.742.949.356.363.267.571.476.279.586.76035.537.540.543.246.552.359.367.074.479.183.388.492.099.67043.345.448.851.755.361.769.377.685.590.595.01001041128051.253.557.260.464.371.179.388.196.61021071121161259059.261.865.669.173.380.689.398.610811311812412813710067.370.174.277.982.490.199.3109118124130136140149
Table D.4
                        Quantiles
                        
                        of the Student's
                        t
                        distribution (
                        3.​22
                        ) with
                        
                        degrees of freedom for some typical (most commonly used) values of
                        p
                        from 0.55 to 0.999
                      























10.1580.3250.7271.0001.3763.086.3112.731.863.7318320.1420.2890.6170.8161.0611.892.924.306.969.9270.730.1370.2770.5840.7650.9781.642.353.184.545.8422.240.1340.2710.5690.7410.9411.532.132.783.754.6013.050.1320.2670.5590.7270.9201.482.022.573.364.039.6860.1310.2650.5530.7180.9061.441.942.453.143.718.0270.1300.2630.5490.7110.8961.411.892.363.003.507.0680.1300.2620.5460.7060.8891.401.862.312.903.366.4490.1290.2610.5430.7030.8831.381.832.262.823.256.01100.1290.2600.5420.7000.8791.371.812.232.763.175.69110.1290.2600.5400.6970.8761.361.802.202.723.115.45120.1280.2590.5390.6950.8731.361.782.182.683.055.26130.1280.2590.5380.6940.8701.351.772.162.653.015.11140.1280.2580.5370.6920.8681.351.762.142.622.984.99150.1280.2580.5360.6910.8661.341.752.132.602.954.88160.1280.2580.5350.6900.8651.341.752.122.582.924.79170.1280.2570.5340.6890.8631.331.742.112.572.904.71180.1270.2570.5340.6880.8621.331.732.102.552.884.65190.1270.2570.5330.6880.8611.331.732.092.542.864.59200.1270.2570.5330.6870.8601.331.722.092.532.854.54210.1270.2570.5320.6860.8591.321.722.082.522.834.49220.1270.2560.5320.6860.8581.321.722.072.512.824.45230.1270.2560.5320.6850.8581.321.712.072.502.814.42240.1270.2560.5310.6850.8571.321.712.062.492.804.38250.1270.2560.5310.6840.8561.321.712.062.492.794.35260.1270.2560.5310.6840.8561.311.712.062.482.784.32270.1270.2560.5310.6840.8551.311.702.052.472.774.30280.1270.2560.5300.6830.8551.311.702.052.472.764.28290.1270.2560.5300.6830.8541.311.702.052.462.764.25300.1270.2560.5300.6830.8541.311.702.042.462.754.23400.1260.2550.5290.6810.8511.301.682.022.422.704.09600.1260.2540.5270.6790.8481.301.672.002.392.663.961200.1260.2540.5260.6770.8451.291.661.982.362.623.84

0.1260.2530.5240.6740.8421.281.641.962.212.583.72
Table D.5
                        95. percentiles (
                        
                        ) of the
                        F
                        distribution (
                        3.​23
                        );
                        
                        degrees of freedom in the numerator and
                        
                        in the denominator
                       

234567891012152024304060120



161200216225230234237239241242244246248249250251252253254218.519.019.219.319.319.319.419.419.419.419.419.419.519.519.519.519.519.519.5310.19.559.289.129.018.948.898.858.818.798.748.708.668.648.628.598.578.558.5347.716.946.596.396.266.166.096.046.005.965.915.865.805.775.755.725.695.665.6356.615.795.415.195.054.954.884.824.774.744.684.624.564.534.504.464.434.404.3765.995.144.764.534.394.284.214.154.104.064.003.943.873.843.813.773.743.703.6775.594.744.354.123.973.873.793.733.683.643.573.513.443.413.383.343.303.273.2385.324.464.073.843.693.583.503.443.393.353.283.223.153.123.083.043.012.972.9395.124.263.863.633.483.373.293.233.183.143.073.012.942.902.862.832.792.752.71104.964.103.713.483.333.223.143.073.022.982.912.852.772.742.702.662.622.582.54114.843.983.593.363.203.093.012.952.902.852.792.722.652.612.572.532.492.452.40124.753.893.493.263.113.002.912.852.802.752.692.622.542.512.472.432.382.342.30134.673.813.413.183.032.922.832.772.712.672.602.532.462.422.382.342.302.252.21144.603.743.343.112.962.852.762.702.652.602.532.462.392.352.312.272.222.182.13154.543.683.293.062.902.792.712.642.592.542.482.402.332.292.252.202.162.112.07164.493.633.243.012.852.742.662.592.542.492.422.352.282.242.192.152.112.062.01174.453.593.202.962.812.702.612.552.492.452.382.312.232.192.152.102.062.011.96184.413.553.162.932.772.662.582.512.462.412.342.272.192.152.112.062.021.971.92194.383.523.132.902.742.632.542.482.422.382.312.232.162.112.072.031.981.931.88204.353.493.102.872.712.602.512.452.392.352.282.202.122.082.041.991.951.901.84224.303.443.052.822.662.552.462.402.342.302.232.152.072.031.981.941.891.841.78244.263.403.012.782.622.512.422.362.302.252.182.112.031.981.941.891.841.791.73264.233.372.982.742.592.472.392.322.272.222.152.071.991.951.901.851.801.751.69284.203.342.952.712.562.452.362.292.242.192.122.041.961.911.871.821.771.711.65304.173.322.922.692.532.422.332.272.212.162.092.011.931.891.841.791.741.681.62404.083.232.842.612.452.342.252.182.122.082.001.921.841.791.741.691.641.581.51604.003.152.762.532.372.252.172.102.041.991.921.841.751.701.651.591.531.471.391203.923.072.682.452.292.182.092.021.961.911.831.751.661.611.551.501.431.351.25

3.843.002.412.111.921.791.701.621.561.521.441.371.281.521.461.391.321.221.00
Table D.6
                        99. percentiles (
                        
                        ) of the
                        F
                        distribution (
                        3.​23
                        );
                        
                        degrees of freedom in the numerator and
                        
                        in the denominator
                       

234567891012152024304060120



4052500054035625576458595928598160226056610661576209623562616287631363396366298.599.099.299.399.399.399.499.499.499.499.499.499.599.599.599.599.599.599.5334.130.829.528.728.227.927.727.527.427.227.126.926.726.626.526.426.326.226.1421.218.016.716.015.515.215.014.814.714.614.414.214.013.913.813.813.713.613.5516.313.312.111.411.010.710.510.310.210.19.899.729.559.479.389.299.209.119.02613.810.99.789.158.758.478.268.107.987.877.727.567.407.317.237.147.066.976.88712.39.558.457.857.467.196.996.846.726.626.476.316.166.075.995.915.825.745.65811.38.657.597.016.636.376.186.035.915.815.675.525.365.285.205.125.034.954.86910.68.026.996.426.065.805.615.475.355.265.114.964.814.734.654.574.484.404.311010.07.566.555.995.645.395.205.064.944.854.714.564.414.334.254.174.084.003.91119.657.216.225.675.325.074.894.744.634.544.404.254.104.023.943.863.783.693.60129.336.935.955.415.064.824.644.504.394.304.164.013.863.783.703.623.543.453.36139.076.705.745.214.864.624.444.304.194.103.963.823.663.593.513.433.343.253.17148.866.515.565.044.694.464.284.144.033.943.803.663.513.433.353.273.183.093.00158.686.365.424.894.564.324.144.003.893.803.673.523.373.293.213.133.052.962.87168.536.235.294.774.444.204.033.893.783.693.553.413.263.183.103.022.932.842.75178.406.115.184.674.344.103.933.793.683.593.463.313.163.083.002.922.832.752.65188.296.015.094.584.254.013.843.713.603.513.373.233.083.002.922.842.752.662.57198.185.935.014.504.173.943.773.633.523.433.303.153.002.922.842.762.672.582.49208.105.854.944.434.103.873.703.563.463.373.233.092.942.862.782.692.612.522.42227.955.724.824.313.993.763.593.453.353.263.122.982.832.752.672.582.502.402.31247.825.614.724.223.903.673.503.363.263.173.032.892.742.662.582.492.402.312.21267.725.534.644.143.823.593.423.293.183.092.962.812.662.582.502.422.332.232.13287.645.454.574.073.753.533.363.233.123.032.902.752.602.522.442.352.262.172.06307.565.394.514.023.703.473.303.173.072.982.842.702.552.472.392.302.212.112.01407.315.184.313.833.513.293.122.992.892.802.662.522.372.292.202.112.021.921.80607.084.984.133.653.343.122.952.822.722.632.502.352.202.122.031.941.841.731.601206.854.793.953.483.172.962.792.662.562.472.342.192.031.951.861.761.661.531.38

6.634.613.783.323.022.802.642.512.412.322.182.041.881.791.701.591.471.321.00



                  D.1 Calculating Quantiles with
                  Mathematica


                  
                    Arbitrary
                    
                  
                  
                    
                    quantiles
                  
                  not given in the following tables can be calculated by interpolation or by resorting to a general tool like
                  Mathematica
                  [1]. For example, to obtain the 90. percentile of the
                  
                  distribution with
                  
                  degrees of freedom, the 0.995th quantile of the Student's
                  t
                  distribution with
                  
                  degree of freedom and the 95. percentile of the
                  F
                  distribution for
                  
                  we issue the commands
                  
                  which give (in the same order as above)
                  
                  (Compare these values to entries in the corresponding Tables.) Definite integrals of all mentioned distributions can be obtained by commands of the form
                  
                  Here we have only demonstrated a sample calculation of integrating the density of the
                  F
                  distribution with parameters required by the Example on p. 187: the four command lines listed above yield the values
                  
                  The calculation for other distributions proceeds along the same lines.
                

Reference
1.
                          S. Wolfram, Wolfram
                          Mathematica
                          .
                          http://​www.​wolfram.​com
 






Index



A


Acceptance region


Airplane engines


Auto-correlation (temporal)


Auto-regression model


Average (mean) value




B


Ballistic diffusion


Bayes formula


Binary communication channel


Binomial
distribution
formula
symbol


Black-Body radiation


Blood types


Bombardment of London


Bootstrap


Box diagram


Box-Muller transformation


Branching fraction


Breit-Wigner distribution


Brownian motion


Buffon's experiment




C


Cantelli's inequality


Carry


Cauchy distribution


Characteristic
function
scale of distribution


Characterizing function


Chebyshev inequality


Coefficient
of determination
of linear correlation


Combinations


Complete set of events


Complex random variable


Conditional probability


Confidence
interval
for correlation coefficient
for sample mean
for sample variance
level
region


Constraint equation


Constraint (Lagrange)


Continuous distribution


Convolution
continuous
discrete


Correlation
and causality
coefficient
linear (Pearson)
non-parametric (Spearman)


Covariance
matrix
sample


Cramér-Rao inequality


Cumulative function




D


Decay


                  meson
                



constant
nuclear
radioactive
time
neutron
width


                  boson
                


Decile


Detailed balance


Detection efficiency


Device failure time


Difference of events


Diffusion
ballistic
function
log-normal
normal
of thermal neutrons
self-


Dipole radiation


Discrete distribution


Distribution
binomial
normal approximation
Boltzmann
Bose-Einstein
Breit-Wigner
candidate
Cauchy



continuous
cumulative
discrete
equilibrium
exponential



Fermi-Dirac
Fréchet
function
continuous
discrete
empirical
Gumbel
hyper-exponential
hypo-exponential

                  isotropic (
                  
                  ,
                  
                  )
                
isotropic (hyperplane)
Lévy
log-normal
Lorentz
maximum-entropy
continuous
discrete
Maxwell
Maxwell-Boltzmann
mode
multinomial
multivariate normal
negative binomial

                  order
                  

normal
normal, multivariate
of blood types
of energy losses in detector
of extreme rainfall
of extreme values
of maximal values
of minimal values
of sample median
Pareto
Pascal
Poisson
stable

                  Student's (
                  
                  )
                
target
uniform
in circle
in sphere

                  over directions (
                  
                  ,
                  
                  )
                
Voigt
Weibull


Dosage of active ingredient


Drawing


Drift function




E


Effective deviation


Efficiency
of detector
of estimator
of vaccine


Electron mobility in semiconductor


El Niño


Energy
free
internal


ENSO (El Niño Southern Oscillation)


Entropy
information (Shannon)
of continuous distribution
of discrete distribution
relative
thermodynamic


Equation
Chapman-Kolmogorov
constraint
Fokker-Planck
Langevin
likelihood
Montroll-Weiss
Yule-Walker


Error function (erf)


Estimate
of spectrum (max-entropy)


Estimator
biased
consistent
efficient
unbiased



Eugene Onegin



Event
certain
complementary
compound
elementary
exclusive
impossible
incompatible
independent
universal


Exam grades


Expected value


Experiment
Buffon
random


Exponential distribution


Extinction
probability
time


Extreme value distribution




F


Fat tails


Fisher transformation


Flu medicine


Formula
Bayes
binomial
Planck
product
Sokhotsky-Plemelj
total probability


Free energy


Function
characteristic
characterizing
(cumulative) distribution
diffusion

                  Dirac (
                  
                  )
                
distribution
drift
erf
Heaviside (step)
importance
likelihood
moment-generating
partition
probability-generating
propagator moment
step (Heaviside)
transition


FWHM




G


General linear regression


Generating function
and convolutions
probability


Generator of random numbers
linear
non-linear



                  Global release of
                  





H


HXRBS


Hypercube


Hypersphere


Hypothesis
alternative
null




I


Importance function


Importance sampling


Independent variables


Inequality
Cantelli's
Chebyshev
Cramér-Rao
Jensen's


Information
entropy (Shannon)
of sample


Internal energy


Inter-quartile range (IQR)




J


Jacobi matrix


Jensen's inequality


Joint probability density




K


Kullback-Leibler distance


Kurtosis




L


Lagrange multiplier


LCG


Least median of squares (LMS)


Lévy flights


Likelihood
interval
region


Linear
congruential generator
correlation
regression
error in both coordinates


Log-normal diffusion


LOLA


Lorentz distribution




M


MAD, MADN


Magnetic dipoles


Magnetization in superconductor


Marginal probability density


Markov
chain
at long times
ergodic
irreducible
Monte Carlo
reversible
process
propagator
density function


Matrix
covariance
Jacobi
Markov (stochastic)
stochastic (Markov)
Toeplitz


Maximum-entropy
distribution
spectral analysis (MESA)


Maxwell distribution


Mean
sample


Measurable set, space


Measure
Dirac


Median
absolute deviation (MAD)
of squares of residuals
sample


MESA (Maximum-entropy spectral analysis)


Meteorites


Method
maximum likelihood
MCMC
MESA
Monte Carlo
Markov-chain (MCMC)
numerical integration
variance reduction
of least squares
rejection
transformation (inverse)


Metropolis-Hastings algorithm


Mixing (MCMC)


Mode


Model
nested
of births
of births and deaths
of deaths
of weather


Moment
generating function
indeterminate


Multinomial
distribution
symbol




N


Neutron decay time


Nimbus 7 satellite


Noise in electric circuit


Non-Parametric correlation


Normal
diffusion
distribution
multivariate
system of equations


Nuclear decay chain




O


Order of generator


Outcome of experiment


Outliers




P


Pareto
distribution
principle (80/20)


Partition function


Percentile


Permutations


Phase sum



                  Photo-disintegration of
                  



Planck formula


Pochammer symbol


Polymer molecule


Population
dynamics


Power
of test
set
spectral density (PSD)
tails


Primary ionization


Principle
maximum likelihood
of indifference
of insufficient reason
of maximum entropy
Pareto (80/20)


Probability
acceptance (MCMC)
conditional
density
in quantum mechanics
joint
marginal
Markov propagator
function
generating function
measure
of event
of extinction
posterior
prior
transition (single-step)


Problem
Monty Hall
newsboy
of indeterminate moments


Process
Markov
continuous-time
discrete-time
propagator
memoryless
Ornstein-Uhlenbeck
random
Wiener


Product
formula
of events


Propagation of errors


Propagator
density function
moment function
of Markov process


Pseudo-random numbers




Q




                  -
                  
                  plot
                


Quantile


Quantum defects in atoms


Quartile


Quasi-random sequence




R


Rabbits and foxes


Radioactive decay


Random
experiment
process
variable
complex
continuous
discrete
realization
vector
walk
continuous-time
discrete-time


Rank
correlation coefficient


Realization of random variable


Regression
by singular-value decomposition
fitting a constant
fitting a polynomial
fitting a straight line
for binned data
general linear
linear
error in both coordinates
with constraints
non-linear
robust
with orthogonal polynomials


Rejection region


Residual


Return
period
value


Risk level


Robust
regression
statistics


ROC curve




S


Sample
correlation
covariance matrix
distribution
of sums and differences
of variance ratios
of variances
mean
rank
space
statistic
variance


Scintillator (light yield)


Searching for the lost plane


Seed (random number generator)


Seismic velocity


Self-diffusion


Sensitivity of test


Sequence
quasi-random
serially uniform
Sobol
unbiased
uncorrelated
uniform


Set
measurable
power


Sharks and ice cream




                  -algebra
                


Significance of test


Single-step transition probability


Skewness


Sokhotsky-Plemelj formula


Southern oscillation


Spatial resolution of detector


Specificity of test


Spectral
analysis (maximum-entropy)
line shape
line width


Stable distributions


Standard deviation


State
periodic
reproducible


Statistic


Statistical
significance
tests


Stochastic variable


Student's distribution


Sub-diffusion


Sum of events


Super-diffusion


Symbol
binomial
multinomial
Pochammer




T


Temperature anomaly


Tensile strength of glass fibers


Test
Anderson-Darling


                  (Pearson's)
                
comparing binned data
comparing sample means
comparing sample variances
confidence level



Kolmogorov-Smirnov
of sample mean
of sample variance
parametric
power
risk level
sensitivity
specificity
statistical
statistical significance


                  (Student's)
                


Theorem
Berry-Esséen
central limit
generalized
Fisher-Tippett-Gnedenko
Gauss-Markov
Laplace's limit
Perron-Frobenius
Szegő


Thermal expansion of copper


Thinning



                  ``Three-
                  
                  '' rule
                


Three-level system


Time
decay
of extinction
to failure


Total probability formula


Transformation
Box-Muller
Fisher
of random variable


                  -dim
                
1-dim


Transition function




V


Variance
lower bound
minimal
sample


Variations




W


Width of spectral line




Footnotes


1



                        Gaussian integrals with linear terms in the exponent can be handled by using the formulas
                        


 



2



                        One has
                        
                        ; the congruence relation is commutative,
                        
                        , and transitive,
                        
                        .
                      

 













Graduate Texts in Physics

Editorial Board

Jean-Marc Di Meglio

Bâtiment Condorcet, Université Paris Diderot, Paris, France

William T. Rhodes

Dept. of Computer & Elect. Engineering &, Florida Atlantic University, Boca Raton, Florida, USA

Susan Scott

Australian National University, Acton, Australia

Martin Stutzmann

Walter-Schottky-Institut für, TU München, Garching, Germany

Andreas Wipf

Chair for Quantum Physics at FSU Jena, Friedrich-Schiller-Univ Jena, Jena, Germany





Graduate Texts in Physics

Graduate Texts in Physics publishes core learning/teaching material for graduate- and advanced-level undergraduate courses on topics of current and emerging fields within physics, both pure and applied. These textbooks serve students at the MS- or PhD-level and their instructors as comprehensive sources of principles, definitions, derivations, experiments and applications (as relevant) for their mastery and teaching, respectively. International in scope and relevance, the textbooks correspond to course syllabi sufficiently to serve as required reading. Their didactic style, comprehensiveness and coverage of fundamental material also make them suitable as introductions or references for scientists entering, or requiring timely knowledge of, a research field.

          More information about this series at
          http://​www.​springer.​com/​series/​8431






Simon Širca



Probability for Physicists













Simon Širca

Faculty of Mathematics and Physics, University of Ljubljana, Ljubljana, Slovenia





ISSN 1868-4513
e-ISSN 1868-4521



					ISBN 978-3-319-31609-3
e-ISBN 978-3-319-31611-6

DOI 10.1007/978-3-319-31611-6
Library of Congress Control Number: 2016937517
© Springer International Publishing Switzerland 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made.
Printed on acid-free paper
This Springer imprint is published by Springer Nature The registered company is Springer International Publishing AG Switzerland


Preface

University-level introductory books on probability and statistics tend to be long—too long for the attention span and immediate horizon of a typical physics student who might wish to absorb the necessary topics in a swift, direct, involving manner, relying on her existing knowledge and physics intuition rather than asking to be taken through the content at a slow and perhaps over-systematic pace.

                In contrast, this book attempts to deliver a concise, lively, intuitive introduction to probability and statistics for undergraduate and graduate students of physics and other natural sciences. Conceived primarily as a text for the second-year course on
                Probability in Physics
                at the Department of Physics, Faculty of Mathematics and Physics, University of Ljubljana, it has been designed to be as relieved of unnecessary mathematical ballast as possible, yet never to be mathematically imprecise. At the same time, it is hoped to be colorful and captivating: to this end, I have strived to avoid endless, dry prototypes with tossing coins, throwing dice and births of girls and boys, and replace them wherever possible by physics-motivated examples, always in the faith that the reader is already familiar with "at least something". The book also tries to fill a few common gaps and resurrect some content that seems to be disappearing irretrievably from the modern, Bologna-style curricula. Typical witnesses of such efforts are the sections on extreme-value distributions, linear regression by using singular-value decomposition, and the maximum-likelihood method.
              

                The book consists of four parts. In the first part (Chaps.
                1
                -
                6
                ) we discuss the fundamentals of probability and probability distributions. The second part (Chaps.
                7
                -
                10
                ) is devoted to statistics, that is, the determination of distribution parameters based on samples. Chapters
                11
                -
                14
                of the third part are "applied", as they are the place to reap what has been sown in the first two parts and they invite the reader to a more concrete, computer-based engagement. As such, these chapters lack the concluding exercise sections, but incorporate extended examples in the main text. The fourth part consists of appendices. Optional contents are denoted by asterisks
                
                . Without them, the book is tailored to a compact one-semester course; with them included, it can perhaps serve as a vantage point for a two-semester agenda.
              

                The story-telling and the style are mine; regarding all other issues and doubts I have gladly obeyed the advice of both benevolent, though merciless reviewers, Dr. Martin Horvat and Dr. Gregor Šega. Martin is a treasure-trove of knowledge on an incredible variety of problems in mathematical physics, and in particular of
                answers
                to these problems. He does not terminate the discussions with the elusive "The solution exists!", but rather with a fully functional, tested and documented computer code. His ad hoc products saved me many hours of work. Gregor has shaken my conviction that a partly loose, intuitive notation could be reader-friendly. He helped to furnish the text with an appropriate measure of mathematical rigor, so that I could ultimately run with the physics hare and hunt with the mathematics hounds. I am grateful to them for reading the manuscript so attentively. I would also like to thank my student Mr. Peter Ferjančič for leading the problem-solving classes for two years and for suggesting and solving Problem
                5.​6.​3
                .
              

                I wish to express my gratitude to Professor Claus Ascheron, Senior Editor at Springer, for his effort in preparation and advancement of this book, as well as to Viradasarani Natarajan and his team for its production at Scientific Publishing Services.
                http://​pp.​books.​fmf.​uni-lj.​si
                .
              


Simon Širca


Ljubljana




Contents




Part I Fundamentals of Probability and Probability Distributions





1 Basic Terminology

3




1.​1 Random Experiments and Events

3





1.​2 Basic Combinatorics

6




1.​2.​1 Variations and Permutations

6





1.​2.​2 Combinations Without Repetition

7





1.​2.​3 Combinations with Repetition

8






1.​3 Properties of Probability

8





1.​4 Conditional Probability

11




1.​4.​1 Independent Events

14





1.​4.​2 Bayes Formula

16






1.​5 Problems

18




1.​5.​1 Boltzmann, Bose-Einstein and Fermi-Dirac Distributions

18





1.​5.​2 Blood Types

19





1.​5.​3 Independence of Events in Particle Detection

21





1.​5.​4 Searching for the Lost Plane

22






                          1.5.5 The Monty Hall Problem
                          


22





1.​5.​6 Bayes Formula in Medical Diagnostics

25






                          1.5.7 One-Dimensional Random Walk
                          


27






References

29






2 Probability Distributions

31




2.​1 Dirac Delta

31




2.​1.​1 Composition of the Dirac Delta with a Function

33






2.​2 Heaviside Function

35





2.​3 Discrete and Continuous Distributions

36





2.​4 Random Variables

37





2.​5 One-Dimensional Discrete Distributions

37





2.​6 One-Dimensional Continuous Distributions

39





2.​7 Transformation of Random Variables

41





                          2.7.1 What If the Inverse of
                          y = h(x)
                          Is Not Unique?
                        

44






2.​8 Two-Dimensional Discrete Distributions

45





2.​9 Two-Dimensional Continuous Distributions

47





2.​10 Transformation of Variables in Two and More Dimensions

50





2.​11 Problems

56




2.​11.​1 Black-Body Radiation

56





2.​11.​2 Energy Losses of Particles in a Planar Detector

57





2.​11.​3 Computing Marginal Probability Densities from a Joint Density

58





2.​11.​4 Independence of Random Variables in Two Dimensions

60





2.​11.​5 Transformation of Variables in Two Dimensions

61





2.​11.​6 Distribution of Maximal and Minimal Values

63






References

64






3 Special Continuous Probability Distributions

65




3.​1 Uniform Distribution

65





3.​2 Exponential Distribution

67




3.​2.​1 Is the Decay of Unstable States Truly Exponential?​

70






3.​3 Normal (Gauss) Distribution

70




3.​3.​1 Standardized Normal Distribution

71





3.​3.​2 Measure of Peak Separation

73






3.​4 Maxwell Distribution

74





3.​5 Pareto Distribution

75





                          3.5.1 Estimating the Maximum
                          x
                          in the Sample
                        

77






3.​6 Cauchy Distribution

77






                        3.7 The
                        
                        distribution
                      

79





3.​8 Student's Distribution

79






                        3.9
                        F
                        distribution
                      

80





3.​10 Problems

80




3.​10.​1 In-Flight Decay of Neutral Pions

80





3.​10.​2 Product of Uniformly Distributed Variables

83





3.​10.​3 Joint Distribution of Exponential Variables

84





3.​10.​4 Integral of Maxwell Distribution over Finite Range

85





3.​10.​5 Decay of Unstable States and the Hyper-exponential Distribution

86





3.​10.​6 Nuclear Decay Chains and the Hypo-exponential Distribution

89






References

91






4 Expected Values

93




4.​1 Expected (Average, Mean) Value

93





4.​2 Median

95





4.​3 Quantiles

96





4.​4 Expected Values of Functions of Random Variables

98




4.​4.​1 Probability Densities in Quantum Mechanics

99






4.​5 Variance and Effective Deviation

100





4.​6 Complex Random Variables

101





4.​7 Moments

102




4.​7.​1 Moments of the Cauchy Distribution

105







                        4.8 Two- and
                        d
                        -dimensional Generalizations
                      

106




4.​8.​1 Multivariate Normal Distribution

110





4.​8.​2 Correlation Does Not Imply Causality

111






4.​9 Propagation of Errors

111




4.​9.​1 Multiple Functions and Transformation of the Covariance Matrix

113






4.​10 Problems

115




4.​10.​1 Expected Device Failure Time

115





4.​10.​2 Covariance of Continuous Random Variables

116





4.​10.​3 Conditional Expected Values of Two-Dimensional Distributions

117





4.​10.​4 Expected Values of Hyper- and Hypo-exponential Variables

117





4.​10.​5 Gaussian Noise in an Electric Circuit

119






                          4.10.6 Error Propagation in a Measurement of the Momentum Vector
                          


120






References

121






5 Special Discrete Probability Distributions

123




5.​1 Binomial Distribution

123




5.​1.​1 Expected Value and Variance

126






5.​2 Multinomial Distribution

128





5.​3 Negative Binomial (Pascal) Distribution

129





                          5.3.1 Negative Binomial Distribution of Order
                          k


129






5.​4 Normal Approximation of the Binomial Distribution

130





5.​5 Poisson Distribution

132





5.​6 Problems

135




5.​6.​1 Detection Efficiency

135






                          5.6.2 The Newsboy Problem
                          


136





5.​6.​3 Time to Critical Error

138





5.​6.​4 Counting Events with an Inefficient Detector

140






                          5.6.5 Influence of Primary Ionization on Spatial Resolution
                          


140






References

142






6 Stable Distributions and Random Walks

143




6.​1 Convolution of Continuous Distributions

143




6.​1.​1 The Effect of Convolution on Distribution Moments

146






6.​2 Convolution of Discrete Distributions

147





6.​3 Central Limit Theorem

149




6.​3.​1 Proof of the Central Limit Theorem

150







                        6.4 Stable Distributions
                        


153






                        6.5 Generalized Central Limit Theorem
                        


155






                        6.6 Extreme-Value Distributions
                        


156




6.​6.​1 Fisher-Tippett-Gnedenko Theorem

158





6.​6.​2 Return Values and Return Periods

159





6.​6.​3 Asymptotics of Minimal Values

161







                        6.7 Discrete-Time Random Walks
                        


162




6.​7.​1 Asymptotics

163







                        6.8 Continuous-Time Random Walks
                        


165





6.​9 Problems

167




6.​9.​1 Convolutions with the Normal Distribution

167





6.​9.​2 Spectral Line Width

168





6.​9.​3 Random Structure of Polymer Molecules

169





6.​9.​4 Scattering of Thermal Neutrons in Lead

171






                          6.9.5 Distribution of Extreme Values of Normal Variables
                          


172






References

174






Part II Determination of Distribution Parameters





7 Statistical Inference from Samples

177




7.​1 Statistics and Estimators

178




7.​1.​1 Sample Mean and Sample Variance

179






7.​2 Three Important Sample Distributions

184




7.​2.​1 Sample Distribution of Sums and Differences

184





7.​2.​2 Sample Distribution of Variances

185





7.​2.​3 Sample Distribution of Variance Ratios

186






7.​3 Confidence Intervals

188




7.​3.​1 Confidence Interval for Sample Mean

188





7.​3.​2 Confidence Interval for Sample Variance

191





7.​3.​3 Confidence Region for Sample Mean and Variance

191






7.​4 Outliers and Robust Measures of Mean and Variance

192




7.​4.​1 Chasing Outliers

193





7.​4.​2 Distribution of Sample Median (and Sample Quantiles)

194






7.​5 Sample Correlation

195




7.​5.​1 Linear (Pearson) Correlation

195





7.​5.​2 Non-parametric (Spearman) Correlation

196






7.​6 Problems

198




7.​6.​1 Estimator of Third Moment

198





7.​6.​2 Unbiasedness of Poisson Variable Estimators

199





7.​6.​3 Concentration of Mercury in Fish

199





7.​6.​4 Dosage of Active Ingredient

201






References

201






8 Maximum-Likelihood Method

203




8.​1 Likelihood Function

203





8.​2 Principle of Maximum Likelihood

204





8.​3 Variance of Estimator

206




8.​3.​1 Limit of Large Samples

207






8.​4 Efficiency of Estimator

209





8.​5 Likelihood Intervals

212





8.​6 Simultaneous Determination of Multiple Parameters

214




8.​6.​1 General Method for Arbitrary (Small or Large) Samples

214





8.​6.​2 Asymptotic Method (Large Samples)

215






8.​7 Likelihood Regions

217




8.​7.​1 Alternative Likelihood Regions

218






8.​8 Problems

219




8.​8.​1 Lifetime of Particles in Finite Detector

219





8.​8.​2 Device Failure Due to Corrosion

221





8.​8.​3 Distribution of Extreme Rainfall

222





8.​8.​4 Tensile Strength of Glass Fibers

224






References

224






9 Method of Least Squares

227




9.​1 Linear Regression

228




9.​1.​1 Fitting a Polynomial, Known Uncertainties

230





9.​1.​2 Fitting Observations with Unknown Uncertainties

232





9.​1.​3 Confidence Intervals for Optimal Parameters

235





9.​1.​4 How "Good" Is the Fit?​

236






                          9.1.5 Regression with Orthogonal Polynomials
                          


236





9.​1.​6 Fitting a Straight Line

237





9.​1.​7 Fitting a Straight Line with Uncertainties in both Coordinates

240





9.​1.​8 Fitting a Constant

240





9.​1.​9 Are We Allowed to Simply Discard Some Data?​

242






9.​2 Linear Regression for Binned Data

242





9.​3 Linear Regression with Constraints

245






                        9.4 General Linear Regression by Singular-Value Decomposition
                        


248





9.​5 Robust Linear Regression

249





9.​6 Non-linear Regression

250





9.​7 Problems

253




9.​7.​1 Two Gaussians on Exponential Background

253





9.​7.​2 Time Dependence of the Pressure Gradient

254





9.​7.​3 Thermal Expansion of Copper

255





9.​7.​4 Electron Mobility in Semiconductor

255





9.​7.​5 Quantum Defects in Iodine Atoms

256





9.​7.​6 Magnetization in Superconductor

257






References

257






10 Statistical Tests:​ Verifying Hypotheses

259




10.​1 Basic Concepts

259





10.​2 Parametric Tests for Normal Variables

264




10.​2.​1 Test of Sample Mean

264





10.​2.​2 Test of Sample Variance

265






                          10.2.3 Comparison of Two Sample Means,
                          


265






                          10.2.4 Comparison of Two Sample Means,
                          


266





10.​2.​5 Comparison of Two Sample Variances

267







                        10.3 Pearson's
                        
                        Test
                      

269




10.​3.​1 Comparing Two Sets of Binned Data

271






10.​4 Kolmogorov-Smirnov Test

271




10.​4.​1 Comparison of Two Samples

274





10.​4.​2 Other Tests Based on Empirical Distribution Functions

275






10.​5 Problems

276




10.​5.​1 Test of Mean Decay Time

276





10.​5.​2 Pearson's Test for Two Histogrammed Samples

278





10.​5.​3 Flu Medicine

279





10.​5.​4 Exam Grades

279






References

280






Part III Special Applications of Probability






                      11 Entropy and Information
                      


283




11.​1 Measures of Information and Entropy

283




11.​1.​1 Entropy of Infinite Discrete Probability Distribution

285





11.​1.​2 Entropy of a Continuous Probability Distribution

286





11.​1.​3 Kullback-Leibler Distance

287






11.​2 Principle of Maximum Entropy

288





11.​3 Discrete Distributions with Maximum Entropy

289




11.​3.​1 Lagrange Formalism for Discrete Distributions

289





11.​3.​2 Distribution with Prescribed Mean and Maximum Entropy

291





11.​3.​3 Maxwell-Boltzmann Distribution

292





11.​3.​4 Relation Between Information and Thermodynamic Entropy

294





11.​3.​5 Bose-Einstein Distribution

295





11.​3.​6 Fermi-Dirac Distribution

296






11.​4 Continuous Distributions with Maximum Entropy

297





11.​5 Maximum-Entropy Spectral Analysis

298




11.​5.​1 Calculating the Lagrange Multipliers

300





11.​5.​2 Estimating the Spectrum

302






References

304







                      12 Markov Processes
                      


307




12.​1 Discrete-Time (Classical) Markov Chains

308




12.​1.​1 Long-Time Characteristics of Markov Chains

309






12.​2 Continuous-Time Markov Processes

313




12.​2.​1 Markov Propagator and Its Moments

314





12.​2.​2 Time Evolution of the Moments

316





12.​2.​3 Wiener Process

317





12.​2.​4 Ornstein-Uhlenbeck Process

318






References

323






13 The Monte-Carlo Method

325




13.​1 Historical Introduction and Basic Idea

325





13.​2 Numerical Integration

328




13.​2.​1 Advantage of Monte-Carlo Methods over Quadrature Formulas

332






13.​3 Variance Reduction

333




13.​3.​1 Importance Sampling

333





13.​3.​2 The Monte-Carlo Method with Quasi-Random Sequences

337







                        13.4 Markov-Chain Monte Carlo
                        


339




13.​4.​1 Metropolis-Hastings Algorithm

339






References

345






14 Stochastic Population Modeling

347




14.​1 Modeling Births

347





14.​2 Modeling Deaths

348





14.​3 Modeling Births and Deaths

351




14.​3.​1 Equilibrium State

352






                          14.3.2 General Solution in the Case
                          
                          ,
                          
                          ,
                          


353






                          14.3.3 General Solution in the Case
                          
                          ,
                          
                          ,
                          


353





14.​3.​4 Extinction Probability

353






                          14.3.5 Moments of the Distribution
                          P(t)
                          in the Case
                          
                          ,
                          


354






14.​4 Concluding Example:​ Rabbits and Foxes

357





References

359






              Appendix A: Probability as Measure
              

361



              Appendix B: Generating and Characteristic Functions
              

365


Appendix C: Random Number Generators
381


Appendix D: Tables of Distribution Quantiles
395


Index
409















