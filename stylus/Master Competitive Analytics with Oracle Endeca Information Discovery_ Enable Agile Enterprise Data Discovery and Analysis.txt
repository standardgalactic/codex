























 Copyright © 2015 by McGraw-Hill Education. All rights reserved. Except as permitted under the United States Copyright Act of 1976, no part of this publication may be reproduced or distributed in any form or by any means, or stored in a database or retrieval system, without the prior written permission of the publisher, with the exception that the program listings may be entered, stored, and executed in a computer system, but they may not be reproduced for publication.
ISBN: 978-0-07-183319-6
MHID:       0-07-183319-6
The material in this eBook also appears in the print version of this title: ISBN: 978-0-07-183318-9, MHID: 0-07-183318-8.
eBook conversion by codeMantraVersion 1.0
All trademarks are trademarks of their respective owners. Rather than put a trademark symbol after every occurrence of a trademarked name, we use names in an editorial fashion only, and to the benefit of the trademark owner, with no intention of infringement of the trademark. Where such designations appear in this book, they have been printed with initial caps.
McGraw-Hill Education eBooks are available at special quantity discounts to use as premiums and sales promotions, or for use in corporate training programs. To contact a representative please visit the Contact Us page at www.mhprofessional.com.
Oracle and Java are registered trademarks of Oracle Corporation and/or its affiliates. All other trademarks are the property of their respective owners, and McGraw-Hill Education makes no claim of ownership by the mention of products that contain these marks.
Screen displays of copyrighted Oracle software programs have been reproduced herein with the permission of Oracle Corporation and/or its affiliates.
Information has been obtained by Publisher from sources believed to be reliable. However, because of the possibility of human or mechanical error by our sources, Publisher, or others, Publisher does not guarantee to the accuracy, adequacy, or completeness of any information included in this work and is not responsible for any errors or omissions or the results obtained from the use of such information.
Oracle Corporation does not make any representations or warranties as to the accuracy, adequacy, or completeness of any information contained in this Work, and is not responsible for any errors or omissions.
TERMS OF USE
This is a copyrighted work and McGraw-Hill Education and its licensors reserve all rights in and to the work. Use of this work is subject to these terms. Except as permitted under the Copyright Act of 1976 and the right to store and retrieve one copy of the work, you may not decompile, disassemble, reverse engineer, reproduce, modify, create derivative works based upon, transmit, distribute, disseminate, sell, publish or sublicense the work or any part of it without McGraw-Hill Education's prior consent. You may use the work for your own noncommercial and personal use; any other use of the work is strictly prohibited. Your right to use the work may be terminated if you fail to comply with these terms.
THE WORK IS PROVIDED "AS IS." McGRAW-HILL EDUCATION AND ITS LICENSORS MAKE NO GUARANTEES OR WARRANTIES AS TO THE ACCURACY, ADEQUACY OR COMPLETENESS OF OR RESULTS TO BE OBTAINED FROM USING THE WORK, INCLUDING ANY INFORMATION THAT CAN BE ACCESSED THROUGH THE WORK VIA HYPERLINK OR OTHERWISE, AND EXPRESSLY DISCLAIM ANY WARRANTY, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIED WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. McGraw-Hill Education and its licensors do not warrant or guarantee that the functions contained in the work will meet your requirements or that its operation will be uninterrupted or error free. Neither McGraw-Hill Education nor its licensors shall be liable to you or anyone else for any inaccuracy, error or omission, regardless of cause, in the work or for any damages resulting therefrom. McGraw-Hill Education has no responsibility for the content of any information accessed through the work. Under no circumstances shall McGraw-Hill Education and/or its licensors be liable for any indirect, incidental, special, punitive, consequential or similar damages that result from the use of or inability to use the work, even if any of them has been advised of the possibility of such damages. This limitation of liability shall apply to any claim or cause whatsoever whether such claim or cause arises in contract, tort or otherwise.







We want to dedicate this book to our families and friends,who have supported and inspired us to achieve higher,both professionally and personally.







About the Authors
Helen Sun, Ph.D., is the coauthor of Oracle Big Data Handbook, writing chapters on Endeca and big data architecture development. She has written one of the most downloaded white papers on big data (along with numerous other white papers on information architecture and data governance). Helen is the lead author of Oracle Information Architecture and Framework and has spoken at various industry events, including as a keynote speaker at Open Group's World Conference on Big Data and Oracle OpenWorld. Helen is one of the most distinguished Oracle enterprise architects, and her main role is to assist CIOs and CTOs of Fortune 500 companies in architecture and transformations. She is a highly regarded thought leader in information and big data architecture, leveraging Oracle's most comprehensive business analytics portfolio, including Endeca Information Discovery and other Oracle Big Data solution sets. She regularly performs education and design sessions with Oracle customers on Endeca and analytics with real-world expertise on implementing business analytics using Endeca for a variety of industries, including financial services, healthcare, research, manufacturing, retail, consumer packaged goods (CGP), and the public sector.
William Smith has more than two decades of experience as an enterprise architect with a strong background in decision support systems and analytics. William's early career experience includes working with data from manufacturing control systems and instrumentation in industries such as plastics, petro-chemicals, glass, food, and extrusion processes. William has enterprise systems experience in higher education, finance, consumer research, and healthcare. In the area of analytics, William has worked in the area of predictive failures and statistical process control. William is currently working with customers making use of big data and is developing innovative solutions utilizing Endeca. William's role as an enterprise architect extends into engineered systems, where he was an early adopter of Oracle-engineered systems and is now working to develop engineered systems targeted at specific applications and markets.
About the Technical Editors
Tom Plunkett is the lead author of Oracle Big Data Handbook and other books. Tom is a principal consultant with Oracle and also teaches graduate-level computer science courses for Virginia Tech. Tom is also the chief data officer for AtrocityWatch, a nonprofit organization that uses big data to analyze social media to detect precursors to atrocities. Tom has given presentations on big data at more than 50 conferences. Prior to joining Oracle, Tom practiced patent law with Fliesler Meyer in San Francisco, representing Oracle and other Silicon Valley companies. Tom has a B.A. and J.D. from George Mason University, has an M.S. in computer science from Virginia Tech, and has completed graduate coursework in management science and engineering from Stanford University.
Richard Tomlinson is a director of product management for Oracle Business Analytics at Oracle Corporation. Richard is responsible for product strategy and go-to-market for the Oracle Business Analytics products, including BI tools and BI cloud, Endeca Information Discovery, and big data discovery. Richard has more than 19 years of experience in product marketing and development, presales, and business consulting on analytic solutions. Prior to Oracle, he was at companies specializing in data warehousing, business intelligence, and search, including Endeca, SAS, Business Objects, and Informix.








Contents at a Glance
PART IA Technical Guide to Oracle Endeca Information Discovery
1    Oracle Endeca Information Discovery Architecture
2    Powering Endeca Server
3    Designing Visualization with Endeca Studio
4    Mastering Endeca Information Discovery Integrator
PART IIBusiness Analytics Use Cases and Tutorial
5    Analytical Applications by Industry
6    Implementing Fraud Detection
7    Implementing Healthcare Correlations
8    Industry Best Practices
A   Additional Resources
      Index









Contents
      Acknowledgments
      Introduction
PART IA Technical Guide to Oracle Endeca Information Discovery
1    Oracle Endeca Information Discovery Architecture
      Oracle Endeca Information Discovery vs. Business intelligence
      Introducing Oracle Endeca Information Discovery
      Oracle Endeca Information Discovery Overview
                  Data Server
                  Data Integration Tool
                  Analytics Toolset
      Oracle Endeca Information Discovery Core Components
      Endeca Server
                  Data Model
                  Data Domains
                  Records and Attributes
                  Dgraph
                  Endeca Server Clustering
                  Endeca Query Language
                  Services
      Endeca Information Discovery Integrator
                  Integrator ETL
                  Integration Server
                  Integrator Acquisition System
                  Web Acquisition Toolkit
                  IKM SQL to Endeca Server
      Endeca Information Discovery Studio
                  Endeca Information Discovery Studio Navigation Overview
                  Anatomy of an Endeca Information Discovery Studio Application
                  Provisioning Service
                  Application Administration
                  Major Component Summary
      Endeca Licensing
      Summary
2    Powering Endeca Server
      Planning the Endeca Server Installation
                  Hardware
                  Server Memory
                  Server Storage
                  Operating System
                  Application Server
                  Exalytics
                  Securing Endeca Server
                  Endeca Server Cluster
                  Load Balancers and Endeca Server
                  Installing Endeca Server
      Managing Endeca Server
                  Data Domain Management
                  Endeca Server Backups
                  Replication
                  Performance Monitoring
                  Maintenance of Endeca Server
      Understanding Endeca Server
                  EndecaServer.properties
                  EQL
                  Resource Management
      Summary
3    Designing Visualization with Endeca Studio
      Introducing User-Driven Data Exploration
      Endeca Studio Systems Architecture
                  Endeca Minimal Implementation
                  Installation Requirements for Endeca Studio
                  Installation Requirements for Endeca Provisioning Service
                  Installation Summary
      Endeca Studio Application Development
      Example: FDIC Failed Banks Data
                  Data for the FDIC Failed Banks Example
                  Endeca Studio Application Creation for FDIC Failed Banks Data
                  Tag Cloud for FDIC Failed Banks Data
                  Map Component for FDIC Failed Banks Data
                  Using Enrichments Within Endeca Studio
                  FDIC Failed Banks Data Summary
      FDIC Failed Banks Data Advanced Analysis
                  Advanced Visualization of the FDIC Data
      Example: Flight Delay Information
      User-Driven Data Exploration and Elimination of Cognitive Bias
      Summary
4    Mastering Endeca Information Discovery Integrator
      Enterprise-Driven Data Exploration
      Endeca Information Discovery Integrator Explained
      Endeca Integrator Acquisition System
                  Endeca IAS Crawls
                  Endeca IAS Record Store
                  Example Endeca IAS Crawl
                  Endeca IAS Installation and Operations
      Endeca Web Acquisition Toolkit
                  Endeca WAT Background
                  Endeca WAT Insights
                  Endeca WAT Example
                  Endeca WAT Installation
      Endeca IAS and Endeca WAT Comparison
      Integrator ETL
                  Integrator ETL Background
                  Text Enrichment
                  Integrator ETL Basic Usage
                  Integrator ETL and Text Enrichment
                  Integrator ETL Installation
                  Integrator ETL Text Enrichment Installation
      Integrator ETL Server
                  Integrator ETL Server Background
                  Integrator ETL Server Usage
                  Integrator ETL Server Installation
      Enterprise-Driven Data Exploration and Integrative Thinking
      Summary
PART IIBusiness Analytics Use Cases and Tutorial
5    Analytical Applications by Industry
      Generic Use Cases
                  Brand Reputation
                  Human Resource
                  Customer Churn Prevention
      Industry Use Cases
                  Retail and CPG
                  Telecommunication
                  Financial Services
                  Healthcare and Life Science
                  Manufacturing
                  Public Sector
      IT Use Cases
                  Log Processing
                  Security Management
                  EDW Augmentation
                  ETL Offloading
      Summary
6    Implementing Fraud Detection
      Overview of Voter Fraud Analytics and Detection
      Implementing Voter Fraud Analytics and Detection
                  Election Results Analysis
                  A Deeper Dive of Voter Records
      Summary
7    Implementing Healthcare Correlations
      Healthcare Analytics Use Case Overview
                  Healthcare Analytics Capability Maturity Model
      Use Case Implementation
                  Claims Analysis
                  Patients Analysis
                  Operations Analysis
                  Partners
                  Clinical Research
                  Remote Monitoring
      Summary
8    Industry Best Practices
      Developing Analytical Capabilities
                  Align with the Business Architecture
                  Define the Architecture Vision
                  Assess the Current-State Architecture
                  Establish the Future-State Architecture
                  Determine the Strategic Road Map
                  Implement Governance
                  People and Skills
      Best Practices for Implementing Data Discovery in General
                  Architecture and Planning
                  Process and People
      Best Practices Implementing Oracle Endeca Information Discovery
                  Endeca Server
                  Endeca Studio
                  Integrator ETL, WAT, and IAS
      Future Trends in Analytics
                  What's Happening Today
                  What's Coming Next
      Summary
A   Additional Resources
      Endeca Integrator Reference
      Useful Links
      References
      Index









Acknowledgments
We would like to thank our families for their support in this journey of writing the book. We would like to thank Oracle and the members of the Oracle Endeca Information Discovery community for their support of this project. We would like to thank our technical editors, Richard Tomlinson and Tom Plunkett, for sharing their product expertise and insights in big data and business analytics. We would like to extend our gratitude toward the amazing team of Oracle Press and McGraw-Hill Education editors and production staff, especially Paul Carlstroem and Amanda Russell. They are truly the unsung heroes of this product.









Introduction
Gartner highlighted the top ten technologies and trends that will be strategic for most organizations in 2013. Strategic big data and actionable analytics were among those ten trends. Few technologies have captured more attention than big data, and there is tremendous interest in business use cases featuring big data and analytics.
Oracle Endeca Information Discovery delivers unique capabilities that provide simulation, association, optimization, and other discovery and analytics to empower more decision flexibility at the time and place of every business process action. Endeca is a powerful product, and it is catching significant interest in IT and various business communities. As the first book on Oracle Endeca Information Discovery, the main objective of this book is to provide actionable guidance on creating analytical capabilities using Oracle Endeca Information Discovery. The book is composed of two parts: a complete guide to Endeca technology and a comprehensive list of business analytics use cases enabled by Endeca in various industries, including financial services, healthcare, research, manufacturing, retail, CGP, and the public sector.
Part I is a technical guide on the Endeca product, including the architecture and implementation best practices. It includes the following chapters:
        Chapter 1 contains an overview of Endeca Information Discovery, including the history, key features, an architecture overview, and a comparison with traditional business intelligence technologies.
        Chapter 2 is a deep dive into Endeca Server, including data modeling, search mechanisms, queries and filters, and the Endeca analytical language.
        Chapter 3 focuses on mastering Endeca Information Discovery Studio, including best practices and usage examples of different visual components for exploration and discovery, such as tag clouds, charts, tables, and navigation sidebars, as well as features such as type-ahead. It also discusses in detail the self-servicing capabilities using provisioning services.
        Chapter 4 covers Endeca Integration Suite with a deep dive into this product, including Integrator ETL, Web Acquisition Toolkit, and Integrator Acquisition System.
Part II is a business analytics use case extravaganza. It contains a comprehensive list of business analytics use cases as well as step-by-step guides on implementing some of these use cases.
        Chapter 5 discusses analytical applications by industry with detailed descriptions of use cases for each of the following industries: financial services, healthcare, research, manufacturing, retail, CGP, and the public sector.
        Chapter 6 is a step-by-step guide on how to implement a fraud detection use case in Endeca from source to application.
        Chapter 7 is a detailed implementation guide on a healthcare use case that covers various solution areas, including patient and claims analysis, operational reporting with integration through OBIEE, remote patient monitoring, and clinical research using advanced analytics through integration with Oracle Data Mining.
        Chapter 8 discusses best practices and the enterprise architecture approach to incrementally establishing analytical capabilities and information discovery with Endeca. It also summarizes the book with a look into the future of business analytics.
Finally, the appendix includes additional resources, useful links, references, and the download site for data sources and applications used in this book.
In summary, here are the outstanding features of this book:
        A unique combination of technical how-to with business use cases that are of high interest to many readers
        A complete technical guide on the Oracle Endeca Information Discovery product inside and out, including its architecture and implementation best practices
        A comprehensive list of business analytics use cases that have not been published or compiled through any media or other channels
        Step-by-step guide on how to implement some of these use cases from data sources to analytical applications
Intended Audience
This book is intended for a wide range of audiences, including business and IT professionals such as VPs/directors of business intelligence and data warehouse, CIOs, CTOs, chief data officers, enterprise architects, data architects, information architects, and business and data analysts at various capacities. It's also a hands-on development guide for developers and designers of analytical systems, including IT developers and consulting professionals.








PARTI
A Technical Guide to Oracle Endeca Information Discovery









CHAPTER1
Oracle Endeca Information Discovery Architecture
Wisdom, itself, is often an abstraction, not associated with fact or reality, but with the man who asserts it and the manner of its assertion.
—John Kenneth Galbraith, The Great Crash 1929
This quotation, from John Kenneth Galbraith's classic title chronicling the stock market crash of 1929, is a warning for all generations. The quote comes from a chapter titled "The Twilight of Illusion," which discusses the common mental modes in the period leading up to the crash. Galbraith describes these mental modes as a "failure to know what isn't known."
The quote has particular relevance for the information age we now inhabit. Information technology has become part of every aspect of our professional lives. Corporate operations, manufacturing, healthcare, communications, and commerce all depend on enterprise systems. The information driving these systems primarily resides in relational databases, and business intelligence systems have been used for more than two decades to provide operational reporting and analytics against this information. However, there are also new sources of information, including social media, e-mail, and blogs, that can be critical to decision making. The traditional approach to business intelligence systems was not designed to support the integration and analysis of data originating in these new sources. The inability to access this information is, in the words of Galbraith, a "failure to know what isn't known." Oracle Endeca Information Discovery is complementary to business intelligence systems and facilitates using information from these new data sources. This chapter introduces Oracle Endeca Information Discovery. We start this introduction by comparing and contrasting aspects of Oracle Endeca Information Discovery to business intelligence systems.
Oracle Endeca Information Discovery vs. Business intelligence
To develop an initial understanding of Endeca, you need to appreciate the difference between Endeca and business intelligence systems. Business intelligence systems, like Oracle Business Intelligence Enterprise Edition, provide answers to known questions in reports and dashboards, and they rely on structures known as data models to define the data. Data models are inherent to relational database management systems; they document the relationship between tables in a database. Information is retrieved from the tables in the data model using Structured Query Language (SQL). The tasks of designing and maintaining data models and writing queries can be arduous and require an information technology professional who is skilled with data models and queries as well as knowledgeable with the information the tables contain. This familiarity with the information in the tables is often referred to as domain knowledge: It is the knowledge of subject-matter practices and procedures and how they are implemented in the data model and table data. A further complication is that the information in the tables change, requiring data models and queries to be modified.
Endeca differentiates itself from traditional business intelligence products in many ways. First, Endeca does not require users to formulate questions in advance of looking at the data. This is great for any circumstance where the user has no idea where to start or has only a hunch. Endeca is designed to automatically summarize every attribute in the data and allow users to further refine the scope of the data. Users can explore the data based on summarizations presented "in the moment." This facilitates an ongoing dialogue with the data, allowing information to be evaluated immediately and its importance determined, before deciding that the data is relevant to decision making. Another difference between Endeca and business intelligence tools is related to data models. Business intelligence systems require data models be designed and implemented before any meaningful use of the information can occur, whereas Endeca derives its models from the information. This eliminates the time and labor required when creating data models and eliminates the complications that can occur when the information structure or content changes. (This is a key feature of Endeca and will be covered at length in this book.) As was stated previously, Endeca allows users to explore and evaluate information. By comparison, for business intelligence systems, this type of activity is typically not performed by users, but requires an information technology professional with domain knowledge who is skilled in writing queries and who understands underlying data models. Figure 1-1 shows Endeca's web-based user interface that facilitates data exploration.




Figure 1-1. Endeca's web-based user interface
Perhaps the most compelling way that Oracle Endeca Information Discovery differentiates itself from business intelligence systems is its ability to handle different types of data. Data residing in rows and columns in relational databases belongs to a type of data known as structured data. Structured data has strongly defined data types primarily for storing text or numeric data, along with a well-defined data model. By contrast, unstructured data lacks the fixed fields and organizational features of structured data. Information from raw text found in documents, social media, e-mail, and blogs are all examples of unstructured data. Often, the information originating in unstructured data is called dirty because it is not subject to data standards and may contain ambiguous meanings, spelling errors, and abbreviations. Between structured and unstructured data is semistructured data, which has internal organization but lacks a data model. Two common examples of semistructured data are Extensible Markup Language (XML) with its tags and attributes and JavaScript Object Notation (JSON), an open format used for web services. Data in comma-separated value (CSV) files can also be classified as semistructured. Business intelligence systems are designed to work with structured data, but have challenges with unstructured data.
Oracle Endeca Information Discovery is designed to process any type of data, regardless of structure, and make it available by inferring flexible data models, accommodating dirty data, and even determining relationships to other data sources that have already been processed, regardless of their type. Table 1-1 summarizes the data types we've covered.

TABLE 1-1. Data Type Summary
Introducing Oracle Endeca Information Discovery
Endeca was founded in 1999 in Cambridge, Massachusetts, with a company name derived from the German verb entdecken, meaning "to discover." Endeca was founded at the height of the "dot com bubble" and initially produced a database to be used to expedite web searches related to e-commerce. After the bubble burst, Endeca transformed itself from a product company to a platform company, and Endeca's core in-memory database engine and associated products diverged, one continuing to support e-commerce solutions and the other emerging as an alternative business intelligence technology. E-commerce solutions powered by Endeca Server are ubiquitous and familiar to anyone who has shopped for products on the Web. Many of America's "big-box" retailers use Endeca to drive product search capability. Consumers wanting to purchase a washing machine, for example, can refine their search by manufacturer, price, capacity, and many other options. Figure 1-2 shows an example retail web site for washing machines, with refinements on the left.




FIGURE 1-2. Retail web site using Endeca e-commerce
As the concept of data discovery became more commonplace, Endeca became known as a leader in this new genre of information technology. Data discovery is a term used to describe a set of application features, including the following:
        A data server with the capability to store and model data from disparate sources that does not require predefined metadata.
        A data server with in-memory processing and indexing to maximize performance and eliminate the need for aggregations, summaries, or precalculations to occur prior to application usage.
        Users can develop and refine views and analyses of structured, semistructured, or unstructured data.
        The user interface is optimized for ease of use, allowing users to create and modify applications.
On October 18, 2011, Oracle Corporation announced its acquisition of Endeca. At the time of this acquisition, Oracle already had one of the most competitive business intelligence offerings with Oracle Business Intelligence Enterprise Edition (OBIEE). OBIEE offers best-in-class structured data management and analytics, and Oracle's decision to acquire Endeca enabled Oracle to add best-in-class data discovery and unstructured data management to its portfolio of products. Oracle Corporation has a remarkable track record of acquiring companies and their offerings and quickly integrating these acquisitions with existing offerings. Oracle Endeca Information Discovery is no exception to this: Endeca can be deployed on Oracle's WebLogic application server running on Oracle Exalytics. Endeca is certified to run on Exalytics, taking the guesswork out of hardware selection and ensuring an optimum configuration to enable superior performance and scalability.
 One of the latest features of Endeca is tight integration between it and OBIEE. Endeca has the ability to source data from an OBIEE metadata repository into Endeca Server, allowing users to query and load data from OBIEE. Guided navigation and search can be used to analyze data and join data already in Endeca. This opens the door to enriching unstructured data with structured data ingested from OBIEE. Another compelling use of this tight integration would be to allow data discovery to occur on data from OBIEE and find answers to new questions without needing to make changes to the OBIEE repository.
Oracle also has one of the most compelling big data offerings with its Big Data Appliance, and Endeca's ability to connect to a Hadoop cluster on Big Data Appliance means Oracle has offerings for every possible data storage and analytic scenario.
Oracle Endeca Information Discovery Overview
Endeca is not a single product, but a number of products and product features that are separately installed and licensed. To understand the capabilities of Endeca, it is best to start by contextualizing it against a business intelligence system. Like a typical business intelligence system, Endeca includes the following major components:
        A data server
        A data integration tool
        An analytics toolset
The Oracle Endeca Information Discovery features that differentiate it from a traditional business intelligence system are
        No need to predetermine questions to be answered by the information before ingesting the data
        End-user provisioning and exploration of data via self-service interfaces
        Ease of integration and enrichment of semistructured and unstructured data
        Unification of full-featured, advanced keyword search; data-driven guided navigation; and in-memory analytics
Let's look at the major components, compare them to a business intelligence system, and discuss how they deliver these differentiating features.
Data Server
A typical business intelligence deployment involves a database server designed specifically to store data for rapid retrieval, using indexed and denormalized tables. Likewise, Endeca features a data server designed for rapid retrieval of information acquired through information discovery activities. This design of the data server is a departure from that of a typical RDBMS, however: Data is stored as key-value pairs, and for every attribute, a full inverted search index and a membership index are created to quickly determine the association between attributes and records. The design is optimized for in-memory performance, allowing the retrieval of data to drive analytics at interactive speeds. The server also features its own query language known as Endeca Query Language (EQL), which is a rich set of SQL-like features providing both basic and complex aggregations of search results or the current navigation state.
Data Integration Tool
A typical business intelligence deployment features an extract-transform-load (ETL) tool to acquire information from other databases and data stores, perform transformations on the information, and insert or update records in the reporting database with the transformed data. Likewise, Endeca features an agile ETL tool for IT with all the typical features of a commercial-strength ETL product, along with unique unstructured data discovery features. Let's examine three significant features that provide capabilities above and beyond nominal ETL functionality: Text Enrichment, Text Enrichment with Sentiment Analysis, and the Integrator Acquisition System.
Text Enrichment is the "silver bullet" that allows Endeca to discover significance in unstructured data. It is an add-on feature that provides the ability to find terms and phrases in text and then rank and organize the findings. Text Enrichment includes text analysis capabilities for extracting topics and themes in the data to determine subject matter, as well as for extracting entities to expose people, places, organizations, quotes, products, and custom entities. Text Enrichment also contains summarization capabilities for automatically creating abstracts and topical summaries.
Text Enrichment with Sentiment Analysis delivers all the Text Enrichment capabilities as well as advanced text analysis for extracting opinion or feeling related to each extracted concept. Sentiment is extracted with a score indicating the positive and negative nature of a document, a phrase, or an entity. These scores are used to show varying ranges of positivity and negativity across the data at any point during the search or navigation state.
The Integrator Acquisition System is a set of components that crawl source data stored in a variety of formats, including file systems, JDBC databases, delimited files, web content, and custom data sources.
Analytics Toolset
A typical business intelligence deployment features an analytics toolset. Likewise, Endeca features a web-based analytics suite designed to require little or no training to build or consume new discovery applications. The in-memory performance of the Endeca data server enables analysis to be performed at interactive speeds using an intuitive UI pioneered in online commerce, where ease of use is critical to mass consumer adoption. The Endeca UI combines the best of search, guided navigation, and in-memory analytics to provide the end user with all the tools necessary to discover new insights in structured and unstructured content. The Endeca analytics toolset also allows end users to drag and drop from a library of discovery components to easily create their own applications for personal or small-scale use. It also allows IT to create advanced discovery applications with security and failover that may be published to and consumed by the end-user community. Endeca fully supports both IT provisioned discovery applications and self-service discovery. In self-service mode, end users may upload their own data sets from personal files on their desktop (such as Excel or JSON) and then "mash up" this data with data enterprise sources, including the data warehouse or the Oracle Common Enterprise Information Model.
We've just compared the major components of Endeca to typical business intelligence systems, so you should now have a general understanding of Endeca. Next let's look deeper at these components to gain a better understanding of the Endeca architecture.
Oracle Endeca Information Discovery Core Components
Oracle Endeca Information Discovery comprises three core components. The sections that follow provide an overview of these components and how they relate to the major Endeca features that have been discussed. Chapters 2, 3, and 4 cover these components in greater detail. Figure 1-3 shows an overview of the Endeca core components. These core components are




FIGURE 1-3. Endeca core components
        Endeca Server A hybrid search and analytic database with capabilities to incorporate wide, diverse, and changing data; architected for discovery
        Endeca Information Discovery Integrator A set of tools for loading and optionally enriching diverse information, including structured, semistructured, and structured data
        En deca Information Discovery Studio A discovery application composition environment providing drag-and-drop authoring to create interactive, visually rich information discovery applications
Endeca Server
Endeca Server is at the heart of Endeca and is the core search-analytical database. In Endeca Server, data is organized using a highly flexible data model known as a faceted data model. With this data model, it is not necessary to define a unified schema before loading and analyzing data; data models are derived from the data that is stored in the database, and every record has its own schema based on its own generated attributes. This is irrespective of the data source or whether the source is structured or unstructured.
Data Model
Structured data can be directly loaded into a faceted model using standard ETL tools. Each row becomes a record, and each column becomes an attribute. An example of such data is a sales transaction. Each transaction row becomes a record, and every element of the transaction record becomes an attribute.
Semistructured data from enterprise applications, various feeds, and XML sources can also be loaded as attribute and value pairs. This is a common cause of heterogeneous record structure. For the sales transaction record, it is possible to extend it with more information about specific products. The attributes that go with each product could be very different depending on the product. For example, a road bike has different components than a mountain bike. With Endeca, attributes become attribute and value pairs, and jagged records begin to emerge that look dissimilar, meaning the data sets do not have the same data model but have some commonality between them. With relational database technologies, this is difficult to implement, but the key-value pair data structure in Endeca makes it possible to implement and extend.
With Endeca Server, unstructured data can be linked to records by any available key. In addition, unstructured elements can be stored as their own records for side-by-side analysis. Some examples are documents, RSS feeds, Twitter and Facebook data, and data feeds from discussion forums. To continue with the sales transaction data mentioned previously, you could now integrate into the record online customer reviews and the customers' Facebook or Twitter comments on the product and transaction.
The way to accomplish this through Oracle Endeca Information Discovery is to take the textual fields into the records as new attributes. These attributes can be combined in the same record with the sales transactions, product details, and customer review information. Here is a summary of the data sources in the examples we mentioned:
  
Endeca allows the mapping of sales transactional records with product details and employee information through a product ID and a sales rep ID. The online reviews can also be mapped to the transaction records through transaction IDs that are captured. In addition, users can create a whitelist text tagger to tag the employee first name or last name mentioned in the review text to an employee by full name. Now unstructured, semistructured, and structured data is aligned and loaded and could be analyzed side by side.
Data Domains
Data domains are the largest unit of data over which Endeca Server allows queries to be expressed. No data domains exist on an Endeca Server instance immediately after installation, so a server administrator must create each data domain. Users can specify meaningful names such as staffing, sales, or marketing to identify data domains. The Data Ingest Web Service (DIWS) facilitates loading data into a data domain.
Records and Attributes
Records are the fundamental unit of data in a data domain on Endeca Server. As data is loaded, or ingested, it is stored in data records. Data records generally correspond to traditional records in a source database, but differ in that they are standardized for consistency and classified with attributes. Attributes are the "facets" of the data; they are the storage for metadata. As an example, consider opinion data ingested from unstructured data regarding consumer opinions on automobiles. The data records would have attributes indicating automobile make, model, year, a sentiment such as "recommend" or "lemon," and a feature of the automobile such as "handling" or "transmission."
Dgraph
For each data domain, Endeca Server creates a run time process called Dgraph that manages data domain operations. The Dgraph process of Endeca Server is the main computational module that provides the features of Endeca Server, such as search, refinement computation, and guided navigation. Dgraph maintains indexes of records that are searchable documents of domain data associated with attributes.
The Dgraph is stateless, which facilitates the addition of Dgraph processes for load balancing and redundancy. When more processing capability is required than can be achieved on one Endeca Server instance, an Endeca Server cluster can be deployed.
Endeca Server Clustering
Endeca Server clustering can be used in production deployments to handle heavy workloads and is deployed across multiple servers. Nodes can be added to an Endeca Server cluster as additional processing needs emerge, ensuring the scalability of the Endeca Server cluster. The central feature of the Endeca cluster architecture is a data domain cluster, which is a set of Dgraph processes that handles requests across multiple nodes. One of the Dgraph processes is designated as the leader node, and all other Dgraph processes are referred to as follower nodes. The leader node handles all write and update requests, while the follower nodes allow only read operations. A shared file system is used for disk-based versions of indexes, and only the leader node has write access to the file system. The node hosting the leader node Dgraph process must also host the Cluster Coordinator service, which is responsible for intercluster communications between Dgraph processes and for notifying follower nodes when indexes or data has changed.
Endeca Query Language
Endeca Query Language is a powerful integrated analytics language built within Oracle Endeca Server. EQL enables power users to define and create new metrics to compose their own discovery applications. Built on the core capabilities of Oracle Endeca Server, EQL extends the capabilities of Oracle Endeca Server with a rich analytic language that allows users to explore aggregated and pivoted views of large volumes of data. EQL also supports a variety of data types, including numerical, geospatial, and date/time values that enable applications to work with temporal data, performing time-based sorting, filtering, and analysis. IT professionals have full access to the language for the purpose of building special formulas, metrics, and more that can be made available in discovery applications. Some of the most important EQL features include tight integration with search and navigation, rich analytical functionality, and processing efficiency, as well as a familiar development experience.
This is an example of a simple EQL statement:

An EQL statement starts with either DEFINE or RETURN. DEFINE doesn't return the result set. Rather, it creates the data set as a temp table. A DEFINE statement is typically followed by a RETURN statement that consumes the result set. Endeca uses the DEFINE statement to create views that can then be used to generate charts and other advanced visualization.
The statement then needs to have one or many SELECT elements, separated by commas. The SELECT clause is composed of an expression followed by an alias. Expressions are usually one or more attributes, operators, or functions such as summation or average, as you see in the previous example.
The GROUP BY clause specifies the method of aggregation. Other EQL capabilities include joining, sorting, paging, and filtering. Oracle's documentation for Endeca Server includes product documentation titled "Oracle Endeca Server: EQL Guide." This is an extensive reference for EQL, and will help developers and users learn more about EQL's capabiltites.
Services
Most of the Endeca Server APIs are exposed as SOAP web services. These services are used by other Endeca components to interact with Endeca Server. The major services are briefly discussed in the sections that follow.
Data Ingest Web Service
Data Ingest Web Service (DIWS) provides an interface to ETL tools to load data into the data domains hosted in Oracle Endeca Server.
Conversation Web Service
This web service provides the primary means of querying data in the data domain hosted in Oracle Endeca Server. This service is used by Endeca Information Discovery Studio to query Oracle Endeca Server.


Endeca Server Version Information
Be aware that Endeca Server version numbers do not coincide with the version numbers of the other core components. The current version of Endeca Server is 7.6, whereas the current versions of Endeca Information Discovery Integrator and Endeca Information Discovery Studio are 3.1.


Entity and Collection Configuration Web Service
This web service, also known as sConfig, allows you to create and update collections and views for collections or for data sets.
Configuration Web Service
This web service is for updating the schema and configuring the records in a data domain.
Endeca Information Discovery Integrator
This section provides more detail about the major components of Endeca Information Discovery Integrator. The Endeca Information Discovery Integrator consists of the following five components:
        Integrator ETL
        Integration Server
        Integrator Acquisition System
        Web Acquisition Toolkit
        IKM SQL to Endeca Server
Integrator ETL
Integrator ETL is part of Endeca Information Discovery Integrator and is an integration platform that enables source records to be extracted from many types of sources. Integrator is a powerful graphical-based development environment; its primary purpose is to develop, debug, and execute ETL processes. The graphic diagrams of these processes are referred to as graphs. A graph is a series of sequential components that process data. The user interface for Integrator ETL is based on the popular open-source Eclipse integrated development environment (IDE). Integrator ETL can run graphs without Integrator Server; when the integration server is not used, the graphs are run on the machine running the Integrator ETL IDE.
Integrator ETL is very capable and supports connectivity to a wide variety of other systems and software. JDBC is used to provide connectivity to the following databases:
        Oracle RDBMS
        MySQL
        Microsoft SQL Server
        Informix
        IBM DB2
        PostgreSQL
Bulk loader capability is available for Oracle, Microsoft SQL, Informix, and IBM DB2.
Integration with systems supporting Java Message Service (JMS) is available for the following:
        IBM WebSphere MQ
        Apache Active MQ
        JBoss Messaging
 The following protocols are also supported:
        HTTP
        HTTPS
        FTP
        FTPS
        SMTP
Some proprietary formats are also supported, which include
        Microsoft Excel
        FoxPro (dBase)
Integration Server
The integration server allows graphs to be treated as a production process and run in an enterprisewide team environment. As discussed in the previous section, graphs can run in Integrator ETL. The integration server provides an alternative to this and should be considered for any large-scale deployment of Endeca. As with any production process tool, the integration server allows graphs to be scheduled and monitors the execution status of graphs.
The integration server comes with a web-based user interface for configuration and administration. In addition, the integration server features an API for remote operations and interoperability with other systems.
Integrator Acquisition System
The Integrator Acquisition System (IAS) is a set of components that crawl source data stored in a variety of formats, including file systems, JDBC databases, flat files, web sources, and custom data sources. IAS transforms the data, if necessary, and outputs it to an XML file or a record store that can be accessed by Integrator ETL. Within IAS there are two major processing entities: the IAS Server and the Web Crawler.
The IAS Server crawls JDBC sources, file systems, or custom-created sources. The number of types of sources and file formats supported is extensive. Table 1-2 lists the types of sources and some examples to illustrate the power and versatility that is available with this product. For a complete list of supported formats, please refer to the appendix.

TABLE 1-2. Example IAS Server Crawl File Types
The IAS Web Crawler is installed by default as part of the IAS. The IAS Web Crawler gathers data by crawling HTTP and HTTPS web sites. Once a crawl is completed, the Integrator ETL can access the data acquired during the crawl. A crawl usually writes data directly to Endeca Server. However, data can also be written to an XML file for debugging and development. The Web Crawler is for large-scale crawling and has an architecture that allows developers to create custom plug-ins. Plug-ins provide a means to extract additional content, such as HTML meta tags, from web pages.
Web Acquisition Toolkit
The Endeca Web Acquisition Toolkit (WAT) provides an intuitive, simple-to-use graphical interface for collecting content from the Web, allowing users to rapidly access and integrate any information exposed through a web front end. Endeca WAT allows for information to be collected from many sources, including content from consumer sites, industry forums, government or supplier portals, cloud applications, and other big data sources.
Endeca Web Acquisition Toolkit Design Studio is an IDE for building data integration workflows. Endeca Web Acquisition Toolkit Design Studio combines the best aspects of a web browser and a visual flow editor and eliminates the need to write code by enabling developers to visually navigate applications and data sources and by using a powerful XML editor to generate workflows in minutes.
IKM SQL to Endeca Server
IKM SQL to Endeca Server provides an integration module that allows users of the Oracle Data Integrator (ODI) to write directly to Endeca Server.
Endeca Information Discovery Studio
Oracle Endeca Information Discovery Studio is a web-based application that allows business analysts to rapidly assemble dashboard applications. These applications enable analysts and other end users to explore a full range of structured and unstructured enterprise data from Endeca Server. Each application consists of one or more pages, with each page containing a set of graphical components. Endeca Information Discovery Studio components include functions to
        Navigate to or search for specific data
        Display detailed information about data
        Display graphical representations of the data
        Manipulate and analyze the data
        Highlight specific data values
Endeca Information Discovery Studio is easy to deploy and is ideal for the agile development of enterprise-quality applications. Endeca Information Discovery Studio provides a library of UI components that embody best practices in information discovery applications. Because Endeca Information Discovery Studio is component based, its applications are simple to control, adapt, and extend.
In Endeca Information Discovery Studio, application locale determines the language used for an application. In addition, application locale determines the default data formatting for currency, numbers, and dates. Endeca Information Discovery Studio supports the following languages:
        French
        German
        Italian
        Spanish
        Japanese
        Korean
        Simplified Chinese
        Traditional Chinese
        Portuguese-European
Note that this is a subset of the languages supported by Oracle Endeca Server.
Endeca Information Discovery Studio has administrative features that are accessible to Endeca Studio administrators and developers. Administrative features are part of the Control Panel, where the following operations are available:
        Monitoring Endeca Studio usage
        Creating and managing data sources
        Creating and managing Endeca Server connections
        Configuring Endeca Studio settings
        Monitoring Endeca Studio performance
        Managing and configuring applications
        Managing Endeca Information Discovery Studio users and user access
Endeca Information Discovery Studio Navigation Overview
When logging in to Endeca Information Discovery Studio, the user is presented with three application subsets on the left sidebar, as shown in Figure 1-4 .




FIGURE 1-4. Endeca navigation overview
Clicking the link for any of the subsets reveals a page with icons for accessing applications. Table 1-3 describes the groups.

TABLE 1-3. Endeca Information Discovery Studio Application Subsets
Users who are not administrators for any applications will be able to access only Certified Applications and Community Applications.
Application Information Icon
The application information icon allows users to view summary information about an application. This information includes data sets used, application creator, and application access (public or private).
Application Icon
An application is accessed when a user clicks the application icon.
Anatomy of an Endeca Information Discovery Studio Application
Endeca Information Discovery Studio applications allow users to search and analyze data from Oracle Endeca Server data domains. An Endeca Studio application consists of two or more pages, and each page features components that enable filtering, navigating, or viewing data. Endeca Information Discovery Studio applications promote information discovery, allowing Studio users to uncover previously unknown relationships and trends as they investigate business issues. Endeca Information Discovery Studio applications can be quickly built using the data display and visualization tools provided. The following section summarizes the major features available within an Endeca Studio application.
Chart Components
Endeca applications provide insight into data using chart components. These tools will be familiar to users of spreadsheet software. Many chart components are available, including line charts, bar charts, stacked bar charts, area charts, scatter diagrams, bubble charts, and pie charts.
Tabular Data Elements
Tabular data elements allow detailed viewing of data. These elements include result tables and pivot tables. Tables support extensive formatting options, allowing users to change row and column colors, shades, fonts, and borders.
Summarization Bar
The summarization bar displays key performance indicators and can provide alerts when a critical value or condition for a metric occurs.
Search Box
A search box allows users to search for specific values in attributes. When attributes are located matching the search criteria, display components are updated immediately, including charts, tables, and maps.
Refinement Component
Refinement components allow users to improve the displayed data using search terms or selected attribute values. As users refine the data, the other components may be updated to include only the data for the current refinement.
Tag Cloud
The tag cloud is a visualization tool that may not be familiar to many users, but it provides a unique visualization experience. Tag clouds display all the attribute values for a data set, with more prevalent attributes being displayed in larger text. For example, for attributes whose values are for a brand of car, the text might display the words Ford, Chevrolet, Chrysler, Volkswagen, and so on. A value for each can be displayed with the text. Figure 1-5 shows an example of a tag cloud.




FIGURE 1-5. Example of a tag cloud
Map Components
The map component uses Oracle's map viewer to display and analyze geographic information contained within data. With the map component, three types of layers are possible:
        Numbered point layers display numbered points on a map and corresponding detail for each numbered point.
        Point layers display points on a map and can have multiple layers active, using colors to differentiate between layers.
        A heat map displays a point for each location on a map and a shaded cloud with gradient colors to show relative density of the points on the map or the change in associated value between locations. Figure 1-6 shows an example of a heat map.




FIGURE 1-6. Example of a heat map
User Convenience Features
In addition to the data display components just covered, an Endeca Information Discovery Studio application has features that are convenient and useful:
        Bookmarks allow users to save a given navigation and component state so that they can return to it at a later time or e-mail it to other users.
        Data can be exported from components to CSV files for use outside of Endeca Information Discovery Studio.
Data Sets
Data used in an Endeca Studio application from one source is called a data set. It is possible for an Endeca Studio application to have only one data set, but most applications will have multiple data sets. Data sets can be from an Endeca Server data domain, and they can be from sources other than an Endeca server, including user-uploaded Excel spreadsheets, user-uploaded JSON files, JDBC data sources, and Oracle BI servers. Application administrators have many options for managing data sets, including using filters and specifying columns to be used from tables. As data sets are created from imported or uploaded data, their attributes are determined and made available.
Text Enrichment can be used to enhance data by creating new attributes. When Text Enrichment terms are specified, data sources are searched for these terms. When matches for the terms occur, attributes are created. Enrichment can be based on a single term or a list of terms. For example, the terms nice, pleasant, and fun can be used to create an attribute called positive. This is particularly useful on unstructured data. To use Text Enrichment, the data enrichment plug-in needs to be installed on the Endeca Server being used.
Views and Attribute Groups
Views and attribute groups are useful organization tools within Endeca Information Discovery Studio because they provide a useful mechanism for adding order to the information from data sets. Views in Endeca Information Discovery Studio are similar in some regard to views in an Oracle RDBMS.
 A view is a logical collection of information that is derived from the records in application data sets. Views are composed of attributes, which can be attributes of a data set or of a derived value. For example, a view could be composed of a list of products and a calculated total sales attribute for each product. Views are useful because they allow a subset of data to be defined and encapsulated in intuitive containers for later use.
Within views, attribute groups can organize attributes within a view. For example, an attribute group within a view containing human resources information could be created and named address info to contain all the mailing address information such as street address, city, and ZIP code.
Refinement Rules
For Endeca applications that contain more than one data set, refinement rules allow users to specify relationships between data sets. Refinement rules work best with attributes that contain the same or similar values. When a data set is added to an existing application, Endeca Studio can automatically create refinement rules for attributes that have the same attribute name, data type, multivalue setting, and refinement behavior. Refinement rules are useful because they limit attributes to those that are relevant to both data sets. For example, a data set containing human resources data and a data set containing members of an engineering group can have a refinement rule created that effectively selects only members of the engineering group.
This section summarized the major components of an Endeca Information Discovery Studio application. The components are covered in detail in later chapters.
Provisioning Service
Oracle Endeca Information Discovery Studio includes a provisioning service that facilitates dynamic application creation from data uploaded from a user's desktop. The provisioning service profiles and transforms data into appropriate formats before sending it to Endeca Server for ingestion. Endeca Studio users can upload data from the following sources:
        Microsoft Excel spreadsheets
        JSON files, such as Twitter data files
        Relational databases supported by a JDBC driver
        Oracle Business Intelligence Server
Figure 1-7 shows an example of an application created with the provisioning service using an Excel file containing publicly available city employee data. This application was created with minimal effort.




FIGURE 1-7. Application created with provisioning service
Application Administration
The Application Settings page in Oracle Endeca Information Discovery Studio is available to administrators and developers and is used for configuring and administering an application and for managing the views and groups for the application data. One of the most important administrative tasks is managing application access and application membership. This is discussed in the next two sections.
Application Access Management
Endeca administrators can set an application to be one of two application types:
        Public Public applications are available to all Endeca Information Discovery Studio users.
        Private Private applications are available to application members only. An application member is an Endeca Information Discovery Studio user who has been granted access to an application by the application administrator.
Application Membership
Applications can have two different roles:
        Application members These people have view access to all the content in an application but cannot edit or configure the application or its membership. This is the default application role for a new application member.
        Application administrators These people have complete control over an application. They can edit the application content, configure the application, and control application membership, including whether a member is an administrator. The user who creates the application is automatically an application administrator. An application administrator can also assign other users as application administrators.
Major Component Summary
In the previous three sections, we covered the three core components of Endeca. You are certainly aware that Endeca has its own architecture and terminology unique from other products. If some of the terminology or concepts presented are somewhat unclear, don't worry. All of these components will be covered at length in the next three chapters, with each component given a full chapter.
Endeca Licensing
Oracle licenses each of the core components separately. There are three plug-ins that are also licensed separately, listed here:
        Oracle Endeca Web Acquisition Toolkit This is an add-on module for the Endeca Information Discovery Integrator.
        Oracle Endeca Text Enrichment This includes text analysis capabilities for extracting people, places, organizations, quotes, and themes as well as summarization capabilities for automatically creating abstracts and topical summaries.
        Oracle Endeca Text Enrichment with Sentiment Analysis This delivers all the Text Enrichment capabilities as well as advanced text analysis for extracting aggregate sentiment related to each extracted concept. Sentiment is extracted with a score indicating the positive and negative nature of a document, a phrase, or an entity. These scores are used to show varying ranges of positivity and negativity in search, guided navigation, and analytics.
Oracle sales consultants and Oracle license resellers can provide assistance and answer questions regarding Endeca licensing.
Summary
This chapter introduced Oracle Endeca Information Discovery and provided introductory information on each of the Endeca core components. The next three chapters will deal with these core components in detail:
        Chapter 2 covers installing, configuring, and managing Endeca Server.
        Chapter 3 covers getting Endeca Information Discovery Studio up and running and using it to design and run information discovery applications.
        Chapter 4 guides you through Endeca ETL installation and usage, Endeca ETL Server installation and configuration, Integrator Acquisition System installation and configuration, and other topics associated with the Endeca Information Discovery Integrator.








CHAPTER2
Powering Endeca Server
For a successful technology, reality must take precedence over public relations, for Nature cannot be fooled.
—Richard F. Feynman
Theoretical physicist and Nobel laureate Richard Feynman gave us this pearl of wisdom, and for those who are tasked with selecting and deploying technology, it is eerily familiar. Once a technology is selected to address a problem, to "scratch an itch," the reality of planning the deployment of the technology begins. Oracle Endeca Server is at the center of any successful deployment of Endeca, and carefully planning the installation of Endeca Server is the most important step in ensuring a successful installation.
This chapter is organized into three major areas:
        Planning the Endeca Server Installation Hardware, operating system, and other infrastructure considerations are explored in detail.
        Managing Endeca Server In this section, we cover the day-to-day management of Endeca Server, including performance monitoring, production planning, and backups.
        Understanding Endeca Server We end the chapter with an examination of the processes and memory structures behind Endeca Server.
Planning the Endeca Server Installation
Endeca is a user-centric product; its infrastructure is sophisticated and of enterprise scale. Endeca Server sits at the center of this complex ecosystem. Refer to Figure 2-1 for an overview of the Endeca ecosystem. Endeca Integrator ETL pushes data sets into Endeca Server, and Endeca Server makes them available to Endeca Studio.




FIGURE 2-1. Overview of the Endeca ecosystem
Hardware
Before we delve into the approach for selecting hardware, let's discuss some realities of hardware selection. This often is determined by capacity needs for the immediate future. This approach prevents purchasing more server hardware and product licenses than are needed. However, this approach can be short-sighted when taking into account the normal productive life span of enterprise server hardware, which is three to five years. As you begin the hardware selection process for Endeca Server, assume that its usage will be twice or three times what is projected for the immediate future. This approach allows for growth and ensures that the Endeca Server installation will be ready when users request new projects or uses for Endeca.
Endeca Server is a Java application that is designed for 64-bit Intel x86 processor servers. Server hardware selection involves selecting sufficient processor cores, server memory, and storage to accommodate the anticipated workload. The section that follows describes an approach to capturing workload information and estimating the number of processor cores required. Keep in mind you should use the information presented as a general guide, and if you are considering the purchase of Endeca Server and hardware, you should consult with Oracle engineers to arrive at a more precise estimate.
Workload Analysis and Processor Core Count
Processor core count determines the number of threads of execution that can occur concurrently on a server, without latency. When the number of threads of execution exceeds the available CPU cores, the threads of execution must wait. Endeca Server is designed to limit the number of threads of execution it uses to avoid waits. In Chapter 1, we introduced the Dgraph process of Endeca, the main computational module that provides the features of Endeca Server. Dgraph processes are associated with data domains and have parameters stored in data domain profiles. These parameters affect how the Dgraph process behaves. The parameter --num-compute-threads specifies the number of computational threads in the Dgraph threading pool. This value is usually set to the number of processor cores available on the server. With this parameter, the number of threads possible for the Dgraph process is directly linked to the number of processor cores on the server, and this dictates the throughput of Endeca Server and the amount of workload the server can process. Hence, in selecting the number of processor cores an Endeca Server should have, it is necessary to estimate workload. For Endeca Server, these factors determine workload:
        Number of records to be processed
        Number of concurrent users
        Query complexity
        Number of queries per Endeca Studio page
Determining the number of records to be processed requires that the sources to be used for Endeca Server are known and understood. An exact number is not required; you need just a rough idea. To estimate the number of records, you can use a simple approach: Determine the total number of records in the source tables that are structured and semistructured. The number of concurrent users is simply the total number of users you expect to be using Endeca at the same time. This is not the same as the number of users who have logins and are capable of logging in to Endeca. Note that this is not necessarily the number of named user licenses owned. Also, query complexity has a significant impact on the performance of Endeca Server and is determined by how the server is used. For the purposes of analyzing hardware requirements, we will use these three levels of complexity of query usage:
        Low complexity usage The majority server usage is through Endeca Studio, with few queries.
        Medium complexity usage This level consists of Endeca Studio usage and simple EQL queries, with no more than two defines per query.
        High complexity usage This level consists of usage that includes complex, multistep EQL queries.
The next three figures present charts that depict processor cores required for Endeca Server, using concurrent users, number of records to be processed, and query complexity as variables. These charts are based on models that are used to forecast workload. Figure 2-2 shows the estimated core requirements for an Endeca Server with 1 million rows of data on a Linux server. The horizontal axis shows the number of concurrent users, and the vertical axis shows the number of x86 processor cores required.




FIGURE 2-2. Cores vs. concurrent users for a 1,000,000-row Endeca Server
Figure 2-3 shows the estimated core requirements for an Endeca Server with 10 million rows of data on a Linux server.




FIGURE 2-3. Cores vs. concurrent users for a 10,000,000-row Endeca Server
Figure 2-4 shows the estimated core requirements for an Endeca Server with 100 million rows of data on a Linux server.




FIGURE 2-4. Cores vs. concurrent users for a 100,000,000-row Endeca Server
 Based on the information shown, you can arrive at the following conclusions regarding hardware selection:
        Complex query installations should limit the number of concurrent users to avoid the need for a huge number of processor cores. If a higher number of concurrent users is expected, then a large multicore server or clustering must be used.
        Installations that will have low or medium complexity queries can get by with as few as 8 or 12 core servers, so long as concurrent users remains low, on up to 10 million rows of data.
Server Memory
Besides processor cores, server memory is one of the most critical resources for enterprise servers. Part of Endeca's high performance is due to its in-memory performance, and given this, Endeca Server must be deployed on a server with ample memory. Memory has become economically priced in the last three to five years, so there is really no reason not to purchase servers with as much memory as possible. Seldom do systems administrators and managers regret purchasing servers with too much memory, but they always regret purchasing systems with too little memory. Oracle does not provide any published guidelines regarding memory requirements, other than a minimum of 8GB. For estimating and budget purposes, you can use the following formula:
        Low complexity queries: 8GB + processor cores × 4GB
        Medium and high complexity queries: 8GB + processor cores × 8GB
Also, be aware that if you plan on using data enrichment plug-ins, such as Term Extraction, in Endeca Studio, you should plan on adding an extra 10GB per each instance of data enrichment plug-in that is expected to run concurrently in the data domain. If users in the data domain plan to run Term Extraction, for each such process, additional memory should be provisioned on all Endeca Server machines hosting this data domain.
Server Storage
Endeca Server storage must be high performance; by following these guidelines, you will ensure that storage performance is never an issue with your Endeca Server installation. Internal, locally attached storage or SAN-based storage can be used. If SAN-based storage is used, then connectivity to the SAN-based storage should be via Fibre Channel. iSCSI should be used only for test environments and should not be used for production environments. Individual drive sizes should be kept small, not to exceed 147G per spindle. Only RAID 10 should be used; RAID 5 should never be used because of the overhead it imposes. Lastly, 15,000 RPM drives are preferred, but 10,000 RPM drives can also be used.
The rationale for these recommendations is in part because Endeca Server stores all the files for indexes in a location specified by the endeca-data-dir parameter in the Endeca. properties file. The indexes for all data domains created by Endeca Server will be stored in the directory specified by this parameter. Because of this, it is not possible to provide physically separate storage for data domains located on the same Endeca Server. When specifying storage for Endeca Server, you need to be mindful of this limitation and avoid procuring too few drives and drives with large sizes, such as greater than 147G. Ten 147G drives in a RAID 10 array will provide 735GB of storage and will be sufficient for most Endeca Server installations. Because 8 percent of all drives fail within their first two years of service, there is a disadvantage to purchasing excessive storage. Moreover, drive systems are relatively easy to expand when future needs arise.
Operating System
Now that we have covered the server hardware, let's turn our attention to the operating system for Endeca Server. Endeca Server can be installed on two operating systems: Linux (Oracle Enterprise Linux or Red Hat Enterprise Linux) or Microsoft Windows. Between these two choices of operating systems, Linux will perform better than Microsoft Windows because Endeca Server has been optimized for Linux and processes a greater workload with fewer processor cores. Figure 2-5 illustrates the dramatic difference in performance between Linux and Windows.




FIGURE 2-5. Required cores for Linux versus Windows for 10,000,000 rows and medium query complexity
Besides the performance differences with Microsoft Windows, the logistical issues related to supporting this operating system can affect production environments. Security patches are issued on a weekly basis, and some require an outage. This is not preferred for a production data server. Windows also lacks the command-line scripting capability available in Linux, which is used by systems administrators to automate maintenance tasks.
Notes on Endeca Server on Linux
As mentioned, Endeca Server can be installed on either Oracle Enterprise Linux (OEL) or Red Hat Enterprise Linux (RHEL), version 5 or 6. Oracle strongly recommends installing version 6 of either OEL or RHEL to take advantage of control groups in version 6. Control groups, also called cgroups, are a kernel resource-controlling feature of the operating system. They provide a means to define and allocate the system resources to one or more specific processes, while also controlling the utilization of these resources at a high level. This ensures that the processes do not consume excessive resources. Hence, cgroups help avoid circumstances where the hosting machines run out of memory and must shut down because of their applications running on the servers taking over all available resources on the machine. A Linux shell script called setup_cgroups.sh is provided with Endeca Server and must be run to enable cgroups. Once enabled, the Endeca Server relies on cgroups to allocate resources to all the data domains it is hosting.
Endeca Server can use the Oracle Unbreakable Enterprise Kernel (UEK). The advantage of UEK is that zero-downtime updates are possible with Ksplice, a feature that must be licensed for each server. Ksplice enables customers to apply security and other critical fixes without rebooting. The zero-downtime capability should be considered for critical deployments of Endeca.
Application Server
Endeca Server is a Java application that must be deployed on an application server and requires Oracle WebLogic. WebLogic features an advanced, modern web-based user interface that makes installing Endeca Server easy. WebLogic has a library of dashboards for monitoring and also allows the creation of custom dashboards. We'll cover these dashboards in the performance monitoring section.
Exalytics
One other option to consider for hardware for Endeca Server and other Endeca core components is Oracle Exalytics. Oracle Exalytics is an engineered system by Oracle targeted at business intelligence and enterprise performance management applications. Exalytics is designed for in-memory analytics and, as a result, ships with large amounts of RAM.
The Oracle Exalytics server is suitable for large deployments of Endeca Server. Exalytics comes in two configurations:
        Exalytics X3-4 features 40 cores, 2TB of RAM, 5.4TB of raw storage, and 2.4TB of PCIe flash storage
        Exalytics X5-8 features 128 cores, 4TB of RAM, 9.6TB of raw storage, and 6.4TB of PCIe flash storage
While Exalytics is designed for business intelligence and enterprise performance management applications, it is also certified by Oracle for Endeca. For more details on Oracle Exalytics, confer with Oracle technical resources to determine how Exalytics can be configured for your particular Endeca deployment.
Securing Endeca Server
Endeca Server has two basic mechanisms for providing security, discussed in the next two sections. In addition to these two built-in mechanisms, Endeca Server should be secured with standard network topography and access controls since most threats against Endeca Server will be over network communications.
Endeca Server SSL
Endeca Server uses Secure Sockets Layer (SSL) to enable secure communications with other Endeca components. Using SSL is optional and is a decision that must be made at the time Endeca Server is installed on WebLogic. Oracle recommends using SSL for all production installations of Endeca Server. The use of SSL is a decision that affects all further connections with Endeca Server: If SSL is selected, then connectivity between Endeca Studio, Endeca Integrator, and subcomponents must utilize SSL.
EQL Records Filters
Endeca Server EQL has one type of records filter, known as a DataSourceFilter, that limits the scope of records available to a user. This filter limits the scope of requests prior to the execution of a user EQL query and provides a type of filter for fine-grained access. This filter is created by developers using the Endeca Integrator ETL.
Other Security Considerations
The server OS must be hardened to the greatest extent possible. We won't go into the details of how this is done because security recommendations evolve as new threats emerge, so the best, most current guidelines are available from online sources. The Oracle Technology Network and the National Security Agency (NSA) web sites both have excellent guides to securing Linux and Windows operation systems.
Endeca Server Cluster
Endeca Server Cluster is an optional method of deploying Endeca Server and is a deployment of multiple Endeca Server instances on multiple servers (an instance being a single installation of Endeca Server). From a physical standpoint, Endeca Server Cluster must be installed on homogenous servers, meaning each server must be identical in terms of server make and model, CPU core count, and memory. We refer to these servers as hardware nodes. Endeca Server Cluster requires a clustered file system be available to all hardware nodes to host the indexes.
In Endeca Server Cluster, the primary processing entity is Dgraph nodes, which are Dgraph processes distributed across the hardware nodes that service query requests. Dgraph nodes that service query requests are known as follower nodes. With Endeca Server Cluster, only one Dgraph node can service write requests, and this node is known as the leader node. A service known as the Cluster Coordinator is installed with Endeca Server Cluster and, as its name implies, provides coordination and communication between cluster nodes. Figure 2-6 shows an example Endeca Server Cluster deployment.
Reviewing Figure 2-6, you can make the following observations that lead to more understanding of an Endeca Server Cluster:




FIGURE 2-6. Endeca Server Cluster example
        Data domains in Endeca Server Cluster are composed of data domain nodes serviced by the Dgraph nodes running on each hardware node.
        Multiple data domains can be hosted on Endeca Server Cluster. In this example, there are two data domains: market and opinion. Note that data domain opinion is not shown on Endeca Server 1. The reason for this is that the data domain profile for data domain opinion is not configured to run on Endeca Server 1. Since the leader node for data domain market is on Endeca Server 1, this would allow all the resources of this node to be dedicated to servicing the data domain market leader node.
        Each node has a unique hostname and port. Any of the URLs for the cluster can be used to query or write data to either data domain. Endeca Server Cluster routes all write requests to the leader nodes and balances query requests across follower nodes.
The clustered file system shown at the bottom of Figure 2-6 is an essential element of Endeca Server Cluster in that it provides storage for each of the data domain indexes and is accessible to all hardware nodes. Oracle recommends using Network File System (NFS) as the clustered file system for Endeca Server Cluster. NFS is often hosted on a Linux server, and if this option is chosen, a multimaster technology like HighlyAvailableNFS should be used to ensure you eliminate the NFS server as a single point of failure. HighlyAvailableNFS allows a backup server to recover current NFS activity should the primary NFS server fail.
The best option for providing the NFS clustered file system for Endeca Server Cluster is a Network Attached Storage (NAS) server and, in particular, the Oracle ZFS appliance. The ZFS appliance supports Fibre Channel connectivity and has a wide array of management features for backup and replication, all managed with an easy-to-use web interface.
Now that we have covered the major elements of Endeca Server Cluster, let's examine two benefits not available in a nonclustered Endeca Server deployment. They are discussed in the next two sections.
High Availability
Most information technology service groups and organizations are required to operate under service level agreements (SLAs) requiring a minimum amount of uptime per month or year for enterprise systems. To meet the requirements of these uptime requirements on enterprise server products, clustering technologies are often deployed. When clustering technologies are deployed across multiple servers, this lessens the risk of service loss due to hardware failure. With clustering technologies, the loss of a single server because of hardware failure does not cause a loss of service. This capability is known as high availability, and here are a few key considerations regarding Endeca Server Cluster high availability:
        If a leader node fails, the Cluster Coordinator assigns leader node functionality to one of the surviving follower nodes. If a follower node fails, Endeca Server Cluster no longer routes query requests to the failed follower node.
        Cluster Coordinator must be running on three nodes to ensure Endeca Server Cluster is capable of monitoring and taking corrective action in the event of a node failure. If fewer than three nodes are running Cluster Coordinator, then the high availability capability is deprecated, and the cluster has increased availability. Increased availability is a term denoting more fault tolerance than a single-node installation, but not true high availability.
Horizontal Scaling
When an enterprise server's key resources, processor cores, and memory are not capable of supporting its normal operations, a hardware upgrade is necessary. Usually, this involves procuring a new server with more processor cores and more memory. This type of upgrade is known as vertical scaling. Migrating an enterprise server to new hardware is one of the more complex and difficult tasks facing IT organizations. With clustered enterprise servers, like Endeca Server Cluster, adding processor cores does not require vertical scaling. Rather, a new server can be procured and becomes an additional hardware node running Endeca Server Cluster. This capability is known as horizontal scaling and is one of the most compelling reasons to deploy Endeca Server Cluster.
Load Balancers and Endeca Server
Most IT organizations use load balancers in their enterprise infrastructure because of the capabilities and features they provide and because organizations can easily add configurations to support the server. Load balancers are fabric-enabled devices that can support many applications, and a pool and a virtual IP address are the main components of a single configuration in a load balancer. The pool is a group of enterprise resources that are exposed to the load balancer in the form of URLs. In the case of Endeca Server, these URLs are composed of the hostname, port number, and domain name used to establish connectivity to a data domain. The virtual IP (VIP) address resides on the load balancer and is associated with a network domain name that is descriptive of the Endeca Server data domain. For example, for a data domain named market, you could choose the network domain name marketdatadomain.mycompany.com. Figure 2-7 shows an example of a load balancer deployed with Endeca Server Cluster and a network domain name for the data domain market.




FIGURE 2-7. Load balancer configuration for the data domain market
In this example, the ping administrative operation available for a domain name is used for the health check. Figure 2-8 shows a web browser with the heath check output. When the health check for a particular data domain node becomes unresponsive, the load balancer will stop routing requests to the failed data domain node.




FIGURE 2-8. Web browser with the health check output for the data domain market
Note that the URLs from each of the four Endeca Server Cluster nodes are HTTP, not HTTPS, meaning that we have not deployed SSL on this server. However, the VIP and network domain name provided by the load balancer is SSL. The reason for this is that load balancers can provide SSL to non-SSL sources, which eliminates the need to deploy SSL on Endeca Server. On the load balancer, an SSL certificate is deployed for the company URL, mycompany.com in this case, from a trusted certificate authority. In this example, the load balancer greatly simplifies the details of connections to Endeca Server Cluster. There is no need to worry about SSL certificates from Endeca Server; the URL used for connectivity is easy to remember and does not require an unusual port number. (The port number in this case is 443, which is the standard port number used for SSL connections.) This approach requires a pool and VIP be created for each data domain, so to provide load balancer support of the other data domain on this cluster, data domain opinion, you would need to create another VIP and the domain name opiniondatadomain.mycompany.com. The pool for opiniondatadomain.mycompany.com would not include the URL and health check for Endeca Server 1 since there is no data domain node on this server.
One final comment regarding the use of load balancers: If your IT organization already uses load balancers, then you should consider using them with all Endeca components to provide simplified URLs and SSL, even for nonclustered deployments.
Installing Endeca Server
The most up-to-date installation instructions for Endeca Server are available on the Oracle Technology Network web site at http://otn.oracle.com. The installation files for Endeca Server are available for download from the Oracle software delivery cloud at http://edelivery.oracle.com. The documentation for Endeca Server can also be downloaded from the Oracle software delivery cloud for those who prefer a local copy of documentation.
Be aware that in addition to downloading the Endeca Server installation, you will need to download the WebLogic installation bundle. The Java Development Kit (JDK), as well as the Oracle Application Development Runtime (ADR), must be installed on the server. The installation directions for Endeca Server are detailed but easy to follow; Endeca Server can easily be installed in one to two hours for a single-node installation.
Managing Endeca Server
Now that we have wrapped up our discussion on installing Endeca Server, let's move on to subjects related to Endeca Server management. This section describes common activities for those involved in maintaining Endeca Server. Endeca Server, like all enterprise systems, requires a level of maintenance, monitoring, and management in order to keep it running smoothly. Planning the day-to-day management of Endeca Server will ensure that your Endeca deployment is productive for your end users. In this section we will cover the following:
        Data domain management
        Backups
        Replication
        Performance monitoring
        Maintenance
Data Domain Management
Data domain management involves creating data domains and data domain profiles. Data domain management is one of the primary means of controlling the behavior of Endeca Server with respect to system resources. Endeca Server administrators use the command-line utility endeca-cmd to perform these actions. The Oracle Endeca Server: Administrator's Guide has a comprehensive list of all parameters for endeca-cmd.
Parameters for data domains are stored in data domain profiles. Data domain profiles are created prior to the creation of a data domain. Let's take a look at the endeca-cmd example used to create a data domain profile. Shown next is an example of creating a data domain profile named opinion-data:


NOTE
The backslash character (\) is used for multiline commands in Linux and is used here to allow the command to be shown on multiple lines. We will use this character elsewhere in this chapter in the same manner.
In this example, you can see some of the more important parameters with regard to managing system resources:
         num-compute-threads This sets the number of compute threads available to the Dgraph node, typically equal to the number of CPU cores.
        auto-idle This determines whether the threads for a data domain's Dgraph processes become inactive after a time period determined by the idle-timeout parameter.
        oversubscribe This allows a data domain to be created even if the system has insufficient resources. When Endeca Server prepares to activate a data domain, it first determines whether there are sufficient resources for this to occur; however, this parameter overrides this check.
        read-only This parameter disallows changes to the data in the data domain. Setting a data domain to read-only increases performance.
After a data domain profile is created, it can be used to create a data domain. The command to create a data domain accepts as a parameter the name of a data domain profile.
For clustered environments, the endeca-cmd utility allows for the management of node profiles, which determine certain aspects of a cluster behavior. There are two parameters for node profiles:
        num-cpu-cores Sets the number of CPU cores available on each Endeca Server node
        ram-size-mb Determines the amount of virtual memory to allocate to Endeca Server
These parameters, especially the ram-size-mb parameter, are of particular interest to Endeca Server administrators because they can be used to increase memory. On a server with a large amount of RAM memory, ram-size-mb allows Endeca to access this memory.
Endeca Server Backups
Backups are a normal and important task performed by server administrators. In this section, we cover backups specific to Endeca Server. The goal of this backup strategy is to preserve data configuration stored in Endeca Server and facilitate the rebuilding of a failed Endeca Server machine in the unlikely event of a complete hardware failure. The server operating system installation should include backup client software to write backups to a backup server. A commonly used backup client is Oracle Secure Backup.
Data Domain Backup
We discussed data domain management configuration in the previous section; now let's look at another aspect of data domain management: taking backups of data domains. The endeca-cmd utility with the parameter export-dd takes a "snapshot" of a data domain's index files. The export process occurs asynchronously to the data domain operations, so backups can be run at any time without requiring an outage on the data domain. Only the data domain index data is backed up by this command, and the data domain profile is not captured, nor is any other characteristic of the data domain. The snapshot is stored in the offline directory, specified by the endeca-offline-dir parameter in the EndecaServer.properties file. The following is an example of the command to back up the VoterData data domain:

This creates a series of directories containing the backed-up indexes:

 Using this command as part of the backup strategy, the main directory of the export output in this case is as follows:

It can be coalesced with the tar command and can be compressed with gzip, as shown here:

The resulting file is as follows:

Backing up and restoring the files created with the tar and gzip commands is an effective method for taking backups. The fol lowing is an example of a Linux BASH shell script for backing up domain data. It assumes that the files are ultimately stored in the /backups/Endeca_Indexes directory:


This script can be run several times a day to provide point-in-time recovery that would otherwise not be available for data domain index files. This can be useful in test environments.
Data Domain Profiles
We mentioned that data domain profiles are not backed up with data domain exports. Data domain profiles are not stored on the file system; therefore, they will not be backed up with the file system backups. Data domain profiles contain configuration information important to re-creating a data domain and need to be backed up. The BASH shell script offers a solution to this problem. The script captures the data domain profile settings into a plain-text file. While this file does not allow direct restoration of the data domain profile, the information it contains can facilitate the re-creation of a data domain in minutes. Backups are stored in the /backups/Endeca_DD_Profiles directory.

EndecaServer.properties File
The EndecaServer.properties file is a plain-text file that contains many settings related to the configuration and operational aspects of Endeca Server. We will look at the contents of this file in depth in the next section. (The environment variable $MW_HOME is commonly known as the middleware home and in most installations will be /u01/app/Middleware.) For the purposes of backups, it is important to ensure this file is backed up. Most backup clients will back up entire directory structures, and in the case of Endeca Server, backups of the $MW_HOME should be included in the daily backup schedule. Since we have covered shell scripts to back up data domains and data domain profiles, we would be remiss if we didn't present a BASH shell script to back up this EndecaServer.properties file. Backups are stored in the /backups/Endeca_ServerProperties directory.

Node Profile Backups
If you have deployed Endeca Server Cluster, one final configuration you should capture is the node profile. The node profile has only two settings, and the backup script for capturing the data domain properties can be modified to capture them. Backups are stored in the /backups/Endeca_ServerNodeProfile directory.

 This wraps up the discussion of Endeca Server backups; to summarize, the following should be backed up:
        Data domain indexes with the data domain export command
        Data domain profiles to a plain-text file
        The EndecaServer.properties file
        Node profile settings for clustered environments
Use a scheduling program or facility to schedule backups, and periodically review backups to ensure the efficacy of your backup strategy. The backup directories will fill up over time and should be purged after a set number of days of backups. Here is a simple command to accomplish this:

This command purges the directory used for node profile backups of any backup older than 90 days.
Replication
You may need to replicate a data domain for a variety of reasons, most likely to load a test environment from production. From the command-line interface, there are two methods of replicating a data domain:
        endeca-cmd import-dd imports the files created by endeca-cmd export-dd. This can be used to restore a data domain from a backup or copy a data domain to another server.
        endeca-cmd clone-dd makes a copy of a data domain from an existing data domain. This can be used only on the same server where the existing data domain is located.
As you evaluate your needs for replication, bear in mind that both of these utilities can be used with shell scripts.
Integrator ETL is also used to create data domains and populate them with data and is a third method of replication. When running a graph in Integrator ETL to create a data domain, you replay the steps that were used to create a data domain you want to replicate. However, you are not really copying any data domain from the source to the target with this approach, so this method is not truly replication.
Performance Monitoring
Performance monitoring with Endeca can be carried out with many different utilities. The goal of performance monitoring is to detect excess CPU or memory utilization problems that can cause usability issues with an Endeca deployment. Performance issues can have a variety of causes; most will be caused by excess system usage for the amount of CPU cores or memory available. Issues caused by insufficient CPU cores, memory, or poorly performing storage can be difficult to remedy without curtailing system usage. This chapter began with a section on hardware sizing, emphasized the need to understand workload and to select hardware appropriately, and talked about running Endeca Server on Linux, preferably version 6. Assuming you make good decisions on your choice of hardware to run Endeca Server, the majority of performance problems you will encounter will be related to poorly performing EQL queries or poor production planning, such as running multiple data ingest sessions while users are using Endeca Studio. Once you select a performance monitoring methodology, use it every day to monitor normal operations, and become familiar with the tools you have selected and how a system operating normally appears on the tools. When real performance problems occur, you'll be adept with the monitoring tools because you are familiar with them and will quickly recognize where the deviation in performance is occurring.
Data Domain Monitoring
Endeca Server includes a monitoring tool that is available from a web browser that provides detailed information about Dgraph processes. The web pages for this tool are titled "Dgraph Server Statistics." Guidance on using this tool is not provided in the Oracle Endeca Server documentation. Moreover, the documentation states that "the Dgraph Server Statistics page...is intended for use by Oracle Endeca Support only." Despite this admonition, there is a benefit to perusing the contents of this facility. Much of the data on the Dgraph Server Statistics page is understandable and, when used in conjunction with other monitoring tools, can provide insights into the performance of Endeca Server. The facility is available for each data domain and is accessed via a web browser. You can access the tool via the following URL command:

Figure 2-9 shows this facility.




FIGURE 2-9. Dgraph Server Statistics page
The General Information tab shows information about the Endeca Server installation. The Details tab lists the most expensive queries and "hotspots," which provide detailed information regarding the performance of individual Endeca Server components. Figure 2-10 shows the Details tab.




FIGURE 2-10. Dgraph Server Statistics Details page
The URL command

resets the data on the Server Statistics page.
Monitoring with WebLogic
WebLogic provides a monitoring dashboard that is customizable and can be accessed from the Monitoring Dashboard link shown in Figure 2-11 , which appears in the web form immediately visible after logging into the WebLogic console.




FIGURE 2-11. Link for monitoring the dashboard
The dashboard has a number of prebuilt views, but its real power is the capability to create custom dashboards. Figure 2-12 shows the dashboard with two metrics displayed.




FIGURE 2-12. WebLogic monitoring dashboard
 Using the Types selector, you can find many aspects of the server to monitor, including UNIX/Linux metrics and thread metrics. Once you find a metric you want to monitor, drag it onto the chart. Different chart types are available, including radial gauges and bar charts. Many charts can be displayed on one screen. Note that the WebLogic monitoring dashboard displays real-time data and cannot recall historic data.
OS-Level Monitoring
Endeca Server administrators and systems administrators must collaborate regularly on supporting Endeca Server and troubleshooting performance issues. Systems administrators often use command-line utilities to quickly determine performance characteristics of processes on a Linux server. One of the best tools to use for an overall view is Linux top, which lists the top resource-consuming  processes on a server. Figure 2-13 shows top for Dgraph processes, and a quick inspection reveals process ID 7163 is consuming more memory and CPU resources than other Dgraph processes.




FIGURE 2-13. Linux top for Dgraph processes
Another useful utility for monitoring CPU activity is uptime. An example of uptime output is shown here:

The last three columns of uptime are useful for quickly assessing overall server performance and indicate the 1-, 5-, and 15-minute system load average. Load average indicates how long threads of execution must wait before executing. Values greater than 4.0 indicate a performance issue with regard to CPU resources; values greater than 10.0 indicate extreme system loading.
The Linux command vmstat indicates the extent of memory paging. Because of the importance of memory to Endeca Server performance, it is wise to use this command to monitor paging. Figure 2-14 shows vmstat.




FIGURE 2-14. Linux vmstat screen capture
With vmstat, pay particular attention to the so column. Nonzero numbers in this column indicate that swapping is occurring. Once you observe swapping, you can use top and the Linux command ps to determine which process is having issues.
Eneperf
eneperf is a load-testing tool that ships with Endeca Server; it can be used to replay log files from sessions and can also be used for load testing. Some Endeca admins have used it to debug issues with past sessions by replaying request logs. eneperf is a command-line tool, and running the command from the command line without parameters displays a long list of user instructions. For the sake of brevity, these instructions will not be listed here. The most basic use of eneperf is as follows:

Oracle Enterprise Manager 12c
Oracle Enterprise Manager provides the ability to monitor an Endeca installation, and when monitoring Endeca Server, it provides a single interface for monitoring WebLogic, Endeca Server, the Linux operating system, and storage. If the ZFS appliance is used for storage for Endeca Server, Oracle Enterprise Manager 12c and Ops Center can manage the ZFS appliance. Plug-ins are available to monitor the Tomcat application server and Microsoft Windows Server, should you choose to use these in your Endeca deployment. Figure 2-15 shows an example of a pie chart depicting the uptime status of all systems monitored by Enterprise Manager 12c.




FIGURE 2-15. Enterprise Manager 12c monitored targets summary
Figure 2-16 shows an example of the disk monitoring that is available from Enterprise Manager 12c.




FIGURE 2-16. Enterprise Manager disk I/O monitor
 Enterprise Manager does not merely provide dashboards, but collects historic metrics that can be examined for the last 30 days. Custom dashboards and reports can be created, and because of the historic metric retention, prior performance data can be viewed. Earlier in this chapter, an example Linux shell script for backing up a data domain index was shown. Enterprise Manager 12c also features a scheduling facility that could be used to schedule this type of shell script for automating the maintenance for Endeca Server. Figure 2-17 shows the scheduling facility of Enterprise Manager 12c.




FIGURE 2-17. Enterprise Manager 12c scheduling facility
Enterprise Manager 12c must be installed on a separate server dedicated to hosting Enterprise Manager and requires a database to host a repository database. Enterprise Manager 12c can monitor most enterprise platforms, including database servers, application servers, and most operating systems. Enterprise Manager 12c can monitor not only your Endeca deployment, but many other aspects of your enterprise applications and can greatly enhance your ability to monitor and control your enterprise infrastructure.
Maintenance of Endeca Server
Endeca Server is based on Java, and the stability and security of the Java Development Kit (JDK) installed on the server is an important part of server maintenance. Oracle recommends staying current with Java updates on Endeca Server. To install a Java update, Endeca Server must be stopped, so outages need to be planned and coordinated whenever a Java update needs to be installed. You should test any Java update on a test server for at least two weeks before installing it in production to ensure it does not cause issues with your installation of Endeca Server.
 Maintaining Endeca Server also requires that logs be monitored and rotated. Endeca Server uses the following directory for logs and other diagnostic files for Dgraph:

The types of files in this directory are as follows:
        .pid This stores the process ID of each Dgraph process. This file is always one line only.
        .out This is the stdout/stderr log for the Dgraph process, including startup/shutdown and ingest processes. It's useful for debugging problems.
        .reqlog This records incoming requests to the Dgraph process from web services.

rolls the request log files and archives the current request log file. As request logs are rolled, they have the following extension:

The following directory listing shows rolled request logs:

In this example, the process PID is 3666. This URL can be called using the Linux curl command and easily automated with shell scripts for this maintenance task. Figure 2-18 shows the output from this command being executed on the local Endeca Server.




FIGURE 2-18. Linux curl command used for log roll
The Oracle Endeca Server: Administrator's Guide provides details regarding verbosity settings for logging. Increasing verbosity can be useful in understanding the cause of problems.
This command can be used to enable logging for a domain and sets verbose logging for requests:

 The WebLogic server used in conjunction with Endeca Server also has logs that can be useful, located in the following directory:

The WebLogic server rotates the logs in this directory automatically, except AdminServer.log. There are three logs in this directory:
        AdminServer-diagnostic.log This contains messages from WebLogic applications.
        <endeca-server-domain>.log This is used to monitor the status of the domain.
        AdminServer.log This is the only log file in this directory containing information useful in debugging Endeca Server and can be useful for run time and stability issues.
All of these logs can also be viewed within the WebLogic web-based administrative interface.
Understanding Endeca Server
We have covered installing and managing Endeca Server, and now we will wrap up this chapter with an overview of the internals of Endeca Server and how Endeca Server fits into enterprise architecture with other enterprise products.
EndecaServer.properties
The EndecaServer.properties file contains settings for Endeca Server and can be thought of in the context of init or .ini files. Endeca Server administrators use the EndecaServer.properties file to change directory locations for the following:
        Index files for data domain. This location is common for all data domains on a deployment of Endeca Server.
        Log file directory, where the Endeca Server logs are written.
        The "offline directory," where files from endeca-cmd export-dd are stored.
        Files associated with the Cluster Coordinator, for clustered operations.
        Files associated with the data enrichment plug-in.
Endeca Server administrators will often change these directories to ensure that operating system files and the Endeca Server index files are not on the same logical volume. This ensures that I/O associated with operating system activities does not contend with the data domain index file I/O operations.
The EndecaServer.properties file also contains tuning parameters that can be used to modify the performance characteristics of Endeca Server. These settings are as follows:
        endeca-memory-to-index-size-ratio
        endeca-threads-allowed-per-core
 As a general rule, you should modify these settings only after considerable empirical testing in a test environment and only after working closely with Oracle Endeca Support. Oracle Endeca Support can be engaged by submitting service requests on the Oracle Support Portal.
Settings for control groups are also managed in the file EndecaServer.properties. Recall the control groups are available only if Linux version 6 is used for Endeca Server. The shell script provided with Endeca Server, setup_cgroups.sh, must be run in order to enable control groups.
EQL
As we discussed in Chapter 1, EQL is the query language for Endeca. It has some similarities to SQL, as well as a number of differences. Many Oracle users are accustomed to SQLPlus, a tool that allows queries to be executed from a command-line interface, objects in the database to be managed, and the database itself to be managed and controlled. Endeca Server does not provide a command-line interface for running queries; all queries must be processed via the Conversation web service. It is possible to send a query to Endeca Server via a web browser to the Conversation web service, and the Oracle Endeca documentation provides an example of this. For EQL query development, you should use Endeca Integrator ETL or Endeca Studio. EQL is strictly for query execution. As you have seen in this chapter, the management of Endeca Server is accomplished by using endeca-cmd and by modifying settings in the EndecaServer.properties file. Moreover, EQL is not used to create or manage objects and does not support schema declaration. In subsequent chapters, we will explore the specifics of EQL queries.
Resource Management
The last topic we will cover in this chapter is Endeca Server resource management, as well as some of the vehicles available for resource management. Endeca Server allows the creation of a data domain on a running system only if there are sufficient memory resources for the data domain. If there are insufficient memory resources, then data domain creation is denied. In the prior section "Data Domain Management," we discussed these data domain resource allocation parameters:
        auto-idle
        num-compute-threads
        oversubscribe
        read-only
Endeca Server uses these settings to determine whether it has sufficient capacity to host a data domain. In addition to these settings, Endeca Server maintains and uses historic memory allocation data to determine whether a data domain can be created. Endeca Server administrators should monitor data domain usage and manage data domain operations with the data domain resource allocation parameters. This also involves understanding how a data domain will be used. For example, if the data sets in a data domain will remain static, then it should be set to read-only to increase performance and conserve system resources. It makes sense to use oversubscribe when on ly a subset of data domains on an Endeca Server deployment will be used during any given time. The auto-idle setting is analogous to the energy-saving features found on most personal computers. When a resource is not needed, it is turned off to conserve power. When a data domain is not being used, its Dgraph processes should terminate with auto-idle, allowing the resources they would be consuming to be available to other data domains.
 Control groups, available only on Oracle Enterprise Linux 6 with the Unbreakable Enterprise Kernel, introduce Linux kernel-level memory management. A cgroup is an OS-level container for memory, and all memory for Dgraph processes is allocated out of the cgroup. This disallows the Dgraph processes from using all server memory resources for data domains and reserves memory for normal server operations, even when memory utilization for data domains is at capacity. This allows Endeca Server administrators to log in to the server and perform management tasks to review log files, use OS-level monitoring features, and shut down or restart stalled processes.
Planning activities for Endeca Server should also be part of managing resources. During normal business hours, when many users are using Endeca Studio, data set ingests with Integrator ETL Studio should be avoided or coordinated. Backups using the export feature can occur asynchronously with other data domain usage. However, it is usually best to perform these at times when the system is more lightly loaded, such as just prior to the start of business, at lunch, or just after the close of business. For installations where graphs will be frequently executed, Endeca ETL Server can be used to schedule resource-intensive ingests to occur after hours when there are no users on the system.
Summary
We began this chapter with a quote from theoretical physicist and Nobel laureate Richard Feynman, a leading scientist in quantum mechanics, who famously stated, "If you think you understand quantum mechanics, then you don't understand quantum mechanics," indicating that even a titan intellect considers some subject matters beyond complete understanding. In the spirit of this remark, the goal of this chapter was to provide baseline knowledge for the installation, management, and maintenance of Endeca Server and to not stray from this mission into theoretical platitudes. Endeca Server started out as a dorm-room project at Harvard University, and after thousands of hours of research and development by world-class developers and computer scientists, it is, for all practical purposes, similar to quantum mechanics in that it cannot be fully understood by mere mortals.










CHAPTER3
Designing Visualization with Endeca Studio
The more information you give someone, the more hypotheses they will formulate along the way, and the worse off they will be. They see more random noise and mistake it for information.
—Nassim Nicholas Taleb, The Black Swan: The Impact of the Highly Improbable
The quote from Taleb's The Black Swan expresses a point of view regarding our inability to organize information in our minds, remember how it is organized, reach correct conclusions, and make correct decisions based on these conclusions. This is an inherent aspect of the mind: It is best suited for having ideas, not holding and processing information. If you need to be convinced of this, just look at the number of firms whose products are dedicated to helping professionals organize their busy lives. These products are designed to allow easy capture of thoughts and tasks and allow them to be organized and scheduled at a more convenient time. Many of us remember the days when paper-based planners were commonly used. Today these have been replaced and greatly enhanced by software on mobile devices such as tablet computers and smartphones.
Introducing User-Driven Data Exploration
User-driven data exploration is an activity made possible by Endeca Studio, where users find sources of data, use these sources to create an Endeca Studio application, and determine facts from the data. In this regard, Endeca Studio functions like the personal organization tools we just discussed, only for data. With Endeca Studio, using a feature known as self-service data provisioning, any data that can be put into Microsoft Excel or JSON format can be used to create an Endeca Studio application and be explored and analyzed. Users can also access JDBC-based sources or Oracle BI-sourced data. Applications and data persist in Endeca Studio and can be reviewed at any point in the future. Moreover, once a user determines a data source is useful for fact determination, the user can share the data source (and data from other sources) with data developers to use in Endeca Integrator ETL. Many of these sources will be from the user-driven data exploration; others will be from enterprise sources and crawler-based exploration. You will explore Endeca Integrator ETL and other Endeca Integrator Acquisition System components in depth in the next chapter and extrapolate the idea of user-driven data exploration to one of enterprise data exploration. For this chapter, you will focus on user-driven data exploration and the tools within Endeca Studio that make this possible.
In addition, this chapter will cover basic aspects of Endeca Studio, with a brief overview of the server and run time environment. Then you will take a look at the process of creating an Endeca application with an example application created from public data sources, exemplifying user-driven data exploration.
Endeca Studio Systems Architecture
In this section you will briefly explore the systems architecture of Endeca Studio and then learn about installing it. This information is intended to supplement and add clarification to the information contained in Oracle's documentation. If you are interested in learning only about Endeca Studio and user-driven data exploration, you can skip to the next section.
Endeca Minimal Implementation
In this book, Endeca Studio is covered immediately after Endeca Server because these two core components can provide data discovery functionality by themselves, and this would be the most minimal Endeca implementation possible. This minimal Endeca implementation is a good starting point if you are evaluating Endeca because it enables you to quickly explore the product, understand data discovery, and see the best-of-class features of Endeca Studio in action. This minimal implementation also has a place in the life cycle of Endeca application development that we just covered: user-driven data exploration. Endeca Server has a companion component known as Endeca Provisioning Service that allows Endeca Studio users to create applications by uploading JSON or Microsoft Excel-formatted files or by connecting to a JDBC database source. Without Endeca Provisioning Service, Endeca Studio applications cannot be created; so, installing Endeca Provisioning Service is required for any Endeca Studio deployment. Figure 3-1 depicts the minimal deployment of Endeca with Endeca Studio and Endeca Server.




FIGURE 3-1. Minimal deployment for Endeca
Installation Requirements for Endeca Studio
Endeca Studio is a Java-based application that is deployed on either Oracle WebLogic Server or Tomcat, and it runs on a server running either Red Hat or Oracle Linux or Windows 2008. Oracle recommends Red Hat/Oracle Linux 6, although version 5 is also supported. If you choose Oracle Linux 6, be aware that only the Red Hat-compatible kernel is supported. Oracle WebLogic Server is a product that must be licensed, whereas Tomcat is open source. Oracle WebLogic Server is recommended for reasons we have already touched on in this book, namely, ease of installation, ease of use, and support.
Endeca Studio uses a relational database to store configuration and state. When Endeca Studio is installed, a Hypersonic database server is installed and used by default, but this is not recommended for production use because of performance issues. Endeca Studio has been tested against Oracle 11g and MySQL, and both are suitable. If your organization already has either of these two database servers running elsewhere for utility databases, then you can use them for Endeca Studio following your usual monitoring, backup, and tuning processes for the Endeca Studio database.
Endeca Studio also supports clustering for achieving high availability and high performance. (Chapter 2 discusses high availability.) Endeca Studio clustering involves installing multiple instances of Endeca Server across multiple servers, with each server hosting one instance of Endeca Studio. Clustering should be considered for mission-critical, customer-facing deployments of Endeca Studio that must be up at all costs. As with any clustered environment, the server hardware and operating systems and the application server should be identical and consistent across all cluster node computers. The relational database covered in the previous paragraph must be used instead of the default Hypersonic database, and all Endeca Studio databases must use the same database connection information. Endeca Studio clustering also requires the use of an HTTP load balancer.
The document "Oracle Endeca Information Discovery Studio: Studio Installation Guide" is a complete guide to installing Endeca Studio and describes all aspects of Endeca Studio Installation, including information on changing the relational database used by Endeca Studio.
Installation Requirements for Endeca Provisioning Service
Endeca Provisioning Service has the same installation requirements as Endeca Studio, with one exception: Tomcat is not supported. To recap, the requirements are either Red Hat or Oracle Linux or Windows 2008, with Oracle Linux 6 being the recommended version of Linux. Oracle recommends that Endeca Provisioning Service be installed on a separate server from all the other Endeca components.
As mentioned, the Endeca Provisioning Service installation is covered in detail in the document "Oracle Endeca Information Discovery Studio: Studio Installation Guide." Once Endeca Provisioning Service is installed, you must set connection information in Endeca Studio to Endeca Provisioning Service in order to use this service to create applications in Endeca Studio. After this connection information has been provided, self-service data provisioning with Endeca Studio is ready to use. This allows users to create an Endeca Studio application from user-uploaded JSON or Microsoft Excel spreadsheet files. Endeca Provisioning Service also allows applications to be created from JDBC-connected databases or from  Oracle Business Intelligence Enterprise Edition data. However, connectivity settings in Endeca Provisioning Service for each of these sources must be defined. Oracle provides one document specific to the provisioning service: "Provisioning Service Administration Guide." Refer to this document for instructions on defining connections to JDBC-connected databases or Oracle Business Intelligence Enterprise Edition sources.
 Endeca Provisioning Service requires a repository database. When Endeca Provisioning Service is first installed, an Apache Derby database is created by default. For production installations, you should use either Oracle 11g or MySQL for this database.
Installation Summary
Figure 3-2 depicts an ideal installation of Endeca Studio and Endeca Provisioning Service, with clustering, separate servers for Endeca Studio and Endeca Provisioning Service, and back-end databases for both products. This ideal installation is appropriate for production environments with strong requirements for high service levels and good performance.




FIGURE 3-2. Optimum installation for Endeca Studio
Endeca Studio Application Development
Now that you have learned about the infrastructure needed for Endeca Studio, let's look at Endeca Studio application development. Oracle provides documentation on the basic aspects of Endeca Studio usage. The document "Oracle Endeca Information Discovery Studio: Studio User's Guide" provides information on creating and managing Endeca Studio applications, as well as information on using the components within Endeca Studio. All application development activities, as well as Server and Endeca Studio admin features, are carried out within a web-based interface.
In this chapter you will examine components that enable data visualization and analysis, with some examples. In the first example, using data from a public source, you will see how an application is created and how components are used to convert this data into facts or knowledge.
Example: FDIC Failed Banks Data
Let's take a look at an example of user-driven data exploration with some data available from data.gov, a U.S. federal web site launched in late May 2009, whose stated mission is to increase public access to high-value, machine-readable data sets. A major feature of data.gov is financial data, and the Federal Deposit Insurance Corporation (FDIC) has an interesting data set available. The FDIC is often appointed as the receiver of failed banks, and there is a list of failed banks going to back to October 1, 2000, available as a comma-separated value (CSV) file, banklist.csv. Endeca Studio does not currently support CSV format files as of version 3.1. The solution is trivial, though: Open the file in a spreadsheet program such as Microsoft Excel and then use the program's Save As feature to save the file in .xls format for Microsoft Excel.
Data for the FDIC Failed Banks Example
A quick examination of the CSV file in a text editor illustrates how this file is mostly unintelligible:

Data provided in CSV format is always indented to be consumed by a machine source or opened with an application that can, at a minimum, display the data in columnar format. Opening the file in Microsoft Excel, as shown in Figure 3-3, reveals the structure and makes the data readable, but beyond being able to sort the data by one of the columns, like ST (for state), you cannot really infer much about the data.




FIGURE 3-3. FDIC failed banks data in a spreadsheet
Endeca Studio Application Creation for FDIC Failed Banks Data
To follow along, load this file into Endeca Studio and see whether you can get more information from this CSV file data. After you save the data in Microsoft Excel format, you can use it to create an Endeca Studio application. Figure 3-4 shows the first step in creating an Endeca application from this file. You simply tell Endeca which file to use, and it will display the structure of the file.




FIGURE 3-4. FDIC failed banks data, new Endeca application creation
 From this screen, click Next to proceed to a screen that allows attributes to be modified. For the FDIC data, you can change the name of the column for the state name to State for readability. Figure 3-5 shows this web form. Note that the figure shows the Advanced Options features.




FIGURE 3-5. FDIC failed banks data, attribute review
This screen allows you to select which attributes are available in the application and also adjust the data set. You will examine one of these options later in this chapter, but for now, click Done to create the application from this data. Endeca Studio begins the process of creating the application with a default set of components. This occurs as a background process, as indicated in the message shown in Figure 3-6. When the application is ready for use, it will appear on the main page.




FIGURE 3-6. FDIC Failed Banks application, create status
Figure 3-7 shows the new application with a default set of components. The chart that is created by default shows a record count by acquiring institution. In the FDIC list of failed banks, the acquiring institution is the name of the bank that acquired the failed bank. There are two sort options on this chart. If you select Acquiring Institution by Record Count, you immediately see something fascinating: Out of the total data set, "no acquirer" has the largest record count for Acquiring Institution. This means that the majority of failed banks were not acquired or taken over by other banks.




FIGURE 3-7. Newly created Endeca application with FDIC failed banks data
Tag Cloud for FDIC Failed Banks Data
Enhancing this application is easy. Let's start by adding a new page by clicking the plus sign near the top of the upper-left corner of the web form. Once you add this new page, you add two important tools for an Endeca application: the selected refinements component and available refinements component. The selected refinements component shows what refinements are currently in effect for your data viewing and visualization components. The available refinements component allows your data to be refined with any attribute in your data set. This component lists record counts for each attribute. In Figure 3-8, you can see how the available refinements component displays the Acquiring Institution data.




FIGURE 3-8. Available refinements for acquiring institution
Note that in Figure 3-8 we have limited the number of values to be displayed in the available refinements component to 10, instead of the default value of 20. The selected refinements and available refinements are part of what is called guided navigation in Endeca Studio. You can always determine what refinements are in effect and remove them with the selected refinements component. The available refinements component allows you to select any available attribute value to narrow the focus of Endeca Studio navigation.
The tag cloud component is similar to the available refinements component in that it allows data to be examined in terms of record count and allows refinements to be applied. The tag cloud component also allows refinements to be cascaded. For example, within the FDIC failed banks data, you can add a dimension for State that has a cascaded dimension of City, as shown in Figure 3-9. The tag cloud component also supports multiple dimensions, which are selectable with a drop-down selector. For the FDIC data, Acquiring Institution, State, and City are good candidates to use with the tag cloud.




FIGURE 3-9. Cascaded dimension configuration for tag cloud component
Figure 3-10 shows the result of the cascaded dimension configuration for state and city.




FIGURE 3-10. Tag cloud for State, with City cascaded
 The tag cloud component is useful for showing which dimensions or attribute values have more relevance. They are easy to understand for those who are viewing the Endeca application and are a good option for dashboard functionality. In this example, you can see that the state of Georgia had the most FDIC bank failures, and within Georgia, the city of Atlanta had the most bank failures. Figure 3-11 shows the page we quickly created with the tag cloud, along with a table view.




FIGURE 3-11. Finished web form for FDIC failed banks data
The table view updates as you drill into the data with the tag cloud. The results table has been customized to show only the bank name, acquiring institution, year of failure, and FDIC CERT number.
Map Component for FDIC Failed Banks Data
In this example, we used the tag cloud to display geographic information, and while this works well with the cascaded dimension selection, a component better suited to this purpose is the map component. The map component displays a "heat map" layer on top of a map, for related geography. The FDIC failed banks data is associated only with the United States, but for data that is global in nature, a world map can be displayed. For Endeca Studio to use the map component, the data set must have a dimension of type geo. The geo type contains the location information expressed in latitude and longitude, separated by a space. In the data contained in the file for the FDIC Failed Banks application, you have a city and state and would need to convert this information to a latitude and longitude in order to create a geo type for each record. You can accomplish this type of task, converting city and state to latitude and longitude, in Endeca Integrator ETL with relative ease. (We will cover this task in the next chapter.) For now, Figure 3-12 illustrates the elegant way the map component displays the results.




FIGURE 3-12. Endeca Studio map component with FDIC failed banks data
The map is clickable and allows zooming. In addition, the small circle control near the upper-left corner of the map allows an interesting form of selection, called geospatial range selection. This is shown in Figure 3-13.




FIGURE 3-13. Geospatial range selection feature of the map component
Using Enrichments Within Endeca Studio
You can use enrichments within Endeca Studio to enhance the visualization of data, and in the case of the FDIC failed banks data, you will use enrichments to add geographic groups for major regions of the United States. Let's first review a few essential technical details about enrichments. To use enrichments within Endeca Studio, the data set cannot be read-only. Data sets are set to read-only by Endeca Server administrators, usually to enhance performance. In most cases, a data set will be read-write. Enrichments use the Endeca Server enrichment plug-ins, which are installed with Endeca Server automatically. However, you can use enrichments only if the data enrichment plug-ins have been registered. Registering the data enrichment plug-ins is an installation step that occurs after Endeca Server has been installed and is covered in detail in the installation guide "Oracle Endeca Server: Installation Guide." Registering the enrichment plug-ins is part of a nominal Endeca Server installation, so you should not have any issues.
Enrichments are used to add attributes to a data set, and two separate and distinct methodologies are provided to accomplish this: the Extract Terms enrichment and Whitelist Text Tagging enrichment. For the FDIC failed banks data, you will be using the Whitelist Text Tagging enrichment. With the Whitelist Text Tagging enrichment, you add attributes to a data set based on specific rules provided to Endeca, based on how you think data should be organized. In this example, you want to add attributes for geographic regions of the United States. You will tell Endeca Studio to add an attribute called Southeast to any record in the data set that has a value of AL, AR, DC, FL, GA, KY, LA, MD, MS, NC, PR, SC, TN, VA, or WV. These are, of course, the two-letter state abbreviations for each state in the southeastern United States.
 To add the Whitelist Text Tagging enrichment, you must select Application Settings, select Data Sets on the left side of the web form, and then select Enrichments. After this, select Add Enrichment, which will bring up the New Enrichment window. This process is shown in Figure 3-14.




FIGURE 3-14. Initial steps for enabling the Whitelist Text Tagging enrichment
Click Whitelist Text Tagging and then click Next to begin the process of creating the whitelist enrichment. In this example, you will create a new attribute called Southeast, which will be created from data from the State attribute. In this example, you will enter the whitelist terms on the web form, but it is also possible to upload a text file with the whitelist terms. Selecting Enter Terms and then clicking Edit allows the terms to be entered. Figure 3-15 shows the steps involved in creating the Whitelist Text Tagging enrichment. Once you've saved this configuration, click the play button on the far right to build the enrichment. The enrichment runs, and during this time, you may not run any other enrichments or make changes to enrichments.




FIGURE 3-15. Steps for creating the whitelist enrichment
After the enrichment completes, a new attribute named Southeast will be available and is shown in the Available Refinements list. If you create whitelist enrichments for the other regions, you can have a complete list of available refinements for the United States, as shown in Figure 3-16. To make these new attributes useful as a refinement, select Application Settings and then select Data Sets | Overview | Manage Attributes. Select Multi-Or instead of the default value of Multi-And. This enables you to use the Whitelist Text Tagging enrichment to select all records for an attribute and see the results in the other Endeca Studio components. This is shown in Figure 3-16.




FIGURE 3-16. Setting refinement behavior to Multi-OR for selection
The benefit of the whitelist enrichment is shown on the dashboard. With this dashboard, you can see pie charts of each region of the United States.
In this example, we have limited the height of the chart to 150 pixels to allow all five regions to be easily visible on one page. Total Bank Failures is indicated on the Summarization Bar, and a tag cloud indicates the acquiring institutions.
One other interesting aspect related to Figure 3-17 is that this screenshot was captured from an Apple iPad Mini web browser. Endeca Studio applications are well suited for consumption on mobile devices.




FIGURE 3-17. Dashboard created using whitelist enrichments
FDIC Failed Banks Data Summary
In the first example, you saw how easy it is to create an application using a Microsoft Excel file from a publicly available data source. With both the tag cloud and the map component, you can easily determine which areas of the United States have had the most FDIC bank failure takeovers. With the whitelist enrichment, you were able to add attributes to the data set for the region of the country and use the chart component to display pie charts for each region.
 FDIC Failed Banks Data Advanced Analysis
For the first example of FDIC failed banks data, you obtained data from the data.gov repository. The FDIC web site also has data available for download with additional information, so you will use it in the second example. We will not review the process of creating the Endeca Studio application for this example, except to revisit the advanced features on the attribute review page on application creation. The data set available from the FDIC is more comprehensive than the one available from data.gov. It includes dollar amounts associated with the failures and the insurance fund used to pay depositors. The data is for the entire history of the FDIC, going back to 1934. It also indicates the type of failure, indicating whether the intervention was only assistance or a full failure of the bank.
The data was not without its issues. There is a column called Location that has a city and state in one column separated by a comma. To address this issue, you can use a feature on the advanced attribute review page, namely, the split feature; this will tell the provisioning service to create two attributes. The result is that the Location attribute has values for both state and city, as shown in Figure 3-18.




FIGURE 3-18. Splitting a value into multiple values
Once the application is created, both the city and state names appear as values for the attribute Location. Figure 3-19 shows how these attributes appear in the available refinements component.




FIGURE 3-19. Location data with split attributes
This example illustrates how the "split value into multiple values" utility that is part of the advanced attribute review functions. In the case of this data, for city and state, a better approach would be to separate them prior to loading the Microsoft Excel spreadsheet into Endeca Studio.
Advanced Visualization of the FDIC Data
This new set of FDIC data contains information on the loss occurring because of each bank failure and the total deposits at each failed bank. It also contains data from the start of the FDIC in 1934. Endeca Studio provides a data visualization tool that provides a unique visualization of this data, as shown in Figure 3-20.




FIGURE 3-20. Bubble chart with loss versus total deposits with year ordering
 This type of graph is known as a bubble chart. The bubble chart is one of the best ways to visualize many aspects of data simultaneously. The data in Figure 3-20 presents estimated loss versus total deposits for each year. The diameter of each bubble is determined by the amount of the estimated loss. The legend at the right of the chart lists the years, in decreasing order of magnitude. As you can see, 1989 saw the largest total losses because of bank failures. That year was when the most banks failed because of the savings and loan debacle that occurred at that time. The year 2009 is a close second in terms of estimated losses, but the failed banks during this year had about six times the total deposits. The Detail drop-down menu allows you to further refine the chart because it allows the attributes Institution Name and State to be selected for refining the data, as shown in Figure 3-21.




FIGURE 3-21. Bubble chart with loss versus total deposits with institution name refinement
In Figure 3-21, the size of each bubble now represents the size of the failure for an individual institution. The largest single failure was for Indymac Bank FSB in 2008, and this bank also had one of the largest amount of total deposits. As you can see from the legend, the top three individual institution failures occurred during the time period from 2008 to 2010, which is the time you will probably remember as the most severe economic period since the Great Depression. Figure 3-22 shows how this same chart appears when State is used for the Detail drop-down. You can quickly discern from the tooltip on the largest bubble that Texas had the most losses that year.




FIGURE 3-22. Bubble chart with loss versus total deposits with state refinement
If you click the year shown in the legend, 1989, you see the bubble chart shown in Figure 3-23, a detail of the largest bank failures of 1989. You can see from the tooltip that California was second to Texas for bank failures.




FIGURE 3-23. Bubble chart with loss versus total deposits by state for 1989
 The tag cloud shown in Figure 3-24 illustrates another interesting attribute in the FDIC failed banks data, namely, the insurance fund used to resolve the bank failures.




FIGURE 3-24. Tag cloud for FDIC insurance fund
The size of the text in the tag cloud is related to the number of records in this data set. The FDIC was used the most, followed by the Resolution Trust Corporation (RTC). Let's first select the FDIC and then view the bubble chart shown in Figure 3-25. Figure 3-25 shows a slightly different version of the bubble chart you have been exploring, only with the institutions in place of the year. Hence, this bubble chart depicts the largest individual bank failures funded by the FDIC.




FIGURE 3-25. Bubble chart with loss versus total deposits by institution refined by FDIC
Figure 3-26 depicts the same bubble chart, except RTC is selected in the tag cloud instead of FDIC.




FIGURE 3-26. Bubble chart with loss versus total deposits by institution refined by RTC
A comparison of Figures 3-25 and 3-26 yields interesting information. You can see that the FDIC funded two large bank failures and a number of smaller bank failures. The RTC funded a large number of bank failures of similar magnitudes, and from the institution names, you can see that these banks were savings and loan institutions.
 The bubble chart has a remarkable capability for visualizing the FDIC data. What is more surprising is the relative ease with which you can create these bubble charts. Figure 3-27 depicts the configuration of the bubble chart used in Figures 3-25 and 3-26.




FIGURE 3-27. Bubble chart configuration
As you can see in Figure 3-27, configuring the bubble chart is relatively simple.
Example: Flight Delay Information
In the final example, you will use publicly available data for airline on-time statistics and delay causes to demonstrate two more excellent tools for data visualization: the pivot table and the stacked bar chart. This data is available at the U.S. Department of Transportation Bureau of Transportation Statistics web site. This is a topic familiar to most of us who travel on airlines, and the data available groups flight delays into several groups:
        Delays caused by the airlines, such as late-arriving flight crews
        Delays caused by airport security
        Delays caused by weather
        Delays caused by National Aviation System delay
This data includes the number of occurrences of these delays and the actual time. For this example, you will examine the number of occurrences of the delays. The data available from the web site allows a time period to be specified. In this example, you will be examining data for 2013, and the data is grouped by month.
The pivot table is similar to the bubble chart in that it allows a large number of attributes to be viewed with one component. The pivot table is purely tabular and allows data for a time period with a number of categories to be viewed. Figure 3-28 shows how to set up the pivot table.




FIGURE 3-28. Pivot table configuration
As you can see in Figure 3-28, you are examining data for each airline on a per-month basis. The metrics you are examining are the occurrence of the four delays discussed earlier. Notice the button underneath the tooltip labeled Swap Rows and Columns. With this button you can make your columns airlines and make your rows the months and delay data. This capability is the reason this table is called a pivot table. Figure 3-29 shows the pivot table. Note that the months are shown as numbers, such as 1 for January, 2 for February, and so on.




FIGURE 3-29. Pivot table for airline delay data
Figure 3-30 shows the same table "pivoted." Note that in this view you can view more data on one screen. Also note that data is summed for each category of delay at the bottom of the pivot table. You enable this feature using the View Options menu in the upper-right corner of the pivot table. The pivot table enables large amounts of data to be viewed. Every part of the pivot table is selectable, including row and column headings, as well as the data.




FIGURE 3-30. "Pivoted" flight delay table with summaries
The pivot table is useful for displaying large amounts of attribute values when you need to view the actual values, similar to a spreadsheet, and do summations on the data. The pivot table is also useful for viewing the actual values of attributes after refinements have been set on other controls that are more visual, such as the map component or a graph. You can export the pivot table to a CSV file; this feature is available from the Actions menu in the upper-right corner of the component. To view more rows in the pivot table, it is advisable to set the number of rows to a higher number, as shown in Figure 3-31, where we have set the value to 45.




FIGURE 3-31. Table height for pivot table
Now that we have covered the pivot table for the airline data, you will examine one final visualization component, the stacked bar graph. The stacked bar graph can also depict all four types of delays. For this example, we have chosen Percentage Stacked Bars, which displays a single uniform vertical bar for each group dimension value, with a proportional section showing the percentage for each chart series. Figure 3-32 shows the delay data per airline.




FIGURE 3-32. Stacked bar graph for airline delay data
 Examining this chart reveals that airlines have similar proportions for each type of delay. Hawaiian Airlines has a low occurrence of NAS delays, which are defined as "delays and cancellations attributable to the National Aviation System that refer to a broad set of conditions, such as nonextreme weather conditions, airport operations, heavy traffic volume, and air traffic control." Since Hawaii is far removed from the U.S. mainland and is in a time zone three hours later than U.S. Pacific Time, it is far less likely to be affected by such delays. What is most fascinating about this graph is the relative insignificance of weather, which is counterintuitive for most of us.
User-Driven Data Exploration and Elimination of Cognitive Bias
 As decision makers, none of us is a rational actor, meaning we all make wrong decisions or have inaccurate or incomplete information regarding events and facts.
Cognitive bias is a subject explored frequently in books on decision making and refers to our tendency to make illogical decisions, based on inaccurate or incomplete data, that satisfies an emotional need or the desire to be "right." We are all subject to cognitive bias, without being aware that it is occurring.
User-driven data exploration provides a unique opportunity to eliminate cognitive bias in decision making and thought processes. In the beginning of this chapter, we pointed out an inherent aspect of the mind: It is best suited for having ideas, not holding and processing information. This fundamental nature of the mind in part explains our tendency toward cognitive bias; we simply cannot retain enough information to eliminate cognitive bias. With Endeca Studio and its capability to allow average business users to quickly create applications and examine data, cognitive bias can be replaced by informed, fact-based decision making and idea formation.
In the previous example, you looked at airline delay information. Most of us believe that weather-related delays are the most commonly occurring delays because most of us have been stuck at an airport waiting on a storm to pass or had to wait on an aircraft to be deiced. Yet, when you examine the airline delay information, weather-related delays are insignificant in terms of their frequency of occurrence.
For most of us, the financial troubles occurring from 2008 to 2010 would lead us to form an opinion that the worst-managed bank failures occurred during this time period. Yet the data you examined on FDIC bank failures debunks this idea. While 2009 was a bad year in terms of bank failures, the largest single bank failure occurred in 1989, and the scope and breadth of the late-1980s savings-and-loan crisis resulted in more bank failures with large losses. This tendency to focus on the most recent events is often called recency bias.
User-driven data exploration is a valuable tool for building a wallet or portfolio of data that produces useful facts and knowledge. User-driven data exploration, in part, feeds enterprise data exploration to provide data sources that are combined with other data sources in enterprise data exploration. You will explore these tools and methods in the next chapter.
Summary
In this chapter, you learned about how Endeca Studio visualizations facilitate data exploration and discovery through examples using publicly available data. We covered some powerful features of Endeca Studio such as charts, the map component, and text enrichment. Be aware that the examples that used the map component and text enrichment demonstrated some but not all of the features of these components. The map component has other types of visualization capabilities other than heat maps, such as the capability to display numbered points. Enrichment whitelisting was demonstrated, but the most compelling type of text enrichment is text mining, to extract concepts from text. We will cover this capability in Chapter 4.
We covered the system architecture of Endeca Studio and Endeca Provisioning Service and covered an ideal installation appropriate for production environments with strong requirements for high service level and good performance.
This chapter introduced user-driven data exploration and how this is made possible with Endeca Studio and discussed how this feeds enterprise data exploration, which will be covered in the next chapter.










CHAPTER4
Mastering Endeca Information Discovery Integrator
One requirement for innovation is faith that there will be a future. Innovation, the foundation of the future, cannot thrive unless the top management have declared an unshakable commitment to quality and productivity.
—W. Edwards Deming, Out of the Crisis
At the end of World War II, Japan was a nation whose industrial capability had been shattered. During the three decades since the end of World War II, the rebirth of Japan's industrial capability occurred rapidly, and by the 1970s, Japanese-manufactured products were leading the world in quality and reliability. One of the most noteworthy aspects of Japan's emergence as an industrial superpower was the contribution of W. Edwards Deming and the effort he led to introduce statistical process control (SPC) to manufacturing processes to improve quality. SPC involves the measurement and analysis of key metrics in manufacturing processes and using these metrics to drive process changes. SPC was incorporated into quality programs championed by Deming. The quote from Deming's 1993 book Out of the Crisis underscores the need for organizational commitment in order for innovations such as SPC to flourish. SPC is an early example of how incorporating data into decision making can lead to superior results. By its nature, SPC involves "connecting the dots," in other words, looking at the quality of products produced by manufacturing processes and making changes to controllable inputs to the process. Just as the companies in Japan were able to make SPC part of their corporate DNA, data exploration can be part of any organization's strategy, and in this chapter you will explore how Endeca components make this possible.
Enterprise-Driven Data Exploration
In the previous chapter, we covered Endeca Studio, as well as the concept of user-driven data exploration. In this chapter, we will cover Endeca Information Discovery Integrator and how it enables enterprise-driven data exploration. Enterprise-driven data exploration is enabled by the ability to acquire any data from any source, assess its value, and incorporate information of value into the decision-making processes of an organization. Just as Endeca Studio makes possible user-driven data exploration, Endeca Information Discovery Integrator, which we will refer to as Endeca Integrator, makes enterprise-driven data exploration possible. Enterprise-driven data exploration relies on the visualization and analysis capabilities of Endeca Studio, along with the diverse data collection capability of Endeca Integrator.
Endeca Information Discovery Integrator Explained
In this section, you will begin your exploration of Endeca Integrator. Let's start with the product name, or more specifically, Integrator, which is a word whose definition in information technology can vary. Integrator, in the case of Endeca Integrator, refers to the ability to combine or incorporate data into Endeca Server and is really the output or end result of Endeca Integrator activities and usage. As you may recall, you use the term data ingest when data is added to Endeca Server. The inputs to Endeca Integrator are data sources along with the features used to capture this data. Between the outputs and inputs, Endeca Integrator transforms data. Figure 4-1 summarizes this simple paradigm.




Figure 4-1. Basic paradigm for Endeca Integrator ETL
When data is ingested into Endeca Server, it is loaded into a data set. Endeca Studio users can use these data sets to create applications. Hence, there are two ways to create data sets for Endeca applications. With Endeca Studio, when you create an application from an Excel spreadsheet, JSON source, or JDBC connection, a data set is created behind the scenes. With Endeca Integrator, you create data sets at the output step of the paradigm shown in Figure 4-1. The process of creating applications does not change when data sets are created with Endeca Integrator. The only thing that is different is that the data set is selected at the time of application creation from a list of existing data sets, instead of loading data from an Excel spreadsheet, JSON source, or JDBC connection.
The name Endeca Integrator might suggest that Endeca Integrator is a single enterprise software product, but it is actually composed of three major software products, each with its own set of capabilities and with its own place in the paradigm shown in Figure 4-1. We already covered these individual components in some depth in Chapter 1. What follows is a concise list of these software products in the order they will be covered in this chapter. We will cover the two acquisition tools first and then will cover Integrator ETL to provide insight into how these tools work with Endeca Integrator ETL.
        Endeca Integrator Acquisition System This is a command-line tool that is used to acquire data from a wide variety of unstructured sources and make it available to Integrator ETL. These sources are listed in the appendix.
        Endeca Web Acquisition Toolkit This is a new addition to Endeca Integrator as of Endeca 3.1; it provides different capabilities from the Integrator Acquisition System and also has a graphical user interface. The Web Acquisition Toolkit can access individual elements of a web page, and each of these elements can become an attribute for a schema.
        Endeca Integrator ETL Endeca Integrator ETL is the tool that executes the steps shown in the paradigm in Figure 4-1 of capture, processing, and delivery. ETL is an abbreviation for "extract, transform, and load."
We did not include Integrator ETL Server in this list since it allows the processes shown in Figure 4-1 to be run, scheduled, and managed, but does not itself provide any of the capabilities shown in the figure.
Endeca Integrator Acquisition System
In this section we will cover the Endeca Integrator Acquisition System, or Endeca IAS, a tool that has two major functions. Its primary function is to crawl source data stored in a variety of formats, including file systems, delimited files, and web servers. (You can find a complete list of supported file formats in the appendix.) Its other function is to store this data in a data repository known as a record store and make this data available to Integrator ETL through a web service.
The goal of this section is to introduce Endeca IAS and its capabilities and provide you with sufficient information to decide what place Endeca IAS has in your organization's enterprise-based data exploration. Endeca IAS is a tool whose interface is the command line and lacks any graphical user interface. When selecting this tool, you should be certain that those who will use Endeca IAS have sufficient skills to use a command-line interface tool. Database administrators (DBAs) or systems administrators possess these skills, along with developers who typically work in UNIX or Linux. The files used to configure Endeca IAS are XML files and text files located on the server hosting Endeca IAS. These files can be edited on the server with the vi editor or can be edited on a workstation, and then they are transferred to the server.
Endeca IAS Crawls
Crawl is the name given to the data collection activity of Endeca IAS, and it applies to data collected from all sources. There are two basic types of crawls. The first type consists of crawls of delimited files, file systems, and JDBC data sources, which we will refer to as local crawls. The second type consists of crawls of entities that are accessed by HTTP, which are primarily web content but can consist of other types of content stored in files. There are two sets of documentation provided by Oracle on crawls. "Integrator Acquisition System Developers Guide" covers local crawls, and "Integrator Acquisition System Web Crawler Guide" covers web crawls.
Local crawls are managed by the command-line utility ias-cmd, which provides the ability to create, run, and delete crawls, as well as perform nearly every type of management task related to crawls. Local crawls must be configured to address considerations specific to the file type to be crawled. For example, for a crawl of the archive file types JAR and ZIP, options must be selected to expand the archives. Note that Endeca IAS can only crawl file systems that are mounted to the Endeca IAS server, meaning only file systems on storage local to the server or file systems on shared network drives.
Web crawls have a special shell script used to run crawls, called web-crawler.sh. Web crawls have the ability to navigate deeper and deeper into a web site. The depth of web crawls is governed by the numeric parameter -d for the web-crawler.sh script. Web crawls are configured to be polite or nonpolite. Web crawlers are capable of retrieving data much faster than humans accessing a web site with a web browser, so they can have a crippling impact on the performance of a site. To avoid this, a polite web crawl waits 1 second between web requests to a domain. A nonpolite web crawl has no delay.
 Both local crawls and web crawls can be either full or incremental. A full crawl captures all content specified, and an incremental crawl captures only content that has changed since a previous crawl.
The data from a crawl can be output to XML files or to record stores. Output to XML files is useful for debugging and provides some capabilities for interfacing to anything that can consume XML.
Endeca IAS Record Store
Data collected by Endeca IAS is stored in a record store. A record store is a staging database that resides on the same server as Endeca IAS and makes the content acquired through crawls available to Endeca ETL through web services. Record store maintenance and management is covered extensively in "Integrator Acquisition System Developers Guide." The record store management utilities provide a limited set of command-line utilities for monitoring and maintenance and do not provide any direct access to the data stored. Data can be extracted from record stores into the XML format with the recordstore read-baseline command.
Figure 4-2 shows the major components and features of Endeca IAS. Local crawls, regardless of data type acquired, all use the same command-line tool, whereas web crawls have a command-line tool just for web crawls.




FIGURE 4-2. Endeca IAS taxonomy
Example Endeca IAS Crawl
Because of the volume of data on the Internet, we will cover how to collect web data through web crawls instead of demonstrating a local crawl. Local crawls do not differ greatly from web crawls, so the steps to run a web crawl are nearly the same as those for a local crawl.
 In Chapter 3 we covered public data on bank failures from the web site for the Federal Deposit Insurance Corporation (FDIC). For an Endeca IAS example, we will show how to crawl the FDIC press releases for 1994-2013. The configuration for a crawl requires a seed, which is a list of URLs to be crawled. The example seed consists of a list of the URLs for FDIC press releases. The URLs follow a simple pattern, and the seed file is stored in a file called FDICALL.lst.
The file starts with these lines:

The file ends with these last two lines:

Let's examine the commands needed to execute the web crawl. These are all entered on the Linux command line, and we are showing these commands to emphasize the relative ease of performing a web crawl. You must create a record store to hold the output of the web crawl. Call this record store FDICALL.

The file site.xml stores settings for the crawl, and this file must be modified to reflect the name of the record store. No additional modifications to site.xml or any other file are required to run a web crawl. The actual command to run the crawl is as follows:

This simple command runs the web crawl at a depth of 1. As the web crawl runs, status messages are returned. Since you are writing to a record store, you never actually see the data collected. For debugging purposes, you could have sent the data collected to an XML file for review. You will not be able to examine the data in this example until bringing the data into Endeca ETL.
The crawl depth, as set by the -d flag, sets how many levels of page links will be followed. A URL in the seed has a level of 0, and each link from a seed URL has a level of 1. Links from a level 1 URL have a level of 2, and so on. Setting the depth to a higher number like 4 or more increases the scope and duration of a web crawl, as well as the amount of data acquired by the crawl.
When the crawl completes, the status messages show the successful completion of the crawl:

The data from the crawl is now in the record store FDICALL. You cannot view it until you get into Endeca ETL (you will view the data in the "Integrator ETL" section later in this chapter). The level of effort required to conduct this crawl was minimal, and after a short time learning about Endeca IAS, most IT organizations could quickly put it into service.
Endeca IAS Installation and Operations
In this section we will provide insight regarding the installation of Endeca IAS not covered in Oracle's official installation documentation. Oracle provides an installation guide specifically for Endeca IAS called "Oracle Endeca Information Discovery Integrator: Integrator Acquisition System Installation Guide." It covers in depth the installation of Endeca IAS. Red Hat and Oracle Linux are supported, as is Microsoft Windows 2008 Server. Only Intel 64-bit x86 processors are supported. Two application servers are supported for Endeca IAS: Oracle WebLogic and Jetty Web Server. We recommend Oracle Enterprise Linux 6 for the operating system and Jetty for the application server because of the overall better experience of running Endeca IAS on Jetty. However, we will provide insight on both the WebLogic and Jetty installations.
Oracle WebLogic 11g, version 10.3.6, is required for Endeca IAS. Oracle WebLogic is used for many other components of Oracle Endeca, but be aware that the installation of WebLogic for Endeca IAS differs from the installation of WebLogic for other Endeca components. During the installation of WebLogic, you are prompted to select Products and Components, and for all other Endeca components, this consists of Core Application Server, Administration Console, and Configuration Wizard and Framework. Endeca IAS requires one additional component, the Evaluation Database, as shown in Figure 4-3. If you have a test environment where a single WebLogic installation supports both Endeca Studio and Endeca Server, it would not be possible, so use this environment for Endeca IAS because of this additional requirement.




FIGURE 4-3. WebLogic installation options for Endeca IAS
Jetty is an open-source, Java-based HTTP or web server and Java servlet container, and is usually used for machine-to-machine interactions instead of serving HTML pages to humans. Jetty has many high-profile deployments and is currently used with Apache Hadoop for some operations. Jetty requires Java, and the version of Java installed on your server will determine the version of Jetty that can be used. Java 6 is used for many of the other Endeca components covered in this book, and if you stick with this version of Java, then you must use Jetty version 8. The Eclipse Foundation, the same organization associated with the development environment used for Integrator ETL, maintains Jetty, and Jetty is available for download from its web site. Installing Jetty involves unzipping a ZIP file in a directory of your choice. The directory where Jetty is installed is referred to as JETTY_HOME. To start Jetty, navigate to JETTY_HOME, and type the following:

You can check the status of Jetty from a web browser on the server where Jetty is running by accessing the status page on port 8080. This is the default page used for Jetty, as shown in Figure 4-4.




FIGURE 4-4. Jetty application status page
Once you have either WebLogic or Jetty installed, you can install Endeca IAS. Endeca IAS is installed with a single Linux shell script that prompts for the application server, ports, and other details. This single script is unusual because it contains the encoded data used for every part of the Endeca IAS installation.
When the installation script completes, the service used for Endeca IAS can be started. For Jetty, this is started from the command line with the following command:

For Endeca 3.1, <version> in this line is 3.1.0. You can check the status of the service from a web browser on the server where Endeca IAS is running by accessing the status page on the port that you specified during the installation. With the default value for this port of 8401, the URL is http://localhost:8401/ias?wsdl. Figure 4-5 shows this in a browser.




FIGURE 4-5. Endeca IAS status page
This URL must be accessible externally to the Endeca IAS server to allow Integrator ETL access to the web service. Testing this with a web browser or with the Linux wget command is a good health check to perform on the Endeca IAS. If the URL is not accessible, be sure that no operating system firewalls are blocking the port used by the web service, in this case, port 8401.
Operations of the Endeca IAS server are rather simple, with only two services to manage. Any time the server is rebooted or restarted for any reason, the Jetty HTTP server must be restarted, followed by the Endeca IAS service. These can be easily scripted with Linux shell scripts.
Endeca IAS has no scheduling capability. Since the commands to Endeca IAS are all Linux command line-based, these commands can be put into shell scripts and run by any enterprise scheduler. Endeca ETL Server has the capability to schedule shell scripts, and this would provide an integrated scheduling capability. Endeca IAS crawls could be scheduled to run first, followed by Endeca ETL graphs.
Endeca Web Acquisition Toolkit
The Endeca Web Acquisition Toolkit, or Endeca WAT, is a new offering with Endeca 3.1 and, like Endeca IAS, is a tool intended to capture unstructured data from web sources and make it available to Integrator ETL.
Endeca WAT Background
Oracle has partnered with Kapow Software to offer Kapow Katalyst as part of Endeca, branded as the Oracle Endeca Information Discovery Web Acquisition Toolkit. Kapow Katalyst is a widely used software product for acquiring web data. By partnering with Kapow for this offering, Oracle has chosen a venerable and reliable solution.
One compelling feature of Endeca WAT is the graphical user interface, used to design web acquisition applications. This graphical user interface is referred to as Design Studio, and for the purpose of this book, we will refer to it as Endeca WAT Studio.
Perhaps the most compelling feature of Endeca WAT is its ability to extract field data from elements of web pages that can be used to create attributes. With this feature, extracted dates, product names, user ratings, and comments are each distinct attributes of a schema.

NOTE
Chapter 3 covered Endeca Studio. Endeca WAT refers to its primary user interface, Design Studio. Needless to say, studio is a commonly used name for software features that provide a graphical environment for development tasks. Our choice of Endeca WAT Studio is to avoid confusion between Kapow Katalyst Design Studio and Endeca Studio.
Endeca WAT Insights
Endeca WAT Studio is used to create what are referred to as robots. Robots allow the automation of any task that can be performed in a browser, from clicking buttons or hyperlinks to selecting elements on a web form. Endeca Studio is well suited to developing applications that click a set of recurring buttons or hyperlinks on a web form and extracting data after each click. A common use of Endeca WAT is extracting opinion information for retail web sites that allow users to enter ratings and opinion information. Within a WAT robot, there are many nested loops that depend on the tendency of the web forms to be consistent, regardless of how many pages of opinion data are available. This paradigm is successful until the tag information that Endeca WAT relies on changes from one element to the next. This occurs on web sites that contain web content that spans years or decades.
For example, in Chapter 3, you used data from the FDIC web site on bank failures. The web page for this site at www.fdic.gov/bank/individual/failed/banklist.html provides hyperlinks for bank failures starting at 2000. When you attempt to drill into these hyperlinks with Endeca WAT to obtain press release information on each bank closure, you quickly find the robot reporting errors once it reaches older hyperlinks because of the underlying HTML document format changing. It is possible within WAT to have exception handling to manage this, but you quickly find a complex programming and configuration task emerging in these circumstances.
Endeca WAT also has the capability of collecting data from Microsoft Excel spreadsheets and from PDF files. Endeca WAT stores its data in a database, and the database servers supported include all of the major databases, including Oracle RDBMS and MySQL.
Endeca WAT Example
In the example for Endeca WAT, you will revisit the FDIC press release web site you crawled with Endeca IAS. You will focus on the page for 2014 press releases, using the URL http://fdic.gov/news/news/press/2014. Create a new project and specify this URL. For this example, choose the date tag on the page for a loop. Figure 4-6 shows the robot used to collect date and text information for each press release. After each pass through the loop, the date and text are written to an Oracle database. The Oracle database in this example serves as the repository for data collected by the robot and makes the data available to Endeca ETL.




FIGURE 4-6. Robot to collect FDIC press release data
You can use the For Each Tag Path control on the robot to step through the robot for debugging purposes. This feature makes testing and debugging robots easy. Variables can be viewed in a watch window during this single stepping mode, as shown in Figure 4-7.




FIGURE 4-7. Endeca WAT variable debugging window
When robots are debugged and ready for production usage, they can be uploaded to the Endeca WAT management console. The management console can schedule a robot for execution and notify administrators when a robot has a run-time issue. Figure 4-8 shows the management console.




FIGURE 4-8. Endeca WAT management console
Endeca WAT Installation
In this section we will provide insight regarding the installation of Endeca WAT not covered in Oracle's official installation documentation. Oracle provides an installation guide specifically for Endeca IAS called "Oracle Endeca Web Acquisition Toolkit: Installation Guide." It covers the installation of Endeca WAT. Red Hat and Oracle Linux are supported, as is Microsoft Windows 2008 Server. We have used both the Linux and Windows versions of Endeca WAT and found the Windows version to be superior with regard to the clarity of the visual presentation and usability. Windows 7 and Windows 8 can be used for development and nonproduction environments. We have used Windows 8 successfully. Installing Endeca WAT is straightforward, and in minutes you will be using it.
Endeca IAS and Endeca WAT Comparison
Newcomers to Endeca may view Endeca IAS and Endeca WAT as two tools that deliver the same capability and may think that the primary difference between the two is the user interface. After the previous two major sections, however, you know that this is not correct and that the two tools provide different capabilities.
A good analogy to use in comparing Endeca IAS and Endeca WAT is that of a fishing net versus a fishing pole. A fishing net can easily cover a wide area and catches everything under the net as it is cast. A fishing pole captures fish one at a time, with a particular type of bait.
Endeca IAS crawls capture everything in a web crawl and have the capability of penetrating deeply into a web site, so it is like a wide fishing net. Endeca WAT robots capture specific pieces of information from web sources, and if the structure of the web source changes, the robot may need to be reconfigured. Endeca WAT is much like a fishing pole, with a specific bait or lure. To catch a different fish, you would need to switch to different bait and possibly use stronger fishing line. To summarize this discussion:
        Endeca WAT is useful when you know exactly what is to be captured from a web source.
        Endeca IAS is useful when you are not certain what is to be captured from a web source and you prefer just to capture all content and look for meaningful information. Endeca IAS crawls other sources besides web-based sources.
Integrator ETL
Integrator ETL is the central hub of Endeca Integrator. The other components listed previously provide data to Integrator ETL. Only Integrator ETL is capable of executing all the steps in Figure 4-1. Integrator ETL is an ETL tool, and the "load" part of Integrator ETL facilitates data ingest into Endeca Server. Integrator ETL is not limited to loading data from Endeca IAS and Endeca WAT. Integrator ETL can read data from many data sources, including flat files, Excel spreadsheets, XML files, and nearly every type of database.
Integrator ETL Background
Integrator ETL is an Oracle distribution of CloverETL, a commercial software product that is one of the most widely used ETL software products. The version of CloverETL distributed by Oracle has special features for Endeca, and this is what makes it Endeca ETL. These special features are in a palette of commands on the Discovery tab under the New drop-down menu, with two types of wizards for Endeca. These are shown in Figure 4-9.




FIGURE 4-9. Endeca features added to CloverETL for Endeca ETL
Endeca Server operations are carried out through the web services using Bulk Add/Replace Records, available on the Discovery tab. In addition, output writers are available for nearly all of the major databases, including Oracle, MySQL, DB2, PostgreSQL, and DB2, as well as for Hadoop, via native HDFS connectivity or a JDBC hive.
This versatility enables Endeca ETL to serve as a general-purpose ETL tool for your organization to address any integration needs since it can read nearly any data source and output to a wide array of data servers.
 Oracle provides usage documentation for Integrator ETL, including the "Integrator ETL Designer Guide," which is documentation by CloverETL on how to use the tool, and the "Integrator ETL User's Guide," which is documentation by Oracle that gives instructions and examples specific to Endeca.
Text Enrichment
Earlier in this chapter we covered two unstructured data acquisition tools that are part of Endeca: the Endeca IAS and the Endeca WAT. Both tools acquire unstructured data and make it available to Endeca ETL. Unstructured data usually consists of voluminous amounts of text, and under the circumstance where little is known about the content of the unstructured data, Text Enrichment can provide insight into the content of it. A product known as the Salience Engine from Lexalytics adds the Text Enrichment capability to Endeca ETL. The Salience Engine extracts entities from text data, including people, places, and organizations, and it also extracts quotations and themes. Sentiment Analysis is another feature available from the Salience Engine, and when bundled with Text Enrichment, it is known as Text Enrichment with Sentiment Analysis. Text Enrichment with Sentiment Analysis provides an overall sentiment score for the current document, for specific entities, or for specific themes.
Text Enrichment and Text Enrichment with Sentiment Analysis features can be accessed only from within Integrator ETL. The Salience Engine is installed separately from Integrator ETL and is also licensed separately. We will cover aspects of the installation in the "Integrator ETL Text Enrichment Installation" section of this chapter.
Be aware that Text Enrichment will only process sentences ending with appropriate punctuation, which includes periods, question marks, and exclamation marks. If sentences are not properly terminated with punctuation, then themes may not be extracted. Text Enrichment also supports foreign languages and can process Twitter feeds. Note that Twitter feed processing is supported only for English.
Integrator ETL Basic Usage
Chapter 6 of this book provides a thorough example and explanation of using Endeca ETL. In this chapter we will cover some aspects of Endeca ETL not covered in Chapter 6. As was stated earlier in this chapter, Integrator ETL is based on CloverETL, a popular ETL tool, and has features to support its use with Endeca. In this section, we will cover some basic concepts of Integrator ETL and explain these concepts with some examples. This section does not endeavor to explain ETL tools in general and assumes you are familiar with the basic features of ETL tools. If you have used ETL tools before, many of the components in Endeca ETL will be familiar to you in terms of their function. Because Endeca ETL is a mainstream ETL tool adapted to Endeca usage, we will focus on features specific to Endeca.


Clarifying Enrichment Features of Endeca Studio and Integrator ETL
You will recall that enrichment in Endeca Studio was covered in Chapter 3 with an example using Whitelist Text Tagging. The enrichment feature in Endeca Studio consists of Whitelist Text Tagging and another feature, Extract Terms. In the Integrator ETL documentation called "Integrator ETL User's Guide," there is a section titled "Choosing Text Enrichment or Text Tagging." This can be confusing, so let's try to get this straightened out. Endeca Studio's enrichment features that provide whitelisting and term extraction have a counterpart in Endeca ETL known as Text Tagging. Endeca Studio Enrichment and Endeca ETL Text Enrichment are not the same. Endeca Studio Enrichment does not have the Text Enrichment features provided by the Lexalytics Salience Engine.


Integrator ETL provides a design environment where extract, transform, and load (ETL) processes are developed for Endeca. These processes are known as graphs and consist of components connected by lines that are referred to as edges. Components are added to a graph by dragging them from a palette and making connections with a mouse from one component to another to create edges. Reader components collect data from a variety of sources and can be thought of as providing input to the graphs. Writer components send transformed data to data stores and can be thought of as producing output from the graphs. Between reader components and writer components are transformation components that modify data, combining data from multiple sources, and manage control of program flow in a graph. Let's take a look at a simple example of a graph, as shown in Figure 4-10.




FIGURE 4-10. Endeca ETL graph
In this example, there are two reader components for Microsoft Excel spreadsheets whose data is joined and then written to Endeca Server. This graph is used to combine the FDIC bank failure data with longitude and latitude data to allow the map component to be used with this data in Endeca Studio. This was used in Chapter 3. The transformation component of interest in this example is the ExtHashJoin component, which joins the data from the two spreadsheets. The lines between the components, known as edges, have metadata associated with them.
Edges determine what data moves from one component to another and always have metadata associated with them. You can view the metadata associated with an edge simply by double-clicking it. Figure 4-11 shows how this metadata appears in Endeca ETL; this is the metadata associated with the input to the BankFailureData writer component.




FIGURE 4-11. Endeca ETL metadata
In this metadata, item number 8 is the geo data used to drive the map component. This data consists of latitude and longitude data joined together and separated by a space. This is performed in the ExtHashJoin component, as shown in Figure 4-12.




FIGURE 4-12. Creating geo data by joining latitude and longitude
For Endeca Server to recognize the geo field as GeoSpatial data that can be used with the Endeca Studio map component, you must create a special attribute in the metadata. To do this, you add a special attribute or mdexType and assign it a value of mdex:geocode, as shown in Figure 4-13.




FIGURE 4-13. mdex:geocode attribute required for the Endeca Studio Map component
Integrator ETL and Text Enrichment
Earlier in this chapter, we showed an example of a crawl of FDIC press release information using Endeca IAS. This press release information is unstructured data containing unknown information and can be enhanced by using Text Enrichment. First let's look at how you connect to Endeca IAS from Endeca ETL. Endeca ETL has a component specifically for collecting data from the web service exposed by Endeca IAS, the RECORD_STORE_READER. Figure 4-14 shows the parameters for this component. This component requires the DNS name or IP address of the IAS server, the port used for the web service, and the name of the record store. The client ID is an arbitrary name used to identify the client. The IAS read type specifies that all records should be read, with the Full Extract option, or only records that have changed since the last read by the same client ID, with the incremental option.




FIGURE 4-14. RECORD_STORE_READER Endeca ETL component
 The edge on the output of the RECORD_STORE_READER must have metadata that matches the data stored in the record store. Endeca ETL has a wizard to make this possible. Figure 4-15 shows this wizard.




FIGURE 4-15. Loading metadata from Endeca IAS record store
To make sense of the text collected from RECORD_STORE_READER, you can use the Text Enrichment component. The Text Enrichment component requires that a file called TextEnrichment.properties be present on the Integrator ETL file system. This file determines how Text Enrichment will function. An example of this file is shown here:

 Also, the Salience Engine by Lexalytics must be installed on the same server as Integrator ETL. The installation is covered in the "Integrator ETL Text Enrichment Installation" section later in this chapter. Be careful to use forward slashes in all path names on Windows systems when referring to the installation location of the Salience Engine. The TextEnrichment.properties file is used to specify which features of Text Enrichment are used with the Text Enrichment component. In this example, you will extract entities, themes, and a summary. Figure 4-16 shows the Text Enrichment component. Note that the input field defines which field will be enriched by the component. The location of the text enrichment configuration file must be specified on the component in the configuration parameter field. Note the forward slashes used on Windows systems instead of the usual backslashes used in directory names. In this example, the Endeca_Document_Text field will be enriched. This is one of the fields in the metadata for the RECORD_STORE_READER component. The Salience data path refers to the installation location of the Salience Engine.




FIGURE 4-16. Endeca ETL Text Enrichment component
One final item to configure for the Text Enrichment component is the metadata for the output of the component. This metadata must include all fields added by the Text Enrichment component. These field names are defined in the TextEnrichment.properties file. This is shown in Figure 4-17.




FIGURE 4-17. Endeca ETL Text Enrichment metadata
 With this configuration done, you can now run the graph in debug mode and examine some of the output. Figure 4-18 shows some of the entities discovered by Text Enrichment.




FIGURE 4-18. Sample of entities extracted by Text Enrichment
From Figure 4-18 you can see Text Enrichment generally extracts people correctly, but some additional filtering of this field may be needed. Using Text Enrichment and developing applications that use Text Enrichment are iterative processes, where extraction processes are optimized, or "dialed in," and the quality of the source data is evaluated.
Integrator ETL Installation
Oracle provides an installation guide specifically for Integrator ETL called "Oracle Endeca Information Discovery Integrator: Integrator ETL Installation Guide" that covers in depth the installation of Integrator ETL. Red Hat and Oracle Linux are supported, as is Microsoft Windows 2008 Server. Only Intel 64-bit x86 processors are supported. We have used both the Linux and Windows versions of Integrator ETL and found the Windows version to be a better experience. The installation guide states that Windows 7 can be used for development and nonproduction environments. We have used Windows 7 and Windows 8 successfully.
Since Integrator ETL is a Java-based application, installation differs from the process you would normally use with Windows installer programs. The installation file is delivered as a ZIP file, which should be unzipped to a convenient location. A Windows batch script file is used to facilitate the installation. We recommend running the batch file from the Windows command-line interface instead of from Windows Explorer. This involves opening a Windows command prompt and changing to the directory where the batch file is located using the window change directory command cd. Run the batch file to begin the installation. The installation guide describes two required files, the Eclipse IDE for Java Developers version 3.7 and the Remote System Explorer. These two ZIP files should be located on the Windows file system where you are installing Integrator ETL. Type install.bat to run the installation; an example of this is shown here:

In Integrator ETL, type the name of the executable, as shown here:

To allow easy access to Integrator ETL, create a Windows shortcut and copy it to the Windows taskbar with your other favorite development tools, as shown in Figure 4-19.




FIGURE 4-19. Windows shortcut to Integrator ETL for ease of access
Integrator ETL Text Enrichment Installation
If you plan on using Text Enrichment, be aware that a separate installation is required for the Salience Engine. You can download this from the Oracle Software Delivery Cloud; it is available in two versions, Text Enrichment and Text Enrichment with Sentiment Analysis. The Salience Engine must be installed on the same server hosting Endeca ETL. These are large downloads, 1.7GB as of this writing. Oracle provides a document with this download describing the installation of Text Enrichment titled "Oracle Endeca Text Enrichment Installation Guide." For Windows installations, the Text Enrichment installer downloads and installs required components from the Internet.
If you plan on using Text Enrichment with Foreign Languages, then you must download and install the Text Enrichment foreign languages packages from Oracle Software Delivery Cloud.
Integrator ETL Server
Integrator ETL Server is a scheduling tool for Endeca ETL graphs. It also has the capability of running shell scripts and batch files and, as a result, can be used to run Endeca IAS crawls remotely before the graphs that consume the data are run.
Integrator ETL Server Background
Recall that Endeca ETL is based on CloverETL, and CloverETL, like most ETL tools, has a scheduling platform to run processes on a scheduled basis. This scheduling tool is CloverETL Server. Endeca ETL Server is ostensibly CloverETL Server, but with no special modification.
Integrator ETL Server Usage
Users of Endeca ETL can connect to Endeca ETL Server using a server project and can develop projects on the server just as though they were developing on their local file system. This feature makes Integrator ETL Server easy to use. On Integrator ETL Server, a graph is contained in a sandbox, which is the term used for a project container on Integrator ETL Server. A sandbox on the server is a directory on the server file system.
Endeca ETL Server has all of the features you would expect on a production server, including e-mail notifications, local security or security integration with LDAP, multistep job control, and a rich API that allows interaction with other systems. It also has a built-in WebDAV server that can be used to host files supplied by external systems to be used by graphs. There is a file listener utility that provides capabilities for monitoring files and file systems and then running graphs when a new file arrives or an existing file changes.
Endeca ETL Server is a full-featured scheduling tool and should be considered for any large-scale Endeca installation that uses Endeca ETL.
Integrator ETL Server Installation
The documentation provided for Integrator ETL Server is the CloverETL Server documentation, but with Oracle boilerplate. Oracle points out that although the CloverETL Server documentation has a long list of supported application servers, Oracle supports only Apache Tomcat and WebLogic. Either Windows or Linux can be used as the operating system for Integrator ETL Server. The documentation for Integrator ETL Server does not make specific recommendations regarding versions of operating systems supported. Clover ETL Server uses its own Apache Derby database for a repository database.
Enterprise-Driven Data Exploration and Integrative Thinking
We opened this chapter with a discussion about the involvement of data-driven decision making and the role it played in making post-war Japan a global manufacturing power. This data-driven decision making, which was termed statistical process control, was an early example of enterprise-driven data exploration, using all possible data to understand and control manufacturing processes and determine the importance of data with statistical methods. While there have been many spectacular success stories of enterprise-driven data exploration like this, there have also been high-profile failures, most notably the intelligence and data sharing failures prior to the terrorist attacks of September 11, 2001. Before then, during the summer of 2001, fragments of information possessed by the CIA, FBI, and NSA pointed to the possibility of a terrorist attack on American soil. Because this information was not shared swiftly and efficiently within and across these organizations, there was little chance of detecting what was to occur. CIA director George Tenet later reflected that "the system was blinking red during the summer of 2001." The U.S. Congress investigated these intelligence failures prior to the terrorist attacks, and Senator Richard Shelby made one of the more articulate statements on these failures in a report, "Our joint inquiry has highlighted fundamental problems with information sharing within the intelligence community. Depriving analysts of the information access they need in order to draw inferences and develop the conclusions necessary for informed decision making. The intelligence community's abject failure to connect the dots before September 11, 2001, illustrates the need to wholly rethink the community's approach to these issues." This is not to suggest that these attacks could have been prevented if the intelligence community had shared information, but simply to highlight the conclusions drawn in the wake of the attacks.
Enterprise-driven data exploration can satisfy the desire among today's business leaders to "connect the dots," in other words, to avoid catastrophic failures and experience levels of success like those of post-war Japan. This approach to decision making is part of a mind-set change involving how we think through problems and situations that is emerging in business schools and has been termed integrative thinking at the University of Toronto Rotman School of Business. Integrative thinkers search for nonobvious factors and causes for the problems they see, and they do not assume linear cause-effect relationships. These mind-set changes drive the need for data and are innovations themselves; they are indicative of "faith that there will be a future."
Summary
In this chapter, we covered four products that are part of Endeca Integrator: Endeca IAS, Endeca WAT, Endeca ETL, and Endeca ETL Server. In addition, the Salience Engine from Lexalytics was included in the section on Endeca ETL to provide Text Enrichment functionality. All of these products contribute in some way to getting data into Endeca Server so that Endeca Studio users can analyze it. Oracle has provided a broad set of capabilities to acquire data from nearly any source with these products.
It is important to realize that each these products is in widespread use and has a rich set of features, and you are encouraged to download and test the products to become more familiar with them. The intent of this chapter was to cover each of these products and highlight the role they play in delivering enterprise-driven data exploration.








PARTII
Business Analytics Use Cases and Tutorial









CHAPTER5
Analytical Applications by Industry
Organizations that do not understand the overwhelming importance of managing data and information as tangible assets in the new economy will not survive.
—Tom Peters, bestselling author on business management
Establishing data management capabilities has become imperative for most businesses. The effective use of big data, with the rise of social media, mobile devices, and sensors, is now recognized as a key differentiator for companies to gain a competitive advantage and outperform their peers. In most industries, established leaders and new players will leverage data-driven strategies to innovate, compete, and capture value. We have observed interesting examples of such use of big data in every industry we have studied. In this chapter, we'll cover a wide variety of use cases for analytical applications in different industries and sectors. We will discuss how social media is being used, what other new data sources are being explored, and, more importantly, how the use of such data has affected and changed business outcomes.
We'll organize these use cases into three main categories. The first category is composed of general business use cases that are applicable to all industries, such as sentiment analysis, brand reputation management, human resource analytics, financial analytics, and customer churn prevention. The second main category will cover a multitude of industry use cases of analytical applications, including financial services, retail, telecommunications, healthcare and life science, manufacturing, public sector-spanning education, and federal, state, and local government agencies. The last category that we will cover in this chapter is focused on information technology-centric analytical applications. It includes use cases such as log processing, extract-transform-load (ETL) offloading, data warehouse augmentation, and information technology (IT) security analytics.
Generic Use Cases
Companies in different sectors may not share core business strategies. They are also likely to focus on different aspects of their businesses to improve efficiency and drive revenue. However, there is a set of use cases that is applicable to the majority of companies. This includes use cases in the areas of sentiment analysis, brand reputation management, human resources and retention analytics, and financial analytics. For companies and government agencies that are public facing and consumer or citizen focused, customer churn prevention is also a common analysis that plays a crucial role in expanding market share, increasing revenue, and improving profit margin.
Brand Reputation
During the last shopping weekend before Christmas 2013, web sales jumped 37 percent from the prior year according to the Wall Street Journal. E-commerce revenues of holiday season 2013 from November to December grew by 10 percent, reported CNBC, buoyed by an 18.2 percent increase on Cyber Monday, amid slow mall traffic and weak sales at brick-and-mortar retailers. Thousands of Americans awoke to find that special something missing from beneath the Christmas tree on Christmas morning, a day after UPS acknowledged getting swamped by the seasonal cheer and failing to deliver orders in time. Many disappointed consumers took their complaints to social media and adopted the hashtags #UPSFail and #FedExFail on Twitter to lodge complaints or just to find a sympathetic ear. Figure 5-1 shows a sample trending graph from a sentiment analysis company called hashtags.org.




FIGURE 5-1. Trending graph over 24 hours for #UPSFail
Figure 5-1 is a perfect example of our first use case: sentiment analysis and brand reputation monitoring. The popularity of social media such as Facebook and Twitter gives rise to sentiment analysis, also known as opinion mining. In the wake of the unexpected online volume, companies such as UPS and FedEx are closely monitoring the social media space to reach out to disgruntled customers, especially those with high social influence. Some sample tweets in Figure 5-2 illustrate how this relationship evolves and negative sentiments turn into mutual understanding with real-time communication from the company service representative to the customers.




FIGURE 5-2. Sample tweets on #UPSFail
The application to monitor social media space is straightforward to build. Twitter provides a set of streaming APIs with low-latency access to Twitter's global stream of tweet data. Companies can use these APIs to create applications and dashboards to enable public relations and marketing analysts to monitor and manage brand reputation. Many companies also combine this information with their point-of-sale (POS) and marketing campaign data to correlate sentiment with campaign results and sales outcomes. Many social media analytics firms have emerged in recent years. These firms specialize in benchmarking systems that score a brand performance on major social media properties based on different metrics, including tags, comments, and views. These analytics provide a more complete picture of a brand's social influence across various social channels than simply the number of likes or followers amassed. Some of the companies we looked into are leveraging these package solutions to manage, monitor, and outreach to their customers in the social space for improved brand loyalty and influence.
Human Resource
Human resource (HR) management does not involve a large amount of data. The typical HR events include hiring, promotion, and termination activities. They usually reside in financial, HR, and data warehouse systems that are curated and maintained by IT organizations. The challenge with HR analytics lies in the difficulties of consuming unstructured data, including employee survey data, and blending it with historic HR event data. The ability to analyze and integrate raw text comments from employees can provide unprecedented insights into the overall job performance of managers as well as team dynamics.
 It's relatively easy to find out "what has happened" or "what is happening" with employee turnover rate. However, it's much harder to answer the "why" question. For example, why are there high levels of employee self-termination with especially high-performing individuals? What is the root cause for that? Is it because of better opportunity elsewhere, dissatisfaction with managers, or a misaligned level of compensation? A lot of these answers could come from survey data. Companies are able to link survey answers to employee data to explore relationships and hidden causes. An HR manager can search across all these data sources for certain key words such as turnover and start to find correlations between high occurrences of that key word with job functions, departments or lines of businesses, years on the job, years on the current role, date and nature of last promotion, and so on. Text analytics engines are able to generate sentiment scores of the responses based on the occurrence of words and phrases such as dislike, cut in pay, and not satisfied. These sentiment scores from each of the responses can then be combined with other dimensions and facts from the traditional HR data sources to focus on a set of supervisors and other factors associated with high turnover. Such analytical tools can help companies to quickly identify at-risk employees and take appropriate measures to mitigate the risk of losing valuable contributors.
Figure 5-3 shows an HR analytics application. Users can enter a keyword such as turnover in the search box on the left or explore data through the Guided Navigation panel. The Survey Summary panel on the top right displays summarized metrics, including number of responses, number of positive versus negative responses, and number of supervisors. There are two tag clouds below the Survey Summary panel. The Themes Known section displays what the employees selected from a predefined drop-down list in the survey as the primary theme for their free-form comment, and Themes Discovered contains themes that have been automatically derived from the comments using text enrichment capabilities.




FIGURE 5-3. HR analytics with survey responses
We have started to notice similar techniques being deployed in expense reporting and compliance auditing through correlating expense items with e-mails based on dates and expense line items.
Customer Churn Prevention
The cost to recruit a new customer is believed to be five times the cost of retaining an existing one. It costs even more to re-acquire defected customers. In fact, a number of independent studies have proven that churn remains one of the biggest destructors of enterprise value for banks and other consumer-intensive companies. Churn has an equal or greater impact on customer lifetime value (LTV) when compared to one of the most used key performance indicators (KPIs) such as average revenue per user (ARPU). This applies to many industries other than consumer banking, including telecommunication, retail loyalty programs, consumer package goods (CPG), and insurance companies.
It's not surprising that consumer-facing companies have been looking for effective ways to accurately determine the churn analytic guidance of their existing customers. The concept of a 360-degree view of customer is nothing new. Currently, most companies have gone through some level of customer master data management (MDM) effort to obtain a consolidated view of customers across various lines of businesses. Customer master data is acknowledged as critical, but to gain further insight into their customers, other sources of information are now used. New data sources have emerged that allow companies to get a holistic and true 360-degree view of the customer that incorporates activities through all channels and touch points, including call centers, web site browsing history, local office and branch visits, ATM transactions, branch or online surveys, and social media interactions including Twitter and Facebook. A deeper customer profile analysis becomes possible that can provide real-time and actionable insights when and where it's needed. The customer service representatives can determine the course of action that is most suited to how they are interacting with the customer, whether through phone or online chat.
For example, a telecommunication call center service representative answers a complaint call from an existing customer. The representative can now quickly glance at the customer profile, including churn probability score, projected LTV score, recent activities, and the customer's social and financial impact. With that holistic information at hand, the customer representative can ease the frustration by avoiding redundant questions and can demonstrate their understanding of the urgency of the situation. For instance, the customer representative could view the stats related to repeated dropped call complaints from the same customer in the past or that this customer is near the end of the current contract. Based on this information, the representative could offer the customer incentives (pre-approved based on business processes) such as a free two-month extension of the contract, a newer-generation cell phone, or a new feature with no cost, all of which are designed to keep the customer from switching to a different carrier. The outcome of these offers can then be loaded back into the system to understand and further improve the effectiveness of each of the offers in customer churn prevention.
Figure 5-4 is a sample dashboard that shows the customers by segments, sentiment scores based on their recent social media postings, and their churn probability scores.




FIGURE 5-4. Customer churn prevention sample application
Industry Use Cases
McKinsey Global Institute published a report on big data in five industries, including healthcare, public sector, retail, manufacturing, and personal-location data. It found that big data generated tremendous value in each of these domains. Many believe that the innovative use of big data is crucial for leading companies to outperform their peers. In this section, we'll describe key big data analytical use cases in a variety of industries.
Retail and CPG
Retail is an important sector of our economy. It is also an industry that faces a high level of competition and is known for its low profit margin. It is estimated that a retailer using big data to the fullest extent could increase its operating margin by more than 60 percent. So, how are companies in retail businesses capturing the value of big data and exceeding competition?
Market Basket Analysis and Customer Profiling
Let's take a quick look back at the 2002 science-fiction thriller Minority Report. For those who haven't seen the movie, there are some interesting depictions of the futuristic shopping experience of the main character. Using a retinal scanner, the billboards on the walls of a subway station call out the names of people passing by. An advertisement for American Express shows the passerby's name on an American Express card, with the Member Since field dynamically updated to reflect that person's membership. A Guinness billboard speaks to Tom Cruise's character as he walks by, saying, "Hey, John, you look like you could use a Guinness!" The most interesting example, however, is when Cruise's character walks into a Gap store. A virtual shop assistant welcomes him back and asks if he enjoyed the shirts he had bought previously.
It might sound scary, but it's more realistic than most people think. Advertisers are developing digital posters that will recognize people's faces and respond if they are paying attention. Cameras attached to the billboards will scan to see whether passersby are looking and immediately change their display to keep them watching.
Some retailers in Europe have embedded cameras in the eyes of the mannequins that feed data into facial-recognition software like that used by police. It logs the age, gender, and race of passersby. Facial recognition technology has been around for some years, but only recently has it been used in retail because it is now affordable. Most retailers have a number of security cameras throughout their stores, but these new cameras that utilize face-recognition technology and incorporate it into mannequins could provide more accurate data because they're positioned at eye level and are much closer to the passing consumers.
The systems can also detect frowning or nodding to determine the interest at the display from the potential shoppers. The retailers are using these techniques to enhance the shopping experience, to improve product assortment, and to help brands better understand their customers. As a result of such analysis and insight, an apparel store introduced a children's line after the results showed that kids made up more than half its mid-afternoon traffic. Another store found that a third of the visitors using one of its doors after 4 P.M. were Asian, prompting it to place Chinese-speaking staff by that entrance. The company that developed this technology is now testing a new product that recognizes words to allow retailers to eavesdrop on what shoppers say about the mannequin's attire. They also plan to add screens next to the mannequin to prompt customers about products relevant to their profile, much like cookies and pop-up ads on a web site.
While some believe this raises legal and ethical issues as well as privacy concerns, customer profiling has been around for a long time. The most celebrated use case in this category has to be the Target pregnant teen story.
A man walked into a Target in the Minneapolis area and demanded to talk to the manager. He was apparently angry and was holding coupons that had been addressed to his daughter. "My daughter got this in the mail!" he said. "She's still in high school, and you're sending her coupons for baby clothes and cribs? Are you trying to encourage her to get pregnant?"
The manager looked at the coupon book that was sent to the man's daughter. The mailer contained advertisements for maternity clothing, nursery furniture, and pictures of smiling babies. The manager was at a loss and apologized to the customer. He then called a few days later to apologize again. On the phone, though, the father was somewhat embarrassed. "I had a talk with my daughter," he said. "It turns out there's been some activities in my house I haven't been completely aware of. She's due in August. I owe you an apology."
For years, retailers have been mining through customer demographic information, point-of-sale (POS) data, and loyalty program history to understand shopping habits. The traditional demographic information that most retail stores collect includes age, marital status, household size (such as number of children and their age), approximate distance of potential shoppers' residence to the closest store, estimated household income, recent moving history, credit cards frequently used, and recently visited web sites. In addition to the variety of demographic information they collect every time the shopper visits the store, shops online, or calls customer service, most retailers buy enrichment data on shopping ethnicity; job history; the magazines shoppers read; credit history; the year you bought (or lost) your house; education status; social media feeds; personal and brand preference on a variety of products including coffee, paper towels, cereal, or applesauce; political orientation; reading habits; charitable giving; and the number and makes of cars owned.
In the case of the pregnancy prediction, Target was able to identify 25 products (such as prenatal vitamins, cocoa butter lotion, diaper-size bag, maternity clothes) that, when analyzed together, could generate a "pregnancy score" for each of the shoppers. And that score has been proven to be incredibly accurate. More importantly, it would also allow them to estimate her due date to within a small window so Target could send coupons timed to specific stages of her pregnancy.
The game-changer in this case is the ability to integrate various data sources and allow for ever-narrower segmentation of customers and therefore much more precisely tailored products or services.
 In addition to market basket analysis and customer profiling, there are many other innovative ways retailers are unlocking the big data value.
Demand Forecast
Demand forecast is another area where retailers are trying to better understand customers through the smart use of new data sources. As we discussed earlier in this chapter, a surge in online shopping during the 2013 holiday season left stores breaking promises to deliver packages by Christmas, suggesting that retailers and shipping companies still don't fully understand consumers' buying patterns in the Internet era. In addition, new business models are making accurate and timely demand forecast an even more critical success factor.
For example, one well-known sports apparel manufacturer and retailer is changing its go-to-market approach from selling individual items to assortment-based merchandising. In other words, instead of selling shoes and T-shirts separately, they are switching to a model that sells an entire outfit, including matching shoes, socks, hats, shirts, and pants or shorts. The assortments could be sports themed, location based, and season oriented. This changes what, where, and how data is to be sourced and applied in the business processes. A variety of new data sources will be integrated into the system, including social media and survey feedback on new product and assortment development, store layout maps for efficient display planning as well as foot traffic analysis, weather information, and location data with residents' demographics, local sports teams, and event schedule, all of which to help determine and maximize the operational efficiency for inventory, manufacturing, and logistics planning and management.
Figure 5-5 describes the demand forecast model used at a CPG company to predict the success of various digital initiatives and launches of new products.




FIGURE 5-5. Sample demand forecast model
Location-Based Marketing
The prevalence of smartphones gives rise to location-based marketing. It targets consumers who are close to stores or already in them. Some retailers have developed mobile applications for iOS and Android devices that integrate with their loyalty management program. The customers can download these applications and use them to store digital coupons, keep loyalty cards, and scan product bar codes to compare prices as well as obtain more information about the product details and manufacturer information. Retailers have also teamed up with mobile notification software providers for real-time customer engagement. These platforms can be implemented through distributing mobile beacons that can be plugged into any electrical outlet and transmit inaudible sound signals in stores. Some retailers alternatively have chosen to distribute signals through their existing PA systems. These methods allow retailers to track the customer movements in the store or a shopping mall with accuracy to the exact aisle they are in. This allows customers to receive deals sent to these mobile applications on their smartphones, enabling retailers to upsell customers with in-store conversion tools. It can also provide a channel for retailers to sell advertising to partners to target users while they are shopping.
There are similar location-based marketing use cases in the "Telecommunication" section we'll discuss in more detail next.
Telecommunication
Documents leaked by National Security Agency (NSA) contractor Edward Snowden indicate that the NSA is collecting 5 billion phone records a day. That's nearly 2 trillion records a year. The NSA gathers location data from around the world by tapping into the cables that connect mobile networks globally and that serve U.S. cell phones as well as foreign ones. This recent revelation has stirred up controversy over privacy. It also brought awareness to the general public regarding location tracking. Cell phones broadcast locations even when they are not in use. They will reveal your location even if you have your GPS or location service turned off.
According to McKinsey, overall personal location data totals more than 1 petabyte per year globally, and users of services enabled by personal-location data could capture $600 billion in consumer surplus as a result of price transparency and a better match between products and consumer needs.
Location-Based Promotion
Location-based promotion, also known as geofencing (when smartphone users enter a certain area), uses geolocation information derived from passive network location updates to identify potential customers for the geomarketing campaigns of mobile advertisers. The main business goals are to increase campaign and advertisement response, maximize offer conversion, and reduce cost per acquisition (CPA).
The main data sources used for this type of analytical applications include cellular network signaling, base station data, GPS-enabled devices, Wi-Fi, hotspots, and IP data. Some vendors also use additional data sets to improve accuracy, including billing data, census data with enriched demographic information, and call record data to understand social connections.
Sample geomarketing scenarios include port-of-departure, retail time-sensitive, and retail time-insensitive product offers. Port-of-departure use cases target customers who are in a location that can be identified as a travel hub, where customers may be interested in travel-related products, such as travel insurance. The most critical element for this use case is the correct filtering of false positives, such as people working or living close to the travel hub or passing through the area on a regular basis. Time-sensitive retail advertisement use cases target customers in close vicinity of a particular venue, such as restaurants. The advertisement is a short-term offer to drive traffic to the venue and requires accurate time and location determination. Time-insensitive retail advertisement use cases target customers who travel in the proximity of a retail outlet. This type of campaign runs over longer periods and is broken into two phases: a capture phase and a redeem phase. Advertisements are delivered only during the capture phase when customers are near the retail outlet and could reach a large number of eligible subscribers.
Because of privacy restrictions, mobile customers need to consent to have location data used for marketing purposes, and customers can opt out at any time.
Real-time event processing and analytics enable cell service providers (CSPs) to track a customer's location, and when a customer enters a certain area, the CSP can correlate that demographic, usage, and preference data to create targeted offers and promotions for CSP and partner services. CSPs can also analyze a subscriber's mobile network location data over a certain period to look for patterns or relationships that would be valuable to advertisers and partners.
One telecommunication company that has successfully implemented this type of solution is Turkcell, which realized that the GSM network presents many opportunities to provide real-time business recommendations with its data. The ability to immediately recognize the key event patterns for call activations with associated locations could enable new ways to enhance customer satisfaction by offering targeted services and promotions, when and where relevant. This is not without challenges because of a wealth of streaming telecommunication elements and different data representations. Turkcell developed analytical applications to capture real-time streaming information immediately from its GSM network for identifying subscriber events from mobile devices with a GeoSpatial and SMS text message with temporal relevance. This enables Turkcell to offer a wide range of new, valuable, and interesting services delivered when they are most applicable. These applications improved customer satisfaction and introduced a wide array of revenue opportunities through the ability to predict trends based on changing locations in association with real-time external conditions such as weather, traffic, and events.
Fraud Prevention
Communications fraud, or the use of telecommunication products or services without intention to pay, is a major issue for telecommunication organizations. The practice is fostered by prepaid card usage and is growing rapidly. Anonymous network-branded prepaid cards are a tempting vehicle for money launderers, particularly since these cards can be used as cash vehicles—for example, to withdraw cash at ATMs. It is estimated that prepaid card fraud represents an average loss of $5 per $10,000 in transactions. For a communications company with billions of transactions, this could result in millions of dollars lost through fraud every year.
Naturally, telco companies want to combat communications fraud and money laundering by introducing advanced analytical solutions to monitor key parameters of prepaid card usage and issue alerts or block fraudulent activity. This type of fraud prevention would require extremely fast analysis of petabytes of uncompressed customer data to identify patterns and relationships, build predictive models, and apply those models to even larger data volumes to make accurate fraud predictions.
Companies are deploying real-time analytical solutions to monitor key parameters and usage, such as call detail records (CDRs), usage data, location, and prepaid card top-ups. They then issue alerts or block fraudulent activity. Data discovery tools are used to explore questions such as "why" and "what if" to detect and identify anomalies and detect recurring fraud patterns.
The same telecommunication company mentioned earlier, Turkcell, has developed this type of analytical solution that enables it to complete antifraud analysis for large amounts of call-data records in just a few hours. It is able to achieve extreme data analysis speed through a row-wise information search, including day, time, and duration of calls, as well as number of credit recharges on the same day or at the same location. The system enables analysts to detect fraud patterns almost immediately. Further, the architecture design allows Turkcell to scale the solution as needed to support rapid communications data growth.
Machine to Machine
Machine to machine (M2M) devices and applications generate a wealth of usage, telemetry, and sensor data that can be monetized and analyzed. Enabling these capabilities requires comprehensive analytics and big data technologies ranging from operational reporting, analytics, and BI to big data and real-time event processing in order to capture the high-volume and high-velocity event data from M2M devices for analysis.
Real-time event processing is seen as a major opportunity for the next generation of M2M applications. This refers to the scenario when several data sources contribute to a decision process without human intervention. Such data sources include sensor and telemetry data from M2M-enabled devices, including smart meters, fleet management, manufacturing, automotive, home automation, and various data sources from operations of a smart city. Big data technologies store and analyze this sensor and telemetry data and are an important part of the M2M application development platform from device to data center.
Here are some examples of M2M applications: One large auto manufacturer is increasing data scale to include 13 years of dealer history. It also added new data sources, including telematics data, from its new hybrid vehicles. The manufacturer uses an analytical solution with these data sources to prevent and catch problems early. This allows the manufacturer to recommend what service is due and which dealer to choose based on dealer service history. It also enables its customers to perform social media interaction with the share function built in to the communication module in the cars.
Another example is with a navigation and communication branch of a large auto manufacturer. The manufacturer collects large amounts of telemetry data by nature. With customer consent, the manufacturer is developing applications and data services to enable potential customers such as rental car companies to improve fleet management and speed up car return processes.
Network Operations: DPI Analysis
The voluminous nature of network data has meant that traditional network performance management and optimization and planning tools are typically the domain of niche vertical industry software platforms from network equipment providers (NEPs) or niche Internet service providers (ISPs). The introduction of big data analytics opens the door to general-purpose analytics and commercial off-the-shelf (COTS) analytics platforms in the network domain. CSPs can now identify trends in networks with the potential of overcapacity and undercapacity by analyzing across different attributes, including type of network traffic, geography, events, key dates, and time of day.
Big data analytic applications enable fast processing of a high volume of raw network data. They enable CSPs to analyze network capacity, outages, and congestion, as well as plan for growth in network capacity where it is most needed. In addition, they enable CSPs to analyze and determine root cause for faults and outages. Data discovery tools enable CSPs to explore questions such as "why" and "what if" and bring new agility to business intelligence.
 Streaming capabilities are also enabling CSPs to process and analyze raw network data in real time, detect outages and congestion, and act on these events. They can be applied to different aspects of network operations, including capacity and coverage planning, network utilization management and operations, new service introduction, network traffic management, and congestion prevention.
Various data sources used in improving network operations include call detail records, network logs, signaling logs, deep packet inspection (DPI), network element (NE) logs and counters, element management system (EMS), and various network management system (NMS) logs.
Financial Services
Similar to the retail industry, financial services organizations have long realized the importance of data and have been an innovative force in analytics and insights. Much of data architecture work and discussions we have had have been with financial companies in the areas of master data management, data integration architecture, and data governance. Even with higher profit margins and deeper pockets, there still have been compromises in terms of the capabilities of historic data storage and limitations in the ability to incorporate unstructured data. Big data has permanently changed the landscape with drastically lower storage costs and new tools for text enrichment functionalities.
Big data analytics use cases in financial services typically fall into one of the following four areas: customer insight, product innovation, company insight, or loss prevention. Customer-focused use cases include credit worthiness analysis, customer profiling and targeting, portfolio management, and collections. Product innovation helps determine bank products and bundles, insurance offers, money management portfolio development, and trading strategies. Loss prevention is a key area where analytics can be applied in different aspects to detect payment fraud and mortgage fraud and to measure and mitigate various risk areas including reputation and liquidity internally and for counterparties. Last but not least, company insight analytics can enhance brand awareness, monitor employee sentiment, and improve compliance management in insider trading detection and investigation to ensure the alignment of fiduciary responsibilities of a company and its employees.
Reputation Management
Let's first take a look at a use case for compliance and reputation management. One company we studied has been trying to prevent the extension of financing to corporate or wealth management clients with questionable reputations on environmental issues, ethical lapses, or regulatory malfeasance. However, it has faced difficulties in processing unstructured data from multiple sources as well as analyzing large volumes of data in a timely manner.
With new capabilities in unstructured data integration with their compliance engine and text enrichment, this company is now able to capture data from thousands of third-party sources, such as print media, government agencies, nongovernment organizations (NGOs), think tanks, web sites, newsletters, social media, and blogs, to detect controversial information and compliance issues at companies and other large institutions. The company is now able to provide standardized information and user experience across departments; this allows financial and risk analysts to vet clients to ensure their reputational risk profile falls in line with the bank's developing reputational risk posture and to obtain a 360-degree view of a company in the process of deciding whether they want to do business with them.
Fraud Prevention
Large financial institutions play a critical role in detecting financial criminal and terrorist activity. However, their ability to effectively meet these requirements is affected by the following challenges:
        The expansion of anti-money laundering laws to include a growing number of activities, such as gaming, organized crime, drug trafficking, and the financing of terrorism
        The ever-growing volume of information that must be captured, stored, and assessed
        The challenge of correlating data in disparate formats from a multitude of sources
Their IT systems need to provide abilities to automatically collect and process large volumes of data from an array of sources, including currency transaction reports (CTRs), suspicious activity reports (SARs), negotiable instrument logs (NILs), Internet-based activity and transactions, and much more. Detecting fraud was once a cumbersome and difficult process that required sampling a subset of data, building a model, finding an outlier that breaks the model, and rebuilding the model. Now, more and more large financial organizations are moving to an approach that allows them to capture and analyze unlimited volumes of unstructured data, integrate with billions of records, and build models based on an entire universe of data rather than a subset, making sampling obsolete. They can examine every incidence of fraud going back many years for every single person or business entity for fraud detection and prevention as well as credit and operational risk analysis across different lines of business, including home loans, insurance, and online banking.
Personalized Premium and Insurance Plan
In an effort to be more competitive, insurance companies have always wanted to offer their customers the lowest possible premium, but only to those who are unlikely to make a claim, thereby optimizing their profits. One way to approach this problem is to collect more detailed data about individual driving habits and then assess their risk.
In fact, some insurance companies are now starting to collect data on driving habits by utilizing sensors in their customers' cars. The sensors capture driving data, such as routes driven, miles driven, time of day, and braking abruptness. As you would expect, these insurance companies use this data to assess driver risk. They compare individual driving patterns with other statistical information, such as average miles driven in an area and peak hours of drivers on the road. This is correlated with policy and profile information to offer a competitive and more profitable rate. The result is a personalized insurance plan. These unique capabilities, delivered from big data analytics, are revolutionizing the insurance industry.
To accomplish this task, a great amount of continuous data must be collected, stored, and correlated. This automobile sensor data must be integrated with master data and certain reference data, including customer profile information, as well as weather, traffic, and accident statistics. With new tools to integrate and analyze across all these data sources, it is possible to address the storage, retrieval, modeling, and processing sides of the requirements.
Portfolio and Trade Analysis
When a leading investment bank tried to do some portfolio analysis 18 months ago, it faced challenges with the large volumes of data that its data scientists wanted to use. The firm was limited to using smaller sample sets. The desire to work with large volumes and varieties of data from all angles remained a lofty goal until recently, when the bank looked into a schema-less data design and data model approach. Now the data scientists can incorporate market events in a timely manner and discover how they correlate with performance characteristics. They are able to reconcile the front-office trading activities with what is occurring in the back office and determine what went wrong in real time, instead of days, weeks, or months later. Ultimately, they have a scalable solution for portfolio analysis.
The U.S. securities markets have undergone a significant transformation over the past few decades, particularly in the last few years. Regulatory changes and technological advances have contributed to a tremendous growth in trading volume and the further distribution of order flow across multiple exchanges and trading platforms. Moreover, securities often trade on various disperse markets, including over-the-counter (OTC) instruments. Additionally, products with similar nature and objective are traded on different markets. The Securities and Exchange Commission (SEC) Rule 613, called Consolidated Audit Trail (CAT), was approved in 2012 to serve as a comprehensive audit trail that allows regulators to more efficiently and accurately track activity in National Market System (NMS) securities throughout the U.S. markets. The CAT is being created by a joint plan of the 18 national securities exchanges and FINRA, collectively known as the Self-Regulatory Organizations (SROs).
As a result, the SEC is building a trading activity analysis and repository system for CAT. The main objective of this system is to assist the efforts to detect and deter fraudulent and manipulative acts and practices in the marketplace and to regulate their markets and members. In addition, it is intended to benefit the commission in its market analysis efforts, such as simulating the market effect of proposed policy changes, investigating market reconstructions, and understanding causes of unusual market activity. Further, this solution enables the timely pursuit of potential violations, which could play a great role in successfully seeking to freeze and recover any profits received from illegal activity.
The total number of messages per day is estimated to be 58 billion and 13TB in size. It's currently under a request for proposals (RFP) process. Companies that submitted a bid to build this system include Google, IBM, NASDAQ, FINRA, AxiomSL, Software AG, Tata, Wipro, and Sungard Data Systems. This is perhaps one of the most interesting big data implementation projects to watch in 2014.
Mortgage Risk Management
The recent economic challenges have affected the number of delinquent loans and credit card payments in retail banking, resulting in growing defaults. Most banking and mortgage companies have a vast amount of complex and nonuniform customer interaction data across delivery channels. In addition, operational systems store customer interaction data for only a limited amount of time for analysis. To address these challenges, one large bank implemented an analytical system that mashes up different types of data, including transactions, online web logs, mobile interactions, ATM transactions, call center interactions, and server logs. From this, its mortgage analysts can determine the most appropriate collection calls to make and even tie the collection activities into upsell campaigns. Some of the positive outcomes from this solution include early detection of potential defaults, lower default rates on retail loan products, improved customer satisfaction, and increased revenue with the ability to offer upsell campaigns during the collections process.
Healthcare and Life Science
Based on McKinsey, if U.S. healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. Two-thirds of that would be in the form of reducing U.S. healthcare expenditures by about 8 percent. These goals can be achieved by using data innovatively and by breaking down political and operational boundaries. Some of the use cases discussed in this section include remote patient monitoring, health profile analytics, optimal treatment pathways, clinical research and study design analysis, and personalized medicine.
Remote Patient Monitoring
A majority of older adults are challenged by chronic and acute illnesses and/or injuries. In fact, eight out of ten older Americans are living with the health challenges of one or more chronic diseases.
Telemedicine can be used to deliver care to patients, regardless of their physical location or ability. Research has shown that patients who receive such care are more likely to have better health outcomes and less likely to be admitted or readmitted to the hospital, resulting in huge cost savings. One well-proven form of telemedicine is remote patient monitoring. Remote patient monitoring may include two-way video consultations with a health provider, ongoing remote measurement of vital signs, or automated or phone-based check-ups of physical and mental well-being. These systems include monitoring devices that capture and send information to caregivers, including patients' vital signs such as blood sugar level, heart rate, and body temperature. They can also transmit feedback from caregivers to the patients who are being monitored. In some cases, such devices even include "chip-on-a-pill" technology, which are pharmaceuticals that keep track of medication and treatment adherence. Some instruments can also self-activate and alert patients and caregivers that a test or medication must be taken. Data is subsequently transferred to healthcare professionals, where it is triaged through patient-specific algorithms to categorize risk and alert appropriate caregivers and clinicians when answers or data exceeds predetermined values. Studies have shown that the combination of the two interventions—in-home monitoring and coaching—after hospitalization for congestive heart failure (CHF) reduced rehospitalizations for heart failure by 72 percent and reduced all cardiac-related hospitalizations by 63 percent.
More generally, the use of data from remote monitoring systems can reduce patient in-hospital bed days, cut emergency room visits, improve the targeting of nursing home care and outpatient physician appointments, and reduce long-term health complications. According to a study by the Center for Technology and Aging, the U.S. healthcare system could reduce its costs by nearly $200 billion during the next 25 years if remote monitoring tools were used routinely in cases of CHF, diabetes, chronic obstructive pulmonary disease (COPD), and chronic wounds or skin ulcers.
Health Profile Analytics and Optimal Treatment Pathways
Advanced analytics can be applied to segment patient profiles to identify individuals who would benefit from preemptive care or lifestyle changes. Advanced analytics help identify patients who are at high risk of developing a specific disease (for example, diabetes) and would benefit from a proactive care or treatment program. In addition, they could enable the better selection of patients with preexisting conditions for inclusion in a disease-management program that best matches their needs. Further, patient data can provide an enhanced ability to measure the outcome of these programs as a basis for determining future funding and direction.
Similar methods can be applied to analyze patient and outcome data, to compare the effectiveness of various interventions, and to identify the most clinically effective and cost-effective treatments to apply. This analytical approach provides great potential to reduce incidences of overtreatment, as well as reducing incidences of undertreatment (in other words, cases in which a specific therapy should have been prescribed but was not).
Figures 5-6, 5-7, and 5-8 show sample analytical applications for healthcare correlations that allow analysts to understand readmission rates and to improve healthcare outcomes. Figure 5-7 contains a number of visual components. It includes a search box on the top left with a Guided Navigation section below the search box that enables the user to filter patient demographics and admission diagnoses. The metric bar contains key performance indicators for patient number, readmission statistics, average stay, and average number of procedures. The discovery components can be customized to display patient county breakdowns and more detailed diagnosis analysis. Figure 5-8 is a map view of patients and hospital correlations in the New England area that's under analysis.




FIGURE 5-6. Sample data sources for healthcare correlation analysis




FIGURE 5-7. Sample discharge analysis application




FIGURE 5-8. Map view of patients and hospital correlations
This sample analytical application brings together data from patient registration systems (responsible for admit, discharge, and transfer [ADT] events), electronic medical record (EMR) systems, billing systems, and unstructured data such as nurse notes and radiology reports. The users of this system include healthcare providers whose job is focusing on improving care management, ensuring successful medical outcomes, and accelerating operational performance improvements.
Clinical Analysis and Trial Design
Clinical analysis refers to analyzing physician order-entry data and comparing it against medical guidelines to alert for potential errors such as adverse drug reactions or events. Big data analytical applications can add more intelligence by including image analysis of medical images (such as X-ray, CT scan, or MRI) for prediagnosis, routinely scanning medical research literature to create a repository of medical expertise capable of proposing treatment options to physicians based on a patient's medical records, symptoms, and past treatment outcomes.
Big data analytical applications are also being used to improve the design of clinical trials. Through mining patient data in different regions and locations and historic clinical trial outcome, statistical methods can assess patient recruitment feasibility, expedite clinical trials, provide recommendations on more effective protocol designs, and suggest trial sites with large numbers of potentially eligible patients and strong track records. Predictive modeling leverages the aggregation of research data for new drugs and the determination of the most efficient and cost-effective allocation of R&D resources. A simulation and modeling approach is being used on preclinical and early clinical data sets to predict clinical outcomes as promptly as possible. Evaluation factors can include product safety, efficacy, potential side effects, and overall trial outcomes. This new class of analytical solutions can reduce costs by suspending research and clinical trials on suboptimal compounds earlier in the research cycle.
 The benefits of this approach include lower R&D costs and earlier revenue from a leaner, faster, and more targeted R&D pipeline. These solutions help bring drugs to market faster and produce more targeted compounds with a higher potential market and therapeutic success rate. Predictive modeling can shave 3 to 5 years off the approximately 13 years it can take to bring a new compound to market.
Personalized Medicine
The recent revolution in genetic research has affected virtually every aspect of medicine. It is recognized that the genetic composition of humans plays a significant role in an individual's health and predisposition to common diseases such as heart disease and cancer. In addition, patients with the same disease often respond differently to the same treatments, partly because of genetic variation. The adjustment of drug dosages according to a patient's molecular profile to minimize side effects and maximize response is known as modular segmentation.
This new genomic era provides excellent opportunities to identify gene polymorphisms, to identify genetic changes that are responsible for human disease, and to build an understanding of how such changes cause disease. In the clinical arena, it is now possible to utilize the emerging genetic and genomic knowledge to diagnose and treat patients. New solutions include the analysis of a large set of genome data to improve R&D productivity and develop personalized medicine. These solutions examine relationships among genetic variation, predisposition for specific diseases, and specific drug responses to account for the genetic variability of individuals in the drug development process.
For personalized medicine to be a fully functioning reality at the clinical level, certain features are essential: an electronic medical record, personalized genomic data available for clinical use, physician access to electronic decision support tools, a personalized health plan, personalized treatments, and personal clinical information available for research use.
Specific advantages that personalized medicine may offer patients and clinicians include the following:
        Ability to make more informed medical decisions
        Higher probability of desired outcomes thanks to better-targeted therapies
        Reduced probability of negative side effects
        Focus on prevention and prediction of disease rather than reaction to it
        Earlier disease intervention than has been possible in the past
        Reduced healthcare costs
Manufacturing
A common misconception of using big data is that it concerns only social media and consumer interaction. Contrary to this misunderstanding, big data and analytical applications are achieving extensive value for manufacturing companies to enhance and optimize operational efficiencies and capabilities.
Quality and Warranty Management
Quality management for manufacturing companies is critical to success. The ability to determine the root cause of quality issues in a timely manner, especially under public pressure, has always been challenging. One large auto manufacturer experienced just that.
 Known and cherished for quality standards, this auto manufacturer was under tremendous fire from media and government and faced litigation pressure for dismissing some claims regarding unexpected acceleration. For reliability engineers, it is always a challenge to sort through thousands of claims a day to prioritize the investigation to proactively discover engineering and design issues and filter out other causes such as driver error. Much of the information needed is unstructured data coming from customer complaints and technician resolution notes.
The auto manufacturer deployed a data discovery application that allows them to go beyond data classification and find hidden correlations through simple search interfaces. Traditionally, it is difficult to trace failures back to suppliers and find out what suppliers and parts contributed to the warranty problems. The reason is that all of the underlying data needed for the analysis is scattered across various systems, including enterprise resource planning (ERP) systems, product life-cycle management (PLM) applications, and quality and warranty management systems. The ability of the data discovery platform to join this variety of structured and unstructured data sources allows the reliability engineers to answer not only the "what" questions but also the "why" questions. In addition, the most vital questions are the questions that can't be asked. These are the unscripted questions that surface only through exploring the data, testing hunches, and uncovering surprising trends.
For example, the auto manufacturer was investigating some rising trend in battery and smoke complaints. The data discovery system enabled them with self-service data loading, iterative visualization, visual summary, automated data profiling and clustering, and an all-record search to cast a wide net across all of the data sources mentioned earlier. The application automatically configured data range filters based on the actual data values for ease of exploration. The search capability allowed the engineers to leverage their expertise, intuition, and tribal knowledge to ask relevant questions. Through iteratively applying filters, changing dimensions to geographic display, and using different visualization methods such as a heat map, the engineers were able to pinpoint the root cause that low temperature in northern Midwest areas caused premature failure of the batteries. With iterative discovery and the process of exclusion, they were able to test various hypothesis, remove noise, and get to the answer in a few minutes. Other techniques they applied included using bubble charts to look at multiple variables at one time and using dimension cascade for automatic grouping and drilling. Ultimately, they were not only able to confirm battery problems but were able to determine where and when it was happening. They also found out which supplier and part was causing the defect. That enabled them to look into sourcing alternatives and take other actionable measures to correct the problem in a timely manner.
Production Equipment Monitoring
Another usage pattern for the manufacturing industry takes big data into real time. The data factory and data warehouses typically store historic information for equipment performance history and maintenance records. Advanced algorithms can be built to analyze correlations between certain events and characteristics, which can be used to predict future mechanical failures. As sensor data moves through the event-processing engine, a match for that event with the predictive model will occur and determine whether an alert needs to be raised. Maintenance staff can be deployed before a breakdown occurs. After the maintenance staff is deployed, they can report to the system whether that prediction is valid. That information will feed back to the predictive model and continue to improve the model and capability.
Public Sector
With ever-shrinking budgets, government agencies in many parts of the country are under pressure to provide better services with less funding. Big data presents significant potential to improve productivity and to minimize waste, fraud, and abuse.
Law Enforcement and Public Safety
Leveraging technology to search and correlate crime data is an absolute necessity in the modern era of intelligence-led policing. Let's take a moment to consider the challenges faced by policing organizations today. Data and information are always fragmented. Specific systems aligned to specific functions have grown organically over a number of years, and it is not uncommon for a policing organization to rely on hundreds of siloed applications to support their operations.
Police officers need to access multiple systems to find answers to commonly asked questions, such as these: Who is the registered owner of the vehicle caught on CCTV? Which suspects have these physically identifying marks similar to those described by a witness? Who do these partial fingerprints retrieved from a crime scene belong to? Who else can I put in a photo identity line-up? The police officer in the field responding to incidents needs to know answers to these questions in real time and with reliability.
Large police departments are combining data from social media with their existing departmental data to identify hot spots, correlate deployment of thousands of police officers, initiate operational orders, and retrospectively identify potential witnesses. The Chicago Police Department leveraged information discovery capabilities during the May 2012 NATO Summit in that city. In addition to the 7,000 visiting dignitaries, the summit brought to the city tens of thousands of protestors. The application's unstructured data analysis capabilities were crucial in integrating social media information with their departmental 911 and force deployment information. With this analysis, officers could be rapidly redeployed to preemptively address evolving threats.
Waste, Abuse, and Fraud Prevention
Revenue departments at various levels of government are leveraging big data technologies to optimize, deepen, and extend their revenue fraud prevention capabilities. Some of them are developing analytical applications, including classification, duplicate filing, clustering, and anomaly, to prioritize audits and to respond more quickly to taxpayer inquiries, such as when a refund will be processed. Some of the effort has been focused on providing easy access to the data so the auditor needs less frequent contact with a taxpayer undergoing an audit.
A number of innovative leaders in this space are also finding ways to deepen the analysis. Some are leveraging crowd sourcing to digitize paper filings. Text analytics are being used to integrate auditor notes and other relevant content repository. Some are also using social media to correlate claim and filing accuracy. For example, "check-ins" on Facebook or Foursquare can reveal location information about the taxpayer. Year-over-year comparisons are also becoming commonplace with more historic filings becoming available.
Moreover, many state agencies are breaking down boundaries and including external data sources in their analysis. For example, the state of Georgia is using Nexus Lexus services to supply public records, including voting records, incarceration records, birth and mortality data, and house purchase and mortgage rate for primary residence, as well as litigation data. Several states have formed the Multistate Clearinghouse to share data across states. In some cases, this organization is able to locate individuals who filed a nonresident return in one state but failed to file a resident return in their home state.
Service Improvement
There are many examples of how government agencies are using big data technologies to improve services, including reducing infant mortality and child fatality, pest infestation prevention, and better city planning.
Big data has great potential to improve city planning. One use case is to use mobile device location data to develop algorithms to look for patterns and help transportation planners on traffic reports. Here's an example: A coastal city near another major business hub is trying to attract businesses to the city, but they face the stigma of a reputation for crime and poverty. They leverage anonymized location data, combined with DMV records and U.S. census information, to show the patterns of wealth and vibrancy in the city to convince national chains that don't know the city well to open a store there and then help them choose ideal locations.
IT Use Cases
In this section, we'll cover use cases that are pertinent to IT departments for various industries, including log processing, security management and intrusion detection, enterprise data warehouse expansion, and ETL offloading.
Log Processing
IT systems generate a significant number of log files. This includes network logs, operating system-level logs, storage server logs, various application server logs, and database log files; almost every application and software package generates logs continuously. These log files are typically discarded within a short period because of the size, the cost of storage, and the low value density.
It is not until recently that IT professionals have realized there is tremendous information and insight behind these perceived valueless piles of data. New analytical applications have been developed to mine these system logs to discover patterns of alerts and signals to predict potential system failures before they occur. More and more applications now proactively design architecture that will enable long-term storage and analysis of logs to provide insights on user behavior and to improve site and application design. Many of the health insurance exchange implementations have incorporated design for forensic capabilities to store and mine log files in order to detect and prevent fraud, waste, and abuse based on the mandate from the Center for Medicare and Medicaid Services (CMS).
Security Management
Similar to fraud detection in financial services, the effective management of security control versus access is always a dilemma. It is often challenging to balance prevalent information access with nonintrusive but effective security measures. It is also important to be more proactive in determining a potential security breach without creating false alerts. The challenges are because most of these security measures are rule based. The simple fact is we don't know what we don't know. Sometimes, the answer is to increase restrictions. As was mentioned earlier, false alerts tend to dilute the confidence in future alerts.
The new method of security management is to use the historic access patterns to determine anomalous behavior. The data exploration, clustering, and association mechanisms can look into combinations of factors such as role, function, time of the day, day of the week, location (home, office, public), device, application type, activity performed, and more. This can point to the unknown associations and help security analysts define and design better security measures.
 This doesn't mean we abandon the rule-based security measures today. These rules come from trials and errors and past experience gathered in the field, and they are tremendously valuable. With new data-driven discovery and pattern detection capabilities, there is more power in the way we can proactively manage security and detect illegal access.
Here's an example: One large U.S. bank was challenged with increased cyberthreats and had reached "security appliance" fatigue. With every new threat, a vendor would pop up with a new appliance. The bank had huge amounts of security data, including Windows and IDS logs, but had difficulty leveraging it. It was looking for a data-driven security strategy that included preventing account takeover fueled by malware. Incident response involved a time-consuming process of examining voluminous log files. The new solution stored more than 120 different types of data, including transactions, logs, fraud alerts, server logs, firewall logs, and IDS logs for more than two years, resulting in more than 120TB of data. They are now able to spot anomalies in log files and other machine data that could indicate a hacking or phishing attempt.
The biggest benefit with the big data strategy for forensics has been speed of detection. The combined signs of a spear phishing attack with the statistical methodologies boosts the bank's ability to identify potential attacks. The bank can now quickly act on intelligence received from various sources on malware threats and counter them.
EDW Augmentation
A common usage pattern lately is data warehouse augmentation. Here is what a traditional data warehouse process looks like: Data sources are being loaded into a data warehouse hub through ETL. Sometimes they are then moved into high-performance data marts for departmental analytics and reporting. Most companies can't afford to keep data in their warehouse indefinitely. The older the data becomes, the more aggregated it becomes. Detailed and granular data archived to tape is usually never used.
Big data changes this fundamentally. More and more companies are building a data reservoir to augment their existing data warehouse architecture. Data in the data reservoir is persistent, and the focus is on data processing, data storage, and reuse of the most granular level of data.
Information discovery can also be performed within a data reservoir. We've seen this pattern emerge that creates a data reservoir for discovery data marts that taps into a wide range of data elements. It simplifies data acquisition and enables discovery on raw data. The value of the new data sets can be evaluated and incorporated into production and operations, once confirmed.
ETL Offloading
ETL offloading, also known as a data factory, is a usage pattern that enables an organization to integrate and transform in batch mode large diverse data sets before moving data into data warehouses. It's not a use case of analytical application, but, rather, an extension and necessary step toward building these applications. As a result, data in the data factory is transient for the purpose of processing.
Summary
By now, you should have a bird's-eye view of the different ways companies, government agencies, and other organizations are leveraging big data to realize value. The main focus of this chapter is to describe what is occurring. The innovative use of big data discussed can be accomplished through different implementations of tools and architecture. There are multiple capability areas of big data technologies, and Oracle Endeca Information Discovery is one of them. In the next few chapters, we'll drill into some of these use cases and examine in detail how organizations are using Oracle Endeca Information Discovery to enable new business and analytical capabilities.
Chapter 6 details the use case implementation of fraud, waste, and abuse prevention.
Chapter 7 provides a step-by-step tutorial on how to build a healthcare correlation application using Oracle Endeca Information Discovery from source to application.
Chapter 8 discusses guidelines, best practices, guiding principles to establish big data and analytical capabilities. It also covers the challenges companies face in this area, common mistakes, and measures to mitigate such risks and to overcome these challenges.








CHAPTER6
Implementing Fraud Detection
To me, ideas are worth nothing unless executed. They are just a multiplier. Execution is worth millions.
—Steve Jobs
Recognized as one of the most innovative leaders in modern times, Steve Jobs stressed the importance of execution on numerous occasions. As he eloquently stated, there's a tremendous amount of craftsmanship in between a great idea and a great product.
In this chapter and the next, we will cover executing and implementing Oracle Endeca Information Discovery applications. We will cover some of the use cases discussed in the previous chapter and walk through the implementation details. The use cases in this book leverage public data sets with some modifications for illustration purposes. The focus of this chapter is voter fraud analytics and detection, and you can obtain the data sources used in this use case through data portals of various government agencies. We have modified some records to illustrate abnormality analysis and detection. They do not reflect or indicate real-world occurrences of fraud, and the use case by no means indicates there were actual occurrences of voter fraud in the municipality and counties where the data originated.
This chapter first gives an overview of the voter fraud use case and then breaks down the implementation steps into two parts: election results analysis to confirm alleged fraud and a deep dive into voter records to look for potential fraudsters.
Overview of Voter Fraud Analytics and Detection
To date, a majority of U.S. states have prosecuted voter fraud. In some cases, people who committed the fraud might have done so unintentionally and were not aware of their ineligibility. For example, recently a former drug offender in Iowa was acquitted of perjury of voter fraud, an offense that could lead to 15 years in prison upon conviction. The jury found her not guilty and considered it an honest mistake when she cast a ballot under the belief her voting rights had been restored after leaving probation.
While unintentional mistakes exist, there have been many controversies in recent years regarding the pervasiveness of intentional election fraud, especially in tight races.
There are various forms of election fraud. Large-scale fraud often occurs prior to the election. This includes electoral and demographic manipulation, voter intimidation, disenfranchisement, and vote buying. It is also believed that there are considerable numbers of deceased voters who still remain on the rolls across the country. In addition, some counties have more registered voters than residents because in some areas people voted more than once in the same county or in different states.
Implementing Voter Fraud Analytics and Detection
This use case originated from allegations that there had been potential voter irregularities in a certain county. You will first examine a simple data source for recent election results. The election results are public records, and they usually include aggregated counts of voter turnout versus registered voters, as well as election results based on precinct. This is a good starting point to understand the outcome of the election and what potential fraudulent activities might exist. (If you want to follow along, you can download the sample application and the data set. The download site URL can be found in the "Useful Links" section of the appendix in this book.) As you discover and confirm potential anomalies, you will then drill into detailed voter records, including voter demographic information, location and addresses, and voting history. You will enrich this set of detailed voter records with additional data sources from live statistics and property tax records in nearby regions. We will take you step by step through how you would explore these separate data sources and how you would integrate and analyze them to discover potential fraud.
Election Results Analysis
Let's start with a simple election results data feed from the Secretary of State web site. Here are the columns this use case will use:
        County Number The number assigned to a county within the state.
        Precinct Name The name of the precinct. This is the level where the aggregation of registered voters and total voters for an election is tallied.
        Election Date The date when the election was held.
        Election Type The type of election such as primary, general, or special.
        Registered Voters The total number of registered voters in the precinct.
        Total Voters The total number of voters who voted on that election date.
Figure 6-1 shows a sampling of the data sheet.




FIGURE 6-1. Election result data set snippet
 In this use case, we will walk you through using the Provisioning Service within Oracle Endeca Information Discovery Studio to create a simple application with this data source. The following sections cover the step-by-step instructions on how to accomplish this.
Step 1: Launch Endeca Studio and Create a New Application
Figure 6-2 displays the Endeca Studio home page, which lists the available Information Discovery applications. The last icon allows you to create a new application.




FIGURE 6-2. Endeca Studio home page
Step 2: Enter the Application Name and Description and Select a Data Source
After you click the New Application icon, you enter the application name and description on the New Discovery Application screen. (Application description is an optional field.) There are two file format options available: Excel or JSON. In this case, we are choosing Excel; then the Select a file option appears that allows you to navigate to the file location, as shown in Figure 6-3.




FIGURE 6-3. Creating a new Information Discovery application
After you select a file, Endeca Studio will parse the file content and display the data it contains in a window, as shown in Figure 6-4.




FIGURE 6-4. Data set display in Endeca Studio
 Step 3: Review and Modify Metadata
After clicking Next in the top-right corner of the screen, the metadata of the file will be loaded. We'll keep the default settings of all the field names and data types in this case, as shown in Figure 6-5.




FIGURE 6-5. Data set metadata screen in Endeca Studio
Step 4: Create the Application
Upon clicking Done, Endeca Studio loads the data set and creates an Information Discovery application, as shown in Figure 6-6.




FIGURE 6-6. Default Information Discovery application
Step 5: Start Exploring Data
The default layout in an Endeca Information Discovery application is a two-column panel, as shown in Figure 6-6. The left panel has a search box and refinements. The search box works just like Google. When the user enters a search item, the "type-ahead" hint will be displayed to allow the user to select a matching attribute value or to search on the text entered.
Selected Refinements tracks the filters applied through the search or refinement section. It creates a breadcrumb that enables the user to track, add, or remove the applied filters.
The Available Refinements section displays attributes available for filtering. Endeca Studio automatically clusters the attribute values and displays the most applicable filtering options, as shown in Figure 6-7.




FIGURE 6-7. Available refinements
The results table lists records for the current refinement. You can also configure the result table to display a set of metrics aggregated by one or more dimensions. We provide an example for this in Chapter 7. (You can find details of the Endeca Studio components in Chapter 4 of this book.)
Step 6: Discover Insights
The first thing you can do as you continue exploring the election result data set is to configure the chart component in the top-right section of the page. As you mouse over the gear icon on the top-right corner of the chart, the "option" hover text will appear. Click the gear icon to configure the  chart. You will see four tabs on the chart configuration screen that appears: Data Selection, Chart Type, Chart Configuration, and Style Options. The Data Selection tab is prepopulated, and we will keep the default settings for this use case. Click the Chart Type tab, select the Bar-Line type, and choose Single axis, as shown in Figure 6-8.




FIGURE 6-8. Chart type selection
On the Chart Configuration tab, select the sum of Registered Voters in the bar section of Series Metrics and the sum of Total Voters for the line display. Also, remove all other attributes from the Dimensions section except for Precinct Name. Figure 6-9 shows the final selection and configuration.




FIGURE 6-9. Chart configuration
 Then click Save and Exit. Endeca will take you back to the application screen with the newly configured chart displayed, as shown in Figure 6-10.




FIGURE 6-10. Election results chart
As you navigate from page to page, you are able to determine the voter turnout from precinct to precinct. On page 2 of the election result chart, notice that one of the counties has more voters in the November election than the total registered voters, as shown in Figure 6-11.




FIGURE 6-11. Comparing total voters and registered voters
A Deeper Dive of Voter Records
To further investigate, we'll need to incorporate additional data sources, including voter records, live statistics, and property tax records from various state and county recorders' offices. The tool of choice in this case is Endeca Integrator ETL because you need to join several data sources for analysis.
Data Sources
In addition to the election results data set we loaded through the Endeca Provisioning Service in the previous section of this chapter, in this section, we will include new data sets for a more in-depth analysis of voter records. You can do the initial exploration of these new data sets through Endeca Provisioning Service as well. When you add a new data set using Provisioning Service, Endeca Studio will automatically create a new tab in the application with the default page layout.
Voter Records This data set contains details on registered voters, including demographic information, location data with both residential address and mailing address, voting history, and party affiliation. You can add a tag cloud for the mailing address's city and state on the Voter tab, as shown in Figure 6-12. It provides a view of the mailing address' city and state range for the voters. It is not surprising to see the city of Toledo has the majority of record counts since this hypothetical case is in Lucas County, Ohio, but the other cities such as Chicago and Cleveland give you a hint of which subset of data to load for the property ownership of these voters.




FIGURE 6-12. Voter details
The chart component (below the tag cloud) in Figure 6-13 shows voter party affiliation for each of the precincts. Notice that the rightmost precinct has a significant number of Libertarians. If you change the chart setting to show all precincts instead of the default of 50, you see an interesting picture in Figure 6-13, which gives you a good view of the voter party affiliation breakout within this county by precinct. Even though the goal is to analyze election anomalies, you are able to obtain an unexpected discovery of the data set that could be used for future campaign initiatives. Also, note that this chart does not show Independent voters. In this data set, the column Party Affiliation is null for the voters who didn't vote in primaries and didn't declare a party affiliation. You'll fix that later in this use case.




FIGURE 6-13. Voter party affiliation
Live Statistics The live statistics records contain information including name, age at death, birthday, and date of death, as shown in Figure 6-14. Note that the Name field is a concatenation of last name and first name divided by a comma. You will use this data set to look for any occurrences of votes being cast from deceased voters by fraudsters.




FIGURE 6-14. Live statistics data set
Property Tax Information Property tax information, as shown in Figure 6-15, can be obtained through the county treasury and assessor office.




FIGURE 6-15. Property tax data set
In addition to the property account number and address, other information in this data set includes first name, last name, tax status, new property status, and tax valuation. You will leverage this data set to look for voters who might have primary residence status in other states outside of the county where they have cast votes in past elections.
Integration
Let's use Endeca Integrator ETL to load the data set into a data domain on the Endeca Server instance called VoterFraud. (You can find details on using Endeca Integrator ETL in Chapter 4.) We will walk you through the necessary steps to design, develop, and run an ETL job within Endeca Integrator ETL for this use case in the following sections.
Step 1: Create a New Project and Configure the Project Workspace You have two options for creating a new project. The first option is to select File | New | CloverETLProject. The New CloverETL Project dialog will open, as shown in Figure 6-16.




FIGURE 6-16. Creating a new CloverETL project
The second option is to duplicate an existing project by right-clicking and using the copy and paste options. As an alternative to duplicating the entire project, you can copy and paste individual components from one project to another. This is the recommended approach for modifying the workspace parameter file. Figure 6-17 displays the contents of the workspace file.




FIGURE 6-17. Workspace settings
In the Server section of the workspace file, you specify parameters for the Endeca Server hostname or IP address, server port, server context, and name of the data domain. As you'll see in the later steps, these project-level variables will be referenced many times in various graph components, and specifying them here avoids hard-coding these parameters in different components.
Step 2: Import Data Sources You can import data sources into an Integrator project in a number of ways. The easiest is to drag a file from its file system location to the data-in folder in the navigation panel on the left side of the screen, as demonstrated in Figure 6-18.




FIGURE 6-18. Importing data sources
Underneath the data-in folder are the data sources included in this project.
Step 3: Create Graphs To create a new graph, right-click the Graph folder in the navigation panel on the left of the screen, select New, and select ETL Graph. A new graph definition window will appear, as shown in Figure 6-19. You can name the graph VotingResults and enter a meaningful description for documentation and reference purposes. Below the Description field, leave the Allow inclusion of parameters from external file? box checked based on the default setting. You can also keep the default location and filename for the workspace parameter file. This will ensure the project variables defined in the workspace file from the previous step can be referenced in the components of this graph.




FIGURE 6-19. Creating a new ETL graph
Alternatively, you can copy and paste existing graph objects from other projects. This is what we will show how to do with the InitDataDomain and ResetDataDomain graphs. You simply copy and paste the same two graph files from the GettingStarted project downloaded from Oracle Technology Network. These graphs and projects have followed the best practice of using global project variables instead of hard-coding server configuration information in the individual components. As a result, you do not have to reconfigure the components to point to the server and data domain for this project. Figure 6-20 shows an overview of the Data Domain Property settings.




FIGURE 6-20. Configuring a graph component
 The InitDataDomain graph, as its name suggests, is the first step in the data loading process. This step checks for the existence of this data domain on the Endeca Server instance and either creates or enables the data domain based on the result of the check (see Figure 6-21).




FIGURE 6-21. InitDataDomain graph design
The ResetDataDomain graph will erase everything in the data domain, including the data and configuration.
The new VotingResults graph will display an empty palette in the middle section of the screen. We will be creating readers, transformers, joiners, and writers in this graph. To create a data reader, you can select a reader type from the component panel on the right side of the project screen. Another method is to simply drag and drop the input file in the Data-in folder onto the graph design palette. Endeca will create a reader automatically based on the input file type. You can use this method to create data readers for each of the four input files in this project. To modify the properties of the reader component, you right-click the reader and select Edit from the pop-up menu. The property window will be displayed for editing purposes.
Metadata is an important component in an ETL project. In a graph design view, the Outline window displayed in the bottom-left corner of the screen contains the Metadata folder. There are a couple of ways to create metadata. One convenient method is to drag an input file in the Data-in folder into the Metadata folder in the Outline window. Endeca Integrator ETL will automatically extract the metadata from the input file that you can further define or modify, as shown in Figure 6-22.




FIGURE 6-22. Extracting metadata
 Numerous metadata files are being created and extracted throughout the graph steps, so it's important to provide a meaningful name for the metadata file each step of the way to avoid complexity in the debugging process.
You can use the Reformat component from the Transformers library, available in the right palette, to create transformations that change the string capitalization, concatenate columns to form full names, and replace null values with default values. When you right-click the Reformat component and select Edit, you will be able to click the Transform button to further define the transformation details, as shown in Figure 6-23.




FIGURE 6-23. Defining transformation details with the Reformat control
The EXT_HASH_JOIN is used to define the join condition of different data sets and map input attributes to output attributes (see Figure 6-24).




FIGURE 6-24. Using EXT_HASH_JOIN control to join data sets
You can choose different join conditions (inner, left outer, and full outer), depending on the requirement and data sets, and you can apply additional transformation as needed through the middle section by selecting from the multitude of built-in functions Endeca provides, or by inputting the condition directly in the script window. Endeca will check the validity of the condition and provide feedback if the syntax entered is incorrect.
The last component of the graph is the Bulk Load to Data Domain control from the Writers library. Figure 6-25 shows the configuration screen when you right-click the control and select Edit.




FIGURE 6-25. Configure Bulk Load to Data Domain control
As mentioned earlier, you specify the project variables in the configuration settings for the server name, port, context, and data domain name. Endeca will extract the variable values defined in the workspace file shown earlier in Figure 6-17 at run time. Figure 6-26 shows the final product of the ETL graph design view.




FIGURE 6-26. VotingResults graph design view
The lines that connect Reader components to the Formatter to Joiner components and eventually to the Writer component are called Edge lines in Endeca Integrator ETL; they are available in the top-right corner in the Palette window. For each of the Edge lines, you can apply metadata by right-clicking the dotted line when the Edge line is initially created. Once a metadata file is selected and applied to the Edge line, the dotted line will turn into a solid line.
You can use two useful tools to help you debug the ETL process: debug mode and the Trash component. The bug icon on the Edge line indicates that the step is run in debug mode. This can be activated by right-clicking the Edge line and selecting Enable Debug. With debug mode enabled, you can view the data output for that step during run time. The Trash component is also useful for debugging during ETL development. It simply replaces the writer and allows you to view the output result during the design and testing stages. Please note that the unused components on the ETL graph need to be disabled during the test runs.
 The small numbers displayed in the top-left corner of the controls are to specify the run sequences. These numbers ensure the dependent steps are completed before the next steps can be run.
Step 4: Run Graphs You'll want to run the graphs in the following order:
    1.    Initialize Data Domain (provisioning a new data domain, if necessary)
    2.    Reset Data Domain (clearing all data, schema, and config from the data domain)
    3.    Load Voting Data (loading voting data into the data domain)
The run time information will be displayed in the console window below the graph designer, as shown in Figures 6-27 and 6-28.




FIGURE 6-27. Run time output for InitDataDomain graph




FIGURE 6-28. Run time output for VotingResults graph
As the integrated voter data is loaded into the Endeca Server VoterFraud data domain, you can now create a new application in Endeca Studio.
Voter Fraud Application Walk-Through
You are now back at the Endeca Studio interface to create a new application called Voter Fraud Analysis. Instead of using the Upload a File option, you can choose the Use a Pre-built Endeca Server and select the VoterFraud data domain in the drop-down list. Endeca Studio will create a default application based on the data source selected.
You can configure the same bar chart as shown in Figure 6-13, but you get a very different view in Figure 6-29 when converting the null value in the party affiliation column to "I" (for "Independents") through the transformation function within Endeca Integrator ETL. You can also accomplish this through the Provisioning Service in Endeca Studio.




FIGURE 6-29. New party affiliation spread with Independents
Back to the fraud analysis: You can create a new chart to display out-of-state property and death record matches, as shown in Figure 6-30.




FIGURE 6-30. Chart configuration for possible match
First, you use Record Count as the Value axis and AddrLine for the Category axis. The AddrLine attribute is from the property tax data source. You use a left-outer join in the joiner  component in Endeca Integrator ETL so that all the voter records will be preserved, but only those property records that have a match in the voter record set will be included. This behaves much like a left-outer join in a traditional ANSI SQL statement. You also choose FullName in the Color selection to enable the full names being displayed on the chart, along with their out-of-state property addresses. Figure 6-31 displays the individuals who have indicated having residential addresses in one of the precincts of the county you are examining and have out-of-state properties claimed as primary residence in their tax records. The fact that these registered voters have out-of-state properties does not automatically lead to fraud. However, it provides a direction for further investigation.




FIGURE 6-31. Out-of-state property matches
Next, take a look at the record match from the live statistics data set. You can simply use the same chart and change the dimension selection to Death Date. Since there are quite a number of deceased records matching your voter data, you apply the precinct filter to narrow down the list to three, as shown in Figure 6-32. The precinct you have chosen is from the hypothetical discovery in the earlier section of this chapter, where more voters have been reported than the total number of registered voters.




FIGURE 6-32. Deceased records matches
Out of the three records, one individual passed away in February 2014. The elections we are investigating occurred prior to the date, so you can focus your attention on the other two individuals. Click the record of one of the individuals in the chart. Endeca Studio automatically adds the filter to all the components on the page, as shown in Figure 6-33.




FIGURE 6-33. Deceased person voting
As you can see in the result table and chart of Figure 6-33, this individual passed away in 2006 but continued to vote in the recent elections, including 2010, 2011, 2012, and 2013.
Summary
In this chapter, we walked you through how to use Endeca to develop a fraud analysis application. We hope you have a better understanding of some of the features and capabilities of Endeca and how to apply them in real-world analytical applications.
In the first section of the use case, the end user can leverage Endeca's self-service functions, with agile capabilities to combine data and mash it up without any scripting or IT help. As we demonstrated in this chapter, Endeca Studio provides strong and powerful discovery dashboards that are easy to create, modify, and use. Endeca Studio has a rich set of discovery components and visualization, including world-class search, guided navigation, and filtering that updates every component on the page, all integrated together. Endeca Integrator ETL is an enterprise-scale ETL tool that is easy to use with powerful features. The in-memory architecture of Endeca Server enables speed-of-thought analysis, as you probably have noticed throughout the use case development.
In the next chapter, we cover additional and more sophisticated features of Endeca in regard to the healthcare correlation use case. We cover features including the map component, EQL, integration with OBIEE, and Oracle Advanced Analytics.










CHAPTER7
Implementing Healthcare Correlations
We are what we repeatedly do. Excellence, then, is not an act, but a habit.
—Aristotle
Greek philosopher Aristotle's quote is astoundingly relevant today. Excellence in business decision making is not easy to establish. Business leaders are looking for new ways to build the organizational competency that propels their employees to explore through ambiguity and to use information effectively for operational decisions. Decision making occurs at every level of an organization. Knowledge workers at those different levels need to improve at solving problems, be more proactive in detecting potential issues or concerns, and be more efficient at finding opportunities for improvement. Essentially, an organization needs to foster a habit of information exploration that creates a learning organization. It needs to be one that is vibrant and always exploring and building new knowledge. It needs to be one that never takes anything for granted. It needs to be one that never settles into the conventional wisdom and never says "we know it all."
This habit of information exploration is particularly important for healthcare, an industry that is undergoing significant change. The cost of healthcare in our country has been rising quickly and is not sustainable for the longer term. Meanwhile, access to care is becoming more challenging because of the ever-growing aging population. These factors drive the need for innovative ways to approach our healthcare system. Over the last few years there has been increasing investment in electronic healthcare technologies that hold the promise to improve care delivery. In addition, we are seeing movement from an episodic-based system to a longitudinal-organized system that delivers long-term value.
Healthcare systems today require deeper collaboration among traditional healthcare providers, pharmaceutical researchers, and academic medical centers. The convergence between healthcare and life sciences is breaking down the traditional walls through interoperability that allows for better decision making. Ultimately, it is about improving outcomes and driving down the cost of care.
Most healthcare organizations, as a result, are asking themselves whether they have the right technology and the right road map to meet their immediate challenges, as well as to build a platform for the future. Analytics plays a key role in enabling this road map. The use case for this chapter will take a look at the evolution of healthcare analytics and how you can use Oracle Endeca Information Discovery throughout different stages of this journey.
This chapter is organized into the following three parts:
        Use case overview Background and overview of the healthcare use case in the context of the Healthcare Analytics Capability Maturity Model
        Use case implementation Detailed implementation steps
        Use case summary Summary of the use case and how it is implemented in Endeca
Healthcare Analytics Use Case Overview
Most healthcare organizations are faced with disparate data sources, including electronic medical records (EMRs) such as patient records, procedures, medications, and labs; departmental research databases; clinical data warehouses; medical ontology; and significant unutilized, unstructured content, including doctor's notes, tests, and results summaries. Their analytical requirements range from measuring required key performance indicators (KPIs), including readmission rates, mortality rates, hospital-acquired infections, and surgical care improvement, to answering new questions such as the following: What does our patient population look like? What is the geographical distribution of patients and clinics? Where are we using high-cost medications? Are there correlations with procedures and nursing facilities for re-admission?
It's a long journey, and it takes continuous improvement to become an analytics-driven organization. Many healthcare organizations are at different stages in terms of their analytical capabilities. Their requirements and business priorities will also play a role in determining what capabilities are needed and how to lay out a strategic road map. To put things in context, let's first take a look at the Healthcare Analytics Capability Maturity Model, including the different stages of analytical capabilities, as well as the characteristics of these stages.
Healthcare Analytics Capability Maturity Model
The Healthcare Analytics Capability Maturity Model (HACMM) can be categorized into the following six stages:
        Level 0: Fragmented
        Level 1: Managed
        Level 2: Systematic
        Level 3: Advanced
        Level 4: Optimized
        Level 5: Innovative
Figure 7-1 is a visual representation of this maturity model.




FIGURE 7-1. The Healthcare Analytics Capability Maturity Model (HACMM)
In the next section, we'll describe each of these stages through the following angles: definition, characteristics, incorporation of data sources, architecture, infrastructure, and people and skills, as well as state of governance.
Level 0: Fragmented
Definition:
        Level 0 is defined by fragmented point solutions with inefficient and inconsistent versions of the truth.
Characteristics:
        The main characteristic of this stage is the multitude of vendor-based and internally developed applications that are used to address specific analytic needs as they arise.
Data Sources:
        Overlapping data content leads to multiple versions of analytic truth.
Architecture:
        The fragmented point solutions are neither colocated in a data warehouse nor architecturally integrated with one another.
Infrastructure:
        Technologies and tools are informal and not standardized. Decisions are heavily influenced by project team and/or technology-savvy individuals, instead of based on a set of commonly agreed upon guiding principles.
Organization and People:
        Reports are labor intensive and inconsistent.
        Analysts spend the majority of their time assembling reports and cross-checking numbers to avoid mistakes and correct data quality issues manually.
        Analytical skills are spread throughout different parts of the organization. Skill levels are inconsistent. The sharing of expertise and best practices is rare and ad hoc.
Governance:
        Data governance is limited or nonexistent.
        Analytical and data standards do not exist.
Level 1: Managed
Definition:
        Level 1 is identified by the existence of an enterprise data warehouse as the foundation of data and technology.
Characteristics:
        A searchable metadata repository is available across the enterprise.
        The data warehouse is updated within one month of source system changes.
Data Sources:
        The data (including EMR data, revenue cycle, and financial data) is colocated in a single data warehouse.
        Data content might include insurance claims.
Architecture:
        Analytical master data capabilities are established for patient indexes, facilities, and providers.
        The main focus of integration capability is ETL tools for enterprise data warehouse data loading needs.
        Metadata is managed through the combination of data dictionary of the data warehouse and ETL tool repository, as well as the BI metadata repository.
Infrastructure:
        The organization has started to set enterprise-wide standards for information management technologies.
        The IT organization starts to look into the consolidation and optimization of technology portfolios around business intelligence, data warehouse, and integration tools.
Organization and People:
        The enterprise data warehouse team reports organizationally to the global CIO instead of regional or lines of business IT.
        A BI Center of Excellence starts to form in some organizations and crosses lines of businesses to share best practices and establish standards.
Governance:
        Data governance is forming around the data quality of source systems.
        Functional data stewardship is taking shape.
Level 2: Systematic
Definition:
        Level 2 is defined by a standardized vocabulary across the enterprise, and patient registries move from analytical only to focus on the operational level.
Characteristics:
        A master vocabulary with reference data is identified and standardized across disparate source system content in the data warehouse.
        Naming, definitions, and data types are consistent with local standards.
Data Sources:
        Patient registries are defined and integrated at the operational level.
        Additional data sources are incorporated into the enterprise data warehouse, including the costing model, supply chain information, and patient experience.
Architecture:
        The analytic objective is focused on consistent and efficient production of reports supporting basic management and operations of the healthcare organization.
        Key performance indicators are easily accessible from the executive level to the frontline managers and knowledge workers.
Infrastructure:
        Information management technology services are optimized and consolidated with other infrastructure and application technology services into enterprise-wide, holistic, and integrated enabling services.
Organization and People:
        Corporate and business unit data analysts meet regularly to collaborate and prioritize new features for the enterprise data warehouse.
Governance:
        Data governance forms around the definition and evolution of patient registries and other master data management including providers, facilities, and employees.
        Data governance expands to raise the data literacy of the organization and develop a data acquisition strategy for additional data sources.
Level 3: Advanced
Definition:
        Level 3 is defined by automated external reporting with efficient, consistent, and agile production.
Characteristics:
        The main characteristic of this level is manifested in the reduction of waste and patient care variability.
        Analytical objectives include measuring and managing evidence-based care and clinical effectiveness.
        There is more focus toward consistent and efficient production of reports required for regulatory and accreditation requirements, payer incentives, and specialty society databases with adherence to industry-standard vocabularies.
        Population-based analytics are used to suggest improvements to individual patient care.
Data Sources:
        Clinical text data content is available for simple keyword searches.
        The precision of registries is improved by including data from lab, pharmacy, and clinical observations in the definition of the patient cohorts.
        Data content expands to include insurance claims (if not already included) and health insurance exchange data feeds.
Architecture:
        Enterprise data warehouse content is organized into evidence-based and standardized data marts that combine clinical and cost data associated with patient registries.
        On average, the enterprise data warehouse is updated within one day of source system changes.
Infrastructure:
        There is integration with existing systems across LOBs, virtualization, and user-centric tools.
Organization and People:
        Permanent multidisciplinary teams are in place that continuously monitor opportunities to improve quality as well as reduce risk and cost across different business processes, including acute-care processes, chronic diseases, patient safety scenarios, and other internal workflows.
Governance:
        Centralized data governance exists for the review and approval of externally released data.
        Data governance expands to support care management teams that are focused on improving the health of patient populations.
Level 4: Optimized
Definition:
        The key marker for level 4 of healthcare analytical maturity is population health management and suggestive analytics to improve clinical risk intervention.
Characteristics:
        The accountable-care organization shares in the financial risk and reward that is tied to clinical outcomes.
        Analytics data is available at the point of care to support the objectives of maximizing the quality of individual patient care, population management, and the economics of care.
        Analytic focus expands to address the diagnosis-based and fixed-fee per-capita reimbursement models.
        Patients are flagged in registries who are unable or unwilling to participate in care protocols.
Data Sources:
        Data content expands to include bedside devices, external pharmacy data, and detailed activity-based costing.
        Data content expands to include home monitoring data, long-term care facility data, and protocol-specific patient-reported outcomes.
Architecture:
        On average, the enterprise data warehouse is updated multiple times per day as the source system data changes.
        Data services are defined, built, managed, and published to provide consistent service-based data management capabilities.
Infrastructure:
        Infrastructure capabilities focus on interoperability and service-oriented architecture.
        There is a balance of centralized services versus local autonomy to achieve an economy of scale and agility.
Organization and People:
        The enterprise data warehouse and analytical teams report organizationally to a C-level executive who is accountable for balancing the cost of care with the quality of care.
Governance:
        Data governance plays a major role in the accuracy of metrics supporting quality-based compensation plans for clinicians and executives.
Level 5: Innovative
Definition:
        Level 5 of the analytical maturity model is defined in terms of healthcare advances in personalized medicine with mature and governed prescriptive analytics capabilities.
Characteristics:
        Analytic objectives expand to wellness management, physical and behavioral functional health, and mass customization of care.
Data Sources:
        Data content expands to include 24/7 biometrics data, genomic data, and familial data.
Architecture:
        Analytics expands to include the natural language processing (NLP) of text, prescriptive analytics, and interventional decision support.
        Prescriptive analytics are available at the point of care to improve patient-specific outcomes based upon population outcomes.
        The enterprise data warehouse is updated in real time to reflect changes in the source systems.
Infrastructure:
        The information architecture group actively reviews, evaluates technology trends, and jointly works with the business to adopt new technologies and tools in evaluating the effectiveness of initiatives such as creating new business models and driving new business capabilities.
Organization and People:
        Physicians, hospitals, employers, payers, and patients collaborate to share risk and reward (for example, financial reward to patients for healthy behavior).
Governance:
        Governance councils, information owners, and data stewards have optimized processes in resolving problems pertaining to cross-functional information management issues.
        Best practices are identified, documented, and communicated.
        The information architecture team ensures that these best practices are extended across the enterprise through reference architecture development and sharing, as well as architecture review activities.
Use Case Implementation
The purpose of describing this maturity model is to provide a framework to guide organizations in establishing analytical capabilities in an incremental manner. It is also a good way to organize the use case within such a context.
The sample application in this chapter is intended to show how you can use Endeca to advance analytical capabilities in different stages of the maturity journey. The application is composed of the following function areas:
        Claims analysis
        Patients analysis
        Operations analysis
        Partners
        Clinical research
        Remote monitoring
Figure 7-2 shows an overview of the application. Each of the functional areas is in its own page.




FIGURE 7-2. Overview of healthcare analytics application
 We will discuss architecture characteristics of each stage for the healthcare analytics application and use one example in these functional areas to highlight some of the Endeca features. The tab that is selected in Figure 7-2 is the Patients Analysis component. We'll cover each of these subject areas in the next sections of this chapter.
Claims Analysis
Healthcare companies in the early stages of analytical capability face tremendous challenge. It takes time and effort to build an enterprise data warehouse. Meanwhile, the business urgently needs better data for improved decision making. In the case where an enterprise data warehouse is not yet in place, Endeca can be used as a starting point to integrate different data sources for analysis with quick time to value.
In the Claims Analysis function of the case study, you will integrate three data sources: claims, labs, and drug information. In this example, the claims data has been enriched with an external medical thesaurus with detailed procedure definitions and condition descriptions based on the IDC diagnostic code and procedure code. Use Endeca Integrator ETL to integrate these data sources into an Endeca data domain called HealthCare.
We covered Endeca Integrator ETL in Chapters 4 and 6, so we won't be going over the details of how to develop and configure this ETL graph. Figures 7-3 and 7-4 show the final configuration of the Endeca Integrator ETL graph and the run statistics.




FIGURE 7-3. Claims Analysis graph




FIGURE 7-4. Claims Analysis run statistics
 To follow along, create an application using a prebuilt Endeca Server option in Endeca Studio. A default two-column application will be created based on the data, as shown in various examples in the past chapters. You can then organize the refinement area based on the attribute groups, as shown in Figure 7-5.




FIGURE 7-5. Claims Analysis refinement groups
Attribute groups are a good way to organize attributes and can be defined in Application Settings or by preloading configuration files via Integrator.
 In the top portion of the right panel, you create a tag cloud group by adding a new Component Container with a two-column (50/50) configuration, as shown in Figure 7-6.




FIGURE 7-6. Adding a Component Container
You then add two tag clouds, one for condition description and one for procedure description. This allows a side-by-side comparison of the most common conditions with the most performed procedures, as shown in Figure 7-7.




FIGURE 7-7. Condition and procedure description comparison
You also create a chart group to display different breakdowns for user-driven analysis, as shown in Figure 7-8.




FIGURE 7-8. Charts for user-driven analysis
The charts are configured to allow analysts to select any dimensions to understand the breakdowns such as age at first claim, places served, drug count, lab count, length of stay, and so on. Users can drill into these specific areas to refine selections and compare attributes to understand the historic trends of claims.
 Last but not least, a results table is configured to allow analysts to examine the records in detail as needed. As you'll notice in Figure 7-9, the results table is automatically configured based on the same attribute groups defined and displayed in the Available Refinements section shown earlier in Figure 7-5. It allows for ease of analysis and enhances productivities for data analysts.




FIGURE 7-9. Claims results table
 You can complete an application like this in Endeca in a matter of hours or days instead of weeks or months using a traditional BI approach. It allows for quick time to value in the early stages of the analytical journey.
Patients Analysis
Master data management is one of the key indicators that an organization is moving toward maturity in its information architecture. The intention of this book is not to teach you how to develop a patient master. Rather, we will show you how to integrate Endeca with master data hubs such as a patient registry.
 As you might have noticed in the "Claims Analysis" section, you haven't yet included detailed patient information. You have joined claims, labs, and drug information by matching on the patient ID. However, there are different patient attributes in each of these data sets that are not aligned or standardized. Let's assume the fictitious healthcare provider company has already established a master patient index as it moves into the second stage of the maturity curve. Instead of reinventing the wheel, you will simply leverage the existing master data hub and incorporate the patient registry as a data source in Endeca.
A typical hub-and-spoke master data architecture includes a master data hub, integration architecture, data quality management, data stewardship functions, and data services that subscribe and publish master data into various sources and consumer applications. These master data services will publish mastered patient data to Endeca like they would for other data consumers. For ease of demonstration purposes, we will be loading the extracted patient registry file into Endeca. Once you have a data set, you can create a new tab in the application called Patients Analysis. The patient master data is rather wide and contains many attributes that you do not need for your analysis. So, you'll modify the view definition to include only the attributes of interest, as shown in Figure 7-10. Another option in lieu of an EQL filter is to load only the data needed for analysis. This is a more recommended approach because it will reduce the size of the index and improve overall performance. Here we are assuming that some of the attributes are not needed for this particular analysis but might be useful for future use.




FIGURE 7-10. EQL for new view definition
Next you'll configure a map view to display a geographical patient population breakout. First you add a new Map component; then you click the Options button to configure the Map component. Click Patient Master as the data source, and a list of views with geo data type attributes will be displayed for selection. You can choose New Patient Master on the Data Selection tab, Heat Layer on the Layer Type tab, and geo_code as the Points Definition setting. The Layer Property tab allows you to define the layer name, the geo filtering attribute, the size of points, color options, and heat options. You can leave the rest of the configurations with their default settings.
The fictitious healthcare organization used in this case study is global in nature. The default map view gives you a good sense of patient distribution across the globe, as shown in Figure 7-11.




FIGURE 7-11. Map view of patients' concentration globally
 One of the goals of this analysis is to understand whether the current clinic locations and numbers are sufficient to service the ever-changing patient population. You will start the analysis with the city of Chicago.
First, you upload a new data source that contains clinic locations in the Chicago area. Figure 7-12 displays the attribute definition of this data source.




FIGURE 7-12. Health clinic data source overview
 Once you have the new data set for clinical locations loaded, you go back to the Studio page/tab for patients analysis. In the Available Refinements section on the left, select Chicago and submit the selection. The map automatically zooms into the Chicago area. You then click the Options button for the Map Component again to modify the configuration. On the Map Layer definition screen, click the check box before Health Clinic to include it in the map view. Select Numbered Points for Layer Type and Location as the geo attribute. On the Details Templates tab, you replace Record ID with the Site Name attribute so that the clinic name will be displayed instead of a numeric ID. Finally, keep the default settings for the rest of the configuration screens, save the changes, and exit the configuration screen.
Figure 7-13 gives you a clear view that certain areas such as downtown Chicago and the northwest and southeast suburbs lack sufficient coverage of health clinics compared to the patient density in those areas.




FIGURE 7-13. Health clinic locations compared to patient density in Chicago
To summarize this section, you are able to use Endeca to consume patient master information and combine it with additional data sources such as clinic and hospital locations for quick and effective analysis of patient population compared to available healthcare facilities.
Operations Analysis
As organizations continue to evolve their analytical capabilities, they will most likely implement multiple business intelligence (BI) tools as a data consumption layer for the enterprise data warehouse.
For organizations that have already implemented various dashboards and reports with a standard BI tool such as Oracle Business Intelligence Enterprise Edition (OBIEE), Endeca allows you to define an iFrame that could link to an existing OBIEE dashboard. Figure 7-14 displays the configuration in Endeca. (We have obfuscated the URL field for security reasons.)




FIGURE 7-14. iFrame component configuration
Figure 7-15 shows an embedded OBIEE dashboard for profitability analysis. All OBIEE controls are available within the Endeca frame based on security and permission settings for the given user. This feature of Endeca enables the seamless integration of data analysis across different analytical platforms and allows an organization to maximize its existing investment in BI and departmental analytical solutions without having to reinvent the wheel.




FIGURE 7-15. Embedded OBIEE profitability analysis dashboard
Partners
In the previous section on operations analysis, we showed you Endeca's ability of integrating analytics through the consumption layer. Integration with an existing BI tool can also occur at the metadata layer. If you don't want to display a prebuilt report or dashboard in your Endeca application, you can extract data through an OBIEE metadata repository (or Common Enterprise Information Model). This method ensures the sharing of common definitions between these analytical tools. Next, we will show the step-by-step instructions on how to extract partner information from OBIEE subject areas.
In the Control Panel, go to Data Source Library under the Information Discovery section. Click the + New Data Source button. In the Define Connection section, select Oracle BI. Enter a data source name, description, and connection information accordingly, as shown in Figure 7-16.




FIGURE 7-16. Adding a new Oracle BI data source
 Click the Next button. Endeca will connect to the OBIEE metadata repository and prepopulate the Subject Area drop-down list with available subjects based on the privilege of the login user account you have provided. Select the subject area on the next screen, as shown in Figure 7-17.




FIGURE 7-17. Selecting the subject area
Based on the selected subject, Endeca will again read the OBIEE repository and populate the list of available presentation tables. Select one of the presentation tables in the list. In this case, you can select the Partner table (see Figure 7-18).




FIGURE 7-18. Selecting a presentation table
Click Next and Finish, and the newly defined OBIEE connection will be displayed in the data library list, as shown in Figure 7-19.




FIGURE 7-19. New OBIEE connection available
 Now you can create a new data set in the healthcare analytics application. As you will notice in Figure 7-20, a new option is now available—Load Data from a Database.




FIGURE 7-20. Creating a new data set using an OBIEE data source
Endeca will promote user authentication information, as shown in Figure 7-21, to ensure the application user has the proper credentials to access this data source.




FIGURE 7-21. Authentication screen to connect to OBIEE
The data set will be imported from the underlying data warehouse through the OBIEE metadata repository for further definition, as shown in Figure 7-22. The steps beyond this are similar to importing data from an Excel or JSON file.




FIGURE 7-22. Attribute definition imported from OBIEE
 Once the data set is loaded, the Partner screen is now ready for use, as shown in Figure 7-23.




FIGURE 7-23. Partner screen
This capability of Endeca is unique and valuable. It enables an organization to share metadata across various analytical tools to enforce data standardization and governance. In addition, it allows for quick time to value for extending analytical capabilities.
Clinical Research
Many healthcare organizations are using analytics to enhance and facilitate clinical research as they continue to mature their analytical capabilities. In this section of the use case, we'll demonstrate how to integrate Endeca with another product within Oracle's analytical portfolio, called Oracle Data Mining (ODM) for predictive analytics. ODM is part of the Oracle Advanced Analytics (OAA) option for Oracle databases. The other component of OAA is Oracle R Enterprise (ORE). Oracle Big Data Handbook, published by Oracle Press, covers these two products in more detail.
The data source for this use case is based on lung cancer surgical results. Figure 7-24 shows a snippet of the data.




FIGURE 7-24. Lung cancer surgery data set
You load this data set into an Oracle database and use Oracle Data Mining to generate a statistical model that can be used to predict the potential outcome for future patients. You first define and run a data exploration workflow in ODM, as shown in Figure 7-25.




FIGURE 7-25. ODM data exploration workflow
 ODM outputs a screen that allows you to explore the correlation of each of the attributes with the defined outcome, which is the one-year survival in this case, as shown in Figure 7-26.




FIGURE 7-26. ODM data exploration for initial correlation
This capability allows a data scientist to gain an initial understanding of the data set at hand and define a modeling strategy. Next you create and run a class build component in ODM workflow, as shown in Figure 7-27.




FIGURE 7-27. ODM Class Build workflow
The output of the ODM Class Build workflow is the comparison window in Figure 7-28. It provides details pertaining to overall accuracies, confidence levels, and cost models for each of the algorithms, allowing a data scientist to compare and choose the best algorithm.




FIGURE 7-28. Class Build comparison
Based on this comparison information, we decide to use Support Vector Machine (SVM) for our modeling. More information about the SVM and other classification algorithms can be found at the Oracle Technology Network Advance Analytics site as well as the Oracle documentation called "Oracle Data Mining Concepts." Figure 7-29 shows the final workflow configuration.




FIGURE 7-29. ODM workflow final design
The output of the analytical model can be consumed within Endeca, just like any other data source. Clinical research analysts can consume the predictive analytics output within Endeca to compare historic trends and the patient profile. They can slice and dice the data by selecting different attributes within the Endeca chart components, as shown in Figures 7-30 and 7-31.




FIGURE 7-30. Survival prediction analysis




FIGURE 7-31. Refinement with range values
 In this case, Endeca is an effective tool to incorporate predictive analytics that allows you to determine the effectiveness of different treatment options for different patient populations, provide a basis for continuous clinical improvements, and eventually advance patient care outcome.
Remote Monitoring
Parkinson's disease (PD) is one of the most devastating degenerative disorders of the central nervous system. Caring for PD is an ongoing process. Doctors may recommend regular follow-up appointments with neurologists trained in movement disorders to evaluate patients' condition and symptoms over time and track the disease progression. Traditional tests and measures require frequent onsite visits and are costly and difficult to implement for the long term.
Various research studies are in place to better diagnose, manage, and eventually conquer this disease. Scientists looking for the cause of PD continue to search for possible environmental factors, such as toxins, that may trigger the disorder and to study genetic factors to determine how defective genes play a role. Other scientists are working to develop new protective drugs that can delay, prevent, or reverse the disease. One such research project is called Parkinson's Voice Initiative (PVI). Led by a few research specialists at the University of Oxford, the PVI team has developed methods for detecting and tracking Parkinson's disease progression from voice recordings. In a published paper, they were able to demonstrate the ability to perform Unified Parkinson's Disease Rating Scale (UPDRS) assessment remotely using self-administered and noninvasive voice recordings. This method has the potential to significantly cut down on costs for ongoing symptom tracking, and it promises the feasibility of frequent, remote, and accurate UPDRS tracking.
 In the Remote Monitoring portion of the sample healthcare application, you can set up Endeca to consume the telemetry data set from the Parkinson's disease voice recording captured through the At-Home-Testing-Device (AHTD). Here's a high-level overview of the data capturing, transmitting, and consumption process:
    1.    Parkinson's patients speak into the microphone at their homes.
    2.    At-Home-Testing-Device (AHTD) records speech.
    3.    The speech signals are transmitted to a dedicated clinic server hosted in the medical center via the Internet.
    4.    Speech signals are processed and mapped to UPDRS.
    5.    Predicated UPDRS results, as well as voice recording details, are imported into Endeca.
 As always, Endeca creates an application based on the data with the default two-column display. You can add a new Summarization Bar component on the top of the screen and define two flags and three summary metrics, as shown in Figure 7-32.




FIGURE 7-32. Patient monitoring Summary Items list
Figures 7-33 and 7-34 show the definition screens for UPDRS Decline flag and Average UPDRS metric, respectively.




FIGURE 7-33. Patient remote monitoring flag definition




FIGURE 7-34. Patient remote monitoring metric definition
Figure 7-35 is sample output for one of the Parkinson's disease patients being monitoring remotely.




FIGURE 7-35. Patient remote monitoring output screen
When a clinic staff clicks the flag, more details will be displayed, as shown in Figure 7-36.




FIGURE 7-36. Patient remote monitoring flag details
 The Actions section in the definition screen, as shown in Figure 7-34, allows you to define a URL for an application that this measure can link to, including URL parameters. It could launch the patient care system with a click of the metric in the Summary Bar, for example, which allows clinical staff to navigate into the clinical management and operational systems for further actions.
With the ever-growing "Internet-of-Things," remote patient monitoring is becoming more and more economical and effective. Endeca is an excellent tool to consume the telemetry and biometric data from home-monitoring devices and allows clinical staff members to manage the care of their patients through an integrated and holistic view.
Summary
In this chapter, we described the Healthcare Analytics Capability Maturity Model. We then walked you through how you can leverage Oracle Endeca Information Discovery to enable a healthcare organization in different stages of the analytical capabilities:
        Level 0: Fragmented How to create an analytical data mart for claims analysis
        Level 1: Managed How to integrate patient master data into Endeca, overlaying it with clinic location data to analyze patient population versus available healthcare facilities
        Level 2: Systematic How to consume an OBIEE dashboard within Endeca for operational analysis
        Level 3: Advanced How to extract data from an OBIEE repository for partner analysis
        Level 4: Optimized How to integrate with ODM (advanced analytics) for clinical research and predictive analysis
        Level 5: Innovative How to analyze and monitor remote patient biometric/telemetry data for real-time and personalized care with cost effectiveness
This step-by-step use case was intended to demonstrate how Endeca can be used as a vehicle to propel an organization to the next level of analytical maturity and eventually reach excellence in healthcare analytics through making information exploration a habit.
As we've shown, you can leverage Endeca in every aspect of the analytical and clinical functional areas for every stage of the analytical journey. Endeca greatly enhances the decision-making capabilities because of its ease of use, its rich self-service capabilities, its flexibility in integrating with existing BI and data warehouse architecture, and, most importantly, its quick time to value.








CHAPTER8
Industry Best Practices
The greatest enemy of knowledge is not ignorance, it is the illusion of knowledge.
—Daniel J. Boorstin
In his book The Discoverers: A History of Man's Search to Know His World and Himself, Daniel Boorstin documented in great detail the history of human discovery through a series of 82 essays. It was in that book that Boorstin wrote this famous line, often misattributed to Stephen Hawking, that the great menace to progress is not ignorance but the illusion of knowledge.
More data will lead to more correlations. Some of the correlations might be legitimate, but some might be noise. In addition, statistical modeling and analytical outcome will be consumed by analysts at different levels of the organization. How well can these knowledge workers interpret analytical results? How valid are the analytical models? How much confidence do you have in your business decision-making process, based on the quality and completeness of your data, and the analytical outcome? Which decisions can you automate, and which decisions need to be evaluated by humans?
The focus of this book is on Oracle Endeca Information Discovery. However, analytics is a broader topic and covers more areas and concepts. In this chapter, we will first discuss best practices for developing analytical capabilities in an organization. We'll then cover the best practices for implementing data discovery capabilities in general as well as with Oracle Endeca Information Discovery. Finally, we'll summarize with a look into the future of business analytics and data discovery.
Developing Analytical Capabilities
The IT industry has seen many evolutions, and it is in the midst of another major paradigm shift. Few technologies have captured more attention than big data, and there is tremendous interest in business use cases featuring big data and analytics. Gartner highlighted the top ten technologies and trends that will be strategic for most organizations in 2013. Strategic big data and actionable analytics were among these ten trends. In 2014, Gartner released its top ten IT trends again. This time, the list included mobile, Internet of Things, and smart machines. Big data and analytics become enablers—a hidden force that's behind the scenes driving these businesses and IT innovations.
New devices and new data sources introduce new technologies. One thing that we all have learned through various paradigm shifts is that new tools rarely replace old tools. Over time, the IT landscape becomes more complex and multifaceted. As a result, big data and analytics initiatives benefit from an enterprise architecture (EA) approach to ensure alignment and success. It's a delicate balance between strategic direction versus exploratory initiatives. The traditional EA approach tends to focus on the long-term vision, whereas Oracle applies an adaptable and flexible approach to enterprise architecture. Referred to as the Oracle Enterprise Architecture Framework (OEAF), it includes a "just enough, just in time" process for architecture development, called the Oracle Architecture Development Process (OADP), as shown in Figure 8-1. This methodology ensures that the appropriate guidelines are established during the planning and architectural phases.




Figure 8-1. OEAF and OADP
Information architecture is one of the four domains of OEAF. The focus of this layer is to manage the architecture of various data assets of an enterprise, including the types of data, the business processes they support, and the technologies, people, and information processes needed to enable the effective use of these assets. Figure 8-2 is a high-level view of Oracle's Information Architecture Framework. It's composed of data realms and management capabilities.




FIGURE 8-2. Oracle Information Architecture Framework
 Data realms are the black slices in the middle of the diagram in Figure 8-2. They are the different types and classifications of data, including master data, reference data, metadata, transactions, analytical data, documents and content, and big data. The second element, which is depicted at the outside circle of this diagram, includes the capabilities you need to manage the different aspects of the varieties of these data types and classifications.
 There are nine level-1 management capabilities. They include data integration, infrastructure management, data sharing and distribution, business intelligence and data warehousing, data governance and metadata management, data security, master data management, information sharing and delivery, and the enterprise data model. We'll provide links in the appendix where you can find out more details about these capabilities, maturity model, and architecture development process.
The rationale of starting with this capability model is that it's important to avoid looking at big data and new analytical initiatives as singular entities or endpoints. Rather, you need to find out how it fits into your existing information architecture to be able to build new capabilities incrementally and to maximize your existing investment. Figure 8-3 highlights the overall process to develop an analytical architecture.




FIGURE 8-3. Architecture development process for business analytics
The six main steps of the architecture development approach are as follows:
    1.    Align with the business architecture.
    2.    Define the architecture vision.
    3.    Assess the current-state architecture.
    4.    Establish the future-state architecture.
    5.    Determine the strategic road map.
    6.    Implement governance.
We will cover each of these steps next.
Align with the Business Architecture
A number of key business strategy requirements drive the needs for business analytics. Companies are investing in business analytics to enhance business competitiveness, enable fast-paced business processes, centralize business control, improve operational efficiency, support compliance and reorganizations, and facilitate mergers and acquisitions. It's critical to understand and align with main priorities and what's driving your organization.
The business operating model is a key element of business architecture. It is used to define the degree of business process standardization versus the degree of business process and data integration. The four types of operating models identified in Enterprise Architecture As Strategy: Creating a Foundation for Business Execution (Harvard Business Review Press, 2006) are coordinated, diversified, unified, and replicated.
It is important to understand an organization's current- and future-state operating models in order to support analytical investment decision making and align architecture recommendations with the operational characteristics of the business. For example, a coordinated operating model might imply a higher value in an investment in analytics as a service than investment in consolidated storage hardware platforms. Similarly, a diversified operating model has low standardization and data integration. It enables maximum agility in the business. Companies moving from a replicated operating model to a unified model will require significant change in breaking down the data-sharing barrier and thus require enhanced data integration capabilities. Understanding the overall direction will help you define an analytical architecture and implement analytical solutions that enable and facilitate business transformation.
Business processes are another component of the business architecture. We are not suggesting that you develop a full-blown business architecture including a detailed business process mapping before you start any analytical initiative. However, it's important to understand your data assets in the context of your business processes, what processes they support, how important these processes are to the overall business success, how critical these data sets are in support of these processes, and what the risks are of data breaching or the data set becoming unavailable. Developing a high-level data asset rationalization in the context of key business processes will ensure efficiency in the overall analytical solution.
Define the Architecture Vision
Principles define the guiding framework for architecture. Analytical initiatives must comply with established architectural principles in organizations, such as leveraging open standards. Here are some common guiding principles with regard to analytical architecture.
Alignment with Overall Information Architecture
The first guiding principle is the need to align with the overall information architecture.
Statement The analytical solution needs to be in alignment with the enterprise information architecture.
Rationale The value of big data is maximized when it can be correlated with existing enterprise information and business processes.
Implications Here are the implications:
        Appropriate technology, process, and people are needed to correlate and integrate big data analytic results with enterprise information, applications, and business processes.
        Conduct high-level data and technology rationalization in the overall information architecture to avoid big data and analytical silos.
Approachable Analytics
Our second guiding principle is around analytics.
Statement Information and analysis must be available to all users, processes, and applications across the organization that can benefit from it. The reach of decision-making analysis must expand to include all knowledge workers in the organization and the applications they use.
Rationale More than 50 percent of users do not use a BI solution today because of a perceived complexity and lack of flexibility. The new analytical solution needs to attend to users' skill level and information needs for their specific roles.
Implications Here are the implications:
        The analytical solution needs to provide multi-user and multi-usage type support. The architecture must provide the ability to work with different forms of data using query, manipulation, and rendering techniques that are appropriate for the task. It must also support various forms of analysis such as OLAP, statistical analysis, and sentiment analysis.
        Analytical solution needs to enable more user self-service to remove IT bottleneck.
        Analysis should be integrated into application user interfaces, devices, and processes such that users gain insight where and when they need it.
        Analytical systems should be integrated with business processes in a way to automatically leverage the available information to optimize operational processes.
        Analytics should be available as a shared service to promote its use.
        The architecture must enable end users who are not familiar with data structures and analytical tools to view information pertinent to their needs.
Continuous Availability
The third guiding principle is regarding the availability of the solution.
Statement The analytical solution needs to provide continuous availability to users where and when they need it.
Rationale Analytical solutions are most effective when they become embedded within critical business processes. As the analytical culture continues to mature for an organization, these solutions will increasingly become tier-1 applications that dictate minimal or no downtime.
Implications Here are the implications:
        Analytical solutions need to be designed with high availability and disaster recovery capabilities in mind.
        The analytical architecture needs to incorporate elastic scalability to account for increasing data volume and model frequency without performance degradation.
Data Storage Agnostic
The fourth guiding principle attends to the data storage approach.
Statement The analytical architecture needs to encapsulate the storage layer complexity and diversity from the data and information consumers.
Rationale Data acquisition will come in various forms. Consumption-layer architecture should avoid introducing new data or analysis processing with a new set of tools and technologies, particularly if doing so produces new silos of data and analysis artifacts that cannot integrate easily and properly.
Implications Here are the implications:
        The data consumption architecture components should be capable of handling different types of data and different forms of analysis processing.
        Virtualization capabilities are required to establish uniformed data access across data stores.
Shared Metadata
The fifth principle speaks to the approach to metadata management.
Statement The analytical architecture must provide the ability to define and share metadata across various analytical tools.
Rationale Information can have different meanings to different people and lines of businesses. Standardizing and sharing metadata at the enterprise level improves the accuracy of analysis and reduces the cost of reconciliation.
Implication Here's the implication:
        The analytical architecture must provide a way to catalog and define analytical artifacts, including predictive models, formulas, calculations, aggregation, cross-references, graphs, tables, charts, dashboards, and reports.
Actionable Insights
The sixth guiding principle is regarding the focus of the insights from the analytical solutions.
Statement The analytical solution needs to be capable of initiating actions automatically or through human intervention, based on insights obtained from the analysis.
Rationale Analysis is most effective when appropriate action can be taken at the time of the insight and discovery.
Implications Here are the implications:
        The architecture must support predictive analysis in addition to the traditional descriptive analytics.
        The system must provide functionalities of alerts and event subscription.
Context-Based Governance
The seventh guiding principle focuses on the evolution of data governance.
Statement Data governance needs to be redefined to focus on the challenges introduced by these new classes of data, technologies, and analytical solutions.
Rationale We are seeing a continued move to machine learning and decision automation to keep pace with speed and the volume of data. As companies strive to operationalize analytics, they need to focus more on the optimal mix between human and machine capability and judgment.
Implications Here are the implications:
        Proper governance needs to be in place to facilitate the move to automated decision making and to determine when human intervention/interpretation is required based on the analytical model quality and level of confidence.
        The analytical solution needs to be capable of synthesizing analytical models to establish and measure analytical quality.
        The initial data exploration requires a looser governance model to maximize agility. However, as valuable data is uncovered and becomes incorporated into standard business processes, these same data assets need to follow the same governance standards at the enterprise level.
        The analytical architecture needs to incorporate data life-cycle management. Data archiving also becomes more important to avoid stale data sets plaguing analytical results and to ensure retention requirements are met.
Balanced Access and Security Control
The eighth principle speaks to the changing landscape of security management.
Statement While pervasive information access is the overall goal, it's important not to lose sight of security control.
Rationale New challenges arise in security with the introduction of a large amount of data, especially those originating from machine-to-machine devices with no endpoint control, often referred to as the Internet of Things. There is also more complexity in access control around a multitude of mobile devices because bring-your-own-device (BYOD) is seeing wide adoption in enterprises.
Implications Here are the implications:
        The information architecture effort needs to involve stratifying data assets based on security requirements and sensitivity, identifying highly sensitive data, and applying tight control through defense-in-depth, a multilayered security architecture.
        The architecture needs to provide forensic analysis capabilities to detect, report, and prevent uncharacteristic usage and access.
Assess the Current-State Architecture
Loosely speaking, an architecture for an IT system describes the various components of the system, the inputs and outputs of each component, and how the system interfaces to the other systems that it interacts with. We have been working with CIOs, CTOs, and chief architects of various organizations for quite some time on information architecture and big data analytics initiatives. It's common to hear this comment: "Our business wants to pursue big data solutions, but our information landscape is a mess." The biggest pain point we have encountered in many organizations is the barrier to data sharing. It's a result of numerous challenges within any organization; some are cultural in nature, some are related to existing processes and organization boundaries, and others are a result of siloed applications and inconsistent data definitions. Common symptoms of information silos include lack of data dimension, multiple reporting repositories, lack of insight into unstructured data, and proliferation of isolated departmental data marts. They are often the cause of poor user satisfaction and subpar business intelligence adoption.
Oracle's Architecture Development Process (OADP), as shown in Figures 8-1 and 8-3, focuses on "just-in-time" and "just-enough" architecture, especially when it comes to current-state assessment. We do not recommend a full-blown, "boil-the-ocean" current-state analysis, which prolongs the solution timeline with little value yielded. Instead, we recommend that enterprises take a top-down approach, getting a high-level architecture overview of the current-state capabilities based on the information architecture capability model we mentioned earlier and identifying key pain points and main obstacles that are prohibiting the adoption of new analytical solutions.
How do you know what is enough current-state information? The answer is simple. It's enough as long as you can determine the gap between the current-state and future-state vision, and it's sufficient enough as long as you have the information needed to develop a strategic road map for actionable recommendations. This means you take an iterative approach while developing the current state, the future state, and the road map, by taking a high-level baseline of relevant capabilities, and then drill into specific architecture components and business domains based on the priorities and dependencies.
Establish the Future-State Architecture
Establishing the future-state analytical architecture implies putting together a blueprint. Not having it would be similar to building a home without a blueprint. In other words, there is a greater potential for failure without a sound architecture. An analytical architecture includes not just the blueprint of different components and how they fit together, but also the standards, processes, applications, and infrastructure.
New Capabilities and Requirements
The high-level requirements of this new analytical platform are as follows:
        More types of data and storage platforms This is key to embracing the unstructured and schema-free data types found in most big data. Once these new data types are addressed, an organization obtains greater business value from big data and broader and more agile data sourcing for analytics. The new storage platform includes all data for analytics, including data warehouse appliances, columnar databases, and HDFS. The low cost of the HDFS platform for historical data retention and parallel processing of more data and unstructured data holds the promise of expanding analytical capabilities to the next level.
        Information exploration and discovery capability The types of analytics currently on the rise (based on technologies for SQL, NoSQL, mining, statistics, and natural language processing) are all related to discovering facts about the business that were previously unknown.
        Real time Most enterprises are expecting real-time capabilities from analytical solutions to support fast business processes and decision making. Traditional relational databases and batch-oriented Hadoop systems were not built for real-time operations. Real-time functions include Apache Storm on Hadoop, Cloudera Impala, and Event Processing Engine.
Hybrid Data and Storage Layer
With these new requirements, it's important to plug new capabilities into a holistic architecture vision. Let's first look at the data storage and processing layer.
No discussion about data architecture can occur without covering data warehouse architecture. Most organizations have data located across a large number of heterogeneous data sources. Analysts spend more time finding, gathering, and processing data than analyzing it. Data quality, access, and security are key issues for most organizations. The most common challenge analysts face is data collection across multiple systems and then cleaning, harmonizing, and integrating it. It is estimated that 80 percent of the time analysts spend is around data preparation. It's hardly a good use of such hard-to-find and valuable resources.
According to The Data Warehouse Institute (TDWI), the biggest trend in data warehouse architecture right now is the movement toward more diversified data platforms and related tools within the physical layer of the extended data warehouse environment. While there's a clear benefit to a centralized physical data warehousing strategy, more and more organizations are looking to establish a logical data warehouse, which is composed of a data hub or reservoir that combines different databases (relational or NoSQL) and HDFS. Capabilities in data federations are needed, including canonical data model, schema mapping, conflict resolution, and autonomy policy enforcement. Success comes from a solid logical design based mostly on business structures. In our experience, the best success occurs when the logical design is based primarily on the business—how it's organized, its key business processes, and how the business defines prominent entities such as customers, products, and financials.
In reality, most architecture evolves into a hybrid state, despite standards, plans, and preferences. Centralized and distributed architectures are rarely 100 percent even though that might be the initial objective. The key is to architect a multiplatform data environment without being overwhelmed by its complexity, which a good architectural design can avoid. Hence, for many user organizations, a multiplatform physical plan coupled with a cross-platform logical design is a new data platform and architecture that's conducive to the new age of big data and analytical solutions.
 Here are some guidelines for building this hybrid platform:
        First define logical layering and organize and model your data asset according to purpose, business domain, and existing implementation.
        Integrate between technologies and layers so both IT-based and user-based efforts are supported.
        Consolidate where it makes sense and virtualize access as best as possible so that you can reduce the number of physical deployments and operational complexity.
Figure 8-4 is a sample reference architecture for this hybrid data platform from Oracle's architecture repository.




FIGURE 8-4. Hybrid data platform reference architecture
Here are the main layers of a hybrid data platform:
        Staging layer This is a landing pad for data quality management, the initial cleansing, mapping, standardization, and conflict resolution.
        Foundation layer This focuses on historic data management at the atomic level. Process-neutral, this layer represents an enterprise data model with a holistic vision.
        Access and performance layer This is also known as an analytical layer, with business/functional-specific models, snapshots, aggregations, and summaries.
        Discovery layer This is where data exploration occurs with a significant amount of user autonomy.
 It takes careful consideration to decide on the placement of data assets into each of the layers. Factors to consider include data subject area, level of grain (level of data detail and granularity), and level of time relevance (whether it's operational or historic). The focus of consideration needs to be on data and information consumption. These layers will correspond to different types of data storage and retrieval mechanisms with the most effective integration capabilities, performance characteristics, and security measures in place.
Data and Analytics as a Service
A solid data hub is the first piece of the architecture design. However, the architects' work doesn't stop there. Now let's look at how to deliver data and analytics as a service. Figure 8-5 shows a data-as-a-service blueprint. Through integrated data and shared services, the main objectives of data and analytics as services are to provide data interoperability, integrated analytics, and an agile data platform that allows you to establish quick time-to-value capabilities but also enables you to continue to embrace open-source innovations.




FIGURE 8-5. Blueprint for data and analytics as a service
The top layer in Figure 8-5 represents an end-to-end big data life-cycle process, described in detail as follows:
        Acquisition As various data is captured, it can be stored and processed in traditional RDBMSs, flat files, HDFS, NoSQL databases, or a streaming event model.
        Organization Architecturally, one of the critical components that links big data to the rest of the data realms is the organize layer. This layer needs to extend across all of the data types and domains and bridge the gap between the traditional and new data acquisition and processing framework.
        Analysis The analytical layer contains advanced analytics capabilities such as various data mining and predictive analytics algorithms, spatial and text analytics, and the event processing engine to analyze streamed data in real time.
        Consumption The BI layer will be equipped with information exploration, discovery, and advanced visualization, on top of traditional BI components such as reports, dashboards, queries, and scorecards.
        Governance and security This covers the entire spectrum of the data and information landscape at the enterprise level.
As mentioned earlier, recent analyst predictions call for the need to build smaller and nimble analytics capabilities. This is where service-oriented architecture plays a key role. Let's look at the service layers in more detail in the context of these life-cycle capabilities.
Acquisition
There are high-level categories of services in this stage:
        Data movement services for bulk and incremental movement and replication services
        Data processing services for batch and real-time as well as stream processing
        Storage information life-cycle management (ILM) services for archiving and retention management
Organization
IDC predicts the cohabitation of traditional database technology (RDBMS) with the newer Hadoop ecosystem and NoSQL databases, concluding that in the short term, information management will become more complex for most organizations.
Various organization services play a critical role in reducing this complexity. Categories of services include
        Data enrichment services
        Data aggregation services
        Data joining and merging services
        Data model services for canonical data model, schema mapping, and conflict resolution
        Integrated metadata services, critical to providing a common language for data residing across different data storage models
Analysis
Let's move on to analytics as a service. Event correlation services are getting more and more mainstream for smart grid, fraud, healthcare, intelligence for security, cyber, financial, and public safety use cases. Other categories of analysis services include data mining, predictive analytics, spatial, and text. Typical data mining services will expose algorithms to perform the following:
        Classification
        Pattern matching
        Anomaly detection
        Attribute importance
        Association rules
        Clustering
 On the predictive analytics services side, there are algorithms for the following:
        Regression
        Time series analysis
        Case-based reasoning
        Neural networks
        Multilayer perceptron (MLP)
Consumption
The BI infrastructure can be exposed to provide reports, graphs, and advanced visualization as a service. Content search, publishing, and subscription capabilities can also be exposed as services.
        Various consumption services include the traditional query and reports, distribution, and alerting services.
        More advanced consumption services have surfaced recently. One example is services to provide federated searching, automatic clustering of data, data taxonomy, and context-based faceted search.
        Another example is a virtual documents service that dynamically builds a consolidated view based also on the access and interest point.
        There are also new types of exploration and visualization services that combine tabular, tag cloud, guided navigation, maps, charts, and statistical visualization packages (from R, for example) to send striking messages and discern data relationships that simple tables and numbers cannot provide.
Security
There are four areas of security in an analytical architecture.
        Perimeter security This includes guarding access to the cluster through network security, firewalls, and, ultimately, authentication to confirm user identities.
        Data security This includes protecting the data in the cluster from unauthorized visibility through masking and encryption, both at rest and in transit.
        Access security This includes defining what authenticated users and applications can do with the data in the cluster through file system ACLs and fine-grained authorization.
        Visibility This includes reporting on the origins of data and on data usage through centralized auditing and lineage capabilities.
The security infrastructure can be exposed to perform authentication, access control, auditing, encryption and decryption, and so on, as a service. Security services include two main categories: services to ensure the level of data access such as authentication, authorization, and auditing (the most famous AAA security that we often discuss); and services to provide data protection, such as encryption, masking, and redaction.
In short, the purpose of this blueprint is to further your understanding of a big data vision, help you prioritize the capability requirements and gaps, and establish a road map to incrementally build up these capabilities in a holistic and thoughtful manner. Figure 8-6 shows a sample unified information architecture.




FIGURE 8-6. Unified information architecture
Determine the Strategic Road Map
Companies vary in their level of analytical maturity, but most want to gain a competitive advantage using data-driven decision making. Enterprises are trying to improve productivity, increase revenue, reduce costs, and manage risk and uncertainty in decision making. The potential benefits warrant attention, and companies are making investments to improve their analytic strength. Figure 8-7 contains an overview of an incremental approach to improving business analytical capabilities over time.




FIGURE 8-7. An incremental approach to analytical maturity
The three stages are Explore, Expand, and Transform.
In the Explore stage, most common activities include use case development and prioritization, proof of concept with a few use cases, building of a small Hadoop cluster or NoSQL database to supplement the existing data warehouse, and initiation of data discovery capabilities to show early business benefit. The key is to take small steps and be aware of the overall future state architecture, focus on quick time to market, and allow for continued adoption of open-source architecture and capabilities.
The Expand phase is oriented toward increasing scale and repeatability. More data sources are introduced that require expansion of the hybrid data platform. Integration among these components (between RDBMS and HDFS, for example) is a critical capability to ensure success. Data discovery becomes enterprise-scale and mainstream. This is the phase when data and analytics as a service will mature.
The Transform phase focuses on innovative solutions powered by this agile data platform and analytical services established and matured through the first two stages. With this solid foundation, the enterprise is fully prepared to adopt cutting-edge solutions and embrace new analytical trends, which we will discuss in the "What's Coming Next?" section.
The key decisions are where to invest first in terms of business area and components and how to build up these capabilities over time once a future-state architecture is determined.
 The rule of thumb is to choose a business area that is mature for success based on business priority, perceived value and return of investment, availability of required historic data sources to produce meaningful insights, and adequate skill levels of analysts.
Implement Governance
As Boorstin pointed out, the biggest threat to knowledge is the illusion of knowledge. People tend to be more cautious when they know what they don't know. With the rise of machine learning, data mining, and other advanced analytics, IDC predicts that "decision and automation solutions, utilizing a mix of cognitive computing, rules management, analytics, biometrics, rich media recognition software, and commercialized high-performance computing infrastructure will proliferate."
So, data governance, in the context of big data, will need to focus on determining when automated decisions are appropriate and when human intervention and interpretation are required.
Controls to synthesize analytical models and to establish and measure analytical quality will also emerge to provide effective governance for business decision making.
Data quality continues to be relevant for analytical capabilities. But quality standards need to be based on the nature of consumption. For example, financial reporting requires clean and carefully modeled data, whereas raw data is usually appropriate for exploratory analytics.
In short, the focus and approach for data governance need to be relevant and adaptive to the data types in question and the nature of information consumption.
People and Skills
A "skills gap" is the largest barrier to success with new data and analytical platforms. According to TDWI, the majority of the organizations investing in business analytics intend to upgrade their analytical strengths by improving the skills of existing staff and hiring new analytical talent. Various sources, including the U.S. Bureau of Labor Statistics, U.S. Census, Dun & Bradstreet, and McKinsey Global Institute, indicate that by 2018 the demand for deep analytical talent in the United States could be 50 to 60 percent greater than the supply.
Success with business analytics requires more than just technology. Organizations must upgrade their business and technical analytical skills to make full use of the available technology and to apply the results of analytics to the appropriate business opportunities.
To accelerate an organization's analytical maturity, a center of excellence is beneficial. A center of excellence is a cross-functional team of analytic and domain specialists who plan and prioritize analytics initiatives, manage and support those initiatives, and promote broader use of information and analytics best practices throughout the enterprise. Although it will not have direct ownership of all aspects of the analytics framework, an analytics center of excellence will provide oversight, guidance, and coordination for technology, process, data stewardship, and the overall analytics program—both from an infrastructure and support perspective and a governance perspective. Another benefit is that it could also produce significant value by finding out where IT, domain, and analytical resources exist and making the best use of them.
Because an analytics center of excellence mobilizes analytic resources for the good of the organization, not just for specific business units or one-off projects, it ultimately changes the culture of the organization to appreciate the value of analytics-driven decisions and continuous learning.
Best Practices for Implementing Data Discovery in General
Traditional business intelligence falls short of meeting the full analytical needs of business users. According to various industry reports from TDWI and IDC, more and more enterprises are looking into establishing data discovery capabilities to reduce operational cost, proactively discover potential threats and issues, and gain competitive advantage. Data discovery removes the obstacle between business users and the data they need to analyze through self-service data provisioning and easy-to-use advanced visualization. But for all its promise to fulfill long-awaited analytics needs, organizations must carefully consider their approach to data discovery. Here are some of the best practices.
Architecture and Planning
The first set of best practices involves architecture and planning how this new class of data discovery capability fits into the overall information architecture, how to support effective data provisioning, and how to design a proper security model.
Integration with the Existing BI and Data Management Ecosystem
A number of easily downloadable, desktop-based tools have emerged in the market, empowering end users with the interfaces and flexibility they need to analyze data from their own perspective. Some of these offerings can also integrate with mature back-end infrastructures. However, there are inefficiencies when they are deployed without an enterprise integration plan. The risk of this ungoverned approach is that it can lead to users keeping local copies of data, replicating the same good, old "spread mart" problem.
Integration with the full spectrum of BI and analytics solutions is also a critical success factor. It's important to understand and educate the users about the strengths and gaps of each of these tools and put the best one in use for what it's intended for. For example, make OBIEE the recommended choice for standard dashboard and ad hoc reporting tool for operational staff, use Oracle Endeca Information Discovery for data exploration and discovery for business analysts, and enable data scientists with advanced analytical products such as Oracle Data Mining, Oracle R Enterprise, Oracle Text Analytics, and Oracle Spatial.
Data Provisioning
Data discovery applications allow business users to analyze a new data set without IT involvement. However, the majority of these users also need access to data residing in the enterprise data warehouse and various operational stores as well as departmental data marts. Providing unified information access is critical to enable users to work with the growing data and content with less IT involvement.
The most balanced approach is for IT to create predefined data sources that leverage existing data warehouse and data marts, as well as BI metadata, and let users personalize discovery and visualization by incorporating additional data sets as needed. There needs to be a continuous evaluation of personalized versus enterprise-level data sets throughout the analytical life cycle.
Security
Earlier, we discussed four components of security, including perimeter security, data protection, access control, and visibility. Traditional security measures tend to focus on locking down the perimeters. While network security and physical isolation still play a critical role, more enterprises are implementing the defense-in-depth approach with a multilayered security structure.
The hybrid data platform and self-service data provisioning introduce a new set of security challenges. Here are some recommendations around security for analytics and data discovery systems:
        Establish data classification and governance based on data sensitivity in all data store environments and platforms.
        Store highly sensitive data on an RDBMS environment with mature and sophisticated data protection capabilities, including transparent data encryption, database vault, secure backup, database firewall, data masking, and data redaction.
        Establish predefined data sources to enable integrated access for qualified users.
        Implement label security (row-level security) capabilities and fine-grained access control from different types of users.
        Implement separation of duty to protect data access from superusers and administrators.
Many new types of big data sources do not contain sensitive data, such as sensor data and social media feeds. It might not be practical or beneficial to go through a detailed rationalization process for all data assets.
 One rising approach is to implement a stratified security measure that is composed of a top tier of critically sensitive data and everything else. For the most sensitive data assets, it's recommended that you implement every security measure available, as we listed previously. For less sensitive data assets, open access is typical for the majority of users. This is not a move away from security and access control. The new approach is to allow open access while implementing sophisticated forensic analysis to detect, alert, and prevent unusual behavior.
Process and People
Organizational change is important to whether a data discovery application is going to achieve its potential. A company must be ready to be an analytical-driven organization. It needs to foster intellectual curiosity, the desire to access information, and the habit of data exploration for decision making. Preparing for this organizational change is a process, and it takes time and effort.
Communicate Data Discovery Capabilities
You can't expect people to know what they don't know. It's important to provide education on data discovery and how it differs from and complements traditional business intelligence systems. Demos and training sessions are essential. Initial handholding is also critical to success. You should also conduct use case workshops with different lines of business; prioritize them based on value, impact, cost, risk, and practicality; and identify a set of best-fit use cases for initial prototyping. There is significant benefit to generating enthusiasm in the business community with new insights through these prototypes and quickly turning them into operational solutions.
Look for Quick Wins
It is advisable to strive for success early and demonstrate value. This is critical when adopting a new generation of business intelligence into your organization. A successful initial deployment leads to additional deployments. Find and document repeatable analysis patterns so that once an application is viable, it is made available to other users in the organization.
Training
Experienced consultants from the vendor typically conduct most initial prototypes. This is not a substitute for training. A data discovery platform usually involves a sophisticated architecture for data storage, integration, visualization, and data provision capabilities. Apart from user training, it is also important to provide training for IT staff to enable them to manage and support this platform, to plan for growth, to implement integration and solutions, and to incorporate this platform into the existing operational procedures.
Collaboration
What do you see in Figure 8-8? Some people see a donkey immediately. But others first notice a hummingbird made with ceramic tiles.




FIGURE 8-8. The art of interpretation
Different people notice different things. The same visualization or data set can present varied interpretations. Collaborative exploration is the convergence between data discovery and collaboration tools. It allows sharing of the output and discovery results as well as the hypothesis, which can also be published onto an enterprise social media portal for broader input, group thinking, and the ability to connect the dots. Collaboration also fosters sharing and reuse of data sets that bring greater benefit from these assets.
Best Practices Implementing Oracle Endeca Information Discovery
We started our chapter with a broad discussion on establishing analytical capabilities. We then moved on to general best practices for enabling information discovery. This section lists the practical considerations for implementing Endeca Information Discovery.
Endeca Server
Endeca Server is the foundation of the discovery application. It's important to follow these best practices:
        Conduct proper sizing It's important to size the deployment based on data volume such as number of records, number of attributes, size of attributes per record, number of users, and estimated analytical query complexity.
        Design for high availability Enterprise applications require a certain SLA. It's common for discovery and analytical applications to become part of the critical business processes. Consequently, it's important to design and deploy these applications with high availability. It's recommended to plan production deployment with Endeca Server Cluster for automatic failover that protects the environment and maximizes uptime in the case of physical hardware failure.
        Schedule backup of data domains and views Backup and recoverability is another area that needs to be taken into consideration for implementation. Use the scripts provided in Chapter 2 to export a data domain and all related directories. Integrator ETL can be used to schedule and run backup jobs as well.
        Establish security Data breaches can be costly for an organization, as some companies have learned the hard way. It's important to establish defense-in-depth with proper server and OS hardening. Enable SSL for the production environment. Note that SSL needs to be enabled during installation. Data access control can be achieved through an EQL filtering mechanism as well as data domain permissions.
        Continuous monitoring Define operational procedures to monitor the health of Endeca services. You can design scripts to either ping Dgraph processes or run a "low-cost" query to the Dgraphs based on your defined interval. You can further define query latency thresholds to determine outage or other health issues. These scripts can be scheduled in Oracle Enterprise Manager for an integrated view of server, storage, and software.
For more detailed implementation guidance of Endeca Server, please refer to Chapter 2.
Endeca Studio
Endeca Studio is a self-service user interface for business analysts. Here are a few considerations that could make the user experience more pleasant and positive:
        Educate users on performance considerations Fast response time is one of the key differentiators for Endeca. However, any system can experience performance challenges if no proper governance is put in place. Educating the users about potential performance pitfalls can improve user experience through setting the right expectations. First make them understand that loading a large data set through a provision service is a lengthy process and could potentially affect the usability of other Endeca Studio applications. So, it might be advisable to load large data sets after hours, for example. Some other considerations to improve Endeca Studio applications include the following:
              Reduce the number of components per page.
              Avoid inefficient EQL queries.
              Display the minimum number of columns needed in components such as the result table and available refinements.
        Provide a predefined data source IT should provide business analysts with predefined data sources to use in Endeca Studio to provide data source connectivity to the enterprise data warehouse, appropriate departmental data marts, and existing OBIEE environment. This will provide ease of data access and reduce data copying and proliferation.
        Choose the right visualization Training is important to educate users on different types of visualization, even though the design is extremely intuitive with Endeca Studio. Keep in mind that users might be at different technical levels, and some might benefit from a basic understanding of the proper uses of different visualization components.
For more detailed implementation guidance of Endeca Information Discovery Studio, please refer to Chapter 3.
Integrator ETL, WAT, and IAS
The Endeca Information Discovery platform provides a number of best-of-breed integration capabilities, including Integrator ETL, WAT, and IAS, as was discussed in Chapter 4. Here are some of the guidelines on how to use these tools effectively:
        Automate data refresh Use ETL Integrator Server to automate scheduled workflow for certain data you want to acquire on a regular basis.
        Integrator ETL development and design
              Use provisioning services to explore the data set.
              Use global variables for ease of change management.
              Use proper metadata naming to avoid confusion.
              Follow a consistent naming convention for ease of design maintenance.
              Iteratively define and test each component and step in ETL for ease of troubleshooting.
        Use trash and debug components for validation before running loads against the server to minimize the need for cleanup and performance implications.
        IAS source management IAS is a command tool, and XML is used extensively for configuration purposes. Establish a development process to use source control tools to ease maintenance and change management.
        IAS versus WAT Both tools possess web acquisition capabilities. Please refer to the appendix for a comparison of the two products and a quick lookup guide for the supported data source types for each product.
For more detailed implementation guidance of Endeca Information Discovery Integrator ETL, WAT, and IAS, please refer to Chapter 4.
Future Trends in Analytics
In Chapter 5, we began Part II of this book with various industry use cases for business analytics. To wrap up Part II and this book, we will now take a sneak peek into the future to see what some of the trends are now and will be in business analytics in the coming years.
What's Happening Today
As we mentioned earlier in this book, many innovations are taking place in the analytical space. The following technologies are becoming mainstream: mobile, real time, and pervasive visualization.
Mobile
Mobile provides reach. Convenience of information access is the most critical factor to establish the habit of fact-based decision making. Information needs to be available when there is a decision to be made. Business intelligence and data discovery applications are going to be accessed more and more from mobile devices as we move forward.
From pushing static data and alerts into mobile devices to fully interactive mobile applications, the key to a successful mobile BI solution is in delivering the practical and tactical information needed to make immediate decisions. The development life cycle for the next generation of BI applications will be in a matter of days instead of weeks or months. This will likely trigger significant change in how IT operates and manages the software development life cycle and conducts change management.
Real Time
Real-time data for discovery and exploration is also becoming mainstream and a standard requirement.
Historical data is important to run advanced data mining and predictive analytical models to understand customer profiles, demographics, purchasing history, and buying patterns in comparison with other customers in the same clustering. Retailers now have mobile apps that will capture your aisle location in the grocery store and what you have put into your basket to provide real-time recommendations and coupons, based on the batch analytical model as well as the real-time input. The same real-time recommendations apply when you shop online with the input of your recent clicks and browsed items. Your action of whether you redeem that coupon and respond to that promotion will be further captured and stored back to the historic Hadoop cluster to continue to fine-tune the recommendation engine and model.
A real-time streaming event and historical batch model combination gives organizations the most powerful analytical insight and real-time decision.
Pervasive Visualization
Lack of ease of use for less technical employees is often quoted as one of the biggest barriers to BI adoption. There has been tremendous demand to put analysis capabilities in the hands of users, not elite analytics or BI experts. Forrester analysts have pointed out the following key characteristics of advanced visualization as it compares to traditional BI: dynamic data, visual querying, linked multidimensional visualization, animation, personalization, and actionable alerts.
The way we process information is guided by how it is presented to us, including attributes such as color, size, texture, density, and movement that activate our visual sensitivity. Figure 8-9 illustrates how boldface, size, and orientation can easily affect our ability to process information and data.




FIGURE 8-9. Power of visualization
As you go through each of the squares in Figure 8-9 clockwise from the top left, you can see how much easier you are able to count the occurrences of the number 8 with simple changes of boldface, orientation, and size. Colin Ware pointed out in his book Information Visualization (Morgan Kaufmann, 2012) that our brain obtains more information through vision than through all of the other senses combined.
Visualization is where data meets design. It is not just about bells and whistles. The main benefit of data visualization lies in its ability to facilitate how our brain processes information and discerns meaning from large amounts of data. We are seeing more vendor solutions that provide preexisting templates, familiar search functions, and guided visual component selection based on the nature of data.
What's Coming Next
As mobile, real-time, and pervasive visualization continue to mature, we have seen development in some of the following new trends, including natural language processing, cognitive BI, and GeoSpatial revolution. There are a variety of innovative technologies in the BI space, but these three technologies are likely to see quantum leaps and pervasiveness of their usage in the next few years.
Natural Language Processing
Natural language processing (NLP) is at the center of modern software that processes and understands human language. The heart of Web 2.0 is social media and user-generated content. It presents tremendous potential value. Some of the NLP functions are already included in data discovery tools such as Endeca Information Discovery. Examples include word and sentence tokenization, text classification and sentiment analysis, spelling correction, information extraction, parsing, and theme extraction. We've also seen technologies like Siri and a slew of voice-prompt software on other mobile devices. The challenges we face stem from the highly ambiguous nature of natural language. More advanced NLP capabilities will be incorporated into BI and data exploration, including algorithms such as n-gram language modeling, Naive Bayes and Maxent classifiers, sequence models like Hidden Markov Models, probabilistic dependency and constituent parsing, and vector-space models of meaning. These algorithms represent the mix of knowledge-engineered, statistical, and machine-learning techniques to disambiguate and respond to natural language input. They will allow our future BI applications to analyze and process complex human language, use data science techniques to automatically generate hypotheses, evaluate a panel of responses based on relevant evidence, discern meaningful information from them, answer our questions through voice prompt, and continue to get smarter based on outcomes with each iteration and interaction. The user interface for BI in the not-so-distant future is going to be more human-to-human with voice and natural language instead of through traditional computer or handheld input devices.
Cognitive BI
We are already seeing cognitive technology in the BI space for automated information discovery and reporting. Big data systems today are not limited to querying information stored in predefined views, tagged semantic models, or static data models. Data is being accessed via automated indexing for likely correlations and through guided navigation, as is with Endeca Information Discovery.
Human decision making is a complex process. Rational decision making is characterized with the following steps: We collect lots of information, examine a wide variety of alternatives, and then make decisions that maximize the possibility of obtaining the original objective. Situation awareness and mental models play a key role in decision making also. However, we do not make decisions in a manner consistent with this rational model. Nobel Prize winner Herbert Simon has argued that humans possess bounded rationality. We are cognitively limited, such that we can't possibly be as comprehensive in our information gathering and analysis as assumed. Our cognitive limitations lead to errors in judgment, not because of a lack of intelligence, but simply because we are human. Psychologists describe these systematic mistakes as cognitive biases. This concept was covered in Chapter 3.
The new generation of BI technology will be more invested in decision-making support on cognitive orientation to overcome these limitations. There are new research projects that are focused on developing related theories, frameworks, and technologies through combining data warehousing, data mining, cognitive psychology, knowledge management, and decision-making theories to facilitate cognitive business intelligence. These projects examine novel concepts, models, algorithms and system architecture such as ontology and experience representation, situation awareness parsing, data warehouse query construction, and guided situation presentation. In time, machines might just make better decisions than humans.
GeoSpatial Revolution
Geographers were dealing with big data way before the term existed. With the explosion of data sets of all types, GeoSpatial data is also omnipresent, spanning from local to global and across themes ranging from natural hazards to energy to water to geology. According to Anthony Robinson, a professor in geography at Penn State, GeoSpatial revolution involves major transformations in how we navigate, share stories, and, ultimately, make decisions.
With the combination of the speed of the Internet, capabilities of satellite technologies, and software such as Google Earth, we are going to see explosive development in how GeoSpatial solutions will change our lives. We are now a location on the map, and we are the center of the context for everything else on the map around us.
One example of an application is the location-based personal assistant as discussed earlier. We are moving away from having to actively search for something to searches telling us what we might be interested in checking out based on location, time of the day, schedules, past activities, demographic information, and general personal preferences. Crowd sourcing is another example that could facilitate real-time reporting of ground conditions in the case of a natural disaster or emergency situation. With the availability of LI-DAR data (LI-DAR stands for Light Detection and Ranging), 3-D terrain models with laser-point accuracy can now be generated for better urban planning such as shadow analysis to determine the impact of a proposed high-rise on a nearby park. The uses and applications are limitless for the commercial, healthcare, and public sectors.
We are continuing to see an evolutionary convergence of search and geospatial technologies from daily personal activities to disaster response and public safety. With mobile technologies, we are becoming individual sensors. Every piece of information we now share with our families, friends, colleagues, and the public has some GeoSpatial tagging on it. We now have access to this entire information ecosystem—it's a treasure trove that BI solutions will find limitless applications for, and we will see these solutions woven seamlessly into every part of our lives.
Summary
The future of analytics is exciting, and the possibilities are limitless. With the availability of more data and the ever-growing sophistication of analytical tools, our world is becoming more transparent. We enjoy the convenience and productivity these solutions offer, but we dislike having our lives under a microscope and available to the public. Various privacy concerns have triggered many recent debates on whether our Internet search history should be "forgotten." Transparency and privacy are going to require an intricate balance that needs to be watched closely in the coming days.








APPENDIX
Additional Resources
Endeca Integrator Reference
This section of the appendix recaps the two major data acquisition tools that are part of Endeca Integrator: the Integrator Acquisition System and the Web Application Toolkit. These two products have many similarities and differences. The information in this appendix is a quick lookup guide for the supported data source types for each product.
Be aware of the following when considering using either the Integrator Acquisition System and the Web Application Toolkit:
        Both of the products write data to intermediate storage for consumption by Endeca ETL; after processing within Endeca ETL, the data is written to Endeca Server. The Web Application Toolkit supports REST-based web services and is not compatible with Endeca Server, which uses WSDL-based web services.
        The Web Application Toolkit has a powerful graphical design interface and can run on a Windows workstation, while the Integrator Acquisition System is a command-line tool. Users of the Integrator Acquisition System need to possess good skills in working with a command-line tool.
        The Web Application Toolkit is primarily for acquiring data from web sources. It also has strong capabilities to crawl Microsoft Excel spreadsheets and to extract text from PDF files. The Integrator Acquisition System supports web crawls, file system crawls, and database crawls. The list of supported source types is extensive and is provided at the end of this appendix.
The following sections cover the supported source types for the Integrator Acquisition System.
Archive Formats

 Database Formats

E-mail Formats


Multimedia Formats

Other Formats


Presentation Formats

 Raster Image Formats


Spreadsheet Formats


Text and Markup Formats

Vector Image Formats


Word Processing Formats



Useful Links
These are sites where you can find more information about the topics covered in this book:
        Download site for data sets and applications from this book:
          www.endeca.pro/book/download
        Oracle Endeca Information Discovery:
          www.oracle.com/us/solutions/business-analytics/business-intelligence/endeca/resources/index.html
        Endeca Information Discovery Community Forum:
          https://community.oracle.com/community/developer/english/business_intelligence/information_discovery/endeca_information_discovery
        Endeca Information Discovery Whitepaper:
          www.oracle.com/webapps/dialogue/ns/dlgwelcome.jsp?p_ext=Y&p_dlg_id=14616402&src=7878540&Act=51&sckw=WWMK13048931MPP004
        Oracle Business Analytics:
          www.oracle.com/us/solutions/business-analytics/overview/index.html
        Oracle Big Data Handbook:
          www.mhprofessional.com/product.php?isbn=0071827269
        Oracle Enterprise Architecture:
          www.oracle.com/us/solutions/enterprise-architecture/index.html
        Art of Critical Decision Making:
          www.thegreatcourses.com/tgc/courses/course_detail.aspx?cid=5932&ai=69119&cm_mmc=youtube-_-brand-_-video-_-5932
        "Big data: The next frontier for innovation, competition, and productivity" report from McKinsey Global Institute:
          www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation
        TDWI Best Practices Report: Data Visualization and Discovery for Better Business Decisions:
          http://download.1105media.com/pub/tdwi/files/TDWI_BPReport_Q313_DVIS_Web.pdf
        Information Week: 5 Big Business Intelligence Trends for 2014:
          www.informationweek.com/software/information-management/5-big-business-intelligence-trends-for-2014/d/d-id/1113468
        Gartner Identifies the Top 10 Strategic Technology Trends for 2014:
          www.gartner.com/newsroom/id/2603623
        IIA 2014 Analytics Predictions:
          http://iianalytics.com/resources/archived-webinars/2014-analytics-predictions/
References
Boorstin, D. (1983). The Discoverers: A History of Man's Search to Know His World and Himself. Random House.
Deming, W.E. (2000). Out of the Crisis. The MIT Press.
Galbraith, J.K. (2009). The Great Crash 1929. Houghton Mifflin Company.
Howson, C., (2014). "5 Big Business Intelligence Trends for 2014." Information Week.
Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., Byers, A.H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute.
Olofson, C. (2014). Big Data in the Enterprise: When Worlds Collide. IDC.
Roberto, M.A. (2009). Art of Critical Decision Making. The Teaching Company.
Ross, J.W., Weill, P., Robertson, D. (2006). Enterprise Architecture As Strategy: Creating a Foundation for Business Execution. Harvard Business Press.
Russom, P. (2014). Best Practices Report: Evolving Data Warehouse Architectures in the Age of Big Data Analytics. The Data Warehouse Institute.
Simon, H.A. (1957). Models of Man. John Wiley.
Stodder, D. (2013). Best Practices Report: Data Visualization and Discovery for Better Business Decisions. The Data Warehouse Institute.
Taleb, N. (2010). The Black Swan: The Impact of the Highly Improbable. Random House.
Tsanas, A, Little, M.A., McSharry, P.E., Ramig, L.O. (2009). Accurate telemonitoring of Parkinson's disease progression by non-invasive speech tests. IEEE Transactions on Biomedical Engineering.
Ware, C. (2012). Information Visualization: Perception for Design. Morgan Kaufmann.








Index
Please note that index links point to page beginnings from the print edition. Locations are approximate in e-readers, and you may need to page down one or more times after clicking a link to get to the indexed material.
A

access control
      best practices, 202-203
      Endeca Server, 215
      security measures, 208
acquisition as a service, 207
actionable insights, 201-202
ADR (Application Development Runtime), 37
advertisement use cases, 122-123
AHTD (At-Home-Testing-Device), 190
analytic architecture, 203-209
analytical applications. See also applications
      brand reputation, 114-116
      customer churn prevention, 118-119
      HR, 116-118
      by industry, 113-136
      use cases. See use cases
analytical architecture, 208-209
analytical capabilities
      developing, 196-211
      expanding, 185, 204
      maturity, 171, 209-210
analytical skills, 211
analytics. See also data discovery
      actionable insights, 201-202
      approachable analytics, 200
      big data and, 196
      business, 198, 199
      considerations, 200
      continuous availability, 200-201
      current state architecture, 203
      Expand phase, 209
      Explore stage, 209
      future state architecture, 203-209
      future trends, 216-220
      predictive, 188-189, 206, 208
      security. See security
      as a service, 206-208
      Transform phase, 209
analytics center of excellence, 211
analytics toolsets, 9
analytics trends, 216-220
application administrators, 23
Application Development Runtime (ADR), 37
application icon, 19
application information icon, 18
application members, 23
application membership, 23
application servers, 32
applications
      access management, 22-23
      administration of, 22-23
      analytical. See analytical applications
      certified, 18
      discovery, 8, 9, 212, 216
      enterprise, 11, 51, 214
      healthcare, 171-193
      mobile, 122, 217
      private, 23
      public, 23
      roles, 23
      voter fraud, 156-161
architectural principles, 199-203
 Aristotle, 164
ARPU (average revenue per user), 118
At-Home-Testing-Device (AHTD), 190
attribute groups, 21-22, 173
attributes, 12, 19-22, 70, 177
auto manufacturers, 124, 131-132
auto-idle setting, 54
average revenue per user (ARPU), 118
B

backup jobs, 215
backups
      data domain profiles, 41-42
      data domains, 39-41, 215
      Endeca Server, 39-44, 215
      node profiles, 43-44
      views, 215
bar graphs, 82-84
best practices, 195-220
      analytical capabilities, 196-211
      architecture vision, 199-203
      business architecture, 199
      current state architecture, 203
      data discovery, general, 211-214
      data governance, 202, 210
      data storage, 201
      Endeca IAS, 216
      Endeca Information Discovery implementation, 214-216
      Endeca Server, 214-215
      Endeca Studio, 215
      Endeca WAT, 216
      future state architecture, 203-209
      Integrator ETL, 216
      people and skills, 211
      processes and people, 213-214
      strategic road map, 209-210
BI (business intelligence), 4-6, 32, 219
BI systems, 4-6, 8
big data. See also data
      analytics and, 196
      considerations, 114
      governance issues, 210
      industry use cases, 119, 121, 124-136
      security measures, 212-213
Big Data Appliance, 8
The Black Swan, 58
bookmarks, 20
Boorstin, Daniel, 196
brand reputation monitoring, 114-116
bring-your-own-device (BYOD), 202-203
bubble charts, 75-80
Bulk Load to Data Domain control, 155
business analytics, 198, 199. See also analytics
business architecture, 199-203
business intelligence. See BI
business operating model, 199
business processes, 199
business strategy requirements, 199
businesses. See companies
BYOD (bring-your-own-device), 202-203
C

call detail records (CDRs), 123
cameras, hidden, 119-120
CAT (Consolidated Audit Trail), 127
CDRs (call detail records), 123
cell phones, 122
cell service providers (CSPs), 123, 124-125
census information, 134
centers of excellence, 211
Certified Applications, 18
cgroups, 55
chart groups, 174, 176
charts. See also graphs
      bubble, 75-80
      components, 19
      considerations, 47
      pie, 73
churn, 118-119
churn analytics, 118-119
city planning, 134
claims analysis, 172-176
claims results table, 175, 176
clinical analysis, 130-131
clinical research, 186-189
clinical trials, 130
CloverETL, 99, 100
Cluster Coordinator, 12, 33, 35, 53
clustered file system, 34-35
clustering technologies, 33-35
clusters, server, 12, 33-35
cognitive BI, 219
cognitive bias, 85
collaboration, 213, 214
comma-separated value (CSV) file, 6, 20, 62
commercial off-the-shelf (COTS) analytics, 124
 Common Enterprise Information Model, 181
communications fraud, 123-124
Community Applications, 18
companies
      brand reputation, 114-116
      customer churn prevention, 118-119
      financial, 125
      insurance, 126
      manufacturing, 131
      retail. See retailers
      telco, 123
company insight analytics, 125
component containers, 174, 175
Configuration Web Service, 14
Consolidated Audit Trail (CAT), 127
consumer package goods (CPG), 118, 119-122
consumption services, 208
continuous availability, 200-201
control groups, 31-32, 54, 55
Conversation Web Service, 13, 54
coordinated operating model, 199
correlations
      considerations, 196
      event, 207
      healthcare. See healthcare correlations
      hidden, 132
cost per acquisition (CPA), 122
COTS (commercial off-the-shelf) analytics, 124
CPA (cost per acquisition), 122
CPG (consumer package goods), 118, 119-122
crawls, 90-93
crime data, 133
CSPs (cell service providers), 123, 124-125
CSV (comma-separated value) file, 6, 20, 62
CTRs (currency transaction reports), 126
curl command, 52
currency transaction reports (CTRs), 126
current state architecture, 203
customer churn prevention, 118-119
customer insight, 125
customer master data, 118
customers
      defected, 118-119
      disgruntled, 115
      location-based marketing, 122-123
      profiling, 118, 119-121
      reaching out to, 115
      segmentation of, 120
      social media interaction, 124
cyberthreats, 135
D

dashboards, 179-180
data. See also big data
      access to, 202-203
      clean, 210
      correlations, 196
      dirty, 5-6
      geospatial, 101, 102, 219-220
      historical, 217
      hybrid, 204-206
      LI-DAR, 220
      patient, 164-165
      quality, 210
      raw, 210
      real-time, 217
      security, 202-203, 208
      semistructured, 5-6
      sensitive, 212-213
      as a service, 206-207
      structured, 5-6
      unstructured, 5-6
      visibility, 208
      visualization. See data visualization
data breaches, 215
data consumption, 201
data discovery. See also analytics; Endeca Information Discovery
      architecture/planning, 211-213
      best practices, general, 211-214
      data provisioning, 212
      data warehouses, 135
      education/training, 114, 213
      security. See security
data discovery tools, 123
data domain clusters, 12
data domain profiles, 38, 39, 41-42
data domains
      backups, 39-41, 215
      considerations, 12
      Endeca Server Cluster, 33-35
      load balancers and, 35-37
      managing, 38-39
      memory usage, 54
      monitoring, 45-46, 47
      replicating, 44
      resource management, 54
data factory, 135
data governance, 202, 210
data hubs, 177-179, 204, 206, 277
data ingest, 88
 Data Ingest Web Service (DIWS), 12, 13
data integration tools, 9
data management, 198
data models, 4, 5, 11
data provisioning, 212
data realms, 197
data reservoir, 135
data servers, 8-9. See also servers
data sets
      considerations, 199
      Endeca Studio, 20-21, 69-71
      joining, 154-155
      via OBIEE data sources, 182, 184
data sources
      demand forecasts, 121
      Endeca Studio, 215
      importing, 149, 151
      OBIEE, 182, 184
      patient analysis, 178, 179
data storage, 125, 201
data types, 12-13
data visualization
      advanced, 75-80
      in Endeca Studio, 57-85, 215
      example, 75-80, 217, 218
      future trends, 217
      pervasive, 217
data warehouses
      augmentation, 135
      best practices, 204
      The Data Warehouse Institute, 204
      information discovery, 135
database server. See data server
databases
      considerations, 60, 97
      formats, 26
      JDBC-connected, 15, 60
      relational, 5, 6, 204
      repository, 61
data-driven security strategies, 134-135
debug mode, 155
deep packet inspection. See DPI
DEFINE statement, 13
demand forecast, 121
Deming, W. Edwards, 88
demographic information, 120
Dgraph nodes, 33
Dgraph processes, 12, 27, 45-46, 47
discovery, 196. See also Endeca Information Discovery
discovery applications, 8, 9, 212, 216
disease-management programs, 128
diversified operating model, 199
DIWS (Data Ingest Web Service), 12, 13
DMV records, 134
domain knowledge, 4
DPI (deep packet inspection), 125
DPI analysis, 124-125
drive systems, 30-31
driving data, 126
drug dosages/reactions, 130, 131
E

EA (enterprise architecture) approach, 196
e-commerce solutions, 6
edge lines, 155
edges, 101
education/training, 114, 213, 215
EDW augmentation apps, 135
election fraud, 138
election results analysis, 139-145. See also voter fraud analytics/detection
electronic medical records (EMRs), 164
element management system (EMS), 125
employee data, 116-118
employees
      analytical skills, 211
      collaboration, 213, 214
      ease of use and, 217
      interpretation issues, 213, 214
      organizational change and, 213-214
      "skills gap," 211
      training, 213
      turnover rates, 117
EMRs (electronic medical records), 164
EMS (element management system), 125
Endeca (company), 6-8
Endeca IAS, 90-96
      best practices, 216
      components, 15
      considerations, 222
      crawls, 90-93
      described, 9, 15, 89, 90
      vs. Endeca WAT, 98
      IAS Server crawl file types, 15, 16
      IAS Web Crawler, 15, 90-93
      installation, 93-95
      operations, 95-96
      record stores, 90, 91
      reference, 222-231
       taxonomy, 91
      uses for, 98
Endeca IAS Server, 15, 16, 90, 95
Endeca Information Discovery
      architecture, 1-24
      best practices, 214-216
      vs. business intelligence systems, 4-6, 8
      components, 8-23
      features, 8
      implementing, 214-216
      introduction to, 6-8
      licensing, 24
      major components, 8-9
      overview, 8-9
Endeca Information Discovery Integrator. See Endeca Integrator
Endeca Integrator
      components, 14-17
      described, 10
      Endeca IAS. See Endeca IAS
      Endeca WAT. See Endeca WAT
      enterprise-driven data exploration, 88, 110
      IKM SQL to Endeca Server, 17
      integration server, 15
      Integrator ETL. See Integrator ETL
      Integrator ETL Server, 55, 90, 96, 109
      overview, 88-90
      reference, 222-231
      versions, 13
Endeca Integrator Acquisition System. See Endeca IAS
Endeca Provisioning Service, 59, 60-61
Endeca Query Language. See EQL
Endeca Server, 25-55. See also servers
      access control, 215
      backups, 39-44, 215
      basics, 53-55
      best practices, 214-215
      clustering, 12, 33-35
      continuous monitoring, 215
      data domains. See data domains
      data model, 11
      described, 8-9, 10
      Dgraph. See Dgraph entries
      directories, 53
      Endeca Provisioning Service, 59, 60-61
      enrichment plug-ins, 70
      EQL. See EQL
      hardware selection, 27-30
      high availability and, 214-215
      installing, 37
      instances, 33
      load balancers and, 35-37
      logfiles, 52-53
      maintenance of, 51-53
      managing, 37-51
      memory, 30, 54-55
      operating system, 31-32
      overview, 11-14
      performance, 53-54
      performance monitoring, 45-51
      planning installation of, 26-37
      proper sizing of, 214
      properties, 53-54
      records/attributes, 12
      resource management, 54-55
      securing, 32-33
      security, 215
      services, 13-14
      storage, 30-31
      version information, 13
Endeca Server APIs, 13
Endeca Server Cluster, 12, 33-35
Endeca Server enrichment plug-ins, 70
Endeca services, monitoring health of, 215
Endeca Studio, 57-85
      application administration, 22-23
      application creation, 62-65
      application development, 61
      best practices, 215
      clustering, 60
      data sets, 20-21, 69-71
      data sources, 215
      data visualization, 57-85, 215
      described, 10, 17
      enrichment features, 100
      failed banks example, 61-80
      flight delay example, 81-84
      installation requirements, 60, 61
      languages, 17
      map component, 68-69, 70
      minimal implementation, 59
      navigation overview, 18-19
      overview, 17-23
      performance, 215
      provisioning service, 22
      systems architecture, 58-61
      tag clouds, 66-68
      user-driven data exploration, 58
      using enrichments within, 69-73
      versions, 13
 Endeca Studio applications
      administration of, 22-23
      anatomy of, 19-22
      components, 19-22
      performance, 215
Endeca WAT, 96-98
      background, 96
      best practices, 216
      considerations, 222
      described, 89, 96
      vs. Endeca IAS, 98
      example, 97-98
      insight, 96-97
      installation, 97-98
      overview, 15-16
      reference, 222-231
      robots, 96-98
      uses for, 98
Endeca WAT Studio, 96
Endeca Web Acquisition Toolkit. See Endeca WAT
EndecaServer.properties file, 42-43, 53-54
eneperf tool, 49-50
enrichments, 69-73
entdecken, 6
enterprise applications, 11, 51, 214
enterprise architecture (EA) approach, 196
enterprise-driven data exploration, 88, 110
Entity and Collection Configuration Web Service (sConfig), 14
EQL (Endeca Query Language), 8-9, 12-13, 54
EQL records filters, 33
equipment monitoring, 132
ETL graphs. See graphs
ETL IDE, 14
ETL offloading, 135
ETL processes, 101
ETL (extract-transform-load) tool, 9
event correlations, 207
Exalytics Server, 7, 32
expressions, 13
Extract Terms enrichment, 70
extract-transform-load. See ETL; Integrator ETL
F

faceted data model, 11
facial recognition technology, 119-120
failed banks example. See FDIC failed banks example
FDIC (Federal Deposit Insurance Corporation), 61-62
FDIC failed banks example, 61-80
      advanced data analysis, 75-80
      advanced data visualization, 75-80
      data summary, 73
      map component, 68-69, 70
      tag clouds, 66-68
FDIC press release data, 92, 97, 102
Federal Deposit Insurance Corporation. See FDIC
Feynman, Richard, 55
files
      CSV, 6, 20, 62
      log. See log files
      .out, 52
      PDF, 97, 222
      .pid, 52
      .reqlog, 52
      seed, 92
      XML, 91
financial services, 125-127
flight delay example, 81-84
follower nodes, 12, 33
foreign languages, 17, 109
fraud detection, 137-161
fraud prevention
      financial institutions, 125, 126
      public sector, 133
      telecommunications, 123-124
      voter. See voter fraud analytics/detection
G

Galbraith, John Kenneth, 4
generic use cases, 114-119
genetic research, 131
genome data, 131
geofencing, 122-123
geographical groups, 69
geographical regions, 69-73
geomarketing scenarios, 122
geospatial data, 101, 102, 219-220
geospatial range selection, 69, 70
governance models, 202, 210
graphs. See also charts; Dgraph entries
      bar, 82-84
      bulk loading, 155
      configuring, 151-152
      considerations, 14, 101
       creating, 151-157
      described, 14, 101
      formatting, 153-154
      metadata, 152-153
      running, 156-157
      working with, 14, 15
GROUP BY clause, 13
GSM networks, 123
guided navigation, 66
H

HACMM (Healthcare Analytics Capability Maturity Model), 165-170
Hadoop clusters, 8
hardware nodes, 33
health profile analytics, 128-130
healthcare, 127-131
healthcare analytics
      claims analysis, 172-176
      clinical research, 186-189
      drug dosages/reactions, 130, 131
      HACMM, 165-170
      health profile analytics, 128-130
      operations analysis, 179-180
      partners, 181-186
      patient data, 164-165
      patients analysis, 176-179
      predictive analytics, 188-189
      remote patient monitoring, 128, 189-193
      use case implementation, 171-193
      use case overview, 164-170
      use case summary, 193-194
Healthcare Analytics Capability Maturity Model (HACMM), 165-170
healthcare applications, 171-193
healthcare correlations, 163-194
healthcare organizations, 164, 165, 186
healthcare systems, 128, 164
heat maps, 20, 21
high availability, 34-35, 214-215
historical batch model, 217
historical data, 217
horizontal scaling, 35
HR analytics, 116-118
HR analytics applications, 117-118
HR event data, 116-118
HR (human resource) management, 116-118
hub-and-spoke master data architecture, 177
human discovery, history of, 196
human resources. See HR entries
hybrid data, 204-206
hybrid data platform, 204-206
I

IAS. See Endeca IAS
IDC diagnostic codes, 172
IKM SQL to Endeca Server, 17
importing data sources, 149, 151
industry best practices. See best practices
industry use cases, 114, 119-134
information. See also data
      demographic, 120
      discovery. See data discovery
      evaluating, 5
      new sources of, 4
information architecture, 199-203
information technology. See IT
insurance companies, 126
insurance plans, 126
integration server, 15
integrative thinking, 110
Integrator Acquisition System. See Endeca IAS
Integrator ETL, 99-109. See also Endeca Integrator; ETL entries
      background, 99-100
      basic paradigm, 89
      basic usage, 100-102
      best practices, 216
      data domains, 44
      described, 14, 90, 99
      enrichment features, 100
      graphs, 101
      installation, 107-109
      overview, 14-15, 99
      Text Enrichment, 9, 21, 100, 102-107, 109
Integrator ETL Server, 55, 90, 96, 109
Internet service providers (ISPs), 124
ISPs (Internet service providers), 124
IT systems, 126, 134, 203
IT use cases, 114, 134-135
J

Java Database Connectivity. See JDBC
Java Development Kit (JDK), 37, 51
Java updates, 51
JDBC (Java Database Connectivity), 14
JDBC database sources, 15, 58, 59
 JDBC-connected databases, 15, 60
JDK (Java Development Kit), 37, 51
Jetty Web Server, 93, 94-95
jobs, backup, 215
Jobs, Steve, 138
join conditions, 154-155
K

Kapow Katalyst Design Studio, 96
Kapow Software, 96
key performance indicators (KPIs), 118, 165
key-value pairs, 8
KPIs (key performance indicators), 118, 165
L

languages, 17, 109, 218
law enforcement, 133
leader nodes, 33
licensing, 24
LI-DAR (Light Detection and Ranging) data, 220
life science, 127-131
lifetime value (LTV), 118
Light Detection and Ranging (LI-DAR) data, 220
links, useful, 231-232
Linux curl command, 52
Linux systems
      Endeca Server on, 31-32
      Oracle Linux, 60, 93
      resource management, 54-55
      system requirements, 60
live statistics, 147-148, 160
load average, 49
load balancers, 35-37
local crawls, 90-91
location-based marketing, 122
location-based promotion, 122-123
log files
      Endeca Server, 52-53
      NE logs, 125
      NIL logs, 126
      NMS logs, 125
      processing, 134
      system, 134
      WebLogic server, 53
log processing apps, 134
loss prevention, 125
LTV (lifetime value), 118
M

M2M (machine to machine) devices, 124
machine to machine (M2M) devices, 124
malware threats, 135
manufacturing, 131-132
map components, 20, 68-69, 70, 101
map views, 177, 178
market basket analysis, 119-121
master data management (MDM), 118, 176
maturity models, 165-171, 209-210
MDM (master data management), 118, 176
medications, 128
medicine, personalized, 131
memory
      data domains, 54
      Endeca Server, 30, 54-55
      RAM, 39
memory resources, 54
metadata
      ETL, 101, 102, 107
      in graphs, 152-153
      management of, 201
      shared, 185, 201
Minority Report, 119
mobile devices, 73
mobile notification software, 122
mobile technology, 216-217
modular segmentation, 131
mortgage risk management, 127
My Applications, 18
N

NAS (Network Attached Storage) server, 34
National Market System (NMS) securities, 127
National Security Agency (NSA), 33, 122
natural language processing (NLP), 218
NE (network element) logs, 125
negotiable instrument logs (NILs), 126
NEPs (network equipment providers), 124
Network Attached Storage (NAS) server, 34
network element (NE) logs, 125
network equipment providers (NEPs), 124
Network File System (NFS), 34
network management system (NMS) logs, 125
network operations, 124-125
NFS (Network File System), 34
NILs (negotiable instrument logs), 126
NLP (natural language processing), 218
 NMS (network management system) logs, 125
NMS (National Market System) securities, 127
node profiles, 43-44
NSA (National Security Agency), 33, 122
numbered point layers, 20
O

OADP (Oracle Architecture Development Process), 196, 197, 203
OBIEE (Oracle Business Intelligence Enterprise Edition), 7, 8
OBIEE connections, 182, 183
OBIEE dashboard, 180
OBIEE metadata repository, 181, 182, 184
ODI (Oracle Data Integrator), 17
ODM (Oracle Data Mining), 186-188
ODM workflows, 186-188
OEAF (Oracle Enterprise Architecture Framework), 196, 197
operating models, 199
operating systems. See also specific systems
      Endeca Server and, 31-32
      Endeca Studio and, 60
      hardening, 33
      Integrator ETL Server and, 60
operations analysis, 179-180
opinion mining, 115
optimal treatment pathways, 128-130
Oracle Architecture Development Process (OADP), 196, 197, 203
Oracle Business Intelligence Enterprise Edition. See OBIEE
Oracle Corporation, 7-8
Oracle Data Integrator (ODI), 17
Oracle Data Mining. See ODM
Oracle Endeca Information Discovery. See Endeca Information Discovery
Oracle Endeca Support, 54
Oracle Endeca Text Enrichment plug-in, 24
Oracle Endeca Text Enrichment with Sentiment Analysis plug-in, 24
Oracle Endeca Web Acquisition Toolkit plug-in, 24
Oracle Enterprise Architecture Framework (OEAF), 196, 197
Oracle Enterprise Manager 12 c, 50-51
Oracle Exalytics, 7, 32
Oracle Information Architecture Framework, 196-198
Oracle Linux, 60, 93
Oracle Technology Network, 33
Oracle WebLogic Server, 60
organization as a service, 207
organizational change, 213-214
OS-level monitoring, 47-49
.out files, 52
P

Parkinson's disease (PD), 189-193
Parkinson's Voice Initiative (PVI), 189
Partner screen, 185
patient data, 164-165
patient monitoring, 128, 189-193
patients analysis, 176-179
PD (Parkinson's disease), 189-193
PDF files, 97, 222
performance
      Endeca Server, 53-54
      Endeca Studio, 215
      Endeca Studio applications, 215
      KPIs, 118, 165
      monitoring, 45-51
perimeter security, 208
personal location data, 122
personalized insurance plans, 126
personalized medicine, 131
personnel. See employees
pervasive visualization, 217
Peters, Tom, 114
pharmaceuticals, 128
phone records, 122
.pid files, 52
pivot tables, 81-82, 83
plug-ins, 24, 30, 70
point layers, 20
point-of-sale (POS), 115
police departments, 133
pool, 35, 37
portfolio analysis, 126-127
port-of-departure use cases, 122
POS (point-of-sale), 115
predictive analytics, 188-189, 206, 208
predictive analytics services, 208
prepaid card usage, 123
private applications, 23
procedure codes, 172
processor core count, 27-30
product innovation, 125
production equipment monitoring, 132
 provisioning service, 22
public applications, 23
public safety, 133
public sector apps, 133-134
PVI (Parkinson's Voice Initiative), 189
Q

quality management, 131-132
quality standards, 210
quantum mechanics, 55
R

RAID drives, 30
RAM memory, 39
R&D resources, 130-131
reader components, 101
real-time data, 217
real-time streaming events, 217
recency bias, 85
record stores, 90, 91, 102-104
records. See also data
      CDRs, 123
      described, 12
      DMV, 134
      EMR, 164
      EQL, 33
      filters, 33
      patient, 164
      phone, 122
      voter, 145-161
Red Hat, 60
reference, 232
refinement components, 19
refinement rules, 22
remote patient monitoring, 128, 189-193
replication, 44
repository database, 61
reputation management, 125
.reqlog files, 52
request for proposals (RFP) process, 127
resource management, 54-55
resources, 221-232
results tables, 174
retail web sites, 6-7, 96
retailers
      brand reputation, 114
      consumer package goods, 118, 119-122
      CPG and, 119-122
      customer churn prevention, 118-119
      demand forecast, 121
      facial recognition, 119-120
      mobile applications, 122, 217
      time-sensitive ads, 122-123
RETURN statement, 13
RFP (request for proposals) process, 127
risk management, 127
risk profile, 125
robots, 96-98
Ross, Jeanne, 199
S

Salience Engine, 100, 106
SAN-based storage, 30
SARs (suspicious activity reports), 126
sConfig (Entity and Collection Configuration Web Service), 14
search box, 19
SEC (Securities and Exchange Commission), 127
Secure Sockets Layer. See SSL
Securities and Exchange Commission (SEC), 127
security
      access control, 208
      analytical architecture, 208-209
      big data, 212-213
      data, 202-203, 208
      data visibility, 208
      Endeca Server, 215
      NSA, 33, 122
      perimeter, 208
      recommendations, 212-213
security cameras, 119-120
security infrastructure, 208
security management apps, 134-135
security services, 208-209
seed files, 92
SELECT clause, 13
Self-Regulatory Organizations (SROs), 127
self-service data provisioning, 58
semistructured data, 5-6
Sentiment Analysis, 9, 24, 100, 114-116
sentiment scores, 117
server clusters, 12, 33-35
servers. See also Endeca Server
      application, 32
      data, 8-9
      Endeca IAS Server, 15, 16, 90, 95
       Exalytics, 7, 32
      Integrator ETL Server, 55, 90, 96, 109
      Jetty Web Server, 93, 94-95
      WebLogic. See WebLogic Server
service improvement, 134
service level agreements (SLAs), 34
services, 13-14
Shelby, Richard, 110
"skills gap," 211
SLAs (service level agreements), 34
Snowden, Edward, 122
SOAP web services, 13-14
social services, 134
SPC (statistical process control), 88
spreadsheets, 101
SQL (Structured Query Language), 4
SQLPlus, 54
SROs (Self-Regulatory Organizations), 127
SSL (Secure Sockets Layer), 32, 35-36, 215
SSL certificates, 36
stacked bar graph, 82-84
statistical process control (SPC), 88
storage layer, 204-206
strategic road map, 209-210
structured data, 5-6
Structured Query Language (SQL), 4
subject areas, 181-182
summarization bar, 19
survey responses, 117-118
suspicious activity reports (SARs), 126
system load average, 49
T

tabular data elements, 19
tag cloud component, 66-68
tag clouds
      claims analysis, 174
      described, 19
      example of, 20
      FDIC insurance fund, 79
Taleb, Nassim Nicholas, 58
TDWI (The Data Warehouse Institute), 204
telecommunications, 122-125
telemedicine, 128
telemetry data, 124
Tenent, George, 110
Term Extraction plug-in, 30
terrorist attacks, 110
text analysis, 24
text analytics engines, 117
Text Enrichment, 9, 21, 100, 102-107, 109
Text Enrichment with Sentiment Analysis, 100
Text Tagging, 100
The Data Warehouse Institute (TDWI), 204
time-sensitive retail advertisement use cases, 122-123
Tomcat servers, 60
trade analysis, 126-127
training/education, 213, 215
transformation components, 101
Trash component, 155
trial design, 130-131
Turkcell, 123-124
Twitter, 114-116
Twitter feeds, 100
U

UEK (Unbreakable Enterprise Kernel), 32, 55
Unbreakable Enterprise Kernel (UEK), 32, 55
Unified Parkinson's Disease Rating Scale (UPDRS) assessment, 189-191
unstructured data, 5-6
UPDRS (Unified Parkinson's Disease Rating Scale) assessment, 189-191
UPS example, 114-116
uptime utility, 49
URLs, 92
U.S. securities markets, 127
use cases, 113-136
      generic, 114-119
      industry, 114, 119-134
      IT, 114, 134-135
user convenience features, 20
user-driven data exploration, 58, 85
V

vertical scaling, 35
views, 21-22, 215
VIP (virtual IP) address, 35, 37
virtual IP (VIP) address, 35, 37
visualization. See data visualization
vmstat command, 49
voter fraud analytics/detection, 137-161
      election results analysis, 139-145
      implementing, 138-161
      overview, 138
      voter records analysis, 145-161
 voter fraud application, 156-161
voter recrods, 145-161
W

Ware, Colin, 217
warranty management, 131-132
WAT. See Endeca WAT
Web Acquisition Toolkit. See Endeca WAT
Web Crawler, 15, 90-93
web crawls, 90-93
web services, 13-14
web sites
      retail, 6-7, 96
      useful links, 231-232
WebLogic Server
      described, 32
      Endeca IAS installation, 93, 94
      log files, 53
      monitoring with, 46-47, 48
Whitelist Text Tagging, 100
Whitelist Text Tagging enrichment, 70, 71-73, 74
Windows 2008, 60
Windows systems
      considerations, 31
      Endeca Server on, 31
      Endeca Studio on, 60
      Endeca WAT on, 98
      Integrator ETL on, 107-108
      shortcuts, 108
workload analysis, 27-30
workspace settings, 149, 150
writer components, 101
X

XML files, 91
Z

ZFS appliance, 34



