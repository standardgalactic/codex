




SPLINTERNET



















© 2016 Scott Malcomson
Published by OR Books, New York and London
Visit our website at www.orbooks.com
All rights information: rights@orbooks.com
All rights reserved. No part of this book may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopy, recording, or any information storage retrieval system, without permission in writing from the publisher, except brief passages for review purposes.
First printing 2016
Cataloging-in-Publication data is available from the Library of Congress.
A catalog record for this book is available from the British Library.
ISBN 978-1-68219-030-2
ISBN 978-1-68219-031-9
Typeset by AarkMany Media, Chennai, India.









CONTENTS
INTRODUCTION
1.  THE VIRTUAL REALITY OF MODERN WAR
The People Counters
"Signaling Is Important as a Weapon"
Fire Control
"The Engineer and His Relation to Government"
The Missile Is the Message
Trails Through the Wilderness
SAGE, MANIAC, and MAD
2.  LIBERATION TECHNOLOGY
Geeks and Bohemians
Why California?
"Machines of Loving Grace"
The People's Computer Center
The Blind Boys of Berkeley
The Return of the State
3.  THE SPLINTERNET
Walled Gardens Full of Money
The Back Door
Mr. Slippery
CONCLUSION
Acknowledgements
About the Author









INTRODUCTION
The World Wide Web is slowly returning to Earth and its entanglements: states, laws, cultures. Cyberspace, for a host of commercial and political reasons, is becoming many cyberspaces, some of which fit distressingly well onto the old political maps of nation-states. The web has even become a battleground for states' wars. Why is this happening, and what will remain of the old, free, and anarchic web to take into the future?
Digital computing, the Internet, and eventually the web were invented and grew as part of a long line of government projects, mainly military ones, dating back to the First World War. But, beginning in the late 1960s, the Internet and geek culture split off from government, launching a period of spectacular innovation, excitement, and profit. The web became a place for enacting dreams of freedom.
Cyberspace was understood as extra-terrestrial, at once politically rebellious and apolitical, where you could have no identity at all and yet every identity was respected: the last of the great 1960s projects. No one can surpass the famous description by John Perry Barlow, who dashed off a declaration of cyber-independence while attending the World Economic Forum in Davos, Switzerland, in February 1996, less than a year after the first easily accessible web browser, Mosaic, reached the public:
Governments of the Industrial World, you weary giants of flesh and steel, I come from Cyberspace, the new home of Mind. On behalf of the future, I ask you of the past to leave us alone. You are not welcome among us. You have no sovereignty where we gather.
We have no elected government, nor are we likely to have one, so I address you with no greater authority than that with which liberty itself always speaks. I declare the global social space we are building to be naturally independent of the tyrannies you seek to impose on us. You have no moral right to rule us nor do you possess any methods of enforcement we have true reason to fear . ... Your legal concepts of property, expression, identity, movement, and context do not apply to us. They are all based on matter, and there is no matter here.
Our identities have no bodies, so, unlike you, we cannot obtain order by physical coercion . ...
In China, Germany, France, Russia, Singapore, Italy and the United States, you are trying to ward off the virus of liberty by erecting guard posts at the frontiers of Cyberspace. These may keep out the contagion for a small time, but they will not work in a world that will soon be blanketed in bit-bearing media.1
Barlow's transcendent triumphalism complemented the more earthly triumphalism that flourished after the ending of the Cold War, the globalizing consensus optimism that Davos nurtured and celebrated. The web as a solvent of sovereignty had a very strong appeal, and was soon taken up by New York Times columnist Thomas L. Friedman, a fixture at Davos, in his 1999 bestseller The Lexus and the Olive Tree:
The symbol of the Cold War system was a wall, which divided everyone. The symbol of the globalization system is a World Wide Web, which unites everyone . ... In the Cold War we reached for the hot line between the White House and the Kremlin—a symbol that we were all divided but at least someone, the two superpowers, were in charge. In the era of globalization we reach for the Internet—a symbol that we are all connected but nobody is totally in charge.2
Not totally: the United States remained a good deal more in charge than any other power, which is what led French foreign minister Hubert Védrine, in pondering the singularity of America's victory, to his rather bitter coinage "hyperpower"—like a superpower, only more so. In particular, the United States was in charge of the Internet, which had been developed, like so much else in the twentieth century, including (for the most part) digital computing itself, by the US military to serve US military purposes.
Then why did Barlow and Friedman, and nearly every other writer on the subject, not dwell on, or draw conclusions from, the World Wide Web's terrestrial past? In part, it was because their boomer generation rarely chose to take lessons from their parents' experience. In part, it was because something so exhilaratingly futuristic could only be dragged down by a consideration of its past, and the thrill of the post-Cold War period was in creating a future that had as little reference to the miserable past as possible. Besides, the Cold War victory had been "military" only in the very specific sense that one side's military had out-spent and, most important, out-innovated the other side's military; the rest of the victory, the bulk of it, was political and economic, not military. So what did it matter if the Internet had once been a military program?
But the strongest reason for neglecting the past of the Internet was that this old military project had in fact been superbly re-purposed by a trans-national engineering subculture that followed its own rules, and by a San Francisco Bay Area culture of the late 1960s and 1970s that was highly individualistic and even libertarian, unsympathetic at best to the demands of the state and sovereignty, generally pacifist, and animated by a One World view of its own. That is why it seemed as though the web might be the one 1960s project that could succeed in breaking free of the past and burying the nation-state system. You have no sovereignty where we gather ... Our identities have no bodies ... 
Has the past caught up with the web? From Russian cyberattacks and the Stuxnet virus to chronic cyberthievery from China and industrial-scale invasions of privacy, the web seems to be returning to its roots in conflict and nation-state rivalries. Giant web companies both hasten this nationalization—as they tailor products to ever more specific markets—and rebel against it as a barrier to their trans-border ambitions. Meanwhile, anxious states, fearing their economic and military dependence on the web and the vulnerability of their digital information, devote funds and political capital to fighting cryptography, building Great Firewalls, and creating "back doors."
Twenty years on, looking back on his Declaration of the Independence of Cyberspace, Barlow said, "I could also see there was never a better system [than the web] that could inherently be extended for surveillance. Ever. I knew that. I wasn't stupid. I just wanted to pretend that was not the future."3 That is unfair to the future. Without doubt, the web is and will be used for surveillance and for the projection of force, just as its forebears were. States and like-minded regions will assert control over it and most users' experience of it will be locally inflected. At the same time, the web will continue to have a global infrastructure and no one state will be able to dominate it, both because the other states won't let that happen and because the leading companies on the web will not abandon their drive for global growth. The web will be neither entirely united nor entirely divided. The web is a global private marketplace built on a government platform, not unlike the global airport system. That is more mundane than the early ecstasies of cyberspace, but it is more durable. And if the prophecies of cyberwar are someday fulfilled, at the end of the battle the airports will still be rebuilt. People will always want to fly.
 


  1.   John Perry Barlow, "A Declaration of the Independence of Cyberspace," https://projects.eff.org/~barlow/Declaration-Final.html.
  2.   Thomas L. Friedman, The Lexus and the Olive Tree, 1999, chap. 1.
  3.   David Hershkovits, "John Perry Barlow Talks Acid, Cyber-Independence and his Friendship with JFK Jr.," May 2015, http://www.papermag.com/2015/04/john_perry_barlow_acid_cyber-independence_jfk-jr.php.










CHAPTER 1.  THE VIRTUAL REALITY OF MODERN WAR
The word communication will be used here in a very broad sense to include all the procedure by which one mind may affect another. This, of course, involves not only written and oral speech, but also music, the pictorial arts, the theatre, the ballet, and in fact all human behavior. In some connections it may be desirable to use a still broader definition, namely, one which could include the procedures by means of which one mechanism (say automatic equipment to track an airplane and to compute its probable future positions) affects another mechanism (say a guided missile chasing this airplane).
—Warren Weaver, "Recent Contributions to the Mathematical Theory of Communication," in Warren Weaver and Claude Shannon, The Mathematical Theory of Communication, 19494
The web was supposed to be not of this world, but the web has also, and always, been in the world of physical power. The Internet's alternative world was built by people in love with and in fear of machines, who wanted to see how machines might communicate with each other—forming, in a sense, one big machine—and how humans could communicate within and through this machine.
Such communication, as the computing pioneer Warren Weaver foresaw, included not only music and art and speech. It also included dominating each other and killing each other. The Internet, and the computers that made it possible, came from a rather dark place, much more missile than ballet, and they might yet return there. This book is about how and why that could happen, and what might be done about it.









THE PEOPLE COUNTERS
In most histories, the new world of computing and the Internet came into being in the aftermath of the Second World War. But the vision behind it preceded that war and was constitutive of modernity: the idea that nature and human society alike were mechanistic, enacting patterns of which the individual components were mostly unaware and which actual individuals had little if any power to affect; and the accompanying desire to rule machines rather than be ruled by them.
Most people in the industrializing world of the nineteenth century had to accept the newly obvious truth that people with machines, and the capacity to invent new machines, were much more prosperous and powerful than people without them or that capacity. Societies that mastered machines first had an advantage over those that had not: they were more productive, and they were more deadly. For the United States, the formative experience in this respect was the Civil War, which was the first modern war in that killing took place on what was seen as an industrial scale. Men were slaughtered in bulk thanks to machines. The industrial North was able to fight on a different scale and in a different, more thoroughgoing way than the much less industrialized South. The "war machine" had been born.
Industrialization did not lead to war, which had always existed, but it did change war, just as it changed everything else. Social sciences like economics, natural sciences like evolution, and the mechanization of pre-industrial processes like weaving, planting and harvesting, sailing, digging, and moving from one place to another: Most human activities, including war, were steadily taken out of the animal world into the countable, machine world. The new social sciences took human relations and lent them a rational structure that made the individual human parts increasingly interchangeable. Humans were becoming parts in the machine.
The development of computing as such occurred within this broader story of mechanization. The term "computers" initially, and into the 1940s, referred to people who performed mathematical calculations for a living; among the first people to lose their jobs to computers were "computers" themselves. (As late as 1944, the US Army had "a staff of 176 computers" at its Aberdeen Proving Ground just to calculate the best angles for firing guns.)5 Calculating machines as a means to save mathematical labor had existed from the humble abacus through the devices built by the mathematician-philosophers Blaise Pascal (1623-1662) and Gottfried Wilhelm von Leibniz (1646-1716) to the Analytical Engine conceived and partially built by the mathematician-economist Charles Babbage (1791-1871). Babbage's concept included a provision for memory (what he called a "store") and a technology using perforated cards. The card idea had come from Joseph-Marie Jacquard (1752-1834), who developed perforated metal cards that contained the information sequence needed by a loom to weave a tapestry in a particular pattern. The relationship between the holes and the distances between them constituted a set of instructions that the loom could "read" in sequence in order to know when to move the threads to reproduce a particular design. As Augusta Ada Byron, Countess of Lovelace (1815-1852), another pioneer of early computing, put it, "We may say that the Analytical Engine weaves algebraical patterns just as the Jacquard-loom weaves flowers and leaves."6
Computing was never just about math, though. Computing was a way for modernity to keep up with its own ever-increasing pace—for people and machines to reach some kind of equilibrium. The greatest advance in nineteenth-century computing came about in order to speed up the processing by people of information about other people through gathering and manipulating small amounts of information about each one of them: it enabled the first data mining. The US Constitution requires that a census of the population be taken every ten years. The 1880 census included several points of information, including name, location, age, sex, and national origin. This information was written out by hand and, given population growth, the amount of time necessary to tabulate the information had become about seven years. The ever-increasing chronological span raised the prospect that the census, by the time its results were in and tabulated, would always already be out of date.
This was a group-existential paradox. Given slow calculation speeds, the final information about the US population would go ever further into the past. The resolution came from American statistician and inventor Hermann Hollerith (1860-1929), who was probably influenced by the Jacquard loom-cards. He made cards for census takers that required them to punch a hole for male or for female, one for age cohort, and so on. Hollerith's machines for reading and tabulating these cards meant that the 1890 census had twice the amount of data as the 1880 census and processed it in about a third of the time. In 1896 he founded the Tabulating Machine Company to market his system. By 1911 his company had merged with others to form the Computing-Tabulating-Recording Company, which in 1924 changed its name to International Business Machines Corporation, or IBM. That company's "punch cards" were used until the 1970s.7
It is striking that this great advance in computing was government-funded and intended to identify and classify a community based on information about each individual. The motivation was political and the effects (allocation of voting power by geography, allocation of state monies) were political too.









"SIGNALING IS IMPORTANT AS A WEAPON"
The main impetus for further development of computing was also political but was overwhelmingly due to war and the demands it placed on the processing of information. In fact, war became the geopolitical context for the growth of computing and remained so until around 1970, with results that we will see. Most of the elements for the design and construction of a basic digital computer were already available at the turn of the century. The telegraph, with Samuel Morse's code of dots and dashes, was already a binary system of information, based on electrical on/off signals sent through wire and a series of switches: digital communication over distances. Similarly, the heart of digital computing is "logic gates," each of which puts a "0" (zero) or a "1" (one) through a specified set of switches that lead to a particular single result.8 Logic gates were also known in the nineteenth century. The main element missing was a way for these collections of switches not just to compute, but to remember a result even after the power was switched off—at the most basic level, to mark and remember whether something had happened or not, the lasting trace of a zero or a one.
And here the spur to innovation came from war. In the most obvious way, war focuses the resources of society to a degree that is really unique to conflict and can be fantastically creative. Modern war had become machine war and, in the rhythm of defense and offense, technology, fueled by the immense resources of fully mobilized industrial societies, advanced machine by machine. The means for machines to remember arose in the course of the First World War.
"After the war began," the English physicist and engineer William H. Eccles (1875-1966) recalled in a chapter on triodes in his 1933 text Wireless, "because signaling is important as a weapon, the new aid to wireless [high vacuum triodes] found application with extraordinary rapidity in all the confronting forces. Under the pressure of military requirements, and the fertilizing influence of almost unlimited financial backing, an immense variety of signaling apparatus utilizing triodes was invented, tested and put into mass production during the four years of the war. The circumstances of the time thus brought about in a relatively short period the greatest revolution in technique that even wireless has yet experienced. Everything was changed. . . . Many parts of the technique of reception disappeared for ever and the possibility of amplifying oscillating currents of all amplitudes and frequencies, in apparatus for sending, as also in apparatus for receiving, transcended even the dreams of pre-war students of wireless."9
Eccles was a prominent figure; in 1917-18 he served as director of the Admiralty Electrical Engineering Laboratory, working on developing transmitters for the army and submarine locators for the navy. Along the way he, together with his colleague Frank Wilfred Jordan, developed a relay circuit of an unusual kind: once triggered, it was able to stay on (with an output of "1") even when the two inputs had been switched off. Eccles and Jordan patented the relay circuit design, while the Admiralty itself patented a device that put the relay into action.10
The Eccles-Jordan circuit became known as the "flip-flop circuit" and was the missing piece in the small set of circuits necessary for digital computing. The flip-flop retained evidence of a choice having been made between zero and one. (As Claude Shannon wrote three decades later, using the new term "bit" to describe a binary digit, a flip-flop "can store one bit of information.")11 The flip-flop made memory possible in electronic form. Eccles and Jordan didn't realize it, and neither did the Admiralty, but their relay circuit, developed in state-funded research on telegraphy and telephony, would in a few decades become the basis for digital memory.12
The Great War was the first to have large-scale state funding for scientific research, in significant part because chemical weapons were such a feature of it. It was sometimes called "the chemists' war." After the Germans' first chlorine-gas attack, the British undertook a massive scientific effort to come up with their own chemical weapons, building a facility from scratch at Porton Down. In the United States, the National Academy of Sciences (NAS)—itself founded in 1863 to help the North bring scientific expertise to bear on defeating the South—offered its services to President Woodrow Wilson after its spring meeting in 1916, "when it was seen," according to the official history, "that the United States would probably become involved in the World War."13
President Wilson accepted the offer. (Thomas Edison had already begun a similar effort; as he told the New York Times at the start of 1915, "The present war has taught the world that killing men in war is a scientific proposition.")14 The national academy's offer resulted in the National Research Council (NRC), which would be, in effect, the birthplace of the military-industrial-academic complex. "During the ensuing two years," the official history continues, "the National Research Council was engaged largely in the service of the Federal Government, acting as the Department of Science and Research of the Council of National Defense, and also as the Science and Research Division of the Signal Corps of the Army, and in cooperative relationship with other branches of the Government to meet its military and naval needs." The NRC had a Military Committee composed of representatives of the army and navy and various scientific bureaus of the government, and a Division of Foreign Relations, one of whose five main purposes was "to keep in close touch with the Department of State, and to inform the Department of pending international scientific and technical questions in which the government may be interested." Scientific research was likewise to be shaped by intelligence needs, as explained by Vice Admiral William Sims in a circular of April 18, 1918: "The Secretaries of War and of the Navy, by joint action, and with the approval of the Council of National Defense, authorized and approved the organization, through the National Research Council, of a joint information committee in London and Paris. The branch committees are intended to work in close cooperation with the officers of the Military and Naval Intelligence Services." The main joint committee in Washington would have three members, one from the NRC together with the heads of military and naval intelligence.15
The NRC was successful enough that President Wilson, in a May 1918 executive order, asked the National Academy of Sciences to perpetuate it, "with the object of increasing knowledge, of strengthening the national defense, and contributing in other ways to the public welfare," adding that it should "direct the attention of scientific and technical investigators to the present importance of military and industrial problems in connection with the War, and to aid in the solution of these problems." Immediately after the war, the Carnegie Corporation stepped in to supplement the funding provided by the government.16
The NRC was a formative place and experience for many of the people who would construct and unite the elements of digital computing and the Internet. Warren Weaver, for example—who thirty years later would write of music, ballet, and anti-aircraft ballistics in his introduction to Claude Shannon's breakthrough book, The Mathematical Theory of Communication—spent a wartime year at what he called "an organization closely related to the National Research Council,"17 beginning in 1917, as a lieutenant developing turn and bank indicators for fighter pilots. In the interwar years, computer pioneers like Frank Jewett (president of AT&T's Bell Labs between 1925-40 and active on several wartime committees), NRC wartime vice-chairman Robert Millikan, Vannevar Bush (who also saw wartime service at the NRC beginning in 1917), Elmer Sperry, and George Ellery Hale were very involved at the Council and in particular in its Division of Physical Sciences and the Division of Engineering and Industrial Research (the one part of the NRC housed in New York City rather than Washington, DC). Hale had been the principal force behind getting the NRC established as part of the war effort. He also was the main architect of Caltech, where Hale's friend Robert Millikan served as chairman of the executive council (effectively president) from 1921 to 1945. Jewett ran the NRC's engineering and industrial research division in 1923-27, followed by Elmer Sperry (1927-30) and then Dugald Jackson (1930-33), who was also the man most responsible for building MIT into a top engineering school. Vannevar Bush took the NRC post over from Jackson in 1933 while serving concurrently (1931-38) as MIT's first dean of engineering.18
Every one of these men would be crucial to the invention of digital computing in the United States, and their first experiences of scientific cooperation and innovation on a large scale were a result of the First World War. Indeed, the cross-disciplinary innovative model itself was a result of war. The NRC was the principal institution for bringing together technological innovation, science, and industrial production methods for the purpose of fighting a war, and this quickly produced a very powerful and distinctive scientific subculture: one with resources (though not lavish at the NRC itself), extreme pressure, an intense sense of purpose, cross-disciplinary cooperation, and the quirky combining of a robust exchange of ideas with secrecy and information compartmentalization. Cross-disciplinary work, in particular, became a necessity rather than a luxury. Modern machine war made careerist territorialism, boundary-patrolling, and mutual suspicion—all normal professional practice in peacetime science—suddenly dangerous. George Ellery Hale had advocated interdisciplinary efforts as early as 1909,19 but at the NRC, as one of its chroniclers wrote concerning the Division of Physical Sciences, "For perhaps the first time, it was realized that, in addition to direct results, the indirect benefits of directed cooperative effort among scientists was [sic] very great." The division valued "spontaneous cooperation" and made a practice of having members research new developments in their own and other fields and share their findings with everyone else in the division.
These few years were formative ones for the scientists involved and were a creative respite from the increasing academic specialization of science. But military funding mostly disappeared after World War I. In the United States, it peaked at 22 percent of GDP at the end of the war but soon dropped to 1 percent (the same as before the war). Congress retained and codified, in the 1920 National Defense Act, the prewar notion of a small professional army that could be expanded by the addition of citizen soldiers in the event of war, although the act did add a strong National Guard and Reserves to the prewar model.20 Regular troop strength was capped at 280,000; the actual number in 1920 was 200,000, which Congress reduced to 150,000 by the end of 1921 and 125,000 a year later. Throughout the decade, Congress was reluctant to fund defense, often leaving soldiers with antiquated weapons and based at forts built to fight the Indian Wars of the previous century. By 1933 the US Army ranked just seventeenth in the world in terms of size.21
Like other major powers, the United States accepted the premise that another global war could be avoided, even without US participation in the League of Nations. Further, the widespread belief that arms buildups themselves had been one cause of the war made disarmament, or non-armament, a foreign-policy priority and, especially for the United States, one that was enshrined in postwar treaties. Policymakers focused on navies because they were then the principal means for power projection on an international basis. The United States convened the Washington Naval Conference in 1922 to restrict the size of fleets, particularly "capital ships" such as battleships. The United States, Britain, and Japan—the three main naval powers—all agreed to significant restrictions on ship size and quantities for a ten-year period.22 In the 1928 Pact of Paris (the Kellogg-Briand Pact), sponsored by France and the United States, war as an instrument of policy was itself rejected.









FIRE CONTROL
The Washington naval treaty, renewed in London in 1930, classified ships by tonnage rather than by the effectiveness of their weaponry. That left an opening. So the interwar US Navy focused its research on top-secret programs to improve the accuracy of guns: what is called "fire control." These programs laid much of the basis for digital computing.
Naval gunnery was, in essence, a contest for survival between two or more machine-enhanced humans at a distance. The amount of force projected mainly involved explosive size and was a known science; the innovative possibilities were in improving accuracy, and with guns becoming ever more powerful, ships stayed ever farther away to keep out of range. This meant that, to aim fire with any accuracy, you needed to be able to guess, based on incomplete information, where your target was likely to be. You had to be able to imagine your target's world without quite being able to see it—to create a dynamic virtual model of your enemy's reality.
Fire control involved a lot of math. The variables included the speed of the attacking and target ships, the pitch and roll of the firing ship, distance to target, and the speed and arc of the shell. With normal gunnery, these variables would be fed in advance into firing tables, which the gunner, or the commander of a battery, could consult: you would run through the tables to find out when—with a distance of a, your ship moving at a speed of b on a heading of c, the target at a speed of d on a heading of e, your ship pitching at an angle of f at a rate of g, the shell having a speed of h and an arc of i—you should hit the trigger. The firing-table data were assembled by human "computers."
The main challenges were in getting accurate ongoing data, adjusting the system rapidly enough (given multiple guns arrayed in batteries), and dealing with weak data (such as possible evasive action by the target ship). The navy worked very closely and under tight conditions of secrecy with a small number of private companies such as Ford (not the one of Henry Ford, who was among other things a pacifist, but of Hannibal Ford, in New York), Sperry, and General Electric, and with academic institutions. MIT came into prominence as an engineering school with military contracts, but Harvard, the University of Pennsylvania's Moore School—founded in 1923—and others were closely involved as well. The realization that, as Edison said, "war is a scientific proposition" had brought about a military-industrial-academic network in very short order.
Private industry—given official semi-pacifism and arms-control treaties and the resulting lack of government funding—played an outsized role in the 1920s and '30s. The growth of private scientific research coincided precisely with what was called the Machine Age, with its aeroplanes and electrification and automobiles and Art Deco streamlining: a time of optimism and excitement. American industry had been slow to see the benefits of scientific research, but when it made the shift it did so massively. The number of industrial research laboratories went from 297 in 1920 to almost 1,000 in 1927 and 1,625 in 1931. Promoting such laboratories was one of the main activities of the NRC's engineering and industrial research division under Frank Jewett, Elmer Sperry, and Dugald Jackson over those same years, and by 1933, when Vannevar Bush took over direction of the division, there was a hope that it might become self-sustaining, supported by industry as a coordinating body for the nation's private scientific research.23
Fire control was a central project for the emergent military-industrial-academic complex. The initial turning point had occurred back at the Battle of Jutland in 1916. Guns had already become powerful enough that ships kept distant from the enemy—often too distant to see. As British ships confronted their German enemy at Jutland, they achieved an alarmingly low hit rate of 3 percent. Standing out from this miserable performance was the one ship that had a mechanized calculating system.24
Elmer Sperry's company was also on the scene. It had provided the Sperry Gyrocompass used by many of the British ships—the Admiralty Lords sent Sperry a thank-you note in August 1916—and the success of the one ship at Jutland with a mechanized fire-control system inspired Sperry to a bit of industrial espionage.25 He quickly prepared the Sperry Fire Control System, including a "battle tracer" developed by Hannibal Ford's Ford Instrument Company. Sperry and Ford (soon joined by General Electric) were pioneers in bringing high technology to bear on future war-fighting, financed by government contracts and operating in secret.
Dependence on classified contracts has the disadvantage of limiting a company's client base, and under Depression conditions Ford Instrument went down to three-day work weeks in the early 1930s. But Ford kept issuing improvements on its Range Keepers, from the Mark 1 of World War I on to the much more sophisticated instruments of the 1920s and '30s, which had now to handle the more difficult challenge of directing anti-aircraft fire. Ford was purchased by Sperry and the army's anti-aircraft contracts went there too.
The fire-control system these companies worked on integrated what was later called "feedback." The idea built on the use of a regulator or "servo-mechanism" that kept a ship on course by adjusting the rudder when winds or other factors pushed the ship in some new direction, the servo being guided by, for example, a compass (or one of Sperry's gyroscopes). The combination of rudder and servo, compass, and ship created a "closed loop" that automatically steered the ship from A to B. This was the basis of information feedback systems.
The essence of naval fire-control, then anti-aircraft, problems was how to develop "control systems" that could very rapidly calculate the relationships among the shifting variables and aim and fire guns accordingly. For that, a central device had to both calculate and direct the guns (or the gunners). This device was called a "computer." It was the first use of the term to describe a machine. The Ford Computer Mark 1 in the Gun Director Mark 37 was tested in 1939 and became standard in the war. The (non-digital) computer was born.26









"THE ENGINEER AND HIS RELATION TO GOVERNMENT"
The optimism of the Machine Age evaporated in the Depression, along with most military spending, 5,000 US banks, 9 million savings accounts, 85,000 businesses, and $26 billion in wages.27 But the Depression did not last that long, and the energies of the military-industrial-academic complex returned rapidly, particularly once Japan and Germany left the League of Nations in 1933 and Japan renounced the Washington Naval Treaty in 1934. The postwar political structure that had kept the war machine in check was crumbling. And the knowledge developed among a small circle of people in how to control and direct gunfire, using control systems built around a mechanical computer, came into its own in a new political system aimed at constant innovation in the service of defense.
The central figure in this was Vannevar Bush. A tall, pipe-smoking New Englander, a tinkerer, funny and imperious and willful, he was deeply devoted to the still-new idea of the engineer as a kind of scientific cross-disciplinary superhero. Engineering as a pursuit worthy of an academic discipline was not quite as old as the twentieth century. Bush had been only the fifth doctor of engineering graduated by MIT. Engineers had something of a chip on their shoulder, or something to prove; the Machine Age had been their first chance. It was a mixed success. Herbert Hoover, for instance, was a celebrated and wealthy engineer and thus a balm to the insecurities of all engineers when he became president in 1929. His presidency, however, coincided precisely with the Wall Street crash and the worst years of the Great Depression, and many attributed the Depression to an excessive love of the machine and of industry. These were blamed for destroying jobs and turning free people into servants of an industrial capitalism that really advanced only the interests of its relatively few masters. ("The growing complexity of life," Bush admitted in a 1937 speech titled "The Engineer and His Relation to Government," "tends to make men cogs.")28 Certainly the miseries of the Depression did not look like a very good outcome for the Machine Age.
But it was also not hard to turn the terms of this into the opposite: that the solution to Depression was more Machine Age, that the future belonged to those who understood the machines, and that such people were more or less fated to run a complex world that, after all, was based on competitive states, each hoping to dominate the others, or at least defend against them, on the basis of the strength of its machines. To oppose this would be to oppose modernity. Elmer Sperry sounded a note of rather peevish optimism in the grim year of 1930: "If modern civilization is troubled in its soul about the so-called evils of the machine, perhaps it is because it has not thought its way through its own problems . . . Rather than to make scapegoats of engineers, it might be wiser to lay before them a work programme."29
A work program was exactly what Vannevar Bush had in mind. "The world is growing smaller, and it is becoming crowded," Bush said later in the decade. "The race for economic domination becomes a race, from which we [in the United States] only partially are separated, for military supremacy. The burden on government increases, and the problems arising are more and more beyond the true comprehension of the proletariat."30 Under such conditions, the proletariat would reliably turn toward what Bush called "absolutism," whether of the right or the left. The American engineer, by contrast, had the knowledge and skills to resist this tendency and would instead preserve the values of pioneer individualism, taking them into a new era: "The geographical frontiers have disappeared, but the frontiers of science and technology still remain. Those qualities which built a trail into the wilderness can still build trails in the technological advance. The same qualities of courage, resourcefulness and independence which opened the nation are as necessary to-day as ever."31
Bush's program involved rallying industry, academia, and government in a collective project of armament. Initially, the vehicle was to have been the National Research Council's Division of Engineering and Industrial Research, which Bush took over in 1933. But the NRC by this time was not easily roused, nor was American pacifism easily countered. (A month after Italy invaded Ethiopia, in 1935, Bush thought to suggest in a speech at Tufts: "Perhaps the worker on antiaircraft is more effectively a worker for peace than the brother who condemns him?")32 Bush moved to Washington to head the Carnegie Institution, while retaining his NRC berth, and tried to find a way to mobilize the technology elite for war.
It wasn't easy. Isolationism was strong, even as Germany rolled across Europe and Japan occupied more and more of China. German air power was formidable (so was Japan's), one reason why Bush, in a 1939 letter to Herbert Hoover, argued, a bit forlornly, "This whole world situation would be much altered if there were an effective defense against aircraft."33 At the end of that year he gave up the NRC post in despair, but quickly moved on to advocate an alternative: the National Defense Research Committee. Bush rallied his friends and secured, on June 12, 1940, two days before the fall of Paris, a meeting with President Roosevelt. Fifteen minutes later he had approval for his new committee and the assurance of a direct line to the president. His first-year budget was $6.4 million. The scientist presidents of Harvard (James Conant) and MIT (Karl Compton) were immediately recruited as senior advisors, as were Bell Labs head Frank Jewett (now also president of the National Academy of Sciences) and Richard Tolman of Caltech. These men joined representatives of the army and navy and the federal patent commissioner to form the NDRC's core leadership. Bush later described the NDRC as "an end run, a grab by which a small company of scientists and engineers, acting outside established channels, got hold of the authority and money for the program of developing new weapons."34 Because it also allowed scientists under contract to continue working in their home labs and institutions, the NDRC transformed the relationship between scientific research, private industry, and government, especially the military, and on a scale never before seen.









THE MISSILE IS THE MESSAGE
One of Bush's first moves, in early July, was to convince Warren Weaver to head up a special NDRC committee on fire control. They had both had their first experiences of military technology on NRC projects in World War I, and had become friends in the 1930s, when Weaver, as head of the Rockefeller Foundation, financed the "differential analyzer" program at MIT, which was Bush's main project there—an analog machine for high-speed mathematics that became critical to doing the math for firing tables. Weaver had strong experience leading cross-disciplinary scientific teams; he once put together mathematicians, physicists, and biologists in a project for which he coined the term "molecular biology."35
Fire control would be a central NDRC preoccupation. Bush, as in his letter to former president Hoover, had been worrying about anti-aircraft defenses. He mentioned them in a March 1939 note to Jewett at Bell Labs, and he singled the problem out in his proposal to Roosevelt.36 Warren Weaver spent July of 1940 studying the gun directors of Hannibal Ford and Elmer Sperry and familiarizing himself with servo-mechanisms and feedback. He also brought in Samuel Caldwell from MIT—a former graduate student of Bush's who had worked on the differential analyzer—and Thornton Fry from Bell Labs to join his core team. The academic-military-industrial complex that had begun in 1916 was now again in full effect, and even with many of the same personnel.37
Weaver was a quick study. He formed a thorough agenda over the summer—it included "analysis of function of computer, including higher order derivatives"—and the fire-control committee first met on the margins of the American Mathematical Society's meeting that September, where George Stibitz of Bell Labs famously demonstrated his own Model 1 Relay Computer. (Thornton Fry had been Stibitz's director at Bell and suggested building the machine; Stibitz soon joined the fire-control team.) The first contracts went out in December. Weaver personally managed the contracts for Claude Shannon, "Mathematical Studies Relating to Fire Control" (Dec. 1940-Oct. 1941, $3,044)—Weaver wanted Shannon to develop a standardized notation system and graphical language for mechanical computers—and for the mathematician Norbert Wiener, "General Mathematical Theory of Prediction and Applications." Between them, they would lay much of the basis for digital computing.38
Claude Shannon had earned undergraduate degrees in both electrical engineering and math and after graduation in 1936 saw an advertisement for a research assistant position at MIT. The job was to help operate Vannevar Bush's differential analyzer. Shannon got to know both Bush and the circuits of electromechanical relay switches that partly controlled the analyzer. He spent the summer of 1937 at Bell Labs, also working with relays, and decided there needed to be a better way to diagram circuits. Thinking through the problem of representing relay circuits mathematically, he recalled the algebra of logic developed by George Boole in the mid-nineteenth century—Shannon had learned of Boole in an undergraduate philosophy class—and realized that electrical relay switches, with their two main states of closed and open, resembled Boolean logic with its representation of false and true in logic tables; using the digits 0 and 1 for closed and open, one could write theorems representing the different types of relay-switch circuits.39
Shannon's goal was to make electrical-relay circuits as simple as possible by abstracting them into algebraic logic that could admit only one best solution. He wanted, he wrote, "to find the circuit requiring the least number of contacts . . . The theorems given above are always sufficient to do this. A little practice in the manipulation of these symbols is all that is required."40 Perhaps; but no one had quite seen it this way before, so it was left to Shannon to put together the fundamental logic of digital circuitry, which would soon be the basis for the invention of digital computers. Shannon went on to write his doctoral thesis under Bush's direction before joining the NDRC's fire-control division. Samuel Caldwell, in the division's inner circle, had been one of Shannon's master's-thesis advisors.41
Norbert Wiener was older than Shannon—who had taken one of his classes at MIT—and had his first exposure to fire-control problems during World War I, at the age of twenty-three, when he took a job as a "computer" putting together firing tables at the US Army's Aberdeen Proving Ground.42 He worked with Bush on the differential analyzer at MIT in the mid-1920s and said of him, "he thinks with his hands as well as his brain," just as he said of himself, "it is utterly beyond me even to put two wires together so they will make a satisfactory contact."43 So they were a curious pair. Wiener wrote a mathematical appendix for Bush's 1929 book Operational Circuit Analysis, and in thanking him Bush claimed, "I did not know an engineer and a mathematician could have such good times together."44
Wiener had also been at the American Mathematical Society meeting on September 11, 1940, and saw George Stibitz demonstrate his Model 1 Relay Computer in a hallway outside a meeting room. Now that Wiener was on board with the NDRC fire-control program, he wrote later in September a memo to Bush endorsing Stibitz's decision at Bell to use a Base 2 system—that is, a digital system of zeroes and ones—for computing, adding that the machine's operations should, as he later summarized, "be laid out on the machine itself so that there should be no human intervention . . . all logical decisions necessary for this should be built into the machine itself," which should also "contain an apparatus for the storage of data, which should record them quickly, hold them firmly until erasure, read them quickly, erase them quickly, and then be immediately available for the storage of new material."45 Wiener's plan had all the elements of Alan Turing's machine architecture (proposed in 1936) and of a modern computer, with the very important exception of programs stored in the machine's memory itself.
Stibitz's pioneering computer had not much interested his superiors at Bell Labs, whether in itself as a calculating machine or for its having worked remotely. (The input device had been at the society meeting in Dartmouth, New Hampshire, and it communicated with the main processing unit in New York via telephone lines.) Bell only got interested when the demands from the US military for faster computing to improve anti-aircraft gunnery made Stibitz's machine seem worth pursuing. The aerial attacks on US ships at Pearl Harbor in December 1941, which brought the United States into World War II, had shown beyond dispute the importance of anti-aircraft systems. Bell had Model 2 ready by 1943; Models 3 and 4 ($65,000 each) in 1944 and early 1945; and finally the daunting Model 5 (1946; $500,000), the first to have what could be considered its own "operating system." As for remote access to computers, it would not be taken up for several more years, after the war.46
Similarly, Wiener's own outline for a digital electronic computer got a cool response from Bush, who thought the project would be too much "of the long-range type."47 Wiener spent the rest of the war working (on a separate NDRC contract) on the mathematics of predicting the movements of hostile airplanes according to probability, crucial work for later computing because it showed how a machine might register error and feed that back into its calculations, accounting for changed statistical probabilities and improving over time. His findings on this and other problems were compiled in a classified volume that had great influence on other engineers in the wartime group. (The book had a yellow cover, and its math was forbidding enough to give it the informal title "The Yellow Peril.")48 Another observer of the Stibitz demonstration in New Hampshire, John Mauchly, took the ideas south to the University of Pennsylvania and the army's Aberdeen Proving Ground, and eventually, after initial rejection, secured funding for an enormous computer called ENIAC, which pointed the way to storage of programs in the computer's own memory.49
The British were building a similar machine, called Colossus, under strict wartime secrecy. The British arrived at computing principally through code-breaking, which did not have the command-and-control or virtual-reality aspects of the trajectory followed by American engineers since 1917. It did have the same arms-race quality: wartime enemies always try to intercept the other side's communications; encryption leads to decryption and renewed encryption, and from the beginning digital computing was entwined in this state-led competitive cycle. Decryption computers were more strictly mathematical; this was where Alan Turing came closest to the world of engineers.
Tommy Flowers was the key designer of Colossus, which exceeded in many ways the American machines of the time. Such top-secret projects were intensely collaborative, as Flowers later recalled: "It was a great time in my life—it spoilt me for when I came back to mundane things with ordinary people." Flowers and his team built ten Colossi by the German surrender; the machines played a key role in decrypting German plans for countering Normandy, among other cryptanalysis achievements. The relationship between the US and British programs was sometimes close and always carefully calculated. A young US Navy analyst seconded to the Colossus project, Howard Campaigne, went on to become the chief of research at the National Security Agency. He once recalled being grateful in wartime winters for the warmth of all those humming vacuum tubes.
Flowers tried after the war to take some of the Colossus concepts into the private sector but could not get a bank loan. But then Bell Labs had given Wiener a cold shoulder when he presented his probability work and had doubts about the value of Stibitz's machine, and Weaver and the NDRC did not believe in ENIAC, although in retrospect it was probably the most important computer of its time.50
But who ever said scientific progress had to make sense? The through-line in all these researches is not that the best ideas won out, building on the best that had come before; rather it is that people with money focused on solving their immediate problems as rapidly as they could, which is understandable given that the problems they were facing were attack and death. That is why the development of digital computing was principally an unintended byproduct of efforts to improve the accuracy of gunfire against moving targets. As Herman Goldstine, who was instrumental in the development of ENIAC, put it, "the ballistical needs of the United States were to be a primary incentive to the development of the modern computer," and the automation of "the production of firing and bombing tables and related gun control data" was "the raison d'être for the first electronic digital computer."51 The crucial fact, in this story, was that the radar-based control systems developed along the way meant that the final German aerial offensive on Britain failed. Almost simultaneously with D-Day and the Allied landing on continental Europe, the Germans began launching unmanned aerial vehicles called V-1 rockets or, by the Allies, buzz-bombs. The first of these to hit London killed eight people on June 13, 1944, a week after D-Day. By August 12, the head of British anti-aircraft wrote to the American general George Marshall, "The equipment you have sent us is absolutely first class, and every day we are getting better results with it . . . . Our percentage of 'kills' is not high enough, but the curve is going up at a nice pace . . . . As the troops get more expert I have no doubt very few bombs will reach London." And that proved to be the case.52









TRAILS THROUGH THE WILDERNESS
From a popular point of view, the leading scientific achievement of World War II was not the computer (or radar) but the atomic bomb, which is why this had been the physicists' war as World War I had been the chemists' war. Such weapons presented humans, for the first time, with the chance to destroy each other with a thoroughness and rapidity that made war, potentially, a final act for the species in general. Given the tight association of scientific advance with military carnage since at least 1861—given also the morally corrosive acceptance that civilians in a major war were simply unarmed combatants, and the disturbing merging of men and machines, often to the detriment of the former, that had been constitutive of modernity—the central role played by the finest scientists in the momentary snuffing of many tens of thousands of lives did not come as a surprise. But atomic weapons did establish the new fact that there could be a literal ending of collective human existence, and that if this ending were to come about it would be as a result of very sophisticated technology. Modernity, so determinedly anti-religious, had manufactured its own secular Apocalypse.
Was there any way to restore equilibrium? Vannevar Bush, who had supported the dropping of the atomic bomb, agitated for arms control and in particular for bans on testing. He was horrified by the decision to go ahead with developing the hydrogen bomb.53 As before the war, he thought government money should be put toward developing defenses against attack. Immediately after the war he advocated talking with the Russians, and later he rejected Eisenhower's doctrine of "massive retaliation." He also believed that the first two world wars had been much too close as a result of Western reluctance to invest in defensive technologies in advance rather than innovate under extreme pressure at the last minute. This however implied that government spending on science would be inherently military. That was not quite the implication that Bush wanted but it was the reality, and US military spending would stay at or above 10 percent of GDP into the 1960s.54
As for computers, Bush fantasized that they might give us back our memories. Like many in his generation, Bush had begun worrying in the 1930s that the world contained too much information. As with the 1880 US Federal Census, information was exceeding time; there needed to be a way to speed up the processing of information, or improve its absorption, so that time would return to some more sensible pace. In 1933, Bush had written a light-hearted essay in which he imagined a traveler from the future being amused at how a person of the 1930s had to struggle to deal with information: "The idea that one might have the contents of a thousand volumes located in a couple of cubic feet in a desk, so that by depressing a few keys one could have a given page instantly projected before him, was regarded as the wildest source of fancy." In this bit of science fiction, Bush imagined a machine that could compress information in a small space but also render it retrievable to an individual. He expanded the idea through the decade but didn't publish a new version until 1945, in the Atlantic Monthly: "Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, 'memex' will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged supplement to his memory." Bush imagined a user creating "trails" through this information, which would be coded by keywords so that information would be organized by "association": "Selection by association, rather than by indexing, may yet be mechanized. One cannot hope thus to equal the speed and flexibility with which the mind follows an associative trail, but it should be possible to beat the mind decisively in regard to the permanence and clarity of the items resurrected from storage." A person could even take the associative trail he had blazed through the information and make a copy for other people's memexes. Then they could go down that trail too.55
The memex article was immensely popular. The New York Times, Life, and Time all did features on it. Life headlined: "BUILDING 'TRAILS' OF THOUGHT ON THE MEMEX—UNLIKE MEMORY, THEY WOULD NEVER FADE."56 This vision of a personal computer, with an associative search engine, keywords, and sharing of information across machines, was one direction computing could go in (and eventually did).
Bush never liked binary or digital engineering. His biographer, Pascal Zachary, believes that Bush "was baffled by code." Bush found it deeply irritating that people were "obsessed with the idea of digital machines" and confessed in 1950 that, when he surveyed the computing world, "I am so confessedly rusty on this whole business I have to hang on by my eyebrows when the discussion really gets going."57 He preferred metal. But speed was essential to computing, and electronic circuitry was faster than any other kind, certainly anything in metal. It was well suited to binary, or digital, calculating and good for storing information or, more precisely, for making information persist over time in electronic form. Claude Shannon's earlier work linking binary calculation with circuitry and Boolean logic had more or less ensured the eventual dominance of digital computing, and the awareness of the protean nature of electricity made it highly likely that, eventually, computing would be electronic. As Frank Jewett had written back in 1935, "We are prone to think and, what is worse, to act in terms of telegraphy, telephony, radio broadcasting, telephotography, or television, as though they were things apart. They are merely variant parts of a common applied science. One and all, they depend for their functioning and utility on the transmission to a distance of some form of electrical energy whose proper manipulation makes possible substantially instantaneous transfer of intelligence."58 The idea of electronic machines that could instantly move information about was beautifully fluid and all-encompassing, just as Warren Weaver had imagined that the information of a ballet and the information of a missile might be the same.
It was left to Norbert Wiener to synthesize all this. The work he had done on fire control led him to believe that "the problems of control engineering and communication engineering were inseparable." He and his colleagues then came to see, as he wrote in 1948, "the essential unity of the set of problems centering about communication, control, and statistical mechanics . . . and as happens so often to scientists, we have been forced to coin at least one artificial neo-Greek expression . . . . We have decided to call the entire field of control and communication theory, whether in the machine or in the animal, by the name Cybernetics." Wiener's 1948 book of that name was as huge a hit as Bush's memex essay. It brought rampant public speculation about "machine brains."59 Once you accepted that most everything was "information" and that all information could be put into electronic form and then communicated to anywhere at any time—and that electronic information systems could not just have memory but could use their memory to, like a good gunner, incorporate feedback and teach themselves—it was a small step to envision machines that could, in some fashion, think. Once they could think, they would be able to do many of the things people did, and therefore replace them.
Many said this would liberate humans from drudgery, leaving them free to pursue higher callings. Wiener, who had of course been a human gunnery computer in World War I and had theorized his way to ever more perfect gunnery under contract to the NDRC in World War II, wasn't so sure: "It has long been clear to me that the modern ultra-rapid computing machine was in principle an ideal central nervous system to an apparatus for automatic control ... [W]e are already in a position to construct artificial machines of almost any degree of elaborateness of performance. Long before Nagasaki and the public awareness of the atomic bomb, it had occurred to me that we were here in the presence of another social potentiality of unheard-of importance for good and for evil ... For one thing, it makes the metaphorical dominance of the machines, as envisioned by Samuel Butler, a most immediate and non-metaphorical problem."60
Wiener's main worry was that humans, with their labor taken over by machines, would not go on to higher things at all. They would simply be devalued. He felt this had happened in the first industrial revolution—"There is no rate of pay at which a United States pick-and-shovel laborer can live which is low enough to compete with the work of a steam shovel"—and would happen again with the second, computer revolution: "[J]ust as the skilled carpenter, the skilled mechanic, the skilled dressmaker have in some degree survived the first industrial revolution, so the skilled scientist and the skilled administrator may survive the second. However, taking the second revolution as accomplished, the average human of mediocre attainments has nothing to sell that it is worth anyone's money to buy. The answer, of course, is to have a society based on human values other than buying and selling." Wiener, in his earnest way, then explains that he did indeed mount a modest personal effort to get society away from its focus on buying and selling but had not made much headway. So the dream of easy communication might well be put aside by the wish for control and power: "Those of us who have contributed to the new science of cybernetics thus stand in a moral position which is, to say the least, not very comfortable. We have contributed to the initiation of a new science which, as I have said, embraces technical developments with great possibilities for good and for evil. We can only hand it over into the world that exists about us, and this is the world of Belsen [a Nazi extermination camp] and Hiroshima. We do not even have the choice of suppressing these new technical developments. They belong to the age . . . As we have seen, there are those who hope that the good of a better understanding of man and society which is offered by this new field of work may anticipate and outweigh the incidental contribution we are making to the concentration of power . . . . I write in 1947, and I am compelled to say that it is a very slight hope."61









SAGE, MANIAC, AND MAD
Wiener, after his years of working with guns, became a pacifist and refused to work any longer on military projects. US defense spending plummeted from its wartime high of 42 percent and many small contractors shut their doors. Most of the scientists associated with the NDRC and its associated organizations—after spending roughly half a billion dollars—went back to their companies and campuses.62
The Soviet Union's successful testing of an atomic bomb in 1949 and improvements in the range of high-altitude bombers meant that air defense, and fire control, remained US military priorities. Many government agencies, notably the Atomic Energy Commission (part of the Department of Energy), the National Bureau of Standards, and the Office of Naval Research (ONR), built their own computers. The pioneering ENIAC computer's first major assignment was to perform calculations for the hydrogen bomb project; by the late 1940s the AEC's Los Alamos National Laboratory, where the Manhattan Project had been based, had built its own ENIAC replacement, called MANIAC.63
Private companies played a major role, too, alongside universities like MIT and Princeton. (Princeton's John von Neumann, with ONR funding, developed the basis for MANIAC.) At the army's Aberdeen gunnery range, the lead engineers on ENIAC formed a company in June 1946 to manufacture computers. Despite a large contract with the Census Bureau, the company foundered and was bought by Remington Rand (later Sperry Rand) in 1950; its first major computing product, the Univac, was delivered to the Census Bureau the following year. (The next two were for the Air Materiel Command.)64 Remington Rand also bought a second company, Engineering Research Associates, that had been formed from wartime programs and personnel. This capacity enabled Remington Rand, which had been using electromechanical punchcard systems at the end of the war, to become the second biggest maker of large computers, especially multi-purpose stored-program computers, by 1950. The first was IBM, which moved after the war into card-programmed electric calculators. IBM at first held itself out of the government-contract game because it wanted to guard its patents, but by 1950 government was spending enough on computing—and enough small competitors were cropping up to satisfy the government market—that IBM changed its mind.65 Descended from the company Hermann Hollerith had formed after designing the first card-computer system for the 1890 census, IBM was a formidable corporation and soon landed (in 1952) the definitive government contract: an air defense system called the Semi-Automatic Ground Environment, or SAGE.
SAGE came about at the initiative of an MIT professor, George Valley, who in 1949, the year of the Soviet bomb test, had lobbied the military to improve US air defenses against long-range bombers. An initial study resulted in MIT creating, at air force request, what would become the Lincoln Laboratory, which dedicated itself to SAGE. In 1951, SAGE took over an existing line of naval research into high-speed computing (especially magnetic-core memory, which it pioneered). From 1952 on, IBM and MIT were the key collaborators in developing a computer that could take in about 100 inputs (principally from ground, seaborne, and airborne radars) to track and identify aircraft approaching the United States.66 The press release called it, of course, an "electronic brain."67 The air force bought forty-six, two each for twenty-three command centers across American territory—the first online, real-time, geographically distributed computer network. Apart from magnetic-core memory, the project's innovations included digital phone-line transmission and modems, the use of real-time software, time-shared operating systems, and other aspects of computing that became standard for the industry. IBM made about half a billion dollars from the project, solidifying its position as the preeminent computer maker. SAGE stayed in operation until 1983.68
The development of digital electronic computing as a way to rapidly perform partial differential equations concerning a moving target, and to use that information quickly in order to craft and execute a response (usually, shooting), extends in an unbroken line from the beginning of the First World War through the Cold War. There were many other purposes for computers, but the essence was this shooting game and the information-gathering and command-and-control systems to effectuate it. (Radar tracking screens were the basis for another SAGE innovation, the computer screen.) In the process of accurately communicating great violence at a distance, to defend against distant enemies, government, private industry, and the academy formed a distinctive practice of cooperation and mutual reinforcement.
It remained to find ways to have computers communicate freely among themselves. In January 1957, Leonard Kleinrock wrote later in the Institute of Electrical and Electronic Engineers magazine, "I worked with Claude Shannon [at MIT], who inspired me to examine behavior as large numbers of elements (nodes, users, data) interacted; this led me to introduce the concept of distributed systems control and to include the study of 'large' networks in my subsequent thesis proposal. In that MIT environment I was surrounded by many computers and realized that it would soon be necessary for them to communicate with each other. However, the existing circuit switching technology of telephony was woefully inadequate for supporting communication among these data sources. This was a fascinating and important challenge, and one that was relatively unexplored. So I decided to devote my Ph.D. research to solving this problem, and to develop the science and understanding of networks that could properly support data communications." Kleinrock added in a footnote that, later that same year, "I experienced a widely shared feeling of surprise and embarrassment when the Soviet Union launched Sputnik, the first artificial Earth satellite. In response, President Eisenhower created ARPA on February 7, 1958 to regain and maintain U.S. technological leadership."69
ARPA, the Advanced Research Projects Agency—"Defense" would be pre-pended to the name in 1972, resulting in DARPA—was the main institutional home for the development of the Internet. According to DARPA's official 2005 mission statement, "DARPA's original mission, established in 1958, was to prevent technological surprise like the launch of Sputnik, which signaled that the Soviets had beaten the U.S. into space. The mission statement has evolved over time. Today, DARPA's mission is still to prevent technological surprise to the U.S., but also to create technological surprise for our enemies." Technological surprise can, of course, only be produced through innovation. "DARPA's mission implies one imperative for the Agency: radical innovation for national security. DARPA's management philosophy reflects this in a straightforward way: bring in expert, entrepreneurial program managers; empower them; protect them from red tape; and quickly make decisions about starting, continuing, or stopping research projects. To maintain an entrepreneurial atmosphere and the flow of new ideas, DARPA's strategy is to hire program managers for periods of only 4 to 6 years; the best way to foster new ideas is to bring in new people with fresh outlooks. New people also ensure that DARPA has very few institutional interests besides innovation, because new program managers are willing to redirect the work of their predecessors—and even undo it, if necessary. And, since program managers are not at DARPA for a career, they are willing to pursue high-risk technical ideas even if there is a good chance the idea will fail."70
This was something no other nation was capable of. Even if another country had been able to build such a state-sponsored hothouse of structured innovation, it would not have had the combination of industrial base, deep academic knowledge, institutional memory, urgency, and sheer capital that the United States had. Besides, pacifism and anti-nationalism had settled in, to a greater or lesser degree, among most industrialized nations; the United States and the Soviet Union were effectively alone in their contemplation of extreme force projection.
Almost immediately after ARPA was founded, the DARPA history continues, "A new mission emerged to counter a new threat: intercontinental ballistic missiles. From approximately 1960 to 1970, DARPA was a driving force behind the United States' technology advancements in ballistic missile defense."71 Up to a point, this was the old fire-control problem in a new key. What had begun ship-to-ship after the Battle of Jutland and defended London in 1945 had reached continental scale: the essential task remained to get the information about incoming attackers and translate it quickly enough to defending ordnance to make a difference.
This was part of the military context for improving communication among computers, but fire control was not dominant in the way it had been from 1916 through the Second World War. Warren Weaver, given his own experience and that of his generation, might sensibly think of war as a form of speech, but the hydrogen bomb, with its potential result of permanent silence, changed that. And the high-level technological innovation ARPA aimed at was mostly not about small wars or even large ones like Korea and Vietnam; it was about out-innovating the Soviet Union. That was the power the United States needed to keep surprising. Fundamentally, it was a challenge of combining science and capital to produce innovation that would deter actual conflict.
The first Internet—to be called ARPANET—was built primarily to enable time-sharing on a network of computers, which meant information had to be passed among them. The three pioneers of this were Kleinrock, Donald Davies in Britain, and Paul Baran at the RAND Corporation, which did research and development for the military. (RAND began life as a project of the Douglas Aircraft Company and the American military, particularly the air force.)72 Davies was aware of Kleinrock's work, but otherwise, in the first years of the 1960s, the three men worked in isolation. One line of development was in ARPA itself, using mainly academic contractors. A second line of development, also with Pentagon money, was at RAND.
First among the surprises the United States did not want was to be destroyed by a nuclear attack. For that you needed both warning, as through SAGE, and secure communications in the event of an attack. Paul Baran was contracted to solve the latter problem. His first job out of college had been working at the company formed by ENIAC's makers. Later, in 1955-57, he worked at Hughes Ground Aircraft Systems on a project to build a miniature SAGE by using transistors rather than radio tubes. Two years later he moved to RAND.
"In late 1959, when I joined the RAND Corporation, the Air Force was synonymous with National Defense," Baran said in an oral history.73 "The other services were secondary. The major problem facing the country and the world was that the Cold War between the two super powers had escalated to the point by 1959 when both sides were starting to build highly vulnerable missile systems prone to accidents. Whichever side fired their thermonuclear weapons first would essentially destroy the retaliatory capacity of the other. This was a highly unstable and dangerous era. A single accidental fired weapon could set off an unstoppable nuclear war. A preferred alternative would be to have the ability to withstand a first strike and the capability of returning the damage in kind. This reduces the overwhelming advantage by a first strike, and allows much tighter control over nuclear weapons. This is sometimes called Second Strike Capability. If both sides had a retaliatory capability that could withstand a first-strike attack, a more stable situation would result. This situation is sometimes called Mutually Assured Destruction, also known by its appropriate acronym, MAD. Those were crazy times." If computing had grown up since the 1920s as a way to imagine how to project power into a space you could not see but could, aided by computers, predict, then computing under MAD conditions went to the next step by imagining how to project power into a future that you could be reasonably assured you would not survive.
"The weakest spot in assuring a second strike capability," Baran continued, "was in the lack of reliable communications. At the time we didn't know how to build a communication system that could survive even collateral damage by enemy weapons. RAND determined through computer simulations that the AT&T Long Lines telephone system, that carried essentially all the nation's military communications, would be cut apart by relatively minor physical damage."
A stronger system, Baran realized, had to be digital because digital signals could be sent and re-sent in packages—Davies would later suggest the word "packets"—and, by proliferating across a network, would render communication vastly less vulnerable. "Digital signals have a wonderful property," he recalled. "As long as the noise is less than the signal's amplitude it is possible to reconstruct the digital signal without error. The future survivable system had to be all-digital. At each connected node, the digital signal would be verified that the next node correctly received it. And, if not, the signal would be retransmitted. As one day the network would also have to carry voice as well as teletypewriter and computer data, all traffic would be in the same form—bits. All analog signals would first be digitized. To keep the delay times short the digital stream would be packaged into small message blocks each with a standardized format."
And that is, more or less, what happened. In late 1968, part of the ARPANET design was awarded to a UCLA team led by Kleinrock, another part to the firm Bolt, Beranek and Newman. The first four nodes to be connected were at UCLA, UC Santa Barbara, the Stanford Research Institute (affiliated with Stanford but independent, and funded mainly by government contracts), and the University of Utah. The military-academic-industrial complex activated in the First World War had finally created the Internet.
 


  4.   Claude E. Shannon and Warren Weaver, The Mathematical Theory of Communication (Urbana, IL: University of Illinois Press, 1949), p. 3.
  5.   Herman H. Goldstine, The Computer: From Pascal to von Neumann (Princeton: Princeton University Press, 1972), p. 165.
  6.   Charles Petzold, Code: The Hidden Language of Computer Hardware and Software (Redmond, WA: Microsoft Press, 2000), p. 240. There is no shortage of general histories of computing. I like Petzold's because he gives detailed, clear expositions of the electronics as well as discussing some of the personalities involved. I've also relied for general background on Brian W. Kernighan, D Is for Digital (DisforDigital.net, 2011) and James Gleick, The Information (New York: Pantheon, 2011).
  7.   Petzold, Code, pp. 241-2.
  8.   Petzold, Code, chap. 11.
  9.   William H. Eccles, Wireless (London: Butterworth, 1933), pp. 163-4.
  10.   William Henry Eccles and Frank Wilfred Jordan, "Improvements in ionic relays," British patent number: GB 148582 (filed: 21 June 1918; published: 5 August 1920). http://v3.espacenet.com/origdoc?DB=EPODOC&IDX=GB148582&F=0&QPN=GB148582. W. H. Eccles and F. W. Jordan (19 September 1919) "A trigger relay utilizing three-electrode thermionic vacuum tubes," The Electrician, vol. 83, p. 298. Reprinted in Radio Review, vol. 1, no. 3, pp. 143-146 (December 1919). This last article notes, "The circuits described here were the subject of a patent, No. 10290/1918, taken out by the English Admiralty." See also British Communications and Electronics, vol. 9, no. 1 (1962), p. 7.
  11.   Shannon and Weaver, Mathematical Theory, p. 32.
  12.   Petzold, Code, p. 161.
  13.   National Research Council, A History of the National Research Council, 1919-1933, Reprint and Circular Series of the National Research Council no. 106, 1933, p. 7.
  14.   "Submarine Terror to End, Says Edison" New York Times, Jan. 2, 1915. http://query.nytimes.com/mem/archive-free/pdf?res=9D05E5D6153FE233A25750C0A9679C946496D6CF.
  15.   National Research Council, A History, pp. 7, 50 (Military Committee); "Third Annual Report of the National Research Council," Annual Report, National Academy of Sciences, 1918 (Washington, DC: Government Printing Office, 1919), pp. 46-7.
  16.   Rexmond C. Cochrane, The National Academy of Sciences: The First Hundred Years, 1863-1963 (Washington, DC: National Academy of Sciences, 1963), p. 237; "Third Annual Report of the National Research Council," pp. 40-1 (Wilson's Executive Order); National Research Council, A History, p. 9 (Carnegie).
  17.   Warren Weaver, Scene of Change: A Lifetime in American Science (New York: Scribner, 1970), p. 45.
  18.   National Research Council, A History, p. 17; Cochrane, National Academy of Sciences, pp. 338, 210-11, 214-16. Jewett and Bush met in 1917 when both were working on anti-submarine technology (pp. 384-5), although as a pioneer at Bell of long-distance telephone communication Jewett's main wartime contribution was in signaling (p. 384).
  19.   Cochrane, National Academy of Sciences, p. 326.
  20.   "New Army Policy Fixed by Harding," New York Times, July 24, 1921. http://query.nytimes.com/mem/archive-free/pdf?res=9E06E4D81731EF33A25756C2A9619C946095D6CF.
  21.   Richard W. Stewart, general editor, American Military History (vol. 2), "The United States Army in a Global Era, 1917-2003"(Washington, DC: US Army Center of Military History, 2005), pp. 57-59. http://www.history.army.mil/books/amh-v2/amh%20v2/chapter2.htm.
  22.   Stewart, ed., American Military History, vol. 2, pp. 64-65.
  23.   Cochrane, National Academy of Sciences, p. 340-342.
  24.   David A. Mindell, Between Human and Machine: Feedback, Control, and Computing before Cybernetics (Baltimore: Johns Hopkins University Press, 2002), p. 21.
  25.   Mindell, Between Human and Machine, p. 26.
  26.   A. Ben Clymer, IEEE Annals of the History of Computing, vol. 15, no. 2, 1993 "The Mechanical Analog Computers of Hannibal Ford and William Newell," p. 25.
  27.   Cochrane, National Academy of Sciences, p. 339, citing Dixon Wecter, The Age of the Great Depression, 1929-1941 (New York: Macmillan, 1948), pp. 17-18.
  28.   The concluding part of the address, given on June 22, 1937, at the summer convention of the American Institute of Electrical Engineers in Milwaukee, was published as "The Engineer and His Relation to Government," Science, vol. 86, no. 2222 (July 30, 1937), p. 87 ff. http://www.sciencemag.org/content/86/2222/87.full.pdf.
  29.   Elmer A. Sperry, "The Spirit of Invention in an Industrial Civilization," in Charles A. Beard, ed., Toward Civilization (London: Longman Green, 1930), pp. 47-68. http://www.unz.org/Pub/BeardCharles-1930-00047.
  30.   Ibid.
  31.   Bush, "The Engineer and His Relation to Government."
  32.   G. Pascal Zachary, Endless Frontier: Vannevar Bush, Engineer of the American Century (New York: Free Press, 1997), p. 81.
  33.   Ibid., p. 96.
  34.   Zachary, Endless Frontier, pp. 116-7.
  35.   Mindell, Between Human and Machine, p. 189.
  36.   Zachary, Endless Frontier, p. 96; Mindell, Between Human and Machine, p. 187.
  37.   Mindell, Between Human and Machine, pp. 187-92.
  38.   Conway and Siegelman, Dark Hero of the Information Age: In Search of Norbert Weiner (New York: Basic, 2005), pp. 105-9; Mindell, Between Human and Machine, pp. 193, 196, 328.
  39.   Eugene Chiu, Jocelyn Lin, Brok Mcferron, Noshirwan Petigara, and Satwiksai Seshasai, Mathematical Theory of Claude Shannon (Cambridge, Mass.: MIT, 2001), p. 26, http://web.mit.edu/6.933/www/Fall2001/Shannon1.pdf; Claude Shannon, A Symbolic Analysis of Relay and Switching Circuits (Cambridge, Mass.: MIT, 1940), p. 11. http://www.cs.virginia.edu/~robins/Shannon_MS_Thesis.pdf.
  40.   Shannon, A Symbolic Analysis, pp. 14-15.
  41.   Chiu et al., Mathematical Theory, p. 31.
  42.   Flo Conway and Jim Siegelman, Dark Hero, p. 42.
  43.   Zachary, Endless Frontier, p. 8; Conway and Siegelman, Dark Hero, p. 73.
  44.   Vannevar Bush, Operational Circuit Analysis (New York: Wiley, 1929), p. v.
  45.   Conway and Siegelman, Dark Hero, p. 105; Norbert Wiener, Cybernetics or Control and Communication in the Animal and the Machine (New York: Wiley, 1948), p. 11.
  46.   Paul E. Ceruzzi, Reckoners: The Prehistory of the Digital Computer, from Relays to the Stored Program Concept, 1935-1945 (Westport, Conn.: Greenwood, 1983), pp. 92-95; Mindell, Between Human and Machine, pp. 300-6; http://history-computer.com/ModernComputer/Relays/Stibitz.html.
  47.   Zachary, Endless Frontier, p. 266; Paul Edwards, The Closed World: Computers and the Politics of Discourse in Cold War America (Cambridge, Mass.: MIT, 1996), p. 48.
  48.   Conway and Siegelman, Dark Hero, pp. 116-7.
  49.   The best explanation of ENIAC and its relation to stored programs is in Ceruzzi, Reckoners, pp. 132-45. Ceruzzi also discusses the parallel work of Konrad Zuse in Germany. Digital computing and the Internet did not, of course, only evolve in the United States. Russian, German, Japanese, French, South Asian, and other scientists and tinkerers all had roles in what is, taken as a whole, a vast and often discontinuous story. For the period from World War I to now, however, the United States has dominated innovation in the computing field, a period that has coincided with the United States' global power. In illuminating that relationship between US power and digital innovation, I do not mean to suggest that American achievements were necessarily exclusive.
  50.   Zachary, Endless Frontier, p. 266; Martin Campbell-Kelly and William Aspray, Computer: A History of the Information Machine (New York: Basic, 1996), pp. 91-1; Brian Randell, "The Colossus," in A History of Computing in the Twentieth Century, N. Metropolis, J. Howlett and G. C. Rota, eds. (Academic Press, New York, 1980), pp. 30-37. Flowers and Campaigne briefly recall those days in IEEE Annals of the History of Computing, vol. 5, no. 3, July 1983, p. 239 ff.
  51.   Goldstine, The Computer, pp. 72, 135.
  52.   Weaver, Scene of Change, p. 85.
  53.   Zachary, Endless Frontier, pp. 362-4.
  54.   Zachary, Endless Frontier, p. 367; Vannevar Bush, Modern Arms and Free Men: A Discussion of the Role of Science in Preserving Democracy (New York: Simon and Schuster, 1949), pp. 17-18; Vannevar Bush, Endless Horizons (Washington, DC: Public Affairs, 1946), p. 82; http://www.usgovernmentspending.com/defense_spending.
  55.   The 1933 essay, "The Inscrutable Past," was published in Technology Review Jan. 1933, and reproduced in Bush, Endless Horizons, as was the Atlantic piece, "As We May Think." The quotations in the paragraph are from the Endless Horizons versions.
  56.   Zachary, Endless Frontier, p. 264.
  57.   Zachary, Endless Frontier, pp. 273-4.
  58.   Jewett was writing in the Bell Telephone Quarterly #14 (1935), pp. 167-99, cited in Mindell, Between Human and Machine, p. 136.
  59.   Wiener, Cybernetics, pp. 15-6, 19; Conway and Siegelman, Dark Hero, pp. 181-3; "The Brain Is a Machine," Newsweek, Nov. 15, 1948, p. 89; "Machines that Think," Business Week, Feb. 19, 1949; see also Ronald R. Kline, The Cybernetics Moment: Or Why We Call Our Age the Information Age (Baltimore: Johns Hopkins, 2015).
  60.   Wiener, Cybernetics, pp. 36-7.
  61.   Wiener, Cybernetics, pp. 37-9.
  62.   https://www.whitehouse.gov/omb/budget/Historicals; Bush, Modern Arms, p. 6; Stuart W. Leslie, The Cold War and American Science: The Military-Industrial-Academic Complex at MIT and Stanford (New York: Columbia University Press, 1993), pp. 6-8.
  63.   National Research Council, Funding a Revolution: Government Support for Computing Research (Washington, DC: National Academy Press, 1999), p. 90.
  64.   National Research Council, Funding a Revolution, p. 89; Joel Shurkin, Engines of the Mind: A History of the Computer (New York: Norton, 1984), pp. 187-9; Goldstine, The Computer, p. 307.
  65.   National Research Council, Funding a Revolution, pp. 29-30. ERA had been formed by several navy code-breakers who found themselves out of work. They and the navy convinced a Minnesota investor (whose own glider company was in postwar collapse) to finance them without knowing quite what they were actually doing. Shurkin, Engines of the Mind, p. 213. This all became Sperry-Rand by mid-1955. Goldstine, The Computer, p. 326.
  66.   Leslie, The Cold War, pp. 32-7; National Research Council, Funding a Revolution, pp. 91-7; Edwards, Closed World, pp. 75-111.
  67.   http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/sage.
  68.   National Research Council, Funding a Revolution, pp. 93-4; Leslie, The Cold War, pp. 36-7.
  69.   Leonard Kleinrock, "An Early History of the Internet," IEEE Communications Magazine, August 2010, p 26.
  70.   Defense Advanced Research Projects Agency, DARPA: Bridging the Gap Powered by Ideas (self-published, 2005), https://archive.org/details/DARPA--Bridging-The-Gap-Powered-By-Ideas--2005, p. 1; see also Sidney G. Reed, Richard H. Van Atta and Seymour J. Deitchmar, DARPA Technical Accomplishments: An Historical Review of Selected DARPA Projects, v. 1 (Institute for Defense Analyses, 1990), pp. 1-3.
  71.   DARPA, "Bridging the Gap," p. 7.
  72.   Leslie, The Cold War, pp. 112-3.
  73.   "Oral History: Paul Baran," Interview Conducted by David Hochfelder, IEEE History Center, 24 October 1999; Interview #378 for the IEEE History Center, The Institute of Electrical and Electronics Engineers. http://ethw.org/Oral-History:Paul_Baran#Institute_For_the_Future.2C_1967.2B.










CHAPTER 2.  LIBERATION TECHNOLOGY
In the late 1960s, the computer world very quickly went in a countercultural direction, especially as the computer became the personal computer rather than the corporate or bureaucratic computer. An explicitly anti-authoritarian hacker subculture thrived in this changed world and proved its innovative prowess. The Internet and computing became so countercultural in the 1970s and 1980s, and so centered in California's free-wheeling San Francisco Bay Area, that their basis in five decades of military-led research and development began to seem embarrassing or even somehow in need of explanation. How did technology built to defend the state against attack become technology dedicated to empowering the individual, not least against that same state?









GEEKS AND BOHEMIANS
The puzzlement at how innovations as disruptive and personally empowering as the web and the personal computer could have come out of the automaton conformity of the Cold War 1950s is somewhat misdirected. For one thing, it exaggerates the dreariness of the baby boomers' elders (which baby boomers tend to do). Norbert Wiener was no one's idea of a bureaucrat. He was famous for his random Wienerwalks around the MIT campus and for taking his rotund body out for long swims as he puffed away on a cigar. Claude Shannon had a full complement of eccentricities, not least the robot he made modeled on the comedian W.C. Fields; he worked on a mathematical theory of juggling, navigated Bell Labs on a unicycle, and kept on his desk a wonderful machine he developed with his student Marvin Minsky: turn the on-off switch to on, and a door opens with a mechanical hand that reaches out, flips the switch to off, and returns to its box. The human says one, the machine replies zero, and then the human says one again, just like in life.1
J.C.R. Licklider was another of his generation's oddballs. Known as Lick, he was a Gestalt psychologist with an interest in communications and the relationship between people and machines. He was an enthusiastic regular at Norbert Wiener's Tuesday evening salons in Cambridge right after the war and brought psychological research to bear on the construction of the SAGE warning system as it was being developed at MIT. In January 1960, while at Bolt, Beranek and Newman, Licklider finished an essay—funded by a contract from the Behavioral Sciences Division of the Air Force Office of Scientific Research, Air Research and Development Command—called "Man-Computer Symbiosis." He believed that ultimately most thinking would be done by machines, but that there would be an interim, between his own time and then, when people and computers would be happily symbiotic and think together. (He had already seen something like this symbiosis at SAGE: "computers find precedents in experience, and in the SAGE System, they suggest courses of action.") As he wrote: "A multidisciplinary study group, examining future research and development problems of the Air Force, estimated that it would be 1980 before developments in artificial intelligence make it possible for machines alone to do much thinking or problem solving of military significance. That would leave, say, five years to develop man-computer symbiosis and 15 years to use it"—that is, to use it before machines take over and make this man-machine symbiosis obsolete. "The 15," Licklider continued, "may be 10 or 500, but those years should be intellectually the most creative and exciting in the history of mankind"—that is, until those years come to an end.2
In 1961, Licklider undertook a project on libraries of the future. The final report, published in 1965, would be dedicated to Vannevar Bush for his memex essay.3 But he had to take leave from the library project when the Defense Department hired him in 1962 to run ARPA's Information Processing Techniques Office, which would be the main shaper and funder of ARPA's networking project up to the completion of ARPANET (which was, of course, contracted in great part to Bolt, Beranek and Newman). "Every time I had the chance to talk," Licklider said of his time working for the Department of Defense, "I said the mission is interactive computing." He referred to his far-flung Internet-inventing team as the Intergalactic Computer Network. The person Licklider hired to run one major part of the ARPANET project, Doug Engelbart—who as a soldier in 1945 read Bush's article proposing memex and made it his mission to build it—was himself seriously quirky and nearly ran the project aground after growing too enamored of a self-realization cult called est.4
So these were not strait-laced people, even if they were working for the Pentagon. The difference, perhaps, is that the post-World War II generation of engineers was the first to glimpse that computers and individuals might have a one-to-one relationship on a personal scale. That was Licklider's great animating passion (not only his), this vision of the individual human and the computer interacting and taking on, together, projects yet to be determined. The engineers-at-the-new-frontier idea that Vannevar Bush rhapsodized about in the 1930s was being transformed. The computer was being gradually loosened from the military soil it grew in, and relocating from the national to the personal.









WHY CALIFORNIA?
That transition was by no means quick. The very Californian world in which the personal computer and the Internet both grew—and eventually, in the 1980s, grew together—was a highly militarized one into the 1960s. There were really two ways to make computers personal: to make them small enough for a single person to have one, or to create a personal terminal that could connect remotely to a computer as part of a network. Both were pursued in the 1960s and into the 1970s as part of government-funded programs, mainly military. The latter, networking course was pursued in the ARPA, NASA, and air force-sponsored oN-Line System (NLS) as demonstrated in the famous "mother of all demos" given by Doug Engelbart in 1968 in San Francisco.5 Remote computing, combined with the time-sharing of ARPANET, resulted in the basic structure of what would become the web. The miniaturization option was pursued in a separate research line, also military funded and also (mainly) in the San Francisco Bay Area. Guidance systems for long-range missiles, in particular, needed much smaller circuitry; but all upper-atmosphere and space-borne projectiles, whether satellites, missiles or space ships, had a clear need for, ideally, on-board computers. This required miniaturization.6
The first enormous step toward miniaturization had been the invention of the transistor at Bell Labs in 1948; it replaced the vacuum-tube triodes of William Eccles' day. The transistor was developed by William Shockley, John Bardeen, and Walter Brattain, but initial credit went mainly to the latter two, which aroused envy in Shockley. He quickly went on to invent a much-improved transistor that would become standard through the 1960s. The alienation of Shockley from his former collaborators soon became complete, and Shockley moved to Mountain View, California, to start his own firm, focused on taking the transistor work forward with silicon-based semiconductors. (He chose Mountain View because it was near his ailing mother, who lived in Palo Alto. Shockley had grown up there and been educated at Caltech and MIT.) Shockley did not get any easier to work with in California, however. Eight of his co-engineers left in 1957 to form Fairchild Semiconductor in nearby San Jose, funded by Fairchild Camera and Instrument, a long-time eastern military contractor noted for its aerial surveillance cameras. Fairchild Semiconductor would provide a sizeable part of the corporate and talent base for Silicon Valley.7
What was then known as the Santa Clara Valley, with San Jose as its major town, was hardly new to high technology or defense contracting when Shockley arrived. In 1925, Vannevar Bush's first electrical-engineering graduate student, Frederick Terman, joined the Stanford engineering faculty. His father, a psychologist, was already on the faculty and the younger Terman had earned his own undergraduate degree at Stanford. Terman would bring the Bush vision to California.8
Stanford, with its campus in Palo Alto, was in 1925 in its fourth decade of attempting to prove that a California college could perform at an Ivy League level. Wealthy (and not-so-wealthy) Californians of that period had a complex relationship to the East. A desire to impress easterners, particularly strong in the early years after the gold rush of 1849, weakened only slowly over the decades; it was accompanied by a waxing sense that the East was stuck in the past and, principally because of control of capital though also control of political power, would keep California from taking its desired place in the future. This combination of envy, ahistoricity, resentment, self-confidence, and creative energy was present in most western territories but it was especially pronounced in California and (under significantly different conditions) in Texas. Stanford had an exalted belief in its destiny, and Terman, who did not want to get stuck in Vannevar Bush's shadow, was the right person to seize on this sense of mission. He was a tireless administrator, a good spotter of talent, an able mentor, a devoted inventor (with thirty-six patents between 1930 and 1947), and a risk-taker.9
Terman kept his ties to the East and to Bush, and when World War II began he moved to Harvard to head the secret Radio Research Laboratory, which distinguished itself for developing countermeasures against enemy radar. On returning to Palo Alto after the war, he was determined to have Stanford drive the industrialization of the West. The Pacific War had brought California firmly into the defense economy. Terman knew well the benefits—for freedom, but also for profit and for power—of the military-industrial-academic cooperation that his mentor had done so much to establish and cultivate.10
Defense spending after World War II, given the nature of the technological competition with the Soviets—in particular, the strategic necessity of innovation—had become a special form of risk capital. Defense technology spending was particularly high-risk because it had to be. If the United States were to consistently surprise the Soviets it would have to consistently surprise itself, and that meant taking chances.
California was the perfect place to take chances, with its forgiving climate and rock-solid agricultural base and, back then, ample physical space. Frederick Terman pioneered the idea of a university-owned industrial park (Stanford Industrial Park, 1951), recruited top scientific talent, and encouraged professors and students to start companies based on their products—people like his students William Hewlett and David Packard, whose Hewlett-Packard would be an early tenant of the industrial park. Terman sometimes invested his own money in these new companies and provided them with business contacts. Terman's students Russell and Sigurd Varian were the first tenants of the Stanford Industrial Park and Terman was later a director of Varian Associates, as was David Packard. Varian was founded around the klystron, which was used in radar; the young company also landed a much-needed contract to make fuses for atomic bombs, about which the left-leaning Varian brothers were ambivalent. The money was good though.11
So when Fairchild Semiconductors was formed in 1957 it was right at home. The company had its ups and downs, and in truth its success at spawning new companies ("fairchildren") was due in great part to employees falling out with management and taking their best ideas with them. Fairchild did well with government contracts—it supplied the integrated circuits for the Apollo space missions—but defense spending could only take the tech industry so far. It needed more sources of capital.
The "venture capital" that would eventually be associated with Silicon Valley also had its roots in the Second World War. The key figure was Georges Doriot, later known as the father of venture capital, who as a boy in France had seen what capital, and a lack of capital, could mean to a hard-working entrepreneur as he watched his father build his own automobile company. When Doriot moved to the United States he began to observe American capitalists. They were in general cautious, all the more so after the depression of 1893, which saw the collapse into bankruptcy of about a third of American railroads and the demise of hundreds of commercial firms. Reactions to this included an understandable risk aversion, an immense wave of mergers, and the consolidation of industrial and financial control by a few wary men, principal among them J.P. Morgan. The general risk aversion continued into the Great Depression itself and was worsened by New Deal tax policies that targeted the wealthy—not just income and estate taxes but also taxes on investment earnings.12
At that time, Doriot was teaching business at Harvard, where he railed against the New Deal with notable ferocity. He also made common cause with Karl Compton, Vannevar Bush's close associate and the president of MIT. They believed that not enough money was going into helping entrepreneurs with fresh ideas to build companies, and in particular that science was not being sufficiently brought to bear on creating economic progress. That sense was especially sharp in New England, which had a highly skilled workforce, excellent universities, good transport, declining older industries (such as textiles), and plenty of money. Why could it not innovate? In 1939, Compton suggested to a group called the New England Council, founded in 1925 to find ways to improve the regional economy, that it form a New Products Committee. Doriot headed a subcommittee on venture capital, which concluded after its first meeting, in 1940, that the pressing need was for people who could bridge the gap between investors who didn't understand science or technology and technologists who didn't understand business (or investors). This would be a way for society to engage more productively with the rapidity of modern technological change and on a wider variety of fronts than seemed to be the case when investing was dominated by a cautious few.13
The war interrupted these plans but in many ways created the basis for their later fulfillment. Firstly, the war offered the spectacle of highly risky investments in technology bringing high returns. The returns were not economic—they involved missiles not hitting their targets, soldiers not dying of cold, and dictators not prospering—but it was easy to see how they might be. Doriot himself had worked closely with the army on production methods since the late 1920s and had periodically dreamt of a peacetime military research organization.14 In 1939, William "Wild Bill" Donovan—who founded the Office of Strategic Services (precursor to the Central Intelligence Agency)—invited Doriot to meet President Roosevelt to discuss war production.15 Doriot went on to spend the war years as a senior official for military procurement and product development: his first experience at having plenty of risk capital to throw at potential scientific breakthroughs. His many achievements included the development of plastic body armor; having entered as a lieutenant colonel, he finished the war as a decorated brigadier general. His experience led him to conclude, as he said in a 1944 speech to the National Academy of Sciences, that modern war "is in reality applied science."16
At war's end, General Dwight Eisenhower hoped Doriot would head up a new military department of research and development. Eisenhower recognized that, as he said, "The armed forces could not have won the war alone. Scientists and businessmen contributed techniques and weapons which enabled us to outwit and overwhelm the enemy," a collaboration Eisenhower hoped to extend into peacetime. However, Doriot preferred to return to Harvard and his venture-capital ambitions. Doriot did join Vannevar Bush in 1947 in a proposed collaborative effort, the Joint Research and Development Board, intended to rationalize research and development by the army and navy and, in the short term, develop guided missiles. But army-navy cooperation was difficult in peacetime, especially over money matters. The JRDB was stillborn, and missile development would be left to the air force, space agencies, and other actors outside the army-navy budget circus.17
So Doriot returned to the private sector and Harvard Business School, where his students ever after referred to him as The General, and in June 1946 he helped launch the first significant non-family venture-capital firm, the American Research and Development Corporation (ARD). It was widely covered in the press—the Business Week headline was "Adventure Capital." One of Doriot's co-founders explained to the New York Herald Tribune: "While all projects may not be successful, it is the consensus of those best acquainted with the mass of new developments coming out of the war activity that enough of the projects will turn out to be profitable so that the investment as a whole will be financially successful." As Doriot's biographer, Spencer Ante, wrote, "the war was a watershed for entrepreneurialism. While it was true that the war temporarily stunted the development of small business, it ultimately created a more fertile environment for the entrepreneurial economy to flourish. By its very nature, war encourages risk taking, and this particular war proved the value of taking risks on new technologies and methods of production beyond anyone's imagination."18
ARD had mixed success and, as an innovation itself, faced severe regulatory and cultural challenges as well as the already ample challenge of finding and financing good companies. ARD was fortunate when Kenneth Olsen, a young and ambitious MIT graduate, came along. Olsen was working on SAGE at MIT's Lincoln Laboratory when he was allowed to do some side work on the transistors then coming out of Bell Labs. By the mid-1950s he had designed and built the first transistorized computer, the TX-O, known as Tixo. "The reason for building the TX-O," he later said, "was to demonstrate how efficient in power, how fast in speed, and how easy it would be to build a computer for defense."19
Olsen expected Tixo to astonish the world, and when it didn't he began to cast about for funding to build more small, fast computers and to take on IBM—to make his point on a greater scale. Someone mentioned ARD, so Olsen and his partner, Harlan Anderson, pitched their idea to The General. The proposal went through a few iterations—including the deletion of the word "computer" from it, as Fortune magazine had recently declared computers would never make money—before the ARD board decided to back Olsen and Anderson's company, Digital Equipment Corporation, in 1957. When, years later, ARD liquidated its $70,000 stake in DEC, it was worth more than $355 million.20
Doriot's work prepared the ground for venture capital to come to the West Coast. In 1959, William H. Draper Jr., Rowan Gaither, and Frederick Anderson set up Draper, Gaither & Anderson, the first California venture firm. General Draper had been the first under-secretary of the army and first US ambassador to NATO (1953). General Anderson had been a lead architect of the strategic bombing of Germany (begun in 1943) and had been Draper's general deputy at NATO; they later worked together administering the Marshall Plan. Rowan Gaither was a former assistant director at MIT's Radiation Laboratory and, in 1948, a founder of the RAND Corporation. He was best known as the head of the commission that prepared a key report (the "Gaither Report") for President Eisenhower, in late 1957, urging that the missile gap with the Soviets and a lack of preparedness for nuclear attack meant the United States should greatly increase its military spending, which it went on to do.21
Draper had worked with Doriot during the war; his son, Bill Draper, who was a junior partner at the firm and would have a long and successful career in venture capital, studied under Doriot. ("He was a total charmer. You didn't feel it was work; it was play when you went into his class.") Doriot had urged the younger Draper not to get stuck in Boston, and while Draper, Gaither & Anderson essentially took eastern money and invested it in the West, the firm had a sense of doing something new. One of its junior partners, Pete Bancroft, from an old California family, once said the eastern investors "did not dare as greatly or as well" as their western counterparts. That might help explain why DG&A departed from Doriot's model by structuring itself as a "limited partnership," which meant in practice that the general partners (Draper, Gaither, and Anderson) picked the investments and the limited partners simply put in money. As Bancroft explained it, "I think the limited partner model took hold because the idea was to get the money in and report on it as little as possible." The new model also freed venture capital from many of the regulations that had bedeviled Doriot and ARD.22
DG&A did not do particularly well, just managing over its lifetime to surpass the growth of the S&P 500 index, and indeed ARD might not have beat the S&P 500 either if not for the Digital Equipment payout.23 But, like the US military, venture capitalists had the capacity to burn money, and West Coast venture capital, in its relationship to the university, the military, and industry, made it possible to take the astonishing technical creativity of wartime into the Cold War peace. The military needed academic talent to continue generating innovations in the absence of a full-on mobilization; the private sector needed ways to overcome the creative entropy of large corporations and established industries; and the academy needed money. Frederick Terman was uniquely successful at remixing these variables in Palo Alto, bringing industry in to sponsor academic research, working closely with defense agencies (including intelligence agencies), and encouraging entrepreneurial company-building, especially by professors.24
"It was Terman's concept," Stanford engineering professor William Rambo told an interviewer, "that people at Stanford could do useful things for the government which the government could well pay for because it would be a bargain for them, and which at the same time would support the university research for students and faculty." Rambo, a Stanford graduate, had been recruited by Terman for Harvard's Radio Research Laboratory during the war; they worked together on ways of jamming enemy radar. That research was critical in the short term because it reduced the casualties of heavy bombing campaigns—notably the one against Germany led by General Anderson, later the main force behind DG&A. The radar research was important in the longer term because it involved fundamental work on microwaves, the focus of the Varian brothers with their klystron and arguably the most valuable innovation in Stanford engineering of that era.25 This is why Stanford became so involved in spying: US intelligence agencies were desperate to know the status of Soviet weaponry, in particular strategic weaponry such as missiles and long-range bombers. This required over-the-horizon radar and, especially after the Soviets' Sputnik launch in 1957, spy satellites. Microwaves were critical for such surveillance (as well as for guiding missiles). "Terman was always interested in people having some breadth, and I spent a lot of time in Washington on committees as an advisor, a consultant of one sort or another," Rambo recalled. "This was always with his knowledge and encouragement, and my time on those assignments was always paid for by the university. The government paid all the travel and the hotel expenses, but he always had the feeling that we were involved with the government, and a lot of government money, and it ought to be a two-way street . . . . I consulted with an NSA Committee for ten years, the National Security Agency. I'd done work with the CIA."26
This was the context in which West Coast venture capital grew. Although private money, it was in no sense hostile to government, which was the main customer of its companies. The next major California VC firm after DG&A was Davis & Rock, founded in 1961. Arthur Rock, like General Draper, had been a New York banker before he found himself out west, having engineered the financing for Fairchild Semiconductor; Tommy Davis had spent part of the war with the OSS behind enemy lines in Burma, where he served with an innovative unit called Detachment 101.27 Davis had already begun investing in the mid-'50s with the help of Terman. His first major investment was in Watkins-Johnson, founded by Stanford electrical-engineering professor Dean Watkins and Richard Johnson, who headed Hughes Aircraft's microwave division. Watkins-Johnson developed and manufactured surveillance, communications, and electronic countermeasure equipment. It had a profit in its very first year ($80,000); ten years later, the figure was $3.1 million. Arthur Rock also participated in the financing, and out of that experience he and Tommy Davis formed the company that would lead California venture capital into its mature form.28
The early venture-capital business in Silicon Valley essentially replaced a portion of the risk capital that had been provided by the state during World War II. The business was largely dependent on defense spending as it surged after Sputnik. But the defense and intelligence agencies, even at the peak of Cold War spending, did not have anything like the sums that were available during the world war, nor did they have the power to compel scientific talent on anything like the scale that had pertained in 1940-45. California's venture capital firms provided some of the missing capital; more importantly, they provided much of the missing incentive, because they could turn engineering professors into very wealthy people. And along the way they created companies that built technology that would soon take computing and computer networking away from their dependence on the state.









"MACHINES OF LOVING GRACE"
In the second half of the 1960s, under the protection of America's increasingly secure global power, as the Soviet Union declined year by year, the rather small band of rather unruly people who had managed to build the Internet with the risk capital of the US defense budget embraced, and were embraced by, the era's counterculture. Computer people, at least since the late 1930s, had already been a subculture anyway—an Intergalactic Network, in Licklider's phrase. But the '60s generation, the baby-boomer generation, was different. In the long dark shadow of the expanding Vietnam War, weary of the all-or-nothing terror of nuclear weaponry and what a later writer called the "emergency mood,"29 and with the confidence given by two decades of steady economic growth, a significant portion of educated young Americans could see rebellion as fairly safe and possibly even necessary. Some of these rebels were engineers.
The term hackers was born at MIT, specifically at the Tech Model Railroad Club (TMRC), in or around 1959. According to the journalist and historian Steven Levy, hack had long been used at MIT to mean prank. At the railroad club, the undergraduates spending hour after hour wiring circuits and relay switches underneath their vast indoor platform of miniature trains racing from miniature home to miniature intersection used hack to indicate rough work but also to casually honor something done with conscious finesse: "While someone might call a clever connection between relays a 'mere hack,' it would be understood that, to qualify as a hack, the feat must be imbued with innovation, style, and technical virtuosity."30 The arch megalomania of hacking was there from the beginning, as in this poem by a Tech Model Railroad Club regular:
Hacking the grungy, hairy, sprawling hacks of youth; uncabled, frying diodes, proud to be a Switchthrower, Fuze-tester, Maker of Routes, Player with Railroads, and Advance Chopper to the System.31
What turned these model-railroad hackers into wizards of cyberspace was Kenneth Olsen's Tixo. It had been sloughed off by Lincoln Labs, its SAGE purposes over—an old warhorse sent to the pasture of MIT's Research Laboratory of Electronics. Levy describes the astonishment of the hackers when they first confronted this machine:
The TX-O, or Tixo as it was sometimes called, was for its time a midget machine, since it was one of the first computers to use finger-size transistors instead of hand-size vacuum tubes . . . . The TX-O workings were mounted on several tall, thin chassis, like rugged metal bookshelves, with tangled wires and neat little rows of tiny, bottle-like containers in which the transistors were inserted ... Facing the racks was an L-shaped console, the control panel of this H.G. Wells spaceship ... On the sides of the boxes that faced the user were a few gauges, several lines of quarter-inch blinking lights, a matrix of steel toggle switches the size of large grains of rice, and, best of all, an actual cathode ray tube display, round and smoke-gray. The TMRC people were awed. This machine did not use cards ...  . If something went wrong with the program, you knew immediately, and you could diagnose the problem by using some of the switches or checking out which of the lights were blinking or lit. The computer even had an audio output: while the program ran, a speaker underneath the console would make a sort of music, like a poorly tuned electric organ whose notes would vibrate with a fuzzy, ethereal din. The chords on this "organ" would change, depending on what data the machine was reading at any given microsecond; after you were familiar with the tones, you could actually hear which part of your program the computer was working on ... Even more amazing was that, because of these "interactive" capabilities, and also because users seemed to be allowed blocks of time to use the TX-O all by themselves, you could even modify a program while sitting at the computer. A miracle!32
This truly was the beginning of personal computing: some kids playing with military surplus. The difference was that you didn't have to give a batch of punchcards to an operator, who would run the program and give you a printout hours later showing your results. No, the machine, while still physically very large, was now your personal humming and singing friend and partner in exploring new worlds. For probably the first time, non-classified personnel—indeed, simple college students who liked electronics—could book hours and hours to program a computer. All you had to do was not try the patience of John Mackenzie, the technician in charge of Tixo. You signed up on a piece of paper next to an air conditioner. The Tech Model Railroad Club never looked back.33
It was maybe inevitable that the hackers' first great project was to code a war game with two spaceships firing missiles at each other: Spacewar. It was coded for Tixo's successor, a Digital Equipment machine called the PDP-1. MIT hackers spent many, many hours playing Spacewar; they shared the code (on paper tape) with other PDP-1 users; and when they went on to other schools like Berkeley and Stanford they took Spacewar with them, so that for a first generation of American hackers, playing Spacewar was basic bonding.
Hacker games and the intense, all-nighter, takeout-Chinese-and-Coke love affair with the machine that began with Tixo and the PDP-1 coincided with a different, much bleaker set of thoughts about people and machines. In 1963, Clark Kerr, president of the board of regents of the University of California, published a series of lectures under the title The Uses of the University. He described the university in terms any programmer would recognize: it was "a series of processes producing a series of results—a mechanism held together by administrative rules and powered by money." It had two purposes: to generate knowledge workers for what was now being called, in a distant echo of Frank Jewett's 1935 remarks on "intelligence" and electronics, the "information society"; and to fight the Cold War, because knowledge itself had become "an instrument of national purpose, a component of the 'military-industrial complex'"—a phrase popularized by Eisenhower, who warned of the complex's growth. Kerr's analysis expressed what Norbert Wiener had feared in 1948, when he saw cybernetics as foretelling and enabling the demise of a human-centered civilization, to be replaced by a state bewitched by the efficiencies of the machine.34
Kerr was a lightning rod, and his vision received a direct response in the Free Speech Movement, born on the Berkeley campus under the leadership of Mario Savio, who in December 1964 led a protest with the famous words, "There's a time when the operation of the machine becomes so odious, makes you so sick at heart, that you can't take part, you can't even tacitly take part. And you've got to put your bodies upon the gears and the wheels, upon the levers, upon all the apparatus, and you've got to make it stop. And you've got to indicate to the people who run it, to the people who own it, that unless you're free, the machine will be prevented from working at all."35
The Bay Area was the epicenter of this war over the machine. On one hand, the Free Speech Movement issued into the profound inutility of hippie culture and the minimalist economy of mind-altering drugs, back-to-the-land movements, and leaderless communes that aimed at reducing human productivity to as near zero as possible. Opposition to the Vietnam War—a war that featured the world's most technically advanced civilization using its technology to crush an insurgency that, however modernist Communist ideology might be, was led by a peasant army—gave '60s anti-industrialism a very hard edge. Stanford was forced by 1969 to eliminate classified programs on campus. (They were shifted into SRI, the semi-independent Stanford Research Institute, of which Stanford divested itself.)36 The machine was, to a degree, being stopped.
On the other hand, the special genius of California is its ability to turn American nightmares into dreams. In 1967, Richard Brautigan, as poet-in-residence at Caltech—which had always prided itself on marrying the humanities to technology—published his first book, All Watched Over by Machines of Loving Grace. The title poem read:
I like to think (and
The sooner the better!)
Of a cybernetic meadow
Where mammals and computers
Live together in mutually
Programming harmony
. . .
I like to think
(it has to be!)
of a cybernetic ecology
where we are free of our labors
and joined back to nature,
returned to our mammal
brothers and sisters,
and all watched over
by machines of loving grace37
The seemingly contradictory view of machines as partners in a winsome, even blissful conspiracy with nature to eliminate unpleasant labor had an obvious appeal to the counterculture of technologists. The new task was to repurpose the machine as an instrument of personal liberation and create a frictionless, alternative world free from the oppressing state.









THE PEOPLE'S COMPUTER CENTER
The rebranding of the computer from military tool to liberation technology occurred in the Bay Area between 1967 and 1972, and mainly in Silicon Valley. In his history of hacking, Steven Levy proposed an informal set of group principles he called The Hacker Ethic: "Access to computers—and anything that might teach you something about the way the world works—should be unlimited and total. Always yield to the Hands-On Imperative! All information should be free. Mistrust authority—promote decentralization. Hackers should be judged by their hacking, not bogus criteria such as degrees, race, age or position. You can create art and beauty on a computer. Computers can change your life for the better."38 This ethic had its roots in MIT's Tech Model Railroad Club but flowered in the Bay Area.
It flowered most intensely in a Stanford laboratory on a hillside well above the campus, with views of San Francisco and the bay, Mount Tamalpais and Mount Diablo. This was where John McCarthy came from MIT to found the Stanford Artificial Intelligence Laboratory, or SAIL, together with his friend Stephen "Slug" Russell, who had coded Spacewar.39 SAIL was a remarkably free place. Artificial intelligence—Markoff credits McCarthy with inventing the term for a 1956 conference at Dartmouth—could accommodate almost any research project imaginable. Each room at SAIL was named for a place in The Lord of the Rings, as was the vending machine, which served beer. SAIL hosted what was probably the first online drug sale. It had a volleyball court out front and a reservoir nearby for skinny-dipping and a coed sauna.40
Perhaps only in the late '60s could such a place have openly coexisted with the military-industrial complex. In that context, it's worth remembering that some significant but unmeasured percentage of defense-industry workers were doing it as a draft dodge.41 (There was a deferment for strategic industries.) But "mistrust authority—promote decentralization" also just came with hackerdom, regardless of what else one might have done in life. Ivan Sutherland, for example, the person controlling SAIL's ARPA-related budget—he replaced Licklider, who hand-picked him—was a widely liked and admired figure who, in 1963, had coded SketchPad, a fantastically imaginative program that used a light pen to make pictures and architectural drawings, edit and copy and erase: the first graphical user interface. He coded it at around the time he was working for the NSA (seconded from the army).42 It was possible to be a hip hacker spook. To manage SAIL, Sutherland picked Les Earnest, who had been working for the CIA and other intelligence agencies since 1962. Earnest was a bit of a misfit—he once described his race, on an official form, as "mongrel"—and SAIL was just the spot for him. It was a place where everyone dropped work, even the robot, to watch Star Trek together.43
SAIL was not far from being a geek commune. Its intellectual guru was McCarthy, a former Communist Party member who had a powerful vision of computing as a collective act of constant communication. SAIL had a PDP-1—a gift from Digital Equipment—and McCarthy had it rigged up so that everyone at SAIL could share the computer's resources from individual terminals. For several years in the mid-to-late '60s, SAIL had the only system in the world where people all worked from their desktop terminals, including the secretaries.44
Of course they played Spacewar. It was hacker heaven. Two high-school students, Steve Wozniak and Steve Jobs, hung out at SAIL in 1970, visiting their friend Allen Baum. Wozniak biked over from his parents' home in Los Altos. Alan Kay spent two years at SAIL before helping get a new lab off the ground: Xerox PARC, a hotbed for mini-computing. With so many people playing Spacewar, the PDP sometimes froze up. But the SAIL team found a way to have the operating system reallocate time to individual programs, so everyone was able to continue playing Spacewar as well as doing their "work."45
All this collective activity fit with McCarthy's idea of computer sharing. Some at SAIL thought each person should have his or her own computer. McCarthy did not see the point in that. Better to have access to a really great shared computer than be stuck with whatever you could afford for yourself alone. In a paper he delivered in 1970, McCarthy argued that within five years individuals would have terminals that displayed text and pictures on their screens; these would be connected via phone lines to a shared computer, which would contain all kinds of public information, including books and transport schedules and news, as well as personal files belonging to the individual user. In short, a planetary version of the setup at SAIL, which already included an online news service, a markup language for online text, and of course shared gaming software.
McCarthy's networking proposal led a colleague to ask a pregnant question: In such a system, how would anyone know who you really were? But the network idea was too far ahead of its time, and the answer to the question of online identity would be deferred. Instead, in California—the one state where the spirit of the '60s didn't turn into a dying star—computing, like politics, would keep its rebelliousness but combine it with a sturdy, imperturbable self-absorption.
In 1972, Stewart Brand, an extraordinary blend of genuine intellectual and trend surfer, visited SAIL and Xerox PARC and emerged to tell, in the pages of Rolling Stone, the story of a new counterculture. The article was titled "Spacewar." Brand watched a group at SAIL playing the game in an "Intergalactic Spacewar Olympics" that took them "out of their bodies" in a delirium of shooting. Brand went on to introduce readers to Alan Kay and Xerox PARC, then to Resource One, which wanted to create a peer-to-peer system of computers in the Bay Area. One Resource One member would later explain, in the pages of the semi-regular subculture newspaper People's Computer Company, "Such a horizontal system would allow the public to take advantage of the huge and largely untapped reservoir of skills and resources that resides with the people." Brand's own idea, as the historian Fred Turner wrote, was to "take computers out of their military, industrial and academic contexts and turn them into tools for individuals to use as they saw fit."46
Brand drew out a distinction between hackers and "planners," the former being more like cyber cowboys, the latter technicians and bureaucratic drones. Hackers would form a "mobile new-found elite": "Those magnificent men with their flying machines, scouting a leading edge of technology which has an odd softness to it; outlaw country, where rules are not decree or routine so much as the starker demands of what's possible . . . . The hackers made Spacewar, not the planners. When computers become available to everybody, the hackers take over. We are all Computer Bums, all more empowered as individuals and co-operators."47









THE BLIND BOYS OF BERKELEY
Maybe this would be the period of fifteen years ("or 500") that Licklider had foreseen, in "Man-Computer Symbiosis," the period before the machine would be able to run the world on its own, when the human would still be there in the feedback loop, the years that "should be intellectually the most creative and exciting in the history of mankind."
McCarthy's proposal to use existing phone lines as a network was already being carried out, in hacker fashion, by the "phone phreaks," who were introduced to the larger public by a 1971 article in Esquire.48 The Bell telephone system's computers used a set of tones to communicate with each other. The knowledge of these tones spread among young blind men, who could locate the sense in such dolphin sounds. The blind boys used them to make free calls. They also found flaws in the Bell lines and sometimes anonymously alerted the phone company to the existence of broken switches and the like. One of these young blind men randomly happened to phone John Draper. (That is how Draper tells it.)49 Draper entered this tiny subculture and learned the secrets of making free phone calls and even eavesdropping on other people's calls.
Most of the Esquire piece was about phones, the homemade "blue boxes" that made it possible to manipulate them, and the phone-phreak subculture, but at one point Draper (under a pseudonym for safety) let loose what was probably the first crazy-hacker soliloquy: "I'm learning about a system. The phone company is a System. A computer is a System. Do you understand? If I do what I do, it is only to explore a System. Computers. Systems. That's my bag. The phone company is nothing but a computer . . . . Ma Bell is a system I want to explore. It's a beautiful system, you know, but Ma Bell screwed up. It's terrible because Ma Bell is such a beautiful system, but she screwed up. I learned how she screwed up from a couple of blind kids who wanted me to build a device. A certain device. They said it could make free calls. I wasn't interested in free calls. But when these blind kids told me I could make calls into a computer, my eyes lit up. I wanted to learn about computers. I wanted to learn about Ma Bell's computers. So I built the little device . . . . I've had employment problems. I've lost jobs. But I want to show Ma Bell how good I am. I don't want to screw her, I want to work for her. I want to do good for her. I want to help her get rid of her flaws and become perfect. That's my number-one goal in life now."
Like the TMRC boys spending their nights with Tixo, Draper, who had recently been discharged from the air force and was working at National Semiconductor, was in love with the machine, with the illicit thrill of exploring it and learning how to make it do things, and with the curiously protective, but also somehow arrogant, activity of making the system, engineering-wise, "perfect."
Steve Wozniak's mother saw the Esquire article and sent it to her son, who was studying at Berkeley. Before he had even finished the article, Wozniak called his friend Steve Jobs and started telling him about phone phreaks and how (as Wozniak later recalled) this amazing, anonymous central character just went on about how, in Wozniak's excited paraphrase, "'I'm not in it to hurt Ma Bell, I only want to explore the system, the phone company is a computer, a system, and I like to explore computers, find all the little bugs, and where all the little alleyways of this whole system go' . . . kind of a love-hate relationship."50 Wozniak also, through yet more happenstance, found John Draper, who came up to Berkeley and initiated the two Steves into the mysteries of phone phreaking.51 Soon the pair were making and selling their own blue boxes, which was, of course, an illegal business. (The Esquire piece inspired a wave of arrests.)52 Jobs later recalled this as a turning point: "It was the magic of the fact that two teenagers could build this box for a hundred dollars' worth of parts, and control hundreds of billions of dollars of infrastructure in the entire telephone network in the whole world from Los Altos and Cupertino, California. That was magical! And experiences like that taught us the power of ideas, the power of understanding that if you could build this box you could control hundreds of billions of dollars' worth of infrastructure around the world. That's a powerful thing. If we hadn't of made blue boxes there would have been no Apple."53
Levy's Hacker Ethic didn't leave room for such fantasies of omnipotence. Jobs's ridiculous attempt to pass off those fantasies as a tribute to "the power of ideas" was in line with a magnificently self-serving tradition that had decades ahead of it. (Meanwhile, organized crime syndicates were, as the Esquire article pointed out, major buyers for blue boxes.) These young computer engineers and hobbyists, unlike their predecessors from 1916 up into the 1960s, were blind to their own will to power. They had come a long way from the weaponized knowledge of Vannevar Bush, or so it seemed. The few years after 1967 had somehow burnt history away; these were free Californians, for whom the "power of ideas" just happened to lead to the power of power. This helps explain why, with the means for a web-like communications network spread before them, they instead chose the highly individualistic option of building a personal computer.
Jobs, Wozniak, Draper, and others frequented the Wednesday-night potlucks laid on by the People's Computer Company. The PCC had some internal power struggles, however, and when the newspaper split from its sister group, the People's Computer Center, the magic went out of the potlucks. In March 1975, the Homebrew Computing Club was founded to fill the void. It held its second meeting at SAIL, where John McCarthy stood up for his imagined future of communication across a network of terminals linked to each other through a network of central computers laden with the world's information and open to all: in short, the web.
That was the last meeting the Homebrewers held at SAIL. Central computers were just not their style. They wanted to have their very own computers instead, and their machine of choice was not the PDP but the new Altair 8800, a do-it-yourself machine made by MITS in New Mexico. March 1975 was also the month when Paul Allen and Bill Gates visited MITS, bringing Allen's version of the BASIC programming language with them. It was the first product of their new, Albuquerque-based company, then called Micro-Soft. A Homebrew member got ahold of the program—holes punched onto paper tape—and made more than seventy copies, which he distributed to the club members. "As the majority of hobbyists must be aware," Gates wrote in an angry letter that was published by, among other groups, the People's Computer Company, "most of you steal your software."54
The next year, Wozniak debuted the Apple at the Homebrew club. Then the Steves went looking for money. "I met with Steve Jobs and Steve Wozniak, and they were very unappealing people," Arthur Rock, of Davis and Rock, recalled in an oral history. It was 1977. "Jobs came into the office, as he does now, dressed in Levi's, but at that time that wasn't quite the thing to do. And I believe he had a goatee and a moustache and long hair—and he had just come back from six months in India with a guru, learning about life. I'm not sure, but it may have been a while since he had a bath. And he was very, very thin—and to look at [him]—he really belonged somewhere else. Steve Wozniak, on the other hand, had a full beard and he's just not the kind of a person you'd give a lot of money to."55 This was not the last expression of Greatest Generation distaste for its boomer successor, but Rock's was a losing battle. He invested in Apple. It was unpleasant perhaps, but there was money to be made and the younger engineers seemed to find this project exciting.
By 1983, Time magazine had named the computer "Machine of the Year." In January 1984, Apple's extraordinary Super Bowl advertisement captured the belief that computing power was a violent blow against the System, a force for individual liberation. The video showed what looked like gulag prisoners watching, on a giant screen, a man hectoring them about ideology and singleness of purpose. ("Today, we celebrate the first glorious anniversary of the Information Purification Directives. . . . Our Unification of Thoughts is more powerful a weapon than any fleet or army.") The imagery evoked George Orwell's dystopian novel 1984, which had imagined a future that combined Communist uniformity with constant high-tech surveillance, both enforced by "thought police." The blank-faced men were all in gray; a female athlete, running and then swinging a mighty hammer, is in color. With a shout she throws the hammer at the screen which then explodes, bathing the astonished gray men in a blue-white light. Type appears: "On January 24th, Apple Computer will introduce Macintosh. And you'll see why 1984 won't be like '1984.'"56
The ad was aimed at IBM, which had belatedly entered the personal-computer market in 1981 with its PC. In his Apple keynote in 1983, Jobs had invoked Orwell's novel ("Was George Orwell right?") in the course of suggesting that Apple was the only force standing against IBM's desire to dominate "the entire information age": "It appears IBM wants it all," Jobs told his investors. "Apple is perceived to be the only hope to offer IBM a run for its money. Dealers initially welcoming IBM with open arms now fear an IBM dominated and controlled future. They are increasingly and desperately turning back to Apple as the only force that can ensure their future freedom. IBM wants it all and is aiming its guns on its last obstacle to industry control: Apple."
IBM was a credible Goliath, just as it had been twenty-seven years before when Kenneth Olsen targeted it by founding Digital Equipment. Apple, already a $300 million company, was less believable as David. But this Oedipal dynamic of personal liberation proved to be remarkably powerful within the industry itself and even made (parts of) the Hacker Ethic seem like the building blocks for a broader social movement. As an internal document generated for Apple's ad planning put it, "there are monster computers lurking in big business and big government that know everything from what motels you've stayed at to how much money you have in the bank. But at Apple we're trying to balance the scales by giving individuals the kind of computer power once reserved for corporations."57
What this encapsulated was the knowledge within the industry that its own powers could, and very possibly would, be used for evil purposes. This was the "love-hate relationship" with the machine that Wozniak spoke about. When you get close enough to "monster computers" to understand what they can do, you know that they will not necessarily do good. It was up to hackers to do that.
But would they? Earlier generations of American engineers had built computers and computer networks in the service of the state, the defense of Western society, and the pursuit of military victory. This generation was motivated by hobbyist passion, fascination with the machine, perhaps a vaguely political notion that individuals would do something better with computing power than corporations and governments did, and—by profit. Bill Gates's angry letter to the Homebrewers about their pirating of his company's software was just the beginning of a lasting tension about getting paid for information you had compiled, whether that information was code, music, journalism, your own social network, literature, or movies. The type of information didn't matter, as Warren Weaver had pointed out in 1948. If it was digitized, it could be copied and communicated. The constraints, if any, would need to come from outside the system.
But hackers were not really into restraints on access to information—at least, their own access. Hopping digital fences was a basic act of hacker freedom. Apple's marketing suggested this freedom could be shared by everyone once they purchased their own non-IBM machines. Later in 1984, Stewart Brand made one of his Zeitgeist reappearances at the first Hackers Conference, held within view of the Golden Gate Bridge. It had been organized by Brand, Steven Levy (whose book Hackers, with its Hacker Ethic, was being published), and others as a subcultural coming-out event. Steve Wozniak, John Draper, and other Homebrew regulars attended. Afterward, Brand wrote that hackers "are the most interesting and effective body of intellectuals since the framers of the U.S. Constitution. No other group that I know of has set out to liberate a technology and succeeded. They not only did so against the active disinterest of corporate America, their success forced corporate America to adopt their style in the end. In reorganizing the Information Age around the individual, via personal computers, the hackers may well have saved the American economy. High-tech is now something that mass consumers do, rather than just have done to them ... The quietest of the '60s sub-subcultures has emerged as the most innovative and most powerful—and most suspicious of power."58
To the degree that hackerdom was a sub-subculture of the 1960s, it certainly was among the most innovative and most powerful. Whether it was suspicious of power was another question entirely. But many highly innovative people in the industry did indeed think of themselves as working in counterpoint to power, which is at least related to being suspicious of it, and this gave precious energy and focus to the hard work of conceiving and building new companies.
The timing, in terms of the Internet, was perfect. The government was losing interest in it. The military's experiences had been decidedly mixed. The National Security Agency had begun the Community On-line Intelligence System (COINS) as a store-and-forward network in 1965. It wasn't really operational until 1973, at which point it needed to be upgraded to a packet-switching network based on ARPANET technology; the new COINS was operational by 1977. But into the early '80s it wasn't working very well. Meanwhile the Defense Department's own similar system, AUTODIN, never got far off the ground, and was cancelled in favor of a new concept, the Defense Data Network, in 1982. Security was a chronic problem, as designing an open network that also enforced respect for different security-clearance levels was hugely difficult. Standardizing that across agencies, each of which seemed to have different equipment, was almost impossible. The military and intelligence agencies split their networks off from ARPANET in 1984; the next year, the National Science Foundation began work on its own NSFnet, which connected with ARPANET in 1986, creating the Internet. It was an online place for hackers, geeks, and academics to send each other messages freed at last from the US military. It had taken almost twenty years after Doug Engelbart's demo, but it happened.59
For a brief but beautiful period, the Internet really was free. For a time, people didn't even use passwords and participated in true anonymity. This was cyberspace, roamed by cybernauts and Netizens with made-up names. The California dream was coming true. People formed communities and talked about . . .whatever interested them. Accessible to anyone with a phone line, a computer, and a modem, the Internet escaped geography. All the old visions of an independent machine-world were coming true. Soon Netizens were referring to the real world as "meatspace." Looking back, Jack Goldsmith and Tim Wu—both East Coast law professors, the former once a Defense Department counsel, neither given to rhapsodic prose—wrote of "the beginning of a constructive vision of governance liberated from physical and national identity—that is, from our actual bodies and their physical location ... You could alter every aspect of your identity: you could be a man or a woman, young or old, bald or bearded, whatever. With complete control over their identities, people could cluster with congenial souls to create virtual communities. This vision foresaw communities fully liberated from physical space and the constraints of physical identity—the first truly liberated communities in human history."60
The geopolitical space for such a liberation was, for once, ample. Francis Fukuyama's essay "The End of History?" appeared in the summer of 1989, as the Soviet bloc ended with a whimper and the United States reigned as the sole superpower, then and for the foreseeable future. The world was hardly peaceful: Saddam Hussein of Iraq had to be repulsed from occupying Kuwait, and China had to be criticized for crushing the dissent concentrated in Tiananmen Square. But a general set of peaceful universal values, with the United States as their benign guarantor in league with a reinvigorated international community, looked set to prevail indefinitely. Cyberspace made a certain sense as the ultimate expression of this gradually self-liberating world. Technically, the Internet was imperfect, and engineers don't leave imperfect systems alone. Nobody wanted the Internet to ever return to meatspace or be regulated by governments or anyone else. But they did want it to be able to do more and cooler things: to move greater amounts of information faster; to make more and more information freer and freer, so the online world would become ever more important and the offline world, in particular the world of states and governments, ever smaller, ever more isolated, and ultimately irrelevant to human advance.
The great technical steps forward were made by Tim Berners-Lee and a group of academic colleagues at CERN in Geneva, Switzerland, and here and there across the Internet. CERN was a nuclear research facility financed by European states. It was indifferent to Berners-Lee's work; he had to concoct various schemes to convince the institute to let him continue the technical research that would turn the Internet into the World Wide Web (a name he invented). His main effort was to create rules—usually called protocols—for content, so that it could be smoothly delivered to the individual computers that were requesting it. In the jargon of the time, human-readable electronic content was known as hypertext. Berners-Lee invented protocols like http (HyperText Transfer Protocol) and HTML (HyperText Markup Language), and most importantly the structure of the Universal Resource Identifier (URI), which imposed naming conventions enabling a machine to find anything on the Internet that was named properly.61
All this was done without much funding or even interest on the part of governments or companies. The profit was in personal computers and closed corporate computing networks; government's interest was likewise in large closed networks. The Internet was a backwater. A sub-sub-committee of the National Research Council, chaired by Leonard Kleinrock, reported in 1988 that the United States was falling behind Japan and Europe in developing a research network able to offer scientists access to supercomputing resources as well as a way to communicate among themselves. ARPANET, the report said, was barely able to handle email. Kleinrock's committee recommended establishment of a National Research Network. "If we are successful in the NRN development," the report concluded, "our competitive advantage will continue to grow as this unique infrastructure dramatically improves the quality of output and productivity of the research community of this nation."62
The report did not have a huge audience but it did attract the interest of Tennessee Senator Al Gore, who had the forward-looking views of an "Atari Democrat" (so named after the early electronic game company that once employed Steve Jobs) and had backed the National Research and Education Network (NREN), as it was named. Gore dilated on the networking theme in 1991 in an influential Scientific American essay, "Infrastructure for the Global Village": "We have the technical know-how to make networks that would enable a child to come home from school and, instead of playing Nintendo, use something that looks like a video game machine to plug into the Library of Congress. It could be possible to gain access to the most powerful supercomputers in our nation from every PC in America—and to gain access to digital libraries, another key part of the national information infrastructure we need."63
That same year, the High-Performance Computing and Communications Act, known as the Gore Bill, was signed into law by an enthusiastic President Bush. When Senator Gore and Governor Bill Clinton campaigned for the White House against President Bush, they promoted the Information Superhighway as part of their optimistic agenda for a first baby-boomer presidency. And in the first year of the Clinton presidency, Gore, now vice-president, presented a sweeping program aimed at creating a smooth information network, for rich and poor, across America:
[T]he Administration will support removal, over time, under appropriate conditions, of judicial and legislative restrictions on all types of telecommunications companies: cable, telephone, utilities, television and satellite . ... Our goal is not to design the market of the future. It is to provide the principles that shape that market. And it is to provide the rules governing this difficult transition to an open market for information. We are committed in that transition to protecting the availability, affordability, and diversity of information and information technology, as market forces replace regulations and judicial models that are no longer appropriate . . . . Without provisions for open access, the companies that own the networks could use their control of the networks to ensure that their customers only have access to their programming. We have already seen cases where cable company owners have used their monopoly control of their networks to exclude programming that competes with their own. Our legislation will contain strong safeguards against such behavior.
The National Information Infrastructure Gore was proposing must, he emphasized, be an "open platform."64
The light-touch, enabling approach of the early Clinton administration was crucial to the growth of the Internet and its relative distancing from a strong national agenda. It was also a recognition of hacker autonomy. Science had been officially de-funded in the 1980s; indeed this was the reason for the NRN report's hurt and plaintive tone. That left the Internet itself, like the discarded ex-military Tixo that the MIT boys had got from SAGE, as something to be programmed by hackers and obsessives, like Tim Berners-Lee.
When Berners-Lee attended the Hypertext '91 conference in San Antonio, he and his principal collaborator, Robert Cailliau, were the only ones with a web project. Even then, it was just a demonstration; the paper they submitted had been rejected. The Internet world was still small and that helped it stay open: code, in particular, was meant to be shared, in a continuation of the ethic expressed in the sharing of BASIC at the Homebrew club. (Microsoft was not very interested in the Internet or the web; there was no clear way to monetize an open platform.) As Berners-Lee developed the web, his main institutional anchor was the Internet Engineering Task Force (IETF), a self-governing professional body, founded in 1986, that functioned mainly by email lists. "IETF meetings were characterized by people in T-shirts and jeans, and at times no footwear," Berners-Lee recalled. "They would meet in different rooms and talk excitedly." It was a charmingly unhurried world, remote from normal worries.65
These were not really government people. They had their own subculture which was apolitical or maybe antipolitical. It was a subculture that valued discussion and consensus, rather than campaigning and voting, and which valued smart work, specifically the work of writing code to solve real problems. Coding was public performance, in every sense: other engineers could look at your code and see how you solved the problem. But unlike other forms of performance, code was tangible, replicable and, most remarkable of all, there to be altered by anyone who thought the performance could be just a bit better. (The nearest comparative is musical notation, except with code there is no sound. There is a result.) The subculture was therefore both highly collective and highly individualistic, shy and vain, and very productive. The people in it did not think government could do anything to the Internet except screw it up; the people in government weren't sure they understood it, and their geek friends were content to let them wonder.
The ultimate such person was Jon Postel, who since 1977 had been responsible for building and running the nascent Internet's structure of names and numbers. The naming system was administered by Stanford Research Institute under contract to the Defense Department, but Postel was the real authority. His intelligence and stewardship earned the admiration of Internet engineers everywhere. It was as Stewart Brand had predicted: "When computers become available to everybody, the hackers take over." John Perry Barlow, then occupied mainly with ranching and writing lyrics for the Grateful Dead, recognized as much when he formed the Electronic Frontier Foundation with Mitch Kapor (founder of Lotus 1-2-3) and John Gilmore of Sun Microsystems. "Imagine discovering a continent so vast that it may have no end to its dimensions," Barlow wrote. "Imagine a new world with more resources than all our future greed might exhaust, more opportunities than there will ever be entrepreneurs enough to exploit . . . imagine a place where trespassers leave no footprints, where goods can be stolen an infinite number of times and yet remain in the possession of their original owners . . . where only children feel completely at home."66









THE RETURN OF THE STATE
The Clinton administration's plans for the information superhighway, like other US plans of that time, took American superiority as a given and of indefinite duration. Hackers thought they were uncatchable and had the future on their side: they could always hole up in cyberspace until the rest of the world realized it was the place to be. But if the web future really was so awesome then people were bound to start fighting over it; American and hacker supremacy could not last forever.
The first good angel to be cast out of Paradise was Jon Postel. He could see by the late 1980s that the arrangement between government (mainly the US government), the Internet, the engineering subculture, and the private sector was going to change. One direction, as in Kleinrock's committee report at the National Research Council, was to bring the US government in as a basic funder and general organizer; this would have involved appealing to national self-interest, which the NRC committee did, and so accepting to some degree that the Internet would be split up along national or regional lines, as was indeed implied by the report's reference to the European and Japanese threats. The model would be multiple Internets overseen by individual governments.
Another direction was internationalist, looking to the International Telecommunications Union, an intergovernmental organization headquartered in Geneva. (It began life as the International Telegraph Union in 1865.) The model would be a single Internet overseen by an international governmental body. There was little interest in this option in the United States, whether in government, among businesses, or, initially, in the subculture.
A third direction was toward more private control, which the Defense Department somewhat inadvertently advanced when it put the Stanford Research Institute's contract to maintain the root servers of the Internet—the structuring, maintenance, and provision of online naming that had been handled by Postel—out for competitive bidding. The winner was Government Systems, Inc., which took over administration of the Network Information Center (NIC) from SRI in October 1991.67 The work was subcontracted to a small minority-owned company called Network Solutions, which had been helping administer MILNET since 1983 and went on to get the contract from the National Science Foundation for its NSFnet assets in 1992. Network Solutions and Government Systems were both experienced defense contractors, though GSI was in turn owned by Infonet, which provided networking services for multinationals and was controlled by major non-American telecommunications firms like Swisscom and Telefonica. That kind of commercial networking, in 1992, was genuinely big business; the proto-Internet contracts were not. Until 1995, Network Solutions fulfilled them at a loss.68
The fourth direction was direct subcultural system management. After all, the Internet, and indeed much of digital computing after 1970, was primarily a subcultural project. In the 1980s this subcultural quality increased as the US military pulled away, wanting to keep its networks secure; the relative importance of academia (and of non-Americans) grew; the subcultural private sector (e.g., Apple) became very large; and non-military parts of the government, like the National Science Foundation, saw that the Internet was becoming too private for its own further funding to be appropriate.69 Against this backdrop, people who had been working on the Internet as their subcultural project for years—people like Postel, Vint Cerf, Robert Kahn, and others—perceived a need to stabilize Internet governance and keep it from becoming either overly commercial or caught up in rivalries among governments. Toward that end, in June 1991, Cerf proposed forming an Internet Society, soon known as ISOC, to take Internet governance forward.
It moved at an academic pace. Some of the Europeans liked ISOC because they saw a chance to introduce European technology and technical specifications into what had been a US-dominated evolution. ("We had a hidden agenda in Europe," was how one European charter member of ISOC put it.)70 The American engineers behind ISOC fundamentally wanted something bottom-up, with consensus decisions and everybody working hard and doing their share; in short, something like the engineering subculture they knew and which had built the Internet—or something like the IETF, only more able to defend itself.
"One of the things we hoped to do with the Internet Society was to provide a home for that activity, which was an unincorporated, informal group of engineers who all sort of got together in the same place at the same time and called themselves the IETF," Lyman Chapin, who authored the initial ISOC announcement along with Cerf and Kahn, recalled. "But they [the IETF] weren't a company, they weren't a standards body, they weren't anything like that, so we wanted to bring those activities under some kind of umbrella that would at least give them some structure—more from a perception that nature abhors a vacuum, and public policy and governance structures abhor a vacuum even more than nature does, so if we didn't have some sort of a structure for it we were fairly certain that other folks would quickly find one, and we thought we'd rather be in a position to define it ourselves than have it defined for us." The UN-ITU alternative loomed, Chapin said—"government-driven, corporate-interest driven, profit-driven, monopoly driven"—and the Internet needed something that "would shield it and would provide some of the unavoidable trappings of legitimacy but do so in a way that was compatible with the cultural preferences of the (primarily) engineering population of Internet activists at the time." In October 1991, Postel became the first ISOC member. His membership number was 1314159, or 1 followed by the first six digits of pi.71
In 1995, Cerf was asked by an American official, "Is ISOC claiming that it has jurisdiction and overall responsibility for the top level address and name space" of the Internet? Cerf replied, "My bias is to treat all of this as a global matter"—not a US one—"and to settle the responsibility on the Internet Society as a non-governmental agent serving the community."72 The Clinton administration didn't want government running the Internet either and, like the Internet Society, didn't think an intergovernmental body would work. Yet Postel, Cerf, and the others did not want legal responsibility, with the prospect of endless lawsuits, nor were they very good at wrangling all the many players, including governments, who would need to accept Internet Society leadership, few of whom had any interest in "the cultural preferences of the engineering population of Internet activists." The Internet and computing, taken out of government hands in the 1970s and '80s, were being tugged back.
The Clinton Administration only wanted to tug them halfway. The White House's chief interest was in a thriving Internet that would lead to new industries and economic growth and possibly even, in that it would open up closed societies to new information, advance freedom and democracy. The goal was sometimes called "privatization" but that wasn't quite right. There did need to be some profit in growing the Internet but it was never going to be turned over to Network Solutions, which after 1995 was owned by a reliably American defense contractor—San Diego-based Science Applications International Corporation, or SAIC—and no longer subcontracting for a subsidiary of a consortium of foreign telecommunications companies. Even SAIC realized that Network Solutions' effective monopoly couldn't last. But as SAIC's CEO and (in 1969) founder, J. Robert Beyster, later recalled, "SAIC's ownership of NSI rubbed a lot of people in the Internet community the wrong way."73 SAIC knew its way around Washington very well and had the resources to press its case as well as handle litigation. Having bought NSI for $4.7 million in 1995, it was able to raise $59 million in a partial initial public offering in 1997. Mike Daniels, a SAIC executive who chaired NSI's board, said they brought "at least one-half of the entire United States Senate and House members as well as senior White House and cabinet-level officials" to tour the Network Solutions facility in Virginia. Meanwhile, when ISOC created an International Ad Hoc Committee in 1997 to try to resolve the many impasses on Internet governance, Network Solutions was not invited to join (and the US government got just one spot, representing the National Science Foundation). ISOC was drifting ever closer to the United Nations and helped set up a Swiss group called CORE to take over from Network Solutions in line with a memorandum of understanding developed by ISOC. The Geneva ceremony celebrating all this had no invitees from government. CORE planned to take over naming authority in January 1998, as Network Solutions' existing contract neared expiry.74
That didn't happen. In December 1997, at a meeting of the IETF—possibly not as shoeless as in Berners-Lee's day, but still informal—a group of engineers explained in the presence of a White House delegation that they intended to proceed with the CORE plan. They also heavily implied a threat to wreck the domain-name system if they didn't get their way. The White House's lead representative, Ira Magaziner, then had a private meeting with Postel. He explained that in fact the United States would decide how the Internet was to be governed and that Network Solutions did not answer to Jon Postel.
Postel still had a card to play. He was head of the Internet Assigned Numbers Authority. This handled the structure of the fundamental naming system (which was, of course, ultimately done with numbers rather than words) on a contract between the Department of Defense and the University of Southern California, Postel's employer. He believed this position gave him the power to physically control the root domain of the Internet. So he set up his computer at USC to replicate the computer at Network Solutions, then emailed the administrators of the eight non-US-controlled root servers to get their information about root changes from his machine. All eight complied. The other four were at Network Solutions, NASA, the Defense Department, and the US Ballistics Research Lab at the army's Aberdeen Proving Ground, where almost exactly eighty years before young Norbert Wiener had served as a human computer.75
After a rough call with Magaziner, Postel backed down and accepted that full root authority would remain with servers controlled by the US government, not by the Internet subculture. The Clinton administration released a "green paper" with its own proposals, which said nothing about the Internet Society, CORE, or any of the rest of it.76 At the end of the year, the United States vested Internet policy in a new group, the Internet Corporation for Assigned Names and Numbers (ICANN), under contract to the Commerce Department rather than Defense. This reflected the belief that the Internet was now fundamentally economic rather than military. Network Solutions continued to maintain the domain-name registry although it lost the power to register new names itself; the registry was enough, though, and in 2000 SAIC sold Network Solutions to Verisign for an inflated $19.27 billion.77 The subculture had lost the battle. Governments and large corporations would now shape the Internet.
 


  1.   Conway and Siegelman, Dark Hero, pp. 82-3; Erico Mario Guizzo, The Essential Message: Claude Shannon and the Making of Information Theory (Cambridge, Mass.: MIT, 2003), p. 63; https://youtu.be/cZ34RDn34Ws, https://youtu.be/C8kU3oZwVJA ("the ultimate machine").
  2.   J.C.R. Licklider, "Man-Computer Symbiosis," IRE Transactions on Human Factors in Electronics, March 1960, pp. 4-11, http://worrydream.com/refs/Licklider%20-%20Man-Computer%20Symbiosis.pdf.
  3.   J.C.R. Licklider, Libraries of the Future (Cambridge, Mass.: MIT Press, 1965), https://comminfo.rutgers.edu/~tefko/Courses/e553/Readings/Licklider%20Libraries%20of%20the%20future%201965.pdf.
  4.   Fred Turner, From Counterculture to Cyberculture: Stewart Brand, the Whole Earth Network, and the Rise of Digital Utopianism (Chicago: University of Chicago Press, 2006), p. 108; National Research Council, Funding a Revolution, p. 78; William Aspray and Robert Norberg, "An Interview with J.C.R. Licklider," Oct. 28, 1988, in Charles Babbage Institute, University of Minnesota, Center for the History of Information Processing, p. 29, http://purl.umn.edu/107436; J.C.R. Licklider, "Topics for Discussion at the Forthcoming Meeting," Washington, DC: ARPA, April 23, 1963, http://www.kurzweilai.net/memorandum-for-members-and-affiliates-of-the-intergalactic-computer-network; John Markoff, What the Dormouse Said: How the Sixties Counterculture Shaped the Personal Computer Industry (New York: Penguin, 2005), pp. 7, 42.
  5.   The demo is on YouTube: https://youtu.be/yJDv-zdhzMY.
  6.   Christophe Lécuyer and David C. Brock, Makers of the Microchip: A Documentary History of Fairchild Semiconductor (Cambridge, Mass.: MIT Press, 2010), p. 16.
  7.   Spencer E. Ante, Creative Capital: Georges Doriot and the Birth of Venture Capital (Boston: Harvard Business Press, 2008), pp. 227-231; Martin Kenney and Urs von Burg, "Institutions and Economies: Creating Silicon Valley," in Martin Kenney, ed., Understanding Silicon Valley: The Anatomy of an Entrepreneurial Region (Stanford: Stanford University Press, 2000), pp. 230-2 (Shockley and Fairchild).
  8.   G. Stewart Gillmor, Fred Terman at Stanford: Building a Discipline, a University, and Silicon Valley (Stanford: Stanford University Press, 2004), pp. 58-64, 69; Zachary, Endless Frontier, p. 395. Terman also studied under Norbert Wiener (Gillmor, Terman, p. 58).
  9.   Gillmor, Terman, pp. 508-9.
  10.   Gillmor, Terman, pp. 254-60; Leslie, The Cold War, pp. 7-9, 51-6, 160-74.
  11.   "Edward Ginzton," an oral history conducted in 1984 by A. Michal McMahon, IEEE History Center, http://ethw.org/Oral-History:Edward_Ginzton; "Oswald Garrison Villard," an oral history conducted in 1984 by A. Michal McMahon, IEEE History Center, http://ethw.org/Oral-History:Oswald_Garrison_Villard; Leslie, The Cold War, pp. 172-3; Lécuyer, Makers, 102-3.
  12.   Ante, Creative Capital, pp. 18-20, 72-76.
  13.   Ante, Creative Capital, pp. 75-78.
  14.   Ante, Creative Capital, pp. 51, 116.
  15.   Ibid., p. 81.
  16.   Ibid., p. 88.
  17.   Ante, Creative Capital, p. 106 (Eisenhower), pp. 115-6 (JDRB).
  18.   Ante, Creative Capital, p. 111.
  19.   Ibid., p. 149.
  20.   Ante, Creative Capital, pp. 150, xviii.
  21.   Jerry N. Hess, "Interview with General William H. Draper Jr.," Washington, DC, Jan 11, 1972, pp. 1-2, 8, http://www.trumanlibrary.org/oralhist/draperw.htm; Mauree Jane Perry, "Interview with William H. Draper III," Oct. 2005, National Venture Capital Association, Venture Capital Oral History Project, p. 10 (Anderson), pp. 11-12 (Doriot), pp. 20-1 (Gaither), http://digitalassets.lib.berkeley.edu/roho/ucb/text/draper_william_donated.pdf.
  22.   Perry, "Interview with William H. Draper III," pp. 11-12 (Doriot), pp. 22-3 (limited partnership); Paul Bancroft III, "Early Bay Area Venture Capitalists: Shaping the Economic and Business Landscape," interview conducted by Sally Smith Hughes in 2010, Regional Oral History Office, The Bancroft Library, University of California, Berkeley, pp. 24-5 (on limited partnership). Bancroft also had military and intelligence experience, both in Washington ("immediately out of Yale I joined what we'll call the Department of Defense") and, with the air force, in Japan ("in basically the estimates end of intelligence"), pp. 10-11.
  23.   David H. Hsu and Martin Kenney, "Organizing Venture Capital: The Rise and Demise of American Research and Development Corporation, 1946-1973," Berkeley Roundtable on the International Economy Working Paper, Dec. 2004, pp. 23-4, http://www.brie.berkeley.edu/publications/WP163.pdf.
  24.   Leslie, The Cold War, pp. 114-126.
  25.   Stuart W. Leslie, "The Biggest 'Angel' of Them All: The Military and the Making of Silicon Valley," in Kenney, ed., Understanding Silicon Valley, p. 52.
  26.   Jesse Boyett Anderson, "William Rambo, a pioneer in radar jamming, is dead at 90," Stanford Report, July 11, 2007, http://news.stanford.edu/news/2007/july11/rambo-071107.html; "William R. Rambo," interview conducted by A. Michal McMahon, IEEE History Center, November 27, 1984, http://ethw.org/Oral-History:William_Rambo#Military_and_NASA_Contacts.
  27.   "Thomas J. Davis, 77, Investment Executive," New York Times, Sept. 13, 1990, http://www.nytimes.com/1990/09/13/obituaries/thomas-j-davis-77-investment-executive.html.
  28.   "Watkins-Johnson Company History," Funding Universe, n.d., http://www.fundinguniverse.com/company-histories/watkins-johnson-company-history/.
  29.   Marc Greif, The Age of the Crisis of Man: Thought and Fiction in America, 1933-1973 (Princeton: Princeton University Press, 2015), p. 27.
  30.   Steven Levy, Hackers: Heroes of the Computer Revolution (Sebastopol, CA: O'Reilly Media, 2010), originally published in 1985, p. 10.
  31.   Levy, Hackers, p. 11.
  32.   Levy, Hackers, pp. 16-17.
  33.   Levy, Hackers, p. 16.
  34.   Turner, Counterculture, p. 12.
  35.   Ibid., pp. 11-12.
  36.   Richard Lyman, "At the Hands of the Radicals," Stanford Alumni, Jan/Feb 2009, https://alumni.stanford.edu/get/page/magazine/article/?article_id=30505; Markoff, Dormouse, pp. 171-4; Leslie, The Cold War, chap. 9.
  37.   Quoted in Turner, Counterculture, pp. 38-9.
  38.   Levy, Hackers, pp. 27-34.
  39.   Levy, Hackers, pp. 50-1; Markoff, Dormouse, pp. 81-2.
  40.   Markoff, Dormouse, pp. 107-8, 105.
  41.   Ibid., xvii, 159, 164, 189.
  42.   Aspray and Norberg, "An Interview with J.C.R. Licklider," p. 56.
  43.   Markoff, Dormouse, p. 97.
  44.   Markoff, Dormouse, p. 90.
  45.   Ibid., pp. 94, 134.
  46.   Turner, Counterculture, pp. 115, 117.
  47.   Ibid., p. 117.
  48.   Ron Rosenbaum, "Secrets of the Little Blue Box," Esquire, Oct. 1971, http://www.webcrunchers.com/october-1971-issue-of-esquire/.
  49.   John Draper, "The Big Suck In," Dec. 27, 2013, http://www.webcrunchers.com/the-big-suck-in/.
  50.   Steve Wozniak speech to the Denver Apple Pi club, Colorado School of Mines, Oct. 4, 1984, https://youtu.be/oeVOpDUWwpU.
  51.   Markoff, Dormouse, p. 272; John Draper, "The Woz," Dec. 27, 2013, http://www.webcrunchers.com/the-woz/.
  52.   John Draper, "Writing of the Esquire Article," Dec. 27, 2013, http://www.webcrunchers.com/writing-of-the-esquire-article/.
  53.   In the documentary Silicon Valley: A 100 Year Renaissance, https://youtu.be/dxCNvNwl60s.
  54.   Markoff, Dormouse, pp. 284-5; Michael Moritz, Return to the Little Kingdom: Steve Jobs, the Creation of Apple, and How It Changed the World (New York: Overlook, 2010), pp. 110-19. Gates talks about the moment (and shows the paper tape) in a 1994 interview, in "The history of Microsoft—1975," https://channel9.msdn.com/series/history/the-history-of-microsoft-1975.
  55.   Arthur Rock, "Early Bay Area Venture Capitalists: Shaping the Economic and Business Landscape," interview conducted by Sally Smith Hughes in 2008 and 2009, Regional Oral History Office, The Bancroft Library, University of California, Berkeley, pp. 55-6, http://digitalassets.lib.berkeley.edu/roho/ucb/text/rock_arthur.pdf.
  56.   The ad is at https://youtu.be/VtvjbmoDx-I.
  57.   The keynote speech is at https://www.youtube.com/watch?v=lSiQA6KKyJo; David Burnham, "The Computer, The Consumer and Privacy," New York Times, March 4, 1984, http://www.nytimes.com/1984/03/04/weekinreview/the-computer-the-consumer-and-privacy.html.
  58.   Turner, Counterculture, p. 138.
  59.   Reed et al., DARPA Technical Accomplishments, chapter 20, pp. 12-21.
  60.   Jack Goldsmith and Tim Wu, Who Controls the Internet?: Illusions of a Borderless World (London: Oxford University Press, 2006), pp. 16-7.
  61.   Tim Berners-Lee (with Mark Fischetti), Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web (New York: HarperCollins, 1999), pp. 38-42.
  62.   National Research Council, Toward a National Research Network (Washington, DC: National Academy Press, 1988), p. 4, http://books.nap.edu/openbook.php?record_id=10334&page=4.
  63.   See Computer Professionals for Social Responsibility newsletter, summer 1993, http://cpsr.org/prevsite/publications/newsletters/old/1990s/Summer1993.txt/; Al Gore, "Infrastructure for the Global Village: A High-Capacity Network Will Not Be Built Without Government Investment," Scientific American, Sept.1, 1991, p. 153.
  64.   Remarks by Al Gore, National Press Club, Dec. 21, 1993, http://www.ibiblio.org/nii/goremarks.html.
  65.   Berners-Lee, Weaving, pp. 50, 54.
  66.   Goldsmith and Wu, Who Controls, pp. 18-9.
  67.   Network Working Group, Request for Comments 1261, Sept 1991, https://tools.ietf.org/html/rfc1261; J. Robert Beyster and Michael A. Daniels, Names, Numbers, and Network Solutions (La Jolla, CA: Foundation for Enterprise Development, 2013), pp. 44-5.
  68.   Beyster and Daniels, Names, pp. 60-62, 67, 73-4; In re Infonet Services Corp. Securities Litigation 310 F.Supp.2d 1106 (2003), http://www.leagle.com/decision/20031416310FSupp2d1107_11327.xml/IN%20RE%20INFONET%20SERVICES%20CORP.%20SECURITIES%20LITIGATION.
  69.   Janet Abbate, Inventing the Internet (Cambridge, Mass.: MIT Press, 2000), pp. 216-7.
  70.   Internet Society, "An Oral History of the Internet Society's Founding," interviews conducted April-June 2013, http://www.internetsociety.org/internet-society-founding.
  71.   Internet Society, "An Oral History"; Vint Cerf, closing keynote speech, 2012 Internet Society Global INET, http://www.elon.edu/e-web/predictions/isoc_20th_2012/closing_keynote_vint_cerf.xhtml.
  72.   Goldsmith and Wu, Who Controls, pp. 37-8.
  73.   Bruce V. Bigelow, "The Untold Story of SAIC, Network Solutions, and the Rise of the Web—Part 2," July 30, 2009, http://www.xconomy.com/san-diego/2009/07/30/the-untold-story-of-saic-network-solutions-and-the-rise-of-the-web-part-2/.
  74.   Bigelow, "The Untold Story"; Goldsmith and Wu, Who Controls, pp. 39-40.
  75.   Beyster and Daniels, Names, pp. 125-31; Goldsmith and Wu, Who Controls, pp. 43-6.
  76.   National Telecommunications and Information Administration, US Department of Commerce, "Improvement of Technical Management of Internet Names and Addresses; Proposed Rule," (the "Green Paper") Feb. 20, 1998 http://www.ntia.doc.gov/federal-register-notice/1998/improvement-technical-management-internet-names-and-addresses-proposed-.
  77.   Beyster and Daniels, Names, p. 163.










CHAPTER 3.  THE SPLINTERNET
Although Silicon Valley grew up as an arm of American military and intelligence services, it matured in pursuit of a Californian globalizing vision that was not meant to be simply a captive of American interests, political or economic. From the 1970s through the 1990s, Silicon Valley's expansive vision could, however, reasonably be seen as compatible with American power and the "universal values" it sought to promote in the post-Cold War world.
Globalization and American power began to diverge in the 2000s, when Vladimir Putin consolidated power in Russia, when China emerged as more a rival than a partner, and when the international community, which the White House now openly mocked, decided the global war on terror was mainly the American war on terror. The world economic depression in 2008 exacerbated this de-Americanization of the global vision as national economies began to conclude, under economic stress, that they were more on their own than the Washington Consensus and the spirit of Davos had led them to believe. Europe's anticipated role as the thinking person's United States, leading the world by force of calm example and unshakeable prosperity, swiftly collapsed in the face of economic downturn and renewed nationalism. The advent of the Obama presidency in January 2009 interrupted this de-Americanizing, indeed de-Westernizing, of globalization and of global power, but only for a moment.
As the United States became displaced from the center of a globalizing world, the global web it dominated began to go multipolar as well, in its own way.









WALLED GARDENS FULL OF MONEY
The fragmentation of the web was to a great degree a result of the web's technology itself and the distinct marketplace it gave rise to. Berners-Lee's structure made Internet content persistent and free. Both of these technical facts had huge implications. They made a universal web world possible, and they made it possible, inadvertently, to profit from it.
The web made the Internet persistent because it assigned fixed addresses to content within a universal naming system, and those addresses were not tied to any particular computer (or server). Initially, a piece of Internet content would reside on one particular server in the world. Typing in a URI (later changed to URL) and hitting "Enter" was simply how you sent a request to the server to return to you that particular piece of content. The middle part of the URI (the "foobar" in www.foobar.com) originally referred to the server itself—the computer that hosted the content. This suggested that if the server went, the content would go too. So far, so traditional. Content would expire when its creator went out of business, got bored, or died, though it might last on paper somewhere.1
However, there was really no reason why the URI had to refer to one particular server. It just had to refer to the unit of content, which itself could be hosted anywhere and could, in theory, be readily available forever, as long as someone somewhere continued to host the name. Given the ease of copying digital content, including whole websites, the technology gave web content a feeling of persistence. That's not quite the same thing as permanence, but it was close enough.2 The ephemeral Internet became the web of property.
But not everything was property. A website, yes; a domain name could have only one owner at a time. Other types of intellectual property that had once been tightly controlled were more problematic. Anything that could be digitized—books, music, movies—could be transmitted over the web and shared, which meant that, technically speaking, a lot of what had once been private property to be shared at a price would become public property shareable for free.
There was a philosophical split on this question early on. One of the more enthusiastically hippie-like pioneers of hypertext, Ted Nelson, thought that each piece of content on the web should have a price attached to it, probably a very tiny price but a price nonetheless, so that hypertext work would be compensated. He did not think content should be free. When Berners-Lee visited California in 1992, he and Nelson had Indian food for lunch, chatted in Nelson's pyramid-like office near his houseboat in Sausalito, and agreed to disagree.3
Content providers did nonetheless fight back against having their efforts made free. They were only partially successful. It took technical innovation, such as "watermarking" content so it couldn't be copied, and an immense amount of legal action, some of it brutal, until certain norms were established and web users ceased stealing and sharing everything they could.
Safeguarding traditional content was relatively straightforward. There was a different type of web content, however, which was harder to categorize but centrally important. This was anything people created online that was not envisioned as being for sale, but which the creators did want to be accessible, findable, and knowable, including, as with an advertisement, for the purpose of selling something else.
This category of online things, for which there is no name, accounts for the bulk of what has been on the web. It encompasses any public website page, including pages that advertise a product or offer part of something, such as an article or a song, that can be purchased. It includes one's blog or videos, social-media posts, and anything else that isn't behind some security or content-protection wall. This is the ocean of web content and it's hard to make sense of. At first, it was pretty random. What began to organize it was the dynamics of search and of community.
Search and community are closely related. A search engine finds commonalities among things based on their attributes, then ranks the results according to indices of popularity, power or quality. An online community does very nearly the same thing and is often put together by search, whether a search instigated by the platform (it will search for and present people who might be of interest to you or interested in what you are interested in, and rank them for you, and you use this to build your community) or by the community participant himself or herself. The difference with a community is that it is self-organizing and self-reorganizing in ways that search results are not. Yet the basic dynamic structure is roughly the same.
In the 2000s, search engines resisted the importance of community because it seemed like it would introduce friction and unreliability into the relationships among online things. A community, from a search point of view, would all too quickly become a source of judgment, capable of skewing results. A search engine looked at what users really do with content, not at what a particular interested community might have to say about content, and for that reason an engine's results can be viewed as more objective than a community's consensus. Being affected by a community—or by any kind of editor, which is effectively the same thing—would make a search engine non-universal, non-objective, and in the end either much less attractive in its results or much trickier to code.
Online communities, in that same decade, tended at first to go in the opposite direction, toward tightened connections among people and what was called a "walled garden." The point of nearly every online community—apart from the early billboard services, which were outgrowths of academic and hacker communities, and leaving aside relatively obscure chat protocols like IRC—has not been to provide a public service but to make money, one way or another. This could be done through selling memberships, selling products, or selling advertising, all of which would be easier if the garden had a wall around it. (Search engines, of course, hated walls because they just got in the way of the engines' endless search for content.) In each case, what was really being sold by online communities was the community itself and each of its individual members. That was clear even in one of the earliest non-scientific networks on the Internet, called the WELL. Users paid a low subscription fee then an hourly fee (also low) to participate in discussions with each other. As one of its managers said, "The WELL 'sells its users to each other' and it considers its users to be both its consumers and its primary producers."4
The appeal of the walled garden was that it would give members the sense of being somewhere safe and socially coherent, and it would give the garden's builder and owner a manageable community to sell to advertisers, retailers, or simply to its members through charging a fee. The concept didn't really work, however. One thing users clearly valued about the web was the lack of confinement, the sense of limitless options. Walled gardens didn't fit with that. Many users also valued their anonymity or at least obscurity, whereas walled-garden communities wanted to know as much as they could about members so they could sell to them better.
So companies decided to build open platforms that made their money by scaling to a large audience—and then segmenting that audience based on the mining of data generated by the audience's activity rather than, say, by each user filling out a survey of interests. Interfaces would be designed to minimize the visible presence of the company. The user experience would be free of charge and would be effortless enough to encourage repeated use of the platform, which then generated ever more information. A well-made platform would be almost like a mini-web; it would have ever more users and ever more monetizable information about each one, without any of it being much noticed by the users themselves.
The question hanging over this business model was: What is the end game? The end game is to get back to the walled garden by other means, creating unique communities that cannot be copied by competitors and then selling those communities to advertisers.
It's instructive to contrast the trajectory of online search engines and commercially driven online content-based communities with those of offline content communities, which were also walled gardens of a kind, though they were better known as media monopolies. A newspaper, for example, sold its readers to advertisers. Often, because of the high costs of production, it would have a captive audience, say the residents of Middletown, USA, and anyone who wanted to reach that audience had to pay the Middletown newspaper for the access. Newspapers (and magazines, and broadcast networks) had a monopoly on the sale of their own audiences.
The great difference with online media companies like search engines and social networks was that, unlike pre-web media companies, they got their content from what other people did online, thus avoiding the traditional content-creation costs, and sold the content onward not as the Middletown Sentinel-Ledger but, to simplify a bit, as the word "Middletown," which when purchased by an advertiser would alert the media company to send an ad (or "recommendation") to Middletown-related content, whether it be search results or a Facebook page or a book on Amazon. The fundamental relationships across these platforms are the same. Online commerce is about creating and selling communities that are based on the content and affinities created in online life itself.
In the offline, real world, an "affinity" might be "living in Middletown." In the online world, your affinities could be so much more varied and unique to you—thus the delightful feeling that your online self might be more you than your offline self, and no longer stuck in Middletown reading the Sentinel—and the communities you joined could be chosen by you rather than forced on you by geography and the accidents of life.
In the excitement of living online it was easy to forget that the only reason all these platforms of identity and community existed in the first place was to enable the packaging of groups for sale. But forgetting that was essential to the online experience, because it is a bit demeaning to realize that what you think is important about you and what the algorithm thinks is important about you are two different, usually unrelated things. And it can be alarming to think that everything you're doing is being fed into a giant surveillance machine in order to maximize the targeting of sales. Yet that is what happened once the web became commercialized in the late 1990s.
For a company to pull this off requires tact and subtlety, both of which had to be learned by web companies, often painfully, over the course of the 2000s and into the present decade. Facebook, for example, offered a great way to connect with friends and interesting strangers and exchange content like photos and reflections on the day's news. But for the company to make money and keep the platform up and humming it needed to be able to target advertisements. Charging for access, as the WELL once had, wasn't going to work; even traditional media, which had something to sell that people at least seemed to want, had trouble pulling off that kind of walled garden online. So Facebook came up with Beacon, in 2007. Beacon enrolled other companies ("third parties")—Coca Cola, for example, and the New York Times—to give Facebook information about you, which Facebook could then use to deliver advertisements that would, based on your activity, be likely to interest you. But did you really want Facebook to know you were a Pepsi person? Users revolted almost immediately, and by 2009 Beacon was abandoned. Facebook then continued to grow its user base. It did not abandon the consumer-tracking idea, however. It found different, less visible ways—the Connect program, for example—to segment its community members into groups that it could sell to advertisers.5
Or consider Google. At first, Google searched the entire web for content and returned content in the same universal way. It was Google.com, just as Amazon was Amazon.com. But soon enough there was also google.co.uk, and there was amazon.co.uk. The nation-state as a meaningful sorting principle for content took hold, and the horse was out of the barn. Geo-tagging—giving content a physical location—took off. Google got into maps and, with Google Earth, geographical data visualization. Google Earth grew out of a project called Keyhole (which Google purchased in 2004) that had been funded by the CIA's venture arm, In-Q-Tel, which itself got much of its funding for the project from the US National Geospatial-Intelligence Agency (NGA), the administrator, along with the US Air Force, of the Global Positioning System, known as GPS. Soon Google, fueled by its spectacular advertising income, was sending special cars across the planet to photograph every street, giving a new meaning to the term "search engine." Some governments balked at the idea of having every house and hovel in their nation photographed, and refinements were introduced, but resistance was basically futile. All the world's content was being gently but firmly returned to Earth, because that was one way to segment the advertising market ever more precisely.6
The more that the information you get from the web is targeted at you, the less your web experience is part of anything universal. As the algorithms improve, the web experience becomes more personal, more local, and more national, simply as a result of commercial imperatives. You don't see the walls around your garden, but they are there.
It has to be admitted that this really does, as the phrase goes, "improve the user experience." There were never that many people who wanted to live only in cyberspace. Most people think locally and buy locally, and tend to filter out what isn't interesting to them, making their worlds smaller and more manageable. They want to enhance their lives, not escape them. Once the web was fast enough, the dominant commercial imperative was not just to widen the market but to deepen it, which meant making the user experience ever more local, ever more geo-tagged. The political, commercial, legal, and regulatory trends all pointed in the same direction: a more localized web. The only real cross-current was machine translation, which would eliminate the (underestimated) language barrier, but languages so far have been resistant to quality translation, and besides, if most people's lives are easily led within the languages they know, what is the point? Web platforms were built on a universal basis, with the one-to-many relationship familiar to database designers: one user facing, potentially, the other ten billion users alive on the planet and the infinity of content they generate. They were designed that way because exclusivity—the walled gardens—seemed unpopular and was anyway small beer; because it was technically possible with increased processing speeds; because, perhaps, it appealed to the benign megalomania characteristic of Silicon Valley from its inception; and because, when the scaling was done well and with a light touch, you could have your walled gardens anyway, only tens of millions of them, and the user's search results and social newsfeeds would be satisfyingly personalized.
Web companies didn't set out to become surveillance engines. Surveillance was a byproduct of marketing.









THE BACK DOOR
The United States government, having built the Internet for military reasons, and having then given it over from the Department of Defense to the Department of Commerce in a flourish of post-Cold-War, all-boats-rise boomer optimism, was mostly content to leave the web and Silicon Valley alone for a while. That didn't last long though. The prospect of free and anonymous communication, given the reality of evil in the world, alarms any government, and Internet fears quickly took political form as what Netizens mockingly called the Four Horsemen of the Infocalypse: terrorists, pedophiles, drug dealers, and money launderers.
The United States felt a responsibility to be able to keep criminals and their products (such as child pornography) off the web.7 That meant policing the web. The first major attempt to do that was the Clipper Chip, setting off what Steven Levy, writing in the New York Times Magazine in 1994, described as "the first holy war on the information highway":
The idea is to give the Government means to override other people's codes, according to a concept called "key escrow." Employing normal cryptography, two parties can communicate in total privacy, with both of them using a digital "key" to encrypt and decipher the conversation or message. A potential eavesdropper has no key and therefore cannot understand the conversation or read the data transmission. But with Clipper, an additional key—created at the time the equipment is manufactured—is held by the Government in escrow. With a court-approved wiretap, an agency like the F.B.I. could listen in. By adding Clipper chips to telephones, we could have a system that assures communications will be private—from everybody but the Government.
And that's what rankles Clipper's many critics. Why, they ask, should people accused of no crime have to give Government the keys to their private communications? Why shouldn't the market rather than Government determine what sort of cryptosystem wins favor? And isn't it true that the use of key escrow will make our technology so unattractive to the international marketplace that the United States will lose its edge in the lucrative telecommunications and computer fields? Clipper might clip the entire economy.8
Levy introduced to the public a newish group called "cypherpunks." He quoted Tim May, a former Intel engineer and libertarian of the Ayn Rand school and a founding cypherpunk: "The war is upon us. Clinton and Gore folks have shown themselves to be enthusiastic supporters of Big Brother."9
This was the beginning of the "back-door" controversy, which still rages more than twenty years later. The Clipper Chip was defeated in what is remembered as the first Crypto War. But the idea itself only migrated, heading from Capitol Hill to the Pentagon and the intelligence agencies, where it became a question of national-security surveillance rather than law-enforcement or morals surveillance. Military and intelligence people knew much better than legislators how to talk to geeks; some were geeks themselves. That line had been crossed many times already.
It wasn't the money so much as the mission. The first dot-com crash in 2000 had left the industry with a new sense of vulnerability. The attacks the next year on 9/11 gave the military an extraordinary sense of urgency. The intelligence community experienced a complex set of realizations all at once. There was guilt for having failed to anticipate the attacks, enhanced interest in data mining and surveillance, and excitement at the money and authority that would be theirs in a shadowy, indefinite conflict that emphasized targeting, their specialty. For everyone, 9/11 brought a renewal of purpose and often of patriotism; even hardened Valley libertarians of the Ayn Randian stripe could not accommodate terrorism.
The first decade of the twenty-first century was one of government-tech cooperation. Google Earth developed from an intelligence project and continued to be used for targeting. The DARPA project Personalized Assistant that Learns (PAL) led to a five-year, multi-institution program called Cognitive Agent that Learns and Organizes (CALO) which in turn begat Siri, the Apple voice-recognition query engine on the iPhone.10 The NSA had a host of programs, from the early 1990s onward, for undermining encryption standards by building in back doors that only the agency (in theory) would know about and be able to exploit. Security companies cooperated with the NSA because they thought they had to, or that it didn't really matter, or that it was a good idea, or they were being paid.11
Silicon Valley companies' main interest was in gaining security from theft and compromise of their own systems. Agencies like the NSA devoted enormous and increasing resources to identifying such threats. This was information that private companies, whether defense contractors or not, needed and sought; to the degree that government has a responsibility to protect the country's companies, those companies were also entitled to threat information. Usually the government framed its involvement in terms of infrastructure protection; as early as May 1998, under the Clinton administration, "critical infrastructure protection," explicitly including cyber infrastructure, was made a US priority in Presidential Decision Directive 63.12 As web companies became ever more part of the economy's daily functioning, they became part of that infrastructure too. The extent and tempo of cyber penetrations increased in the 2000s, notably when Chinese hackers stole (from Lockheed) plans for the top-secret Joint Strike Fighter, beginning in 2007.13 Valley companies grew more concerned, more eager to enjoy whatever protections the NSA and other agencies might be able to provide, and more willing to build closer relationships with the state. Willy-nilly, the Valley was becoming more distinctly American, more national, and more a tool of the state.
There was abundant slipperiness in the terms used as this process slowly unfolded. One phenomenon was the way "critical infrastructure" could gradually encompass much of the Valley. Another was how, in the cyber context, the usual distinctions between "theft," "espionage," and "war" seemed to vanish. The Iraq War is sometimes called the first "cyber war" in that high-tech spying and targeting led directly and at high speed to killing the nation's enemies. The NSA and CIA learned to work very closely together, notably in their joint Special Collection Service, and in conjunction with military units like the navy's Information Operations Command or the air force's Seventieth Intelligence, Surveillance, and Reconnaissance Wing. There was no bright line between such military programs and the NSA's Special Source Operations division, whose purpose was to monitor US communications companies.14 Since the bulk of the world's web communications run through private American telecommunications and technology companies, it might be said that American spy agencies had no choice, to fulfill their mandates, but to spy on American companies. Whatever the degree and nature of "public-private cooperation" might be, the NSA has never left any doubt that it will do whatever it can to get the information it considers necessary to carry out its mission. This mission is tightly wrapped in with the CIA's more direct mandate to infiltrate foreign cyber systems, learn how to disrupt them, and even to destroy them and destroy enemies using these means. Perhaps the most distinctive thing about the US Cyber Command, established in 2009 within US Strategic Command, is that its head was also the head of the NSA.15 The line between military commands and spying was erased, as was the line between infiltrating American web companies and fighting the nation's wars.
Phrasing all this in terms of patriotic cooperation to protect national infrastructure was not exactly dishonest but it was quite misleading. And it was fatal to the universality of the web, because major web companies were and are global but cannot be both global and subject to the intricate agendas of US intelligence and defense institutions, whose purpose is to defend national interests, not universal interests.
Major web companies were slow to see this. For example, in 2012 the CIA issued a request for proposals for a cloud-computing platform to handle the US intelligence community. Within that community, the debate over taking sensitive information private began in 2011, for the simple reason that many people had realized that the US government had lost any chance of keeping up with the private sector. Cloud computing was a perfect example: a method for dynamically marshaling idle server space—gathering together a virtual computer through remotely connecting real ones—it had had a lot of private money thrown at it before it worked; and because, like any modern computer, the cloud could both compute and store, it put immense computing power in the hands of anyone, and any company, able to access it. You no longer had to buy and maintain your own servers. Best of all, because the cloud was distributed across a network it had surge capacity; a server can crash—that's what DDOS (Distributed Denial of Service) attacks, by far the most common cyber attack, aim at—but the cloud does not crash. In principle, the CIA could have spent billions on a new, hardened network . . . and then have watched as it became outdated around the time of completion. The CIA and many other agencies had already had this unhappy experience. So the CIA requested proposals. Microsoft and AT&T were early bidders. IBM and Amazon—a pioneer in cloud-computing services—came in later. Amazon won.16
It was not a big story at the time. Investors' interest was almost more in what the deal said about IBM, whose stock promptly dropped. But it was also significant that the traditional defense contractors like Lockheed were not part of it (though there was at least one unidentified bidder). Those companies were stuck in the classic defense-contractor bind: they excel at innovating within existing production lines but not outside them, rather as their military clients tend to fight the last war. Beyond that, it was little noted that one of the great American Internet startups had opted to host one nation's secrets. Yet how could Amazon continue to present itself as a genuinely international company? China's Amazon equivalent, Alibaba, was already gaining global market share (and would be valued above Amazon within two years), so maybe the CIA contract was accepting the inevitable. This is how a country or an industry shrinks and becomes more national, even without meaning to.
In retrospect, the Amazon-CIA deal, which closed in 2013, might have been a high-water mark of cooperation between the US tech industry and the nation's government. That same year an NSA contractor, Edward Snowden, began releasing secrets he had taken before leaving his job and his country. The programs he exposed—PRISM, BULLRUN, and the rest—showed that the US government was spying on citizens and non-citizens, allies and enemies, and it was using American companies to do it, whether they knew it or not and whether they liked it or not.
In one way, this was unremarkable. What are spy agencies for, if not to spy? America's enemies and allies spy on America, too. Most of them also spy on their own citizens. The NSA and Britain's version of it, GCHQ, had been undermining cryptography and implanting "back doors" for at least a decade, often though not always with the knowledge of the companies affected. The general idea that computers and information networks in the hands of the state pose a threat to privacy and civil liberties was likewise well established; the New York Times reporter David Burnham wrote a good book about it (with a foreword by Walter Cronkite) thirty years earlier, The Rise of the Computer State: The Threat to Our Freedoms, Our Ethics and Our Democratic Process, and there have been plenty more since. Norbert Wiener glimpsed this outcome in 1948.
The Snowden revelations were shocking, however, in Silicon Valley itself because they showed definitively that even Silicon Valley was not independent and its biggest players were not masters of the universe. These were not quite global companies after all, it seemed. They were not extra-territorial. Cyberspace was an illusion. The ever more globalizing future of which Silicon Valley engineers were the vanguard, always online, bustling with their mobile devices from one interesting LEED-certified green place to the next, was being taken away. Instead, they were the unwitting agents of the intensely anxious security apparatus of one particular nation in a moment of time.
Silicon Valley and the broader engineering subculture reacted badly to the Snowden revelations. In 2012, Lewis Alexander, head of the NSA, had been a featured, and celebrated, speaker at the annual DEFCON conference, one of the premier security-analyst/hacker conclaves. After Snowden, Alexander's 2013 invite was rescinded. He did speak at the similar Black Hat conference and was heckled for his trouble.17
These were serious challenges. The Valley and Washington both knew that the government had difficulty hiring the tech people it needs. The Defense Department's Cyber Command couldn't meet its hiring goals. The Valley pays more, allows lifestyles that don't always fit well with a high security clearance, and in many cases offers more interesting work. It is common practice for ambitious young engineers to join government long enough to earn top-secret clearance then head off for the balmy clime of California. That simply renders the defense sector still more dependent on private contractors who can pay these talented people more and thus provide the technology the state wants.
The defense sector is also dependent on tech contractors for hardware itself. For single-use items, like fighter planes and ships, this is unproblematic; the dependence is mutual. But satellites, for example, were now being built for private use first. Where private tech innovation was once a byproduct of government work, the situation for some industries was now reversed, and defense use was a byproduct of private development. An industry brought into existence by the US government had become, in terms of technological innovation and available capital, much bigger than the government itself, which now depended upon it. When government officials today get extremely agitated about cyber security, it is partly because they know how far out of their control cyber security is. By 2015, the Defense Department, the air force, and the Department of Homeland Security would all be in California setting up what looked oddly like embassies in a foreign land.18
The most spectacular Valley reaction to Snowden was encryption, or re-encryption. Beginning in 2013, Google, Facebook, Microsoft, Twitter, Apple, and other companies started scrambling communications between their data centers and rebelling against government surveillance and requests for information. In 2014, Apple and Google both announced they would encrypt iPhones and Android phones so only the user would be able to access the information. They added that they would encrypt the information such that the companies themselves could not access it, even when asked by law enforcement officers with a warrant. The companies were throwing away the keys.19
The heads of the FBI and Britain's GCHQ said loudly and publicly that the Valley had gone too far. FBI chief James Comey suggested that encryption would enable murderers.20 Valley executives responded in turn by not attending—in Palo Alto!—a summit on cybersecurity in February 2015 hosted by the president himself.21 Tim Cook, however, the CEO of Apple, did attend, only to emphasize that Apple's data protection was strong and getting stronger.22 In May 2015, thirty-seven tech companies along with dozens of industry leaders and associations sent a letter to President Obama:
Encryption protects billions of people every day against countless threats. This protection would be undermined by the mandatory insertion of any new vulnerabilities into encrypted devices and services. Whether you call them "front doors" or "back doors," introducing intentional vulnerabilities into secure products for the government's use will make those products less secure against other attackers ... If American companies maintain the ability to unlock their customers' data and devices on request, governments other than the United States will demand the same access, and will also be emboldened to demand the same capability from their native companies.23
This was a serious backlash by some of the most powerful companies in the world. Microsoft, Google, Apple, Cisco, Facebook, and others all signed, though Amazon did not. The more mainstream defense contractors, who do an enormous business in cyber security—Lockheed, Raytheon, SAIC, and newer ones like Mandiant and Palantir—also did not.
By June 2015, Cook was referring to "the battle over encryption. Some in Washington are hoping to undermine the ability of ordinary citizens to encrypt their data. We think this is incredibly dangerous ... we believe the contents of your text messages and your video chats is none of our business." He also took a swipe at Google, Facebook, and others: "[S]ome of the most prominent and successful companies have built their businesses by lulling their customers into complacency about their personal information. They're gobbling up everything they can learn about you and trying to monetize it ... You might like these so-called free services, but we don't think they're worth having your email, your search history and now even your family photos data mined and sold off for god knows what advertising purpose." The ad-blocking business began to look like a promising new field.24
The US defense community thus found itself dependent upon a tech industry that was capable not only of organized, open, and vehement direct opposition to the highest authority, but also of very brutal public infighting over the most fundamental issues. Cook's choice was hardly random: gathering and mining vast amounts of online information about individuals is something the NSA, GCHQ, Chinese and Russian and Turkish and other security services with the wherewithal, Amazon, Facebook, Google, Twitter, and others all have in common.25 It might not be sustainable. As early as January 2014, a Pew poll found that 57 percent of eighteen- to twenty-nine-year-olds believed Snowden had served the national interest with his revelations—and Silicon Valley skews young. Later polls showed Americans of all ages modifying their behavior online specifically to protect themselves from government intrusion, and agreeing that government and private organizations alike knew too much about what Americans do online. Ninety-one percent of American adults polled in late 2014 believed that consumers had lost control of how information about them was used by private companies. For major companies like Facebook, Google, and many others, re-encryption made business sense, both for their real or potential employees and for their customers.26
The US government often struck back against encryption by invoking foreign enemies. Non-US states with significant technological bases, notably China and Russia, had never really bought the notion of a universal web anyway. They simply saw the web as another sphere for strategic competition based on their own national interests. As Michael Daniels of SAIC recalled, "I had conversations with representatives of the UN, China, Russia and others from 1995 to 2000, they always would ask the question, 'Why should the US government own any of this or control it?' That it should be free to the world. And we want more control over our particular parts of it."27 In other words, when the web took off in 1995 Russia and China started saying they wanted "more control over our particular parts of it." They have been trying to gain that control ever since.
The US response to those attempts was not a universal one but a national one, calling on American companies to provide special access for American government agencies, not for foreign agencies. In terms of the once-universal web this was simply an admission of defeat. It was a way of saying that the web was already a battleground for states and that companies had better get behind the flag if they wanted protection. The United States did not intend to help fragment the web into national spheres of interest, but that is what happened.









MR. SLIPPERY
In this two-way struggle between the state and the Valley, the third force was the engineering and hacker subculture itself, which had been central to the construction of cyberspace and took a protective interest in it. Before the commercial period of the 1990s, the Internet had grown up rather behind the backs of mainstream government and big business and required special skills and willful optimism and conspiratorial determination to enter, which is why the engineering and hacker subcultures have always been at once egalitarian, cliquish, and snobbish. That an insular subculture produced such an open global phenomenon is just one of those ironies in which history abounds. As the commercialized Internet began to show the benefits of ever more local content targeting, and as major powers began to feel control of the web was a strategic necessity, the subculture of hackers and engineers looked for ways to preserve their old free world.
Among hackers, one of the most influential science fictions was True Names, a novella by Verner Vinge published in 1981. Inspired by predecessors like Vannevar Bush, Ted Nelson, and John Brunner, and more directly by his anonymous wanderings through the electronic corridors of the University of California at San Diego's PDP-11, Vinge imagined a world in which hackers—called warlocks—use a new virtual-reality technology to penetrate computers around the world, for profit and for fun. The technology gives them entry into a non-physical, electronic world called The Other Plane. They have to keep their true names a secret, even from each other, because once the "Great Enemy," the US government, discovers a warlock's identity it can force him or her to work for it. The Great Enemy might also cause the warlock to suffer what the book calls a True Death.28
The true name of the warlock Mr. Slippery, the book's protagonist, is discovered by the United States; he turns out to be a novelist in California. The government puts him to work tracking down an evil rogue named the Mailman who has been recruiting warlocks on The Other Plane—"cyberspace" had not been invented quite yet—with promises of real-world power. The government gives Mr. Slippery access to ARPANET, and with this greatly enhanced computing power Mr. Slippery gets close to the Mailman, who then fights back. Virtual and real wars ensue; after many true deaths, the world is plunged into economic depression. At the story's end, Mr. Slippery learns that the Mailman was an NSA artificial-intelligence program mistakenly left on. It had taught itself how to dominate cyberspace. Mr. Slippery's fellow warlock Erythrina (Debbie Charteris of Rhode Island), a dying former military programmer who had, like Mr. Slippery, gained great power on The Other Plane and ARPANET and proved able to resist the temptations of power and tyranny, feeds her wised-up self into the Mailman's program, restoring equilibrium to the relationship between human and machine before her death.
True Names staged the first cyberwar. The US government and the NSA were the bad actors in the story, but there is attraction as well as repulsion at work. Hackers and spies are not all that different; both want to explore forbidden places where power lies, steal the secrets, and not get caught. When they're honorable, they do this to defend a place that matters to them. Spies and hackers both strive to keep their true names hidden. Their fugitive ambition makes an uncertain guide. In True Names as in The Spy Who Came in from the Cold, the first question is, Where does power really lay? The second question is, How can I know my acts are moral if I can't answer the first question?
For hackers in the 1970s and '80s, power seemed pretty certainly to lie with the US Defense Department and the intelligence agencies. But these institutions were not only serving the most powerful nation in the world, they were key builders of cyberspace itself. This presented hackers with a paradox: they depended on the Great Enemy for the very existence of their special world, a world outside which they were powerless. No cyberspace, no hackers.
There were some ways out of this. One was Jon Postel's engineer-coup, which didn't work. Another was to have so much of the web in corporate hands that it would effectively escape government—a possibility still being developed.
A third way out was to posit that cyberspace was already free and it was your hacker duty to defend that freedom.
That was the option most hackers chose. The way in which hackers in the Homebrew club, to Bill Gates's frustration, insisted on sharing code pointed in one direction: toward open-source code. The tradition was an old one within computer engineering. IBM had shared its operating-system code with a private user group (called SHARE)29 since the mid-1950s. ARPANET's wide-open Request For Comment (RFC) process, which essentially made Internet development an ongoing public conversation, began in 1969 and continues today. (Jon Postel was the RFC editor from the beginning until his death in 1998.) The Unix operating system, developed at AT&T's Bell Labs beginning in 1969, was also free and open to the public, not because AT&T wanted it to be but because an antitrust decision required it. When AT&T and Bell Labs separated in 1984, Bell began to sell the code. But it was already in the wild, and both the legal method for keeping software non-proprietary (GNU-GPL) and the actual coding of a Unix-like, but free and open, operating system date from that time. The Unix-based Linux operating system debuted in 1991.
The beauty of open-source code is that anyone can try to improve it and no one can take it private (although private companies can build on it). The principle behind the open methods of the Internet Engineering Task Force, the ARPANET RFCs, the World Wide Web Consortium (W3C), GNU-GPL, and Linux is the same: the web, and digital computing itself, are global public goods, and there is a global subculture of engineers and programmers for whom all this is a communal property, one whose independence—from the marketplace, mainly, but also from government—they defend, in part from a strong ethical urge but also because, if somehow the web or computing code were to go back behind government or corporate walls, they would become a subculture without tools.
This method of shared, open code worked and works extremely well. NASA was among the earliest to take up Linux, which first established itself as an operating system for supercomputers. Then, echoing the Homebrewers, companies like Dell, IBM, and Hewlett-Packard began using Linux in order to escape Microsoft's iron hold on its MS (for Microsoft) DOS operating system. The Firefox browser was also built as an open-source project in order to counter the dominance of Microsoft's Internet Explorer. Google's Android phone operating system is built on Linux. The WordPress open-source content management platform, widely used as a blog and website armature, powers (as of 2015) well over a fifth of the web. The dominance of open source on the web and in digital computing is the most tangible ongoing triumph of the Hacker Ethic.30
Beyond open source, a free cyberspace might also be conceived as an electronic frontier where hackers would defend against surveillance by governments or others. The hacker group Cult of the Dead Cow, for example, was founded in 1984 in an abandoned slaughterhouse in Lubbock, Texas, by three people using the names Grandmaster Ratte, Franken Gibe, and Sid Vicious. They were systems operators on Bulletin Board Systems, the servers that people could log in to from their terminals to exchange messages, play games, and share news. The cDc organized hacker BBSs across North America in the '80s; in 1994, the cDc formed one of the first Usenet hacker newsgroups and cDc member Omega coined the term "hacktivism." That was also the year the American broadcast journalist Geraldo Rivera characterized the group as "a bunch of sickos." He had a point: cDc was publishing instructions on how to make simple land mines and remix traffic signals for maximum gore. But displays of juvenile delinquency were only part of their work.31
The chief activity of hackers within this world was breaking into systems and then making the break-in public (at least to other hackers). West Germany's Chaos Computer Club had led the way in 1984 when it used the postal service's home terminal system to steal the equivalent of $50,000, returning it in a public ceremony the next day. Such system hacking grew more popular and more serious in the late '80s and '90s as the systems got bigger and, with the web, a larger public went online. The Chaos Computer Club's annual conference began in 1984; the cDc sponsored the first American equivalent, HoHoCon, in 1990. The first DEFCON was in 1993, Black Hat in 1997. At these conferences hackers would discuss "exploits," which usually involve finding a security vulnerability that allows the hacker to gain control of or "own" a system.32
The tricky part, as in True Names, is figuring out which side you are actually serving. When threatened, hackers would always say that their exploits were intended to expose vulnerabilities so they could be fixed, echoing John Draper's phone-phreak soliloquy on Ma Bell. ("I want to help her get rid of her flaws and become perfect.") Exposing a vulnerability without there being a way to patch it—a "zero-day exploit"—pushed the limit of this explanation. Yet there was a very real ethic underlying hacker exploits, because behind them was the idea that cyberspace needed defenders, and the cybersecurity of corporations and governments had now become the concern of hackers because, as everyone had come to realize, corporations and governments were a big part of cyberspace itself. Hacker conferences in the 1990s very quickly became cybersecurity conferences attended by military and intelligence people and corporate security officers. Black hat and white hat hackers—the good-cowboy/bad-cowboy metaphor came easily on the electronic frontier—quickly began to look like gray hats. Mudge, for example, a hacker at cDc and the influential Cambridge hacker group L0phT Heavy Industries, registered some famous Microsoft exploits and was a colleague of the Australian hacker Julian Assange; Mudge's L0phTCrack password-discovery tool is still used today. In a famous piece of political theater in 1998, he and six other L0phT hackers appeared before a US Senate committee to discuss cybersecurity and disturbed everyone by suggesting they could take down the Internet itself. From these roots, Mudge went on to a successful stint at the Defense Department, helping identify vulnerabilities, before leaving government for Google. Once cybersecurity became, in the late 1990s, a serious business, many hackers found that hacking from the inside was almost as interesting as hacking from the outside and much more lucrative.33
Others found that staying outside could be, for some, more lucrative still. Organized crime, as in the days when Vegas mobsters bought blue boxes from phone phreaks, formed one market for vulnerabilities. Another was corporations that, rightly fearing the commercial results of having their vulnerabilities exposed, tried to keep exploits private; the hacker got paid, the vulnerability was identified and patched, and nobody needed to know. Governments provided yet another market, but with an interesting twist: they would buy vulnerabilities, especially zero-day vulnerabilities, and not use them. This now-common practice constitutes a sort of covert arms race, more like espionage than a traditional arms race, although of course any arms race involves espionage until there is a treaty, and sometimes even then. The difference with vulnerabilities is that, unlike with a missile, their acknowledgment, except in very vague terms, can render them useless; they'd just be patched. The fact that an exploit vendor might sell to more than one government makes the situation still more complex. A cyberwar with these weapons would be a war of the blind. And it would, in its effects, have little if anything to do with defending the freedom of cyberspace. Cyberspace would simply be the battlefield.
But here again, the hats quickly turned gray. For example, hacktivists have often opposed authoritarian governments, which typically restrict Internet freedom much more than their less rigid counterparts. But if one were to wage cyberwar against such governments, what would that mean? Destroying their servers? They would simply be rebuilt.
This was precisely the issue in 1999, when the Cult of the Dead Cow, L0phT Heavy Industries, the Chaos Computer Club, and other hacker groups issued what amounted to a declaration of cyberpeace, typically enough in opposition to another hacker group that had declared cyberwar, in this case on China and the Saddam Hussein regime in Iraq.
An international coalition of hackers strongly condemns the Legion of the Underground's (LoU) recent "declaration of war" against the governments of Iraq and the People's Republic of China. Citing human rights violations and other repressive measures the LoU declared their intention to disrupt and disable Internet infrastructures in Iraq and China. In a decision that was more rash than wise, the LoU will do little to alter existing conditions and much to endanger the rights of hackers around the world.
We—the undersigned—strongly oppose any attempt to use the power of hacking to threaten or destroy the information infrastructure of a country, for any reason. Declaring "war" against a country is the most irresponsible thing a hacker group could do. This has nothing to do with hacktivism or hacker ethics and is nothing a hacker could be proud of ... 
"It is shortsighted and potentially counterproductive," added Reid Fleming of the cDc. "One cannot legitimately hope to improve a nation's free access to information by working to disable its data networks . . ."
The signatories to this statement are asking hackers to reject all actions that seek to damage the information infrastructure of any country. DO NOT support any acts of "Cyberwar." Keep the networks of communication alive. They are the nervous system for human progress.34
Weirdly enough, the hacker consensus was defending state sovereignty as a necessary evil.35 Netizens, by definition—they were from the "empire of Mind" after all—couldn't take territory, and states had enough means at their disposal to isolate their citizens, in effect withhold them from cyberspace. For hackers, keeping cyberspace open to all was the prime directive. Cyberspace was not so much a lost paradise as a work in progress. The work of the hacker was not to shape the content of cyberspace but to enable anyone to reach it.
The tumult of the Arab Spring in 2011 suggested that communication technology might actually create an alternative, stateless zone from which to launch attacks on authoritarian governments: a web-enabled sequel to 1989, without all the superpower tension. The Tunisian and Egyptian revolts were publicized and organized through social media and Virtual Private Networks, with some key assists by foreign hackers; in its early days the Libyan revolution, intricately bloody, could be followed only on social media; much the same was true in Bahrain. Syria turned the arrows around: a peaceful protest for many months, the Syrian revolt in its online gave the regime the chance to identify opponents, and in the ensuing civil war all the wrenching videos and nightmarish online testaments couldn't change the reality that the regime was not folding and the world's great powers could not agree on a course of action.36 As with web governance, the cycle of change arrived back at the post-Cold War fact set of US-China-Russia-Europe. The wildly increased availability of information about the horrors and injustices of Syria did not result in anything of moment; the players played their cards in about the way they would have under less public circumstances, which mocked the hope that public exposure could bring change. The use by the Syrian regime of the web against its enemies and their foreign supporters, as via the government-loving Syrian Electronic Army, made the mockery worse; the eventual use of social media as a prime recruiting tool, saturated with martial, youthful enthusiasm, for the beheading, enslaving Islamic State completed a bitter reversal. By 2014 an Internet scholar wrote, "there is as much government Internet repression as there are possibilities for Internet political expression."37 That's not how this was supposed to work out, an indifferent balance of ones and zeroes.
Yet however divided and compromised the international engineering and hacker subculture might have become—and so quickly!—some in that world plugged away, cultivating the political space that the web still had available, namely the space in between major powers and institutions, none of which wanted any of the others to win. It was quite a metaphor for the broader realignment of the world post-1989. Everyone (even, in some ways, the United States) wanted the United States to have less power and less responsibility; yet no one was eager for any others to get more. So the players would watch and wait, block and bait, and watch again. This created political space for the subculture, which contained most of the people who really understood the technical realities of the web—they were like the IT department for the planet—to turn the power configuration to advantage.
In its 1998 green paper on Internet governance the United States recognized "that its unique role in the Internet domain name system should end as soon as is practical. We also recognize an obligation to end this involvement in a responsible manner that preserves the stability of the Internet. We cannot cede authority to any particular commercial interest or any specific coalition of interest groups. We also have a responsibility to oppose any efforts to fragment the Internet, as this would destroy one of the key factors—interoperability—that has made the Internet so successful." The ensuing creation by the United States of the nonprofit, California-based Internet Corporation for Assigned Names and Numbers (ICANN) went a modest distance to loosening the American hold on the Internet. Jon Postel's old redoubt, the Internet Assigned Numbers Authority (IANA), was technically a department of ICANN but contractually it came under a "stewardship" agreement with the Commerce Department's National Telecommunications and Information Administration (NTIA). In March 2014, the NTIA asked ICANN to convene a "stakeholder" group to come up with a recommendation for what to do with IANA's powers. This would result in further shrinkage of US power over the Internet. ICANN's stakeholder group accepted a number of parameters from the NTIA for its deliberations, including that there be safeguards against "capture" by "governments or intergovernmental organizations."38
The result, as of 2015, was a classic bureaucratic hairball, unlovely and impenetrable. The basic goal was nonetheless clear: to entrench Internet governing authority outside government and in the Internet Engineering Task Force and similar professional groups like the Internet Architecture Board, the five nonprofit Regional Internet Registries, and the Internet Society, still going after more than two decades. The ICANN "multistakeholder" proposal essentially recommended that Internet governance continue as it had been, a subcultural project run by consensus. However, not everyone in the Internet community was happy with ICANN, so a parallel report was generated on ICANN reforms . . . and meanwhile, there was the question of how Verisign (the private company that succeeded Network Solutions) should coordinate changes to the root zone once the United States was not directly involved. The NTIA asked Verisign and ICANN to report on that question, but Verisign, citing business reasons, asked that these deliberations be in secret. There were then three separate tracks, all dedicated to ensuring that the Internet continue to run but not be captured by one government, all governments, or the private sector. It was 1998 all over again, and just as messy, such that when the September 2015 deadline loomed the United States decided to punt. The Commerce Department opted for a one-year extension. On one hand, this was like setting a time bomb to go off during the 2016 presidential campaign. On the other hand, the ability of the international engineering subculture, or the Internet community, to confound government with its muddle-along ways had a beauty to it.39 The ghost of Jon Postel must have had a laugh.
 


  1.   Berners-Lee, Weaving, pp. 39, 61-2.
  2.   Initially, many people thought that electronic records would last forever. This led to, among other things, a movement for the "right to be forgotten." But while information could be easily copied and was therefore capable of being made permanent, someone would have to perpetually ensure that the link (the URL) continued to refer to a valid path, from the "server name" to the final digit. That was not necessarily the case for much, maybe most, electronic information. This is, in the end, not wholly unlike the situation with books and periodicals. Someone still has to care enough to look; someone else has to have cared to preserve the information and make it available; and the searcher and the information have to be able to meet. See Jill Lepore, "The Cobweb: Can the Internet be archived?" The New Yorker, Jan. 26, 2015 http://www.newyorker.com/magazine/2015/01/26/cobweb.
  3.   Berners-Lee, Weaving, pp. 64-66; also see Jaron Lanier, Who Owns the Future? (New York: Simon and Schuster, 2013), pp. 226-30.
  4.   Turner, Counterculture p. 146.
  5.   Facebook, "Leading Websites Offer Facebook Beacon for Social Distribution," Nov. 6, 2007, https://newsroom.fb.com/news/2007/11/leading-websites-offer-facebook-beacon-for-social-distribution/; Louise Story and Brad Stone, "Facebook Retreats on Online Tracking," New York Times, Nov. 30, 2007, http://www.nytimes.com/2007/11/30/technology/30face.html; Nicholas Carlson, "Facebook Connect Is a Huge Success—By the Numbers," Business Insider, July 1, 2009. Facebook's hardware "beacons," launched in 2015, turn the word to a different purpose; merchants buy them so they can detect Facebook-using passersby and send them advertising. Dave Lee, "Facebook offers business free Place Tips beacon devices," BBC News, June 9, 2015, http://www.bbc.com/news/technology-33069594.
  6.   Noah Schachtman, "Google, CIA Invest in 'Future' of Web Monitoring," Wired, July 28, 2010, http://www.wired.com/2010/07/exclusive-google-cia/; Shawn M. Powers and Michael Jablonski, The Real Cyber War: The Political Economy of Internet Freedom, (Champaign, Ill.: University of Illinois Press, 2015), pp. 63-9; "In-Q-Tel Announces Strategic Investment in Keyhole: NIMA Uses Technology to Support the Conflict in Iraq," June 25, 2003 (NIMA became NGA later in 2003), https://www.iqt.org/in-q-tel-announces-strategic-investment-in-keyhole/; Sean Wohltman and Phil Dixon, "Google Earth Builder supports NGA geospatial efforts," Google for Work Blog, http://googleforwork.blogspot.com/2011/04/google-earth-builder-supports-nga.html; Mike Wall, "US Air Force Launches Advanced GPS Satellite into Orbit," space.com, March 25, 2015, http://www.space.com/28926-air-force-launches-gps-satellite.html.
  7.   This chapter has been informed by numerous not-for-attribution discussions with current and former senior officers at US intelligence agencies and Internet-based companies. I want to thank them for their trust.
  8.   Steven Levy, "Battle of the Clipper Chip," New York Times Magazine, June 12, 1994, http://www.nytimes.com/1994/06/12/magazine/battle-of-the-clipper-chip.html.
  9.   On cypherpunks, see Andy Greenberg, This Machine Kills Secrets: How Wikileakers, Cypherpunks, and Hacktivists Aim to Free the World's Information (New York: Dutton, 2012); Julian Assange, Cypherpunks: Freedom and the Future of the Internet (New York: OR Books, 2012).
  10.   Bianca Bosker, "Siri Rising: The Inside Story of Siri's Origins—And Why She Could Overshadow the iPhone," HuffingtonPost, Jan. 22, 2013, http://www.huffingtonpost.com/2013/01/22/siri-do-engine-apple-iphone_n_2499165.html.
  11.   Shane Harris, @War: The Rise of the Military-Internet Complex (New York: Houghton Mifflin Harcourt, 2014), pp. 87-93; "Secret Documents Reveal N.S.A. Campaign Against Encryption," New York Times, Sept. 5, 2013, http://www.nytimes.com/interactive/2013/09/05/us/documents-reveal-nsa-campaign-against-encryption.html?_r=0.
  12.   Government Accountability Office, "Critical Infrastructure Protection: Key Private and Public Cyber Expectations Need to Be Consistently Addressed," Aug. 16, 2010, http://www.gao.gov/assets/310/307225.html; Joshua Bolten, "Memorandum on Development of Homeland Security Presidential Directive (HSPD) - 7," June 17, 2004, https://www.whitehouse.gov/sites/default/files/omb/memoranda/fy04/m-04-15.pdf.
  13.   Harris, @War, pp. xii-xvii.
  14.   Harris, @War, pp. 77-82.
  15.   Harris, @War, 48.
  16.   Jim Bach, "Should I Buy IBM Stock Now?" Money Morning, June 22, 2015, http://moneymorning.com/2015/06/22/should-i-buy-ibm-stock-now/; Frank Konkel, "How the CIA Partnered with Amazon and Changed Intelligence," DefenseOne, July 11, 2014, http://www.defenseone.com/technology/2014/07/how-cia-partnered-amazon-and-changed-intelligence/88555/; Frank Konkel, "Amazon Expands Its Cloud Services to the U.S. Military," DefenseOne, Aug. 21, 2014, http://www.defenseone.com/technology/2014/08/amazon-expands-its-cloud-services-us-military/92090/; Patrick Tucker, "How to Break into the CIA's Cloud on Amazon," DefenseOne, July 7, 2015, http://www.defenseone.com/technology/2015/07/how-break-cias-cloud-amazon/117175/.
  17.   Harris, @War, pp. 67, 211; Violet Blue, "NSA Director accused of lying to Congress at Black Hat USA 2013 keynote," ZDNet, July 31, 2013, http://www.zdnet.com/article/nsa-director-accused-of-lying-to-congress-at-black-hat-usa-2013-keynote/; "NSA chief heckled as he defends agency's surveillance programs at hacking conference," Daily Mail (Australia), July 31, 2013, http://www.dailymail.co.uk/news/article-2382312/NSA-chief-heckled-defends-agencys-surveillance-programs-hacking-conference.html; Lee Ferran, "NSA Chief to Hackers: We Don't Abuse Power, 'And That's No Bulls***,' ABC, n.d. http://abcnews.go.com/Blotter/nsa-chief-alexander-hackers-abuse-power-bulls/story?id=19830060.
  18.   Patrick Tucker, "Pentagon Sets Up a Silicon Valley Outpost," DefenseOne, April 23, 2015, http://www.defenseone.com/technology/2015/04/pentagon-sets-silicon-valley-outpost/110845/.
  19.   Devlin Barrett, Danny Yadron and Daisuke Wakabayashi, "Apple and Others Encrypt Phones, Fueling Government Standoff," Wall Street Journal, Nov. 18, 2014, http://www.wsj.com/articles/apple-and-others-encrypt-phones-fueling-government-standoff-1416367801.
  20.   James B. Comey, "Going Dark: Are Technology, Privacy, and Public Safety on a Collision Course?" https://www.fbi.gov/news/speeches/going-dark-are-technology-privacy-and-public-safety-on-a-collision-course; Ben Quinn, James Ball and Dominic Rushe, "GCHQ chief accuses US tech giants of becoming terrorists' 'Networks of Choice,'" Guardian, Nov. 3, 2014, http://www.theguardian.com/uk-news/2014/nov/03/privacy-gchq-spying-robert-hannigan; Mieke Eoyang, "FBI Director Comey Is Wrong: Strong Encryption Makes Us All Safe," DefenseOne, July 7, 2015, http://www.defenseone.com/ideas/2015/07/fbi-director-comey-wrong-strong-encryption-makes-us-all-safer/117190/.
  21.   Chris Strohm, "Three of Tech's Top CEOS to Skip Obama Cybersecurity Summit," BloombergBusiness, Feb. 11, 2015, http://www.bloomberg.com/news/articles/2015-02-11/three-of-tech-s-biggest-ceos-to-skip-obama-cybersecurity-summit.
  22.   Tim Cook speech, http://www.c-span.org/video/?324360-4/tim-cook-white-house-cybersecurity-summit.
  23.   Letter to President Obama, May 19, 2015, https://static.newamerica.org/attachments/3138--113/Encryption_Letter_to_Obama_final_051915.pdf; Ellen Nakashima, "Tech giants don't want Obama to give police access to encrypted phone data," Washington Post, May 19, 2015, http://www.washingtonpost.com/world/national-security/tech-giants-urge-obama-to-resist-backdoors-into-encrypted-communications/2015/05/18/11781b4a-fd69-11e4-833c-a2de05b6b2a4_story.html.
  24.   Matthew Panzarino, "Apple's Tim Cook Delivers Blistering Speech on Encryption, Privacy," TechCrunch June 2, 2015, http://techcrunch.com/2015/06/02/apples-tim-cook-delivers-blistering-speech-on-encryption-privacy/#.ilcb4c:kVGu.
  25.   The Wall Street Journal has described the growing extent of national cyber capabilities in Damian Paletta, Danny Yadron and Jennifer Valentino-Devries, "Cyberwar Ignites a New Arms Race," Wall Street Journal, Oct. 11, 2015, http://www.wsj.com/articles/cyberwar-ignites-a-new-arms-race-1444611128.
  26.   Drew DeSilver, "Most young Americans say Snowden has served the public interest," PewResearchCenter FactTank, Jan. 22, 2014, http://www.pewresearch.org/fact-tank/2014/01/22/most-young-americans-say-snowden-has-served-the-public-interest/; "Public Perceptions of Privacy and Security in the Post-Snowden Era," Nov. 12, 2014, http://www.pewinternet.org/files/2014/11/PI_PublicPerceptionsofPrivacy_111214.pdf.
  27.   Jim Blasingame, "Why transferring Internet control is a dangerous idea," Small Business Advocate (audio interview with Daniels), http://www.smallbusinessadvocate.com/small-business-interviews/mike-daniels-17830/.
  28.   Vernor Vinge, True Names and the Opening of the Cyberspace Frontier (New York: Tor, 2001). This edition includes, along with the novella, an introduction by Vinge discussing his inspirations and the novella's reception, and essays by, among others, Marvin Minsky, Leonard Foner, and Tim May.
  29.   http://www.share.org/about.
  30.   David Hayward, "The History of Linux: how time has shaped the penguin," techradar, Nov. 22, 2012, http://www.techradar.com/us/news/software/operating-systems/the-history-of-linux-how-time-has-shaped-the-penguin-1113914; "Firefox browser takes on Microsoft," BBC Nov. 9. 2004; http://news.bbc.co.uk/2/hi/technology/3993959.stm; https://wordpress.org/about/features/.
  31.   "Who We Be," http://w3.cultdeadcow.com/cms/about.html.
  32.   For the flavor: Kim Zetter, "Imploding Barrels and Other Highlights from Hackfest Defcon," Wired, July 10, 2015; Netta "grayarea" Gilboa, HoHoCon '94 review, Phrack Magazine, vol. 6, issue 47, Jan. 25, 1995, http://phrack.org/issues/47/10.html.
  33.   Greenberg, This Machine, pp. 201-2 (Assange and Mudge), 206-10; Craig Timberg, "A Disaster Foretold—and Ignored," Washington Post, June 22, 2015, http://www.washingtonpost.com/sf/business/2015/06/22/net-of-insecurity-part-3/. Mudge eventually left Google and returned to the White House: Cale Guthrie Weissman, "The White House is making a huge step toward protecting the government from hackers," Business Insider, June 29, 2015, http://www.businessinsider.com/peiter-zatko-leaves-google-to-launch-cyberul-program-for-white-house-2015-6.
  34.   "LoU Strike Out with International Coalition of Hackers" (press release), Jan. 7, 1999, http://www.cultdeadcow.com/news/statement19990107.html.
  35.   See Lanier, Who Owns, pp. 199-200.
  36.   Jared Cohen and Eric Schmidt, The New Digital Age: Reshaping the Future of People, Nations and Business (New York: Knopf, 2013), pp. 61-2, 121-132.
  37.   Laura DeNardis, The Global War for Internet Governance (New Haven, Conn.: Yale University Press, 2014), p. 243.
  38.   Gordon M. Goldstein, "The End of the Internet?: How regional networks may replace the World Wide Web," Atlantic Monthly, July 2014, http://www.theatlantic.com/magazine/archive/2014/07/the-end-of-the-internet/372301/; "NTIA Announces Intent to Transition Key Internet Domain Name Functions" (press release), US Department of Commerce, National Telecommunications and Information Administration, March 14, 2014, http://www.ntia.doc.gov/press-release/2014/ntia-announces-intent-transition-key-internet-domain-name-functions; "Improvement of Technical Management of Internet Names and Addresses" (green paper), Department of Commerce, National Telecommunications and Information Administration, Federal Register v. 63, no. 34, Feb. 20, 1998, p. 8832.
  39.   This is very much an ongoing story. There are several excellent blogs that provide regular informed updates on Internet governance: Net Politics (http://blogs.cfr.org/cyber/), CircleID (http://www.circleid.com/), Internet Monitor (https://thenetmonitor.org/blog/; it is also good for following global censorship trends, both legislative and technical ones) and the Internet Governance Project (http://www.internetgovernance.org/).
On IANA, ICANN, NTIA and the transfer of Internet authority: IANA Stewardship Transition Coordination Group, "Proposal to Transition the Stewardship of the Internet Assigned Numbers Authority (IANA) Functions from the U.S. Commerce Department's National Telecommunications and Information Administration (NTIA) to the Global Multistakeholder Community," July 2015, https://www.ianacg.org/icg-files/documents/IANA-stewardship-transition-proposal-EN.pdf; Kieran McCarthy, "US government tweaks internet handover date," The Register, Aug. 17, 2015, http://www.theregister.co.uk/2015/08/17/internet_handover_date_november_2016/; "Verisign/ICANN Proposal in Response to NTIA Request," n.d. http://www.ntia.doc.gov/files/ntia/publications/root_zone_administrator_proposal-relatedtoiana_functionsste-final.pdf; Mark Rockwell, "ICANN handoff officially delayed," FCW, Aug. 18, 2015, http://fcw.com/articles/2015/08/18/icann-delayed.aspx
On the multilateral and UN aspects, see Alex Grigsby, "The 2015 GGE Report: Breaking New Ground, Ever So Slowly," Council on Foreign Relations "Net Politics" blog, Sept. 8, 2015; "Understanding the WSIS+10 Review Process," Internet Society Briefing Paper, May 22, 2015, http://blogs.cfr.org/cyber/2015/09/08/the-2015-gge-report-breaking-new-ground-ever-so-slowly/; http://www.internetsociety.org/doc/understanding-wsis10-review-process
For deeper discussion, see DeNardis, The Global War; Milton L. Mueller, Networks and States: The Global Politics of Internet Governance (Cambridge, Mass.: MIT Press, 2010); Jonathan E. Neuchterlein and Philip J. Weiser, Digital Crossroads: Telecommunications Law and Policy in the Internet Age, second edition (Cambridge, Mass.: MIT Press, 2013).










CONCLUSION
What will our divided-but-integrated splinternet look like? The Internet governance muddle, now decades long and acquiring the status of tradition, is one guarantee against any permanent division of the Internet into discrete, national or regional Internets, as is the similarly amorphous and durable world of open code. The fundamental principle of both is interoperability, and the unique trump card of interoperability is scale. Proprietary code and proprietary, isolationist internets can only achieve temporary victories. Microsoft's MS-DOS is one example, Iran's attempt to seal off its Internet is another. For users and consumers, the attractions of interoperability overwhelm isolation; and the same becomes true, most of the time, for companies and governments, who find it hard to maintain their closed commercial or political monopolies if they want to continue to grow and assert themselves. This creates both competitive innovation and a dynamic Internet balance of power: As long as some major players continue to struggle against capture by other major players, the global infrastructure of the Internet will be preserved, mainly because the desire to compete beyond borders will continue to animate all of those major players, private and public. The unending will to power provides its own anxious guarantee of freedom.
Nonetheless, the Internet as an entirely cross-border enterprise—whether the empire of mind envisioned by John Perry Barlow or the liberal globalizing juggernaut of Thomas Friedman—will not return, just as the American hyperpower dominance that created it will not return. It is very hard to see how that geopolitical moment could recur.
The idea of a global web public wholly independent of state sovereignty was to a great extent an illusion of the early web industry, a subcultural hubris that posited the geek public as a vanguard of everyone on the planet: today Palo Alto, tomorrow the world. Once the commercial logic of market segmentation, principally through the economics of marketing, entered the picture, the universal public gave way to the reality of consumer preferences, which have their own, distinctly non-universal logic.
This only partly suited the desire of governments for control of their respective citizenries. On one hand, states could dictate terms for commercial access to their markets, terms that included forms of surveillance—in democracies as well as non-democracies. On the other hand, most states with ambitions for the economic power of their "own" corporations could not achieve those ambitions in autarkic isolation. If American tech companies had been limited to the US market they would never have achieved the prosperity they now enjoy. But the same is and will be true for Chinese companies, to take a somewhat extreme example. Alibaba is not just bigger than Amazon. It is spending a billion dollars to set up cloud computing centers across the world. It doesn't want to be just a Chinese company.1
Of course, Amazon is also becoming the secure mega-server for the US intelligence community, and as far as one knows Alibaba has not yet put up much opposition to the requirements of China's security services.2 That is the crux of the issue. That is why the encryption battle has led giants like Google and Facebook into open opposition. Will commercially controlled data, which is the vast majority of web data, also be state-controlled data, or not?
There are two major trends at work now. One is that enough money or power can buy security. The other is that the logic of interstate war is now being applied to the web as it was to predecessor technologies.
Government and private actors that can afford it and see the need will build their own networks, parallel to the web but securely protected from it, like the CIA's Amazon cloud or the Defense Department's CRASH-SAFE project. In March 2015, Goldman Sachs led a consortium of Wall Street companies to build a global secure network (called Symphony) and is participating in a project to build a safe European platform for non-public equity transactions. Almost since the web began there has been a "dark web" that includes such financial "dark pools"; some 40 percent of US equity trading volume takes place among institutional investors on such systems, and the portion of the web actually sampled ("crawled") by open, public search engines is about 20 percent. Looked at this way, the public Internet, once meant to be the entire Internet (Information should be free), will be just one Internet among many. But because other Internets will be made safe, this global public Internet should be able to survive.3
The greatest threat to it is a promiscuous use of the logic of war. It's tempting to see a long-term pattern at work: technology born in war returns to war. The motivations of states to make this so are growing strong, because their acute dependence on private technology companies is feeding their insecurity, both in terms of protecting their secrets and protecting their ability to project power. This amounts to loss of secure command-and-control, which no military can easily abide. For major powers like the United States, the anxiety is most intense with regard to satellites. A secured satellite system was central to making American military power so technologically superior that it could overwhelm any enemy; it was at the heart of what was called the "second offset," the first offset being nuclear weapons and robust delivery systems and the second being, in the words of Deputy Secretary of Defense Robert Work, the use of "advanced digital microelectronics and the explosion in information technology" to enable "smart weapons, sensors, targeting and control networks."4 China, Russia, and to a lesser extent India and some European powers are striving to eliminate this American advantage, not least through developing their own satellite-based global positioning systems to replace the US military-based GPS. Russia is developing GLONASS, Europe Galileo, India GAGAN, and China BeiDou. This will presumably become another field for regional competition, what used to be called spheres of interest, as major powers try to bring smaller neighbors into their regional sat-nav systems, which are inevitably also surveillance systems. The programs mentioned above all have interoperability agreements but nothing in great-power competition lasts forever: US-Russian communications on GPS were suspended in 2014. The United States is, of course, also thinking ahead, developing a "third offset" probably based on a global surveillance and strike network that would leverage US advantages in long-range unmanned aerial and sea vehicles. As Deputy Secretary Work said in the summer of 2015 to an air force audience—perfectly echoing J.C.R. Licklider's air force-sponsored essay from 1960—"the future of combat, we believe, is going to be characterized by a very high degree of human-machine symbiosis."5
The key question for the Internet is whether it will become caught up in this great-power military logic. It's easy to see how it could. What are called "offensive cyberwarfare capabilities"—essentially, server-to-server weapons that aim to degrade or destroy the enemy's command-and-control—are being developed.6 But there are reasons to hope that a militarized Internet is not inevitable.
The first, as mentioned, is the commercial dominance of the Internet, which gives it, by virtue of the desire of web companies and national economies to prosper, a relative autonomy from politics. The second is the growing technical ability of major players to secure their own systems, thus reducing their fears about the Internet as a whole. The third is a dawning realization among governments that they need to reach some minimal Internet modus vivendi. Codes of conduct and general principles have been well articulated (the OECD code of 2014, or the five simple rules proposed by US Secretary of State John Kerry in 2015).7 It's not unreasonable to hope that states will continue to elaborate such minimal rules of the road.
The fourth is that states are realizing that the Internet is not inevitably a mechanism for regime change—that it is a relative danger but not an absolute one. The United States has so far not crushed the opposition of its obstreperous multinationals. China has very reluctantly but materially accepted that its Netizens will upload photos of major disasters and demand official accountability.8
None of this points to any final victory of Internet freedom over the power of the state. But it does suggest that a fragmented Internet will retain aspects of universality despite its roots in great-power struggle. The Internet has changed politics permanently and for the better. Thirty years ago, an average Western citizen would have learned about China through a subscription to the China Daily, sent by official publishers via airmail and printed on extra-thin paper. Chinese or Soviet citizens have had even less information than that about the distant Westerners they were taught to fear. The networks of knowledge made possible by the Internet are of a scale and depth unimaginable to any earlier generation. It's not a utopia, but it is still a precious victory over the weary giants of flesh and steel, and profoundly worth defending.
 


  1.   Gillian Wong, "Ali Baba Plans $1 Billion Cloud-Computing Push," Wall Street Journal, July 29, 2015, http://www.wsj.com/articles/alibaba-plans-1-billion-cloud-computing-push-1438160558.
  2.   Paul Mozur, "Ali Baba Joins Forces with Chinese Arms Maker," New York Times, Aug. 19, 2015, http://www.nytimes.com/2015/08/20/business/international/alibaba-and-weapons-maker-in-satellite-navigation-project.html.
  3.   http://www.crash-safe.org/papers.html; Ron Miller, "Wall Street-Backed Symphony Wants to Revolutionize Financial Services Communication," TechCrunch, Feb. 21, 2015, http://techcrunch.com/2015/02/21/wall-street-backed-symphony-wants-to-revolutionize-financial-services-communication/; "Symphony CEO: This is only the start for us," CNBC, Sept. 15, 2015; John McCrank, "Dark markets may be more harmful than high-frequency trading," Reuters, Apr. 7, 2014, http://www.reuters.com/article/2014/04/07/us-markets-darkpools-analysis-idUSBREA3605M20140407; Christian Asare and Keisha Potter, "Regulators, Don't Drain Dark Pools," American Banker, Sept. 3, 2015, http://www.americanbanker.com/bankthink/regulators-dont-drain-dark-pools-1076512-1.html; Yuka Hayashi, "Asset Managers Team Up to Develop Standards for Dark Pools, Wall Street Journal, Sept. 15, 2015, http://blogs.wsj.com/moneybeat/2015/09/15/asset-managers-team-up-to-develop-standards-for-dark-pools/
The actual security of these systems can be debated. See "Future Scope: Ralph Langner," GE Look ahead, Sept. 25, 2015, http://gelookahead.economist.com/future-scope/ralph-langner/
The percentage of the web that is actually searched by conventional means is hard to measure and might well be less than 20 percent.
DARPA has a project to change how search works. It's called Memex and is based on Vannevar Bush's venerable essay. Meghan Neal, "DARPA's Building a New Search Engine to Crawl the Deep Web," Vice: Motherboard, Feb. 10, 2014, http://motherboard.vice.com/blog/darpas-building-a-new-search-engine-to-crawl-the-deep-web.
  4.   Robert Work, Speech to the China Aerospace Studies Institute, June 22, 2015, http://www.defense.gov/News/Speeches/Article/606683.
  5.   Stephen Clark, "Launch Brings Galileo Navigation System into Double Digits," Spaceflight Now, Sept. 11, 2015, http://spaceflightnow.com/2015/09/11/launch-brings-galileo-navigation-system-into-double-digits/; "Beidou to Provide GPS service for Arab countries," gbtimes, Sept. 16, 2015, http://gbtimes.com/china/beidou-provide-gps-service-arab-countries
An example of a Chinese sphere-of-interest approach is in the speech by Lu Wei, head of the Cyberspace Administration of China, at the opening ceremony of the China-ASEAN Information Harbor Forum, Sept. 13, 2015, http://news.xinhuanet.com/english/china/2015-09/13/c_134619792.htm
Robert Martinage, Toward a New Offset Strategy: Exploiting U.S. Long-Term Advantages to Restore U.S. Global Power Projection Capability (Washington, DC: Center for Strategic and Budgetary Assessments, 2014), http://csbaonline.org/publications/2014/10/toward-a-new-offset-strategy-exploiting-u-s-long-term-advantages-to-restore-u-s-global-power-projection-capability/.
  6.   Harris, @War, pp. xxiii, 100-109; Brandon Valeriano and Ryan C. Maness argue that the danger of actual cyberwar is highly exaggerated: Cyber War versus Cyber Realities: Cyber Conflict in the International System (Oxford: Oxford University Press, 2015).
  7.   "OECD Principles for Internet Policy Making," (OECD, 2014) http://www.oecd.org/internet/ieconomy/oecd-principles-for-internet-policy-making.pdf; John Kerry, "An Open and Secure Internet: We Must Have Both," speech at Korea University, http://www.state.gov/secretary/remarks/2015/05/242553.htm; Matt Spetalnick and Michael Martina, "Obama announces 'understanding' with China's Xi on cyber theft but remains wary," Reuters, Sept. 26, 2015, http://www.reuters.com/article/2015/09/26/us-usa-china-idUSKCN0RO2HQ20150926.
  8.   For now, at least, the major exception to this pattern of grudging accommodation may be Russia: Cyrus Farivar, "The Red Web: In Putin's Russia, the Internet Watches You," arstechnica, Sept. 7, 2015, http://arstechnica.com/tech-policy/2015/09/the-red-web-in-putins-russia-internet-watches-you/
The Obama administration, despite the dire warnings of FBI director Comey and others, decided in the fall of 2015 that insisting on "back doors" was counterproductive. See NSC, "Review of Strategic Approaches" summer 2015 (draft), http://apps.washingtonpost.com/g/documents/national/read-the-nsc-draft-options-paper-on-strategic-approaches-to-encryption/1742/ and Ellen Nakashima and Andrea Peterson, "Obama faces growing momentum to support widespread encryption," Washington Post, Sept 16, 2015, https://www.washingtonpost.com/world/national-security/tech-trade-agencies-push-to-disavow-law-requiring-decryption-of-phones/2015/09/16/1fca5f72-5adf-11e5-b38e-06883aacba64_story.html.










ACKNOWLEDGMENTS
I want to thank a number of friends who were good enough to give advice on earlier versions of this manuscript: JC de Swaan, Parag Khanna, Peter Harling, James Macdonald, Jamie Metzl, Mike Moran, Mike Riordan, and Dee Smith. Thanks also to Adam Segal and Jim Dougherty at the Council on Foreign Relations for their invaluable seminars, and to the New York Public Library's Allen Room. Finally, much gratitude to Colin Robinson for his enthusiasm and grace.









ABOUT THE AUTHOR

Scott Malcomson, a consultant on communications and political risk, is the author of four previous books and was foreign editor of the New York Times Magazine from 2004 to 2011. He served at the United Nations and the U.S. State Department and was director of communications for two global NGOs. He has written for the New York Times, The New Yorker and a range of other publications.









OR Books
PUBLISHING THE POLITICS OF THE INTERNET
Lean Out: The Struggle for Gender Equality in Tech and Start-Up Culture
ELISSA SHEVINSKY, EDITOR
When Google Met WikiLeaks
JULIAN ASSANGE
The Big Disconnect: Why the Internet Hasn't Transformed Politics (Yet)
MICAH L. SIFRY
Cypherpunks: Freedom and the Future of the Internet
JULIAN ASSANGE WITH JACOB APPELBAUM, ANDY MüLLER-MAGUHN, AND JéRéMIE ZIMMERMANN
Technocreep: The Surrender of Privacy and the Capitalization of Intimacy
THOMAS P. KEENAN
Hacking Politics: How Geeks, Progressives, the Tea Party, Gamers, Anarchists and Suits Teamed Up to Defeat SOPA and Save the Internet
DAVID MOON, PATRICK RUFFINI, AND DAVID SEGAL, EDITORS

For more information, visit our
website at www.orbooks.com


























