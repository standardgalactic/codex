














Contents
Cover
Series Page
Title Page
Copyright
Dedication
Foreword
Preface
Acknowledgments
Part One: Managing Risk
Chapter 1: Risk Management versus Risk Measurement
1.1 Contrasting Risk Management and Risk Measurement
1.2 Redefinition and Refocus for Risk Management
1.3 Quantitative Measurement and a Consistent Framework
1.4 Systemic versus Idiosyncratic Risk
Chapter 2: Risk, Uncertainty, Probability, and Luck
2.1 What Is Risk?
2.2 Risk Measures
2.3 Randomness and the Illusion of Certainty
2.4 Probability and Statistics
2.5 The Curse of Overconfidence
2.6 Luck
Chapter 3: Managing Risk
3.1 Manage People
3.2 Manage Infrastructure—Process, Technology, Data
3.3 Understand the Business
3.4 Organizational Structure
3.5 Brief Overview of Regulatory Issues
3.6 Managing the Unanticipated
3.7 Conclusion
Chapter 4: Financial Risk Events
4.1 Systemic versus Idiosyncratic Risk
4.2 Idiosyncratic Financial Events
4.3 Systemic Financial Events
4.4 Conclusion
Chapter 5: Practical Risk Techniques
5.1 Value of Simple, Approximate Answers
5.2 Volatility and Value at Risk (VaR)
5.3 Extreme Events
5.4 Calculating Volatility and VaR
5.5 Summary for Volatility and VaR
5.6 Portfolio Tools
5.7 Conclusion
Chapter 6: Uses and Limitations of Quantitative Techniques
6.1 Risk Measurement Limitations
Part Two: Measuring Risk
Chapter 7: Introduction to Quantitative Risk Measurement
7.1 Project Implementation
7.2 Typology of Financial Institution Risks
7.3 Conclusion
Chapter 8: Risk and Summary Measures: Volatility and VaR
8.1 Risk and Summary Measures
8.2 Comments Regarding Quantitative Risk Measures
8.3 Methods for Estimating the P&L Distribution
8.4 Techniques and Tools for Tail Events
8.5 Estimating Risk Factor Distributions
8.6 Uncertainty and Randomness—the Illusion of Certainty
8.7 Conclusion
Appendix 8.1: Small-Sample Distribution of VaR and Standard Errors
Appendix 8.2: Second Derivatives and the Parametric Approach
Chapter 9: Using Volatility and VaR
9.1 Simple Portfolio
9.2 Calculating P&L Distribution
9.3 Summary Measures to Standardize and Aggregate
9.4 Tail Risk or Extreme Events
9.5 Conclusion
9.6 Appendix 9.1: Parametric Estimation Using Second Derivatives
Chapter 10: Portfolio Risk Analytics and Reporting
10.1 Volatility, Triangle Addition, and Risk Reduction
10.2 Contribution to Risk
10.3 Best Hedge
10.4 Replicating Portfolio
10.5 Principal Components and Risk Aggregation
10.6 Risk Reporting
10.7 Conclusion
Appendix 10.1: Various Formulae for Marginal Contribution and Volatilities
Appendix B: Stepwise Procedure for Replicating Portfolio
Appendix C: Principal Components Overview
Chapter 11: Credit Risk
11.1 Introduction
11.2 Credit Risk versus Market Risk
11.3 Stylized Credit Risk Model
11.4 Taxonomy of Credit Risk Models
11.5 Static Structural Models
11.6 Static Reduced Form Models—CreditRisk+
11.7 Static Models—Threshold and Mixture Frameworks
11.8 Actuarial versus Equivalent Martingale (Risk-Neutral) Pricing
11.9 Dynamic Reduced Form Models
11.10 Conclusion
Appendix 11.1: Probability Distributions
Chapter 12: Liquidity and Operational Risk
12.1 Liquidity Risk—Asset versus Funding Liquidity
12.2 Asset Liquidity Risk
12.3 Funding Liquidity Risk
12.4 Operational Risk
12.5 Conclusion
Chapter 13: Conclusion
About the Companion Web Site
References
About the Author
Index









Founded in 1807, John Wiley & Sons is the oldest independent publishing company in the United States. With offices in North America, Europe, Australia, and Asia, Wiley is globally committed to developing and marketing print and electronic products and services for our customers' professional and personal knowledge and understanding.
The Wiley Finance series contains books written specifically for finance and investment professionals as well as sophisticated individual investors and their financial advisors. Book topics range from portfolio management to e-commerce, risk management, financial engineering, valuation, and financial instrument analysis, as well as much more.
For a list of available titles, please visit our website at www.WileyFinance.com.



















Copyright © 2012 by Thomas S. Coleman. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
Chapters 1, 2, 3, 4, and parts of 6 were originally published as A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.
Chapters 5, 7, 8, 9, 10, and 11 include figures, tables, and short excerpts that have been modified or reprinted from A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 646-8600, or on the Web at www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing this book, they make no representations or warranties with respect to the accuracy or completeness of the contents of this book and specifically disclaim any implied warranties of merchantability or fitness for a particular purpose. No warranty may be created or extended by sales representatives or written sales materials. The advice and strategies contained herein may not be suitable for your situation. You should consult with a professional where appropriate. Neither the publisher nor author shall be liable for any loss of profit or any other commercial damages, including but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our Customer Care Department within the United States at (800) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. For more information about Wiley products, visit our web site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Coleman, Thomas Sedgwick, 1955-
Quantitative risk management: a practical guide to financial risk/Thomas S. Coleman.
pages cm.—(Wiley finance series; 669)
Includes bibliographical references and index.
ISBN 978-1-118-02658-8 (cloth); ISBN 978-1-118-26077-7 (ebk);
ISBN 978-1-118-22210-2 (ebk); ISBN 978-1-118-23593-5 (ebk)
1.Financial services industry—Risk management.2.Financial risk management. 3. Capital market. I. Title.
HG173.C664 2012
332.1068′1—dc23
2011048533









To Lu and Jim, for making me who I am today.









Foreword
Having been the head of the risk management department at Goldman Sachs for four years (which I sadly feel obligated to note was many years ago during a period when the firm was a highly respected private partnership), and having collaborated on a book called The Practice of Risk Management, I suppose it is not a surprise that I have a point of view about the topic of this book.
Thomas Coleman also brings a point of view to the topic of risk management, and it turns out for better or for worse, we agree. A central theme of this book is that "in reality risk management is as much the art of managing people, processes, and institutions as it is the science of measuring and quantifying risk." I think he is absolutely correct.
This book's title also highlights an important distinction that is sometimes missed in large organizations. Risk measurement, per se, which is a task usually assigned to the "risk management" department, is in reality only one input to the risk management function. As Coleman elaborates, "Risk measurement tools...help one to understand current and past exposures, a valuable and necessary undertaking but clearly not sufficient for actually managing risk." However, "The art of risk management" which he notes is squarely the responsibility of senior management, "is not just in responding to anticipated events, but in building a culture and organization that can respond to risk and withstand unanticipated events. In other words, risk management is about building flexible and robust processes and organizations."
The recognition that risk management is fundamentally about communicating risk up and managing risk from the top leads to the next level of insight. In most financial firms different risks are managed by desks requiring very different metrics. Nonetheless, there must be a comprehensive and transparent aggregation of risks and an ability to disaggregate and drill down. And as Coleman points out, consistency and transparency in this process are key requirements. It is absolutely essential that all risk takers and risk managers speak the same language in describing and understanding their risks.
Finally, Coleman emphasizes throughout that the management of risk is not a function designed to minimize risk. Although risk is usually a reference to the downside of random outcomes, as Coleman puts it, risk management is about taking advantage of opportunities: "controlling the downside and exploiting the upside."
In discussing the measurement of risk the key concept is, of course, the distribution of outcomes. But Coleman rightly emphasizes that this distribution is unknown, and cannot be summarized by a single number, such as a measure of dispersion. Behavioral finance has provided many illustrations of the fact that, as Coleman notes, "human intuition is not very good at working with randomness and probabilities." In order to be successful at managing risk, he suggests, "We must give up any illusion that there is certainty in this world and embrace the future as fluid, changeable, and contingent."
One of my favorite aspects of the book is its clever instruction on working with and developing intuition about probabilities. Consider, for example, a classic problem, that of interpreting medical test results. Coleman considers the case of testing for breast cancer, a disease that afflicts about one woman in twenty. The standard mammogram tests actually report false positives about five percent of the time. In other words, a woman without cancer will get a negative result 95 percent of the time and a positive result 5 percent of the time. Conditional on receiving a positive test result, a natural reaction is to assume the probability of having cancer is very high, close to 95 percent. In fact, that is not true. Consider that out of 1,000 women approximately 5 will have cancer. Approximately 55 will receive positive results. Thus, conditional on receiving a positive test result the probability of having cancer is only about 9 percent, not 95 percent. Using this example as an introduction, the author then develops the ideas of Bayesian updating of probabilities.
Although this book appropriately spends considerable effort describing quantitative risk measurement techniques, that task is not its true focus. It takes seriously its mission as a practical guide. For example, in turning to the problem of managing risk, Coleman insightfully chooses as his first topic managing people, and the first issue addressed is the principal-agent problem. According to Coleman, "Designing compensation and incentive schemes has to be one of the most difficult and underappreciated, but also one of the most important, aspects of risk management." Although he does not come to a definitive conclusion about how to structure employment contracts, he concludes, "careful thinking about preferences, incentives, compensation, and principal-agent problems enlightens many of the most difficult issues in risk management—issues that I think we as a profession have only begun to address in a substantive manner."
There are many well-known limitations to any attempt to quantify risk, and this book provides a useful cautionary list. Among the many concerns, Coleman highlights that "models for measuring risk will not include all positions and all risks"; "risk measures such as VaR and volatility are backward looking"; "VaR does not measure the 'worst case'"; "quantitative techniques are complex and require expertise and experience to use properly"; and finally, "quantitative risk measures do not properly represent extreme events." And perhaps most significantly, while he discusses many of the events of the recent financial crisis, Coleman makes the useful distinction between idiosyncratic risk, which can be managed by a firm, versus systemic risk which arises from an economy-wide event outside the control of the firm. This book is focused on the former. Nonetheless, with respect to the latter he concludes that "Systemic risk events...are far more damaging because they involve substantial dislocations across a range of assets and across a variety of markets. Furthermore, the steps a firm can take to forestall idiosyncratic risk events are often ineffective against systemic events."
Coleman brings to bear some of the recent insights from behavioral finance, and in particular focuses on the problem of overconfidence, which is, in his words, "the most fundamental and difficult (issue) in all of risk management, because confidence is necessary for success, but overconfidence can lead to disaster." Later he elaborates, "Risk management...is also about managing ourselves. Managing our ego, managing our arrogance, our stubbornness, our mistakes. It is not about fancy quantitative techniques but about making good decisions in the face of uncertainty, scanty information, and competing demands." In this context he highlights four characteristics of situations that can lead to risk management mistakes, familiarity, commitment, the herding instinct, and belief inertia.
When focusing on the understanding and communication of risk, Coleman delves deeply into a set of portfolio analysis tools which I helped to develop and utilize while managing risk at Goldman Sachs. These tools, such as the marginal contribution to risk, risk triangles, best hedges, and the best replicating portfolio, were all designed to satisfy the practical need to simplify and highlight the most important aspects of inherently complex combinations of exposures. As we used to repeat often, risk management is about communicating the right information to the right people at the right time.
After covering the theory, the tools, and the practical application, Coleman finally faces the unsatisfying reality that the future is never like the past, and this is particularly true with respect to extreme events. His solution is to recognize this limitation. "Overconfidence in numbers and quantitative techniques, in our ability to represent extreme events, should be subject to severe criticism, because it lulls us into a false sense of security." In the end the firm relies not so much on the risk measurement tools as the good judgment and wisdom of the experienced risk manager. As Coleman correctly concludes, "A poor manager with good risk reports is still a poor manager. The real risk to an organization is in the unanticipated or unexpected—exactly what the quantitative measures capture least well and what a good manager must strive to manage."
Bob Litterman
Partner, Kepos Capital









Preface
Risk management is the art of using lessons from the past to mitigate misfortune and exploit future opportunities—in other words, the art of avoiding the stupid mistakes of yesterday while recognizing that nature can always create new ways for things to go wrong.
This book grew out of a project for the Research Foundation of the CFA Institute. The Research Foundation asked me to write a monograph, a short and practical guide to risk management. I took the commission as a license to write about how I think about risk. Ultimately the project grew far beyond the original mandate and into this book, a book that is, I hope, still a practical guide to financial risk management.
In this book I lay out my view of risk management, a view that has developed over many years as a researcher, trader, and manager. My approach is a little idiosyncratic because risk management itself suffers from a split personality—one side soft management skills, the other side hard mathematics—and any attempt to treat both in the same book will by its nature be something of an experiment. In writing this book I want to do more than just write down the mathematical formulae; I want to explain how we should think about risk, what risk means, why we use a particular risk measure. Most importantly, I want to challenge the accepted wisdom that risk management is or ever should be a separate discipline; managing risk is central to managing a financial firm and must remain the responsibility of anyone who contributes to the profit of the firm.
I entered the financial industry as a trader on a swaps desk. On the desk we lived by the daily and monthly profit and loss. There was nothing more important for managing that P&L than understanding and managing the risk. Risk was around us every day and we needed to build and use practical tools that could help us understand, display, report, and manage risk in all its complexity and variety.
The experience on a trading desk taught me that managing risk is the central part of a financial business. Managing risk is not something to be delegated, not something to be handed over to a risk management department. The measurement of risk can certainly be technical and may require quantitative expertise and a cadre of risk professionals, but the responsibility for management ultimately resides with line managers, senior management, and the board. This lesson is as true for a commercial bank or a portfolio manager as for a trading desk. In any financial business, it is managers who must manage risk, and true risk management can never be devolved to a separate department.
The necessity to manage risk in today's complex markets leads to an inevitable tension between the management side and the quantitative side. Managers traditionally focus on people, process, institutions, incentives—all the components of managing a business. Risk professionals focus on mathematics, models, statistics, data—the quantitative side of the business. Successful performance in today's markets requires that a firm bridge this split personality and integrate both management and quantitative skills.
This book tries to address both sides of the divide. Part One, comprising Chapters 1 through 6, focuses on the management side. I argue that managing risk is as much about managing people, processes, and institutions as it is about numbers, and that a robust and responsive organization is the best tool for responding to a risky environment. But managers also need to be comfortable with quantitative issues: What is risk? How should we think about uncertainty and randomness? What do the quantitative measures such as volatility and VaR mean? These are not just mathematical questions. We need to understand the intuition behind the formulae and use our knowledge to help make decisions.
Part One is not addressed at managers alone. Risk professionals, those focused on building the models and producing the numbers, need to understand how and why the numbers are used in managing risk. As Kendall and Stuart so rightly say, "It's not the figures themselves, it's what you do with them that matters." Part One aims to lay out the common ground where managers and risk professionals meet for the task of measuring and managing risk.
Part Two changes gears to focus on the quantitative tools and techniques for measuring risk. Modern risk measurement is a quantitative field, often the preserve of specialists with mathematical training and expertise. There is no avoiding the statistics, mathematics, and computer technology necessary for risk measurement in today's markets. But we should not shy away from these challenges. The ideas are almost always straightforward, even if the details are difficult. I try to be thorough in covering the theory but also explain the ideas behind the theory. Throughout the book I work with a consistent but simple portfolio to provide examples of key ideas and calculations. Purchasers of the book can access many of these examples online to explore the concepts more fully.
Part Two is aimed primarily at risk professionals, those who need to know the exact formula for calculating, say, the contribution to risk. But managers can also use Part Two to learn more about the concepts behind risk measurement. Chapters 9 and 10 in particular focus on examples and using risk measurement tools. This book should serve as more than simply a reference on how to calculate volatility or learn what a generalized Pareto distribution is. My goal throughout is to find simple explanations for complex concepts—more than anything, I had to explain these concepts to myself.
In the end, this book will be a success if readers come away with both an appreciation of risk management as a management endeavor, and a deeper understanding of the quantitative framework for measuring risk. I hope managers can use this to increase their quantitative skills and knowledge, and that risk professionals can use it to improve their understanding of how the numbers are used in managing the business.
Thomas S. Coleman
Greenwich, CT
March 2012









Acknowledgments
I would like to thank those who helped make this book possible. First and foremost, thanks to Larry Siegel for his valuable insights, suggestions, and diligent editing of the initial Research Foundation manuscript. The Research Foundation of the CFA Institute made this project possible with its generous funding. Many others have contributed throughout the years to my education in managing risk, with special thanks owed to my former colleagues Gian Luca Ambrosio and Michael du Jeu—together we learned many of the world's practical lessons. I thank all those from whom I have learned; the errors, unfortunately, remain my own.
"You haven't told me yet," said Lady Nuttal, "what it is your fiancé does for a living."
"He's a statistician," replied Lamia, with an annoying sense of being on the defensive.
Lady Nuttal was obviously taken aback. It had not occurred to her that statisticians entered into normal social relationships. The species, she would have surmised, was perpetuated in some collateral manner, like mules.
"But Aunt Sara, it's a very interesting profession," said Lamia warmly.
"I don't doubt it," said her aunt, who obviously doubted it very much. "To express anything important in mere figures is so plainly impossible that there must be endless scope for well-paid advice on how to do it. But don't you think that life with a statistician would be rather, shall we say, humdrum?"
Lamia was silent. She felt reluctant to discuss the surprising depth of emotional possibility which she had discovered below Edward's numerical veneer.
"It's not the figures themselves," she said finally, "it's what you do with them that matters."
—K.A.C. Manderville, The Undoing of Lamia Gurdleneck,quoted in Kendall and Stuart (1979, frontispiece).









Part One
Managing Risk









Chapter 1
Risk Management versus Risk Measurement
Managing risk is at the core of managing any financial organization. This statement may seem obvious, even trivial, but remember that the risk management department is usually separate from trading management or line management. Words matter, and using the term risk management for a group that does not actually manage anything leads to the notion that managing risk is somehow different from managing other affairs within the firm. Indeed, a director at a large financial group was quoted in the Financial Times as saying that "A board can't be a risk manager."1 In reality, the board has the same responsibility to understand and monitor the firm's risk as it has to understand and monitor the firm's profit or financial position.
To repeat, managing risk is at the core of managing any financial organization; it is too important a responsibility for a firm's managers to delegate. Managing risk is about making the tactical and strategic decisions to control those risks that should be controlled and to exploit those opportunities that can be exploited. Although managing risk does involve those quantitative tools and activities generally covered in a risk management textbook, in reality, risk management is as much the art of managing people, processes, and institutions as it is the science of measuring and quantifying risk. In fact, one of the central arguments of this book is that risk management is not the same as risk measurement. In the financial industry probably more than any other, risk management must be a central responsibility for line managers from the board and CEO down through individual trading units and portfolio managers. Managers within a financial organization must be, before anything else, risk managers in the true sense of managing the risks that the firm faces.
Extending the focus from the passive measurement and monitoring of risk to the active management of risk also drives one toward tools to help identify the type and direction of risks and tools to help identify hedges and strategies that alter risk. It argues for a tighter connection between risk management (traditionally focused on monitoring risk) and portfolio management (in which one decides how much risk to take in the pursuit of profit).
Risk measurement is necessary to support the management of risk. Risk measurement is the specialized task of quantifying and communicating risk. In the financial industry, risk measurement has, justifiably, grown into a specialized quantitative discipline. In many institutions, those focused on risk measurement will be organized into an independent department with reporting lines separate from line managers.
Risk measurement has three goals:

1. Uncovering known risks faced by the portfolio or the firm. By known risks, I mean risks that can be identified and understood with study and analysis because these or similar risks have been experienced in the past by this particular firm or others. Such risks are often not obvious or immediately apparent, possibly because of the size or diversity of a portfolio, but these risks can be uncovered with diligence.
2. Making the known risks easy to see, understand, and compare—in other words, the effective, simple, and transparent display and reporting of risk. Value at risk, or VaR, is a popular tool in this arena, but there are other, complementary, techniques and tools.
3. Trying to understand and uncover the unknown, or unanticipated risks—those that may not be easy to understand or anticipate, for example, because the organization or industry has not experienced them before.

Risk management, as I just argued, is the responsibility of managers at all levels of an organization. To support the management of risk, risk measurement and reporting should be consistent throughout the firm, from the most disaggregate level (say, the individual trading desk) up to the top management level. Risk measured at the lowest level should aggregate in a consistent manner to firmwide risk. Although this risk aggregation is never easy to accomplish, a senior manager should be able to view firmwide risk, but then, like the layers of an onion or a Russian nesting doll, peel back the layers and look at increasingly detailed and disaggregated risk. A uniform foundation for risk reporting across a firm provides immense benefits that are not available when firmwide and desk-level risks are treated on a different basis.
1.1 Contrasting Risk Management and Risk Measurement
The distinction I draw between risk management and risk measurement argues for a subtle but important change in focus from the standard risk management approach: a focus on understanding and managing risk in addition to the independent measurement of risk. The term risk management, unfortunately, has been appropriated to describe what should be termed risk measurement: the measuring and quantifying of risk. Risk measurement requires specialized expertise and should generally be organized into a department separate from the main risk-taking units within the organization. Managing risk, in contrast, must be treated as a core competence of a financial firm and of those charged with managing the firm. Appropriating the term risk management in this way can mislead one to think that the risk takers' responsibility to manage risk is somehow lessened, diluting their responsibility to make the decisions necessary to effectively manage risk. Managers cannot delegate their responsibilities to manage risk, and there should no more be a separate risk management department than there should be a separate profit management department.
The standard view posits risk management as a separate discipline and an independent department. I argue that risk measurement indeed requires technical skills and often should exist as a separate department. The risk measurement department should support line managers by measuring and assessing risk—in a manner analogous to the accounting department supporting line managers by measuring returns and profit and loss. It still remains line managers' responsibility to manage the risk of the firm. Neither risk measurement experts nor line managers (who have the responsibility for managing risk) should confuse the measurement of risk with the management of risk.
1.2 Redefinition and Refocus for Risk Management
The focus on managing risk argues for a modesty of tools and a boldness of goals. Risk measurement tools can go only so far. They help one to understand current and past exposures, which is a valuable and necessary undertaking but clearly not sufficient for actually managing risk. In contrast, the goal of risk management should be to use the understanding provided by risk measurement to manage future risks. The goal of managing risk with incomplete information is daunting precisely because quantitative risk measurement tools often fail to capture unanticipated events that pose the greatest risk. Making decisions with incomplete information is part of almost any human endeavor. The art of risk management is not just in responding to anticipated events, but in building a culture and organization that can respond to risk and withstand unanticipated events. In other words, risk management is about building flexible and robust processes and organizations with the flexibility to identify and respond to risks that were not important or recognized in the past, the robustness to withstand unforeseen circumstances, and the ability to capitalize on new opportunities.
Possibly the best description of my view of risk management comes from a book not even concerned with financial risk management, the delightful Luck by the philosopher Nicholas Rescher (2001):
The bottom line is that while we cannot control luck [risk] through superstitious interventions, we can indeed influence luck through the less dramatic but infinitely more efficacious principles of prudence. In particular, three resources come to the fore here:

1. Risk management: managing the direction of and the extent of exposure to risk, and adjusting our risk-taking behavior in a sensible way over the overcautious-to-heedless spectrum.
2. Damage control: protecting ourselves against the ravages of bad luck by prudential measures, such as insurance, "hedging one's bets," and the like.
3. Opportunity capitalization:avoiding excessive caution by positioning oneself to take advantage of opportunities so as to enlarge the prospect of converting promising possibilities into actual benefits. (p. 187)

1.3 Quantitative Measurement and a Consistent Framework
The measurement of risk, the language of risk, seemingly even the definition of risk itself—all these can vary dramatically across assets and across the levels of a firm. Traders may talk about DV01 (dollar value of an 01) or adjusted duration for a bond, beta for an equity security, the notional amount of foreign currency for a foreign exchange (FX) position, or the Pandora's box of delta, gamma, theta, and vega for an option. A risk manager assessing the overall risk of a firm might discuss the VaR, or expected shortfall, or lower semivariance.
This plethora of terms is often confusing and seems to suggest substantially different views of risk. (I do not expect that the nonspecialist reader will know what all these terms mean at this point. They will be defined as needed.) Nonetheless, these terms all tackle the same question in one way or another: What is the variability of profits and losses (P&L)? Viewing everything through the lens of P&L variability provides a unifying framework across asset classes and across levels of the firm, from an individual equity trader up through the board.
The underlying foundations can and should be consistent. Measuring and reporting risk in a consistent manner throughout the firm provides substantial benefits. Although reporting needs to be tailored appropriately, it is important that the foundations—the way risk is calculated—be consistent from the granular level up to the aggregate level.
Consistency provides two benefits. First, senior managers can have the confidence that when they manage the firmwide risk, they are actually managing the aggregation of individual units' risks. Senior managers can drill down to the sources of risk when necessary. Second, managers at the individual desk level can know that when there is a question regarding their risk from a senior manager, it is relevant to the risk they are actually managing. The risks may be expressed using different terminology, but when risk is calculated and reported on a consistent basis, the various risks can be translated into a common language.
An example will help demonstrate how the underlying foundations can be consistent even when the language of risk is quite different across levels of a firm. Consider the market risk for a very simple portfolio:

$20 million nominal of a 10-year U.S. Treasury (UST) bond.
€7 million nominal of CAC 40 Index (French equity index) futures.

We can take this as a very simple example of a trading firm, with the bond representing the positions held by a fixed-income trading desk or investment portfolio and the futures representing the positions held by an equity trading desk or investment portfolio. In a real firm, the fixed-income portfolio would have many positions, with a fixed-income trader or portfolio manager involved in the minute-to-minute management of the positions, and a similar situation would exist for the equity portfolio. Senior managers would be responsible for the overall or combined risk but would not have involvement in the day-to-day decisions.
Desk-level traders require a very granular view of their risk. They require, primarily, information on the exposure or sensitivity of a portfolio to market risk factors. The fixed-income trader may measure exposure using duration, DV01 (also called basis point value [BPV] or dollar duration), or 5- or 10-year bond equivalents.2 The equity trader might measure the beta-equivalent notional of the position.
In all cases, the trader is measuring only the exposure or sensitivity—that is, how much the position makes or loses when the market moves a specified amount. A simple report showing the exposure or sensitivity for the fixed-income and equity portfolios might look like Table 1.1, which shows the DV01 for the bond and the beta-equivalent holding for the equity. The DV01 of the bond is $18,288, which means that if the yield falls by 1 basis point (bp), the profit will be $18,288.3 The beta-equivalent position of the equity holding is €7 million, or $9.1 million, in the CAC index.
Table 1.1 Sample Exposure Report.



Yield Curve (per 1 bp down)
Equity (beta-equivalent notional)




10-year par yield$18,288
CAC
$9,100,000


Market P&L and the distribution of P&L are always the result of two elements interacting: the exposure or sensitivity of positions to market risk factors and the distribution of the risk factors. The sample reports in Table 1.1 show only the first, the exposure to market risk factors. Desk-level traders will usually have knowledge of and experience with the markets, intuitively knowing how likely large moves are versus small moves, and so already have an understanding of the distribution of market risk factors. They generally do not require a formal report to tell them how the market might move but can form their own estimates of the distribution of P&L. In the end, however, it is the distribution of P&L that they use to manage their portfolios.
A more senior manager, removed somewhat from day-to-day trading and with responsibility for a wide range of portfolios, may not have the same intimate and up-to-date knowledge as the desk-level trader for judging the likelihood of large versus small moves. The manager may require additional information on the distribution of market moves.
Table 1.2 shows such additional information, the daily volatility or standard deviation of market moves for yields and the CAC index. We see that the standard deviation of 10-year yields is 7.1 bps and of the CAC index is 2.5 percent. This means that 10-year yields will rise or fall by 7.1 bps (or more) and that the CAC index will move by 2.5 percent (or more) roughly one day out of three. In other words, 7.1 bps provides a rough scale for bond market variability and 2.5 percent a rough scale for equity market volatility.
Table 1.2 Volatility or Standard Deviation of Individual Market Yield Moves.

The market and exposure measures from Tables 1.1 and 1.2 can be combined to provide an estimate of the P&L volatility for the bond and equity positions, shown in Table 1.3.4

Bond P&L volatility ≈ $18,288 × 7.15 ≈ $130,750
Equity P&L volatility ≈ $9,100,000 × 0.0254 ≈ $230,825

Table 1.3 Portfolio Sensitivity to One Standard Deviation Moves in Specific Market Risk Factors.

These values give a formal measure of the P&L variability or P&L distribution: the standard deviation of the P&L distributions. The $130,750 for the fixed-income portfolio means that the portfolio will make or lose about $130,750 (or more) roughly one day out of three; $130,750 provides a rough scale for the P&L variability. Table 1.3 combines the information in Tables 1.1 and 1.2 to provide information on the P&L distribution in a logical, comprehensible manner.
A report such as Table 1.3 provides valuable information. Nonetheless, a senior manager will be most concerned with the variability of the overall P&L, taking all the positions and all possible market movements into account. Doing so requires measuring and accounting for how 10-year yields move in relation to equities—that is, taking into consideration the positions in Table 1.1 and possible movements and co-movements, not just the volatilities of yields considered on their own as in Table 1.2.
For this simple two-asset portfolio, an estimate of the variability of the overall P&L can be produced relatively easily. The standard deviation of the combined P&L will be5
(1.1) 
Diagrammatically, the situation might be represented by Figure 1.1. The separate portfolios and individual traders with their detailed exposure reports are represented on the bottom row. (In this example, we have only two, but in a realistic portfolio there would be many more.) Individual traders focus on exposures, using their knowledge of potential market moves to form an assessment of the distribution of P&L.

Figure 1.1 Representation of Risk Reporting at Various Levels

Managers who are more removed from the day-to-day trading may require the combination of exposure and market move information to form an estimate of the P&L distributions. This is done in Table 1.3 and shown diagrammatically in the third row of Figure 1.1. Assessing the overall P&L requires combining the distribution of individual portfolios and assets into an overall distribution—performed in Equation 1.1 and shown diagrammatically in the top row of Figure 1.1.6
The important point is that the goal is the same for all assets and at all levels of the firm: measure, understand, and manage the P&L. This is as true for the individual trader who studies bond DV01s all day as it is for the CEO who examines the firm-wide VaR.
The portfolio we have been considering is particularly simple and has only two assets. The exposure report, Table 1.1, is simple and easy to comprehend. A more realistic portfolio, however, would have many assets with exposures to many market risk factors. For example, the fixed-income portfolio, instead of having a single DV01 of $18,288 included in a simple report like Table 1.1, might show exposure to 10 or 15 yield curve points for each of five or eight currencies. A granular report used by a trader could easily have 30 or 50 or 70 entries—providing the detail necessary for the trader to manage the portfolio moment by moment but proving to be confusing for anyone aiming at an overview of the complete portfolio.
The problem mushrooms when we consider multiple portfolios (say, a government trading desk, a swap trading desk, a credit desk, an equity desk, and an FX trading desk). A senior manager with overall responsibility for multiple portfolios requires tools for aggregating the risk, from simple exposures to individual portfolio distributions up to an overall distribution. The process of aggregation shown in Figure 1.1 becomes absolutely necessary when the number and type of positions and subportfolios increase.
Building the risk and P&L distributions from the bottom up as shown in Figure 1.1 is easy in concept, even though it is invariably difficult in practice. Equally or even more important, however, is going in the opposite direction: drilling down from the overall P&L to uncover and understand the sources of risk. This aspect of risk measurement is not always covered in great depth, but it is critically important. Managing the overall risk means making decisions about what risks to take on or dispose of, and making those decisions requires understanding the sources of the risk.
Consistency in calculating risk measures, building from the disaggregate up to the aggregate level and then drilling back down, is critically important. It is only by using a consistent framework that the full benefits of managing risk throughout the firm can be realized.
1.4 Systemic versus Idiosyncratic Risk
There is an important distinction, when thinking about risk, between what we might call idiosyncratic risk and systemic risk. This distinction is different from, although conceptually related to, the distinction between idiosyncratic and systemic (beta or market-wide) risk in the capital asset pricing model. Idiosyncratic risk is the risk that is specific to a particular firm, and systemic risk is widespread across the financial system. The distinction between the two is sometimes hazy but very important. Barings Bank's 1995 failure was specific to Barings (although its 1890 failure was related to a more general crisis involving Argentine bonds). In contrast, the failure of Lehman Brothers and AIG in 2008 was related to a systemic crisis in the housing market and wider credit markets.
The distinction between idiosyncratic and systemic risk is important for two reasons. First, the sources of idiosyncratic and systemic risk are different. Idiosyncratic risk arises from within a firm and is generally under the control of the firm and its managers. Systemic risk is shared across firms and is often the result of misplaced government intervention, inappropriate economic policies, or exogenous events, such as natural disasters. As a consequence, the response to the two sources of risk will be quite different. Managers within a firm can usually control and manage idiosyncratic risk, but they often cannot control systemic risk. More importantly, firms generally take the macroeconomic environment as given and adapt to it rather than work to alter the systemic risk environment.
The second reason the distinction is important is that the consequences are quite different. A firm-specific risk disaster is serious for the firm and individuals involved, but the repercussions are generally limited to the firm's owners, debtors, and customers. A systemic risk management disaster, however, often has serious implications for the macroeconomy and larger society. Consider the Great Depression of the 1930s, the developing countries' debt crisis of the late 1970s and 1980s, the U.S. savings and loan crisis of the 1980s, the Japanese crisis post-1990, the Russian default of 1998, the various Asian crises of the late 1990s, and the worldwide crisis of 2008, to mention only a few. These events all involved systemic risk and risk management failures, and all had huge costs in the form of direct (bailout) and indirect (lost output) costs.
It is important to remember the distinction between idiosyncratic and systemic risk because in the aftermath of a systemic crisis, the two often become conflated in discussions of the crisis. Better idiosyncratic (individual firm) risk management cannot substitute for adequate systemic (macroeconomic and policy) risk management. Failures of risk management are often held up as the primary driver of systemic failure. Although it is correct that better idiosyncratic risk management can mitigate the impact of systemic risk, it cannot substitute for appropriate macroeconomic policy. Politicians—indeed, all of us participating in the political process—must take responsibility for setting the policies that determine the incentives, rewards, and costs that shape systemic risk.
This book is about idiosyncratic risk and risk management—the risks that an individual firm can control. The topic of systemic risk is vitally important, but it is the subject for a different book—see, for example, the classic Manias, Panics, and Crashes: A History of Financial Crises by Kindleberger (1989) or the more recent This Time Is Different: Eight Centuries of Financial Folly by Reinhart and Rogoff (2009).
Notes
1. Guerrera and Larsen (2008).
2. Fixed-income exposure measures such as these are discussed in many texts, including Coleman (1998).
3. Instead of the DV01 of $18,288, the exposure or sensitivity could be expressed as an adjusted or modified duration of 8.2 or five-year bond equivalent of $39 million. In all cases, it comes to the same thing: measuring how much the portfolio moves for a given move in market yields. The DV01 is the dollar sensitivity to a 1 bp move in yields, and the modified duration is the percentage sensitivity to a 100 bp move in yields. Modified duration can be converted to DV01 by multiplying the modified duration times the dollar holding (and dividing by 10,000 because the duration is percent change per 100 bps and the DV01 is dollars per 1 bp). In this case, $20 million notional of the bond is worth $22.256 million, and 8.2 × 22,256,000/10,000 = $18,288 (within rounding).
4. Assuming linearity as we do here is simple but not necessary. There are alternate methodologies for obtaining the P&L distribution from the underlying position exposures and market risk factors; the linear approach is used here for illustration.
5. How volatilities combine is discussed more in Chapter 8. The correlation between bonds and the CAC equity is 0.24.
6. For more complicated portfolios and for risk measures other than volatility (for example, VaR or expected shortfall), the problem of combining multiple asset distributions into an overall distribution may be difficult but the idea is the same: Combine the individual positions to estimate the variability or dispersion of the overall P&L.









Chapter 2
Risk, Uncertainty, Probability, and Luck
Managing risk requires thinking about risk, and thinking about risk requires thinking about and being comfortable with uncertainty and randomness. It turns out that, as humans, we are often poor at thinking probabilistically. We like certainty in our lives and thinking about randomness does not come naturally; probability is often nonintuitive. We should not abandon the effort, however; just as we can learn to ride a bike as a child we can learn to think probabilistically. Doing so opens horizons, allows us to embrace the fluid, uncertain nature of our world.
This chapter focuses on how to think about risk, uncertainty, and probability. This chapter provides some of the tools we will use throughout the rest of the book, but more importantly, it helps us move from the world as rigid and fixed to a world that is changeable and contingent, which helps us explore the wonderful complexity of our world.
2.1 What Is Risk?
Before asking, "What is risk management?" we need to ask, "What is risk?" This question is not trivial; risk is a very slippery concept. To define risk, we need to consider both the uncertainty of future outcomes and the utility or benefit of those outcomes. When someone ventures onto a frozen lake, that person is taking a risk not just because the ice may break but because if it does break, the result will be bad. In contrast, for a frozen lake upon which no one is trying to cross on foot, we would talk of the chance of ice breaking; we would use the word risk only if the breaking ice had an impact on someone or something. Or, to paraphrase the philosopher George Berkeley, if a tree falls in the forest but there is nobody there for it to fall upon, is it risky?
The word risk is usually associated with downside or bad outcomes, but when trying to understand financial risk, limiting the analysis to just the downside would be a mistake. Managing financial risk is as much about exploiting opportunities for gain as it is about avoiding downside. It is true that, everything else held equal, more randomness is bad and less randomness is good. It is certainly appropriate to focus, as most risk measurement texts do, on downside measures (for example, lower quantiles and VaR). But upside risk cannot be ignored. In financial markets, everything else is never equal and more uncertainty is almost invariably associated with more opportunity for gain. Upside risk might be better called opportunity, but downside risk and upside opportunity are mirror images, and higher risk is compensated by higher expected returns. Successful financial firms are those that effectively manage all risks: controlling the downside and exploiting the upside.1
Risk combines both the uncertainty of outcomes and the utility or benefit of outcomes. For financial firms, the future outcomes are profits—P&L measured in monetary units (that is, in dollars or as rates of return). The assumption that only profits matter is pretty close to the truth because the primary objective of financial firms is to maximize profits. Other things—status, firm ranking, jobs for life, and so on—may matter, but these are secondary and are ignored here.
Future outcomes are summarized by P&L, and the uncertainty in profits is described by the distribution or density function. The distribution and density functions map the many possible realizations for the P&L, with profits sometimes high and sometimes low. Figure 2.1 shows the possible P&L from a $10 coin toss bet (only two possible outcomes) and from a hypothetical yield curve strategy (many possible outcomes). The vertical axis measures the probability of a particular outcome, and the horizontal axis measures the level of profit or loss. For the coin toss, each outcome has a probability of one-half. For the yield curve strategy, there is a range of possible outcomes, each with some probability. In the end, however, what matters is the distribution of P&L—how much one can make or lose.

Figure 2.1 P&L from Coin Toss Bet and Hypothetical Yield Curve Strategy

The distribution function contains all the objective information about the random outcomes, but the benefit (positive or negative) provided by any given level of profit or loss depends on an investor's preferences or utility function—how much an investor values each positive outcome and how much he is averse to each negative one. Whether one distribution is ranked higher than another (one set of outcomes is preferred to another) depends on an investor's preferences.
Generally, there will be no unique ranking of distributions in the sense that distribution F is preferred to distribution G by all investors. Although it is true that in certain cases we can say that distribution F is unambiguously less risky than G, these cases are of limited usefulness. As an example, consider the two distributions in Panel A of Figure 2.2. They have the same mean, but distribution F has lower dispersion and a density function that is inside G. Distribution G will be considered worse and thus riskier by all risk-averse investors.2

Figure 2.2 Distributions with and without Unique Risk Ranking

More often, there will be no unique ranking, and some investors will prefer one distribution while others will prefer another. Panel B of Figure 2.2 shows two distributions: H with less dispersion but lower mean and K with more dispersion but higher mean. A particular investor could determine which distribution is worse given her own preferences, and some investors may prefer H while others prefer K, but there is no unique ranking of which is riskier.
The bottom line is that the riskiness of a distribution will depend on the particular investor's preferences. There is no unique risk ranking for all distributions and all investors. To rank distributions and properly define risk, preferences must be introduced.
Markowitz (1959) implicitly provided a model of preferences when he introduced the mean-variance portfolio allocation framework that is now part of our financial and economic heritage. He considered a hypothetical investor who places positive value on the mean or expected return and negative value on the variance (or standard deviation) of return. For this investor, the trade-off between sets of outcomes depends only on the mean and variance. Risk is usually equated to variance in this framework because variance uniquely measures the disutility resulting from greater dispersion in outcomes.
In the mean-variance Markowitz framework, the problem is reduced to deciding on the trade-off between mean and variance (expected reward and risk). The exact trade-off will vary among investors, depending on their relative valuation of the benefit of mean return and the cost of variance. Even here, the variance uniquely ranks distributions on a preference scale only when the means are equal. In Figure 2.2, Panel B, distribution K might be preferred to H by some investors, even though K has a higher variance (K also has a higher mean). Even when limiting ourselves to quadratic utility, we must consider the precise trade-off between mean and variance.
Markowitz's framework provides immense insight into the investment process and portfolio allocation process, but it is an idealized model. Risk can be uniquely identified with standard deviation or volatility of returns only when returns are normally distributed (so that the distribution is fully characterized by the mean and standard deviation) or when investors' utility is quadratic (so they care only about mean and standard deviation, even if distributions differ in other ways [moments]).
Although risk properly depends on both the distribution and investor preferences, for the rest of this book I focus on the distribution and largely ignore preferences. Preferences are difficult to measure and vary from one investor to another. Importantly, however, I do assume that preferences depend only on P&L: If we know the whole P&L distribution, we can apply it to any particular investor's preferences. Thus, as a working definition of risk for this book, I use the following: Risk is the possibility of P&L being different from what is expected or anticipated; risk is uncertainty or randomness measured by the distribution of future P&L. This statement is relatively general and, effectively, evades the problem of having to consider preferences or the utility of future outcomes, and it achieves the simplification necessary for a fruitful discussion of risk measurement and risk management to proceed.3
2.2 Risk Measures
One important consequence of viewing risk as the distribution of future P&L is that risk is multifaceted and cannot be defined as a single number; we need to consider the full distribution of possible outcomes. In practice, however, we will rarely know or use the full P&L distribution. We will usually use summary measures that tell us things about the distribution because the full distribution is too difficult to measure or too complicated to easily grasp or because we simply want a convenient way to summarize the distribution.
These summary measures can be called risk measures: numbers that summarize important characteristics of the distribution (risk). The first or most important characteristic to summarize is the dispersion, or spread, of the distribution. The standard deviation is the best-known summary measure for the spread of a distribution, and it is an incredibly valuable risk measure. (Although it sometimes does not get the recognition it deserves from theorists, it is widely used in practice.) But plenty of other measures tell us about the spread, the shape, or other specific characteristics of the distribution.
Summary measures for distribution and density functions are common in statistics. For any distribution, the first two features that are of interest are location, on the one hand, and scale (or dispersion), on the other. Location quantifies the central tendency of some typical value, and scale or dispersion quantifies the spread of possible values around the central value. Summary measures are useful but somewhat arbitrary because the properties they are trying to measure are somewhat vague.4 For risk measurement, scale is generally more important than location, primarily because the dispersion of P&L is large relative to the typical value.5
Figure 2.3 shows the P&L distribution (more correctly, the density function) for a hypothetical bond portfolio. The distribution is fairly well behaved, being symmetrical and close to normal or Gaussian. In this case, the mean of the distribution is a good indication of the central tendency of the distribution and serves as a good measure of location. The standard deviation gives a good indication of the spread or dispersion of the distribution and is thus a good measure of scale or dispersion.

Figure 2.3 P&L Distribution for Hypothetical Bond Portfolio

Particular measures work well in particular cases, but in general, one single number does not always work well for characterizing either location or scale. It is totally misleading to think there is a single number that is the risk, that risk can be summarized by a single number that works in all cases for all assets and for all investors. Risk is multifaceted. There are better and worse numbers, some better or worse in particular circumstances, but it will almost never be the case (except for textbook examples such as normality or quadratic utility) that a single number will suffice. Indeed, the all-too-common tendency to reduce risk to a single number is part of the "illusion of certainty" (to use a phrase from Gigerenzer 2002) and epitomizes the difficulty of thinking about uncertainty, to which I turn next.
2.3 Randomness and the Illusion of Certainty
Thinking about uncertainty and randomness is hard, if only because it is more difficult to think about what we do not know than about what we do. Life would be easier if risk could be reduced to a single number, but it cannot be. There is a human tendency and a strong temptation to distill future uncertainty and contingency down to a single, definitive number, providing the illusion of certainty. But many mistakes and misunderstandings ensue when one ignores future contingency and relies on a fixed number to represent the changeable future. The search for a single risk number is an example of the human characteristic of trying to reduce a complex, multifaceted world to a single factor.
To understand, appreciate, and work with risk, we have to move away from rigid, fixed thinking and expand to consider alternatives. We must give up any illusion that there is certainty in this world and embrace the future as fluid, changeable, and contingent. In the words of Gigerenzer (2002), "Giving up the illusion of certainty enables us to enjoy and explore the complexity of the world in which we live" (p. 231).
Difficulties with Human Intuition
Randomness pervades our world, but human intuition is not very good at working with randomness and probabilities. Experience and training do not always groom us to understand or live comfortably with uncertainty. In fact, a whole industry and literature are based on studying how people make mistakes when thinking about and judging probability. In the 1930s, "researchers noted that people could neither make up a sequence of [random] numbers...nor recognize reliably whether a given string was randomly generated" (Mlodinow 2008, ix). The best-known academic research in this area is by the psychologists Daniel Kahneman and Amos Tversky.6
Kahneman and Tversky did much to develop the idea that people use heuristics (rules of thumb or shortcuts for solving complex problems) when faced with problems of uncertainty and randomness. They found that heuristics lead to predictable and consistent mistakes (cognitive biases). They worked together for many years, publishing important early work in the 1970s. Kahneman received the 2002 Nobel Prize in Economic Sciences "for having integrated insights from psychological research into economic science, especially concerning human judgment and decision-making under uncertainty."7 (Tversky died in 1996, and the Nobel Prize is not awarded posthumously.)
One oft-cited experiment shows the difficulty in thinking about randomness and probability. Subjects were asked to assess the probability of statements about someone's occupation and interests given information about the person's background and character.8 In the experiment, Tversky and Kahneman presented participants with a description of Linda—31 years old, single, outspoken, and very bright. In college, Linda majored in philosophy, was deeply concerned with discrimination and social justice, and participated in antinuclear demonstrations. The experiment participants were then asked to rank the probability of three possible descriptions of Linda's current occupation and interests (that is, extrapolating forward from Linda's college background to her current status):

A. Linda is a bank teller
B. Linda is active in the feminist movement
C. Linda is a bank teller and is active in the feminist movement

Eighty-seven percent of the subjects ranked the probability of bank teller and feminist together higher than bank teller alone (in other words, they ranked C, which is both A and B together, above A alone). But this is mathematically impossible. Whatever Linda's current employment and interests are, the probability that Linda is both a bank teller and also an active feminist (C—that is, A and B together) cannot be higher than the probability of her being just a bank teller. No matter what the particulars, the probability of A and B together is never higher than the probability of A alone. Another way to see this problem is to note that the total universe of bank tellers is much larger than the subset of bank tellers who are also active feminists, so it has to be more likely that someone is a bank teller than that she is a bank teller who is also an active feminist.

Further Thoughts about Linda the Bank Teller
The bank teller/feminist combination may be less likely, yet psychologically it is more satisfying. The explanation possibly lies in our everyday experience and in the tasks we practice regularly. The essence of Kahneman and Tversky's experiment is to take Linda's college life and make probability statements about her future occupation. We do not commonly do this. We more frequently do the reverse: meet new acquaintances about whom we have limited information and then try to infer more about their character and background. In other words, it would be common to meet Linda at age 31, find out her current status, and make probability inferences about her college life. The likelihood that Linda had the college background ascribed to her would be much higher if she were currently a bank teller and active feminist than if she were a bank teller alone. In other words, P[college life|bank teller & feminist] > P[college life|bank teller], and P[bank teller & feminist|college life] < P[bank teller|college life]. It may be that we are good at solving the more common problem, whether through practice or innate psychological predisposition, and fail to account for the unusual nature of the problem presented in the experiment. We think we are solving the familiar problem, not the unfamiliar one. This explanation would be consistent with another Kahneman and Tversky experiment (Tversky and Kahneman 1983; Mlodinow 2008, 25) in which doctors are essentially asked to predict symptoms based on an underlying condition. Doctors are usually trained to do the reverse: diagnose underlying conditions based on symptoms.
Alternatively, the explanation may be in how the problem is posed. Possibly when we read C ("bank teller and feminist"), we unconsciously impose symmetry on the problem and reinterpret A as "bank teller and nonfeminist." Given the information we have about Linda, it would be reasonable to assign a higher probability to C than the reinterpreted A. Perhaps the experimental results would change if we chose a better formulation of the problem—for example, by stating A as "Linda is a bank teller, but you do not know if she is active in the feminist movement" because this restatement would make it very explicit that C is, in a sense, a subset of A.
The argument about heuristics (how we think about problems) and how a problem is posed is related to Gigerenzer (2002) and discussed in more detail later.

Such mistakes are not uncommon. Kahneman and Tversky developed the concepts of representativeness, availability of instances or scenarios, and adjustment from an anchor as three heuristics that people use to solve probability problems and deal with uncertainty.9 These heuristics often lead to mistakes or biases, as seen in the Linda example. The fields of behavioral economics and behavioral finance are in large part based on their work, and their work is not limited to the academic arena. Many books have popularized the idea that human intuition is not well suited to dealing with randomness. Taleb (2004, 2007) is well known, but Gigerenzer (2002) and Mlodinow (2008) are particularly informative.
Probability Is Not Intuitive
Thinking carefully about uncertainty and randomness is difficult but genuinely productive. The fact is that dealing with probability and randomness is hard and sometimes just plain weird. Mlodinow (2008), from which the description of the Linda experiment is taken, has further examples. But one particularly nice example of how probability problems are often nonintuitive is the classic birthday problem. It also exhibits the usefulness of probability theory in setting our intuition straight.
The birthday problem is discussed in many texts, with the stimulating book by Aczel (2004) being a particularly good presentation. The problem is simple to state: What is the probability that if you enter a room with 20 people, 2 of those 20 will share the same birthday (same day of the year, not the same year)? Most people would say the probability is small because there are, after all, 365 days to choose from. In fact, the probability is just over 41 percent, a number that I always find surprisingly high. And it only takes 56 people to raise the probability to more than 99 percent. As Aczel put it:
When fifty-six people are present in a room, there is a ninety-nine percent probability that at least two of them share a birthday! How can we get so close to certainty when there are only fifty-six people and a total of three hundred and sixty-five possible days of the year? Chance does seem to work in mysterious ways. If you have three hundred and sixty-five open boxes onto which fifty-six balls are randomly dropped, there is a ninety-nine percent chance that there will be at least two balls in at least one of the boxes. Why does this happen? No one really has an intuition for such things. The natural inclination is to think that because there are over three hundred empty boxes left over after fifty-six balls are dropped, no two balls can share the same spot. The mathematics tells us otherwise, and reality follows the mathematics. In nature, we find much more aggregation—due to pure randomness—than we might otherwise suspect. (pp. 71-72)10
Another example of how intuition can mislead and where probability is not intuitive is in assessing streaks, or runs. Random sequences will exhibit clustering, or bunching (e.g., runs of multiple heads in a sequence of coin flips), and such clustering often appears to our intuition to be nonrandom. The random shuffle on an iPod has actually been adjusted so it appears to us as more random. When the iPod was originally introduced, the random order of songs would periodically produce repetition and users hearing the same song or artist played back to back believed the shuffling was not random. Apple altered the algorithm to be "less random to make it feel more random," according to Steve Jobs.11 The clustering of random sequences is also why subrandom or quasi-random sequences are used for Monte Carlo simulation and Monte Carlo numerical integration; these sequences fill the space to be integrated more uniformly.12
To appreciate how runs can mislead, consider observing 10 heads in a row when flipping a coin. Having 10 in a row is unlikely, with a probability of 1 in 1,024, or 0.098 percent. Yet, if we flip a coin 200 times, there is a 17 percent chance we will observe a run of either 10 heads or 10 tails.13
Runs or streaks occur in real life, and we need to be very careful in interpreting such streaks. As the example of 10 heads shows, unlikely events do occur in a long-repeated process. A very practical example, highly relevant to anyone interested in risk management, is that of Bill Miller, portfolio manager of Legg Mason Value Trust Fund. Through the end of 2005, Bill Miller had a streak of 15 years of beating the S&P 500,14 which is an extraordinary accomplishment, but is it caused by skill or simply luck? We will see that it could easily be entirely because of luck.
The likelihood of a single fund beating the S&P 500 for 15 years in a row is low. Say we choose one particular fund, and let us assume that the fund has only a 50/50 chance of beating the index in a given year (so that no exceptional skill is involved, only luck). The probability of that fund beating the index for the next 15 years is only 1 in 32,768 or 0.003 percent—very low.
But 0.003 percent is not really the relevant probability. We did not select the Value Trust Fund before the streak and follow just that one fund; we are looking back and picking the one fund out of many that had a streak. The streak may have been caused by exceptional skill, but it may also have been caused by our looking backward and considering the one lucky fund that did exceptionally well. Among many funds, one fund will always be particularly lucky, even if we could not say beforehand which fund that would be.
When we look at many funds, how exceptional would it be to observe a streak of 15 years? Say that only 1,000 funds exist (clearly an underestimate), that each fund operates independently, and that each fund has a 50/50 chance of beating the index in a particular year. What would be the chance that, over 15 years, we would see at least 1 of those 1,000 funds with a 15-year streak? It turns out to be much higher than 1 in 32,768—roughly 1 in 30, or 3 percent.15 Therefore, observing a 15-year streak among a pool of funds is not quite so exceptional.
But we are not done yet. Commentators reported in 2003 (earlier in the streak) that "no other fund has ever outperformed the market for a dozen consecutive years over the last 40 years."16 We really should consider the probability that some fund had a 15-year streak during, say, the last 40 years. What would be the chance of finding one fund out of a starting pool of 1,000 that had a 15-year streak sometime in a 40-year period? This scenario gives extra freedom because the streak could be at the beginning, middle, or end of the 40-year period. It turns out that the probability is now much higher, around 33 percent. In other words, the probability of observing such a streak, caused purely by chance, is high.17
The point of this exercise is not to prove that Bill Miller has only average skill. Possibly he has extraordinary skill, possibly not. The point is that a 15-year streak, exceptional as it sounds, does not prove that he has extraordinary skill. We must critically evaluate the world and not be misled by runs, streaks, or other quirks of nature. A streak like Bill Miller's sounds extraordinary. But before we get carried away and ascribe extraordinary skill to Bill Miller, we need to critically evaluate how likely such a streak is due to pure chance. We have seen that it is rather likely. Bill Miller may have exceptional skill, but the 15-year streak does not, on its own, prove the point.1819

Probability Paradoxes and Puzzles: A Long Digression
There are many probability paradoxes and puzzles. In this long digression, I explore random walks and the "Monty Hall problem."19
Random Walks
One interesting and instructive case of a probability paradox is that of random walks—specifically, the number of changes of sign and the time in either positive or negative territory.
The simplest random walk is a process in which, each period, a counter moves up or down by one unit with a probability of half for each. (This example is sometimes colloquially referred to as the drunkard's walk, after a drunkard taking stumbling steps from a lamppost—sometimes going forward and sometimes back but each step completely at random.) A random walk is clearly related to the binomial process and Bernoulli trials because each period is up or down—in other words, an independent Bernoulli trial with probability p = ½.
Random walks provide an excellent starting point for describing many real-life situations, from gambling to the stock market. If we repeatedly toss a fair coin and count the number of heads minus the number of tails, this sequence is a simple random walk. The count (number of heads minus number of tails) could represent a simple game of chance: If we won $1 for every heads and lost $1 for every tails, the count would be our total winnings. With some elaborations (such as a p of not quite one-half and very short times), a random walk can provide a rudimentary description of stock market movements.
Let us consider more carefully a simple random walk representing a game of chance in which we win $1 for every heads and lose $1 for every tails. This is a fair game. My intuition about the law of averages would lead me to think that because heads and tails each have equal chance, we should be up about half the time and we should go from being ahead to being behind fairly often. This assumption may be true in the long run, but the long run is very deceptive. In fact, "intuition leads to an erroneous picture of the probable effects of chance fluctuations."20
Let us say we played 10,000 times. Figure 2.4 shows a particularly well-known example from Feller (1968). In this example, we are ahead (positive winnings) for roughly the first 120 tosses, and we are substantially ahead for a very long period, from about toss 3,000 to about 6,000. There are only 78 changes of sign (going from win to lose or vice versa), which seems to be a small number but is actually more than we should usually expect to see. If we repeated this game (playing 10,000 tosses) many times, then roughly 88 percent of the time we would see fewer than 78 changes of sign in the cumulative winnings. This is extraordinary to me.

Figure 2.4 Sample of 10,000 Tosses of an Ideal Coin

Even more extraordinary would be if we ran this particular example of the game in reverse, starting at the end and playing backward. The reverse is also a random walk, but for this particular example, we would see only eight changes of sign and would be on the negative side for 9,930 out of 10,000 steps—on the winning side only 70 steps. And yet, this outcome is actually fairly likely. The probability is better than 10 percent that in 10,000 tosses of a fair coin, we are almost always on one side or the other—either winning or losing for more than 9,930 out of the 10,000 trials. This result sounds extraordinary, but it is simply another example of how our intuition can mislead. As Feller says, if these results seem startling, "this is due to our faulty intuition and to our having been exposed to too many vague references to a mysterious 'law of averages'" (p. 88).
As a practical matter, we must be careful to examine real-world examples and compare them with probability theory. In a game of chance or other events subject to randomness (such as stock markets), a long winning period might lead us to believe we have skill or that the probability of winning is better than even. Comparison with probability theory forces us to critically evaluate such assumptions.
The Monty Hall Problem
One of the best-known probability puzzles goes under the name of the Monty Hall problem, after the host of the old TV game show Let's Make a Deal. One segment of the original show involved Monty Hall presenting a contestant with three doors. Behind one door was a valuable prize (often a car), and behind the other two were less valuable or worthless prizes (invariably referred to in current presentations as goats). The contestant chose one door, but before the chosen door was opened, Monty Hall would step in and open one of the doors and then give the contestant the opportunity to either stay with his original choice or switch. The probability puzzle is this: Is it better to stay with your original door or switch?
The answer we will eventually come to is that it is better to switch: The chance of winning is one-third if you stay with the original door and two-thirds if you switch.
Before delving into the problem more deeply, however, two particulars are needed. First, the problem as I have written it is actually not well posed and really cannot be answered properly. The heart of the problem, as we will see, is exactly what rules Monty Hall uses to open the doors: Does he always open a door, no matter which door the contestant chooses? Does he always open a door with a goat? The outline of the problem just given is too sloppy in laying out the rules.
Second, this problem has created more controversy and more interest both inside and outside the mathematical community than any comparable brainteaser. The history of the problem is itself interesting, but the controversy also serves to highlight some important truths:

 Thinking carefully about probability is hard but does have value. By doing so, we can get the right answer when intuition may mislead us.
 Assumptions and the framework of the problem are vitally important. We shall see that the answer for the Monty Hall problem depends crucially on the details of how the game show is set up.
 When we get an answer that does not make sense, we usually need to go back and refine our thinking about and assumptions behind the problem. We often find that we did not fully understand how to apply the solution or the implications of some assumption. Ultimately, we end up with deeper insight into the problem and a better understanding of how to apply the solution in the real world. (This is somewhat along the lines of Lakatos's [1976] Proofs and Refutations.)
 Related to the preceding point, probability problems and models are just representations of the world and it is important to understand how well (or how poorly) they reflect the part of the world we are trying to understand. The Monty Hall problem demonstrates this point well. In the actual TV show, Monty Hall did not always act as specified in this idealized problem. Our solution does, however, point us toward what is important—in this case, understanding Monty Hall's rules for opening the doors.

The Monty Hall problem has been around for a considerable time, and its more recent popularity has generated a considerable literature. A recent book by Jason Rosenhouse (2009), on which many points in this exposition are based, is devoted entirely to Monty Hall.21 The first statement of the problem, under a different name but mathematically equivalent, was apparently made by Martin Gardner (1959) in a Scientific American column. That version of the problem, although it generated interest in the mathematical community, did not become famous.
The first appearance of the problem under the rubric of Monty Hall and Let's Make a Deal appears to have been in 1975, in two letters published in the American Statistician by Steve Selvin (1975a, 1975b). Once again, this presentation of the problem generated interest but only within a limited community.
The Monty Hall problem took off with the answer to a question in Parade magazine in September 1990 from reader Craig Whitaker to the columnist Marilyn vos Savant, author of the magazine's "Ask Marilyn" column. Vos Savant was famous for being listed in the Guinness Book of World Records (and inducted into the Guinness Hall of Fame) as the person with the world's highest recorded IQ (228) but is now better known for her (correct) response to the Monty Hall problem.
The question that started the furor was as follows:
Suppose you are on a game show, and you are given the choice of three doors. Behind one door is a car, behind the others, goats. You pick a door, say, Number 1, and the host, who knows what is behind the doors, opens another door, say, Number 3, which has a goat. He says to you, "Do you want to pick door Number 2?" Is it to your advantage to switch your choice of doors? (vos Savant 1990a, 15)
The reply was:
Yes, you should switch. The first door has a one-third chance of winning, but the second door has a two-thirds chance. Here's a good way to visualize what happened. Suppose there are a million doors, and you pick door Number 1. Then the host, who knows what is behind the doors and will always avoid the one with the prize, opens them all except door number 777,777. You would switch to that door pretty fast, wouldn't you? (vos Savant 1990b, 25)
This simple exchange led to a flood of responses—thousands of letters from the general public and the halls of academe. Vos Savant was obliged to follow up with at least two further columns. The responses, many from professional mathematicians and statisticians, were often as rude as they were incorrect (from vos Savant 1996, quoted in Rosenhouse 2009, 24-25):
Since you seem to enjoy coming straight to the point, I will do the same. In the following question and answer, you blew it!
You blew it, and you blew it big!
May I suggest that you obtain and refer to a standard textbook on probability before you try to answer a question of this type again?
You made a mistake, but look at the positive side. If all those PhDs were wrong, the country would be in some very serious trouble.
Unfortunately for these correspondents, vos Savant was absolutely correct, although possibly less careful than an academic mathematician might have been in stating the assumptions of the problem. All those PhDs were wrong.
Let me state the problem in a reasonably precise way:

 There are three doors, with a car randomly placed behind one door and goats behind the other two.
 Monty Hall, the game show host, knows the placement of the car and the goats; the contestant does not.
 The contestant chooses one door, but that door is not opened.
 Monty Hall then opens a door. He follows these rules in doing so:

 Never open the door the contestant has chosen.
 If the car is behind the contestant's door (so that the two nonchosen doors have goats), randomly choose which goat door to open.
 If the car is behind one of the two nonchosen doors (so only one nonchosen door has a goat), open that goat door.

 As a result of these rules, Monty Hall will always open a nonchosen door and that door will always show a goat.
 Most importantly, the rules ensure that a goat door is opened deliberately and systematically, in a decidedly nonrandom way so that a goat door is always opened and a car door is never opened.
 The contestant is now given the choice of staying with her original door or switching to the remaining closed door.

The natural inclination is to assume that there are now two choices (the door originally chosen and the remaining unopened door), and with two choices, there is no benefit to switching; it is 50/50 either way. This natural inclination, however, is mistaken. The chance of winning the car by remaining with the original door is one-third, the chance of winning by switching is two-thirds.
As pointed out earlier, there is a vast literature discussing this problem and its solution. I will outline two explanations for why the one-third versus two-thirds answer is correct, but take my word that, given the rules just outlined, it is correct.22
The first way to see that switching provides a two-thirds chance of winning is to note that the originally chosen door started with a one-third chance of having the car and the other two doors, together, had a two-thirds chance of winning. (Remember that the car was randomly assigned to a door, so any door a contestant might choose has a one-third chance of being the door with the car.) The way that Monty Hall chooses to open a door ensures that he always opens one of the other two doors and always chooses a door with a goat. The manner of his choosing does not alter the one-third probability that the contestant chose the car door originally, nor does it alter the two-thirds probability that the car is behind one of the other two. By switching, the contestant can move from one-third to two-thirds probability of winning. (Essentially, in the two-thirds of the cases in which the car is behind one of the other two doors, Monty Hall reveals which door it is not behind. Monty Hall's door opening provides valuable information.)
An alternative approach, and the only one that seems to have convinced some very astute mathematicians, is to simulate playing the game.23 Take the role of the contestant, always pick Door 1, and try the strategy of sticking with Door 1. (Because the car is randomly assigned to a door, always picking Door 1 ends up the same as randomly picking a door.) Use a random number generator to generate a uniform random variable between 0 and 1 (for example, the RAND() function in Microsoft Excel). If the random number is less than one-third, or 0.3333, then the car is behind Door 1 and you win. Which other door is opened does not matter. Try a few repeats, and you will see that you win roughly one-third of the time.
Now change strategies and switch doors. If the random number is less than one-third, or 0.3333, then the car is behind Door 1 and you lose by switching doors. Which other door is opened really does not matter because both doors have goats and by switching, you lose. If the random number is between 0.3333 and 0.66667, then the car is behind Door 2; Door 3 must be opened, and you switch to Door 2 and win. If the random number is between 0.66667 and 1.0, then the car is behind Door 3; Door 2 must be opened, and you switch to Door 3 and win. Try several repeats. You will soon see that you win two-thirds of the time and lose one-third.
In the end, the strategy of switching wins two-thirds of the time and the strategy of staying wins only one-third. Although nonintuitive, this strategy is correct. In the literature, there are many discussions of the solution, many that go into detail and present solutions from a variety of perspectives.24
In this problem, the rules for choosing the doors are the critical component. Consider an alternate rule. Say that Monty Hall does not know the car location and randomly chooses an unopened door, meaning that he sometimes opens a door with the car and the game ends. In this case, the solution is that if a door with a goat is opened, staying and switching each have a 50/50 chance of winning and there is no benefit to switching.
In the original game, Monty Hall's opening a goat door tells you nothing about your original door; the rules are designed so that Monty Hall always opens a goat door, no matter what your original choice. Heuristically, the probability of the originally chosen door being a winner does not change; it remains at one-third. (This can be formalized using Bayes' rule.)
In the alternate game, opening a door does tell you something about your original choice. When Monty Hall opens a door with a car (roughly one-third of the time), you know for sure that your door is a loser. When Monty Hall opens a goat door (two-thirds of the time), you know that now only two choices are left, with your originally chosen door one of those possibilities.
The actual TV show apparently did not abide by either of these sets of rules but, rather, by a set of rules we might call somewhat malevolent.25 If the contestant chose a goat, Monty Hall would usually open the contestant's door to reveal the goat and end the game. When the contestant chose the car, Monty Hall would open one of the other doors to reveal a goat and then try to persuade the contestant to switch. Under these rules, Monty Hall's opening one of the other doors would be a sure sign that the originally chosen door was a winner. In this case, the best strategy would be to stick with the original door whenever Monty Hall opened another door.
For the actual TV game, the standard problem does not apply and the probability arguments are not relevant. Nonetheless, the analysis of the problem would have been truly valuable to any contestant. The analysis highlights the importance of the rules Monty Hall uses for choosing which door to open. For the actual game, contestants familiar with the probability problem could examine past games, determine the scheme used by Monty Hall to open doors, and substantially improve their chance of winning.

Past/Future Asymmetry
One aspect of uncertainty and randomness that is particularly important is what might be called past/future asymmetry. It is often easy to explain the past but very difficult to predict the future, and events that look preordained when viewed in hindsight were often uncertain at the time. Mlodinow (2008) discusses this topic at some length. One nice example he gives in Chapter 10 is chess:
Unlike card games, chess involves no explicit random element. And yet there is uncertainty because neither player knows for sure what his or her opponent will do next. If the players are expert, at most points in the game it may be possible to see a few moves into the future; if you look out any further, the uncertainty will compound, and no one will be able to say with any confidence exactly how the game will turn out. On the other hand, looking back, it is usually easy to say why each player made the moves he or she made. This again is a probabilistic process whose future is difficult to predict but whose past is easy to understand. (pp. 197-198)
In Chapter 1 of his book, Mlodinow gives examples of manuscripts rejected by publishers: John Grisham's manuscript for A Time to Kill by 26 publishers, J. K. Rowling's first Harry Potter manuscript by 9, and Dr. Seuss's first children's book by 27. Looking back, it is hard to believe that such hugely popular books could ever have been rejected by even one publisher, but it is always easier to look back and explain what happened than it is to look forward and predict what will happen.
Because we always look back at history and so often it is easy to explain the past, we can fall into the trap of thinking that the future should be equally easy to explain and understand. It is not, and the chess example is a good reminder of how uncertain the future can be even for a game with well-defined rules and limited possible moves. We must continually remember that the future is uncertain and all our measurements give us only an imperfect view of what might happen and will never eliminate the inherent uncertainty of the future.
Do Not Worry Too Much about Human Intuition
It is true that thinking about uncertainty is difficult and human intuition is often poor at solving probability problems. Even so, we should not go too far worrying about intuition. So what if human intuition is ill suited to situations involving uncertainty? Human intuition is ill suited to situations involving quantum mechanics, or special relativity, or even plain old classical mechanics. That does not stop us from developing DVD players and MRI scanners (which depend on quantum mechanics) and GPS devices (requiring both special and general relativistic timing corrections) or from calculating projectile trajectories (using classical mechanics). None of these are intuitive; they require science and mathematics to arrive at correct answers, and nobody is particularly surprised that quantitative analysis is required to inform, guide, and correct intuition.
If we were to conduct experiments asking people about relativistic physics, nobody would get the right answers. The paradoxes in relativity are legion and, in fact, are widely taught in undergraduate courses in special relativity. And quantum mechanics is worse: Einstein never could accept quantum entanglement and what he called "spooky action at a distance," but it is reality, nonetheless. Lack of intuition does not stop the development of relativistic physics or quantum mechanics or their practical application.
In the realm of probability, why should anybody be surprised that quantitative analysis is necessary for understanding and dealing with uncertainty? We should be asking how good are the quantitative tools and how useful is the quantitative analysis, not fret that intuition fails. "The key to understanding randomness and all of mathematics is not being able to intuit the answer to every problem immediately but merely having the tools to figure out the answer" (Mlodinow 2008, 108).
This discussion is not meant to belittle intuition. Intuition can be valuable, and not all problems can be solved mathematically. The best seller Blink, by Gladwell (2005), extols the virtues of intuition26 and is itself based in part on research performed by Gigerenzer (2007). My point is that the failure of intuition in certain circumstances does not invalidate the usefulness or importance of formal probabilistic analysis.
Steps toward Probabilistic Numeracy
I am not saying that understanding and working with probability is easy. Nor am I saying that risk management is a science comparable to physics; in many ways, it is harder because it deals with the vagaries of human behavior. But neither should we, as some commentators seem to advocate, just walk away and ignore the analytical and mathematical tools that can help us understand randomness and manage risk. Risk management and risk measurement are hard, and there are and will continue to be mistakes and missteps and problems that cannot be solved exactly, or even approximately. But without the mathematics to systematize and organize the problems, the task would be plainly impossible.
Gigerenzer (2002), who takes a critical approach to the work of Kahneman and Tversky, has a refreshing approach to the problem of living with uncertainty. (Indeed, Gigerenzer [2002] was published outside the United States under the title Reckoning with Risk: Learning to Live with Uncertainty.) Gigerenzer argues that sound statistical (and probabilistic) thinking can be enhanced, both through training and through appropriate tools and techniques:
Many have argued that sound statistical thinking is not easily turned into a "habit of mind."...I disagree with this habit-of-mind story. The central lesson of this book is that people's difficulties in thinking about numbers need not be accepted, because they can be overcome. The difficulties are not simply the mind's fault. Often, the solution can be found in the mind's environment, that is, in the way numerical information is presented. With the aid of intuitively understandable representations, statistical thinking can become a habit of mind. (p. 245)
Gigerenzer (2002, 38) aims to overcome statistical innumeracy through three steps:

1. Defeat the illusion of certainty (the human tendency to believe in the certainty of outcomes or the absence of uncertainty)
2. Learn about actual risks of relevant events and actions
3. Communicate risks in an understandable way

These three steps apply equally to risk management. Most work in risk management focuses on the second—learning about risks—but the first and third are equally important. Thinking about uncertainty is hard, but it is important to recognize that things happen and the future is uncertain. And communicating risk is especially important. The risks a firm faces are often complex and yet need to be shared with a wide audience in an efficient, concise manner. Effectively communicating these risks is a difficult task that deserves far more attention than it is usually given.
2.4 Probability and Statistics
Probability is the science of studying uncertainty and systematizing randomness. Given uncertainty of some form, what should happen, what should we see? A good example is the analysis of streaks, the chance of a team winning a series of games. This kind of problem is discussed in any basic probability text, and Mlodinow (2008) discusses this type of problem.
Consider two teams that play a series of three games, with the first team to win two games being the winner of the series. There are four ways a team can win the series and four ways to lose the series, as laid out in the following table. If the teams are perfectly matched, each has a 50 percent chance of winning a single game, each individual possibility has a probability of one-eighth (0.125 = 0.5 × 0.5 × 0.5), and each team has a 50 percent chance of winning the series:

The analysis seems fairly obvious.27 But consider if the teams are not evenly matched and one team has a 40 percent chance of winning and a 60 percent chance of losing. What is the probability the inferior team still wins the series? We can write down all the possibilities as before, but now the probabilities for outcomes will be different—for example, a WWL for the inferior team will have probability 0.096 (0.4 × 0.4 × 0.6):

It turns out the probability of the inferior team winning the series is 35 percent, not a lot less than the chance of winning an individual game.
The problem becomes more interesting when considering longer series. The winner of the World Series in baseball is the winner of four out of seven games. In baseball, the best team in a league wins roughly 60 percent of its games during a season and the worst team wins roughly 40 percent, so pitting a 60 percent team against a 40 percent team would be roughly equivalent to pitting the top team against the bottom team. What would be the chance that the inferior team would still win the series? We need only write down all the possible ways as we just did (but now there are 128 possible outcomes rather than 8), calculate the probability of each, and sum them up. The result is 29 percent.
To me, a 29 percent chance of such an inferior team winning the series is surprisingly high. It is also a good example of how probability theory can help guide our intuition. I would have thought, before solving the problem, that the probability would be lower, much lower. The analysis, however, forces me to realize that either my intuition is wrong or that my assumptions are wrong.28 Probability theory and analysis help us to critically evaluate our intuition and assumptions and to adjust both so that they more closely align with experience and reality.
The analysis of win/lose situations turns out to be quite valuable and applicable to many problems. It is the same as coin tossing: heads versus tails (although not necessarily with a balanced 50/50 coin). It applies to the streak of the Legg Mason Value Trust Fund. The name given to such a process with two outcomes, one outcome usually (for convenience) labeled success and the other failure, is a Bernoulli trial. When a Bernoulli trial is repeated a number of times, the number of successes that occurs is said to have a binomial distribution.

Bernoulli
Bernoulli trials are named after Jakob Bernoulli (1654-1705, also known as Jacob, James, and Jacques). The Bernoulli family was so prolific that it is difficult to keep all the Bernoullis straight. Over the time from 1650 to 1800, the family produced eight noted mathematicians with three (Jakob, brother Johann, and nephew Daniel) among the world's greatest mathematicians.
The weak law of large numbers originated with Jakob and also goes by the name of Bernoulli's Theorem. It was published as the "Golden Theorem" in Ars Conjectandi in 1713, after Jakob's death. The probabilistic Bernoulli's Theorem should not be confused with the fluid dynamics Bernoulli's Theorem, or principle, which originated with nephew Daniel (1700-1782).

Bernoulli trials and the binomial distribution have immediate application to finance and risk management. We often know (or are told) that there is only a 1 percent chance of losses worse than some amount Y (say, $100,000) in one day. This is the essence of VaR, as I show in Chapter 5. We can now treat losses for a given day as a Bernoulli trial: 99 percent chance of success, and 1 percent chance of failure (losses worse than $100,000). Over 100 days, this is a sequence of 100 Bernoulli trials, and the number of successes or failures will have a binomial distribution.
We can use probability theory to assess the chance of seeing one or more days of large losses. Doing so provides an example of how we must move toward embracing randomness and away from thinking there is any certainty in our world. The number of days worse than $100,000 will have a binomial distribution. Generally, we will not see exactly 1 day out of 100 with large losses, even though with a probability of 1 out of 100 we expect to see 1 day out of 100. Over 100 days, there is only a 37 percent chance of seeing a single day with large losses. There is a 37 percent chance of seeing no losses worse than $100,000, a 19 percent chance of two days, and even an 8 percent chance of three or more days of large losses.29
The intent of this section is not to cover probability theory in depth but, rather, to explain what it is and show how it can be used. Books such as Mlodinow (2008), Gigerenzer (2002), Hacking (2001), Kaplan and Kaplan (2006), and, in particular, Aczel (2004) are very useful. Probability systematizes how we think about uncertainty and randomness. It tells us what we should expect to observe given a certain model or form of randomness in the world—for example, how likely a team is to win a series or how likely it is to see multiple bad trading days in a set of 100 days. Building probabilistic intuition is valuable; I would even say necessary, for any success in managing risk.
Statistics
Probability theory starts with a model of randomness and from there develops statements about what we are likely to observe. Statistics, roughly speaking, works in the opposite direction. We use what we observe in nature to develop statements about the underlying probability model. For example, probability theory might start with knowing that there is a 1 percent chance of a day with losses worse than $100,000 and then tell us the chance that, in a string of 100 days, we will observe exactly one or exactly two or exactly three such days. Statistics starts with the actual losses that we observe over a string of 100 days and attempts to estimate the underlying process: Is the probability of a loss worse than $100,000 equal to 1 percent or 2 percent? Statistics also provides us with estimates of confidence about the probabilities so that we can know, for example, whether we should strongly believe that it is a 1 percent probability or (alternately) whether we should only feel confident that it is somewhere between 0.5 percent and 1.5 percent.
For the technical side of risk measurement, statistics is equally or more important than probability. For the application of risk management, for actually managing risk, however, probability is more important. A firm understanding of how randomness may affect future outcomes is critical, even if the estimation of the underlying model has to be left to others. Without an appreciation of how randomness governs our world, understanding risk is impossible.
Theories of Probability: Frequency versus Belief (Objective versus Subjective)
There are deep philosophical questions concerning the foundations of probability, with two theories that are somewhat at odds. These theories often go under the name of objective probability versus subjective probability or by the words risk versus uncertainty, although better names (used by Hacking 2001) are frequency-type versus belief-type probability. Fortunately, we can safely sidestep much of the debate over the alternate approaches and, for most practical purposes, use the two interchangeably. Nonetheless, the distinction is relevant, and I will discuss the issues here before turning back to more strictly risk management issues.
The objective, or frequency-type, theory of probability is the easiest to understand and is tied to the origins of probability theory in the seventeenth century. Probability theory started with games of chance and gambling, and the idea of frequency-type probability is best demonstrated in this context. Consider an ideal coin, with a 50 percent chance of heads versus tails. Each flip of the coin is a Bernoulli trial, and we know that the probability of a heads is 50 percent. How do we know? It is an objective fact—one that we can measure by inspecting the coin or even better by counting the frequency of heads versus tails over a large number of trials. (The words objective and frequency are applied to this probability approach exactly because this probability approach measures objective facts and can be observed by the frequency of repeated trials.)
Repeated throws of a coin form the archetypal frequency-type probability system. Each throw of the coin is the same as any other, each is independent of all the others, and the throw can be repeated as often and as long as we wish.30 Frequency-type probability reflects how the world is (to use Hacking's phrase). It makes statements that are either true or false: A fair coin either has a one-half probability of landing heads on each throw or it does not; it is a statement about how the world actually is.
For frequency-type probability, laws of large numbers and central limit theorems are fundamental tools. Laws of large numbers tell us that as we repeat trials (flips of the coin), the relative frequency of heads will settle down to the objective probability set by the probabilistic system we are using, one-half for a fair coin. Not only that, but laws of large numbers and central limit theorems tell us how fast and with what range of uncertainty the frequency settles down to its correct value. These tools are incredibly powerful. For example, we can use the usual central limit theorem to say that in a coin-tossing experiment with 100 flips, we have a high probability that we will observe between 40 and 60 heads (and a low probability that we will observe outside that band).31
Frequency-type probability is ideally suited to games of chance, in which the game is repeated always under the same rules. Much of the world of finance fits reasonably well into such a paradigm. Trading in IBM stock is likely to look tomorrow like it does today—not in regard to the stock going up by the exact amount it did yesterday but, rather, in the likelihood that it will go up or down and by how much. New information might come out about IBM, but news about IBM often comes out, which is part of the repeated world of trading stocks. Whether IBM goes up or down is, in effect, as random as the flip of a coin (although possibly a biased coin because stocks generally grow over time). For many practical purposes, the coin that is flipped today can be considered the same as the coin flipped yesterday: We do not know whether IBM will go up or down tomorrow, but we usually do not have any particular reason to think it more likely to go up tomorrow than it has, on average, in the past.
For many problems, however, a frequency-type approach to probability just does not work. Consider the weather tomorrow. What does it mean to say the probability of precipitation tomorrow is 30 percent? This is not a true or false statement about how the world is. Viewed from today, tomorrow is a one-time event. Saying the probability is 30 percent is a statement about our confidence in the outcome or about the credibility of the evidence we use to predict that it will rain tomorrow. We cannot consider frequencies because we cannot repeat tomorrow. What about the probability that an asteroid impact led to the extinction of the dinosaurs? Or the probability that temperatures will rise over the next century (climate change)? None of these are repeatable events to which we can apply frequency concepts or the law of large numbers. Yet we need to apply, commonly do apply, and indeed can sensibly apply probabilistic thinking to these areas.
For these kinds of one-off or unique or nonfrequency situations, we rely on belief-type probabilities, what are often termed subjective probabilities.32 Belief-type probabilities must follow the same rules as frequency-type probabilities but arise from a very different source.
The probability of one-off events, or more precisely, our assessment or beliefs about the probabilities, can be uncovered using a neat trick developed by Bruno de Finetti (1906-1985), an Italian mathematician and co-developer of mean-variance optimization.33 The de Finetti game is a thought experiment, a hypothetical lottery or gamble in which an event is compared with drawing balls from a bag.
Say the event we are considering is receiving a perfect score on an exam; a friend took an exam and claims she is absolutely, 100 percent sure she got a perfect score on the exam (and she will receive the score tomorrow).34 We might be suspicious because, as Ben Franklin so famously said, "Nothing can be said to be certain, except death and taxes," and exam grades in particular are notoriously hard to predict.
We could ask our friend to choose between two no-lose gambles: The first is to receive $10 tomorrow if our friend's test is a perfect score, and the second is to receive $10 if our friend picks a red ball from a bag filled with 100 balls. The bag is filled with 99 red balls and only one black ball so that there is a 99 percent chance our friend would pick a red ball from the bag. Most people would presumably draw from the bag rather than wait for the exam score. It is almost a sure thing to win the $10 by drawing from the bag, and our friend, being reasonable, probably does not assign a higher than 99 percent chance of receiving a perfect score.
Assuming our friend chooses to draw a ball from the bag with 99 red balls, we can then pose another choice between no-lose gambles: $10 if the test score is perfect versus $10 if a red ball is drawn from a bag—this one filled with 80 red and 20 black balls. If our friend chooses the test score, we know the subjective probability is between 99 percent and 80 percent. We can further refine the bounds by posing the choice between $10 for a perfect test score versus $10 for a red ball from a bag with 90 red and 10 black. Depending on the answer, the probability is between 99 percent and 90 percent or 90 percent and 80 percent.
Such a scheme can be used to uncover our own subjective probabilities. Even using the scheme purely as a thought experiment can be extremely instructive. Aczel (2004, 23) points out that people often restate their probabilities when playing this game; it forces us to think more carefully about our subjective probabilities and to make them consistent with assessments of other events. Aczel also points out that, interestingly, weather forecasters do not tend to change their assessments very much; their profession presumably forces them to think carefully about belief-type or subjective probabilities.
Note that the theory of belief-type probability includes more than just personal degrees of belief. Logical probability (that is, statements about the probability of events conditional on evidence or logical relations) is another form of belief-type probability. An example of a logical probability statement would be the following (taken from Hacking 2001, 142): "Relative to recent evidence about a layer of iridium deposits...the probability is 90 percent that the reign of the dinosaurs was brought to an end when a giant asteroid hit the Earth." This is a statement about the probability of some event conditional on evidence. It is intended to express a logical relationship between some hypothesis (here the extinction of dinosaurs) and relevant evidence (here the presence of iridium in asteroids and the distribution of iridium in geological deposits around the globe). In the theory of logical probability, any probability statement is always relative to evidence.
The good news in all this is that the laws of probability that we apply to frequency-type (objective) probability carry over to these belief-type (subjective) probability situations. Laws concerning independence of events, unions of events, conditional probability, and so on, all apply equally to frequency-type and belief-type probability. In fact, for most practical purposes, in our daily lives and in risk management applications, we do not need to make any definite distinction between the two; we can think of probability and leave it at that.

The History of Theories of Probability
The history of the philosophical debate on the foundations of probability is long. The distinction between objective and subjective probability is often ascribed to Knight (1921), but LeRoy and Singell (1987) argue that it more properly belongs to Keynes (1921). (LeRoy and Singell argue that Knight is open to various interpretations but that he drew a distinction between insurable risks and uninsurable uncertainty in which markets collapse because of moral hazard or adverse selection, rather than between objective risks and subjective uncertainties or the applicability or nonapplicability of the probability calculus. They state that "Keynes [1921] explicitly set out exactly the distinction commonly attributed to Knight" [p. 395].)
Frequency-Type Probability. John Venn (1834-1923), the inventor of Venn diagrams, developed one of the first clear statements of limiting frequency theories about probability. Richard von Mises (1883-1953), an Austrian-born applied mathematician, philosopher, and Harvard professor, systematically developed frequency ideas, and A. N. Kolmogorov (1903-1987) published definitive axioms of probability in 1933 and developed fundamental ideas of computational complexity. Karl Popper (1902-1994), an Austrian-born philosopher and professor at the London School of Economics, developed the propensity approach to frequency-type probability.
Belief-Type Probability. John Maynard Keynes (1883-1946), in A Treatise on Probability (1921), provided the first systematic presentation of logical probability. Frank Plumpton Ramsey (1903-1930) and Bruno de Finetti (1906-1985) independently invented the theory of personal probability, but its success is primarily attributed to Leonard J. Savage (1917-1971), who made clear the importance of the concept, as well as the importance of Bayes' rule. De Finetti (and Savage) thought that only personal belief-type probability made sense, whereas Ramsey saw room for a frequency-type concept, especially in quantum mechanics.
There has been, and continues to be, considerable debate over the various theories of probability. To gain an inkling of the potential ferocity of the debate, keep in mind the comment of John Venn, an early developer of the frequency theory, regarding the fact that in the logical theory of probability, a probability is always relative to evidence: "The probability of an event is no more relative to something else than the area of a field is relative to something else" (quoted in Hacking 2001, 143).
A valuable and straightforward exposition of the foundations of modern probability theory is given by the philosopher Ian Hacking (2001). And Hacking (1990, 2006) provides a nice history of probability.

Bayes' Theorem and Belief-Type Probability
One important divergence between the frequency-type and belief-type probability approaches is in the central role played by the law of large numbers versus Bayes' rule. The law of large numbers tells us about how relative frequencies and other observed characteristics stabilize with repeated trials. It is central to understanding and using frequency-type probability.
Bayes' rule (or Theorem), in contrast, is central to belief-type probability—so central, in fact, that belief-type probability or statistics is sometimes called Bayesian probability or statistics. Bayes' rule is very simple in concept; it tells us how to update our probabilities, given some new piece of information. Bayes' rule, however, is a rich source of mistaken probabilistic thinking and confusion. The problems that Bayes' rule applies to seem to be some of the most counterintuitive.
A classic example of the application of Bayes' rule is the case of testing for a disease or condition, such as HIV or breast cancer, with a good, but not perfect, test.35 Consider breast cancer, which is relatively rare in the general population (say, 5 in 1,000). Thus, the prior probability that a woman has breast cancer, given no symptoms and no family history, is only about 0.5 percent. Now consider the woman undergoing a mammogram, which is roughly 95 percent accurate (in the sense that the test falsely reports a positive result about 5 percent of the time). What is the chance that if a patient has a positive mammogram result, she actually has breast cancer? The temptation is to say 95 percent because the test is 95 percent accurate, but that answer ignores the fact that the prior probability is so low, only 0.5 percent. Bayes' rule tells us how to appropriately combine the prior 0.5 percent probability with the 95 percent accuracy of the test.
Before turning to the formalism of Bayes' rule, let us reason out the answer, using what Gigerenzer (2002) calls "natural frequencies." Consider that out of a pool of 1,000 test takers, roughly 5 (5 in 1,000) will actually have cancer and roughly 50 will receive false positives (5 percent false-positive rate, 5 in 100, or 50 in 1,000). That is, there will be roughly 55 positive test results, but only 5 will be true positives. This means the probability of truly having cancer, given a positive test result, is roughly 5 in 55 or 9 percent, not 95 in 100, or 95 percent. This result always surprises me, although when explained in this way, it becomes obvious.36
The formalism of Bayes' rule shows how the conditional probability of one event (in this case, the conditional probability of cancer, given a positive test) can be found from its inverse (in this case, the conditional probability of a positive test, given no cancer, or the false-positive rate).
Say we have two hypotheses—HY: cancer yes and HN: cancer no. We have a prior (unconditional) probability of each hypothesis:

and

We also have a new piece of evidence or information—EY: evidence or test result yes (positive) or EN: evidence or test result no (negative). The test is not perfect, so there is a 95 percent chance the test will be negative with no cancer and a 5 percent chance it will be positive with no cancer:

and

For simplicity, let us assume that the test is perfect if there is cancer (there are no false negatives):

and

Now, what is the probability that there is actually cancer, given a positive test (hypothesis yes, given evidence yes)—that is, what is

Bayes' rule says that
(2.1) 
This can be easily derived from the rules of conditional probability (see Hacking 2001, ch. 7), but we will simply take it as a rule for incorporating new evidence (the fact of a positive test result) to update our prior probabilities for the hypothesis of having cancer—that is, a rule on how to use EY to go from P(HY) to P(HY|EY). Plugging in the probabilities just given, we get

Bayes' rule has applications throughout our everyday lives as well as in risk management. The breast cancer example shows how important it is to use the updated probability—P(HY|EY) = 9 percent—rather than what our intuition initially gravitates toward—the test accuracy, 1 - P(EY|HN) = 95 percent. Failure to apply Bayes' rule is common and leads to harrowing encounters with doctors and severe miscarriages of justice. Mlodinow (2008) relates his personal experience of being told he was infected with HIV with 999 in 1,000 or 99.9 percent certainty. In reality, an appropriate application of Bayes' Theorem to his positive test results in a probability of about 1 in 11 or 9.1 percent. (He did not have HIV.)37 In legal circles, the mistake of using 1 - P(EY|HN) when P(HY|EY) should be used is called the "prosecutor's fallacy." Mlodinow (2008) discusses the cases of Sally Clark and O. J. Simpson. Gigerenzer has carried out research in this arena, and Gigerenzer (2002) devotes considerable attention to the issue: Chapter 8 to the O. J. Simpson trial and Chapter 9 to a celebrated California case, People v. Collins, among others.
Bayes' rule is central to belief-type probability because it tells us how to consistently use new evidence to update our prior probabilities. Bayesian probability theory is sometimes misunderstood, or caricatured, as a vacuous approach that can be used to arrive at whatever result the speaker desires. If the prior probability is silly (say, a prior probability of 1.0 that the equity risk premium is negative), then the resulting posterior will also be silly. Bayes' rule provides a standard set of procedures and formulas for using new evidence in a logical and consistent manner and, as such, is incredibly useful and powerful. Bayes' rule, however, does not excuse us from the hard task of thinking carefully and deeply about the original (prior) probabilities.

Thomas Bayes (1702-1761)
Thomas Bayes was a Presbyterian minister at Mount Sion, Tunbridge Wells, England. Bayes' considerable contribution to the theory of probability rests entirely on a single paper, which he never published. Bayes left the paper to fellow minister Richard Price (a mathematician in his own right and credited with founding the field of actuarial science), who presented it to the Royal Society on December 23, 1763. The paper apparently aroused little interest at the time, and full appreciation was left to Pierre-Simon Laplace (1749-1827). Yet, it has had a fundamental, lasting, and continuing influence on the development of probability and statistics, although it has often been considered controversial. "It is hard to think of a single paper that contains such important, original ideas as does Bayes'. His theorem must stand with Einstein's E = mc2 as one of the great, simple truths" (D. V. Lindley 1987, in Eatwell, Milgate, and Newman 1987, The New Palgrave, vol. 1, 208).

Using Frequency-Type and Belief-Type Probabilities
I have spent time explaining the distinction between frequency-type and belief-type probability for one important reason. Financial risk often combines both frequency-type and belief-type probabilities. For one thing, in the real world the future will never be the same as the past; it may be different not just in the particulars but in the distribution of outcomes itself. There will always be totally new and unexpected events; a new product may be introduced, new competitors may enter our business, new regulations may change the landscape.
There is another important reason why we need to consider both frequency-type and belief-type probabilities: Single events always involve belief-type probability. What is the chance that losses tomorrow will be less than $50,000? That is a question about a single event and as such is a question about belief-type and not frequency-type probability. Probability statements about single events are, inherently, belief type. We may, however, base the belief-type probability on frequency-type probability.
Hacking (2001, 137) discusses the frequency principle, a rule of thumb that governs when and how we switch between frequency-type and belief-type probability. He discusses the following example: A fair coin is tossed, but before we can see the result, the coin is covered. What is the probability that this particular coin toss is heads? This is a single event. We cannot repeat this particular experiment. And yet, it is clear that we should, rationally and objectively, say that the probability is one-half. We know the frequency-type probability for a fair coin turning up heads is one-half, and because we know nothing else about this single trial, we should use this frequency-type probability. The frequency principle is just this: When we know the frequency-type probability and nothing else about the outcome of a single trial, we should use the frequency-type probability.
Something like the frequency principle generally holds. The world is not a repeated game of chance to which fixed rules apply, and so we must always apply some component of subjective or belief-type probability to our management of risk. Aczel (2004) summarizes the situation nicely (emphasis in the original):
When an objective [frequency-type] probability can be determined, it should be used. (No one would want to use a subjective probability to guess what side a die will land on, for example.) In other situations, we do our best to assess our subjective [belief-type] probability of the outcome of an event. (p. 24)

Bayes' Theorem, Streaks, and Fund Performance
We can use Bayes' Theorem to help improve our understanding of fund performance and streaks, such as the streak experienced by the Legg Mason Value Trust Fund discussed earlier. Remember that through 2005, the Value Trust Fund had outperformed the S&P 500 for 15 years straight. And remember that for a single fund having no exceptional skill (that is, with a 50/50 chance of beating the index in any year), the probability of such a streak is very small: (1/2),15 or 0.000031 or 0.0031 percent. For a collection of 1,000 funds, however, the probability that one or more funds would have such a streak is 3 percent. The probability of having one or more such funds during a 40-year period out of a pool of 1,000 is about 32.8 percent.
Now let us turn the question around and consider what such a streak, when it occurs, tells us about funds in general and the Value Trust Fund in particular. Roughly speaking, our earlier application was probabilistic, using probability theory to say something about what we should observe. Our current application is more statistical, using data to make inferences about our underlying model.
Let us start with a simplistic hypothesis or model of the world, a model in which some managers have exceptional skill. Specifically, let us take the hypothesis HY to be that out of every 20 funds, one fund beats the index 60 percent of the time. In other words, there is a small proportion (5 percent) of "60 percent skilled" funds with the other 19 out of 20 (95 percent of funds) being "49.47 percent skilled." On average, funds have a 50 percent chance of beating the index. Of course, there is no certainty in the world, and it would be foolish to assume that exceptional skill exists with probability 1.00—that is, to assume P(HY) = 1.00. We must consider the alternative hypothesis, HN, that there is no special skill, and each and every fund has a 50/50 chance of beating the market in any one year.
In this case, the evidence is observing a streak for some fund among all funds (say, for argument's sake, the pool is 1,000 funds), with EY the evidence of yes observing a 15-year streak in 40 years and EN the evidence of not observing a 15-year streak. Now we can ask, what does this evidence, observing a streak, tell us about the probability of HY (the world has exceptional managers) versus HN (no managers have exceptional skill)?
We start by calculating the probability of observing a streak in a world with exceptional skill versus no exceptional skill:38


Now we can ask, what is P(HY|EY)? That is, what is the probability of the world having skilled managers, given that we observe at least one fund with a streak of 15 years? Bayes' rule (Equation (2.1)) says that

There are two important lessons to take from this equation. First, Bayes' rule itself tells us nothing about what the prior probabilities should be (although Bayes' original paper tried to address this issue). We may start being highly confident that exceptional skill exists [say P(HY) = 0.90] or very skeptical [P(HY) = 0.10]. We are taking the probability P(HY) as pure belief-type probability: we must use experience or judgment to arrive at it, but it is not based on hard, frequency-type evidence. The second lesson is that Bayes' rule tells us how to apply evidence to our belief-type probabilities to consistently update those probabilities in concert with evidence. In fact, when we apply enough and strong-enough evidence, we will find that divergent prior belief-type probabilities [P(HY) and P(HN)] will converge to the same posterior probabilities [P(HY|EY) and P(HN|EY)].
We can examine exactly how much the probabilities will change with the evidence of a streak. Let us say that I am skeptical that the world has managers with superior skill; my prior belief-type probability for HY, the hypothesis that there are funds with superior skill (60 percent skilled funds), is

Then, applying Bayes' rule (Equation (2.1)) gives

In other words, the evidence of a streak alters my initial (low) probability but not by very much.
Now consider the other extreme, where I strongly believe there are managers with superior skill so that my prior is P(HY) = 0.90. Then applying Bayes' rule gives P(HY|EY) = 0.92, and again my initial assessment is not altered very much. In sum, the evidence of a 15-year streak is not strong evidence in favor of superior manager skill. The streak does not prove (but neither does it disprove) the hypothesis that superior skill exists.
Let us now ask a subtly different question: Say we knew or were convinced for some reason that the world contained some managers with superior skill (we take as a given the hypothesis that 5 percent of the managers are 60 percent skilled funds). Now, what does a 15-year streak for a particular fund tell us about that fund? How does that change our assessment of whether that fund is a 60 percent skilled fund versus a 49.47 percent skilled fund?
In this case, the hypothesis HY is that a particular fund is 60 percent skilled and the evidence is a 15-year streak out of 40 years:


Now we can ask, what is P(HY|EY)? That is, what is the probability that this manager is 60 percent skilled, given that this fund has a streak of at least 15 years? Bayes' rule says that

In other words, the evidence that this fund has a 15-year streak changes our probability that this particular fund is a skilled fund from P(HY) = 0.05 to P(HY|EY) = 0.436. (This result is conditional on the world containing a 5 percent smattering of skilled funds among the large pool of all funds.) We could view this either as a big change (from 5 percent probability to 43.6 percent probability) or as further indication that a 15-year streak is weak evidence of skill because we still have less than a 50/50 chance that this particular manager is skilled.
The Legg Mason Value Trust Fund outperformed for the 15 years up to 2005, but performance during the following years definitively broke the streak; the fund underperformed the S&P 500 for 3 out of the 4 years subsequent to 2005.39 We can use Bayes' Theorem to examine how much this evidence would change our probability that the fund is 60 percent skilled. The hypothesis HY is still that the fund is 60 percent skilled, but now P(HY) = 0.436 and


Bayes' Theorem gives

This evidence drops the probability that the Value Trust Fund is skilled, but not as much as I would have thought.
In conclusion, this example shows how we can use probability theory and Bayes' Theorem to organize our belief-type probabilities and combine them with evidence and experience. It also shows how important it is to systematize and organize our probabilistic thinking.A 15-year streak sounds quite impressive, but upon closer examination, we see that it is not as unusual as we might have thought.40

Risk versus Uncertainty or Ambiguity
The good news is that the rules of probability that apply to frequency-type probability apply equally to belief-type probability. We can use the two interchangeably in calculations and for many purposes can ignore any distinction between them.
Although I argue that we can often ignore any distinction between frequency-type (objective) and belief-type (subjective) probability, many writers argue otherwise. This distinction is usually phrased by contrasting risk (roughly corresponding to frequency-type probability) to uncertainty or ambiguity (where numerical probabilities cannot be assigned, usually corresponding to some form of belief-type or subjective probability). One expression of this view is Lowenstein (2000):
Unlike dice, markets are subject not merely to risk, an arithmetic concept, but also to the broader uncertainty that shadows the future generally. Unfortunately, uncertainty, as opposed to risk, is an indefinite condition, one that does not conform to numerical straitjackets. (p. 235)
Lowenstein is a popular author and not a probabilist or statistician, but the same view is held by many who think carefully and deeply about such issues. For example, Gigerenzer (2002) states it as follows:
In this book, I call an uncertainty a risk when it can be expressed as a number such as a probability or frequency on the basis of empirical data....In situations in which a lack of empirical evidence makes it impossible or undesirable to assign numbers to the possible alternative outcomes, I use the term "uncertainty" instead of "risk." (p. 26)
The distinction between risk and uncertainty is usually attributed to Knight (1921) and often called Knightian uncertainty. It is often argued that uncertainty or ambiguity is inherently distinct from risk in the sense that people behave differently in the face of ambiguity than they do when confronted with computable or known probabilities (risk). It is argued that there is ambiguity aversion separate from risk aversion.
Various paradoxes are said to provide evidence in favor of ambiguity and ambiguity aversion, with probably the best known being the Ellsberg paradox (Ellsberg 1961). I am not convinced by these paradoxes, and I maintain that frequency-type (objective) and belief-type (subjective) probabilities can and should be used interchangeably.
My conclusion that frequency-type and belief-type probabilities can, and indeed should, be used interchangeably is not taken lightly, but on balance, I think we have no other choice in risk management and in our daily lives. The future is uncertain, subject to randomness that is not simply replication of a repeated game. But we have to make decisions, and probability theory is such a useful set of tools that we have to use it. The utility of treating frequency-type and belief-type probabilities as often interchangeable outweighs any problems involved in doing so.
When using belief-type probabilities, however, we must be especially careful. We cannot rely on them in the same way as we can rely on frequency-type probabilities in a game of chance. We must be honest with ourselves that we do not, indeed cannot, always know the probabilities. The de Finetti game and Bayes' rule help keep us honest, in the sense of being both realistic in uncovering our prior (belief-type) probabilities and consistent in updating probabilities in the face of new evidence. The formalism imposed by careful thinking about belief-type probability may appear awkward to begin with, but careful thinking about probability pays immense rewards.

Ellsberg Paradox
Daniel Ellsberg (b. 1931) has the distinction of being far better known for political activities than for his contribution to probability and decision theory. Ellsberg obtained his PhD in economics from Harvard in 1962. In 1961, he published a discussion of a paradox that challenges the foundations of belief-type probability and expected utility theory. In the late 1960s, Ellsberg worked at the RAND Corporation, contributing to a top-secret study of documents regarding affairs associated with the Vietnam War. These documents later came to be known as the Pentagon Papers. Ellsberg photocopied them, and in 1971, they were leaked and first published by the New York Times. At least partially in response to the leaked papers, the Nixon administration created the "White House Plumbers," whose apparent first project was breaking into Ellsberg's psychiatrist's office to try to obtain incriminating information on Ellsberg. The Plumbers' best-known project, however, was the Watergate burglaries.
Ellsberg's 1961 paper discusses a series of thought experiments in which you are asked to bet on draws from various urns. (Although popularized by Ellsberg and commonly known by his name, a version of this paradox was apparently noted by Keynes 1921, par. 315, fn 2.)
The experiment I discuss here concerns two urns, each having 100 balls. For Urn 1, you are told (and allowed to verify if you wish) that there are 100 balls, 50 of which are red and 50 black. For Urn 2, in contrast, you are told only that there are 100 balls, with some mix of red and black (and only red or black); you are not told the exact proportions. For the first part of the experiment, you will draw a single ball from Urn 1 and a single ball from Urn 2 and be paid $10 depending on the selection of red versus black. Before you draw, you must decide which payoff you prefer:

RED = $10 if Red, $0 if Black
BLACK = $0 if Red, $10 if Black

When asked to choose between the two payoffs, most people will be indifferent between red versus black for both the first and the second urn. For Urn 1, we have evidence on the 50/50 split, so we can assign a frequency-type probability of 50 percent to both red and black. For Urn 2, we do not have any frequency-type information, but we also do not have any information that red or black is more likely, and most people seem to set their subjective or belief-type probability at 50/50 (red and black equally likely).
In the second part of the experiment, you will draw a single ball and get paid $10 if red, but you get to choose whether the draw is from Urn 1 or Urn 2. It seems that most people have a preference for Urn 1, the urn with the known 50/50 split. (Remember that this is a thought experiment, so when I say "most people" I mean Ellsberg and colleagues he spoke with, and also myself and colleagues I have spoken with. Nonetheless, the conclusion seems pretty firm. And because this is a thought experiment, you can try this on yourself and friends and colleagues.) The preference for red from Urn 1 seems to establish that people assess red from Urn 1 as more likely than red from Urn 2.
Now we get to the crux of the paradox: The preference for Urn 1 is the same if the payoff is $10 on black, which seems to establish black from Urn 1 as more likely than black from Urn 2. In other words, we seem to have the following:

Red 1 preferred to Red 2 ⇒ Red 1 more likely than Red 2.
Black 1 preferred to Black 2 ⇒ Black 1 more likely than Black 2.

But this is an inconsistency. Red 2 and Black 2 cannot both be less likely because that would imply that the total probability for Urn 2 is less than 1.00. (Try it. For any probabilities for Red 1 and Black 1, the relations just given imply that the total probability for Urn 2 is less than 1.00.)
Ellsberg claimed that this inconsistency argues for "uncertainties that are not risk" and "ambiguity" and that belief-type or subjective probabilities (as for Urn 2) are different in a fundamental way from frequency-type probabilities. Subsequent authors have worked to develop theories of probability and expected utility to explain this paradox (see Epstein 1999; Schmeidler 1989).
There are a few obvious critiques of the paradox. Maybe we simply prefer the easier-to-understand Urn 1, not wanting to waste brain cells on thinking through all implications of the problem. Maybe we are deceit averse, wanting to shy away from Urn 2 in case the experimenter somehow manipulates the red and black balls to our disadvantage. But I think the paradox goes deeper. When I think long and hard about the problem (I make sure I fully explain the problem to myself and reliably assure myself that I, as the experimenter, will not cheat), I still prefer the 50/50 Urn 1.
The resolution of the paradox lies in viewing the Ellsberg experiment in the context of a larger meta-experiment:

 X percent probability of single draw (original Ellsberg experiment)
 1 - X percent probability of repeated draws

Real differences exist between Urn 1 and Urn 2, and Urn 1 is less risky (and thus, preferable) in all cases except the Ellsberg single-draw experiment. It does not take much thinking to realize that repeated draws from Urn 2, where we do not know how many are red or black, is more risky than repeated draws from Urn 1, where we know there are precisely 50 red and 50 black. With Urn 2, I might choose the red payoff but have the bad luck that there are no red and all black. For repeated draws, I am stuck with my initial choice. For a single draw, it does not really matter—because I do not have any prior knowledge, and because I get to choose red or black up front, the urn really does behave like a 50/50 split. (Coleman 2011b discusses the problem in more detail and shows how a mixed distribution for Urn 2 will be more risky for repeated draws than the simple 50/50 distribution of Urn 1.)
So, we have a situation in which for a single draw, Urn 1 and Urn 2 are probabilistically equivalent but for repeated or multiple draws, Urn 1 is preferable. For the meta-experiment, it is only in the special case where X = 100 percent that the two urns are equivalent; whenever X < 100 percent, Urn 1 is preferable. Even a small probability that there will be repeated draws leads to Urn 1 being preferred. So, what would be the rational response: Choose Urn 2, which is equivalent to 1 in the single-draw case but worse in any repeated-draw experiment, or for no extra cost, choose Urn 1? The choice is obvious: As long as there is some nonzero chance that the experiment could involve repeated draws (and psychologically it is hard to ignore such a possibility), we should choose Urn 1.
Stated this way, there is no paradox. From this perspective, preference for Urn 1 is rational and fully consistent with expected utility theory. In summary, I do not find the Ellsberg paradox to be evidence in favor of ambiguity or uncertainty. I do not see the need for ambiguity aversion as a supplement to the standard risk aversion of expected utility theory. Similarly, I do not believe that we need to amend the concept of subjective or belief-type probability.

2.5 The Curse of Overconfidence
Much of this chapter has been concerned with how our human intuition can be fooled by randomness and uncertainty. We have seen that it is easy to generate (random) runs and streaks that seem, intuitively, very nonrandom. Humans, however, crave control over their environment, and we will often impose an illusion of certainty and control over purely random events. It is all too easy, all too tempting, to mistake luck for skill, and the result can be overconfidence in our own abilities. There is a fundamental tension here because confidence in one's abilities is as necessary for successful performance in the financial arena as it is in any area of life, but overconfidence can also breed hubris, complacency, and an inability to recognize and adapt to new circumstances.
Gladwell (2009) wrote an interesting essay discussing the importance of psychology, in particular confidence and overconfidence, in the finance industry and in running an investment bank. He focuses specifically on Jimmy Cayne and the fall of Bear Stearns in 2008 (with interesting digressions to the debacle of Gallipoli). With hindsight, Cayne's words and actions can seem to be the purest hubris. But Gladwell argues, convincingly, that such confidence is a necessary component of running an investment bank. If those running the bank did not have such optimism and confidence, why would any customers or competitors have confidence in the bank? And yet such confidence can be maladaptive.
Both Gladwell and Mlodinow (2008) discuss the work of the psychologist Ellen Langer and our desire to control events. Langer showed that our need to feel in control clouds our perception of random events. In one experiment (Langer 1975), subjects bet against a rival. The rival was arranged to be either dapper or a schnook. Against the schnook, subjects bet more aggressively, even though the game was pure chance and no other conditions were altered. Subjects presumably felt more in control and more confident betting against a nervous, awkward rival than against a confident one, although the probabilities were the same in both cases.
In another experiment (Langer and Roth 1975), Yale undergraduates were asked to predict the results of 30 random coin tosses. When queried afterward, the students behaved as if predicting a random coin toss was a skill that could be improved with practice. Subjects for whom tosses were manipulated to exhibit early streaks (but also so that overall they guessed correctly half the time) rated themselves better at the guessing than other subjects, even though all subjects were correct half the time.
The problem of overconfidence may be the most fundamental and difficult in all of risk management because confidence is necessary for success but overconfidence can lead to disaster. This situation is made even worse by the natural human tendency to forget past bad events. Maybe that is just part of the human psyche; it would be hard to survive if past losses remained forever painful.
I know of no foolproof way to avoid overconfidence. Possibly the most insightful part of Gladwell (2009) is in the closing paragraphs, where he contrasts the bridge-playing expertise of Cayne and others at Bear Stearns with the "open world where one day a calamity can happen that no one had dreamed could happen" (p. 7). This discussion harks back to the distinction between frequency-type versus belief-type probability. Bridge is a game of chance, a repeated game with fixed and unchanging rules to which we can apply the law of large numbers. We may momentarily become overconfident as bridge players, but the repeated game will come back to remind us of the underlying probabilities. The real world, in contrast, is not a repeated game, and the truly unexpected sometimes happens. And most importantly, because the unexpected does not happen frequently, we may become overconfident for long periods before nature comes back to remind us that the unexpected does occur.
2.6 Luck
Luck is the irreducible chanciness of life. Luck cannot be controlled, but it can be managed.
What do I mean by luck versus risk? Risk is the interaction of the uncertainty of future outcomes with the benefits and costs of those outcomes. Risk can be studied and modified. Luck is the irreducible chanciness of life—chanciness that remains even after learning all one can about possible future outcomes, understanding how current conditions and exposures are likely to alter future outcomes, and adjusting current conditions and behavior to optimally control costs and benefits. Some things are determined by luck, and it is a fool's errand to try to totally control luck.
The philosopher Rescher (2001) states it well:
The rational domestication of luck is a desideratum that we can achieve to only a very limited extent. In this respect, the seventeenth-century philosophers of chance were distinctly overoptimistic. For while probability theory is a good guide in matters of gambling, with its predesignated formal structures, it is of limited usefulness as a guide among the greater fluidities of life. The analogy of life with games of chance has its limits, since we do not and cannot effectively play life by fixed rules, a fact that sharply restricts the extent to which we can render luck amenable to rational principles of measurement and calculation. (pp. 138-139)
Rescher's point is that luck is to be managed, not controlled. The question is not whether to take risks—that is inevitable and part of the human condition—but rather to appropriately manage luck and keep the odds on one's side.
The thrust of this chapter has been twofold: Randomness and luck are part of the world, and randomness is often hard to recognize and understand. The success or failure of portfolio managers, trading strategies, and firms depends on randomness and luck, and we need to recognize, live with, and manage that randomness and luck.
In the next chapter, I change gears, moving away from the theory of probability and focusing on the business side of managing risk. The insights and approach to uncertainty discussed in this chapter must be internalized to appropriately manage risk on a day-to-day basis.
Notes
1. Gigerenzer (2002, 26) emphasizes the importance of thinking of risk as both positive and negative.
2. Technically, the distribution F is said to dominate G according to second-order stochastic dominance. For a discussion of stochastic dominance, see the essay by Haim Levy in Eatwell, Milgate, and Newman (1987, The New Palgrave, vol. 4, 500-501) or on the Internet (New School, undated). In practice, distributions F and G rarely exist simultaneously in nature because the price system ensures that they do not. Because virtually anyone would consider G worse than F, the asset with distribution G would have to go down in price—thus ensuring that the expected return (mean) would be higher.
3. If we know the whole distribution, we can apply that to any particular investor's preferences to find the utility of the set of P&L outcomes. Thus, focusing on the full distribution means we can evade the issue of preferences.
4. See, for example, Cramér (1974), sections 15.5 and 15.6. The following comments are appropriate: "All measures of location and dispersion, and of similar properties, are to a large extent arbitrary. This is quite natural, since the properties to be described by such parameters are too vaguely defined to admit of unique measurement by means of a single number. Each measure has advantages and disadvantages of its own, and a measure which renders excellent service in one case may be more or less useless in another" (pp. 181-182).
5. For the S&P 500 Index, the daily standard deviation is roughly 1.2 percent and the average daily return is only 0.03 percent (calculated from Ibbotson Associates data for 1926 to 2007, which show the annualized mean and standard deviation for monthly capital appreciation returns are 7.41 percent and 19.15 percent).
6. See, for example, Kahneman and Tversky (1973) and Tversky and Kahneman (1974).
7. http://nobelprize.org/nobel_prizes/economics/laureates/2002/.
8. See Kahneman, Slovic, and Tversky (1982, 90-98) for the original reference. The present description is a somewhat abbreviated version of that in Mlodinow (2008).
9. See Tversky and Kahneman (1974).
10. Feller (1968, 33) also discusses the problem and gives approximations to the probability that two or more people in a group of size r have the same birthday. For a small r (say, around 10), P[2 or more with same birthday] ≈ r(r - 1)/730. For a larger r (say, 15 or more), P[2 or more with same birthday] ≈ 1 - exp[-r(r - 1)/730]. These work quite well. For r = 23 people, the true probability is 0.507 and the approximation is 0.500, and for r = 56, the true is 0.988 and the approximation is 0.985.
11. See Mlodinow (2008, 175) and Maslin (2006).
12. For a discussion of subrandom sequences, see, for example, Press, Teukolsky, Vetterling, and Flannery (2007, section 7.8).
13. I use simulation to arrive at this answer; I do not know of any simple formula for calculating the probability of such a run.
14. The discussion of results through 2005 follows Mlodinow (2008).
15. If each fund has probability p of outperforming in a year (in our case, p = 0.5), then the probability that one fund has a streak of 15 years is p15 = 0.000031 because performance across years is assumed to be independent and we multiply the probability of independent events to get the joint probability (one of the laws of probability—see Aczel 2004, ch. 4, or Hacking 2001, ch. 6). Thus, the probability that the fund does not have a streak is 1 - p15 = 0.999969. Each fund is independent, so for 1,000 funds, the probability that no fund has a streak is (1 - p15)1,000 = 0.9699 (again, we multiply independent events), which means the probability that at least 1 fund has a streak is 1 - 0.9699 = 0.0301.
16. Mauboussin and Bartholdson (2003, quoted in Mlodinow 2008, 180).
17. I arrive at 33 percent by simulating the probability that a single fund would have a 15-year (or longer) run in 40 years (p = 0.000397) and then calculating the probability that none of 1,000 identical and independent funds would have a 15-year streak [(1 - p15)1,000 = 0.672]. Thus, the probability that at least one fund has a streak is (1 - 0.672 = 0.328). Mlodinow (2008, 181) arrives at a probability of roughly 75 percent. Mlodinow may have assumed a more realistic pool of funds—say, 3,500, which would give a probability of 75 percent for at least one streak. Whether the probability is 33 percent or 75 percent, however, does not matter for the point of the argument because either way the probability is high.
18. As a side note, the performance for the Legg Mason Value Trust since 2005 has been not merely average but abysmal. For the four years from 2006 to 2009, the Value Trust underperformed the S&P 500 three years out of four, and overall from year-end 2005 through year-end 2009, it was down 37.5 percent while the S&P 500 was down roughly 2.7 percent.
19. Note that this section is a digression that can be read independently of the rest of the chapter.
20. Feller (1968, 78). This discussion is taken from the classic text on probability, Feller (1968, sections III.4-III.6).
21. The Monty Hall problem is discussed widely—Mlodinow (2008), Gigerenzer (2002), and Aczel (2004), although under a different formulation. Vos Savant (1996) covers the topic in some depth.
22. These arguments are intended to show why the solution is correct, not as a formal proof of the solution. See Rosenhouse (2009) for a proof of the classical problem, together with a large choice of variations.
23. Hoffman (1998) relates how Paul Erdös, one of the most prolific twentieth-century mathematicians, was only convinced of the solution through a Monte Carlo simulation. This is also the method by which I came to understand that switching is the correct strategy.
24. Rosenhouse (2009) discusses the problem and solutions in detail. It is also covered in Mlodinow (2008) and Gigerenzer (2002).
25. See Rosenhouse (2009, 20).
26. Gladwell's book spawned a counterargument (Adler 2009) in which the author makes the case that first impressions are usually wrong and that one ought to do the hard work of analyzing a situation before making a decision.
27. It might seem odd to include the possibilities WWL and WWW separately because in both cases the final game would not be played. They need to be included, however, because the series sometimes goes to three games (as in WLW). And because the series sometimes goes to three games, we must keep track of all the possible ways it could go to three games and count WWL and WWW as separate possibilities.
28. It may be that the worst team in the league has a probability lower than 40 percent of winning a single game. Nonetheless, the World Series pits the best teams from the American and National Leagues, and these teams will be more closely matched than 60 percent/40 percent. Yet, the analysis shows that there is a reasonable chance (better than 30 percent) that the better team will lose the World Series.
29. According to the binomial distribution with p = probability of success and q = 1 - p = probability of failure, the probability of k failures out of n trials is  where  is the binomial coefficient. For q = 0.01, n = 100, P(k = 0) = 0.366, P(k = 1) = 0.370, P(k = 2) = 0.185, P(k ≥ 3)= 0.079.
30. A die would be another simple and common example of a system to which frequency-type probability would naturally apply. An ideal die would have a one-sixth chance of landing with any particular face up. For an actual die, we could examine the die itself and verify its symmetry, and we could also perform repeated throws to actually measure the frequency for each of the six faces.
31. The number of heads will be approximately normally distributed, N(μ = 50, σ<συπ>2 = 25), so that there will be a 95 percent probability the actual number of heads will be within μ ± 2σ or 50 ± 10.
32. The word subjective is unfortunate. It suggests that this type of probability is somehow inferior to the frequency-type or objective probability. Furthermore, belief-type probability statements can be based on logical relations and evidence that can reasonably be labeled objective; an example is a forecast of rain tomorrow based on the observations that a storm system lies to the west and that weather in the middle northern latitudes usually moves from west to east. Like Hacking (2001), I will generally not use the words objective and subjective probability but rather frequency-type and belief-type probability.
33. See Markowitz (2006). See also Bernstein (2007, 108).
34. This example is modified from the nice explanation in Aczel (2004, 21-24).
35. Discussed in Aczel (2004, ch. 16), Gigerenzer (2002, ch. 4), and Mlodinow (2008, ch. 104). See also Hacking (2001, ch. 7).
36. Gigerenzer (2002) stresses the usefulness of formulating applications of Bayes' rule and conditional probability problems in such a manner. He argues that just as our color constancy system can be fooled by artificial lighting (so that his yellow-green Renault appears blue under artificial sodium lights), our probabilistic intuition can be fooled when presented with problems in a form that our intuition has not been adapted or trained to handle. Gigerenzer's solution is to reformulate problems in natural frequencies rather than bemoan the inadequacy of human intuition. This is an example of how proper presentation and communication of a risk problem can clarify rather than obfuscate the issues.
37. To apply Bayes' rule using Gigerenzer's idea of natural frequencies, we need to know that the prior probability of someone like Mlodinow having HIV is about 1 in 10,000 and that the test's false-positive rate is about 1 in 1,000 (or, its accuracy is 99.9 percent). So for a population of 10,000 test-takers, there would be 1 true positive and roughly 10 false positives, for a total of 11 positive tests. In other words, the probability of having HIV given a positive test would be about 1 in 11, or 9.1 percent. Using the formalism of Bayes' rule, we have P(HY) = 0.0001, P(EY|HN) = 0.001, and let us assume P(EY|HY) = 1.00. Then, P(HY|EY) = (1.00 × 0.0001)/(1.00 × 0.0001 + 0.001 × 0.9999) = 0.091 = 9.1 percent. For the record, Mlodinow's test was a false positive and he was not infected. Also, note that the application of Bayes' rule is very dependent on the assumption that Mlodinow is at low risk of HIV infection. For an individual at high risk (say, with a prior probability of 1 percent rather than 0.01 percent), we would get: P(HY|EY) = (1.00 × 0.01)/(1.00 × 0.01 + 0.001 × 0.99) = 0.910 = 91 percent. Bayes' rule tells us how to update the prior probabilities in the presence of new evidence; it does not tell us what the prior probabilities are.
38. By simulation, the probability that a single 60 percent skilled fund has a 15-year streak in 40 years is 0.005143, versus 0.000348 for a 49.47 percent skilled fund. Thus, P(15-yr run in 40 yrs|HY) = 0.05 × P(15-yr run|0.6 manager] + 0.95 × P(15-yr run|0.4947 manager) = 0.05 × 0.005143 + 0.95 × 0.000348 = 0.000588.
39. As noted in an earlier footnote, for the four years from 2006 to 2009, the Value Trust underperformed the S&P 500 for 2006, 2007, and 2008.
40. I am not arguing here against the existence of special skill as much as I am arguing in favor of a critical approach to the data. Focusing only on Legg Mason Value Trust ignores the fact that there were many other winning funds with track records that were not quite as good. Their existence would (I think, greatly) raise the likelihood that funds with superior skill, not pure luck, exist. This assertion does not change the general observation, however, that "beating the market" is hard.









Chapter 3
Managing Risk
In the previous chapter, I discussed uncertainty, risk, and the theory of probability. Now, I change gears and move from hard science to soft business management because when all is said and done, risk management is about managing risk—about managing people, processes, data, and projects. It is not just elegant quantitative techniques; it is the everyday work of actually managing an organization and the risks it faces. Managing risk requires making the tactical and strategic decisions to control those risks that should be controlled and to exploit those opportunities that should be exploited. Managing profits cannot be separated from managing losses or the prospect of losses. Modern portfolio theory tells us that investment decisions are the result of trading off return versus risk; managing risk is just part of managing returns and profits.
Managing risk must be a core competence for any financial firm. The ability to effectively manage risk is the single most important characteristic separating financial firms that are successful and survive over the long run from firms that are not successful. At successful firms, managing risk always has been and continues to be the responsibility of line managers from the board through the CEO and down to individual trading units or portfolio managers. Managers have always known that this is their role, and good managers take their responsibilities seriously. The only thing that has changed in the past 10 or 20 years is the development of more sophisticated analytical tools to measure and quantify risk. One result has been that the technical skills and knowledge required of line managers have gone up. Good managers have embraced these techniques and exploited them to both manage risk more effectively and make the most of new opportunities. Not all firms and managers, however, have undertaken the human capital and institutional investments necessary to translate the new quantitative tools into effective management.
The value of quantitative tools, however, should not be overemphasized. If there is one paramount criticism of the new risk management paradigm, it is that the industry has focused too much on measurement, neglecting the old-fashioned business of managing the risk. Managing risk requires experience and intuition in addition to quantitative measures. The quantitative tools are invaluable aids that help to formalize and standardize a process that otherwise would be driven by hunches and rules of thumb, but they are no substitute for informed judgment. Risk management is as much about apprenticeship and learning by doing as it is about book learning. Risk management is as much about managing people, processes, and projects as it is about quantitative techniques.
3.1 Manage People
Managing people means thinking carefully about incentives and compensation. Although I do not pretend to have the answers for personnel or incentive structures, I do want to emphasize the importance of compensation and incentive schemes for managing risk and building a robust organization that can withstand the inevitable buffeting by the winds of fortune. Managing risk is always difficult for financial products and financial firms, but the principal-agent issues introduced by the separation of ownership and management substantially complicate the problems for most organizations.
As discussed in Chapter 2, risk involves both the uncertainty of outcomes and the utility of outcomes. The distribution of outcomes is objective in the sense that it can, conceptually at least, be observed and agreed upon by everyone. The utility of outcomes, in contrast, depends on individual preferences and is in essence subjective. The preferences that matter are the preferences of the ultimate owner or beneficiary. Consider an individual investor making his own risk decisions. The problem, although difficult, is conceptually straightforward because the individual is making his own decisions about his own preferences. Although preferences might be difficult to uncover, in this case at least it is only the preferences of the owner (who is also the manager of the risk) that matter.
Now consider instead a publicly traded firm—say, a bank or investment firm. The ultimate beneficiaries are now the shareholders. As a rule, the shareholders do not manage the firm; they instead hire professional managers and delegate the authority and responsibility for managing the risks. The preferences of the shareholders are still the relevant preferences for making decisions about risk, but now it is the managers who make most decisions. The shareholders must ensure that the decisions reflect their preferences, but two difficulties arise here. The first is that the managers may not know the owners' preferences, which is a real and potentially challenging problem, but it is not the crux of the problem. Even if the owners' preferences are known, the second difficulty will intrude: The preferences of the managers will not be the same as those of the shareholders, and the interests of the managers and owners will not be aligned. The owners must design a contract or compensation scheme that rewards managers for acting in accordance with owners' preferences and punishes them for acting contrary to those preferences.
This issue goes by the name of the principal-agent problem in the economics literature.1 The essence of the problem is in addressing the difficulties that arise when a principal hires an agent to perform some actions, the interests (preferences) of the two are not the same, and there is incomplete and asymmetric information so that the principal cannot perfectly monitor the agent's behavior. Employer-employee relations are a prime arena for principal-agent issues, and employment contracts are prime examples of contracts that must address principal-agent problems.
In virtually any employer-employee relationship, there will be some divergence of interests. The principal's interest will be to have some tasks or actions performed so as to maximize the principal's profit or some other objective relevant to the principal. Generally, the agent will have other interests. The agent will have to expend effort and act diligently to perform the actions, which is costly to the agent. In a world of perfect information, no uncertainty, and costless monitoring, the principal-agent problem can be remedied. A contract can be written, for example, that specifies the required level of effort or diligence—rewarding the agent depending on the effort expended or on the observed outcome of the action. In such a world, the interests of the principal and agent can be perfectly aligned.
When there is uncertainty, asymmetric information, and costly monitoring, however, the principal-agent problem comes to the fore and designing a contract to align the interests of principal and agent can be very difficult. A compensation scheme cannot generally be based on the agent's effort because this effort can be observed only by the agent (asymmetric information) or is costly to monitor (costly monitoring). There will be difficulties in basing the compensation scheme on observed outcomes. First, it might be difficult or impossible to effectively measure the outcomes (costly monitoring and asymmetric information). Second, because of uncertainty, the outcome might not reflect the agent's effort; rewarding output may reward lazy but lucky agents while punishing diligent but unlucky agents to such a degree that it provides no incentive for agents to work hard. Furthermore, rewarding individuals based on individual measures of output may destroy incentives for joint effort and lead to free-riding problems.
Risk management usually focuses on the problem of measuring risk and the decisions that flow from that problem—combining the uncertainty of outcomes and the utility of outcomes to arrive at the decisions on how to manage risk. In the real world, an additional layer of complexity exists—making sure that managers (agents) actually implement the appropriate measures, either by ensuring that they have the correct incentives or through constant monitoring and control.
Many types of compensation schemes are used in practice, including fixed versus variable compensation (salaries and bonuses or base and commission), deferred compensation, and granting of share ownership with various types and degrees of vesting. Designing compensation and incentive schemes has to be one of the most difficult and underappreciated, but also one of the most important, aspects of risk management. Substantial effort is devoted to measuring and monitoring risk, but unless those managers who have the information also have the incentives to act in concert with the owners' preferences, such risk measurement is useless.
Incentive and compensation schemes are difficult to design—for good times as well as bad times. During good times, it is easier to keep people happy—there is money and status to distribute—but difficult to design incentives that align the principal's and agent's interests. During bad times, it is harder to make people happy—money and status are often in short supply—and it is consequently difficult to retain good people. It is important to design compensation schemes for both good and bad times and to plan for times when the organization is under stress from both high profits (which breeds hubris and a tendency to ignore risk) and low profits (when everybody leaves).
As mentioned at the beginning of this section, I do not have answers for the puzzles of compensation and incentives. The topic is one, however, that rewards careful thinking. There is clearly no substitute for monitoring and measuring risk, but properly designed incentive schemes can go far toward managing and controlling risks. If the interests of managers throughout the organization can be properly aligned, these managers can move part of the way from being disasters in the waiting that require unrelenting monitoring and control to being allies of the principals in controlling and managing risk.
One final issue that I want to mention is the importance of embedded options and payout asymmetry in both compensation and capital structure. In compensation of traders and portfolio managers there is the well-known "trader's put," in which a trader wins if things go well but loses little if things go badly. The trader receives a large bonus in a good year and is let go, with no claw-back of the bonus, in a bad year. Furthermore, traders can often find another trading position with large upside potential.
For hedge funds, the performance fee is often structured as a percentage of returns above a high-water mark (the high-water mark representing the highest net asset value previously achieved by the fund). A straight fee based on percentage of returns may encourage leverage and risk taking—behavior that can be discouraged by adjusting the fee for the risk taken, as discussed in Coleman and Siegel (1999). The high-water mark is designed (and probably originally intended) to make terms more favorable to the investor but, in fact, acts as a put option on returns. The manager receives fees in good times but after a period of losses will not earn performance fees. The payout becomes asymmetric, with performance fees if things go well but no fee penalty if they go badly (and if things go really badly, the manager may be able to close the fund and start again with a new and lower high-water mark). Thus, a high-water mark may hurt rather than help the investor.
The capital structure of publicly traded companies provides the final and possibly the most interesting example of embedded options. A classic article by Merton (1974) shows how shares of a publicly traded company whose capital structure includes both shares and bonds are equivalent to a call on the value of the company (and the risky bond includes a put option). The call option means that shareholders benefit from increased volatility in the value of the company assets (because the value of a call increases as volatility increases), to the detriment of bondholders. This effect becomes particularly important when the firm value is near the par value of the bonds and the company is thus near default. This way of thinking about share value raises the intriguing possibility that shareholders will have an incentive to take on more risk than desired by debtholders and possibly even more than company employees desire, particularly when a company is near default.
In the end, careful thinking about preferences, incentives, compensation, and principal-agent problems enlightens many of the most difficult issues in risk management—issues that I think we as a profession have only begun to address in a substantive manner.
3.2 Manage Infrastructure—Process, Technology, Data
Process and procedure, and the whole arena of operational process and controls, are critically important. These aspects of management are also vastly underappreciated. Many financial disasters—from large and world-renowned ones such as Barings Bank's collapse of 1995 to unpublicized misfortunes on individual trading desks—are the result of simple operational problems or oversights rather than complex risk management failures. To coin a phrase, processes and procedures are not rocket science; nonetheless, losses in this arena hurt as much as any others, possibly more so because they are so easy to prevent and are so obvious after the fact. From Lleo (2009):
Jorion (2007) drew the following key lesson from financial disasters: Although a single source of risk may create large losses, it is not generally enough to result in an actual disaster. For such an event to occur, several types of risks usually need to interact. Most importantly, the lack of appropriate controls appears to be a determining contributor. Although inadequate controls do not trigger the actual financial loss, they allow the organization to take more risk than necessary and also provide enough time for extreme losses to accumulate. (p. 5)
Technology and Data
Risk management and risk measurement projects are as much about boring data and information technology (IT) infrastructure as about fancy quantitative techniques; after all, if you do not know what you own, it is hard to do any sophisticated analysis. In building or implementing a risk management project, often 80 percent of the effort and investment is in data and IT infrastructure and only 20 percent in sophisticated quantitative techniques.
I cannot overemphasize the importance of data and the IT infrastructure required to store and manipulate the data for risk analytics. For market risk, and credit risk even more, good records of positions and counterparties are critical, and these data must be in a form that can be used. An interest rate swap must be stored and recognized as a swap, not forced into a futures system. The cost and effort required to build, acquire, and maintain the data and IT infrastructure should not be underestimated, but neither should they stand as a significant impediment to implementing a risk management project. Building data and IT infrastructure is, again, not rocket science, and the available IT tools have improved vastly over the years.
Often the best return per dollar invested in risk management projects will be in the basic infrastructure—data, IT, operations, daily reporting, and the people to support these activities. These are not high profile areas of the business but there can be big benefits to getting the basics to run smoothly. There is often a presumption on the part of front-desk traders and senior managers that basic infrastructure is taken care of and done well. In reality, back office, operations, middle office, data, and IT infrastructure are too often starved for resources relative to the sophisticated and high profile quantitative arenas. Until, of course, a failure in the basic infrastructure contributes to a large loss, costing many years of profits.
3.3 Understand the Business
A cardinal rule of managing risk is that managers must understand risk. Managers must understand the risks embedded in the business, and they must understand the financial products that make up the risk. This is a simple and obvious rule but one that is often violated: Do the bank board members and CEO understand interest rate or credit default swaps? And yet these instruments make up a huge portion of the risk of many financial firms. And how often, when a firm runs afoul of some new product, has it turned out that senior managers failed to understand the risks?
Managers, both midlevel and senior, must have a basic understanding of and familiarity with the products they are responsible for. In many cases, this means improving managers' financial literacy. Many financial products (derivatives in particular) are said to be so complex that they can be understood only by rocket scientists using complex models run on supercomputers. It may be true that the detailed pricing of many derivatives requires such models and computer power, but the broad behavior of these same products can often be surprisingly simple, analyzed using simple models and hand calculators. Many in research and trading benefit from the aura and status acquired as keepers of complex models, but a concerted effort must be made to reduce complex products to simple ideas. I do not wish to imply that dumbing down is advisable but rather that improved education for managers is required, together with simple and comprehensible explanations from the experts.
Simple explanations for thinking about and understanding risk are invaluable, even indispensable. In fact, when a simple explanation for the risk of a portfolio does not exist, it can be a sign of trouble—that somewhere along the line, somebody does not understand the product or the risk well enough to explain it simply and clearly. Even worse, it may be a sign that somebody does understand the risks but does not want others to understand.

Interest Rate Swaps and Credit Default Swaps: A Long Digression2
This book is not a text on financial products or derivatives, but in this long digression I discuss two simple examples: interest rate swaps and credit default swaps. The goal is twofold. First, I want to show how the fundamental ideas can be easily presented even for products that are usually considered complex. Second, I want to show how these simple explanations have practical application in understanding what happens in financial markets.
Interest Rate Swaps and LTCM
Interest rate swaps (IRSs) are by now old and well-established financial instruments. Even so, they are often considered complex. In fact, they are very simple. For most purposes, particularly changes in interest rates, an IRS behaves like a bond. Its profit and loss (P&L) has the same sensitivity as a bond to changes in interest rates but with no (or, more precisely, much reduced) credit risk.
I assume that readers have a basic knowledge of how an interest rate swap is structured—that a swap is an agreement between two parties to exchange periodic fixed payments for floating interest rate payments for an agreed period.3 Say that we are considering a four-year swap, receiving $5 annually and paying the floating rate annually.4 The cash flows for the swap look like Panel A of Figure 3.1. One year from now, we receive $5 and pay the floating rate (which is set in the market today). In two years, we receive $5 and pay the appropriate floating rate (the rate that will be set at Year 1). On each payment date, we exchange only the net cash flow, so at Year 1 we would receive $1.50 if today's floating rate were 3.50 percent ($5.00 - $3.50).

Figure 3.1 Swap to Receive $5.00 Annual Fixed (Pay Floating) and Equivalence to Long Fixed Bond, Short Floating Bond

Understanding how to value the swap and what the risk is (that is, how it will move as underlying markets move) is not obvious from Panel A of Figure 3.1. We can use a simple trick, however, to make the valuation and risk clear. Because only net cash flows areexchanged on each payment date, it makes no difference to net overall value if we insert +$100 and -$100 at the end. It does, however, completely alter our view of the swap. Now we can view it as being long a fixed-coupon, four-year 5 percent bond and short a floating-rate bond, as shown in Panel B. Furthermore, a floating-rate bond is always worth $100 today, so we now know that the value of the swap is just the difference between the values of two bonds:

Not only do we know the value; we also know the interest rate risk: the risk of the swap will be exactly the same as the risk of the fixed-coupon bond (because a floating-coupon bond is always at par and has no interest rate risk).5
We thus have a very simple explanation of how any standard IRS will behave—like a bond of the same coupon, maturity, and notional amount. This approach may not be precise enough for trading swaps in today's competitive markets (we are ignoring details about day counts, and so on), but it is more than adequate for understanding the broad outlines of what a swap is and how a swap portfolio works.
We can, in fact, use this straightforward view of swaps to help understand what happened with the fund Long-Term Capital Management6 (LTCM) in 1998. LTCM was a large hedge fund that spectacularly collapsed in September 1998 as a result of market disruptions following Russia's de facto debt default that August. At the beginning of 1998, LTCM's capital stood at $4.67 billion, but by the bailout at the end of September, roughly $4.5 billion of that had been lost. LTCM lost virtually all its capital.
The demise of LTCM is a fascinating story and has been extensively discussed, with the account of Lowenstein (2000) being particularly compelling (also see Jorion [2000] for an account). Many reasons can be given for the collapse, and I do not pretend that the complete explanation is simple, but much insight can be gained when one recognizes the size of the exposure to swaps. Lowenstein (2000, 187) recounts a visit by Federal Reserve and Treasury officials to LTCM's offices on September 20, during which officials received a run-through of LTCM's risk reports. One figure that stood out was LTCM's exposure to U.S. dollar-denominated swaps: $240 million per 15 basis point (bp) moves in swap spreads (the presumed one standard deviation move).
As discussed earlier, receiving fixed on a swap is equivalent to being long a fixed-coupon bond, as regards sensitivity to moves in interest rates. The relevant interest rates are swap rates, not U.S. Treasury or corporate bond rates.7 U.S. swap rates will usually be above U.S. Treasury rates and below low-rated corporate yields, although by exactly how much will vary over time.8 The swap spread—the spread between swap rates and U.S. Treasury rates—will depend on the relative demand for U.S. Treasuries versus U.S. swaps. During a period of high risk aversion, such as during the 1998 Russia crisis, there will generally be an increase in demand for Treasuries as investors flock to a safe haven. This flight to safety will push the swap spread higher.
Whatever the determinants of the swap spread, it is common for traders to take positions with respect to the spread. Going short the spread (to benefit when the normally positive spread narrows or moves closer to zero) means going long swaps or receiving fixed—equivalent to going long a fixed-coupon bond and then going short U.S. Treasuries:

There will be no net exposure to the level of rates because if both Treasury and swap rates go up, the swap position loses but the Treasury position benefits. There will be exposure to the swap spread because if swap rates go down and Treasury rates go up, there will be a profit as both the swap position (like a long bond position) benefits from falling rates and the short U.S. Treasury position benefits from rising Treasury rates.
LTCM's position was such that it benefited to the tune of $240 million for each 15 bp narrowing in U.S. swap spreads, or $16 million per single bp. We can easily calculate how large a notional position in bonds this exposure corresponds to. Ten-year swap rates in September 1998 were about 5.70 percent. Thus, a $1 million notional position in 10-year bonds (equivalent to the fixed side of a 10-year swap) would have had a sensitivity of about $750 per bp.9 This analysis implies that the swap spread position was equivalent to a notional bond position of about $21.3 billion, which was a multiple of LTCM's total capital. Furthermore, the $21.3 billion represented only the U.S. dollar swap spread exposure. There was also exposure to U.K. swap spreads and to other market risk factors.
We can also easily calculate that a 45 bp move in swap spreads would have generated a profit or loss of $720 million. LTCM had estimated that a one-year move of one standard deviation was 15 bps. Three standard deviations would be very unlikely for normally distributed spreads (roughly 0.1 percent probability), but financial variables tend to have fat tails—thus, the possibility of a three standard deviation move should not be ignored. Indeed, from April through the end of August, 10-year U.S. swap spreads moved by almost 50 bps. This move is not so surprising when we consider that the default by Russia triggered a minor financial panic: "The morning's New York Times (August 27, 1998) intoned, 'The market turmoil is being compared to the most painful financial disasters in memory.'...Everyone wanted his money back. Burned by foolish speculation in Russia, investors were rejecting risk in any guise, even reasonable risk."10 Everyone piled into the safe haven of U.S. Treasuries, pushing swap spreads higher.
A loss of $720 million would have been 15 percent of LTCM's beginning-year capital. We have to remember, however, that this analysis accounts only for the exposure to U.S. swap spreads. Including U.K. spreads would increase the number. Furthermore, the swap positions were so large (the U.S. position equivalent to $21.3 billion notional) that they could not be quickly liquidated, meaning that LTCM had no practical choice but to live with the losses. In the end, from January 1998 through the bailout, LTCM suffered losses of $1.6 billion because of swaps.11
This is by no means a full explanation of LTCM's collapse, but it is very instructive to realize that many of LTCM's problems resulted from large, concentrated, directional trades. The swap spread position was a directional bet on the swap spread—that the spread would narrow further from the levels earlier in the year. Instead of narrowing, swap spreads widened dramatically during August and September. LTCM simply lost out on a directional bet.
Swap spreads were one large directional bet, and long-term equity volatility was another.12 Together, swap spreads and equity volatility accounted for $2.9 billion of losses out of a total of $4.5 billion. As Lowenstein says, "It was these two trades that broke the firm" (p. 234). There is much more to understanding LTCM's demise than this simple analysis, including the role of leverage and, importantly, the decisions and human personalities that led to taking such large positions. Lowenstein (2000) and Jorion (2000) cover these in detail, and Lowenstein's book in particular is a fascinating read. Nonetheless, this example shows how a simple, broad-stroke understanding of a portfolio and its risks is invaluable.
Credit Default Swaps and AIG
The market for credit default swaps (CDSs) has grown from nothing in the mid-1990s to a huge market today. CDSs are often portrayed as complex, mysterious, even malevolent, but they are really no more complex or mysterious than a corporate bond. Indeed, a CDS behaves, in almost all respects, like a leveraged or financed floating-rate corporate bond. The equivalence between a CDS and a floating-rate bond is very useful because it means that anyone acquainted with corporate bonds—anyone who understands how and why they behave in the market as they do, how they are valued, and what their risks are—understands the most important aspects of a CDS. In essence, a CDS is no harder (and no easier) to value or understand than the underlying corporate bond.
Once again I assume that readers have a basic knowledge of credit default swaps.13 A CDS is an agreement between two parties to exchange a periodic fixed payment in return for the promise to pay any principal shortfall upon default of a specified bond. Figure 3.2 shows the CDS cash flows over time. The periodic premium payment is agreed up front, and (assuming I sell CDS protection) I receive premiums until the maturity of the CDS or default, whichever occurs first. If there is a default, I must cover the principal value of the bond: I must pay 100 less recovery (the recovery value of the bond). This payment of the principal amount is obviously risky, and because the premiums are paid to me only if there is no default, the premiums are also risky.

Figure 3.2 Timeline of CDS Payments, Sell Protection

The details of CDSs are indeed more difficult to understand than those of many other securities, more difficult than bonds or interest rate swaps, but the equivalence between a CDS and a corporate bond mentioned earlier means that a broad view of how and why CDSs behave as they do is easy to grasp.
To see why a CDS behaves like a floating-rate bond or note (FRN), consider a CDS for which I receive the periodic fixed payments and promise to pay principal loss upon default of some bond or some company. That is, I sell CDS protection, which we will see shortly is the same as buying a financed FRN. Figure 3.2 shows the CDS cash flows: I receive premiums until the maturity or default, and I pay out the principal amount upon default.
Now we can use an elegant trick—in essence, the same as that used for the interest rate swap earlier. With any swap agreement, only net cash flows are exchanged. This means we can insert any arbitrary cash flows we wish so long as the same amount is paid and received at the same time and the net is zero. Let us add and subtract LIBOR14 payments at each premium date and also 100 at CDS maturity but only when there is no default. These LIBOR payments are thus risky. But because they net to zero, they have absolutely no impact on the price or risk of the CDS. Panel A of Figure 3.3 shows the original CDS plus these net zero cash flows. Panel B of Figure 3.3 rearranges these cash flows in a convenient manner:

 An FRN by combining

 The CDS premium and +LIBOR into a risky floating coupon, paid only if there is no default.
 +100 into a risky principal repayment, paid only if there is no default.
 Conversion of the payment of -Recovery into receiving +Recovery, paid only if there is default (note that paying a minus amount is the same as receiving a positive amount).

 A LIBOR floater by combining

 -LIBOR into a risky floating coupon, paid until default or maturity, whichever occurs earlier.
 100 paid at maturity if there is no default.
 100 paid at default if there is default.



Figure 3.3 CDS Payments Plus Offsetting Payments Equal FRN Less LIBOR Floater

In Panel B, the FRN behaves just like a standard floating-rate bond or note (FRN): if no default occurs, then I receive a coupon (LIBOR + Spread) and final principal at maturity, and if default occurs, then I receive the coupon up to default and then recovery. The LIBOR floater in Panel B looks awkward but is actually very simple: it is always worth 100 today. It is a LIBOR floating bond with maturity equal to the date of default or maturity of the CDS: Payments are LIBOR + 100 whether there is a default or not, with the date of the 100 payment being determined by the date of default (or CDS maturity). The timing of the payments may be uncertain, but that does not affect the price, because any bond that pays LIBOR + 100, when discounted at LIBOR (as is done for CDSs), is worth 100 irrespective of maturity (that is, irrespective of when the 100 is paid).
This transformation of cash flows is extraordinarily useful because it tells us virtually everything we want to know about the broad how and why of a CDS.15 Selling CDS protection is the same as owning the bond (leveraged—that is, borrowing the initial purchase price of the bond). The CDS will respond to the credit spread of the underlying bond or underlying company in the same way as the FRN would. This view of a CDS is quite different from the usual explanation of a CDS as an insurance product—that the seller of protection insures the bond upon default. Treating a CDS as an insurance contract is technically correct but profoundly uninformative from a risk management perspective, providing virtually no insight into how and why a CDS behaves as it does. In fact, a corporate bond can be treated as embedding an implicit insurance contract.16 The insurance view of a corporate bond, like the insurance view of a CDS, is technically correct but generally uninformative from a portfolio risk management point of view, which is why corporate bonds are rarely treated as insurance products.
Having a simple and straightforward understanding of a CDS as an FRN can be very powerful for understanding the risk of portfolios and how they might behave. We can, in fact, use this approach to gain a better understanding of what brought AIG Financial Products (FP) to its knees in the subprime debacle of the late 2000s. According to press reports, in 2008, AIG FP had notional CDS exposure to highly rated CDSs of roughly $450 billion to $500 billion, with about $60 billion exposed to subprime mortgages and the balance concentrated in exposure to banks.17 Viewing CDSs as leveraged FRNs has two immediate results. First, it reinforces how large a position $450 billion actually is. Outright purchase of $450 billion of bonds, with exposure concentrated in financial institutions and subprime mortgages, certainly would have attracted the attention of senior executives at AIG (apparently, the CDS positions did not). Even the mere recognition that the CDS position is, for all intents and purposes, $450 billion of bonds with all the attendant risks might have prompted a little more scrutiny.
The second result is that it allows us to easily calculate the risk of $450 billion of CDSs, in terms of how much the value might change as credit spreads change. I am not saying that we can calculate AIG FP's exact exposure, but we can get an order-of-magnitude view of what it probably was. We can do this quite easily using the equivalence between CDSs and FRNs. Most CDSs are five-year maturities, and rates were about 5.5 percent in 2008. A five-year par bond (FRN) with a rate of 5.5 percent has a sensitivity to credit spreads, or credit DV01, of about $435 per basis point for $1 million notional.18 Thus, $450 billion of bonds would have sensitivity to credit spreads of very roughly $200 million per basis point. Once again, this analysis emphasizes how large the position was.
With a risk of $200 million per basis point, a widening of 10 bps in the spread would generate $2 billion of losses. A move of 50 bps would generate roughly $10 billion in losses. A 50 bp move in AAA spreads is large by pre-2008 historical standards, but not unheard of. Unfortunately, from mid-2007 through early 2008, spreads on five-year AAA financial issues rose from about 50 bps to about 150 bps. By the end of 2008, spreads had risen to roughly 400 bps; with a risk of $200 million per basis point, this change in spreads would mean losses of $70 billion.19
The exposure of $200 million is not precise, and the moves in aggregate spreads would not track exactly the spreads that AIG FP was exposed to. Nonetheless, given the size of the exposure and the moves in spreads, it is not hard to understand why AIG FP suffered large losses. AIG FP had a huge, concentrated, directional position in subprime, bank, and other bonds with exposure to the financial sector. AIG FP was betting (whether by intent or accident) that spreads would not widen and that the firm would thus earn the coupon on the CDS. The bet simply went wrong. As with LTCM, there is far more to the story than just a spread position (including, as with LTCM, leverage and the human component that led to the positions), but recognizing the large directional nature of AIG's positions makes the source of the losses easier to understand. It does not completely explain the incident, but it does shed valuable light on it.

3.4 Organizational Structure
It is critically important to address the question of what role and organizational structure are best for risk management and risk measurement. This question is closely tied to corporate governance (and regulatory) issues. I review these issues but do not delve into them in detail. The topic is important and should not be glossed over, but it is outside my particular expertise. Furthermore, there is a substantial literature on corporate governance that readers can access.
Two references are particularly valuable. Crouhy, Galai, and Mark (2001, ch. 3) cover a broad range of issues concerned with risk management in a bank. They start with the importance of defining best practices, in terms of policies, measurement methodologies, and supporting data and infrastructure. They also discuss defining risk management roles and responsibilities, limits, and limit monitoring. Crouhy, Galai, and Mark (2006, ch. 4) focus more on the corporate governance aspect and on defining and devolving authority from the board of directors down through the organization.
I discuss the issues of organizational structure and corporate governance from the perspective of a large publicly traded firm, owned by shareholders whose interests are represented by a board of directors. I assume that the firm has a senior management committee responsible for major strategic decisions. Most or all the discussion that follows could also be translated in an obvious manner to a smaller or privately held firm—for example, by substituting the owner for the board or the CEO for the senior management committee.
I start with the role of the board of directors and senior management, following Crouhy, Galai, and Mark (2006, ch. 4). Starting with the board and senior management has to be correct if we truly believe that managing risk is a central function of a financial firm. Crouhy, Gailai, and Mark (2006) specify the role of the board as understanding and ratifying the business strategy and then overseeing management, holding management accountable. The board is not there to manage the business but rather to clearly define the goals of the business and then hold management accountable for reaching those goals. Although this view runs contrary to the view of a director at a large financial group who claimed that "A board can't be a risk manager" (Guerrera and Larsen 2008), in fact the board must manage risk in the same way it manages profits, audit, or any other aspect of the business—not operational management but understanding, oversight, and strategic governance.
For practical execution of the strategic and oversight roles, a board will often delegate specific responsibility to committees. I will consider as an example an archetypal financial firm with two committees of particular importance for risk—the risk management committee and the audit committee. Not all firms will have both, but the roles and responsibilities described must be met in one form or another.
The risk management committee will have responsibility for ratifying risk policies and procedures and for monitoring the effective implementation of these policies and procedures. As Crouhy, Galai, and Mark (2006) state, the committee "is responsible for independently reviewing the identification, measurement, monitoring, and controlling of credit, market, and liquidity risks, including the adequacy of policy guidelines and systems" (p. 94). One area where I diverge from Crouhy, Galai, and Mark slightly (by degree, not qualitatively) is in the level of delegation or devolution of responsibility. I believe that risk is so central to managing a financial firm that the board should retain primary responsibility for risk. The risk committee is invaluable as a forum for developing expertise and advice, but the board itself should take full responsibility for key strategic risk decisions.
An inherent contradiction exists, however, between the board's responsibility to carry out oversight and strategic governance, on the one hand, and to select truly independent nonexecutive directors, on the other. Critical understanding and insight into the complex risks encountered by financial firms will generally be acquired through experience in the financial industry. Nonexecutive directors from outside the industry will often lack the critical skills and experience to properly hold managers and executives accountable—that is, to ask the right questions and understand the answers. Crouhy, Galai, and Mark (2006, 92) propose an interesting solution, establishing a "risk advisory director." This person would be a member of the board (not necessarily a voting member) specializing in risk. The role would be to support board members in risk committee and audit committee meetings, both informing board members with respect to best-practice risk management policies, procedures, and methodologies and also providing an educational perspective on the risks embedded in the firm's business.
Most large financial firms have an audit committee that is responsible for ensuring the accuracy of the firm's financial and regulatory reporting and also compliance with legal, regulatory, and other key standards. The audit committee has an important role in "providing independent verification for the board on whether the bank is actually doing what it says it is doing" (Crouhy, Galai, and Mark 2006, 91). There is a subtle difference between this role and the role of the risk management committee. The audit committee is rightly concerned with risk processes and procedures. The audit committee focuses more on the quality and integrity of the processes and systems, the risk committee more on the substance.
Crouhy, Galai, and Mark (2006, 95) rightly place responsibility for developing and approving business plans that implement the firm's strategic goals with the firm's senior management. Risk decisions will usually be delegated to the senior risk committee of the firm. Because risk taking is so inextricably linked with profit opportunities, the risk committee must include the firm's CEO and senior heads of business units, in addition to the chief risk officer (CRO), chief financial officer, treasurer, and head of compliance.
Regarding the organizational structure within the firm itself, the standard view is laid out most clearly in Crouhy, Galai, and Mark (2006). A CRO and "risk management group" are established, independent of the business or trading units. The senior risk committee delegates to the CRO responsibility for risk policies, methodologies, and infrastructure. The CRO is "responsible for independent monitoring of limits [and] may order positions reduced for market, credit, or operational concerns" (p. 97).
I have a different view, one that is somewhat at variance with accepted wisdom in the risk management industry. I do believe there must be an independent risk monitoring and risk measuring unit, but I also believe that ultimate authority for risk decisions must remain with the managers making trading decisions. Risk is a core component of trading and portfolio management that cannot be dissociated from managing profits, so the management of risk must remain with the managers of the business units. It must ultimately reside with the CEO and senior management committee and devolve down through the chain of management to individual trading units.
Decisions about cutting positions are rightly the responsibility of those managers with the authority to make trading decisions. To my mind, there is a fundamental conflict in asking a CRO to be responsible for cutting positions without giving that CRO the ultimate authority to make trading decisions. The CRO either has the authority to take real trading decisions, in which case he or she is not independent, or the CRO is independent of trading, in which case he or she cannot have real authority.
This view is at variance with the accepted wisdom that proposes a CRO who is independent and who also has the authority to make trading decisions. I believe that the accepted wisdom embeds an inherent contradiction between independence and authority. I also believe that the accepted wisdom can perilously shift responsibility from managers and may lull managers into a false sense that risk is not their concern because it is being managed elsewhere in the organization.
Nonetheless, independence of risk monitoring and risk measurement is critical. Firms already have a paradigm for this approach in the role that audit and finance units play in measuring and monitoring profits. Nobody would suggest that traders or portfolio managers be responsible for producing the P&L statements of the firm. These are produced by an independent finance unit and subject to careful auditing. Areas throughout the organization rely on this P&L and recognize the importance of having verifiable, independent numbers. Risk should be thought of in the same way—information crucial to the organization that must be independently produced and verifiable.
My view of the organizational structure of a risk group is summarized in Figure 3.4. The center of the figure, the core down the middle, shows the primary responsibility for managing risk.20 Managing P&L and other aspects of the organization devolves from the board of directors to senior management (the CEO and senior management committee) and eventually down to individual trading units and business lines. The remaining key items are as follows:

Figure 3.4 Functions and Responsibilities for Risk and P&L


 Finance unit: Develops valuation policy, ensures integrity of P&L, advises board and senior management on P&L and accounting issues.
 Risk unit: Develops risk policies, develops risk reports, ensures integrity of risk reports, advises board and senior management on risk issues.
 Operations/middle office: Books and settles trades, prepares P&L and risk reports, and delivers P&L and risk reports throughout the organization.

This structure gives primary responsibility for managing risk to the managers who have the authority and responsibility to make decisions. At the same time, it emphasizes the role of the risk unit in designing risk policies and advising all levels of the organization on risk matters, from the board down through individual business units. The responsibility for actually running reports, both P&L and risk reports, is given to the operations/middle office group. Risk and P&L reporting are so closely linked that it makes sense to have one operational group responsible for both, instead of finance producing one set (P&L) and risk producing another (risk).
The board and senior managers should rely on the risk unit for advice and direction, but the board and senior management must take responsibility for being informed and educated about risk. It is also important to understand that the risk unit's role of advising the board and senior management includes the responsibility to alert the board and senior management when there are problems with respect to risk, just as the finance unit would with respect to profits.
One final issue to discuss is the use and implementation of limits. There can be a wide variety of limits. For market risk, limits may consist of restrictions or specification of the authorized business and allowed securities to be traded; VaR limits within individual business units and overall for a portfolio or firm; restrictions on types of positions and maximum size of positions; concentration limits that stop traders from putting all their risk in one instrument or one market; stop-loss limits that act as a safety valve and early warning system when losses start to mount; and inventory age limits that ensure examination of illiquid positions or those with unrecognized losses. For credit risk, limits may involve the allowed number of defaults before a business or portfolio requires special attention or controls on the allowed downward migration of credit quality within a loan or other portfolio. For the overall business, there may be limits on the liquidity exposure taken on by the firm.
Limits are an important way of tying the firm's risk appetite, articulated at the board and senior management level, to strategies and behavior at the trading unit or business unit level. Limits are important at the business planning stage because they force managers to think carefully about the scale and scope of a new business in terms of the level of limits and the risk areas across which limits must be granted. Limits are important for ongoing businesses for two reasons. First, they tie the business activity back to the firm's overall risk appetite and to the decision of how to distribute the risk across business lines. Second, limits force managers to compare periodically (say, daily, weekly, or monthly) the risk actually taken in the business with what was intended.
Crouhy, Galai, and Mark (2006) have a discussion of limits, and Marrison (2002, ch. 11) has a particularly clear discussion of the different types of limits and principles for setting limits.
3.5 Brief Overview of Regulatory Issues
Regulation is important not only because firms must operate within the rules set by regulators but also because banking regulation has been a major driver of innovation and adoption of risk management procedures at many institutions. Two problems, however, make it difficult to provide a complete treatment here. First, it is outside my particular expertise. Second, and more important, the topic is changing rapidly and dramatically; anything written here will be quickly out of date. The response to the global financial crisis of 2008-2009 has already changed the regulatory landscape and will continue to do so for many years to come. I will provide only some background, with references for further exploration.
Many texts cover bank regulation, and although these treatments are not current, they do provide background on the conceptual foundations and history of banking regulation. Crouhy, Galai, and Mark (2006) discuss banking regulation and the Basel Accords in Chapter 3 and mid-2000s legislative requirements in the United States regarding corporate governance (the Sarbanes-Oxley Act of 2002) in Chapter 4. Marrison (2002, ch. 23) also covers banking regulations.
Globally, the Basel Committee on Banking Supervision (BCBS) is the primary multilateral regulatory forum for commercial banking. The committee was established in 1974 by the central bank governors of the Group of Ten (G-10) countries. Although the committee itself does not possess formal supervisory authority, it is composed of representatives from central banks and national banking regulators (such as the Bank of England and the Federal Reserve Board) from 28 countries (as of 2010). The BCBS is often referred to as the "BIS Committee" because the committee meets under the auspices and in the offices of the Bank for International Settlements in Basel, Switzerland. Technically, the BIS and the Basel Committee are separate. The original 1988 BCBS accord, history on the committee, valuable research, and current information can be found at the BIS website.21
The most important regulatory requirement for banks is in regard to capital holdings. Regulatory capital is money that is available for covering unanticipated losses. It acts as a buffer or safety net when losses occur, either because assets fall below the level of liabilities or because assets cannot be liquidated quickly. In the 1980s, global regulatory developments accelerated because of concern about the level and quality of capital held by banks in different jurisdictions, with a particular focus on the low level of available capital held by Japanese banks relative to their lending portfolios. The low capital of Japanese banks was believed to give them an unfair competitive advantage.
Although capital is the most important regulatory requirement, two difficulties arise in defining regulatory capital. The first is deciding what level of capital is sufficient. The second is defining what actually counts as capital. Regarding the appropriate level of capital, the problem is determining how much a bank might lose in adverse circumstances, which, in turn, depends on determining the type and amount of assets a bank holds. Neither of these problems is easy to solve, and the issue is compounded by the necessity to have a set of standards that are relatively straightforward and that can be applied equitably across many jurisdictions using standardized accounting measures that are available in all countries.
Early global standards regarding assets were simple. Bank assets were put into broad risk categories, providing guidance as to the amount of capital that had to be reserved against the possibility that the asset would be impaired. Some assets were counted at 100 percent of face value (for example, a loan to a private company, which was considered to be at risk for the whole of the loan amount), and others were given a lower risk weighting (for example, zero percent for cash because cash has no credit risk and is immediately available or 50 percent for housing mortgages). All assets were added up (taking the appropriate risk weighting into account), and these were the bank's total risk-weighted assets. Banks were then required to hold capital equal to a percentage of the risk-weighted assets.
Defining the capital is where the second difficulty arises because defining exactly what counts as capital, and how good that capital is, can be hard. It is widely accepted that equity and reserves are the highest quality form of capital. Equity and reserves—investment in the business provided by outside investors or retained earnings that will disappear in the case of losses—clearly provide a buffer against losses. Other sources of capital—say, undeclared profits—may not be available to cover losses in the same manner and thus may not provide as good a buffer.
Much of the development of global regulation since the 1980s has focused on these three aspects: first, which assets contribute how much to risk-weighted assets; second, what is the appropriate capital ratio; and, third, what counts as capital.
Originally, only the credit risk of assets was taken into account, with no inclusion of market risk (price risk from sources other than default, such as the overall movement of interest rates). New standards published in 1996 and implemented in 1998 sought to include market risk. The rules for risk weighting of assets, however, were still quite crude. The so-called Basel II rules published in 2004 sought to update capital adequacy standards by providing more flexibility but also more precision in the ways that the total risk of assets and total capital are calculated. The details are less important than recognizing that there has been a process for trying to improve how capital requirements are calculated.
The global financial crisis of 2008-2009 highlighted deficiencies in the global regulatory framework, and regulators have responded with Basel III. The process started with a broad framework published in September 2009 and has continued through 2011. Focus has expanded beyond bank-level regulation (setting bank-level capital requirements, for example) to managing system-wide risks, so-called macroprudential regulation.
3.6 Managing the Unanticipated
The ultimate goal for risk management is to build a robust yet flexible organization and set of processes. We need to recognize that quantitative risk measurement tools often fail to capture just those unanticipated events that pose the most risk to an organization. The art of risk management is in building a culture and organization that can respond to and withstand these unanticipated events.
Managing risk for crises, tail events, or disasters requires combining all types of risk—market risk, credit risk, operational risk, liquidity risk, and others. Generally, crises or disasters result from the confluence of multiple events and causes. Examples are the collapse of Barings in 1995 (and also the same firm's collapse in 1890) and the Société Générale trading loss in January 2008.
Risk management is about managing all types of risk together—building a flexible and robust process and organization. The organization must have the flexibility to identify and respond to risks that were not important or recognized in the past and the robustness to withstand unforeseen circumstances. Importantly, it also must incorporate the ability to capitalize on new opportunities.
Examining risk and risk management in other arenas can provide useful insights and comparisons: insight into the difference between measuring and managing risk and comparison with methods for managing risk. Consider the risks in ski mountaineering or backcountry skiing, of which there are many. There is the risk of injury in the wilderness as well as the risk of encountering a crevasse, icefall, or rockfall—as with any mountaineering—but one of the primary risks is exposure to avalanches. Avalanches are catastrophic events that are virtually impossible to forecast with precision or detail.
Ski mountaineering risks and rewards have many parallels with financial risks and rewards. Participating in the financial markets can be rewarding and lucrative; ski mountaineering can be highly enjoyable, combining the challenge of climbing big mountains with the thrill of downhill skiing—all in a beautiful wilderness environment. Financial markets are difficult to predict, and it can be all too easy to take on exposure that suddenly turns bad and leads to ruinous losses; avalanches are also hard to predict, and it is all too easy to stray onto avalanche terrain and trigger a deadly slide.
Managing avalanche risk has a few basic components, and these components have close parallels in managing financial risk:

 Learning about avalanches in general—When and how do they occur?22 The analogy in the financial world would be gaining expertise in a new financial market, product, or activity before jumping in.
 Learning about specific conditions on a particular day and basing decisions on this information—First, is today a high or low avalanche risk day? Then, using this information combined with one's own or the group's risk tolerance, one must decide whether to go out. In financial risk management, this component would be analogous to learning the specific exposures in the portfolio and then deciding whether to continue, expand, or contract the activity.
 Creating damage control strategies—What processes and procedures will mitigate the consequences of disaster when and if it strikes? For example, backcountry skiers should go in a group with every member carrying the tools for group self-rescue—a beacon, probe, and shovel. An avalanche beacon is a small radio transceiver that can be used by group members who are not buried to locate a buried companion, and the probe and shovel are necessary to dig the companion out. A beacon reduces the consequences of being caught and buried by an avalanche: Having a beacon gives a reasonable chance, maybe 50 to 80 percent, of being recovered alive; without a beacon, the chance is effectively zero. Also, safe travel rituals can minimize the effect of an avalanche if it does occur. These damage control strategies are the final component of managing avalanche risk. For financial risk management, this component is analogous to building a robust and flexible organization that can effectively respond to unexpected shocks.

The comparison with backcountry travel in avalanche terrain highlights some important issues that carry over to financial risk management. First is the importance of knowledge and attention to quantitative measurement. Veteran backcountry skiers spend time and effort learning about general and specific conditions and pay considerable attention to quantitative details on weather, snowpack, and so forth. (Those who do not take the time to do so tend not to grow into veterans.) Managers in the financial industry should also spend time and effort to learn quantitative techniques and then use the information acquired with those tools.
Second is the importance of using the knowledge to make specific decisions, combining quantitative knowledge with experience, judgment, and people skills. In almost all avalanche accidents, the avalanche is triggered by the victim or a member of her party. Avalanche accidents usually result from explicit or implicit decisions made by skiers. Decision making requires skill and judgment and the management of one's own and others' emotions and behavior. Group dynamics are one of the most important issues in backcountry decision making. The same is true in managing financial risk. Quantitative measurement is valuable but must be put to good use in making informed decisions. Financial accidents generally do not simply occur but result from implicit or explicit decisions made by managers. Managers must combine the quantitative information and knowledge with experience, judgment, and people skills.
Third, both avalanches and financial accidents or crises are tail events—that is, they happen rarely and the exact timing, size, and location cannot be predicted with any degree of certainty. Nonetheless, the conditions that produce events and the distribution of events are amenable to study. One can say with some confidence that certain situations are more likely to generate an event than others. (A 38-degree slope the day after a two-foot snowfall is likely to avalanche, and for financial events, a firm with $100 million of S&P 500 exposure is more likely to have severe losses than a firm with $10 million of less-risky 10-year bonds.)
Finally, there is an apparent paradox that appears in dealing with both avalanches and financial accidents: With better measurement and management of risk, objective exposure may actually increase. As skiers acquire more skill and tools to manage avalanche risk, they often take on more objective exposure. The analogy in the financial arena is that a firm that is better able to measure and manage the risks it faces may take on greater objective exposure, undertaking trades and activities that it would shy away from undertaking in the absence of such tools and skills.
Upon further consideration, however, this is not paradoxical at all. A skier without knowledge or damage control strategies should take little objective exposure; he should go out only on low-risk days and then only on moderate slopes. Doing so is safe but not very much fun because steep slopes in fresh powder are the most exciting. With knowledge and damage control strategies, a skier will take more objective exposure—go out more often, in higher risk conditions, and on steeper slopes. Going out in higher risk conditions and on steeper slopes means taking on more objective danger, but with proper knowledge, experience, recovery tools, and decision making, the skier can reduce the risk of getting caught in an avalanche or other adverse situations and also reduce the consequences if he does get caught. Most important, the steeper slopes and better snow conditions mean better skiing and a big increase in utility, and with proper management of the risks, it can be accomplished without a disproportionate increase in adverse consequences.
Similarly, a financial firm that can better measure, control, and respond to risks may be able to undertake activities that have both greater profit potential and greater objective exposure without facing a disproportionate increase in the probability of losses.
Investment management always trades off risk and return. Managing risk is not minimizing risk but rather managing the trade-off between risk and return. Good risk management allows the following possibilities:

 Same return with lower risk
 Higher return with same risk

Generally, the result will be some of both—higher return and lower risk. But in some situations, the objective exposure increases. For a financial firm, internal management of exposures might be improved in such a way that larger positions could be taken on with the same probability of loss (more exposure leading to the same risk). This might come about, say, by a more timely reporting of positions and exposures so that better information on portfolio exposures is made available, allowing better management of portfolio diversification. The result would be a decrease in risk in the sense of the likelihood of loss or the impact of losses on the firm but an increase in risk in the sense of larger individual positions and larger profit potential.
This increase in exposure with increased risk management sophistication should not really be surprising. It is simply part of the realization that managing risk goes hand in hand with managing profits and returns. Risk management is not about minimizing risk but, rather, about optimizing the trade-off between risk and return.
Avalanches and financial accidents differ, however, in two important respects. First is the frequency of events. Avalanches occur frequently—many, many times during a season—so that veteran backcountry travelers (those who know enough and wish to survive) are constantly reminded that avalanches do occur. In contrast, severe financial events are spaced years apart; individual and collective memory thus fades, leading to complacency and denial.
Second is the asymmetry of payoffs. The penalty for a mistake in avalanche terrain is injury or death; the penalty in financial markets is losing one's job. The reward on the upside in financial markets can be quite high, so the asymmetry—substantial reward and modest penalty—creates incentive problems.
Maybe the most important lesson to learn from comparing financial risk with avalanche risk is the importance of the "human factor": the confluence of emotion, group dynamics, difficult decision making under uncertainty, and other factors that we humans are always subject to. The final and most important chapter in the popular avalanche text Staying Alive in Avalanche Terrain (Tremper 2008) is simply titled "The Human Factor." In investigating accident after accident, avalanche professionals have found that human decision making was critical: victims either did not notice vital clues or, as is often the case, ignored important flags.
Tremper explains:
There are two kinds of avalanche accidents. First, an estimated two-thirds of fatalities are caused by simple ignorance, and through education, ignorance is relatively easy to cure. The second kind of accident is the subject of this chapter—when the victim(s) knew about the hazard but proceeded anyway. They either simply didn't notice the problem, or more commonly, they overestimated their ability to deal with it.... Smart people regularly do stupid things. (p. 279)
Exactly the same holds for financial accidents and disasters. Ignorance is relatively easy to cure. The goal of quantitative risk measurement, and the subject of the balance of this book, is to educate and inform: to cure ignorance. Ignorance may be caused by a lack of understanding and education, and it is also caused by a lack of information and data—the inability to measure what is happening in a firm. Risk measurement is aimed at addressing these problems. As such, risk measurement has huge benefits. The fact that two-thirds of avalanche fatalities are the result of ignorance probably carries over to the financial arena: Many financial accidents (as we will see in Chapter 4) result from simple mistakes, lack of knowledge, misinformation, or lack of data—in short, financial ignorance that can be cured.
But, as with avalanches, there is a second kind of financial accident—those that are the result of the human factor. Making decisions under uncertainty is hard. Thinking about uncertainty is difficult. Group dynamics, ego, and outside pressures all conspire to cloud our judgment. To paraphrase Tremper, we should be able to practice evidence-based decision making and critically analyze the facts. We should arrive at the right decision automatically if we just have enough information. In reality, it often works out otherwise. Information, education, data—alone these are not sufficient, which brings us back to risk management. Risk management is managing people, managing process, managing data. It is also about managing ourselves—managing our ego, our arrogance, our stubbornness, our mistakes. It is not about fancy quantitative techniques but about making good decisions in the face of uncertainty, scanty information, and competing demands.
Tremper's chapter on the human factor has interesting ideas, many taken from other areas that deal with risky decision making. One point is the importance of regular and accurate feedback, which is relatively easy for avalanches because avalanches occur regularly and publicly. It is more difficult for financial disasters because they occur less frequently and less publicly. Nonetheless, feedback is important and reminds us that things can and do go wrong. Examples of financial disasters can help us be a little more humble in the face of events we cannot control.
A second area Tremper focuses on is the mental shortcuts or heuristics that we often use in making decisions and how these can lead us astray. This point is related to the issue of heuristics and cognitive biases in probabilistic thinking discussed in Chapter 2 of this text. The heuristics discussed in Chapter 2 are related more particularly to the assessment of probabilities, whereas the heuristics here can better be thought of as decision-making shortcuts that often lead us toward errors.
The most important of these heuristics, which carry over naturally to financial risk taking, are as follows:

 Familiarity: We feel more comfortable with what is familiar, which can bias our decision making even in the face of objective evidence. This tendency is particularly a problem when disasters occur infrequently because we can become lulled into thinking that because nothing bad has happened yet, it is unlikely that it will. Tremper points out that snow is stable about 95 percent of the time. If we ski a particular slope regularly, it will feel familiar, but we probably have not seen it when it is cranky. The slope will feel familiar, we will feel that we know it well, but that does not make it any less dangerous.
 Commitment: When we are committed to a goal, it is hard to change in the presence of new evidence; indeed, it is sometimes even hard to recognize that there is new evidence. Success in finance requires dedication and perseverance, commitment to goals, and optimism. But commitment can also blind us to changing circumstances. The balance between persevering to achieve existing goals and responding to changing circumstances is difficult.
 Social proof or the herding instinct: We look to others for clues to appropriate behavior and tend to follow a crowd. This phenomenon has two components. The first is related to the problem of familiarity just discussed. We often look to the experience of others to judge the safety and profitability of unknown activities. When others are doing something and not suffering untoward consequences, we gain confidence that it is safe, sometimes even against our better judgment. The second component is the pressure not to be left behind. When everyone else is making money, it is hard to resist, even if one should know better. Isaac Newton offers a famous example: He invested relatively early in the South Sea Bubble but sold out (on April 20, 1720, at a profit), stating that he "can calculate the motions of the heavenly bodies, but not the madness of people." Unfortunately, he was subsequently caught in the mania during the summer and lost far more than his original profit.23
 Belief and belief inertia: We often miss evidence that is contrary to our beliefs, and our beliefs change slowly in response to new evidence. This point is best summed up by a quote from Josh Billings: "It ain't so much the things we don't know that get us into trouble. It's the things we know that just ain't so."

Unfortunately, decision making is hard. It is hard whether the decisions involve avalanches, medical diagnoses, or risk management in a financial firm. There is no way to avoid this problem. Facts, education, and careful thinking are all necessary for good decision making, but unfortunately, they are not sufficient.
3.7 Conclusion
Quantitative risk measurement as discussed in this book must take its place as a component of standard business practice—a day-in-day-out activity rather than esoteric and left to a coterie of rocket scientists. Risk management must be the responsibility of anyone who contributes to the profit of the firm. Risk tools, good processes, infrastructure, all of these add to prudent business management. In this sense, quantitative risk measurement should be treated just like accounting or market research—an activity and set of tools integral to managing the business.
We need to recognize that managing risk, like managing any aspect of business, is hard. There are no easy answers. Nonetheless I will share one last thought. The task of managing risk is made easier by having a well-planned strategy. A good risk management strategy is simple to state:

 Learn about the risks in general; learn about the business and the people
 Learn about specific exposures and risks; learn about the details of the portfolio
 Manage people, process, organization; focus on group dynamics, the human factor
 Implement damage control strategies to minimize the impact when and if disaster strikes

The problem, of course, is that this strategy may be easy to state but it is fiendishly difficult to implement.
Notes
1. See Stiglitz in Eatwell, Milgate, and Newman (1987, The New Palgrave, vol. 3, 966-971 and references therein, including contributions by Ross 1973; Mirrlees 1974, 1976; and Stiglitz 1974, 1975). The problem is, of course, much older, with an entry in the original Palgrave's Dictionary of Economics (1894-1899) by J. E. C. Munro.
2. Note that this section is a digression that can be read independently of the rest of the chapter.
3. See Coleman (1998b) for a complete discussion.
4. Standard swaps in U.S. dollars involve semiannual payments on the fixed side and quarterly on the floating side, but I use annual payments here just to make the diagrams easier.
5. The exact equivalence between the swap and the net of the fixed coupon bond less the floating bond holds only for the instant before the first floating coupon is set and ignores any differences in day counts or other technical details. Furthermore, there will be some (although small) credit risk embedded in the swap because of counterparty exposure. I ignore these issues for now because they do not matter for understanding the major component of the risk—the change in value with interest rates.
6. Commonly referred to by the name of the management company, Long-Term Capital Management (LTCM).
7. It may sound circular to say U.S. swaps depend on U.S. swap rates, but it is no more so than saying U.S. Treasuries depend on U.S. Treasury rates.
8. Before 2008, I would have said that swap rates are always above Treasury rates, but since November 2008, 30-year swap rates have remained consistently below Treasury rates (with spreads as wide as -40 bps). This is generally thought to be the result of disruption in the repurchase agreement market and low risk appetite among dealers, combined with high demand from corporate customers to receive fixed payments. The combination has put downward pressure on swap rates relative to Treasury rates.
9. See Coleman (1998) for a discussion of bond and swap sensitivity, or DV01.
10. Lowenstein (2000, 153-154).
11. Lowenstein (2000, 234).
12. According to Lowenstein (2000, 126), LTCM had positions equivalent to roughly $40 million per volatility point in both U.S. and European stock markets. (A volatility point is, say, a move from 20 to 21 in implied volatility. An example of an implied volatility index is the VIX index of U.S. stock market volatility.) Implied volatility for such options rose from roughly 20 percent to 35 percent (from early 1998 to September of that year), implying roughly $1.2 billion in losses. The actual losses from swaps were about $1.6 billion and from equity volatility, about $1.3 billion (Lowenstein 2000, 234).
13. See Coleman (2009) for a complete discussion.
14. LIBOR is the London Interbank Offered Rate, a basic, short-term interest rate.
15. The equivalence is not exact when we consider FRNs that actually trade in the market. The technical issue revolves around payment of accrued interest upon default (see Coleman 2009). Although it may not be good enough for trading in the markets, the equivalence is more than satisfactory for our purposes.
16. See Coleman (2009) for a discussion and also the mention by Stiglitz in Eatwell, Milgate, and Newman (1987, The New Palgrave, vol. 3, 967).
17. The Economist ("AIG's Rescue: Size Matters" 2008) reported June 2008 notional exposure of $441 billion, of which $58 billion was exposed to subprime securities and $307 billion exposed to "instruments owned by banks in America and Europe and designed to guarantee the banks' asset quality." Bloomberg (Holm and Popper 2009) reported that AIG FP "provided guarantees on more than $500 billion of assets at the end of 2007, including $61.4 billion in securities tied to subprime mortgages." The Financial Times (Felsted and Guerrera 2008) reported that "based on mid-2007 figures, AIG had $465 billion in super-senior credit default swaps."
18. The interest rate risk of an FRN is close to zero because coupons change with the level of rates. The credit spread risk of an FRN will be roughly the same as the spread risk of a fixed-rate bond (technically, a fixed-rate bond with coupons fixed at the forward floating rate resets). For a fixed-rate bond, the spread risk and the interest rate risk will be close to the same. In other words, to find the credit spread risk of an FRN, we simply need to calculate the interest rate risk of a fixed-coupon bond with its coupon roughly equal to the average floating coupon, which will be the fixed coupon of a par bond with the same maturity.
19. Spreads went back down to roughly 250 bps by early 2010 (figures from Bloomberg). Not all of AIG's positions would have been five years, nor would they all have been financials, but this analysis gives an order-of-magnitude estimate for the kinds of spread movements seen during this period.
20. This organizational layout differs from, for example, Crouhy, Galai, and Mark (2006, Fig. 4.2) in emphasizing the central role for the board and senior management in monitoring and enforcing risk guidelines, with the risk unit playing a supporting role in ensuring integrity of risk reporting, developing risk policy, advising, and so on.
21. See www.bis.org/bcbs.
22. A common problem for beginner backcountry skiers is ignorance of the risks they are taking. One day, there may be little risk from avalanche and another day, great exposure, but in neither case does the beginner even know that she is exposed.
23. See Kindleberger (1989, 38).









Chapter 4
Financial Risk Events
Stories of financial disasters hold a certain unseemly interest, even providing an element of schadenfreude for those in the financial markets. Nonetheless, there are real and substantive benefits to telling and hearing stories of financial disaster. First is the value of regular feedback on the size, impact, and frequency of financial incidents. This feedback helps to remind us that things can go badly; importantly, it can remind us during good times, when we tend to forget past disasters and think that nothing bad can possibly happen. This effect helps protect against what Andrew Haldane, head of financial stability at the Bank of England, has described as "disaster myopia": the tendency for the memory of disasters to fade with time.1 It is the "regular accurate feedback" that Tremper recommends as necessary for good avalanche decision making. It also serves "pour encourager les autres"—to encourage those who have not suffered disaster to behave responsibly.2
The second benefit is very practical: learning how and why disasters occur. We learn through mistakes, but mistakes are costly. In finance, a mistake can lead to losing a job or bankruptcy; in avalanches and climbing, a mistake can lead to injury or death. As Mary Yates, the widow of a professional avalanche forecaster, said, "We are imperfect beings. No matter what you know or how you operate 95 percent of your life, you're not a perfect person. Sometimes these imperfections have big consequences."3 Learning from mistakes can help you identify when and how to make better decisions, and studying others' mistakes can reduce the cost of learning. I think this is an important reason why avalanche accident reports are one of the most popular sections of avalanche websites and why the American Alpine Club's annual Accidents in North American Mountaineering is perennially popular. Yes, there is a voyeuristic appeal, but reviewing others' mistakes imparts invaluable lessons on what to do and what not to do at far lower cost than making the mistakes oneself.
4.1 Systemic versus Idiosyncratic Risk
As discussed in Chapter 1, an important distinction exists between idiosyncratic risk and systemic risk. Idiosyncratic risk arises from within a firm and is generally under the control of the firm and its managers. Systemic risk is shared across firms and is often the result of misplaced government intervention, inappropriate economic policies, or misaligned macroeconomic incentives.
The distinction between idiosyncratic and systemic risks is important because in the aftermath of a systemic crisis, such as that of 2007-2009, they often become conflated in discussions of the crisis. Overall, this book focuses on idiosyncratic risk, but this chapter discusses examples of both idiosyncratic and systemic risk. We will see that systemic risk has been and continues to be a feature of banking and finance for both developed and developing economies. Importantly, the costs of systemic events dwarf those of idiosyncratic events by orders of magnitude. From a societal and macroeconomic perspective, systemic risk events are by far the more important.
The distinction between idiosyncratic and systemic disasters is also important because the sources and solutions for the two are quite different. The tools and techniques in this book are directed toward measuring, managing, and mitigating idiosyncratic risk but are largely ineffective against systemic risk. Identifying and measuring systemic risk resides more in the realm of macroeconomics than in quantitative finance. An analogy might be useful. Learning to swim is an effective individual strategy to mitigate drowning risk for someone at the town pool or visiting the beach. But for someone on the Titanic, the ability to swim was useful but not sufficient. A systemic solution including monitoring iceberg flows, having an adequate number of lifeboats and life belts on the ship, and arranging rescue by nearby ships was necessary (but sadly missing for the Titanic). Similarly, when macroeconomic imbalances alter costs, rewards, and incentives, an individual firm's risk management actions will not solve the macroeconomic problems.4
4.2 Idiosyncratic Financial Events
Financial and trading disasters are often discussed under the rubric "rogue trading." Like many myths, this one contains some truth, but only partial truth. We will see, through examining a variety of events, that many financial disasters are not characterized by rogue trading. Trading disasters occur for a variety of reasons. Sometimes the cause is a rogue trader, as in the case of Barings Bank's 1995 collapse or AIB/Allfirst Financial's losses, but many events have resulted from legitimate trading activity gone wrong or a commercial or hedging activity that developed into outright speculation.
Table 4.1 shows a list of financial events over the years, focusing on events resulting from losses caused by trading in financial markets. It does not cover incidents that are primarily fraudulent rather than trading related, so it does not include Bernard Madoff's fraud. The list is long and, from my experience, reasonably comprehensive regarding the types of financial disasters, but it is not complete. The list clearly does not include events that are not publicly reported, and many fund managers, family trusts, and hedge funds are secretive and loath to reveal losses. For present purposes, Table 4.1 is sufficient; it both shows the scope of losses and includes losses from a wide variety of sources.
Table 4.1 Trading Losses.


Table 4.1 includes few entries relating to the 2008-2009 crisis, and for this reason, it may seem out of date. In fact, the absence of recent events is intentional because Table 4.1 is intended to focus on idiosyncratic trading disasters and not systemic or macroeconomic financial crises. There have been huge losses across the global financial system relating to the recent financial crisis, but these losses are generally associated with the systemic financial crisis and are not purely idiosyncratic risk events. To focus more clearly on purely idiosyncratic events, Table 4.1 does not include most of the recent events. I return to the costs of systemic crises later in this chapter.
Before turning to the table itself, caveats regarding the quoted loss amounts are necessary. These are estimates, often provided by the firm that suffered the loss and after a malefactor has left. Reconstructing trading activity after the fact is always difficult and is sometimes open to different interpretations. Even for simple exchange-traded instruments, it is surprisingly difficult, and financial disasters often involve complex over-the-counter (OTC) instruments for which pricing is hard, compounded with fraud and intentionally concealed prices and trades. Different accounting and mark-to-market standards across jurisdictions mean that different events may have different standards applied. Sometimes the loss that is publicly reported includes restatements for prior incorrectly reported profits rather than simply the economic loss from trading.5 Finally, a firm and the managers that have suffered a loss may have both the motivation and the opportunity to overstate or understate the loss, saying it is larger than it really is to make predecessors look foolish or venal and to flatter future results or smaller than it really is to minimize the culpability of incumbent managers and the damage to the firm.
One final issue regarding the amounts in Table 4.1 needs to be discussed. A dollar lost in 1974 would be equivalent to more than 1 dollar today. Inflation is an obvious factor; a dollar in 1974 could buy more goods or services than it can today. There is also a more subtle effect. The market and the economy have grown over time so that a dollar in 1974, even after adjustment for ordinary (consumer price) inflation, represented a larger proportion of the total market or the total economy; a dollar could buy a larger proportion of the total goods and services produced. Table 4.1 shows both an adjustment in the nominal amounts for inflation (using the U.S. consumer price index [CPI]) and a rough adjustment for the size of the economy using U.S. nominal gross domestic product (GDP) growth. This latter adjustment is only approximate but gives a better idea of the relative importance of losses in different years than one would get by adjusting for inflation alone.6
Thus, Table 4.1 shows the events, with the original currency amount, the original converted to U.S. dollars (at the average FX rate for the approximate year of loss), the U.S. dollar amount in 2007 dollars, and the U.S. dollar amount adjusted so that it is proportionate to 2007 U.S. nominal GDP (that is, adjusted for changes in both inflation and, roughly, the size of the economy). The events are sorted by the size of the loss relative to 2007 nominal GDP.
Categorization and Discussion of Losses
Table 4.1 is interesting in itself and highlights the importance of financial disasters over the years. The name Herstatt, for example, has entered the language as a particular form of cross-currency settlement risk—that which results from differing times for currency transfers.7
We can, however, do more than simply admire the size of the losses in Table 4.1. We can use the events to understand more about the sources and circumstances of financial disasters and losses. I have attempted to provide additional information on each event, shown in Table 4.2, concerning

 Whether the event involved fraud.
 If there was fraud, whether it primarily involved fraudulent trading—that is, actively hiding trades from supervisors or accountants, creating false trading entries, and so on. I mean this to be distinct from simply trading in excess of limits, which often involves taking larger positions than authorized but not actively hiding that fact.
 If there was fraud, whether it was primarily to hide losses that had originated from sources other than fraud. An example is Codelco, where a computer entry led to a wrong-way-around trade that lost $30 million. Subsequent fraudulent trading appears to have been an attempt to make back the original loss.
 Whether the underlying company or business was involved in (primarily) banking, finance, or investment activity.
 Whether the event involved legitimate trading, hedging, or commercial activity that went wrong in some way. For example, Amaranth Advisors' losses in natural gas futures trading were a result of Amaranth's legitimate business activity, even if one might argue, at least in retrospect, that the size and exact form of the position taking may have been foolish. As another example, Aracruz Celulose was a Brazilian pulp producer that lost money in foreign exchange (FX) speculation. The speculation seems to have started as a commercially reasonable strategy to hedge the FX exposure resulting from export earnings, a strategy that grew into leveraged speculation.
 Years over which the losses accumulated.
 Whether there was a failure to segregate activities (particularly trading and back-office).
 Whether there was lax trading supervision or other management or control problems.

Table 4.2 Trading Losses, with Additional Characteristics.




The information shown in Table 4.2 is, to some extent, subjective. The data are based on a reading of published reports of the incidents and reflect my judgment. When the exact nature or circumstance of a loss is not clear from reports, I have tried to note that in the table. I have used my best judgment in sorting events into the various categories; sources are given in the online supplemental information so that others can make their own assessment.
Table 4.3 lists the events, again sorted by the size of the loss relative to 2007 GDP, with a longer description of each event.
Table 4.3 Short Description of Trading Losses.










Fraud
Fraud is an important distinguishing characteristic for the events listed in Table 4.1. There are 35 events in total, and 19 (54 percent) involved fraud in one way or another. Some very large losses have involved fraud (Société Générale, Barings, Sumitomo Corporation, Showa Shell Sekiyu), but by the same token, some of the larger losses have not involved fraud (Long-Term Capital Management, Amaranth Advisors, Orange County). Panel A of Table 4.2 shows events in which fraud appears to have been involved, and Panel B shows those for which fraud does not seem to have been important.
We usually think of fraud as motivated by personal enrichment—the celebrated rogue trader.8 Barings might be the best-known case, in which Nick Leeson reportedly hid losing trades, inflated his group's trading profits, and earned large personal bonuses. In addition to Barings, the events at AIB/Allfirst; Kidder, Peabody & Co.; and Bank of Montreal appear to have involved fraud for personal gain.
Although fraud for personal enrichment jumps to mind first, it does not appear to be the most common source of fraud. I have found it useful when examining the events in Table 4.2, Panel A, to consider the following classifications of fraud:

 Primarily involving actively fraudulent trading, divided into

 Fraud for personal enrichment
 Fraud to make back losses or some other motivation that is not primarily personal enrichment

 Not primarily fraudulent trading, usually to hide losses that have occurred by other means

Before turning to an examination of specific cases, we should note an important philosophical point regarding distinctions between different forms of fraud. On the one hand, in the eyes of the law, in the effect on shareholders or investors or fellow workers, and in the size of losses, there is little distinction among different motivations for fraud. The judge in the National Australia Bank case stated it succinctly: "You and your team saw yourselves as...justified in your criminal conduct by asserting that your principal motives were to make money for the bank (not for yourselves). That is simply no excuse."9 Fraud is fraud, and there is no excuse.
On the other hand, to combat and protect against fraud, we need a more nuanced approach. Understanding the origin of and motivation for fraud, understanding the modalities of fraud is one step toward designing organizations, processes, and procedures that are not vulnerable to fraud. For example, we will see that most frauds are undertaken to cover up other problems, which implies that measures to reduce errors that might grow into fraudulent events will be one strategy to minimize the incidence of fraud.
Fraudulent Trading for Personal Enrichment
In some cases, the primary motivation for or origin of the fraudulent trading appears to be personal gain. These are the cases that most closely fit our idea of rogue trading: hiding trades, creating false trade entries, and so on, in the pursuit of a promotion, a larger bonus, or other direct reward. Barings, AIB/Allfirst, Kidder Peabody, and Bank of Montreal most closely fit this paradigm. Interestingly, this category does not seem to cover the majority of fraud cases, or even the majority of fraudulent trading cases.
Fraudulent Trading for Other Reasons (Usually to Cover Losses)
Other cases involve fraudulent trading, but the intent was usually to cover a (relatively) small loss. Daiwa Bank, Codelco, Sumitomo, and National Australia Bank fall into this category. Codelco is a nice example. A trader for the Chilean state copper company was trading copper futures (as part of his normal job) and apparently entered a buy instead of a sell into a computer system. This wrong-way trade generated a $30 million loss. To try to make back the loss, the trader took unauthorized positions in copper, silver, and gold and grew the loss to almost $210 million. Unfortunately, the evidence of Table 4.2, Panel A (and my own personal experience, cleaning up after such an incident and not as a perpetrator), shows that this pattern is all too common: an otherwise innocent mistake leads to a loss that then leads to a fraudulent cover-up of losses, often with further trading that magnifies the loss.
Daiwa is another example—and one of the most egregious. The fraud apparently started as an attempt to hide a $200,000 loss early in the career of a bond trader in New York, with the fraud continuing to save and protect reputation. The fraud was apparently not for personal benefit but, rather, on behalf and for the benefit of the bank. The fraud continued for 11 years. Management oversight at the branch was very poor, and senior managers misled bank examiners and regulators, both before the trader's confession and more actively after. The bank's U.S. license was revoked, and Daiwa was expelled from the United States.
Of course, one must view the statements of perpetrators who say that they did not act for personal enrichment skeptically, but in the Daiwa case (and other cases), there is reasonable evidence that offenders did not benefit directly, apart from the obvious benefit of retaining a job and more or less standard salary.10
Société Générale is a special case. It could be considered both as a case of trading for personal enrichment and as an odd case of cover-up. Published reports indicate that Jérôme Kerviel, the trader involved, originally hid trades and created false entries to hide excessive profits, not hide losses.
Fraud Other than Directly Fraudulent Trading
Ten events shown in Table 4.2, Panel A, involved fraud but not fraudulent trading, at least in regard to a single trader executing and hiding trades against the employer's interest: Showa Shell Sekiyu, Kashima Oil Co., CITIC Pacific, BAWAG, Morgan Grenfell & Co., the state of West Virginia, China Aviation Oil, Manhattan Investment Fund, Hypo Group Alpe Adria, and NatWest Markets. For all except Morgan Grenfell, the fraud involved covering up losses that were generated in some other way, usually to avoid revealing the losses to shareholders or regulators. Most or all of the losses were generated in relatively standard business.11
Morgan Grenfell was an exception: The fraud involved setting up dummy companies to avoid regulatory restrictions on a fund holding large concentrations in a single company. Investment in the companies was not illegal or fraudulent per se except for the regulatory prohibition on the concentrations, although in retrospect the investments themselves appear to have been based on very poor judgment.
Origin of Fraud
Most of the cases of fraud shown in Table 4.2, Panel A, were motivated more by attempts to cover up a problem than by the goal of personal enrichment. Four out of 19 (Barings, AIB/Allfirst, Bank of Montreal, Kidder Peabody, and possibly Société Générale) were primarily motivated by personal gain. In contrast, 13 out of 19 were primarily motivated by trying to cover up a problem or trading loss.
Various policies and practices are needed to avert fraud no matter what its origin:12

 Separation of front-office and back-office (trade processing and P&L reporting) responsibilities
 Mark-to-market accounting and timely reporting of P&L, with P&L disseminated up the management hierarchy
 Effective risk measurement and reporting architecture
 Strong business line supervisory controls
 Firm understanding by senior management of the business and products traded

These policies and practices ensure that fraud is hard to execute (for example, separation of front-office and back-office functions makes it hard to forge trade tickets) and that mistakes and unusual P&L get recognized early (mark-to-market accounting ensures problems are recognized). High quality information and transparency are the first defense against fraud, but availability of information alone is not sufficient: Managers must understand and be able to use the information.
As argued earlier, however, understanding the origin of or motivation for fraud is also important for developing strategies to combat it. Certain strategies are particularly effective against fraud motivated by personal gain:

 Ensuring that incentive systems do not encourage excessive risk.
 Monitoring and scrutinizing successful traders as much as (or more than) unsuccessful traders.
 Ensuring that traders take regular vacations. (It is hard to maintain a fraud when one is out of the office.)
 Setting up a culture of compliance and responsible risk taking, starting at the top with the board and senior management.

These strategies and practices are well accepted, but there are others, not as often highlighted, that are particularly important to avert fraud that originates in trying to hide losses resulting from other sources:

 Designing systems and processes to make it easy for traders and back-office personnel to do the right thing and hard to do the wrong thing.
 Investing in people and infrastructure to streamline and automate operational procedures to reduce operational errors.
 Setting up a culture that encourages employees to own up to mistakes.

Financial markets can be a complex, fast-moving, and confusing environment. Automation, checklists, and well-designed systems and procedures can smooth both front-office and back-office activity and make it easier to do the right thing. For example, an option-pricing screen that accepts an entry of "101.16" as a U.S. Treasury price of 101 16/32 can lead to confusion between (decimal) $101.16 and $101.50; this is a minor error for an option strike when it is far out of the money but potentially serious when the option is at the money.
Table 4.4 summarizes the total number of events shown in Tables 4.1, 4.2, and 4.3, categorized by whether there was fraud or not and whether the event involved legitimate business activity that went wrong for one reason or another. The categorization by fraud was just discussed; the categorization by legitimate business activity that went wrong is discussed next.
Table 4.4 Summary of Events, Categorized by Fraud and Legitimate Business Activity.

Normal Business Activity Gone Wrong
It might seem odd to think of financial disasters as being the result of normal business, but that is the case for many events. Table 4.2, left panel, shows events for which fraud was not a primary issue. These events constitute 14 out of 35 (plus two for which I could not determine whether fraud was involved) events. When we also consider events that did include fraud, we find that the majority of events were the result of or originated in legitimate trading, hedging, or commercial activity. In total, 23 out of 35 events originated in normal business activity that went wrong (with four events unknown or uncertain).
The meaning of "normal trading, hedging, or commercial activity that went wrong" needs a little clarification. It can be divided into three rough categories:

1. Legitimate trading or hedging that was simply ill judged, not fraudulent
2. Legitimate trading or hedging that involved fraud tangentially
3. Speculation that started from a legitimate commercial activity

The meaning of these categories is best explained by considering the cases that fall under them.
Legitimate Trading or Hedging that Was Simply Ill Judged, Not Fraudulent (11 Cases)
This category includes LTCM, Amaranth Advisors, Orange County, Groupe Caisse d'Epargne, Askin Capital Management, WestLB, Bankhaus Herstatt, Merrill Lynch, Calyon, Union Bank of Switzerland, and Metallgesellschaft. Virtually all of these were financial or investment firms that were undertaking business they were intended to and with at least some expertise in their area. (Metallgesellschaft, although not a financial firm, is included because its hedging program was a significant part of the business strategy rather than an ancillary activity.) After the fact, one can argue that their positions were inappropriate, too large, even irresponsible, but that argument is always easier to make after rather than before. LTCM, for example, was a hedge fund with large positions in a variety of markets, and the positions were particularly large in swap spreads and equity volatility. The fund was highly leveraged and lost virtually all its capital, but there was no malfeasance or wrongdoing. In some cases, there was trading in excess of limits (Group Caisse d'Epargne, Merrill Lynch, Calyon, and probably Bankhaus Herstatt), although not what I would judge as outright fraud. Metallgesellschaft was a case of a commercial firm hedging its business activity.
Legitimate Trading or Hedging that Involved Fraud Tangentially (Five Cases)
This category covers financial or investment firms that were involved in legitimate business but fraud was involved to cover up losses or some other problem. That is, the fraud was not central to the loss. This category includes BAWAG, West Virginia, Manhattan Investment Fund, NatWest Markets, and Morgan Grenfell. West Virginia is a good example. The loss was the result of investing short-term funds (from a fund that pooled short-term assets of local governments) in long-term bonds with substantial leverage. (The situation, by the way, was remarkably similar to Orange County's. The substantive difference is that in the Orange County case, there was no cover-up after the losses.) Manhattan Investment Fund was a famous fraud, but the loss itself appears to have resulted simply from a strategy to short technology stocks during the tech bubble (a strategy that was ultimately correct but, in this case, executed too early).
Speculation that Started from a Legitimate Commercial Activity (Five Cases)
This is an interesting and important category—nonfinancial firms that undertook speculative or other trading that led to large losses. It includes Aracruz, Sadia, Showa Shell Sekiyu, Kashima Oil, CITIC Pacific, and possibly China Aviation Oil Corporation. Aracruz and Sadia were Brazilian companies that apparently moved from legitimate hedging of export earnings to leveraged speculation and are discussed in more detail further on. Showa Shell Sekiyu and Kashima Oil were two Japanese companies that speculated in FX, with the speculative activity probably originating in hedging FX payments related to oil imports. These two cases are particularly important because they highlight the importance of marking to market and recognizing losses early. Press reports indicate that Kashima Oil's losses accumulated over six years (and Showa Shell Sekiyu's over an unspecified but comparable period). Under Japanese accounting rules of the time, the losses could be rolled over and effectively hidden from shareholders. CITIC's losses appear to have originated in an attempt to hedge an acquisition in a foreign currency, but the hedge was highly levered for some reason. Some of these cases involved fraud (Showa Shell Sekiyu, Kashima Oil, CITIC Pacific, China Aviation Oil), and others did not (Aracruz, Sadia).
The first category, legitimate trading, is particularly important when considering risk management for financial institutions. Ten of the 11 cases (excluding Metallgesellschaft) involve financial or investment firms undertaking normal financial or investment activity. These events raise some fundamental questions about managing risk. Fraud is easy to categorize as illegal and unethical (even if the fraud itself can be difficult to identify), and there is no question that fraudulent activities should be prohibited. For legitimate financial activity, in contrast, there is no good way to distinguish between good activity that leads to profits and bad activity that leads to losses.
The bottom line is that there is no unambiguously good versus bad financial activity. Some investments or trading strategies are better than others, but trading and investing is risky and involves taking positions that may or may not work out, which is what makes managing risk, like managing any other part of a business, difficult and challenging.
Note that some frauds listed in Table 4.2, originated in legitimate trading activity (Sumitomo, Daiwa, National Australia Bank, and Codelco), but I do not include these as normal business because fraud was the central component of the event.
The columns "Failure to Segregate Functions" and "Lax Trading Supervision or Management/Control Problem" show that the nonfraudulent losses in Table 4.2, for "financial institutions" are not predominantly the result of operational or supervisory problems. (This finding is naturally in contrast to cases of fraudulent trading, in which failure to segregate functions or supervisory problems are usually present.) Among these 10 cases (excluding Metallgesellschaft), there were no cases of failure to segregate front- and back-office functions. In four cases (LTCM, Amaranth, Orange County, and Askin Capital Management), lax supervision or management and control issues did not appear to be an issue. For Bankhaus Herstatt and UBS, the trading activity (FX trading for Herstatt, equity derivatives related to Japanese bank convertible preference shares for UBS) was not supervised with the same rigor or integrated as fully as other activities at the bank. For Merrill Lynch's mortgage trading, the trader reportedly exceeded trading authorizations. The other three (Caisse d'Epargne, WestLB, and Calyon) may have involved lax trading supervision or other control issues.
When we turn to nonfinancial institutions (those firms for which "Primary Activity Finance or Investing" is no), we also find evidence of financial disasters that originated in normal business practices. Aracruz and Sadia speculated in the FX markets and lost large amounts, but according to a banker familiar with the Brazilian markets, this trading was relatively common and originated from standard business practices. Both firms had large export businesses and thus generated revenues in dollars versus costs in Brazilian reals. Standard business practice would be to hedge future export receipts by selling dollars forward. For many years, this trade was also a profitable speculative strategy because of the differential between Brazilian real and U.S. dollar interest rates. High Brazilian and low U.S. interest rates meant that forward FX rates implied depreciation of the real, but in fact, the real was relatively stable for a long period. This situation led many firms to move from hedging future export earnings to leveraged speculation.13 The real depreciated dramatically, starting in August 2008, which led to large trading losses. Although this depreciation did not last long, the losses were large enough that the firms were forced to close out and crystallize their losses.
Among nonfinancial institutions, even those events that did involve fraud usually originated in some way from a normal business activity. Showa Shell Sekiyu's and Kashima Oil's events probably originated in hedging import or export earnings, CITIC Pacific's event appears to have been a hedging transaction that was the wrong size, and China Aviation Oil may have started hedging jet fuel purchases.
Other Characteristics
In addition to fraud and normal business activity, two other characteristics need to be discussed.
Years over which Fraud Accumulated
Events involving fraud generally also involve longer periods over which losses accumulate, which is natural because the goal of fraud is to hide losses and delay the day of reckoning. We might also think that losses over a longer period would be larger because there would be more time for losses to accumulate, but the largest-loss events in Table 4.2 (LTCM, Société Générale, and Amaranth) were actually losses over a short period. There are competing influences: longer means more time for losses to accumulate, but larger losses come to light faster because they threaten the existence of the institution. In fact, of the three largest events, two resulted in the collapse of the institution (LTCM and Amaranth).
Failure to Segregate and Lax Supervision
"Failure to segregate functions" refers to the failure to separate trading and back-office or record-keeping functions, with Nick Leeson's responsibility for both trading and back-office at Barings being probably the best-known example. Although this fault has been highly publicized, it does not appear to have been a substantial factor in most events—only 3 out of 22 (with 13 unknown or difficult to determine). One reason may be the emphasis segregation of responsibilities has been given in regulations and best practice guidelines: in recent years, firms have learned to close this gap.
"Lax trading supervision or management/control problem" refers to the failure by managers to properly supervise traders or otherwise exercise control. This issue has been a factor in many events (12 out of 21, with 8 unknown and 6 difficult to determine). I have included under this rubric a wide range of problems, from the extraordinary (the behavior of Daiwa managers that eventually led to Daiwa's expulsion from the U.S. banking market) to the all too common (the failure of managers to fully understand or appreciate the risk of products or businesses that subordinates were undertaking, as appears to have been contributing factors with Union Bank of Switzerland and Merrill Lynch in 1987).
Summary
Fraud is a part of many financial disasters, but it is often used to cover up after losses rather than be involved in the original loss. Nonfraud events are almost as common as fraud-related events. Losses resulting from normal business characterize many events. Some of the largest, in fact, were simply bad judgment or bad luck, not involving fraud or the exceeding of trading limits or mandates. LTCM, Amaranth, Union Bank of Switzerland, and Askin Capital all seem to fall in this category.
Lax trading supervision or other management/control problems contributed to many incidents. Failure to separate trading and back-office functions, however, has not been as prevalent, possibly because it is such a well-recognized problem.
Lessons Learned
One valuable product of reviewing financial disasters is to learn lessons on how to better manage a firm. (And it is important to recognize that issues contributing to financial disasters are often general management issues rather than specific risk issues.) The short paper "Rogue Traders: Lies, Losses, and Lessons Learned" (WillmerHale 2008) provides an excellent summary of the topic and reviews a few of the episodes considered here. The discussion is focused specifically on rogue traders (unauthorized trading involving fraud—Daiwa, Barings, AIB/Allfirst, Kidder Peabody, and Société Générale), but it is quite useful generally. Of note, the appendix provides a "lessons learned" checklist (p. 10), supplemented by my own observations:

A. Setting the right tone from the top: senior management and boards must encourage a culture of compliance and responsible risk taking
B. Senior managers must understand the complexities of the products their firms trade
C. Strong operations and middle-office process and infrastructure—to minimize errors, catch errors that do occur, and identify exceptions
D. Strong business line supervisory controls are essential
E. Successful traders may require more, not less, scrutiny
F. Management should ensure that incentive systems do not encourage excessive risk
G. Vacations are a good thing [because they force somebody else to manage the positions, shedding light on any nefarious activity]
H. Risk managers should be encouraged to challenge traders' valuations
I. Operations, risk management, and compliance reporting lines should be separate from the business lines
J. Dual or matrix reporting lines must be clear
K. Strong back-office controls are as essential as front-office controls
L. Effective risk management architecture is critical

4.3 Systemic Financial Events
When we move from idiosyncratic to systemic financial events, we move from small potatoes to real money. Although idiosyncratic losses may be measured in hundreds of millions of dollars, systemic losses are measured in hundreds of billions.
Systemic financial events come in a variety of forms: hyperinflation and currency crashes, government debt default or restructuring, and banking crises. This section touches only the surface. A wide literature covers the topic: Mackay (1932), originally published in 1841, provides an entertaining look at the South Sea Bubble in England, the Mississippi scheme in France, and the tulip mania in Holland. Kindleberger (1989) is a classic work on asset manias and crashes, and Reinhart and Rogoff (2009) is a comprehensive and instructive compendium of financial crises across 800 years and more than 60 countries.
Table 4.5 shows what Reinhart and Rogoff call the "big five" crises in advanced countries from World War II through mid-2000 (that is, before the Great Recession of 2008-2009). Reinhart and Rogoff briefly discuss the bailout costs of financial crises. They point out that the estimates vary widely and, more importantly, that the true costs extend beyond the commonly quoted bailout costs to cover the fiscal impact of reduced tax revenue and other fiscal stimulus costs. Whatever the true costs, however, they are large. Table 4.5 shows that the 1984-1991 U.S. savings and loan (S&L) crisis cost somewhere between 2.4 percent and 3.2 percent of GDP. Stated in terms of 2007 GDP (to be comparable with the losses quoted in Table 4.1), it would be roughly $340 billion to $450 billion. Compared with this amount, the individual company losses are small.
Table 4.5 Selection of Systemic Banking Crises for Developed Countries (prior to 2007).

If we turn to the Great Recession of 2008-2009, the costs are similarly huge. Consider just Fannie Mae and Freddie Mac, which were taken over by the government in late 2008 as the subprime housing crisis exploded. Fannie Mae reportedly lost $136.8 billion in the two-and-a-half years from the fourth quarter of 2007 through the first quarter of 2010. As of May 2010, the U.S. government has provided $145 billion in support to Fannie Mae and Freddie Mac.14 The Congressional Budget Office projects that the total cost may reach $389 billion.15 Note that this is only a fraction of the cost for the overall U.S. financial meltdown, and the United States is only a part of the overall global damage.
Fannie Mae and Freddie Mac are also important because they are examples of the systemic nature of the incentives, costs, and policy decisions that contribute to systemic crises. Fannie and Freddie have suffered such large losses as much because they were following their congressional mandate—to subsidize the U.S. residential housing market and expand access and affordability—as because they made specific management or risk mistakes. For decades, investors assumed that an implicit U.S. guarantee (now made explicit) stood behind Fannie and Freddie paper, and investors provided funding at rates better than those other financial institutions could access. This situation skewed costs and incentives in the mortgage market, contributing to Fannie's and Freddie's large holdings and large losses and also contributing to the overall residential real estate bubble. The skewed incentives were and continue to be government policy, and these skewed incentives contributed to a systemic crisis whose costs overshadow any idiosyncratic disaster.
A number of firms that were involved in idiosyncratic events listed in Table 4.1 were also caught in the systemic crisis of 2008-2009. The losses resulting from systemic problems were many times the losses caused by idiosyncratic events.
As an example, Hypo Group Alpe Adria shows up in Table 4.1 as losing €300 million in 2004 because of a currency swap (with subsequent fraud to hide the extent of the loss). This amount pales next to Hypo's recent credit and other losses. In December 2009, the bank was taken over and rescued by the Republic of Austria. The aftertax loss for 2009 was €1.6 billion, and as of early 2010, the problems were continuing. As another example, Dexia Bank suffered an idiosyncratic loss of €300 million in 2001, but losses for 2008 were €3.3 billion and required state aid from Belgium, France, and Luxembourg.
4.4 Conclusion
Reading about financial disasters helps to provide feedback, reminding us that disasters, fraud, and just plain bad luck do happen. Properly used and analyzed, the events can help us learn what to do and what not to do in managing a financial firm. Many disasters are precipitated by simple and obvious mistakes. Rather than gloat over another's adversity, though, we should take away the lesson that it is all too easy to fall prey to such mistakes.
This book focuses on idiosyncratic risk, and this chapter has focused primarily on idiosyncratic risk events—events that are triggered by circumstances within a single firm and limited to that firm. This is in contrast to systemic or macroeconomic risk events that play out across the whole economy, or even globally. Systemic risk events, however, are far more damaging because they involve substantial dislocations across a range of assets and across a variety of markets. Furthermore, the steps a firm can take to forestall idiosyncratic risk events are often ineffective against systemic events.
Notes
1. See Valencia (2010).
2. The full phrase from Voltaire's Candide is "Dans ce pays-ci, il est bon de tuer de temps en temps un amiral pour encourager les autres." ("In this country [England], it is wise to kill an admiral from time to time to encourage the others.") The original reference was to the execution of Admiral John Byng in 1757. It is used nowadays to refer to punishment or execution whose primary purpose is to set an example, without close regard to actual culpability.
3. From Tremper (2008, 279). Mary Yates's husband, along with three others, was killed in an avalanche they triggered in the La Sal Mountains of southern Utah.
4. Regarding the risks of systemic events, the story of Goldman Sachs provides a useful cautionary tale. As related in Nocera (2009), during 2007 Goldman did not suffer the kinds of losses on mortgage-backed securities that other firms did. The reason was that Goldman had the good sense (and good luck) to identify that there were risks in the mortgage market that it was not comfortable with. As a result, Goldman reduced some mortgage exposures and hedged others. Note, however, that although Goldman did not suffer losses on the scale that Bear Stearns, Merrill Lynch, and Lehman Brothers did during the crisis, it still suffered in the general collapse. Ironically, Goldman was later pilloried in the U.S. Congress for shorting the mortgage market, the very action that mitigated its losses and that prudent idiosyncratic risk management principles would recommend.
5. Kidder, Peabody & Co.'s 1994 loss resulting from U.S. Treasury bond trading is a case in point. The loss is reported by some sources as $350 million. This amount was actually a write-down by Kidder or Kidder's parent, General Electric Company, which reflected both trading losses and the restatement of previously reported, but fictitious, profits. According to U.S. SEC documents, the actual loss caused by trading was $75 million.
6. As an example, the Herstatt loss in 1974 was $180 million at the time. Adjusting for U.S. CPI inflation (320.6 percent from 1974 to 2007) brings it to $760 million in 2007. Adjusting for growth in U.S. nominal GDP (838.8 percent, which adjusts for both inflation and growth in the economy), the loss is equivalent to roughly $1.71 billion in 2007.
7. Note that Herstatt risk refers to the circumstances under which Herstatt was closed rather than the trading loss that caused Herstatt's collapse.
8. By personal gain or enrichment, I mean direct gain over and above retaining one's job and a more-or-less standard salary; personal enrichment would, for example, take place through a large bonus that would not have been paid absent the fraud.
9. Miletic (2005).
10. In the case of Toshihide Iguchi and Daiwa's losses on bond trading, even the U.S. prosecutor said as much (New York Times, September 27, 1995: www.nytimes.com/1995/09/27/business/an-unusual-path-to-big-time-trading.html). In the case of NatWest Markets' loss on mismarking swaption volatilities, the regulator (the Securities and Futures Authority) concluded afterward that the event as a whole was not inspired by the pursuit of personal gain.
11. BAWAG may be an exception. BAWAG apparently invested in a hedge fund that undertook trading outside of BAWAG's authorized investment rules, although it seems that senior BAWAG managers may have directed the hedge fund to do so.
12. See Wilmer Cutler Pickering Hale and Dorr (2008) for a discussion of lessons learned from trading loss events. The report focuses on rogue traders and five of the events discussed here (Daiwa, Barings, AIB/Allfirst, Kidder Peabody, Société Générale).
13. An alternative strategy, one that had the same economic impact, was for a Brazilian company to borrow in dollars (paying low U.S. interest rates) and pay the debt back out of future earnings in reals. This strategy worked well as long as the real did not depreciate substantially; if it did, it would leave the borrower with substantial foreign currency liabilities and FX losses.
14. As of May 2010, according to the New York Times (Applebaum 2010) and Bloomberg (May 10, 2010).
15. Data as of June 2010. See www.cbo.gov/ftpdocs/108xx/doc10878/01-13-FannieFreddie.pdf.









Chapter 5
Practical Risk Techniques
The discussion so far has been how to think about risk and uncertainty in general terms. We now turn to the specifics of financial risk measurement and management. We introduce in this chapter some of the tools of quantitative risk measurement and show how they apply in practice. The goal is to present the ideas and intuition, showing how the tools are used, and to avoid mathematical and technical complication. The details, the mathematical background and formulae, are absolutely important, but these details are left for later chapters.
This chapter is aimed at two audiences, two groups that often speak different languages and inhabit different worlds, but groups that need to work together for the effective management of risk.
The first group are the managers running the firm, trading desk, or portfolio. These managers make the business decisions but often will not have strong technical training—they are consumers of risk measurement services rather than producers of them.
The second group is the risk professionals, or quants, responsible for producing the risk reports and other services. They will generally have a strong technical and mathematical background but often will have less experience in managing a business, communicating ideas in a nontechnical way, and the soft skills of interpersonal interactions.
My goal for both groups is to explain how to think about and use the quantitative tools such as volatility or VaR (value at risk). Each group can have its own challenges in understanding and using risk tools, but these challenges arise from opposite poles of a continuum.
The management group understands how the business works and intuitively how risk affects decisions, but tends to have less grasp of the mathematical arcana behind risk tools. The technical details often present a deterrent to understanding and using these tools. The ideas that risk tools try to capture are not complicated, and when properly explained, the tools can be stripped of much of the technical jargon. This chapter attempts to explain to a manager how their business decision making can be enhanced with proper understanding of risk tools.
The risk professional group has a firm grasp of the mathematical and technical details going into the calculation of the risk measures, but usually has less experience with the running of the business and how risk considerations enter into management decisions. This chapter attempts to explain to a risk professional how risk tools are used in making business decisions.
This chapter is focused on the common ground shared by the two audiences, on using information about the P&L distribution to manage the business. Managers need the tools and the understanding to use the P&L distribution. Risk professionals need to provide the tools, advice, and training to support the whole organization in using the P&L distribution to effectively manage risk. The common ground, however, does not include the countless details of estimating the P&L distribution. Managers need to use the distribution and need to have the confidence that the distribution is a reasonable estimate, but usually will not care about the technical details of how it is estimated. Risk professionals need to produce the distribution, and need to assure outside users that the estimate is reasonable, but do not need to communicate all the gory details.
Risk management is effective when the two groups—managers and risk professionals—work together. More often than not, the skills of the two groups—the management skills to use the P&L distribution versus the technical expertise to produce the P&L distribution—reside in separate individuals with different backgrounds and skills. An effective organization is one in which these two groups work together to solve the firm's problems. The managers must stretch to use unfamiliar tools and understand an unfamiliar language while the risk professionals must strive to provide complex concepts and data in a simple, direct manner.
5.1 Value of Simple, Approximate Answers
One theme of this chapter is the value of simple, approximate answers. It is better to have a broad outline of the risk today than a meticulous and detailed understanding one year hence (after the business has blown up). Physics students are taught to estimate orders of magnitude for physical problems. The delightful "Order of Magnitude Physics" (Sanjoy, Phinner, and Goldreich 2006) is devoted to the topic.
Most technical education emphasizes exact answers. If you are a physicist, you solve for the energy levels of the hydrogen atom to six decimal places. If you are a chemist, you measure reaction rates and concentrations to two or three decimal places. In this book, you learn complementary skills. You learn that an approximate answer is not merely good enough; it's often more useful than an exact answer. When you approach an unfamiliar problem, you want to learn first the main ideas and the important principles, because these ideas and principles structure your understanding of the problem. It is easier to refine this understanding than to create the refined analysis in one step.
So it is with understanding financial risk. We always meet new risks and it is incredibly important to have tools and techniques for understanding the main ideas and the broad outline of the risks. Effective risk management occurs when a firm can integrate management expertise with technical skills.
5.2 Volatility and Value at Risk (VaR)
Before discussing volatility and VaR we need to think about what financial risk measurement is. Financial risk is in some ways so simple, because it is all about money—profit and loss and the variability of P&L. Political risk is about the possibility of insurrection, expropriation—or the Republicans winning the next presidential election. Risk in flying an airplane is that an engine may flame out or that fog may obscure the landing strip. Risk is multifaceted and amorphous in so many areas. For a financial firm, the primary focus is whether tomorrow or next year will show a profit or a loss. Of course, other things matter, but for a financial firm those other things are dominated by the profit and loss—the P&L. What generates the P&L is multifaceted and possibly amorphous but the P&L itself is pretty concrete and simple. Money is something we can measure, something most of us can agree about, that more is better than less and a profit is good and a loss is bad.
From this, it follows that the distribution of P&L is what matters when discussing financial risk. Let's take an extremely simple financial business, betting on the outcome of a coin flip. We make $10 on heads and lose $10 on tails. We could graph the P&L distribution as in Panel A of Figure 5.1. The probability is one-half of losing $10 and one-half of making $10. This kind of distribution is fundamental to how we should think about financial risk. It shows us the possible outcomes (possible losses and gains along the horizontal) and how likely each of these is (probability along the vertical).

Figure 5.1 P&L from Coin Toss Bet and Hypothetical Yield Curve Strategy

Reproduced from Figure 5.1 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

For managing risk, the main thing that we want from the P&L distribution is an understanding of how variable the P&L can be. In this example, it is very simple—either -$10 or +$10. In practice, it is more complicated and we want to know things like how much we might make or lose on a standard day, and how much might we make or lose if things go badly.
When flipping a coin we can have some real confidence that each outcome is actually one-half; in a real business, we cannot have such confidence that the probabilities tomorrow will be exactly as we believe them to be. This goes back to the idea of frequency-type versus belief-type (objective versus subjective) probabilities. But we really have no choice in financial risk management except to use our belief-type probabilities. We must use them with caution, have some humility that our estimates will be wrong, but use them we must.
Panel B of Figure 5.1 shows a more realistic P&L distribution. The possible losses and gains are along the horizontal but, in contrast to Panel A, there may be any of a wide range of possible outcomes. The most likely outcome is somewhere around zero, but there is some possibility of large profits and some possibility of large losses.
Financial risk measurement is really nothing more than what is shown in Figure 5.1—the P&L distribution. When we know the P&L distribution, know the possibilities of gains versus losses, when we understand what generates the distribution and what causes those gains and losses, then we understand virtually everything we can about financial risk.
We don't know what will happen tomorrow because we never can know with certainty what will happen tomorrow. But we have some range on the possibilities. The distribution in Figure 5.1 shows us the possibilities. We have to give up on certainty and embrace uncertainty. We have to move away from thinking "the P&L tomorrow will be $50,000" to "the P&L will most likely be between -$50,000 and +$50,000, but there is a 5 percent chance we will lose $150,000 or worse." This is not easy and a wrenching change from our natural inclination, but it is the essence of a mature understanding of risk.
In this sense, financial risk is extraordinarily simple—we need only to understand the P&L distribution. In practice, of course, financial risk is never simple. The difficulties are twofold. The first is purely conceptual, coming to grips with life as a distribution rather than a unique outcome ("P&L will be between -$50,000 and +$50,000 for two out of every three trading days but will be worse than $150,000 roughly once a year").
The second difficulty is that we will never know the P&L distribution with certainty, and even arriving at a reasonable estimate can be quite difficult. But this is part of living with uncertainty—we have to work hard to envision the possible future outcomes, to learn about how the world is and how it might be. Even more difficult, we have to accept that even our P&L distribution has uncertainty and so we need to treat all our numbers with respect and caution.
Estimating the P&L Distribution
In this chapter I ignore the details of various approaches and all the problems associated with estimating the P&L distribution. Later chapters will deal with these, but for now we just assume that we have a reasonable estimate.
The easiest way to understand how to use the P&L distribution is to work with an actual example. Turn to Figure 5.2, which is an estimate of the one-day P&L distribution for holding $20 million of a U.S. Treasury bond (the 10-year as of January 2009, the 3.75 percent of November 15, 2018). From looking at the graph, we can tell a few things, before talking about any numbers:

Figure 5.2 P&L Distribution for U.S. Treasury Bond, Showing Volatility (Standard Deviation)
Based on Figure 5.2 from A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.


 The price will go up or down with roughly equal chance.
 Most likely we will see no change or a small change one way or the other.
 Large changes are less likely than small changes.
 Large changes do occur.

Summary Measures—Volatility and VaR
There is real value, however, in actually putting numbers around this. Numbers help systematize and organize our thoughts. But they are only tools to help us understand the world, and this means two things. First, we have to understand what the tools and the numbers mean. Numbers without understanding are worse than useless—they mislead, obfuscate, or give a false sense of security.
Second, the numbers are our servants, not we theirs. We can use them to understand the world better but we should not be slaves to them. Numbers should help us be honest with ourselves and they should aid in communicating with others. They should not obfuscate and confuse. Managers should never accept numbers that are not simple, clear, and concise. Risk professionals should always strive to communicate clearly and to enlighten using numbers, but must remember that the numbers must communicate something about the real world. "It's not the figures themselves, it's what you do with them that matters."
So let's put some numbers around the distribution. The most important aspect of the distribution (from a risk management perspective) is the variability, the dispersion, the spread of the distribution. We would like a single number that would tell us the dispersion of the distribution. But there is no one best way to summarize the dispersion. Indeed, the "dispersion of the distribution" is a rather vague concept. Like learning to live with uncertainty itself, we have to learn to live with some vagueness and ambiguity in describing the variability or dispersion of the P&L distribution.
For risk professionals, this vagueness or ambiguity may be one of the hardest things to master in moving from merely measuring risk to a fuller understanding of managing risk. Managers, through inclination and experience, tend to have a much higher tolerance for living with ambiguity and vagueness.
Getting the right balance between vagueness and precision is difficult. Using and understanding quantitative tools requires a careful balance between too much vagueness (where we can't say anything useful) and false precision (where we can be very precise about things with no connection to the real world).
There are two common measures used to summarize the dispersion or variability. (Although there are others, these two are the most common; if you truly understand everything about these, you will understand more than 90 percent of financial professionals.) The two measures are volatility (also known as standard deviation) and Value at Risk (VaR). Personally, I use volatility more and generally prefer to work with it, although this may be a minority opinion among risk professionals. When properly used, volatility and VaR are both useful. They will by and large tell us the same information, although there will be times when they can provide meaningfully different views of the distribution.
Volatility for the Bond
The easiest way to understand volatility and VaR is to turn back to the graph of the P&L distribution. Volatility and VaR are calculated in different ways but both describe what is the spread or dispersion of the distribution.
Figure 5.2 shows the volatility, or standard deviation. For the $20 million bond position, the volatility is $130,800. The volatility measures the spread around the central value (around the mean). It is calculated as an average of squared deviations from the mean. That is, for every possible profit, we calculate the distance from the mean, square that distance, and take the average of the squares (finally taking the square root):

It is important to understand that volatility is an average of gross changes, not net changes. When we consider changes over time, it is natural to think that the ups and downs cancel out, leaving us with a net change (generally close to zero). But volatility is the square root of the more fundamental statistical concept called the variance, and the variance is the average of squared changes. Positives and negatives do not cancel out. We use volatility (square root of the variance) rather than the variance itself because volatility is in the same units as P&L and prices and thus more intuitive.
The most important thing in using volatility (or VaR, for that matter) is understanding what it tells us and how to use the information. Figure 5.2 helps us understand how to use volatility. The volatility tells us the spread for the distribution. For most well-behaved distributions, roughly 30 percent of the outcomes will be better or worse than the volatility. For our bond, the volatility is $130,800 and so we should expect that 30 percent of the time, or roughly one day in three, the loss will be worse than -$130,800 or better than +$130,800.
How do we use this? We need to examine things like our internal tolerance for gains versus losses or the firm capital that is available to absorb losses. Can we live with a loss of $130,800 on a regular basis? How much does such a loss worry me? Does it make my stomach churn? Or is that loss really small, maybe too small? And remember that $130,800 sets the level for "standard trading conditions" and that the P&L will be between -$130,800 and +$130,800 two days out of every three. But that third day can sometimes be a very bad day—every once in a while there will be losses much worse or profits much better than $130,800. How much worse? We'll discuss the issue in more detail shortly, but for now, just gut feeling gets us a long way—maybe asking if I would lose my job if losses were three or four times that. These questions start to set a scale for how much risk is in the $20 million bond position.
In many cases, such as managing a portfolio, a useful aid in informing our gut instinct is to calculate the volatility as a percentage of the assets. Then I can compare that volatility with other investments, things I may have long familiarity with.
Say I was a portfolio manager with $500 million in assets. In this case, a loss of $130,800 would be really small, only 0.03 percent of the portfolio value. This would translate into roughly 0.4 percent annualized volatility. (We translate from daily to annual by multiplying by the square root of the days in a year—roughly 255 trading days—but we discuss this more further on.) Such a figure seems really small relative to, say, the stock market volatility (on the order of 20 percent annualized). This comparison helps to inform us about the bond position.

Digression on the Normal Distribution
The normal distribution is used so much in risk measurement that we really need to discuss it, even though the debate about the pros and cons of the normal distributions is often more esoteric than we are interested in here.
The distribution of P&L usually looks something like that shown in Figure 5.2—bell-shaped with most of the probability toward the center and low probability of large gains or large losses. The normal distribution is the paradigm, or model bell-shaped distribution. Mathematicians have worked with it for many centuries, and it is, as these things go, easy to use. Figure 5.3 shows a sample normal distribution. Sixty-eight percent of the probability is within ±1σ of the mean (so 32 percent outside of ±1σ, roughly 15 percent less than -1σ and 15 percent more than +1σ).1

Figure 5.3 Normal Distribution
Reproduced from Figure 5.4 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

We are most interested in the probability that an observation will be within a certain distance of the mean, say ±1σ or ±2σ. The probability that an observation will be within ±2σ is 95 percent, and the probability it will be below -2σ is 2.5 percent. If the P&L is normally distributed, then we can say that the probability the P&L will be below -2σ is 2.5 percent. The probability it will be below -1σ is about 16 percent.
We do have to be careful in using the normal distribution. The normal is valuable as an aid to intuition, to understand how and why risk measurements behave as they do. But it is not a perfect description of the real world. Sometimes the P&L distribution looks more like Figure 5.4, and in this case there will be losses larger than we would think were we to really believe the normal distribution. In other cases, there will be more large moves (both positive and negative) than the normal distribution would lead us to believe. As in so many aspects of risk management, we have to always use these tools carefully.

Figure 5.4 Nonsymmetrical Distribution with Fat Tail
Reproduced from Figure 5.5 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

When using a normal distribution the volatility (standard deviation) tells us virtually everything about the distribution. For a normal distribution, we know how much probability is in the central part relative to the tails, how much in the upper versus lower tail. The standard deviation sets the scale, or the dispersion, and once we know that the standard deviation of a normal distribution is $130,800, we can calculate the exact probability of a loss of $256,000 or worse (2.5 percent chance) or $304,000 or worse (1 percent chance). For a normal distribution, the volatility tells us virtually everything.
But we can use the volatility even if the distribution is not normal. We can calculate the volatility for any set of data and for any distribution (within reason). For a non-normal distribution, the standard deviation is only one among various ways we might summarize the dispersion, but it is still incredibly valuable in giving us a first view of the dispersion. It will not tell us everything (as it does for a normal distribution) but we should still use it for what it can tell us.

VaR for the Bond
Figure 5.5 shows the same bond P&L distribution as in Figure 5.2 but now with both the volatility (standard deviation) and the 5%/95% VaR drawn in: Panel A shows the volatility and Panel B shows the VaR. The VaR is simply the level of loss where there is a 5 percent chance losses will be worse and 95 percent chance they will be better. The VaR is nothing more and nothing less than one way to summarize the spread, or dispersion, of the bond.

Figure 5.5 P&L Distribution for U.S. Treasury Bond, Showing Volatility and VaR
Reproduced from Figure 5.8 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

The volatility is one number that summarizes the spread and is calculated by taking the average of squared deviations. The 5%/95% VaR is another number that summarizes the spread and is calculated by setting the chance of worse losses at 5 percent. (Remember that for a normal distribution, the probability of losses worse than the volatility is about 16 percent. This means that when P&L is normal, the volatility can be thought of as the 16%/84% VaR.)
It is absolutely critical to remember that what really matters is the underlying P&L distribution, and the volatility and the VaR are simply two ways of summarizing the spread of that distribution. Sometimes one is more useful, sometimes the other. Personally, I use the volatility more often, but many people prefer to look at the VaR first.
For a normal distribution, there really is no difference between them, in the sense that if we know one, we can always calculate the other. For a normal distribution, they are equally useful and it is purely a matter of personal preference which one is used. For a normal distribution, the 5%/95% VaR is 1.645 × the volatility, the 1%/99% VaR is 2.326 × the volatility, and the 0.1%/99.9% VaR is 3.09 × the volatility. Most P&L distributions are not normal and there will be some difference between the volatility and the VaR. They may give us different views of the distribution.
How would we use the VaR? In very much the same way as the volatility, as a way of helping our gut determine whether we have too little or too much risk. For our bond example, the 5%/95% VaR is roughly $215,000, and so we should expect to see losses worse than that roughly 5 percent of the time, or one day out of 20. Can we live with this kind of loss? Again, we can ask our gut whether we would be comfortable seeing such losses once a month, or losses maybe two or three times larger on a yearly basis. If that is too much, then the risk may be too high. If we would not even notice because the portfolio is so large, then maybe the position is too small.
Two Uses for Volatility and VaR—Normal Trading versus Extreme Events
Both volatility and VaR are widely used in the finance industry. There are two related but somewhat divergent uses for these measures, and highlighting these two uses can clarify how and why we use them. Volatility and VaR are used for one or both purposes:

1. To standardize, aggregate, and analyze risk across disparate assets (or securities, trades, portfolios) under standard or usual trading conditions
2. To measure tail risk, or extreme events

Risk measurement texts often focus on the latter—tail events—but it is equally important to focus on risk under standard or usual trading conditions. Standardizing and analyzing risk across disparate assets and large portfolios provides information necessary for understanding and managing risk and P&L under standard trading conditions, which are, by definition, most of the time. Furthermore, analyzing risk under usual trading conditions provides valuable clues to performance under more extreme conditions.
When considering risk under standard or usual trading conditions, volatility can often be more useful than VaR. Volatility is particularly suitable for measuring the spread of the central part of the distribution, exactly what one is interested in when considering usual trading conditions. P&L will be outside of ±1σ roughly 30 percent of the time and inside roughly 70 percent of the time, so the standard deviation gives a good feel for usual trading conditions.
VaR is more commonly used for the second purpose—measuring extreme, or tail, events. In this context, VaR is sometimes referred to as the statistically worst-case loss, but this is a horribly misleading idea. VaR should be viewed as a periodically occurring event that, while not likely, we should be perfectly comfortable with. We should think of VaR as providing a scale for possible large losses, not a maximum loss or worst-case scenario. It really is true in markets that, whatever our worst-case scenario, something worse will happen sometime, somewhere.
We also must remember that, by their nature, tail events are rare, so measuring tail events is inherently difficult and open to large errors and uncertainty. As a result, when applied in this second sense, VaR must be used cautiously and any conclusions treated with care. I have more to say about measuring tail events further on.
The two uses of volatility and VaR can never be precisely separated, but the conceptual differentiation clarifies some of the uses, benefits, and limitations of volatility and VaR. For usual or normal trading conditions, standard statistical and quantitative techniques work pretty well, and the interpretation of results is relatively straightforward. Assuming normality or linearity of the portfolio is often acceptable when considering the central part of the distribution, meaning that simple and computationally efficient techniques can be used.
Measuring tail events, in contrast, is delicate, and the appropriate statistical and quantitative techniques are often complex. In the tails, normality is generally not an appropriate assumption and more sophisticated statistical assumptions and quantitative, numerical, and computational techniques must be applied. The inherent variability of tail events is generally higher than for the central part of the distribution, and uncertainty caused by estimation error and model error is larger. As a result, the estimation of VaR or other summary measures for tail events is inherently more difficult, and the use and interpretation of results more problematic.
Time Scaling
The P&L distribution in Figure 5.2 is the P&L over some specified period. Our example is the P&L for one day, for other examples it might be for 10 days, but it will always be the P&L for some period. We will often want to know what the P&L variability might be for alternate periods. To get a truly correct answer there is no substitute for a complete analysis, focusing specifically on the period of interest.
Having said that, we should remember the value of simple and approximate answers. There is in fact a simple way to approximately convert from one period to another. Volatility and VaR for most financial assets grow like —in other words, the volatility for 10 days will be  = 3.16 × the one-day volatility.
Why is it square root and not linear, as would seem natural? The answer is somewhat subtle. When we look at changes over time—today, tomorrow, next day, and so on, and add them up, some days go up, some days go down, and those changes cancel so that (roughly) the average net change is zero—positives and negatives balance out. But remember that the volatility is the square root of the variance, and the variance is the average of squared changes. For the variance, positives and negatives do not balance out or cancel. The variance is essentially a gross change.
In the simplest case, the squared changes add over time, so that the variance grows linearly with time. In other words, the variance scales linearly. In mathematical and statistical terms, the variance is the more fundamental concept, but in practical applications we use the volatility (standard deviation) because it is much more intuitive. We care about P&L and changes in prices, and volatility is in the same units as P&L and prices, so it makes sense to talk about and think about volatility. Because the variance (the more fundamental concept) scales linearly and volatility is the square root of the variance, the volatility scales like square root.
Consider our example of the U.S. bond. The one-day volatility is $130,800, so the 10-day volatility would be about 130,800 × 3.6 ≈ $413,300. The one-year volatility would be about 130,800 ×  ≈ $2,088,700.
5.3 Extreme Events
The most difficult and vexing problem in quantitative risk measurement is trying to quantify tail, or extreme, events. Tail events are important because large losses are particularly significant, and can, in truly extreme situations, wipe out a firm.
Measuring tail events is difficult for some fundamental reasons. First, tail, or extreme, events are by their nature rare and thus difficult to measure. By definition, we do not see many rare events, so it is difficult to measure them reliably and to form judgments about them. Second, because of the scanty evidence, we are often pushed toward making theoretical assumptions about the tails of distributions (extreme events). Unfortunately, simple and common assumptions are often not appropriate for tails. Most importantly, the assumption of normality is often not very good far out in the tails.
Although rare events are rare, they do occur, and measurements across different periods, markets, and securities show that in many cases, extreme events occur more often than they would if the P&L behaved according to the normal distribution in the tails. This does not mean the normal distribution is a bad choice when looking at the central part of the distribution, but it does mean that it can be a poor approximation when examining extreme events.
Broadly speaking, three approaches can be taken when dealing with tail events:

1. Simple rules of thumb
2. Alternative but tractable distributional assumptions
3. Extreme value theory, which focuses on the asymptotics of tail events

We will stay in the spirit of this chapter and talk only about a simple rule of thumb, leaving the other ideas for later chapters.
Rule of Thumb for Tail Events
Using simple rules of thumb may not sound sophisticated, but it is, in fact, a sensible strategy. Litterman (1996), speaking about Goldman Sachs, says, "Given the non-normality of daily returns that we find in most financial markets, we use as a rule of thumb the assumption that four-standard-deviation events in financial markets happen approximately once per year" (p. 54). We can interpret this statement probabilistically from three different perspectives (all equivalent but each giving a different viewpoint):

1. If daily returns were normal, a once-per-year event would be about 2.7 standard deviations. Litterman's rule of thumb is to assume that actual once-per-year changes are 4.0 standard deviations or 1.5 times larger than changes that would occur if events were normally distributed (4.0σ instead of 2.7σ). This seems a significant but not extreme assumption.
2. If daily returns were normal, a four-standard-deviation event would have a probability of about 0.0032 percent, which would make it roughly a once-per-125-year event (1/0.000032 = 31,250 days, or about 125 years), whereas Litterman's rule of thumb says it is a once-per-one-year event. This seems a much more radical assumption—instead of four-standard-deviation events occurring once every 125 years, he says they occur once every year.
3. If we assume that four-standard-deviation events occur once per year, the probability of a four-standard-deviation event is about 0.39 percent (1/255) instead of 0.003 percent. (This is the same as the second view but stated in probabilities rather than "once per x years.")

Stated the first way, the rule of thumb has intuitive appeal—large moves are 1.5 times larger than they would be if the P&L were normally distributed. There is, in fact, good evidence that returns and P&L in financial markets are not normal and have fatter tails (more large moves) than predicted by a normal distribution. A factor like 1.5 is not huge. For the U.S. bond we have been considering, 2.7 standard deviations (the prediction assuming normality) would be $353,000, while four standard deviations (the rule of thumb) is $523,000. A loss of $523,000 is clearly worse than $353,000 but not enormously worse. Knowing that financial markets have more large losses than predicted by normality, using $523,000 instead of $353,000 seems reasonable.
Only from the latter two perspectives, viewed as probability statements, does the assumption appear extreme. The first view, looking at how much larger losses might be than predicted by normality, is preferable. We see in later chapters that when we apply alternative mathematical assumptions to analyzing tails events, the probabilities start to become more reasonable. Our intuition seems to work reasonably well when applied to loss levels ("once-per-year losses are 1.5 times worse than implied by normality") but less well when applied to probabilities and the normality assumption ("once-per-year events are probability 0.003 percent instead of 0.39 percent").
This may, in fact, be another example of Gigerenzer's (2002) point about the importance of heuristics and how we pose probability problems. It may be that our intuition is well adapted to thinking about the levels of losses. Anyone with experience in the financial markets learns, sometimes the hard way, that gains and losses do in fact have fat tails. It may be that we can work intuitively with extreme events when expressed in levels of losses but that working with probabilities requires more effort. This is not to write off the value of formal probabilistic analysis. The formal analysis lays the foundations that provide justification for the simple and intuitive approximations. The point is that simple approximations are valuable and have their own place.
This rule of thumb is simple and robust—something that is easily understood, easily communicated, and easily used. It is not perfect and we cannot say with precision how reliable it is, but as a simple approximation it serves its purpose—making sure we recognize that losses can occur, and that they are often larger than simple theory would lead us to think.
The simplicity of this rule is itself a huge advantage. Primary attention remains focused on measuring the portfolio volatility, or the behavior during standard trading conditions. Doing so is often a difficult task in itself. Collecting portfolio positions, making reasonable judgments about volatility of individual positions, understanding the interaction of various positions and how these affect the overall portfolio volatility—all of these can be extremely difficult tasks. Given the paucity of observations in the tails, a simple rule of thumb, such as "four-standard-deviation events happen once per year," is a useful supplement to more sophisticated, but more complex, approaches.
This simple rule of thumb corresponds to what is often done in practice, which is to estimate the volatility of the P&L distribution and then assume that the VaR is larger by a fixed factor. The factor is often determined by assuming that the P&L distribution is normal (giving a factor of 2.7 for a once-per-year event), but here the factor is assumed to be larger by an ad hoc amount (4.0 instead of 2.7). Conceptually, the approach is to split the problem into two parts—first estimating the scale of the distribution (generally by the standard deviation or volatility) and subsequently focusing on the tail behavior. This strategy can be very fruitful because the scale of the distribution and the tail behavior can often be analyzed separately.2
5.4 Calculating Volatility and VaR
So far we have been talking about volatility, VaR, and the P&L distribution as if we already knew the distribution, as if somebody gave it to us. This is clearly not the case. We have to estimate them, and that is never easy.
I leave a detailed discussion of the topic to Chapters 8 and 9. It is important, however, to know some of the terms. There are three widely used methods for estimating volatility and VaR: parametric (also called linear, delta normal, or variance-covariance), historic simulation, and Monte Carlo. There are important differences between them and we discuss some of the pros and cons in Chapter 8 and run through an example in Chapter 9. For the moment, the similarities are more important.
Whatever approach we take, the P&L distribution is the fundamental entity. We may talk about volatility and VaR but these simply summarize the distribution itself. As always, it is good to recognize the modesty of our tools. We would like to know what the P&L distribution will be tomorrow, but that is a vain hope; we can only estimate what it was in the past and then assume or hope that it will be similar in the future. Nonetheless, understanding how the portfolio would have behaved is extremely informative and the first step toward understanding how it might behave going forward.
The goal of any approach for estimating the P&L distribution is to estimate how the current portfolio would have behaved under a variety of conditions, the conditions invariably based on history. In other words, we need to estimate the P&L of the current portfolio under a variety of market conditions.
It is generally useful to think of the portfolio P&L as resulting from two components:

1. External market risk factors
2. Positions—that is, the firm's holdings and the security characteristics that determine the sensitivity to risk factors

The distribution of risk factors and the firm's exposure to those risk factors is combined to obtain the distribution of P&L.3
Rather than undertake a detailed discussion of different methodologies—that is for later chapters—we will go through an example of calculating approximate volatility. We basically covered this in Chapter 1. Now, however, we recognize that risk measurement is focused on estimating the P&L distribution. So why do we estimate just the volatility?
If we are willing to assume the P&L distribution is normal, and such an assumption is not perfect but usually a very good starting point, then we need to calculate only the volatility. Remember that a normal distribution is completely described by the volatility (and the mean, but that will often be close enough to zero that we can ignore it). So when we know the volatility, we know the whole distribution.4
We start with the $20 million holding of the U.S. Treasury bond we have been looking at. The bond volatility will be the result of combining the exposure of the bond to market risk factors, combined with the volatility of market risk factors. For the bond, a simple but useful exposure measure is the DV01—the bond sensitivity to a one basis point (bp) move in yields (see Coleman 2011b). The DV01 of the 10-year bond is roughly $915 per bp for each $1 million notional—the bond value will fall by roughly $915 when yields go up by one basis point (say, from 2.53 percent to 2.54 percent). For $20 million notional, the DV01 is roughly $18,300. For small yield changes, price changes are roughly proportional to yield changes:

and so the volatility of prices is approximately the DV01 times the volatility of yields:

The volatility of yield changes was roughly 7.15 bp per day as of January 2009 (the date we are using for all the examples here). We could estimate this by looking at history for 30 or 100 or 500 days using data from Bloomberg or Yahoo finance (http://finance.yahoo.com/) or the Federal Reserve (www.federalreserve.gov/releases/h15/update/)—always remembering to look at daily changes in yields. Simply looking at history would not give a precise estimate but it would give us a rough idea, and a rough idea is what we care about here. In any case, using the 7.15 bp per day, the volatility of the bond is roughly:

This provides valuable information, as we outlined earlier. With this, we can say that such a position would probably make or lose more than $130,800 every third day. Is this a lot? It depends. For an individual investor with total wealth of $500,000, it would be, representing 26 percent of the wealth. For a portfolio with $500 million of assets under management, it would be small, representing only 0.03 percent of the portfolio.
This kind of rough volatility estimate becomes really useful when we want to compare this bond against some other security, say with an equity futures such as the CAC equity index futures.
Consider adding a €7 million long futures position in the CAC 40 index to the $20 million bond position (when the $:€ was 1.30 so that €7 million corresponded to $9.10 million). These are the positions considered in Chapter 1. They are, however, very different positions: the first is a straightforward purchase of a simple bond denominated in U.S. dollars, and the second is a derivatives position in a euro-denominated equity index with no up-front investment. One is a traditional security, the other a derivative security. One is fixed income, the other equity. Which is riskier?
How can we compare and aggregate the risk of these two quite disparate trades? We cannot look at the nominal amount invested because the bond is a $20 million investment and the futures position involves roughly zero investment. They are in different currencies and they are different asset classes. Furthermore, any trader or manager with extensive experience in one would not be likely to have deep familiarity with the other, so relying just on experience and common sense will likely not work.
Both trades, however, have one common denominator: the P&L. Money is money, profits are profits, and losses are directly comparable between the two. (One must, of course, remember to express both in the same currency, either dollars or euros.) We have calculated a rough estimate of the bond volatility and we can do the same for the CAC index futures. This is even easier than for the bond. We simply estimate the volatility of the CAC index (again, we can go to Bloomberg or Yahoo finance for history). The volatility of percent changes as of January 2009 was roughly 40 percent, which translates into a daily volatility of percent changes of 2.536 percent. In other words, a holding of $9.10 million would have a volatility of 2.536 percent, or roughly $230,800.
To emphasize that it is the P&L distributions that really matter, Figure 5.6 shows the P&L distributions for these two trades, assuming the P&L is distributed normally with daily volatilities of $130,800 and $230,800. The P&L is not exactly normal but for a rough comparison of standard trading conditions, this assumption is perfectly good—it gets us most of the way to the truth.
The distribution for the U.S. bond in Panel A is narrower (less dispersed) than for the CAC index in Panel B. The daily volatility for the bond is $130,800, and for the CAC index futures, it is $230,800. This figure provides an easy and direct comparison between the two. Panel C shows the two distributions overlaid, and we can say that the CAC futures position is riskier because the distribution is more dispersed (and both are centered around zero).

Figure 5.6 P&L Distribution for Bond and Equity Futures Compared
Reproduced from Figure 5.7 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

Multiple Assets
We usually need to go beyond comparing one asset against another. When we combine assets into a portfolio, we need to ask, "What is the overall portfolio volatility?" Combining distributions and volatilities across assets can get complicated and usually needs data, computers, and programming. We talk about this in detail in later chapters. The important point here is that there is a method to combine the distributions in a reasonable way, and that we can use the combined P&L distribution to build the same kind of intuition that we did earlier for the bond alone.
In our example with the bond and the CAC futures, the combined volatility is roughly $291,000. Is this large? Is it small? We have to ask, "Are we comfortable with P&L worse than -$291,000 or better than +$291,000 every third day?" Or (using Litterman's rule of thumb that we should expect to see 4-σ days roughly once per year) "Are we comfortable with losses of $1.16 million once per year?" What matters here is that we are going to treat the overall portfolio P&L distribution the same as we did for the bond alone.
5.5 Summary for Volatility and VaR
Volatility and VaR both measure spread or dispersion of the P&L distribution. It is the P&L distribution that matters—how likely we are to see losses versus gains. We use the volatility and VaR to summarize the spread of the distribution and to build intuition for risk, but it is actually the distribution, such as displayed in Figure 5.2, that matters.
For risk, the most important characteristic of the distribution is the spread or dispersion. Volatility and VaR are simply two different numbers that summarize the dispersion. (For a normal distribution. they are interchangeable, otherwise they show us slightly different views.) There is nothing magic about volatility and VaR. When you truly understand what they mean, you see that they are incredibly simple.
The importance of the numbers is what we do with them—"It's not the figures themselves, it's what you do with them that matters." How do we use them to inform our decisions? How do we use them to get a gut feeling for the risk?
Table 5.1 provides a cheat sheet for volatility and VaR. What is really important is how we use volatility and VaR. We need to understand what they are trying to tell us.
Table 5.1 Some Suggestions for Using Volatility and VaR.



Measure
Probability
Intuition




Volatility, σ
Better or worse, 30%
There is roughly a 30% chance of profit better than +σ or loss worse than -σ, so we should expect bigger P&L roughly one day out of three. So if volatility is $100,000, the P&L should be worse than -100,000 or better than +100,000 roughly one day out of three.


Volatility, σ
Worse: 15%
There is roughly a 15% chance of loss worse than -σ, so we should expect losses worse than this roughly one day out of seven.


5%/95% VaR
Worse: 5%
There is roughly a 5% chance of loss worse than this, so we should expect worse losses roughly one day out of 20. But be a little cautious—tail events are hard to measure.


1%/99% VaR
Worse: 1%
There is roughly a 1% chance of loss worse than this, so we should expect worse losses roughly one day out of 100. But be quite cautious—tail events are hard to measure. In fact, the smaller the probability, the more cautious you want to be; the less you want to believe the number as anything but a rough guide.


Extreme events
4σ occurs once per year
Suggested by Litterman, this rule of thumb assumes that four-standard-deviation events happen approximately once per year.


Time scaling

To go from one-day volatility (or VaR) to d-day volatility, multiply by . So to go from one-day to 10-day, multiply by  = 3.16.


Volatility comparison

Compare with other assets, assets that we have experience with. It is often useful to measure volatility as a percentage of one's portfolio—so for a bond with σ = $130,800 and $20 million investment, this is roughly 0.654% per day. Scale to annual: 0.654 ×  = 0.654 × 15.97 = 10.4%. Compare this with 20% to 25% volatility for S&P index—bonds are less volatile.


5.6 Portfolio Tools
Think of volatility and VaR as the standard quantitative risk measurement tools. They help us understand the size of the risk. But they don't tell us where the risk comes from or how we can alter it. They are flat in the sense of telling us the size but nothing about the composition. We need tools for understanding the composition of the risk—where it comes from and how we can change it.
There are two basic problems in understanding risk:

1. Risks combine in a nonlinear, often nonintuitive manner. Risks don't just add. Sometimes two risks add, sometimes they subtract. It's not obvious, without some work, how different risks will add or subtract.
2. For more than two risks, it becomes impossible to understand without quantitative tools to aid our understanding. A large portfolio can be so complex that we need simple, straightforward tools to help untangle the risk and point us in the direction of how to manage that risk. We need tools for drilling down to uncover the sources of risk.

Litterman (1996) expresses this well:
Volatility and VaR characterize, in slightly different ways, the degree of dispersion in the distribution of gains and losses, and therefore are useful for monitoring risk. They do not, however, provide much guidance for risk management. To manage risk, you have to understand what the sources of risk are in the portfolio and what trades will provide effective ways to reduce risk. Thus, risk management requires additional analysis—in particular, a decomposition of risk, an ability to find potential hedges, and an ability to find simple representations for complex positions. (p. 59)
There are three main tools we discuss here that are useful for understanding the sources of risk:

1. Contribution to risk
2. Best hedges
3. Replicating portfolios

Many of these ideas are based on Robert Litterman's Hot Spots and Hedges (Litterman 1996), some of which also appeared in Risk magazine in March 1997 and May 1997. The idea of contribution to risk was developed independently by Litterman and M. B. Garman (Risk magazine 1996).
These tools can give a view into a portfolio. They are anything but perfect, but remember that the manager who understands today the broad contours of the portfolio's risk is better off than the manager who understands next year the exact details (after the business has blown up). A simple approach can provide powerful insights where it is applicable and many, even most, portfolios are locally linear and amenable to these techniques. Again, Litterman (1996, 53) summarizes the situation well:
Many risk managers today seem to forget that the key benefit of a simple approach, such as the linear approximation implicit in traditional portfolio analysis, is the powerful insight it can provide in contexts where it is valid.
With very few exceptions, portfolios will have locally linear exposures about which the application of portfolio risk analysis tools can provide useful information.
Marginal Contribution
Risk is not additive. Volatility and VaR for different assets sometimes add, sometimes subtract. Because they are not additive, understanding where the risk comes from can be difficult. We would like to decompose or break down the overall volatility into contributions resulting from different risk factors or assets or subportfolios. (I focus on volatility, but pretty much everything applies equally to VaR.) We would like to be able to say something like: "The overall portfolio volatility is $291,300. Thirty percent comes from the bond, seventy percent from the equity futures."
We cannot use the individual asset volatilities because they simply do not add. In our example of the bond and the futures, the bond volatility is $130,800 and the equity futures is $230,800. They add to $361,600, not the $291,300 portfolio volatility. The problems mount for complex portfolios.
It turns out, however, that the overall volatility does decompose into additive components, but these components are not the asset volatilities; they are instead something we will call the marginal contribution or contribution to volatility (or VaR or risk). We go over the formulae in some detail in later chapters but here we only care that there is some way to consistently split the overall volatility into additive components resulting from individual assets or risk factors.
We have to be a little careful, however. Just because we have a formula to split the volatility into additive components does not mean those components actually tell us anything useful. For example, we could use a rule that arbitrarily splits the overall volatility equally among all assets or risk factors: With 10 risk factors we assign one-tenth of the volatility to each risk factor. This is a simple rule, but also useless.
The beauty of the marginal contribution is that it decomposes the portfolio volatility into meaningful components. It tells us something really useful: how small changes in individual risk factors or assets contribute to the change in the overall portfolio volatility. (Thus the term marginal, denoting small changes or changes at the margin.) We can say, "The overall portfolio volatility is $291,300. When all positions change by 1 percent, the overall volatility also changes by 1 percent or $2,913. Thirty percent, or $836, comes from the bond; 70 percent, or $2,077, from the equity futures." This is an incredibly useful view into the overall risk. The marginal contribution breaks the volatility down into components, components that are additive and tell us how the overall volatility responds to small percentage changes in positions.
The marginal contribution is particularly useful for large and complex portfolios. Such portfolios are particularly hard to understand, and they tend to change incrementally, with relatively small changes in individual positions.
We can examine the marginal contribution to volatility using our example of the U.S. Treasury bond and the CAC equity futures. Table 5.2 shows the individual asset volatilities and the marginal contributions to volatility. The total volatility decomposes into roughly 30 percent due to the bond and 70 percent due to the equity futures. This is a really useful decomposition because it tells us that even though the notional amount of the bond is much larger, the CAC futures contributes most of the risk, and changes in the CAC futures position would have much more impact on the portfolio risk than changes in the bond position.
Table 5.2 Volatility for Simple Portfolio with Contribution to Risk.

For this simple portfolio, the individual position volatilities might tell us much the same—the CAC is more important—but that would not work for large complex portfolios. In such a case, the marginal contribution comes into its own.
All-or-Nothing Contribution
The marginal contribution tells us just that—what is the contribution to the portfolio volatility for a small change, at the margin, to a position. There are times, however, when we want to ask a different question—what is the contribution due to the whole position? How would the volatility change if we completely removed a particular position? Conceptually, this is very simple, because it just means recalculating the portfolio volatility with one particular position removed. (There are more efficient ways to calculate it, which we discuss in later chapters.)
This is the all-or-nothing contribution—how much the volatility changes when the position is set to zero. In my career, I have generally found this less useful than other measures. The marginal contribution is for small changes only but is additive, so it provides a useful decomposition of the volatility. For large changes in position, I find that the best hedge position and best replicating portfolio, discussed next, provide more useful information. Having said that, the point of these tools is to build intuition and understanding around the portfolio risk, and different people will have different tastes and find different measures more or less useful.
Terminology
Before we leave the topic of marginal contribution and all-or-nothing contribution, we have to address an annoying problem: inconsistent terminology. This may seem trivial, but it actually serves as a serious impediment to better understanding and use of these tools, particularly marginal contribution. Some authors use the term marginal contribution as I do here, some use a different term, and still others, rather oddly, use marginal to refer to the all-or-nothing contribution. When a risk professional talks about marginal contribution, one needs to be careful about what he means. The lack of consistent terminology can lead to potential confusion and misunderstanding.
Table 5.3 provides a guide to the various terms used by different writers. Particularly confusing is that the RiskMetrics Group uses the word marginal for the all-or-nothing measure (even though the word marginal is commonly used to denote small changes at the margin and not large, finite changes) and uses the word incremental for the infinitesimal measure (arguably also at odds with common usage of the word incremental). Most of the literature uses the reverse terminology. Nor, unfortunately, are texts always clear in their explanation of the formulas or concepts.
Table 5.3 Terms for Contribution to Risk.



Source
Infinitesimal
All or Nothing




This book
Marginal contribution or contribution to risk
All-or-nothing contribution to risk


Litterman (1996)
Contribution to risk



Crouhy, Galai, and Mark (2001)
Delta VaR
Incremental VaR


Marrison (2002)
VaR contribution



Mina and Xiao/RiskMetrics (2001)
Incremental VaR
Marginal VaR


Jorion (2007)
Marginal VaR and component VaR
Incremental VaR


Reproduced from Exhibit 5.2 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.



Best Hedge Positions and Replicating Portfolios
The final tools for understanding portfolio risk are the related ideas of a best hedge and replicating portfolio. To start, consider a single asset currently in the portfolio. We can ask, "Using this one asset alone, what is the position that would best hedge the portfolio or optimally replicate the existing portfolio?" The best hedge for asset A is the position in A that, when combined with the existing portfolio, reduces the portfolio volatility as much as possible. The mirror of that position provides an optimal replicating portfolio.
This is much easier to see with an example. Continuing with the simple portfolio of a $20 million Treasury bond and €7 million CAC equity futures, Table 5.4 continues from Table 5.2. For the CAC futures, the "Best Hedge Position" is short €0.95 million. This says that if we want to choose an amount of CAC futures that provides the best hedge for the whole rest of the portfolio (in this case the "rest of the portfolio" is only the bond, but in general it would be a whole set of positions) we would need to be short €0.95 million.
Table 5.4 Volatility for Simple Portfolio with All-or-Nothing Contribution and Replicating Positions.

The terminology can be a little confusing. We have to be clear whether our best hedge position is net (netting off with the position in the original portfolio) or gross (new position that must be added to the existing portfolio). In Table 5.4, the €0.95 million is net—the absolute position that is the best hedge. In many ways, however, the gross position of €7.95 million—the new position that would have to added—is more useful. Short €7.95 million is the new hedge we would have to put on to hedge the existing portfolio (the original €7 million would then net out to leave the figure shown in the table). The mirror, long €7.95 million, would be the best replicating portfolio or the best representation of the full portfolio using only the CAC futures.
When we calculate the best hedge we would also like to know how good that hedge is—how much it would change the portfolio volatility. Table 5.4 shows the volatility at the best hedge position. For the CAC, this shows what the portfolio volatility would be if we added a short €7.95 million CAC position to the existing portfolio. We can then calculate how much the volatility is reduced, shown in Table 5.4 as a percentage.
We can calculate a "Best Hedge Position" for any single instrument in our portfolio. In Table 5.4, we have done that for both the bond and the CAC futures. The best hedge positions are valuable because they help us understand how the portfolio behaves, by comparing the portfolio with simple (single-asset) portfolios.
We can also go one step further, and ask, "What is the top best hedge?" Slightly clumsy wording, but the goal is to see what single position among all possible choices would be the best portfolio hedge. For our simple example, we have only two possibilities, but in general we would have more. Table 5.4 shows that the top best hedge position would be the CAC futures.
We can use this information and build intuition in two ways. First, we can say that short €7.95 million CAC futures would provide the best hedge to the existing portfolio. This could help in an emergency situation, if we needed to quickly reduce the risk but could only transact in a limited universe of liquid securities.5
Second, we can say the existing portfolio behaves most like long €7.95 million of CAC futures. This gives us a simple way to describe how the portfolio behaves. This helps build our intuition about the portfolio by summarizing it in a simple way. The idea can also be extended to multiple assets, for which the idea of a replicating portfolio can become even more useful.
Before turning from the topic of best hedges, we need to ask whether the best hedge positions tell us anything different from the marginal contribution. In the simple portfolio shown in Table 5.4, the best hedge does not really tell us anything more than the marginal contribution—the CAC futures has the highest marginal contribution and is also the top hedge.
For large, complex portfolios, however, marginal contribution and best hedges will tell us different information. To see how this can happen and what marginal contribution versus best hedges tells us, say that we added to our portfolio $40 million of five-year U.S. Treasury bonds. The CAC futures would still provide the biggest contribution to volatility, roughly double either of the other bonds. Tables 5.5 and 5.6 show the marginal contribution.
Table 5.5 Volatility for More Complex Portfolio with Contribution to Risk.

Table 5.6 Volatility for More Complex Portfolio with All-or-Nothing Contribution and Replicating Positions.

The marginal contribution for the CAC futures is so large because the CAC futures are more volatile than either the 10-year or the 5-year bonds. For either the 10-year or the 5-year bonds, a small change in position (either one on its own) contributes less than a small change in the CAC futures. But the CAC futures are no longer the top hedge—the top hedge is now one of the two bonds (see Table 5.6). This also makes sense. The two bonds usually move together so that, in a vague sense, they behave as the same security. The 10-year bond will act as a very good hedge to the 5-year bond and vice versa. Since this portfolio is more heavily weighted toward bonds than the portfolio in Table 5.4, the portfolio behaves more like a bond portfolio. The best replicating portfolio will be a bond rather than the equity, and, in fact, the 5-year is the best (with the 10-year almost as good).
Replicating Portfolio for Multiple Assets
We can easily extend the idea of a replicating portfolio to multiple assets. We might ask, "Using only five assets, what five assets would best replicate the existing portfolio?" The mirror of this replicating portfolio will, of course, be a hedging portfolio, one that if executed would best reduce the volatility.
The replicating portfolio can be a valuable tool because it gives a simple summary of the portfolio using a small number of assets. When the replicating or hedging portfolio gives a large reduction in volatility, then we know that the replicating portfolio gives a simple and a useful summary of the portfolio. Such a summary can serve both to help managers understand how the portfolio behaves and to communicate the makeup of the portfolio to outside constituencies without disclosing details of the underlying portfolio.
Our simple two-asset portfolio is so simple that we cannot really give an example of a multiple-asset replicating portfolio. The idea comes into its own for large and complex portfolios. We return in Chapter 10 to the idea of replicating portfolios and examine a more complicated portfolio.
5.7 Conclusion
This chapter has aimed to explain some of the basic tools used in quantitative risk measurement—volatility and VaR to measure the size of risk, and marginal contribution and best hedges to understand the composition of risk. These tools are important but they are not the only tools, and I have only outlined the intuition and have not laid out the technical foundations. The focus has been on how to use and think about these tools, with little or no attention directed toward how to estimate or calculate them.
Later chapters cover the details, the formulae, and the calculations necessary to produce the numbers. Chapter 8 concentrates on the formulae and technicalities behind volatility and VaR, while Chapter 9 applies these to a simple portfolio to make the concepts and calculations concrete. Chapter 10 turns to the portfolio tools of marginal contribution, best hedges, and so on.
Notes
1. The rule that 32 percent of the probability falls outside of ±1σ is strictly true for the normal distribution. For most reasonable P&L distributions we run into in finance, it will be somewhere on the order of 20 percent to 30 percent. In other words, under standard trading conditions, P&L will be within ±1σ roughly one day out of three or four or five.
2. Take the simple example of owning $1 million versus $100 million of a U.S. Treasury bond. The scale of the distribution will be very different, but the shape of the distribution will not change. The tail behavior—for example, the ratio of the VaR to the volatility—will be the same because it is determined by the market risk factor (say, the yield) and the shape of the distribution, not the size of the holding.
3. As Jorion (2007, 247) nicely expresses it: "The potential for losses results from exposures to the risk factors, as well as the distribution of these risk factors."
4. The approach we are using here—assuming the P&L distribution is normal and estimating the volatility—is the parametric approach that we discuss in more detail in Chapters 8 and 9.
5. We would, however, have to be very careful in using just a single asset to hedge the whole portfolio. A single asset will not usually provide a good hedge and we need to look carefully at how much the single hedge might reduce the volatility. We also have to be cognizant that the risk reduction potential during extreme circumstances could be different from during normal times. We would have to use the numbers in Table 5.4 with caution.









Chapter 6
Uses and Limitations of Quantitative Techniques
We have now finished our examination of Risk Management: how we should think about risk, the proper role of management within risk management, and some of the intuition behind the numbers. In the following chapters we turn to Quantitative Risk Measurement; we tackle the mathematics and the technical details. What is the definition of volatility and VaR, what is the contribution to volatility, how do we handle fat tails? We need to get these details right. That is not to say we always need the perfect answer—an approximation that tells us 90 percent of the answer today is better than the perfect answer that arrives too late. But we do need to be careful, understanding the technical details well enough to separate wheat from chaff and apply reasoned judgment and common sense to the technical details.
I hope the following chapters are valuable to a wide range of readers. They are, of course, primarily aimed at quantitative users whose job it is to understand and produce the numbers. But I do not want to dissuade less-technical readers from perusing some of the chapters. I have tried to provide the intuition behind the mathematics at the same time as giving the formulae. I have illustrated many of the quantitative tools with examples, particularly in Chapters 9 and 10, and these should be accessible to all readers.
Before turning to the technical chapters, however, it is worthwhile to review some of the limitations of quantitative techniques. Such a review rightly falls in this first section, firmly under risk management rather than risk measurement, because managers need to appreciate not only the power but also the limitation of quantitative techniques. Quantitative techniques work best in the hands of those who understand the techniques but who are also keenly aware of the limits and boundaries of what these techniques can provide. A deep appreciation of the limitations gives the user the confidence to rely on the techniques when appropriate and the good sense to turn elsewhere when necessary. Like most helpful tools, these techniques work well when used properly, and the key is to understand their limitations in order to avoid misusing them. The real risk to an organization is in the unanticipated or unexpected—exactly what quantitative measures capture least well.
6.1 Risk Measurement Limitations
Like any set of techniques or tools, risk measurement has definite limitations. This is not a problem; it is just the way the world is. A hammer is a useful tool, but it has limitations. It is good for pounding in a nail but not good for sawing a plank. Appreciating risk measurement limitations helps us understand when and where quantitative techniques are (and are not) useful. Failure to understand the limitations of risk measurement techniques, however, is a problem. Misusing the techniques in the face of limitations leads to mistakes, misunderstandings, and errors.
Models for Measuring Risk Will Not Include All Positions and All Risks
The models used to measure VaR, volatility, or whatever else will never include all positions and all risks. Positions may be missed for a variety of reasons. Perhaps some legacy computer system does not feed the main risk system, or some new system is not yet integrated. A new product may not yet be modeled, or someone may simply neglect to book a trade in a timely manner. A good and robust risk system will have processes and procedures for checking that all positions are captured and reporting those that are not. Nonetheless, there is always some possibility that positions are missed.
Likewise, the risk of positions that are included may not be properly represented. A complex derivative security may not be modeled correctly. Some product may have an unexpected sensitivity that is not captured by the risk system.
Missing positions and missing risks mean that the risk measures reported will not perfectly represent the actual risk. In reality, nobody should be surprised that a reported risk number is not absolutely perfect. It is an estimate, and like any estimate, it will be subject to errors—one possible error being that the positions or risks do not perfectly model the real world. A risk system should be viewed as a tool for summarizing and aggregating a large amount of information in a concise manner. It will not be perfect, and users should recognize that in using the results.
Risk Measures Such as VaR and Volatility Are Backward Looking
Quantitative techniques can tell us things about how positions and a portfolio would have behaved under past conditions—conditions that are ultimately derived from past experience. This is not a criticism, and contrary to what some commentators say, it is not a weakness of risk measurement techniques. It is simply the way the world is: We can seek to understand the past, but we cannot know the future. Understanding the past is terribly important because understanding current exposures and how they would have behaved in the past is the first step toward managing the future. As George Santayana said, "Those who cannot remember the past are condemned to repeat it."
The mistake here would be to think that these backward-looking tools measure the future. A manager needs to use judgment to interpret backward-looking information and incorporate it into the current decisions that will, together with randomness and luck, produce the future. Recognizing the backward-looking nature of the tools reminds us of the limitations and argues for care in using tools such as VaR and volatility.
VaR Does Not Measure the Worst Case
Statistical measures such as volatility, VaR, expected shortfall, and others provide summary information about the dispersion of the P&L distribution and will never tell us the worst case. VaR is often talked about and thought about as a statistically worst-case loss, but that is a misleading way to think. Whatever VaR level we choose, we can always do worse, and in fact, we are guaranteed to do worse at some point. Expected shortfall is useful relative to VaR exactly because it incorporates information on the losses worse than the VaR level, but expected shortfall does not change the fact that it is simply a summary statistic providing information about the distribution rather than about individual events that have not happened yet.
Litterman's (1996, footnote 1) recommendation for how to think of VaR is good: "Think of [VaR] not as a worst case, but rather as a regularly occurring event with which we should be comfortable" (p. 74). Thinking of VaR as a worst case is both intellectually lazy and dangerous. It is intellectually lazy because a worst case relieves one of the responsibility of thinking of the consequences and responses to yet worse outcomes. It is dangerous because it is certain that results will, at some point, be worse.
VaR, volatility, and other risk measures should be viewed as a set of measuring tools that tell us about the likely level of losses (the "regularly occurring event with which we should be comfortable"). When viewed this way, they push us toward thinking about what to do when something worse occurs, how much worse things could actually get and why, and how to react when things do get worse. Not only do they push us toward thinking about those possibilities, but they also provide quantitative information on how bad "worse" might be.
Quantitative Techniques Are Complex and Require Expertise and Experience to Use Properly
On the one hand, quantitative techniques used in modern risk measurement are indeed complex. On the other hand, risk management experts, like other experts, seem to make everything complicated. A balance needs to be struck. General managers and board members have a responsibility to understand the complex businesses they oversee. The financial business overall, not just risk measurement, is complex and is becoming more complex all the time. Managers at financial firms should take their responsibilities seriously and learn enough about the business, including risk measurement, that they can effectively use the available tools. In this day and age, lack of technical expertise cannot be an excuse for failing to use or understand risk measurement information.
Risk managers, however, have the corresponding responsibility to explain their techniques and results to nonexperts in a simple, concise, transparent manner. Most of the ideas behind risk measurement are simple, even if the details necessary to get the results are complex. Simple ideas, clear presentation, and concise description must be the goals for anyone engaged in measuring risk.
Quantitative Risk Measures Do Not Properly Represent Extreme Events
Quantitative risk measures do not catch extreme events. Experience does not. Imagination can try, but even that fails. Extreme events are extreme and hard to predict, and that is just the way life is. We need to recognize this limitation, but it is hardly a failure of risk techniques. To criticize the field of risk measurement because we cannot represent extreme events very well is just silly, like criticizing the sky because it is blue. Anybody who does not like extreme events should not be in the financial markets. Luck, both good and bad, is part of the world. We can use quantitative tools to try to put some estimates around extreme events, but we have to learn to live with uncertainty, particularly when it comes to extreme events.
Failure to appreciate our limitations, however, is a serious mistake. Overconfidence in numbers and quantitative techniques and in our ability to represent extreme events should be subject to severe criticism because it lulls us into a false sense of security. Understanding the limitations, however, does not mean throwing out the tools that we have at our disposal for estimating extreme events, even if they have limitations.









Part Two
Measuring Risk









Chapter 7
Introduction to Quantitative Risk Measurement
The first section of this book focused on how to think about risk and risk management. The second section of this book focuses on how to calculate risk.
The emphasis in Part One was on understanding risk and knowing the tools and what they tell us. We ignored the technical details of volatility and VaR because risk management is more than just quantitative measurement; risk management is managing people, projects, and institutions. In the end, risk management is not the numbers; risk management is what you do with them.
But the numbers are important. In fact, they are critically important. Without numbers, there is little we can do; without measuring risk, we cannot manage risk. Kendall and Stuart are right when they say that it is not the figures themselves but what we do with them that matters. But their maxim applies only after the numbers have been produced. Somehow, someway, we must produce the numbers that summarize, quantify, and measure the risk. It is that task, the task of measuring risk, to which we now turn.
The correct balance between hard numbers and soft management skills is never easy to pull off. The mathematicians among us demand rigor, consistency, and complete (and complex) models that account for every last detail. The managers among us demand simple answers delivered yesterday with no budget for programming, data, or personnel.
Part Two focuses on the mathematical, the quantitative side of the balance. The goal, however, is to present practical solutions backed by solid quantitative techniques. These chapters are meant to take the ideas discussed in Chapter 5 and give the mathematics and theory behind them. These chapters should serve as a reference for the risk professional needing the formulae for, say, the expected shortfall, assuming the P&L is distributed as a mixture of normals.
These chapters should also serve as a guide for the manager who has less technical training but nonetheless needs background on the pros and cons of estimating volatility or VaR parametrically (delta-normal) versus by Monte Carlo. The goal in risk measurement and management is to marry the technical expertise necessary for producing the numbers with the common sense, judgment, and experience required to do something sensible with them.
I have tried to be rigorous but in the end risk measurement is an applied field and it is more important to get a good answer today than to wait for the perfect answer next year. The objective is to have a good enough view of the theoretically perfect method to understand what shortcuts will work, when, and why. And what shortcuts will not work. Identifying the theoretically perfect solution, however, is important because it provides the goal toward which the organization should work. Building and implementing a risk system is never finished, and we always need to recognize where our systems and procedures can be improved and then work on implementing those improvements.
7.1 Project Implementation
Risk measurement is an applied science and, as such, we need to take the theoretical ideas and actually make them work. Make them work on computer systems, with complex and messy data, used by people with varying degrees of sophistication and knowledge.
Risk projects are as much about boring data and IT infrastructure as about fancy quantitative techniques. In building or implementing a risk management project, roughly 80 percent of the effort and investment is in data and IT infrastructure, and only 20 percent in sophisticated quantitative techniques. I cannot overemphasize the importance of data and the IT infrastructure required to store and manipulate the data. The bottom line is that if you don't know what is in the portfolio, it is hard to do any sophisticated analysis on the portfolio. For market risk, but credit risk in particular, good records of positions and counter parties is critical, and these data must be in a form that can be used.
Data
Data are always a big issue. Obtaining and using data is often more challenging than anticipated. Good quality and timely data, however, form the bedrock of any risk project.
Data can roughly be separated into external and internal data. External data are items such as history on market risk factors or security characteristics. (Is that bond the trader bought this morning maturing on the 15th or 31st of August?) We need to collect, clean, warehouse, update, and distribute these data.
Internal data are sometimes even harder than external data. One might think that collecting and using internal data would be easier because such data are under the control of the firm. But it never is easier. Positions and security details are squirreled away in obscure legacy systems that cannot talk to each other. New products and securities invariably start out in a spreadsheet. This is reasonable for one, two, or three trades. But if a new business succeeds, it becomes 100, 200, 300 trades, and suddenly things start to creak and break. The risk due to the product is suddenly important, the data are difficult to get and often unreliable, and there is never the budget or staff to build a proper valuation system and data warehouse until something blows up.
IT Systems
All the data need to be cleaned, stored, and manipulated. The ideas presented in this book need to be translated into computer code. Much as we might like, we cannot do all this on an HP 12-C and the back of an envelope. (Having said this, the ability to use those tools is critically important in getting the complicated systems to work properly.)
The cost and effort spent on acquiring and maintaining the IT infrastructure should not be underestimated, but neither should it stand as a significant impediment to implementing a risk project. Building data and IT infrastructure is not rocket science, and IT tools continue to improve. A system that would have taken many man-years to build a few short years ago can now be built in a short period with a small team. Nonetheless the programming and systems development requires good systems skills combined with a firm grasp of mathematics, statistics, and probability.
Daily Production
Numbers need to be produced daily and delivered accurately, reliably, on time, to the right people. The daily production process needs to be managed and implemented appropriately. The skills for doing this are different from the skills required to build the systems. It requires attention to detail but also the patience to manage the same process every day.
Summary
I want to highlight these issues of data, systems, and daily production but I do not provide a template or guidebook for them. In the end, these issues are as critical, maybe even more critical, than the theoretical and technical issues covered in the following chapters. A successful risk project depends on getting data, systems, and daily production right. These will often take the majority of resources.
7.2 Typology of Financial Institution Risks
Before turning to the details of measuring risk, I want to provide a high-level view of the types of risk faced by a financial institution.
We have defined risk as: the possibility of P&L being different from what is expected or anticipated; risk is uncertainty or randomness measured by the distribution of future P&L. In this sense, there is no distinction between, for example, market risk versus operational risk: Both encompass the possibility of gains or losses different from what is expected. Nonetheless, the sources, circumstances, and results of risk arising from different parts of a financial business are so different that there is considerable benefit from distinguishing between different risks within a financial organization.1
I discuss five major categories of risk:

1. Market risk
2. Credit risk
3. Liquidity risk
4. Operational risk
5. Other (legal and regulatory, business, strategic, reputational)

These are discussed in somewhat greater detail further on and in later chapters. I spend the most time on market and credit risk because these are the most amenable to mathematical analysis and thus have been the most studied. The areas of liquidity, operational, and other risks, however, should not be downplayed simply because they are less amenable to analysis with sophisticated mathematical tools. Remember, as we saw in Chapter 4, that many of the worst financial disasters can be traced to operational issues.
Market Risk
This is the first thing that comes to mind for financial institutions—price risk associated with market-traded securities, assets, and financial instruments. Financial institutions are in the business of trading or managing financial assets, and market risk is the possibility that prices of traded assets will differ from what is desired, expected, or planned.2 Marrison (2002, 4) uses losses in the stock market as the classic example of market risk, citing among other instances the fall in the Dow Jones index of 31 percent during one week in October 1987 and 23 percent on Black Monday, October 19.
Market risk may go under different names in differing circumstances. For example, in managing a portfolio, it may be tracking error relative to a benchmark. In a trading context, it may be basis risk, the relation between prices of two closely but not identical assets. For an options trade, it may be volatility risk. These are all, however, risks associated with market prices, and differ only in the particular circumstances in which they arise.
Market risk can usefully be categorized as depending on particular factors (see also Crouhy, Galai, and Mark 2006):

 Equity price risk
Associated with changes or variability in stock prices. This is often split into general market risk (associated with the level of the overall market or a market index) and idiosyncratic risk that is specific to the particular company.
 Interest rate risk
Risk associated with interest rates and fixed income (fixed interest) securities. This may appear alone, for example, with U.S. Treasury bonds, which are pure interest rate instruments, or combined with other risks, for example, a corporate bond that combines credit and interest rate risk. Interest rate risk will often be decomposed into risk from different parts of the curve. Furthermore, differences between similar but not identical instruments may be treated as basis risk or spread risk. Basis risk is not limited to interest rates but is common.
 Foreign exchange risk
Risk in assets or instruments (including cash) denominated in different currencies, different from the home currency of the portfolio or investor.
 Commodity price risk
Risk from changes in prices of commodities, whether traded in financial markets or only in physical markets. Commodity prices are not conceptually different from other asset prices. But as Crouhy, Galai, and Mark (2006) point out, the degree of variability can be different because of particular considerations such as concentration of supply in the hands of a few suppliers; the ease or cost of storage of the commodity itself; or perishability (for example, wheat) versus durability (for example, gold).
 Credit risk
Risk that change in the credit quality of the entity behind a particular instrument will affect the value of the instrument. A corporate bond is a prime example, an example for which the credit quality of the issuing company will determine the market demand and thus market price of the bond itself. Credit risk is usually classified separately from market risk but the line between credit and market risk is increasingly fuzzy. Many traded securities incorporate credit risk—the market price of a corporate bond will change as the credit standing of the issuing company varies. Furthermore, with the rise of credit derivatives, many previously nontraded credit risks have become traded.

Credit Risk
Credit risk is the risk that the value of a portfolio changes due to unexpected changes in the credit quality of issuers or trading partners. This subsumes both losses due to defaults and losses caused by changes in credit quality, such as the downgrading of a counterparty in an internal or external rating system. (McNeil, Frey, and Embrechts 2005, 327)
Credit risk ultimately arises from defaults—nonrepayment of promised amounts. Credit risk is listed as a factor under the market risk section discussed earlier but is also given its own classification here; the distinction between market and nonmarket credit risk is blurry. One distinction might be that before default, it is usefully treated as market risk, while the actual default is considered credit risk. Another distinction might be that when priced and traded in the market (as in a corporate bond or in a credit default swap), it is market risk, while when it is not traded (as in trade settlement), it is nonmarket risk. Yet another is that changes related to a specific company, such as downgrades and defaults, are credit risk while changes in general market sentiment, such as changes in an industry credit spread, are market risk.3 In the end, the distinction is difficult to make, and credit risk arises in so many and such varied forms that it is worth considering on its own.
Credit risk deserves its own section, even though the line between market risk and credit risk is blurry, because credit risk differs in some important respects from market risk. First, market risk focuses on internal entities within the financial organization, such as trading desks or portfolios. The measurement and management of market risk, for example, imposing limits, is done by trading desk or portfolio. Credit risk, in contrast, focuses on external issuers or counterparties. Limits, for example, are imposed by counterparty. Second, the horizon for credit risk is generally longer—market risk generally is short (days) while credit risk is longer (say one year). This raises some different modeling issues that deserve attention on their own. Finally, and most importantly, the modeling of credit risk is often dramatically different from market risk—market risk relies on observed market prices while credit risk must be constructed from the underlying processes generating defaults and other credit losses.
Analysis of credit risk traces back to commercial banks and their portfolios of loans. It is easy to see that a major, maybe the major, risk for a loan is the risk that the issuer will default, that is, credit risk. Credit risk, however, extends much further than simply loans, and it permeates finance. Some of the ways credit risk appears are:

 Single-issuer credit risk, such as with loans and bonds. The default of the issuer means nonrepayment of the principal and promised interest on a bank loan or bond.
 Multiple-issuer credit risk such as with securitized mortgage bonds. Such bonds are issued by a bank or investment bank but the underlying assets are a collection of loans or other obligations for a large number of individuals or companies. Default of one or more of the underlying loans creates credit losses.
 Counterparty risk resulting from contracts between parties, often over-the-counter (OTC) derivatives contracts. OTC transactions, such as interest rate swaps, are contracts between two parties, and if one party defaults, it may substantially affect the payoff to the other party. Other contracts, such as letters of credit, insurance, and financial guarantees, will also entail credit risk if there is potential for loss upon default of one party.
Settlement risk. Associated with delivery and settlement of trades, the possibility that one side fails to settle a trade after being paid.4

Credit risk measurement has grown in sophistication and importance over recent years. The financial crisis that hit in 2008-2009 was driven by credit issues, particularly those related to subprime mortgages in the United States. Furthermore, the past few years haves seen tremendous growth in new financial instruments specifically dependent on credit such as credit default swaps (CDS).
Credit risk will generally be asymmetric. It is asymmetric in two senses. First, the distribution of P&L for a credit portfolio will be strongly skewed or asymmetric, with a probability of large losses but not a similar probability of large gains. Second, exposure to credit risk is generally present only when a position has positive value or is an asset. When a position is positive and the counterparty defaults, the firm (bank) loses, up to the full value of the position. When a position is negative and the counterparty defaults, however, the bank cannot walk away from the obligation.
Liquidity Risk
Liquidity risk is very important but one of the more difficult risks to conceptualize and measure. Liquidity risk actually comprises two distinct concepts—asset liquidity and funding liquidity. These two can interact but it is necessary to keep them conceptually distinct, and it is unfortunate that they both go under the rubric of liquidity risk.
Funding liquidity risk, also called cash-flow risk, refers to the ability to raise or retain the debt for financing leveraged positions, meeting margin or collateral calls, or meeting fund redemptions. This issue is particularly critical for leveraged portfolios using short-term debt (such as repurchase agreements) that are subject to margin calls.
Asset liquidity risk refers to the ability to execute transactions in the necessary size at the prevailing market price in a timely fashion. Asset liquidity will differ, sometimes dramatically, across instruments, market conditions, and at different times. The markets for some assets, say, G-7 government bonds or currencies, are so deep and developed that most trades can be executed with minimal impact on market prices. Other markets, say, for an esoteric derivative or local currency emerging market bond, may be active during normal times for moderate-size trades, but effectively shut during market disruption.
Funding and asset liquidity risk can interact in a lethal combination. Adverse price movements, or even a turn in market sentiment, may induce margin calls or cancellation of loans, putting pressure on funding liquidity. If the portfolio does not have sufficient cash or sources of new funding this will require the selling of assets. If the positions are large relative to normal market transactions or concentrated in illiquid securities, poor asset liquidity may mean sales can be done only at very disadvantageous prices. The fall in prices may trigger further margin calls, then further asset sales, leading into a death spiral.
Jorion (2007, 333) summarizes it well:
Funding liquidity risk arises when financing cannot be maintained owing to creditor or investor demands. The resulting need for cash may require selling assets. Asset liquidity risk arises when a forced liquidation of assets creates unfavorable price movements. Thus liquidity considerations should be viewed in the context of both the assets and the liabilities of the financial institution.
...
During episodes of systemic risk...liquidity evaporates.... Liquidity risk probably is the weakest spot of market risk management systems.
Operational Risk
Operational risk is crucial but difficult to measure. Indeed, I argue that given the current state of understanding, the focus should be as much on managing as on measuring operational risk. We may not be able to measure it very well, but it is so critical it cannot be ignored and must be managed nonetheless.
Even the definition of operational risk is difficult and in flux. The general industry consensus (incorporating guidance from Basel regulators) defines "Operational risk [as] the risk of loss resulting from inadequate or failed processes, people, and systems or from external events" (Jorion 2007, 495). This is a balance between older narrow definitions (risk arising from operations or trade processing) and overly broad definitions that include everything not market or credit risk.
Quantitative measurement and statistical analysis of "inadequate or failed processes, people, and systems" is difficult. Nonetheless, there can be substantial returns to a disciplined approach, even if it is somewhat more qualitative than that applied to, say, market or credit risk. The Basel Committee on Banking Supervision (BCBS) (2003) outlines a framework for measuring operational risk that looks particularly useful.
Operational risk is so important because operational failures appear central to many financial disasters. Lleo (2008, quoting Jorion [2007]) summarizes the situation well: "Jorion (2007) drew the following key lesson from financial disasters: while a single source of risk may create large losses, it is not generally enough to result in an actual disaster. For such an event to occur, several types of risks usually need to interact. Most importantly, the lack of appropriate [operational] controls appears as a determining contributor: while inadequate controls do not trigger the actual financial loss, they allow the organization to take more risk than necessary and also provide enough time for extreme losses to accumulate."
Much can be accomplished in controlling operational risks by improving process and procedures to reduce frequency and severity of errors, reduce costs, and improve productivity. As Jorion (2007, 505) points out, "The key to controlling operational risk lies in control systems and competent managers. BCBS (2003) provides common-sense advice." One aim is to make processes such that it is easy for people to do the right thing and hard to do the wrong. Furthermore, improved process and procedures can both control operational risk and increase profits by reducing costs, for example, by making costs insensitive to trade volumes. This argues against McNeil, Frey, and Embrechts (2005, 464), who state, "An essential difference between operational risk, on the one hand, and market and credit risk, on the other, is that operational risk has no upside for a bank."
Other Risks
I group other risks together. These other risks include such things as legal and regulatory risk, general business risk, strategic risk, and reputational risk. These are clearly important but I do not discuss them in detail.
7.3 Conclusion
We now turn to examining risk measurement in detail. Chapter 8 focuses on the tools that form the foundation of quantitative risk measurement: volatility and VaR. Chapter 9 then applies these tools to a particularly simple portfolio, the U.S. Treasury bond and CAC index futures introduced in Chapter 1. The goal of Chapter 9 is to work through a simple example in enough detail to make the ideas come alive. Although Chapters 8 and 9 concentrate on market risk, nearly all the ideas, ideas about how to conceptualize the P&L distribution and how to summarize and estimate the distribution, apply equally well to credit and other forms of risk.
Chapter 10 focuses on risk reporting and portfolio analysis tools. These tools help us to move from static monitoring of the risk (the strength of volatility and VaR) to active management of risk. Volatility and VaR help us calibrate what the scale of potential losses is and they tell us about the spread in the P&L distribution. But managing risk requires understanding the sources of risk and how changes in the portfolio are likely to alter our exposure to losses. As such, Chapter 10 is probably the most important and useful chapter in this book.
Chapter 11 turns to credit risk—risk ultimately resulting from the actual or potential default (bankruptcy), or nonperformance of contracts. In many ways, the quantitative analysis of credit risk is no different from market risk—we care about the P&L distribution and can use the volatility or VaR to summarize that distribution. Certain characteristics of credit risk, however, require that we treat credit risk on its own. First, obtaining the P&L distribution for credit risk often requires detailed credit modeling. For market risk, the market risk factors are generally known and observed, and our task is to translate or map the distribution of market risk factors to our firm's particular holdings and securities. For credit risk, in contrast, there are no data on past defaults of our particular loan or bond—if the loan had already defaulted, we would not own it. We have to build the distribution of profits and losses from scratch, based on often-complex models.
The second reason we want to treat credit risk separately is to emphasize the often-skewed nature of the P&L distribution. Market risk is most often relatively symmetric, looking something like the bell-shaped curve of the normal distribution. Credit risk is more often highly skewed with a long left tail—a distribution that looks like Figure 8.3, with many large losses and not many large gains. It is often claimed, incorrectly, I believe, that this is due to the characteristic that credit losses often consist of many small gains and a few large losses. In fact, the skewed shape of the credit P&L distribution is more often due to the tendency of credit losses to move together. When credits go bad, they go bad together. This may result from, say, all loans being sensitive to the overall economy and thus going bad together when the economy goes into recession. Whatever the reason, however, this argues that for credit we need to pay particular attention to correlation and co-movement across credits. Unfortunately, co-movement across credits is probably the hardest part of measuring credit risk.
Chapter 12 focuses on liquidity and operational risk. These areas are not as developed, mathematically, as market and credit risk. There is, however, considerable work being done in these areas.
Although this second section of the book focuses on quantitative techniques and tools, we need to remember that all the mathematics is focused on actually managing risk. The focus must be on how these tools and techniques add to prudent business management. In this sense, quantitative risk measurement should be treated just like accounting or market research—an activity and set of tools integral to managing the business.
Notes
1. Crouhy, Galai, and Mark (2006, Appendix to Chapter 1) lay out a nice typology of risk, and Marrison (2002, 4 ff) outlines various risks faced by banks.
2. Although market risk is usually focused on traded securities or assets, it can also include risk for untraded or thinly traded securities when prices can be modeled or inferred.
3. See Marrison 2002, 226-227.
4. Also called Herstatt risk after Bankhaus Herstatt, a small German bank that in 1974 failed in the middle of the New York Stock Exchange trading day, after receiving payments for FX trades but before delivering payments to settle the trades.









Chapter 8
Risk and Summary Measures: Volatility and VaR
As argued in earlier chapters, risk measurement is the measurement of the profit and loss (P&L) distribution. This chapter introduces the standard quantitative techniques used to analyze the P&L distribution. By standard, I mean those most widely discussed in the literature and applied in the industry. In practice this means volatility and value at risk. Remember, however, that measuring risk is only the first step in managing risk, and there is more to measuring risk than just VaR.
Value at risk (usually referred to as VaR) is the most widely used and quoted quantitative risk measure. Much of this chapter focuses on introducing VaR and demystifying its uses and abuses. VaR, however, is only one measure among many that help to quantify and understand risk. Whatever tools we use, the important goal is to understand the distribution and potential variability of the P&L.
There are many good texts that cover VaR and quantitative risk measurement. Starting with the most nontechnical and intuitive, there is Crouhy, Galai, and Mark (2006) Chapter 7, and Crouhy, Galai, and Mark (2001) Chapter 5. Marrison (2002) has a concise introduction at the end of Chapter 5 and then devotes Chapter 6 to methods for VaR estimation. Jorion (2007) is a broad and detailed reference with Chapter 5 covering the basics, and additional topics such as the estimating and application of VaR covered throughout the book. McNeil, Frey, and Embrechts (2005) is the most advanced and covers technical issues in great detail.
8.1 Risk and Summary Measures
Remember the definition: Risk is the possibility of P&L different from expected or anticipated: variability in outcomes, uncertainty, or randomness. A distribution or density function is how we represent random possibilities, so the P&L distribution is the central focus for risk measurement.
Figure 8.1 shows the distribution (technically the density function1) of P&L from a hypothetical bond strategy with many possible outcomes, some giving high profits, some large losses. The horizontal axis shows the P&L, ranging from large losses to the left up to large profits to the right. The vertical axis shows the probability of any particular P&L. The pattern shown, with approximate symmetry of gains and losses and with higher probability in the middle, occurs frequently in the financial markets.

Figure 8.1 Profit or Loss from Hypothetical Bond Strategy Based on Figure 5.1 from A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

If we knew the full distribution of P&L, we would know most everything there is to know about the risk of the particular trade or portfolio. But it is rare that we will know or use the full distribution. We will usually be satisfied with some summary measure because the full distribution is too complicated to easily grasp or we simply want a convenient way to summarize the distribution.
Summary measures for distribution and density functions are common in statistics. For any distribution, the first two features that are of interest are location on the one hand and scale or dispersion on the other. Location quantifies the central tendency or some typical value, while scale or dispersion quantifies the spread of possible values around the central value. For risk measurement, scale is generally more important than location, primarily because the dispersion of P&L is large relative to the typical value.2
The summary measures that we use are often called risk measures: Numbers that summarize important characteristics of the distribution. We must remember, however, that although summary measures are extraordinarily useful, they are to some degree arbitrary, more useful in some circumstances and less useful in others. Risk itself is not a precise concept and depends on investor preferences; different investors may view the risk of the same investment differently. Because the property we are trying to measure (risk) is somewhat vague, the summary measures themselves will, of necessity, also be somewhat arbitrary. The statistician Cramer's remarks regarding location and scale measures are appropriate here: "Each measure has advantages and disadvantages of its own, and a measure which renders excellent service in one case may be more or less useless in another" (Cramér 1974, 181-182). Using these quantitative measures requires common sense, experience, and judgment.
Volatility and VaR
The most familiar measures of location and scale are the mean and standard deviation (commonly called volatility and commonly denoted σ). An example of a distribution and its mean and standard deviation are shown for a hypothetical yield curve strategy in Figure 8.2. Panel A shows a lower dispersion distribution (less spread out), and Panel B shows a higher dispersion distribution (more spread out). The mean is zero for both, but the standard deviation is higher for the distribution in Panel B.

Figure 8.2 Location and Scale for P&L from Hypothetical Yield Curve Strategy—Mean and Standard Deviation Reproduced from Figure 5.2 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

Volatility (or standard deviation) is the average of squared deviations from the mean. Say we have the distribution or density shown in Figure 8.2. The value of the P&L is P—the profit or loss for the period (day, week, month). This is the horizontal axis, it is a random variable, and can take values ranging from losses (negative P) to profits (positive P). The curve shown in Figure 8.2 is the density, written as g(P)dp, and gives the probability that P will be between P and P+dP. The volatility is:

If instead of the distribution or density shown in Figure 8.2 we have a set of discrete data, with P&L observations Pi, then the volatility is:

The volatility is effectively an average of deviations from the mean. The greater the dispersion around the mean, the larger the volatility will be.
Volatility is an ideal summary measure in many cases, particularly when the distribution is symmetric, the focus is mostly on the central part of the distribution, and the extremes in the tails are either well-behaved or not of primary interest. When the distribution is nonsymmetric (skewed) or if the focus is particularly on the tails of the distribution, volatility may have drawbacks. For example, given a distribution such as that shown in Figure 8.3, which is nonsymmetric and has a fat left tail, volatility (standard deviation) may not provide a good representation of the left tail—that is, the risk of large losses.

Figure 8.3 Distribution where Volatility May Be Less Useful Summary Measure Reproduced from Figure 5.5 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

The standard deviation (volatility) is one dispersion measure relevant for risk measurement. The standard deviation is well known from statistics and is widely used, but it is by no means the only summary measure we could use. Value at risk, or VaR, is another popular summary measure.
VaR is a quantile of the distribution and is summarized graphically in Figure 8.4. A quantile is characterized by two numbers: first, a probability level Z, defined by the user, and second, a resulting level of profit or loss Y. The definition for VaRZ is: the P&L level Y such that there is a probability Z that the P&L will be worse than Y and a probability 1 - Z that it will be better than Y. The P&L is measured over some fixed time horizon, for example, one day. In Figure 8.4, we can see that the VaR5% is the point on the horizontal axis chosen so that the probability, the area under the curve below Y, is 5 percent. The idea behind VaR is simple: the level of loss is specified in such a way that a worse loss happens with a predefined probability.3
The idea behind VaR is quite simple: the level of loss such that worse happens with a predefined probability. In Figure 8.4, we have chosen the probability level Z to be 5 percent, so that we require the area in the left-hand tail of the distribution (the probability of a bad P&L) to be 5 percent. For this particular example, the P&L level Y that fits is -$215,000, and so the VaR5% = -$215,000. The area at and to the left of -$215,000 is 5 percent, so we have fixed the probability of losing $215,000 or worse at 5 percent.

Figure 8.4 Five Percent VaR for P&L from Hypothetical Yield Curve Strategy Reproduced from Figure 5.3 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

The definition of VaR can be expressed mathematically as:
(8.1) 
This is simply the Z quantile of the P&L distribution.4
If we knew the true distribution of P&L we could simply calculate the VaR for any Z using equation (8.1). In practice, we do not know the true distribution, but let us pretend for illustration that we do know the distribution of P&L and that it is normal.5 In such a case, the calculation is easy:

where
μ = mean of the normal distribution
σ = standard deviation (volatility) of the normal distribution.
Working with the distribution shown in panel A of Figure 8.2 and 8.4, the mean is zero (μ = 0) and the volatility is $130,800. For a normal distribution, P[Standard Normal ≤ -1.64] = 0.05 so that

The volatility (standard deviation) and the VaR each summarize the dispersion of the distribution in their own way. For nice, symmetrical, well-behaved distributions, such as those shown in Figures 8.2 and 8.4, they can be used almost interchangeably. In Figure 8.2, we could ask what is the probability that the P&L is less than the standard deviation—what is the probability to the left of −1σ? For a normal (Gaussian) distribution, the probability will be 15.9 percent. In other words, we could think of the volatility as the 15.9%/84.1% VaR. Alternatively, we can note that for a normal (Gaussian) distribution, the probability to the left of −1.64σ is 5 percent so that -1.64σ is the 5%/95% VaR. For a normal distribution, volatility and VaR are direct transforms of each other and we can thus move easily from volatility to VaR and back, and in this sense, they can be used interchangeably.6
It is important to remember that volatility and VaR are merely summary measures and that each may be more or less useful, depending on circumstances. There is nothing magical about either one, although we might sometimes be tempted to think so. They merely summarize the distribution, albeit in somewhat different ways—either by looking at an average of deviations from the mean (volatility) or at a point on the tail of the distribution (VaR). Indeed, for well-behaved symmetrical distributions, they can be used almost interchangeably, and for a normal (Gaussian) distribution, we can easily convert from one to the other.
Relation between Volatility and VaR
VaR and volatility are the two measures most often used to summarize risk and they do have different strengths and weaknesses for characterizing the risk of a portfolio. In some cases, however, they can be used interchangeably, and it is important to understand the connection between them. In fact, in many practical applications, volatility is calculated first, and then a value for VaR is inferred from that volatility.
The Z% VaR is defined (see equation (8.1)) as the P&L level Y that satisfies:

For VaR, one chooses Z and then calculates the implied value of Y (which will vary with the particular distribution of P&L). Common choices when using VaR are Z = 5 percent and Z = 1 percent, but any choice will work.
For volatility, in contrast, the value of P&L Y is determined by the distribution of P&L and the definition of the standard deviation:

For a specific distribution, however, there is a relationship between volatility and VaR: We can always calculate the value of Z corresponding to Y = σ and consider volatility as one particular choice for probability level (Zσ) and thus as simply one choice for VaR. The actual value of probability Zσ will depend on the particular distribution of P&L. For a normal distribution Zσ = 15.9 percent, and for many distributions one might use in finance Zσ is not too far from 15 percent. (For example, for the t-distribution with n = 6, discussed further on, Z = 12.2 percent, while for a mixture of normals with 99 percent σ and 1 percent 5*σ, Z = 13.5 percent). On this basis, one can often treat volatility simply as the VaR for a rather high value of Z, generally on the order of 12 to 15 percent. And it is on this basis that, in what follows, I sometimes discuss VaR and volatility as if they were interchangeable.
When using VaR in actual applications, VaR is often derived from the volatility. As just discussed, there is a relationship between volatility and VaR for each specific distribution. This means that knowing the volatility (and the distribution) we can calculate the VaR for any level of Z: Simply choose Z and then calculate Y from equation (8.1). As a practical matter, this means that for a specific distribution, the VaR will be a multiple of the volatility. For the normal distribution, these multiplicative factors for some popular probabilities are shown in Table 8.1 (together with the dollar VaR for the P&L distribution in Figures 8.2 and 8.4).
Table 8.1 Various Combinations of Probability (Z) and P&L (Y) for Normal Distribution.

The relationship between volatility and VaR is useful when the P&L distribution is symmetric or close, but far less useful for a nonsymmetric distribution, such as Figure 8.2.
Subadditivity—Volatility, VaR, and Expected Shortfall
Although VaR is a popular summary risk measure, like any summary measure it has strengths and weaknesses. One issue deserves particular mention. Diversification is a key concept in risk measurement and a risk measure should reflect diversification: The risk of a portfolio should be the same or less than the risk of the subcomponents. In other words, if we have some measure for the risk of Portfolio A, call it Risk(Portfolio A), we would want that measure to satisfy:

This is technically called subadditivity. Interestingly, VaR does not always satisfy this condition.7
McNeill, Frey, and Embrechts (2005, 241 ff) provide a simple example in which the VaR is not subadditive. Consider a set of corporate bonds that may default. Each bond costs $100 up front and in one year there will be one of two outcomes:

 No default, with profit $5 (the original $100 plus $5 interest). The probability is 98 percent.
 Default, with loss $100 (the bond is totally written off). The probability is 2 percent.

Portfolio A consists of purchasing $10,000 worth a single bond. This portfolio has 98 percent probability of a profit of $500, and a 2 percent probability of a loss of $10,000:

Portfolio B consists of 100 separate bonds, each worth $100, with each bond independent of the other bonds. This portfolio is also worth $10,000 initially, but the profit and loss will be binomially distributed:

Portfolio B is clearly less risky than Portfolio A. Portfolio A is a single bond with which we lose everything upon default. Portfolio B is diversified across 100 independent bonds, and this diversification intuitively makes Portfolio B less risky.
The density functions for the two portfolios are shown in Figure 8.5.8 For Portfolio A, there are only two possible outcomes, −$10,000 and +$500. (Note that the density for a single bond will look the same, but the two outcomes are −$100 and +$5.) For Portfolio B there are a range of outcomes, centered around $290.

Figure 8.5 Density Function for Profit and Loss for Two Portfolios of Defaultable Bonds Note: This is the density for the number of defaults. Portfolio A is $10,000 worth of a single bond with a probability of default of 0.02, a loss of $10,000 upon default, and a profit of $500 with no default. Portfolio B is 100 bonds, each $100, with independent probability of default of 0.02, loss of $100 upon default, and gain of $5 with no default. The density for Portfolio B is binomial.

Figure 8.5 reinforces the idea that Portfolio B is less risky. The profit for Portfolio B is spread out but there is almost no chance of a really bad outcome. For Portfolio A, the probability of losing $10,000 is 2 percent, while for Portfolio B, there is no possibility of losing $10,000; the probability of all 100 bonds defaulting together would be:

Portfolio A is effectively a portfolio with 100 versions of the same bond, and Portfolio B is a portfolio with 100 bonds with identical characteristics but bonds that default independently, which thus provides diversification. Portfolio B should be less risky. Subadditivity says that, at a minimum, the risk of a portfolio of multiple bonds should be no worse than the sum of the risk of the bonds on their own. Thus the risk for each portfolio (A and B) should be no worse than 100 times the risk of an individual bond.
Let us now calculate the 5%/95% VaR for an individual bond and for the portfolios. For a single bond, the $5 profit actually is the 5%/95% VaR. To see this, remember that the 5%/95% VaR is defined to be that profit such that there is a 5 percent probability of a worse profit. For a single bond, there is a 2 percent probability of profit less than $4.99, and 100 percent probability of profit less than $5.01; the VaR has to be between $4.99 and $5.01, and it is in fact $5. For Portfolio A, the same argument shows that the VaR is $500.
For Portfolio B, the density is more spread out. The probability of having n or more defaults and the associated profit is shown in Table 8.2. This table shows the distribution function corresponding to the density function in Figure 8.5. There is a 1.5 percent probability of six or more defaults and a 5.1 percent probability of five or more defaults. This means the 5%/95% VaR is five defaults, or a loss of $25.
Table 8.2 Number of Defaults and Profit for Portfolio B of Defaultable Bonds.
Note: This is the number of defaults for Portfolio B, consisting of 100 bonds, each with an independent probability of default of 0.02, a loss of $100 upon default, and a gain of $5 with no default. The P&L distribution will be binomially distributed: $500 - 105 × Binomial[100,0.02]. The table shows the number of defaults, profit, and probability of observing that number of defaults (or more) for entries around the 5%/95% VaR.



Number Defaults, n
Profit ($)
Prob. ≥ n defaults




6
−130
0.015


5
−25
0.051


4
80
0.141


3
185
0.323


The VaR for Portfolio A is 100 times the VaR for a single bond ($500 versus 100 × $5). The VaR for Portfolio B, however, is much worse: −$25 ($25 loss) instead of +$500. Subadditivity would say the VaR of Portfolio B should not be worse than $500, but it is much worse at −$25.
We are now in the odd position of having Portfolio A with a VaR of $500 and Portfolio B with a VaR of −$25. This makes no sense because Portfolio B is more diversified, is clearly less risky, and so should not have a worse VaR. The problem is the nonsubadditivity of the VaR: the VaR for Portfolio B is worse (at −$25) than 100 times the individual bonds (100 × $5 or +$500).
In contrast to VaR, volatility is subadditive. So also is another risk measure, called expected shortfall, or ES, which is closely related to VaR. Expected shortfall is (for most cases) the expected P&L given that the loss exceeds the VaR, also called the conditional VaR:

For comparison, we can calculate the VaR, the volatility, and the expected shortfall for the two portfolios in our simple example, and for a single bond:

 Portfolio A: VaR $500, Volatility $1,470, Expected Shortfall −$3,700.
 Portfolio B: VaR −$25, Volatility $147, Expected Shortfall −$68.50.
 Single bond: VaR $5, Volatility $14.70, Expected Shortfall −$37.

We can see that VaR gives nonintuitive results. For Portfolio A, the VaR, the volatility, and the expected shortfall are all 100 times the values for a single bond, and this makes sense. For Portfolio B, the volatility and expected shortfall are both better than 100 times the single bond, and thus show Portfolio B as less risky, reflecting diversification. This also makes sense. Where things break down is the VaR for Portfolio B, which is substantially worse than 100 times the VaR for the single bond. The problem is that VaR is not subadditive. In this case, VaR is nonsubadditive because of the very skewed P&L distribution. The distribution for the diversified portfolio is substantially less skewed.
Essentially, ES takes into account losses that are worse than the value-at-risk level, while the VaR measures only the loss at the value-at-risk level and does not depend on the distribution below that point. This means that expected shortfall will differentiate between two distributions with the same VaR, but one of them has a much fatter tail, as shown in Figure 8.6.

Figure 8.6 Two Distributions, One with Thin and One with Fat Tails, Both Having the Same VaR but Different Expected Shortfall
Reproduced from Figure 5.6 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

Time Scaling or Time Aggregation
So far, we have taken the P&L distribution for some fixed time period, say one day, and volatility and VaR apply only for that time period. We often want to measure the distribution over longer (or shorter) time periods, and it would be very useful if we had a simple rule for moving from shorter-period to longer-period distributions.
The question, then, is how does the one-day (shorter-period) distribution translate or scale to a longer period? This is referred to as the problem of time scaling or time aggregation. Particularly, how do summary measures such as volatility and VaR scale with time?
The question is simple to pose because the P&L for multiple days will simply be the sum of P&L for individual days:

When daily P&L is normally distributed, the sum will also be normal. This simplifies time aggregation, since the form of the distribution does not change when moving to longer time periods. If we further assume that each day has the same variance and that P&L is independent from one day to the next, then the scaling rule is the well-known square root of time:

This results from knowing that for the sum of independent variables, the variance scales linearly:

so that volatility scales as a square root.
This simple rule is widely used and very useful, but the assumptions on which it is based are not always appropriate. The assumption of independence is reasonable for financial time series.10 Removing the assumption of normality makes little difference, since distributions such as the t-distribution and simple mixture of normals will scale in the same way.11 The assumption that variances are the same every day, however, is more likely to be violated, and there does not seem to be any simple approach to relaxing this assumption (see Jorion, 2007, 133).
The discussion so far has implicitly assumed that the time aggregation is over a relatively short period, such as a week or a month, when mean growth and discounting are not important. Aggregation over long time periods such as a year are discussed in a later chapter under the topic Economic Capital, where the question of the level of equity capital and bankruptcy is considered.
8.2 Comments Regarding Quantitative Risk Measures
I want to highlight some points that I think are not sufficiently elaborated in standard treatments of quantitative risk measures, VaR in particular, or that are misunderstood by many users. I am not criticizing or rejecting VaR. There are many critiques of VaR, many which are not justified. Some commentators have said that it is useless and even a fraud. In my experience, views usually fall at one of two extremes:

1. Pro-VaR: It is the silver bullet that answers all risk-measurement questions.
2. Anti-VaR: It is at best useless, more often outright misleading or worse.

As often happens, the truth is closer to a synthesis of the two views: VaR can provide useful information but has definite limitations. When properly understood and appropriately applied VaR provides information and insight, but when VaR is misapplied or misunderstood it can certainly be misleading.
Standard Trading Conditions versus Extreme Events
There are two related but somewhat divergent uses of summary risk measures such as VaR and volatility:

1. To standardize, aggregate, and analyze risk across disparate assets (or securities, trades, portfolios) under standard or usual trading conditions.
2. To measure tail risk or extreme events.

Risk measurement theory and texts usually focus on extreme events and VaR, but risk measurement in practice focuses as much on standard trading conditions and summary measures other than VaR. Paying heed to risk under standard trading conditions is important for two reasons. First, comparing and analyzing risk across disparate assets and complex portfolios provides information necessary for understanding and managing trading results under standard trading conditions (which is, by definition, most of the time), and can also provide valuable clues to performance under more extreme conditions. Second, focus on VaR alone for measuring tail risk or extreme events can be as misleading as it is enlightening. Measuring tail events is very difficult and delicate, and blind reliance on a single statistic is a mistake.
Used in the first sense (measuring risk under standard trading conditions), risk summary measures are an aid to understanding and comparing different assets, trades, or portfolios: providing a user with important and useful information on how much a trade might be expected to make or lose, even when the user is not intimately familiar with a particular security or market. The focus is on normal or usual trading conditions. In this context, volatility is used as often or more than VaR (a quantile of the distribution). This use of risk measures is relatively straightforward, robust, and generally noncontroversial. I also think it is widely underappreciated as a tool in risk measurement.
The need to compare across disparate products was apparently the driving force for the original development of VaR at JPMorgan—in response to chairman Dennis Weatherstone's need to understand risk across the various divisions and products of the bank.12 Weatherstone came from the FX trading desk and had a good intuitive grasp of risk, but needed some way to quickly and easily compare risks with which he was not so intimately familiar. VaR and volatility are good tools for such comparisons. They are also good as tools for aggregating disparate risks into an overall number. But VaR or volatility is no substitute for a true understanding of risks. Consider JPMorgan again: "Weatherstone had been a trader himself; he understood both the limits and the value of VaR. It told him things he hadn't known before. He could use it to help him make judgments" (Nocera 2009). VaR is a valuable tool for comparing across products but no substitute for true understanding and good judgment.
Used in the second sense of measuring tail or extreme events, VaR is sometimes referred to as the "statistically worst-case loss." This is a horribly misleading idea, since no matter how one chooses VaR, one can be virtually assured that something worse will eventually happen. By their nature, tail events are rare, and so measuring tail events is inherently difficult and open to large errors and uncertainty. As a result, when applied in this second sense, VaR must be used carefully and any conclusions treated with care.
These two uses of summary risk measures can never be precisely separated, but the conceptual differentiation clarifies some of their uses, benefits, and limitations. For usual or normal trading conditions, the statistical and quantitative techniques are pretty standard, and the interpretation of results relatively straightforward. For example, using volatility and assuming normality or linearity of the portfolio may be satisfactory when considering the central part of the distribution, meaning that simple and computationally efficient methods will often be appropriate. Measuring tail events, in contrast, is delicate, and the appropriate statistical and quantitative techniques often difficult. Normality is generally not appropriate, requiring more complex statistical assumptions and more sophisticated quantitative, numerical, and computational techniques. The inherent variability of tail events is generally higher than for the central part of the distribution, and uncertainty due to estimation error and model error is larger. As a result, the estimation of VaR or other summary measures for tail events is inherently more difficult, and the use and interpretation of results is more problematic.
Miscellaneous Comments Regarding VaR
The first point to highlight is that VaR is sometimes referred to as the "worst-case loss" or "statistically worst-case loss," and as mentioned before, this is a horribly misleading idea. By definition, there is a probability that losses will be worse than the VaR. Furthermore, no matter what loss one might choose as the "statistically worst-case loss," one can be assured that sometime, somewhere, it will be worse.13 In reality, VaR is best thought of as measuring outcomes that, while out of the ordinary, are still reasonably likely and not worst-case possibilities. The most reasonable statement I have seen comes from the excellent paper by Litterman (1996, footnote 1) "We think of this [VaR measured as one-day, once-per-year or Z = 1/250] not as a 'worst case,' but rather as a regularly occurring event with which we should be comfortable."
In fact, using the word worst in relation to VaR shows a profound misunderstanding of probability. I would even go so far as to argue that "statistically best-case loss" is a better term because the VaR is closer to the best rather than the worst that we should see on bad days. To see why I say this, let us look at an example. Consider the 1%/99% VaR for our $20 million bond holding. The VaR is roughly −$304,200. We should expect to see this loss roughly 1 out of 100 days. But what will we actually see on the worst out of 100 days? What will the loss actually be on that day? We cannot say with certainty, but we can calculate the distribution; the appendix at the end of this chapter gives the formulae (Appendix 8.1, "Distribution of Extremes"). If we are willing to assume that the P&L itself is normally distributed, then the P&L on that 1 out of 100 days will be worse than −$304,200 with 67 percent probability and better than −$304,200 with only 37 percent probability. In other words, −$304,200 is closer to the best we will see on that bad day than the worst. The VaR tells us a lot, but it never tells us the worst we should expect.14
The second point is that VaR is one among various summary measures of the distribution, and as with any summary measure, there may be much hidden from sight. Examples of characteristics that a particular summary measure may not highlight would be the degree of asymmetry or fat tails in the distribution, about which more is discussed further on. VaR is one among many measures of dispersion, sometimes useful, sometimes less so. Like any measure of dispersion, the choice of VaR is to some extent arbitrary and it should be used and judged on its efficacy. There will be cases when a measure of dispersion other than VaR, or some other approach to summarizing the distribution of P&L, proves more useful than slavishly using VaR.
Third, VaR is usually used as a measure of the tail of the distribution. As there is large variability and uncertainty in tail events, VaR, when used to measure tail events, must be employed with special caution. As a corollary, estimating VaR can be challenging precisely because it is a tail measure. The lower the probability of Z (for example, going from 5%/95% to 1%/99%), the more difficult estimating VaR is. Tail events are by their nature rare and consequently hard to measure.
Fourth, any estimate of VaR is based on how the portfolio would have behaved under certain historical conditions. Parametric VaR approximates history by a parametric distribution; Historical VaR directly uses the historically observed distribution for risk factors; Monte Carlo uses an assumed distribution of risk factors, based in some way on history. These show how the portfolio would have behaved in the past and may not predict how the portfolio will behave in the future. I do not mean this as a criticism. Although VaR is often criticized as backward-looking, doing so misses the point that understanding how the portfolio would have behaved under past circumstances provides valuable information and insight. Understanding the past is the first step toward understanding what might happen in the future. Remember George Santayana's words.
The final point to highlight is that putting a number on risk, with VaR or any other measure, does not reduce the underlying variability, only our ignorance about that variability. Nonetheless, by measuring and assigning a number, we often fall into an "illusion of certainty" (to borrow a term from Gigerenzer 2002, 38): a human tendency to believe in the certainty of outcomes and underestimate or misestimate the importance of chance. Assigning a number to VaR provides an example of which our intuition may not fully anticipate the degree of variability. With the 1%/99% VaR, we might expect to see 1 day out of 100 with a P&L worse than the VaR. But the world is chancy and we may or may not see exactly one day and even if we do see one, that day may or may not be near the 1 percent VaR. If we examine 100 trading days, there is a good chance that we will see no days worse than the VaR (37 percent chance) or two days worse than the VaR (26 percent chance), and a decent chance we will see a P&L substantially worse than the VaR. Just because we know the VaR does not mean the world will cooperate and deliver exactly that P&L. The world is chancy and we should never forget Benjamin Franklin's maxim: "...[I]n this world nothing is certain but death and taxes" (letter to Jean Baptiste Leroy 1789).
8.3 Methods for Estimating the P&L Distribution
So far we have been talking about the P&L distribution as if we already knew the distribution, as if somebody gave it to us. This is clearly not the case. We have to estimate the P&L distribution, and that is never easy. The process is usually complex, dependent on the particular circumstances, and subject to the details of the actual portfolio. The basic idea, however, is simple: we want to trace out an estimate of the P&L distribution, or enough of the distribution to allow calculation of the appropriate summary measures, usually the volatility or the VaR.
Our first inclination for estimating the portfolio P&L might be to simply measure the price history of our past portfolio. If our portfolio is just a single asset, say the 10-year U.S. Treasury bond introduced in Chapter 1, and we haven't changed that portfolio for a long time, then maybe this would be feasible. Real-world portfolios, however, include multiple trades and instruments that change over time. The history on the past portfolio usually will not represent how the current portfolio might behave. The behavior and interaction of many instruments, many not in the portfolio in the past, needs to be incorporated.
It is generally useful to think of the P&L as resulting from two components:

1. External market risk factors.
2. Positions—that is, the firm's holdings and the security characteristics that determine the sensitivity to risk factors.

The exposure to risk factors and the distribution of risk factors is combined to obtain the distribution of P&L.
It is fruitful to split the P&L into market risk factors versus positions and their sensitivity to risk factors for two primary reasons. First, multiple positions will usually depend on a single risk factor (or a small group of risk factors). This reduces the dimensionality of the problem. The P&L will depend on a relatively small number of risk factors rather than a large number of positions.
FX forward contracts are a good example. A portfolio might contain a one-week, one-month, and two-month forward contract on USD-EUR (all having aged from originally traded three-month contracts). These forward contracts will all depend, first and most importantly on the spot USD-EUR FX rate.15 The spot FX rate will be the risk factor that determines the prices and price changes of these three, plus all other USD-EUR contracts.16
The second reason it is fruitful to separate market risk factors versus positions is that portfolio positions and consequent exposures can and often do change over time, sometimes dramatically. Market risk factors are external to the firm and conceptually different from portfolio positions. Positions and consequent exposures are generally under the control of the firm and may change frequently. Market risk factors are generally independent of a firm's actions, and distributions of these factors generally do not change dramatically over short periods (such as days or weeks).17
There are three widely used methods for estimating volatility and VaR: parametric (also called linear, delta normal, or variance-covariance), historical simulation, and Monte Carlo. The three methods differ in both assumptions about and estimation of market risk factor distributions and in how exposures to risk factors are treated, but they share many common attributes and limitations. Most importantly, they all share the conceptual distinction between market risk factors and portfolio positions.
There is considerable debate on the pros and cons of alternate methods, but one should never forget that we can only estimate the P&L distribution; we will never have the true volatility, VaR, or whatever. The alternative methods are merely alternative estimation strategies and should be judged on their usefulness. In different circumstances and for different portfolios, one will be better than another, but there is no single right answer.
Outline for Generating P&L Distribution
As pointed out earlier, estimating the P&L distribution and implementing quantitative risk measurement techniques is usually complex. Nonetheless, estimation usually conforms to the following four steps in one way or another. These steps are shown figuratively in Figure 8.7 (see also Jorion 2007, 107):

1. Asset/Risk Factor Mapping—Calculate transformation from individual assets to risk factors
2. Risk Factor Distributions—Estimate the range of possible levels and changes in market risk factors
3. Generate P&L Distribution—Generate risk factor P&L and sum to produce the portfolio P&L distribution
4. Calculate Risk Measures—Estimate the VaR, volatility, or other desired characteristics of the P&L distribution

The following sections examines these steps in detail, contrasting the three approaches (parametric, historical, and Monte Carlo) where appropriate. In the next chapter, we develop a simple portfolio and estimate the P&L distribution by each of the three methods.

Figure 8.7 Outline of Methodology for Estimating VaR or other Characteristics of the P&L Distribution

Step 1—Asset/Risk Factor Mapping
By asset/risk factor mapping, I mean the transformation from the actual positions and securities in the portfolio to market risk factors. The mapping allows us to calculate P&L as a function of market moves. As pointed out earlier, risk factors are treated as the fundamental variables because multiple positions or securities usually depend on a single risk factor or small group of risk factors.
Conceptually, the most straightforward mapping or transformation is to build a full valuation model of the instrument or security as a function of the market risk factors. This is sometimes complex but it does provide the framework with which we can understand the how and why of mapping. It also, in a real sense, provides the gold standard against which we want to compare alternative methods.
There are, of course, times when we cannot use the valuation model approach. The various methods for translating from assets to risk factors, ordered roughly from most accurate to least accurate are:

 Valuation Model—Using a pricing model to obtain asset price as a function of the risk factor. Valuation model mapping may use either sensitivities (deltas or linear approximation) or full revaluation. Sensitivities generally work well for assets for which P&L is close to linear, as for most bonds and equities, and is used for parametric estimation. Full revaluation is necessary for highly nonlinear products such as options, and is often used for historical or Monte Carlo estimation.
 Statistical or empirical factor mapping—Using the empirical relation between an asset and an index, as in using the beta for an equity.
 Other mapping or binning—Map each actual asset to some set of standardized assets.
 Proxy—Replace an asset with a proxy.

Valuation Model
We might represent the pricing model (assuming for simplicity only a single risk factor) as the pricing function f():

Different assets that depend on the same risk factors may well have different pricing functions. For example, both a bond and a bond option depend on yields and the forward curve, but the functional form of the pricing model will be quite different.
To understand the valuation model approach, let us revisit the example of USD-EUR FX forward contracts mentioned earlier. Say our portfolio is U.S. dollar-denominated, but we have a €100 million euro cash balance and also one-month, two-month, and three-month forward contracts to sell $100 million forward (maybe put on to hedge the purchase of euro-denominated bonds in a USD-denominated portfolio). In this case, we will be at risk to changes in the value of the euro cash balance and the present value (PV) of the forward contracts. The market risk factor for all of these will be the spot USD-EUR FX rate, X.
The dollar value of the euro cash balance is:

The PV of forward contract i (PV in millions of dollars, for a contract that sells for $100 million at an agreed rate of Fi) would be:





This formula tells us how the U.S. dollar market value will change as the spot FX rate, X, changes.
We have accomplished two things with this mapping or transformation. First, we have reduced the cash balance and three contracts to a single market risk factor, the spot FX rate X. (We are ignoring the small risk with respect to the USD and EUR interest rates, but we will return to this shortly.) Any additional forward USD-EUR contracts, of whatever maturity, will similarly map to spot FX risk. For large portfolios, this reduction to a smaller number of market risk factors is quite important.
Second, we have illuminated some of the dependency in risk across different positions. The euro cash balance and the forward contracts have the same risk, and the mapping to the market risk factor X makes this explicit.
This example also highlights another aspect of full valuation modeling. The FX forwards depend on the U.S. dollar and euro interest rates, the same rates that bonds, swaps, or interest rate futures will depend on. The valuation model method, when done properly, will ensure that similar risks across diverse assets (in this case, interest rate risk for FX forwards and for bonds or swaps) will be properly, and automatically, captured.18
Another example of when valuation modeling works well is bonds or swaps priced off a yield curve. A yield curve would be built out of a relatively small number of bonds (for example 1-, 5-, 10-, and 30-year bonds). All similar fixed income instruments could be valued off the yield curve and would depend on the 1-, 5-, 10-, and 30-year bond yields as market risk factors. We could now use those yields as risk factors for all bonds, whatever the bond maturity.19
For the U.S. 10-year Treasury bond introduced in Chapter 1, and which we revisit again in Chapter 9, such a yield curve model is particularly simple and transparent. The 10-year bond depends only on the 10-year yield, and so the yield curve model effectively collapses to the standard yield-to-maturity formula. The 10-year yield is the market risk factor and the price-from-yield function is the valuation model.
Parametric estimation will use a linear approximation to the valuation model—sensitivities or deltas. The P&L is approximated by a Taylor series expansion; a linear function of risk factor changes:


Sensitivities are the derivative of the pricing function:

This is just using the first derivative of the valuation function to approximate changes in the P&L as a function of changes in risk factors. For our FX forward example, the sensitivity with respect to the spot FX rate will be:

It is convenient in this case to measure the sensitivity to a percent change in the FX rate, which would give:

For a contract to sell $100 million 30 days forward with F = 1.42, X = 1.40, ru = 0.43 percent, and re = 0.26 percent, the sensitivity will be about $0.9857 million per 1 percentage point change in the FX rate (say, going from 1.393 to 1.407).
Historical and Monte Carlo estimation may use either linear (sensitivities) or full valuation, but commonly uses full valuation. For full revaluation, the function fi(rf) itself would be used and fi(rf) would be reevaluated for each required value of the risk factor rf.20
Examples of common assets, risk factors, and pricing models are shown in Table 8.3.
Table 8.3 Common Assets Transformed through a Pricing Model.



Asset
Risk Factor(s)
Pricing Model




Government bond
Spot rates or forward rates
PV off forward curve


Interest rate swaps
Spot rates or forward rates
PV off forward curve


FX forward contract
Spot FX rate and Interest rate levels or differentials
PV of future currency amounts translated to base currency and discounted to today


Options
Underlying asset (could be interest rates, equities, commodities, FX rates) and interest rates for discounting
Option pricing model such as Black-Scholes


Credit default swaps
Credit spreads or default rates, interest rates for discounting
Stochastic default process, uncertain cash flows discounted back to today


Corporate bond
Credit spreads or default rates, interest rates for discounting
Stochastic default process, uncertain cash flows discounted back to today


Using a pricing model as described here is in a sense the most accurate way of determining the P&L or sensitivity, since it uses a mark-to-market model. It is also generally the most closely aligned with the valuation and analytics done at the micro level by the units managing the risk.
Statistical or Empirical Factor Mapping
There are many situations in which there is not a formal pricing model, but the relationship between an asset and risk factor is well understood from empirical or statistical analysis. The primary example would be single stocks (equities). Market practice, supported by considerable academic research (the capital asset pricing model, or CAPM), is to consider the percentage or logarithmic return on the equity to be made up of market and idiosyncratic components:

The market is usually taken to be a broad market index such as, for the United States, the S&P 500, the Russell 3000, or the Wilshire 5000. The residual is idiosyncratic, uncorrelated with the market and assumed uncorrelated with other stocks. The P&L for a collection of stocks will then be:

Common practice is to use rm as the risk factor, with sensitivity (∑iωiβi). There is in fact a second factor, the idiosyncratic component ∑iωiεi. If one is willing to assume the idiosyncratic components are jointly independent and normal, then this is the sum of independent normals, and one can either calculate its variance from the βi and the equities' variances, or, for a large portfolio, assume it is negligible. Alternatively, one can estimate the variance from history, estimate its correlation with risk factors other than rm, and use it as a risk factor on its own.
This example of equities assumes a single factor—rm. One can take the analysis another step, along the lines of arbitrage pricing theory, and include multiple market factors, either observable factors such as industry-specific return, or statistically derived factors derived from something like principal components analysis. The return for asset i is then

The P&L for a collection of assets will be:

Statistical factor mapping is most commonly applied to equities, but may also be applied to credit spreads (where factors might include rating and industry).
Other Mapping or Binning
Many texts (for example, Marrison 2002, 131 ff; Jorion 2007, 283 ff; Mina and Xiao/RiskMetrics 2001, 43 ff) discuss cash flow mapping or binning for fixed-income instruments. The idea and terminology is mostly an artifact of the RiskMetrics methodology.
In RiskMetrics fixed-cash-flow (nonoption) instruments are priced by discounting the cash flows using a yield curve model built assuming linearly interpolated zero rates. For calculations, a limited set of yield curve points (for example, 1mth, 3mth, 6mth, 1yr, 2yr, 3yr, 4yr, 5yr, 7yr, 9yr, 10yr, 15yr, 20yr, 30yr) are used as risk factors. As described so far, this falls under the "Pricing Model" approach discussed earlier, in that an instrument is modeled as its constituent cash flows and each cash flow is priced by discounting off a yield curve.
For parametric VaR calculations in RiskMetrics, however, only pricing models for a limited set of cash flows vertices (those corresponding to the yield curve points) are actually used. As a result, pricing an arbitrary cash flow off the yield curve and calculating sensitivity to yield curve points is not feasible. Instead, an arbitrary cash flow is mapped or binned to the two nearest neighbor vertices. Early RiskMetric implementations binned cash flows by requiring that the PV of the original and binned cash flows be equal, and that the volatility of the original cash flow be the linear interpolation of the nearest vertices. A more recent implementation bins by requiring the PV of the original and binned cash flows be equal, and that the sensitivity to the nearest zero rates at the nearest vertices be equal. (To ensure equality of PV, a third cash flow with maturity zero, that is, cash that has no sensitivity to interest rates, is also used.)
Binning as outlined here is an artifact of the RiskMetrics implementation, but more generally, binning may be a useful shortcut to quickly evaluate a new or unusual instrument. In such a case, the new instrument is represented by a combination of existing instruments. Changes in the PV are most important, since the volatility or VaR (and the P&L distribution) are usually calculated as the change from current market value. Thus, the important criterion for representing a new instrument as a combination of existing instruments is to ensure that the sensitivity to risk factors is the same or as close as possible. Matching PVs, as is done in RiskMetrics, seems to be of decidedly secondary importance.
Proxy
There are some cases in which a mapping from asset to risk factor or (more often) the risk factor itself is just not available. An emerging market country making its first bond issue in the global markets would be a prime example, as there would be no history on the distribution of the relevant risk factor (bond yields for that country). In such a case, one must make informed guesses about the relationship between the asset and a risk factor, and what risk factor might provide a reasonable substitute or proxy for the true risk factor. Some proxy risk factor must be used.
Step 2—Risk Factor Distributions
We now turn from the portfolio positions (mapping assets to risk factors) to focus on the market risk factors. The goal is to estimate the range of possible levels and changes in market risk factors. We need to trace out the distribution of changes in market risk factors, basically to trace out the curve such as in Figure 8.1 at the beginning of this chapter.
It is at this point where the differences between estimation approaches (parametric, historical, and Monte Carlo) start to become more apparent.
Parametric (Delta Normal or Variance-Covariance)
For parametric estimation, we assume that the distribution of market risk factors is jointly normal. This considerably simplifies estimation of the market risk distributions. For a single risk factor, the only parameters are the mean (often assumed zero for risk measurement purposes) and the volatility or standard deviation. The shape of the distribution is the bell-shaped curve that we have been considering in examples such as in Figure 8.1 or Figure 5.3. The exact spread or dispersion is set by the volatility.
When we consider multiple assets, then we introduce the covariances or the correlations across assets, but again, this is a relatively easy parameter or set of parameters to estimate. At least easy relative to the task of estimating the joint distribution if we do not assume normality.
Estimation of means, variances (volatilities), and covariances is one of the most-studied problems in statistics, so there is a huge literature on the problem. In the simplest case, the volatility is estimated by the sample standard deviation of historical changes:


The covariances (between risk factors rf1 and rf2) are estimated by:

In other words, as long as we are willing to assume that changes in risk factors are indeed normal, we can estimate the full distribution just by calculating a relatively small set of numbers. The downside, of course, is that risk factor changes may not, indeed usually are not, normal—the tails are fat relative to the normal. For studying the central part of the distribution, however, normality may be a reasonable approximation.
Historical
The parametric approach just discussed estimates the parameters of the risk factor distributions, not individual risk factor measurements. (Given the assumed functional form for the distribution, we can generate observations if need be.) The historical approach, in contrast, uses historical observations as finite-sample realizations of the risk factor distributions. The parametric approach assumes a particular distributional functional form and so comes up with the complete distribution (conditional on the assumed functional form, of course). The historical approach uses history as a set of observations from the distribution, generated by nature.
In summary, the idea behind the historical approach is very simple: use the historical observations of risk factor changes as the distribution. Go back in history and simply use the actual observations. We see in the next section, where we discuss generating the P&L distribution, that the historical approach is just a form of Monte Carlo estimation, with the distribution of market risk factors being the empirical distribution rather than a fitted or chosen distribution.
Conceptually, the historical approach is very simple. There are, however, some subtle points regarding revaluation using historical risk factors that we discuss shortly and again in Chapter 9. Furthermore, one point that we need to emphasize is that historical estimation as often practiced will not produce highly reliable estimates. A typical application might use one or two years of historical data (on the order of 300 to 600 observations). For any Monte Carlo application, this would be considered an unreasonably small number of draws from the joint distribution. Yet we do exactly that with historical estimation. In using the results of a historical estimation with relatively few historical observations, we need to keep in mind that the estimates may have a high degree of uncertainty.
Monte Carlo
Like the historical approach, the Monte Carlo approach uses a finite set of observations as a finite-sample realization of the risk factor distribution. The difference between the historical and the Monte Carlo approach is in how the finite set of observations is generated. For the historical approach, we take a set of historical observations. For the Monte Carlo approach, synthetic or simulated observations are generated by some numerical algorithm (pseudo-random number generator). The random numbers are generated according to some given distribution.
The term Monte Carlo refers to how the finite set of risk factor observations is generated, not to how the underlying distribution is chosen. The underlying market risk factor distribution may be (and in fact often is) assumed normal. In that case, the estimation of the parameters of the distribution is exactly the same as for the parametric approach. Whatever method is used for choosing the underlying joint distribution, the essence of the Monte Carlo approach is that a finite set of observations for the market risk factors are generated by a computer algorithm.
Comment Regarding Daily Correlation—The Closing-Time Problem
Any realistic portfolio contains multiple assets. The covariance, correlation, or co-movement between assets is a major, sometimes the major, determinant of the overall portfolio risk. We want to be careful estimating the correlations.
We will generally use closing prices, and changes from one close to another, to measure risk factor changes. For daily changes, we calculate the change from yesterday's close to today's close. For weekly changes, we calculate the change from last week's close to this week's close.
When considering assets traded in different time zones, an important issue arises—the closing times will be different. Closing prices for equities in London will be about 11 a.m. and for equities in New York will be about 4 p.m., both measured by New York time. Any news or events that occur between 11 a.m. and 4 p.m. will be reflected in today's New York closing price but not London's. Such events will show up in tomorrow's London prices.
The noncontemporaneous closing times will induce spurious correlations between price changes measured in the two venues. The same-day correlation between London today and New York today will be reduced, and the lagged correlation between London tomorrow and New York today will be raised. Both effects will be spurious.
This effect can be large. Say that the true same-day correlation between two equities, one traded in London and the other in New York, is 0.95, and that the true lagged correlation is zero. In other words, the two equities move together with a high degree of certainty but there is no serial correlation. Were we to measure this correlation using daily changes in closing prices, we would observe a same-day correlation of 0.75 and lagged correlation of 0.20. If the equities were traded in Japan and New York, the same-day correlation would fall to 0.44 and the lagged correlation would be 0.51. These are substantially misleading results.21
The effect will be the same whether we use the parametric or historical approach. For the parametric approach, we see the effect directly in the calculated risk factor correlations. For the historical approach, it is hidden in the implied correlations across risk factors, but it is real nonetheless.
One simple expedient is to avoid using changes in daily closing prices and use weekly changes instead, scaling down to daily later, if necessary. For weekly data, the closing-time effect is substantially reduced relative to daily data—for London and New York with a true correlation of 0.95, the measured same-day correlation would be 0.92 (versus daily 0.75), and the lagged correlation would be 0.03 (versus daily 0.20).
Step 3—Generate P&L Distribution
This is where the differences between the three approaches come to the fore. Generating the P&L distribution with the parametric approach is fast and simple, while the historical and Monte Carlo approaches often require substantial computer resources. The payoff for the extra effort is the prospect that the results will better reflect reality.
Like any modeling effort, however, there are two issues we must keep in mind. First, we need to evaluate the trade-off between the potential for more realistic results versus the time, effort, and complexity required to produce those results. Sometimes a simple but quick result is better than a sophisticated result that we don't have. Second, we need to critically evaluate the inputs to our models. Just because a particular approach has the potential to better reflect reality does not mean it will. It depends on the quality of the inputs—junk input to a perfect model still produces junk output.
Parametric (Delta Normal)
The parametric approach is also called the delta or linear method, delta-VaR, delta-normal, or variance-covariance method. We use linear sensitivities (first derivatives).22
As an example, a bond position might be represented by sensitivities to points on the yield curve; $20M of the 10-year U.S. Treasury bond we have been working with has a sensitivity of $18,300/bp to a fall in the 10-year yield.
For the distribution of risk factor changes, we assume a specific parametric functional form that depends on a small number of parameters. The normal distribution (multivariate joint normal distribution) is almost always chosen as the distribution because of its mathematical tractability.
Individual risk factor P&Ls will be a linear function of changes in risk factors:

When the risk factors are normally distributed, the risk factor P&Ls will also be normal because a linear function of a normal variate is still normal.
The overall portfolio P&L is the sum of the individual risk factor P&Ls:




The individual risk factor P&Ls will be jointly normal. The portfolio P&L, the sum of the individual factor P&Ls, will also be normal because the sum of normal variables is itself normal.
The parameters of the joint normal distribution are the mean and the variance-covariance matrix. The linearity of positions and joint normality of the market risk factors combines to produce normality of the total P&L distribution. Normality of the total P&L means that the distribution is fully described by the mean (often set to zero) and the variance-covariance matrix.
If we denote the risk factor sensitivities by the column vector:

and the variance-covariance matrix by:

then the portfolio variance can be easily calculated as:

Parametric VaR is easy computationally. The approach has advantages and disadvantages (see also Marrison 2002, 104 ff):
Advantages:

 Fast relative to Monte Carlo or historical simulation.
Allows calculation of marginal VaR or contribution to risk as discussed in Chapter 10.

Disadvantages:

 Does not handle risk from nonlinear positions (such as options) well.
 The parametric assumption is often not appropriate (normality in particular), for example, in measuring tail events (although see the discussion further on under Alternative Distributional Assumptions).

Historical and Monte Carlo
The parametric approach we just discussed estimated the parameters of the portfolio P&L distribution, not individual measurements or P&L observations. There was no necessity to estimate individual points of the distribution because, when we assume the distribution is normal, estimating the volatility (standard deviation, variance) tells us everything. For a normal distribution with volatility σp (and mean either zero or estimated), we can calculate any probability statement we want: VaR for any probability level, expected shortfall, whatever we may require.
The historical and Monte Carlo approaches are quite different from the parametric approach: we start with a risk factor distribution represented by a finite set of observations. (To reiterate, the difference between historical and Monte Carlo is in how the finite set of risk factor observations are generated.) We will use the finite set of risk factor observations to generate a finite set of P&L observations. This finite set of P&L observations is now a finite-sample realization of the P&L distribution.
Our finite set of risk factor observations represent a set of days (or weeks, or months, whatever is our appropriate period). For historical estimation, they really are days. For Monte Carlo, they are synthetic or simulated days. Whatever the case, each day has a complete set of risk factor observations with which we can calculate the P&L for every risk factor.
For each day, we calculate the P&L for each instrument, using the mapping or binning from Step 1. We may use either sensitivities (linear approximation) or full revaluation (using the full valuation model). If we use sensitivities, then the P&L for a particular day for risk factor 1 would be:

If we use full revaluation, then

There is a slightly subtle point regarding the changes in risk factors. The "base value risk factor 1" is the value today. To this, we apply the "change in risk factor 1." For pure Monte Carlo, everything is pretty straightforward. For historical simulation, it can be confusing because we want to apply the change that occurred in the past to today's base value. If we are doing our analysis as of January 27, 2009, one of our historical days might be the change from April 5-6, 2007. We generally do not want to calculate

The PV as of April 5 might have been very different from the PV as of today. What we are trying to estimate is the effect of changes on today's portfolio. We return to this issue in Chapter 9 when we go over a specific example.
Once we have the P&L for a particular day due to all risk factors and instruments, we can add the separate P&Ls to obtain an estimate of the overall portfolio P&L for that day.
We then repeat the whole process for each of our days and so end up with a finite sample of synthetic observations for the portfolio P&L. This set of observations is our finite-sample estimate of the P&L distribution.
Step 4—Calculate Risk Measures
Once we have our P&L distribution, we can use it to calculate things like the volatility or VaR. Again, the approaches differ. With parametric estimation, we have a parametric distribution, while for historical and Monte Carlo, we have a finite sample.
Parametric (Delta Normal)
The P&L distribution will be normal and we simply use well-known results from statistics to make probability statements. We estimated the volatility in Step 3, so we know what that is. For example, if the volatility were $130,800 (as it is for the $20 million U.S. Treasury bond introduced in Chapter 1), the value of the VaR for various levels of probability Z would be as shown in Table 8.1 (reproduced here).
We can do more with the P&L distribution, as is discussed in later sections.
Table 8.1(Repeated) Various Combinations of Probability (Z) and P&L (Y) for Normal Distribution.

Historical and Monte Carlo
Here the P&L distribution will be the finite set of P&L observations. We can graph the observations as a histogram. We will more often want to calculate the volatility and the VaR. The volatility will be the usual formula:

The VaR will be the empirical quantile. The P&L sample will be n observations {xi} arranged in ascending order, {x1 ≤ x2 ≤ ... ≤ xn}. The Z percent VaR is the Z percent quantile. If n × Z is not an integer, then there is a unique quantile equal to the observed value xμ+1 where μ = integer smaller than n × Z. If n × Z is an integer, then the quantile is indeterminate between xnz and xnz+1. For example, with Z = 0.01 and n = 101, n × Z = 1.01 and the 1 percent quantile is the first observation. With n = 100, n × Z = 1 and the 1 percent quantile is indeterminate between the first and second observations.
Summary
Different approaches have different advantages and disadvantages. Table 8.4 summarizes the pros and cons in tabular format.
Table 8.4 Comparison of Parametric, Historical Simulation, and Monte Carlo Approaches.
Reproduced from Exhibit 5.1 of A Practical Guide to Risk Management,©2011 by the Research Foundation of CFA Institute.


8.4 Techniques and Tools for Tail Events
The most difficult and vexing problem in quantitative risk measurement is trying to quantify tail or extreme events.
Tail events are important because large losses are particularly significant and VaR is often used to quantify the likelihood of large losses. The probability level Z is chosen low, say 1 percent or 0.1 percent, to produce a low probability that losses will be worse than the VaR and a high probability that they will be better. Figure 8.8 shows how a low level for Z implies that the VaR measures the left-hand tail of the distribution. Using VaR in this manner requires focusing on the tail of the distribution.

Figure 8.8 VaR for Low Probability Level Z Reproduced from Figure 5.9 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

Jorion (2007) has a discussion of copulas in Section 8.3, alternate parametric distributions in Section 4.2.6, and an introduction to extreme value theory in Section 5.4; Marrison (2002, 157 ff) discusses some alternative approaches for tail events; Beirlant, Schoutens, and Segers (2005) discuss extreme value theory; McNeil (1999) has an introduction to extreme value theory in risk management, and Embrechts, Klüppelberg, and Mikosch (2003) wrote a text devoted to extreme value theory. McNeil, Frey, and Embrechts (2005) provide excellent coverage of many of the quantitative techniques, including alternative distributions, copulas, and extreme value theory.
Measuring tail events is difficult for two fundamental reasons. First, tail or extreme events are by their nature rare and thus difficult to measure. By definition, we do not see many rare events, so it is difficult to make reliable measurements of them and to form judgments about them. Second, because of the scanty evidence, we are often pushed toward making mathematical or statistical assumptions about the tails of distributions (extreme events), but simple and common assumptions are often not appropriate. Most importantly, the assumption of normality is often not very good far out in the tails. Although rare events are rare, they do occur, and measurements across different periods, markets, and securities show that in many cases extreme events occur more often than they would if the P&L behaved according to the normal distribution in the tails. This does not mean the normal distribution is a bad choice when looking at the central part of the distribution, but it does show that the normal distribution can be a poor approximation when examining extreme events.
One example, among many, of the nonnormal nature of extreme events is given in the beginning sections of Beirlant, Schoutens, and Segers (2005). They look at the number of large negative returns for the Dow Jones Industrial Average for the period from 1954 to 2004. There were 10 log returns of −5.82 percent or worse (out of 50 years, or roughly 12,500 days). These events are shown in Table 8.5. Let us assume that the volatility (standard deviation) of log returns is 25 percent annualized, or 1.58 percent daily.23 Using this estimate for volatility, we can calculate how many standard deviations away from the mean each move is, which is also shown in Table 8.5 in the column "No. Sigma (Z-score)."
Table 8.5 Ten Largest Down Moves of the Dow, 1954 to 2004.
Reproduced from Figure 5.1 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute. Source: Based on Beirlant, Schoutens, and Segers (2005, Table 1).

With annualized volatility of 25 percent, a move of −5.82 percent is 3.68σ from the mean. According to Table 8.5, there were 10 down moves of −3.68σ or worse over the 50 years. Now we can ask how likely it would be to observe 10 such down moves in 50 years of daily returns if the distribution were normal. Even with the high 25 percent estimate of volatility, the probability of a single observation from a normal distribution being −3.68σ from the mean or worse is tiny—only 0.0117 percent. But we have roughly 12,500 days, so the likelihood of observing one or several such moves in such a long period will be much higher. The probability of observing one or more such moves out of 12,500 days would be roughly 77 percent; two such moves, 43 percent. But the probability of observing 10 or more moves would be minuscule—0.0003 percent.24 We can continue—for example, asking what is the probability of five or more moves worse than −4.53σ (assuming normality), which turns out to be less than 0.000006 percent. In every case, the probability of observing what is shown in Table 8.5 is minuscule.
We should not get carried away, however. The probabilities implied by a normal distribution do indeed fall off far too quickly given the observations in Table 8.5. The probability of observing 10 moves of −3.68σ or worse is 0.0003 percent, which is miniscule. But the loss levels do not fall off so quickly. For a loss only 20 percent lower, −2.944σ, we are almost sure to observe 10 moves, still assuming normality. The probability we will see 10 or more moves of −2.944σ or worse, assuming normality, is 99.6 percent. This result seems extraordinary—that the probability of 10 moves of −2.944σ or worse assuming normality is 0.996, whereas the probability of 10 moves of −3.68σ or worse is 0.000003—but it happens to be true. Assuming normality, the probabilities fall off very quickly as the loss levels get worse, but it does not take large changes in loss levels to cause very large falloffs in probability.
The fact that moderate changes in loss levels lead to large changes in probabilities (assuming normality) ties in nicely with Litterman's rule of thumb discussed in Chapter 5. That rule boils down to assuming actual loss levels are somewhat, but not hugely, larger than predicted by normality. We see again here that moderate changes in loss levels lead to large changes in probabilities (assuming normality).
Figure 8.9 shows the problem with normal tails from a different perspective. The line-connected squares show the expected frequency of events in the tail (excluding the 1987 crash for now) under the assumption of normality and assuming 12,500 days in total. The dots show the empirical frequency. We can see that the normal distribution predicts far too few extreme events.

Figure 8.9 Empirical and Normal Distribution for Tail of Dow Changes, 1954 to 2004 Note: Frequency is the number of days with large down moves (calculated or observed) out of 12,500 days. These observations are from Table 8.5, derived from Beirlant, Schoutens, and Segers (2005, table 1), but excluding the October 1987 observations (which is far to the left at -16σ). Reproduced from Figure 5.10 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

Collecting 50 years of daily data may not be practical for most applications, but Table 8.5 and Figure 8.9 do show us that if we want to consider extreme events, we need to address the issue of fat-tailed distributions. Treating the tails as if they are normal leads to thinking such large moves are much less likely than they actually are. (It also leads to guffaws when a trader says, "We are having a 10 standard deviation move every day," when what the trader should really say is that "events are not behaving according to the normal distribution, with more large moves than predicted by normality.")
The following sections discuss some alternative approaches that are used in practice and covered in the literature. First, I briefly discuss the value of simple rules of thumb and review the rule of thumb introduced in Chapter 5. Second, I consider two simple alternative distributions. They are simple but they model fat tails in an effective manner. Third, I briefly review extreme value theory, the asymptotic theory of maxima—the analogy for tails of standard asymptotic theory and the central limit theorem for the mean of a distribution. Finally, I review copulas, which provide a method to describe dependence among multivariate random variables when the distribution is other than normal.
Before turning to formal mathematical and statistical methods, I want to reiterate the usefulness of simple rules of thumb. In Chapter 5, we discussed Litterman's (1996) maxim to "use as a rule of thumb the assumption that four-standard-deviation events in financial markets happen approximately once per year" (p. 54). This comes down to assuming that once-per-year events are actually 4.0 standard deviations instead of 2.7 standard deviations, as would be predicted by assuming normality.
We can think of Litterman's rule of thumb as saying: "Assume that once-per-year extreme events are 1.5 times larger than normality would tell us." This is simple, and its simplicity is appealing because it is easy to use and easy to explain. As we saw in Chapter 5, we can translate this into probabilistic terms. This rule of thumb turns out to be strong, probabilistically speaking. We could also extend the rule of thumb to "assume once-per-decade extreme events are two times larger than normality would tell us." Again, we will see that this is strong, probabilistically speaking, much stronger than suggested by the Dow Jones observations discussed here.
Alternative Distributional Assumptions
I discuss here two simple alternative assumptions for the distribution of P&L, both distributions having more weight in the tails (fat tails) compared with the normal distribution.25 These are simple models, but useful precisely because they are simple but also capture the fat-tailed character of observed data. In later sections, we discuss more complex techniques.
The first distribution is the Student t-distribution. (See also Jorion 2007, 87-88; Cramér 1974, para 18.2.) The Student t-distribution has one parameter, n or ν, the shape parameter (or called degrees of freedom when used in statistical applications). The distribution is symmetric, mean zero, with variance n/(n - 2).26 For n > 2, the variance is finite, and for large n converges to the standardized normal. Low values of n on the order of 3 to 6 appear to match reasonably well with the tails of financial data (Jorion 2007, 130). A standard t-variate t times a constant ct will have variance ct2n/(n - 2) or standard deviation ct√n/(n - 2). The probability that such a random variable will be more than ξ standard deviations from the mean (will be less than -ξ times the standard deviation) is:

The Student t-distribution is well known and relatively easy to work with but has the disadvantage relative to the normal that the sum of t-distributed variables will generally not be t-distributed. The fact that normal variables sum to a normal variable provides huge computational benefits as we saw in discussing the parametric approach to estimation earlier. When we assume the individual market risk factors are normal (and use linear sensitivities or first derivatives for position exposures), then the overall portfolio P&L will also be normal. The computations required for calculating the portfolio distribution are tremendously simplified when we assume normality. This benefit disappears if we assume the individual market risk factors are Student t-distributed.
The second distribution we consider is a two-point mixture of normals. Unlike the Student t-distribution this will share many of the computational benefits of the simple normal distribution. The mixture of normals has a distribution function that is the sum of two normal distribution functions: the first with probability (1 - α) and standard deviation σmix and the second with probability α and standard deviation βσmix (both with mean μ).27 The variance for such a random variable will be σmix2[(1 - α) + αβ2]. Parameters on the order of α = 0.0125 and β = 2.5 give a distribution similar to the normal in the central part of the distribution but fatter in the tails. The probability that such a random variable X will be more than ξ standard deviations from the mean (that is, less than -ξ times the standard deviation σmix√[(1 - α) + αβ2]) will be



The mixture of normals is appealing because it combines simplicity with a straightforward motivation for fat tails. Say the P&L distribution for each day is normal but periodically there is a day that is still normal but with higher volatility. This is an attractive assumption since it is what trading in the markets actually feels like—periods of relative quiescence interspersed with days of mayhem. The conditional distribution, conditional on knowing whether the day is low or high volatility, will be normal. In contrast, the unconditional distribution, not knowing whether a day is low or high volatility, will be non normal and will exhibit fat tails. A two-point mixture of normals provides a rough approximation to this.28
The biggest advantage in using the two-point mixture of normals is that the computational simplicity associated with the normal distribution can be applied to the two normal distributions separately and the results combined using the two-point mixture. For example, the mixture of normals can be used with parametric VaR estimation to rectify the problem that the normal distribution (usually assumed with parametric VaR) does not handle tail events well.
With either of these two distributions, calculation of VaR is slightly more difficult but the fundamental ideas remain the same. Both distributions (for appropriate choice of parameters) have tails that are fatter than the normal distribution. If we had the observed volatility and knew that the P&L was normal, or t-distributed, or a mixture of normals, we could calculate the VaR for various probability levels using equation (8.1), reproduced here:
(8.1)
Figure 8.10 shows the three distributions for our hypothetical bond trade.29 Visually, the three densities do not appear very different. The overall shape is the same for all three and it is only when examining the densities far out in the tails that the differences become apparent.

Figure 8.10 Density Functions for Normal, t-distribution, and Mixture of Normals Note: The t-distribution has degrees-of-freedom ν = 9 and the mixture of normals has α = 1.25%, β = 2.5.

The formulae for the VaR and expected shortfall (ES) for a normal, t-distribution, and two-point mixture of normals are:30




VaRZ
ESZ




Normalt-distributionmixture
μ + σ·Φ−1(z)
μ + σ·{ϕ[Φ−1(z)]}/z








μ + σ·y′ where y′ solves(1−α)Φ[y′√[(1−α)+αβ2]]+ αΦ[y′√[(1−α)+αβ2]/β]= Z
 *


Table 8.6 shows the VaR and expected shortfall for a standard normal and a t-variate (6 degrees of freedom, scaled so that volatility is 1.0).
Table 8.6 VaR and Expected Shortfall for Normal, Student t, and Mixture of Normals.
Note: The t-distribution has degrees-of-freedom ν = 6 and the mixture of normals has α = 1%, β = 5.

Note that as the probability level Z gets smaller (for example, going from 5%/95% VaR to 1%/99% VaR), the expected shortfall for the Student t and mixture of normal distributions get larger relative to the normal, a result of the fatter tails.We can use these alternative distributions to examine the returns for the Dow Jones over the period from 1954 to 2004 discussed earlier. Figure 8.11 expands from Figure 8.9, showing the expected frequency of events in the tail (excluding the 1987 crash, again) for a normal, mixture of normals, and Student t-distribution (in all cases assuming a standard deviation of 25 percent) versus the empirical frequency. As we saw in Figure 8.9, the normal distribution gives far too few extreme events. The mixture of normals and the Student t-distribution, however, give a much more realistic representation of the actual data.

Figure 8.11 Empirical and Selected Distributions for Tail of Changes in Dow Jones Index, 1954 to 2004 Notes: Frequency is the number of days with large down moves (calculated or observed) out of 12,500 days. The empirical observations are from Table 8.5, derived from Beirlant, Schoutens, and Segers (2005, table 1), but excluding the October 1987 observation (which is far to the left at −16σ). The "Normal" is a standard normal distribution (standard deviation 1.00). The "Mixture of Normals" is a two-point mixture with α = 1.25, β = 2.5. (This means 1.25 percent probability of a normal with standard deviation 2.5 × 0.9687 and 98.75 percent probability of a normal with standard deviation 0.9687, giving a distribution with overall standard deviation 1.00. The term 0.9687 is calculated as 1/√{0.9875 + 0.0125 × 2.52}.) The t-distribution is a standardized t-distribution with 9 degrees of freedom multiplied times 1/√{9/(9 − 2)}, to give a distribution with standard deviation 1.00. Reproduced from Figure 5.1 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

By using either the mixture of normals or Student t-distribution, we could have some confidence that we would not be horribly off in making statements about extreme events related to changes in the Dow Jones Industrial Average.
The Student t-distribution shown in Figure 8.11 is for degrees of freedom ν = 9.31 The mixture of normals is for α = 1.25 percent (probability of high-volatility regime) and β = 2.5 (ratio of high volatility to low volatility). We can use these distributions to examine some probability statements about the Dow Jones observations.

 What is the probability of an observation 4.53σ from the mean? (Remember from Table 8.5 that this occurred five times in 50 years, so is roughly a once-per-10-years event, or probability 0.039 percent.)

 Under normality, the probability is 0.0003 percent.
 Under the t-distribution, it is 0.031 percent.
 Under the mixture of normals, it is 0.039 percent.


In other words, normality predicts far too low a probability, the same thing we see in Figures 8.9 and 8.11.

 What is the size of a move that, under a normal distribution, would produce the same probability as the t-distribution (0.031 percent) or the mixture of normals (0.039 percent)?

 A normal move of 3.42σ has probability 0.031 percent, the same as the t-distribution move of 4.53σ. In other words, the t-distribution predicts a move 1.32-times larger than the normal distribution.
 A normal move of 3.36σ has probability 0.039 percent, the same as the mixture of normals of 4.53σ, so the mixture predicts a move 1.35 times larger than the normal distribution.


Compare this with the rules of thumb mentioned earlier. Litterman's rule is that once-per-year events are 1.5 times larger than predicted by normality. My rule (extending Litterman's) is that once-per-decade events are 2 times larger than predicted by normality. The Dow Jones data for 1954 to 2004 seem to indicate that these rules of thumb are quite strong. The Dow Jones observations show that once-per-decade moves are roughly 1.4 times larger than predicted by normality.

 What is the probability of having five moves of 4.53σ or larger under the normal distribution, the t-distribution, and the mixture of normals?

 For normality, the probability of five such moves is less than 0.0001 percent.
 For the t-distribution, the probability of five such moves is 35 percent.
 For the mixture of normals, the probability of five such moves is 54 percent.


We still need to be careful, however. We have chosen the parameters (α = 1.25 and β = 2.5 for the mixture of normals, degrees of freedom = 9 for the t-distribution) based on a very small number of observations in the tails (essentially 10 out of 12,500). Furthermore, we have ignored the 1987 crash. The probability of observing one such large move (the 1987 crash was −16.22σ) would be tiny, even assuming the t-distribution or mixture of normals.32 Nonetheless, simple rules of thumb and alternative distributions are incredibly valuable. They may not solve all our problems—it is a fool's dream to think any mathematical model will perfectly represent our complex real world—but they help us think more carefully and fruitfully about the issues.
Extreme Value Theory33
Extreme value theory (EVT) is the name given to the study of the asymptotics of tail events. The law of large numbers and the central limit theorem are familiar to everyone. They provide tools to study the sum and average of a sequence of random variables as the number of observations gets large. Essentially, they say that the average will ultimately behave the same no matter what the distribution of the original variables (within some limits), thus providing a simple characterization of the average as the number of observations grows. The beauty and power of these is that they hold true no matter what the distribution of the underlying random numbers, as long as the random variables do not have too much chance of being too large—for example, if they have finite mean and variance.34
The formal study of the tails of distributions goes under the name Extreme Value Theory, or EVT. The central limit theorem studies the average of a sequence. EVT, in contrast, studies the maximum (or related characteristics) of a sequence—that is, the tails of a distribution. As such, EVT provides tools and techniques particularly suited to analyzing VaR and tail events. Broadly speaking, there are two approaches to studying the tails. The first is to consider the maximum of a sequence of random variables. This leads to the generalized extreme value (GEV) distribution. The second is to consider the threshold exceedances, that is, all the observations that exceed some specified high level. This leads to the generalized pareto distribution (GPD). The threshold exceedance approach follows from the maximum GEV approach but the GPD distribution is generally considered more useful in practice because it makes more efficient use of the limited data on extreme events.
The beauty and power of EVT is that it provides a simple characterization of the tails, analogous to the central limit theorem for the mean, no matter what the distribution of the original random variables (within limits).
For maxima of random variables Mn = max(X1,. . ., Xn), suitably normalized analogously to the central limit theorem normalization by the mean and standard deviation, the only possible limiting distributions (apart from the degenerate case of a constant) are in the generalized extreme value (GEV) family (McNeil, Frey, and Embrechts, [2005], 265). The standardized GEV distribution is:

               
                       
Introducing location and scale parameters gives the general GEV: Hξ,μ,σ(x) = Hξ[(x − μ)/σ]. The parameter ξ is known as the shape parameter and determines three types, also known by other names:

 ξ>0 - Fréchet distribution (type II maximum extreme value distribution).

 This is the most studied type of distribution in EVT because it has fat tails and is thus of particular interest in finance.
 Examples of distributions that lead to the Fréchet limit are the Fréchet itself, inverse gamma, t-distribution, F (McNeil, Frey, and Embrechts, (2005, 269)).
 The k-th moment will be finite only for k ≤ 1/ξ (technically, for a non-negative random variable with distribution of the Fréchet class, E(Xk) = ∞ for k > 1/ξ, see McNeil, Frey, and Embrechts, (2005, 268)).

 To make life confusing, the distribution can be parameterized differently. Wolfram's Mathematica parameterizes with "shape parameter" α = 1/ξ, scale β = σ, and ordinate y = 1 + ξx: 



 ξ = 0 - Gumbel distribution (type I maximum extreme value distribution).

 The tails decay faster than the Fréchet type. Essentially, distributions in this class have tails that decay exponentially.
 Distributions of this type have finite moments (technically, for a non-negative random variable with distribution of the Gumbel class, E(Xk) < ∞ for all k > 0, McNeil, Frey, and Embrechts, (2005, 269)).
 Nonetheless, there is a great deal of variety in the tails of the distribution. McNeil, Frey, and Embrechts, (2005, 269) point out that both the normal, with thin tails, and the lognormal, with much fatter tails, belong to the Gumbel class and that empirically it would be difficult to distinguish the lognormal tail behavior from the Fréchet type35
 Examples of Gumbel-class distributions are the normal, lognormal, hyperbolic, and generalized hyperbolic distributions, normal mixture models (but excluding the t-distribution, which is a boundary case), gamma, chi-squared, standard Weibull (confusingly, different from the Weibull case of the GEV).

 ξ<0 Weibull distribution.

 Not important here because of the short tail and finite endpoint.


Estimates of ξ on the order of 0.2 to 0.4 are consistent with stock market data.

Figure 8.12 Generalized Extreme Value Distribution Note: The line is a Frechet (type II maximum extreme value distribution) with ξ = +0.5. The short dash or dot is a Gumbel distribution (type I maximum extreme value distribution) with ξ = 0.0. The long dash is a Weibull distribution with ξ = −0.5.

The GEV distribution provides a simple asymptotic characterization of the maxima for, in essence, any P&L distribution that one might use. This is very powerful, but for actual use including only the maxima is wasteful of data. Something called threshold exceedances are a more practical solution.
Threshold exceedances are values that exceed a certain specified high level. They are extreme in the sense that they are the large observations in a set of observations. The key functions for discussing exceedances are the excess distribution and the mean excess functions. The excess distribution function gives the probability conditional on the random variable exceeding some specified level u.
Let X be the variable representing the random P&L. We focus on the exceedance X − u, the amount by which X exceeds the level u, and on the size of the exceedance, y (which will be non-negative). The probability that the exceedance X − u is less than an amount y (conditional on X exceeding u) is the Excess Distribution:

The mean excess function is the mean conditional on exceeding the threshold u (for random variables X with finite mean):

We saw that in the case of maxima effectively any distribution leads, asymptotically and for suitably scaled maxima, to the GEV distribution and that the tail behavior can be characterized according to the shape parameter ξ. It turns out that the characterization according to ξ carries over to threshold exceedances: if a distribution function F converges to a GEV distribution with parameter ξ, then as the threshold u rises, the excess distribution Fu converges to a generalized pareto distribution (GPD) with the same parameter ξ (see McNeil, Frey, and Embrechts, 2005, 277). The GPD is given by:

where β > 0 and y ≥ 0 for ξ ≥ 0 and 0 ≤ y ≤ −β/ξ for ξ < 0.
The parameter ξ is again the shape parameter while β is a scale parameter.
In practical applications, the GPD is useful for modeling the excess distribution function and the mean excess function because both functions are simple for the GPD. The excess distribution for GPD is also GPD:

The mean excess function for the GPD is:

with 0 ≤ u < ∞ for 0 ≤ ξ < 1 (remember that the mean is not finite for ξ ≥ 1) and 0 ≤ u < −β/ξ for ξ < 0.
The GPD results can be used by assuming that if we choose a high but finite threshold u, the observed excess distribution function will actually be GPD. We then fit the parameters ξ and β from the observed exceedances, and use the resulting fitted GPD to make statements about VaR or expected shortfall. We know that asymptotically, the excess distribution function for virtually any P&L distribution will converge to GPD. So by using the GPD for a finite sample of observed P&L, we have some justification to think that we are using a functional form that is flexible enough to capture all types of tail behavior but also based on actual tail distributions.
Copulas
Copulas provide tools to address dependence of random variables in a multivariate context, specifically multivariate distributions that are not normal. Copulas do not tackle tail events directly but provide tools to move beyond the limits of multivariate normality. Fat tails push one to consider distributions other than the normal, and copulas are a tool for using non-normal multivariate distributions. McNeill, Frey, Embrechts (2005) devote Chapter 5 to copulas and provide a useful treatment of the subject. This section is no more than an introduction.
There are a variety of distributions that might be used to model the P&L and risk factors, and which also have fatter tails than the normal. The t-distribution is popular, but there are others.36 In the univariate case, when we are dealing with only a single variable, the mathematics and intuition are somewhat more complex, but there are no substantial difficulties.
When we turn from a single variable to multiple variables, however, we need to address the issue of co-movement and dependence across risk factors. In the multivariate normal context, dependence is wholly incorporated in the correlation or covariance matrix. Much of our intuition is based on multivariate normality, but this intuition and the mathematics behind it do not carry over well to nonnormal distributions.
Measuring and modeling dependence turns out to be challenging when the distribution is not normal. Linear correlation, the tool most of us use to think about dependence, is a deficient measure of dependence in the general multivariate context. McNeil, Frey, Embrechts (2005, ch. 5, particularly section 5.2) provides a good review of the issues. It turns out that copulas provide a useful approach and set of tools for modeling and describing dependence among multivariate random variables that extends well to non-normal distributions.
One way to think of copulas is as an alternative method for writing a multivariate distribution. Consider a multivariate random variable of d dimensions, with distribution function F(x1, . . ., xd) and marginals {F1(x1), . . ., Fd(xd)}. It turns out that this multivariate distribution can be written in either of two forms:

 Usual multivariate distribution: F(x1, . . ., xd)
 In terms of marginals and copula: C(F1(x1), . . ., Fd(xd))

There will always exist a function C(F1(x1), . . ., Fd(xd)), called a copula, which is itself a d-dimensional distribution function on [0, 1]d with standard uniform marginals.37
What this says is that any multivariate distribution can be thought of as either a multivariate distribution, or as a combination of marginal distributions and a copula. The power of the latter approach is that it isolates the dependence across variables in the copula, with the marginals separate. (This is somewhat analogous to the linear [normal] case, in which the dependence structure can be isolated in the correlation matrix, with the variances separate.)
The power of the copula approach is that it allows the specification of marginals and dependence structure separately. By using copulas we can focus on the dependence structure separately from the marginals. A multivariate distribution can be created by mixing and matching marginals and copulas.
The final issue I raise in this introduction is tail dependence. McNeil, Frey, Embrechts (2005, section 5.2) cover this in some detail, but the basic idea is to measure the dependence of two variables far out in the tails, for extreme observations of both variables. This is particularly important for risk measurement because it is when variables move together simultaneously that the largest losses occur. Furthermore, such simultaneous moves are not uncommon, as extreme values tend to cluster and assets tend to be either volatile or quiescent together. In the simple bond and equity portfolio introduced in Chapter 1, we are particularly concerned with the possibility that both the bond and the equity will have large moves, in the same direction.
We saw earlier that the normal distribution, not having fat tails, does a poor job at representing financial market movements for large moves. What is perhaps more surprising is the behavior of the joint normal distribution for large joint moves. If we go far enough out in the tails, two jointly normal variables will eventually behave independently no matter what the correlation (as long as it is not ±1). This is very troublesome because it means that even if we could use marginal distributions that were fat-tailed, the dependence structure implied by joint normality provides a poor model for assets having large moves simultaneously.
With copulas we can, in fact, mix and match marginal distributions and copulas, building a joint distribution that has a chosen set of marginals (say, t-distribution to model fat tails) matched with a copula that represents the dependence structure. Jorion (2007, 209) and McNeil, Frey, and Embrechts (2005, 195, 213) provide plots of simulations for so-called meta distributions, mixing marginals and copulas. Jorion examines three possible choices for bivariate distributions:

1. Normal marginals and normal copula—produces the usual bivariate normal distribution
2. Student t marginals and normal copula—produces a hybrid distribution
3. Student t marginals and Student t copula—produces the usual bivariate Student t-distribution

In Chapter 9, we apply these three distributions plus two others to estimating our simple portfolio by Monte Carlo. We will be able in that exercise to explicitly see the tail behavior of the normal distribution. We will see that for large joint moves, the bond and the equity start to behave independently.
8.5 Estimating Risk Factor Distributions
The focus of this book is on how to think about risk and how to measure risk—in other words, how to think about and measure the P&L distribution. Most of our attention has been directed toward issues that are particular to risk measurement, such as how to map from securities to risk factors, or the definition of VaR. To use the P&L distribution, however, we need to estimate it, and this means estimating market risk factor distributions (as discussed in Section 8.3, Step 2). This takes us into the field of statistics and time-series econometrics. I do not want to cover econometrics in depth, as there are many good textbooks, but I will give a brief overview.38
We will focus on the parametric approach to estimating the P&L distribution, which means we assume that market risk factors are normally distributed.39 The normal distribution is completely determined by the standard deviation or variance-covariance matrix (and the mean). Thus, at its simplest, estimating the risk factor distribution means estimating the standard deviation (volatility) from risk factor changes {Δrf1, . . ., Δrfn}, using the standard formula (given further on).
So far, so simple. There are, however, a host of questions hidden in this simple-sounding approach:

 What are the observations on risk factor changes {Δrf1, . . ., Δrfn}? Dollar change? Yield change? Percent or logarithmic change?
 How many observations do we use?
 A similar but not identical question: What historical period should we use?
 What frequency of data: hourly, daily, weekly, monthly?
 What about changing volatility? It appears from casual observation (buttressed by much research) that market volatility changes over time. How do we deal with changing volatility?

Before answering these questions, however, let us review some of the stylized facts about markets and risk factor distributions.
Financial Time Series Stylized Facts
There are some observations about financial time series (market risk factors) that are well enough established to earn the name stylized facts—observations that most everyone agrees on. (See, particularly, McNeil, Frey, and Embrechts 2005, 117 ff.)
Volatility Varies over Time and Extreme Values Cluster
Varying volatility and clustering means that if we have a large change in a risk factor (say, yields jump up today) we are more likely to see a large change tomorrow. Tomorrow's change, however, may be up or down. A large increase indicates large changes are likely to follow, but don't indicate which direction. Extreme values, both up and down, cluster together.
This clustering of large changes together and small changes together indicates that volatility is not constant, that it varies over time. Most importantly, it indicates that volatility is persistent. If volatility is high one day, it will tend to stay high over subsequent days, changing incrementally rather than all at once.
Another result is that if we look at squared changes (Δrft2), we will see serial correlation. A large squared change will tend to be followed by a large squared change (there will be correlation between Δrft2 and Δrft+12). Furthermore, we will be able to forecast these squared changes—when we observe a large Δrft2, we can forecast that Δrft+12 will be above average.
This is all for a single risk factor series. For multiple series, we also have volatility clustering, but here we have the added observation that clustering takes place in many risk factors simultaneously. Markets as a whole tend to get volatile or quiescent. When IBM has big moves and gets more volatile, so does General Motors; they are both volatile because the overall market is volatile. A rough but useful way to think is that we are in a low or a high volatility market or regime for all or many assets together. (This is the idea of the simple mixture of normals as an approximation to a multivariate fat-tailed distribution.)
Changes Are Independent and Cannot Be Forecasted
For a single series, when we look at changes rather than squared changes (Δrft rather than Δrft2) we do not see serial correlation. Furthermore, it is very difficult (practically impossible) to predict tomorrow's change from today's change or past history. Strictly speaking, changes are not independent, they are only uncorrelated and difficult to predict, but the term captures the idea.40
Describing this in terms of the distribution will help clarify ideas. A large change today in the market risk factor is an indication that the distribution has a large dispersion or high volatility (looks like Panel B of Figure 8.2 rather than Panel A). Since volatility tends to vary over time incrementally rather than jump all at once, a high volatility today indicates that tomorrow's distribution will also have a high volatility. But today's large change doesn't tell us what part of the distribution we will draw from tomorrow; it doesn't tell us anything about tomorrow's particular value. We can use today's large change to predict that tomorrow will be a large change (so changes are not probabilistically independent) but we cannot use today's change to predict the direction of tomorrow's actual change (and in this sense changes are unrelated over time).
When we turn to multiple series, we do not see serial cross-correlation; that is, we do not see correlation between between Δrfat and Δrfbt+1).41 We will see correlation between series for the same period, in other words, between Δrfat and Δrfbt. This correlation will obviously be different across different series. For example, the correlation between the 5-year and 10-year U.S. Treasury bonds will be high but the correlation between the 10-year U.S. bond and IBM stock will be much less.
Correlations Change over Time
When we look at correlations across series we see that correlations change over time. Unlike the stylized facts we have discussed so far, however, this is more difficult to verify and measure. Our first instinct might be to simply calculate correlations over some nonoverlapping periods, say monthly periods, and compare the resulting correlations. This turns out not to tell us much, however, because when we use short periods, we should expect to see substantial variation in measured correlation, simply because of random variation. This is another example of how we need to carefully think about the random nature of the world.
Let us take a short digression and look at a particular example. The correlation between daily changes in yields of the 10-year U.S. Treasury and percent changes in the IBM stock price is 0.457 for the period January 2008 through January 2009. Now let us measure correlations over 12 roughly one-month periods (21 trading days). The 12 correlations range from 0.05 to 0.69. This looks like good evidence for changing correlations, with a large range. But we have to ask, what would we expect purely from random variation? It turns out that we would expect this amount of variation purely from random sampling variability.
The distribution of the correlation coefficient is complicated and skewed, but it is well-known (Hald 1952, 608 ff) that applying the Fisher transformation:

produces the variable z that is approximately normal with


In other words, the variable

will be, to a good approximation, normally distributed with mean 0 and variance 1.
For our example of correlation between the U.S. Treasury and IBM, let us assume that the true correlation is

For our monthly samples, n = 21, so that

and the upper and lower 4.2 percent bands are


Why pick 4.2 percent bands? Because there will be 8.4 percent or roughly a one-twelfth probability outside these bands. For 12 monthly correlations, we would expect roughly 11 out of 12 to be inside these bands and one outside these bands. In fact, we see exactly that: 11 of our calculated correlations are within the bands and only the smallest, 0.05, is outside the bands and then only by a small amount.
The conclusion we should take from this simple exercise is that 12 monthly correlations that range from 0.05 to 0.69 is exactly what we would expect to observe. Such an observation provides no strong evidence that the correlation between 10-year U.S. Treasury yields and the IBM stock price is different from 0.457 or that it changes over time.
This is just another example of how we need to think about randomness and uncertainty. Although correlations no doubt do change, we have to turn a skeptical eye toward evidence in this arena as in others. In fact, we have somewhat understated the problems with measuring correlations. As McNeil, Frey, and Embrechts (2005, 98) argue, the problem of measuring correlation is even more difficult for fat-tailed distributions. Since financial time series generally are fat-tailed, this is a practical consideration.
The only reliable way to test for and measure changing correlations is to specify formal statistical models of how the correlation changes and then test these models.
Changing correlations is no doubt important, but the clustering of joint extreme events mentioned earlier and the tail behavior discussed under copulas in the prior section and Section 9.4 are also important in this context. The view that "correlations all go to one in times of market stress" will be captured by joint distributions that exhibit tail dependence. The usual multivariate Student t distribution exhibits this kind of tail dependence, and the simple mixture of normals mimics such behavior.
Fat Tails
The final stylized fact is that financial time series have fat tails. As we have discussed this in various places already, I do not discuss it any further here.
Overview of Fitting Volatility
We now turn to fitting volatility and answering some of the questions raised at the beginning of this section.
Use Changes
First and most important, we need to look at changes. As mentioned earlier, changes are uncorrelated (speaking very loosely, they are independent). This means

In technical jargon, the levels will be autoregressive and we want to focus on the independent changes. We should never look at the volatility or correlation of levels.
As a general rule of thumb, we want to look at percent changes or changes in logs (which is for most purposes the same). For most prices, FX rates, equity indexes, and so on, what matters is the percent change rather than the absolute change. The S&P index today is roughly 1000 and a change of 10 points means a change in wealth of 1 percent. In 1929, the S&P composite was around 20 and a change of 10 points would have been a change of 50 percent. Comparing changes in the S&P index over time, we absolutely need to look at percent changes or log changes.
The one possible exception is changes in yields, where it may make sense to look at absolute changes, changes in yields measured in basis points. There is considerable debate as to whether one should consider changes in log yields or changes in yields as more fundamental, and there is no settled answer.
Simplest—Fixed Period
The simplest approach is to use a fixed period and calculate the volatility and covariances. The period should be long enough to have some stability in the estimated volatility but short enough to capture changes in volatility.
As mentioned before and well-known from any statistics text, the standard estimator of the volatility would be:

and the elements of the variance-covariance matrix would be:

where {Δrf1, . . ., Δrfn} is a set of n observations on risk factor changes. (The correlation matrix is calculated from the variance-covariance matrix by dividing by the volatilities.)
The question is, how many observations n to use? Thirty is probably too few, 125 (roughly a half-year) might be a minimum. With 30 observations, there will be considerable random sampling variability. Let us say the true volatility for some risk factor (say the CAC equity index we have been considering) is 20 percent per year. Using the formula for the small-sample variation in the variance given in Appendix 8.1, with 30 observations the 2.5 percent confidence bands will be 15 percent and 25 percent. That is, there is a 5 percent chance that using 30 observations we would calculate the volatility to be less than 15 percent or more than 25 percent. For 125 observations, in contrast, the 2.5 percent confidence bands would be 17.5 percent and 22.5 percent. Still wide, but substantially less.
More Sophisticated—Exponential Weighting
More sophisticated would be to use exponential weighting, weighting recent historical observations more heavily than more distant observations. This is the approach used by RiskMetrics (Mina and Xiao/RiskMetrics 2001). The formulae will be (showing the corresponding formulae for equally weighted):




To measure how fast the decay takes effect, we can measure the half-life, or the number of periods required before the weight is ½:

As an example, λ = 0.9 ⇒ n1/2 = 6.6 or roughly six observations. This is far too few, meaning that the decay of λ = 0.9 is far too fast. A decay of λ = 0.99 ⇒ n1/2 = 69, which would be more reasonable.
Alternatively, we can solve for decay, which gives a specified half-life:

We can also solve for the fraction of the total weight accounted for by the first n* periods relative to the fraction accounted for with no exponential weighting. For exponential weighting, the weight accounted for by the first n* periods is:



Full Econometrics—ARCH and GARCH
The most sophisticated and proper way to estimate time-varying volatility is using ARCH (autoregressive conditionally heteroscedastic) and GARCH (generalized ARCH) models. Many texts cover the econometrics of such models. McNeil, Frey, and Embrechts (2005, ch. 4) and Anderson (2001) review the econometrics and the application to financial time series.
I do not cover these because they are beyond the scope of this book. More importantly, although these are the theoretically appropriate models, they are not practical in a multivariate context for more than a few variables. Since most practical risk measurement applications work with tens or hundreds of risk factors, these models are usually not practical.
The Curse of Dimensionality
One major problem in practical applications is in estimating the variance-covariance or correlation matrix. The number of parameters we need to estimate grows very fast, faster than the number of historical observations.
There will usually be many risk factors, on the order of tens or hundreds. For k risk factors, there will be (k2 + k)/2 independent parameters in the variance-covariance matrix. This number gets large quickly. For 100 risk factors, we will have over 5,000 parameters to estimate, with only limited data. If we use two years of daily data, roughly 500 periods, we will have roughly 50,000 observations. Speaking very roughly, this means only 10 observations per estimated parameter—a very small number.
In practice this shows up as poorly estimated elements of the variance-covariance matrix or correlation matrix. More specifically, the estimated variance-covariance matrix may not be positive semi-definite (a requirement for a variance-covariance matrix, the matrix analogue of σ2 ≥ 0).
There are various ad hoc methods for dealing with this problem, but I do not discuss them here.
8.6 Uncertainty and Randomness—the Illusion of Certainty
Uncertainty and randomness enters all aspects of finance and enters into both the estimation and the use of volatility and VaR; the maxim that there is nothing certain but death and taxes is as true for VaR and volatility as for any aspect of finance. There is, however, a natural tendency to fall into an "illusion of certainty"; because we have measured the volatility or VaR, somehow the future has become less random. Much of the first section of this book focused on the notion that human intuition and our natural training often do not prepare us well to recognize and manage randomness.
Uncertainty and randomness will enter into quantitative risk measurement in a number of different ways:

 First, any measurement, say of volatility or VaR, is an estimate, based in some manner on history and various assumptions. Like any estimate, it is subject to statistical uncertainty resulting from various sources, some easy to quantify, some impossible. Among the sources of uncertainty will be:

 Measurement error, for example, if P&L is reported incorrectly, late, or not at all for some period or some portion of the portfolio
 Finite data samples and the resulting standard statistical uncertainty
 Incorrect or inappropriate assumptions or approximations
 Outright errors in programming or data collection

 Second, the world changes over time so that an estimate based on history may not be appropriate for the current environment
 Finally, actual trading results and P&L for particular days will be random and so we will always have results that exhibit variability from or around parameters, even if those parameters were known exactly.

Variability in 1 percent VaR for 100-day P&L
I focus in this section on the last of these uncertainties, the inherent uncertainty existing even when VaR and volatility are known. Even when the VaR is known with certainty, there will still be variability in the observed trading results. This probabilistic variability in P&L is relatively easy to visualize using the 1%/99% VaR and the P&L experienced over a 100-day period. Let us pretend that the P&L is normally distributed and that we have somehow estimated the VaR with no error. The true 1%/99% VaR will be −2.33·σ. For the 100 trading days, we would anticipate:42

 There should be one day with P&L as bad or worse than −2.33·σ
 The worst P&L and the empirical quantile (the average of the first and second-worst P&L) should not be too far from −2.33·σ

Neither of these is fully borne out. There is only a 37 percent chance there will be one day with P&L as bad or worse than −2.33·σ. There is actually a 27 percent chance of two or more days and 36 percent chance of no days worse than −2.33·σ.43
The P&L for the worst trading day and the empirical quantile will also deviate from the value −2.33·σ. The P&L for the worst trading day has a 10 percent probability to be outside of the range [−3.28·σ, −1.89·σ], a rather wide band. Remember that under a normal distribution the probability of observing a loss of −3.28·σ or worse would be only 0.05 percent, so one would think of this as an unlikely event, and yet as the worst loss in a 100-day period, it has probability of 5 percent and is not particularly unlikely.44 The empirical quantile (average of first and second-worst trading day) has a 10 percent probability to be outside of the range [−2.92·σ, −1.82·σ], again a rather wide range around the true value of −2.33·σ.
Remember that these results are not because the VaR is wrongly estimated but simply result from inherent variability and uncertainty. We should always expect to see variability in actual trading results, and in this example, we have simply calculated how much variability there will be.
This example understates the variability we could expect to see in actual trading results, for a number of reasons:

 We generally will not know the true value for the VaR, only an estimate that is itself uncertain.
 We will generally not know the true distribution of P&L with certainty or confidence. Trading results in the real world appear to exhibit fat tails relative to a normal distribution. As a result, the worst P&L observed over a period will likely be worse than assumed in this example.
 The world is nonstationary with circumstances changing constantly and so estimates based on the past may not be fully representative of the future.

For these and other reasons, we should expect to see more variability than this, but at least this example gives a sense of how much the P&L may vary. One should always keep in mind that simply calculating a number (such as VaR) does not mean the variability has been controlled, simply that we have some, possibly rough, idea of how much variability there may be.
8.7 Conclusion
This chapter has covered the mathematics behind the standard risk measurement tools, focusing on volatility and VaR. Chapter 9 turns to using these tools for a particularly simple portfolio, the U.S. Treasury bond and the CAC equity index futures we introduced in Chapter 1.
A full understanding of the details presented in this chapter requires a fair degree of technical sophistication. We should never lose sight of the fact, however, that the ideas are straightforward. Using the tools presented here requires an understanding of the concepts but does not necessarily require command of every technical nuance.
Appendix 8.1: Small-Sample Distribution of VaR and Standard Errors
Cramér (1974, para 28.5 and para 28.6) discusses the distribution of observed quantiles and extreme values or order statistics.
Distribution of Quantiles
Consider a sample of n observations {xi} from a one-dimensional distribution, with the Z percent quantile qz (for example, we might have Z = 0.01 and for a standard normal distribution the quantile qz = −2.3263). If n*Z is not an integer and the observations are arranged in ascending order, {x1 ≤ x2 ≤...≤ xn}, then there is a unique quantile equal to the observed value xμ+1 where μ = integer smaller than n*Z.45 Then the density of the observed quantile (xμ+1) is
(8.2)
where
F(x) = underlying distribution function
f(x) = density function
This expression can be integrated numerically to find the mean, variance, and any confidence bands desired for any quantile, given an underlying distribution F(x). But the use of (8.2) is limited because it applies only to sample quantiles when n*Z is not an integer. With 100 observations the Z = .01 quantile will be indeterminate between the first and second observations and formula (8.2) cannot be used. Either the first or the second observation could be used to estimate the 1 percent-quantile, and expression (8.4) below could be applied, but neither the first nor the second observation is ideal as an estimator of the 1 percent quantile because both are biased. For the first, second, and the average of the first and second, the mean and standard error will be:46




Mean
Std Error




1st observation
−2.508
0.429


Avg 1st and 2nd
−2.328
0.336


2nd observation
−2.148
0.309


An alternative, and easier, approach is to use the asymptotic expression derived by Cramér (and also quoted by Jorion [2007, 126], referencing Kendall [1994]) Cramér shows that asymptotically, the sample quantile is distributed normally:
(8.3) 
Using equation (8.3) with 100 observations, an underlying normal distribution, Z = 0.01, qz = −2.3263, f(qz) = 0.0267, the asymptotic standard error of the quantile will be

Note that the asymptotic formula does not give a terribly wrong answer for the average of the first and second observations (0.336 by simulation), even with only 100 observations.
Distribution of Extremes (Order Statistics)
Consider a sample of n observations {xi} from a one-dimensional distribution, as in the preceding section. Now consider the νth-from the top observation (so for 100 observations ν = 1 is the largest, ν = 100 is the smallest, and ν = 99 is the second-smallest). The density will be:
(8.4)
Once again, this expression can be numerically evaluated to provide the mean, variance, and any confidence bands.
We can also use this expression to graph the distribution of the P&L that we would observe on an extreme day. As mentioned in Section 8.2, the VaR might better be termed the "statistically best-case loss" rather than worst-case loss. This is because when we actually experience, say, the worst out of 100 days, the P&L will have a large probability of being worse than the VaR and a much smaller chance of being better. Consider the 1%/99% VaR. For a normal distribution, this will be −2.326. For our $20 million bond holding, the VaR is roughly −$304,200. We should expect to see this loss roughly 1 out of 100 days. But what will we actually see on the worst out of 100 days? Figure 8.13 shows the distribution of the worst out of 100 days, assuming that the underlying P&L distribution is normal. Panel A shows the loss for a standardized normal (σ = 1, 1%/99% VaR = −2.326) while Panel B shows the loss for the $20 million bond position (σ = $130,800, 1%/99% VaR = −$304,200). The graph shows that there is a good chance the P&L will be worse than −$304,200. In fact the P&L on that 1-out-of-100 day will be worse than −$304,200 with 63 percent probability and better than −$304,200 with only 37 percent probability.

Figure 8.13 Distribution for P&L for Worst out of 100 Days

Distribution of Variance
For a normal distribution, the 15.866 percent/84 percent VaR is the same value as the volatility. The sampling distribution of the volatility and the 15.866 percent quantile will, however, be quite different. The volatility is calculated using all observations according to the formula:

The sample Z percent-quantile sz, in contrast, is calculated from the ordered observations according to

Or at most a proportion Z of the observations are less than sz and at least a proportion (1−Z) of the observation are equal to or greater than sz.
If the sample variance is s2, then the small-sample distribution of (n − 1)s2/σ2 is chi-squared (see, for example, Kmenta 1971, 139 ff). We can then determine the probabilities of the sampling distribution as:

Asymptotically the sampling distribution of s2 is normal:


so that asymptotically

Comparison of Volatility and VaR (Quantile) Distributions
Using the distributions for the variance and the quantile given here, we can compare the sampling distribution for estimated volatility (square root of the variance) and estimated VaR (quantile) from a normal distribution.
There is a subtle question here, which is how exactly do we estimate the VaR? There are two common ways:

 From the volatility

a. Assume a functional form for the P&L distribution.
b. Estimate the volatility from the data.
c. Calculate the VaR as a multiple of the volatility.

 As the empirical quantile

a. Estimate the VaR directly as the appropriate quantile of the empirical distribution.


In the first case, the sampling distribution of the VaR will inherit that of the volatility (the VaR is the volatility grossed up by a fixed multiple—the VaR is the quantile of the assumed distribution and is conditional on the assumed functional form for the P&L distribution). The second case is the comparison we are really concerned with.
We will first compare the volatility and the 15.866 percent quantile, which for a standard normal distribution both have value 1.0. For 100 observations, we estimate the volatility by the usual estimator for the standard deviation, and the 15.866 percent quantile by the fifteenth-from-the-bottom observation (the empirical 15.866 percent quantile). The sampling distribution for the volatility is tighter than for the quantile. Table 8.7 and Figure 8.14 show the lower and upper 5 percent confidence bands (so that 90 percent of the probability mass is between the 5 percent and 95 percent level). These show that there is a 5 percent chance the volatility will be below 0.882 and a 5 percent chance the fifteenth observation will be below 0.806.47
Table 8.7 Comparison of Sampling Distribution for Volatility and VaR, 100 Observations.


Figure 8.14 Confidence Bands for Volatility and 15.866 Percent VaR, 100 observations

The more interesting comparison, however, is between the VaR estimated by way of the volatility versus the VaR estimated by the empirical quantile for realistic values of VaR (say, 1 percent/99 percent VaR). Consider the 1 percent/99 percent VaR for 255 observations (roughly one year of trading days). The empirical 1 percent quantile will be the second observation. Table 8.8 shows the confidence bands for the VaR estimated from the volatility and from the second observation.48
Table 8.8 Comparison of Sampling Distribution for 1 Percent/99 Percent VaR Estimated from Volatility and by Empirical Quantile—255 Observations.

The empirical quantile (the second observation) has confidence bands that are extremely wide relative to those for the volatility (−10 percent/ +28 percent of the true value). This is hardly surprising. If the distribution is in fact normal, then it will be efficient to use all observations, obtain a precise estimate of the volatility, and then infer the quantile. Relying on only the observations in the far end of the tail, conditional on the distribution being normal, produces a less efficient estimate. (Also note that the asymptotic confidence bands are not very representative of the actual confidence bands.)49
There is another context in which we may wish to examine the sampling distribution of the volatility or the VaR. Say that we knew the value of the volatility or VaR, and we wanted to calculate the probability of some particular trading results. For example, we may want to assess the probability of volatility over a year being within a particular range, or the probability that the largest loss over 100 trading days will be worse than a chosen value. The distributions given earlier give the answer to that question. There is a subtle point here with respect to VaR. No matter how the VaR is estimated and no matter what the VaR sampling distribution is, the distribution of the observed empirical quantile (conditional on the VaR value) will be given by (8.1) or (8.3). Observed P&L results, for example, the observed quantile over a trading period, will have the sample distribution of the quantile (VaR) and not the volatility.
Appendix 8.2: Second Derivatives and the Parametric Approach
One of the biggest drawbacks with the parametric or linear estimation approach is that it cannot capture nonlinear instruments well. This is not usually a fatal flaw, as most portfolios will be at least locally linear and the parametric approach can provide useful information.50 More problematic, however, is that with the standard approach, there is no way to confirm that nonlinearities are small or tell when they are large enough to make a difference (thus requiring an alternate approach).
I discuss in this section a way to estimate the effect of nonlinearity in the asset payoff using second derivative (gamma) information from the original assets or risk factors; a measure in particular that indicates when linearity fails to provide a good summary of the P&L distribution.51 Although this is more involved than the straightforward calculation of the portfolio variance, it is orders of magnitude less computationally intensive than Monte Carlo techniques. This measure provides a good indicator for the breakdown of linearity but not necessarily a good estimate for the size of the nonlinearity effect.
To lay out the ideas, consider first the univariate case with a single risk factor, where f represents risk factor changes. We assume that the risk factor changes are normal. Since we are particularly focused on whether and how nonlinearity in asset payoff translates into deviations from normality, assuming normality in the risk factor distribution (and then examining deviations from normality in the resulting P&L distribution) is the appropriate approach.
The portfolio P&L p will be approximated by:
(8.5) 
This will not be normal because of the term f2, and the important question is how large are the deviations from normality. The idea is to calculate the first three higher moments of the distribution (variance, skew, kurtosis) and then use an asymptotic Cornish-Fisher expansion of the inverse pdf to examine how seriously the quantiles (VaR) for this distribution deviate from those of a normal.
If we assume that Equation (8.5) is an exact expression (third and higher order derivatives are zero), then the higher order products of the P&L will be:52

Assuming that f is mean zero and normally distributed, the expectation for all odd-ordered terms in f will be zero, and the even terms will be E[fj] = j!/(j/2)! × σj/2j/2. This gives the expectations:

The central moments of p will be:
(8.6a) 
(8.6b)
(8.6c)

(8.6d)




(8.6e)
For the univariate case, the central moments  will be:








Once the variance, skew, and kurtosis have been calculated, one can evaluate whether they are large enough to make a substantial difference by evaluating the approximate quantiles of the P&L distribution and comparing them with the normal.
The Cornish-Fisher expansion for the inverse of a general pdf function F(.) can be used to evaluate the (approximate) quantiles for the P&L distribution, accounting for skew and kurtosis. These quantiles can be compared to the normal quantiles. If they are substantially different, then we can infer that the nonlinearity of asset payoffs has a substantial impact on the P&L distribution; if they do not differ substantially, then the nonlinearity of the payoff has not substantially altered the P&L distribution relative to normality.
The Cornish-Fisher expansion is an asymptotic expansion for the inverse pdf for a general distribution function. The terms up to third order (terms of the same order are in square brackets) are53
(8.7) 



y = μ + σw is solution to inverse pdf: F(y) = prob, that is, approximate Cornish-Fisher critical value for a probability level prob
x = solution to standard normal pdf: Φ(x) = prob, that is, the critical value for probability level prob with a standard normal distribution (note that this is the lower tail probability so that x = −1.6449 for prob = 0.05, x = 1.6449 for prob = 0.95).
m3 = skew
m4 = excess kurtosis
γ3 = κ5/σ5
κ5 = 5th cumulant from before
Care must be exercised when the skew and kurtosis are large enough to indicate a breakdown of the linear approximation. The Cornish-Fisher expansion can indeed be used to approximate the quantiles, but the accuracy of the approximation given by Equation (8.7) will not be very good when the skew and kurtosis are large. The Cornish-Fisher expansion is asymptotic and, particularly for quantiles far out in the tails, requires many terms when the distribution deviates substantially from normal. The truncated expansion in (8.7) will not be good for large values of skew and kurtosis, as seen in the example discussed in Chapter 9. In this sense, the current approach provides an indicator for when linearity breaks down, but not necessarily an effective approximation; in such cases either the historical or Monte Carlo methods must be used.
For the univariate case, this complicated approach is unnecessary; one can compare the delta and gamma (first and second derivatives) directly. For the multivariate case, however, it is impossible to evaluate the effect of first and second derivative terms without accounting for covariance effects; the calculation of the portfolio skew and kurtosis is the only effective approach.
For the multivariate case, assume the risk factor changes are a jointly normal vector F. The P&L will be, approximately:
(8.8)
If we assume this is an exact expression, and calculate the moments as before (retaining only terms that will be nonzero for a multivariate normal):

For a multivariate normal, the central moments for k > 2 can all be expressed in terms of σij (Isserlis 1918 and see Wikipedia entry under "Multivariate Normal Distribution"):


For example, for 4th order:


This is messy but can be programmed without too much trouble.
The distribution of P&L is univariate and so the portfolio variance, skew, and kurtosis will be scalars and the central moments will be given by the same expressions as before (Equation (8.6a)). The Cornish-Fisher expansion (Equation (8.7)) can be applied to the overall portfolio just as for the univariate risk factor case to evaluate whether the skew and kurtosis are small enough that the linear approach is valid, or large enough to require serious attention to nonlinearity.
Notes
1. The distribution function is F(Y) = Prob[P&L ≤ Y] and the density function is the derivative (where it exists): f(Y) = Prob[P&L = Y].
2. For the S&P 500 index, the daily standard deviation is roughly 1.2 percent while the average daily return is only 0.03 percent, calculated from Ibbotson Associates data for 1926-2007, which show the annualized mean and standard deviation for monthly capital appreciation returns, which are 7.41 percent and 19.15 percent, respectively.
3. In the literature, the probability level chosen can be either the probability that loss will be worse than Y (my Z) or the probability that loss will be better than Y (my 1- Z). Jorion (2007), for example, uses 1- Z. For clarity, I will generally quote both Z and 1- Z as in "5% /95% VaR."
4. As I mentioned earlier, I will take Z to be the small probability that the loss will be worse than Y, so Z might be 1 percent or 5 percent. If the P&L distribution function is F(y), then the VaRZ or Z-quantile is F−1(Z), where F−1 is the inverse distribution function. When the distribution function is not continuous, there are technicalities that I will ignore (see McNeil, Frey, and Embrechts 2005, 39). To add to the confusion, some texts change sign on the P&L distribution and discuss the upper tail of the distribution (defining VaRZ = Y s.t. P[P&L ≥ Y] = Z; e.g., McNeil, Frey, and Embrechts 2005) while others focus on the lower tail but change the sign on Y to make the VaR a positive number (e.g., Jorion 2007).
5. I intentionally use the word pretend rather than assume. Returns in financial markets are usually independent over time and approximately normal. The approximation is reasonably good for the central part of the distribution but less good for the tails of the distribution, where evidence shows that large profits and large losses occur more often than is implied by a normal approximation. This is usually called the problem of fat tails, meaning that observed returns or P&L have tails that are fatter (more probability mass in the tails relative to the middle) than the normal distribution. The issue of fat tails is critically important in calculating VaR since VaR is a measure of the tail of the distribution. For the moment, however, I want to illustrate the concept of VaR using a particularly simple distribution. I discuss issues surrounding fat tails more fully further on.
6. A third summary measure, which I only mention here and is discussed more fully shortly, is expected shortfall. For most cases, the expected shortfall is just the average loss conditional on the loss being worse than the VaR: expected shortfall = E [Loss|Loss < Y]. In Figure 8.3, VaR is the point Y, and the expected shortfall is the average of all losses Y and worse. In other words, the expected shortfall takes account not just of the point Y but also of how much worse losses could be.
7. Technically, VaR is subadditive when the underlying risk factor distributions are elliptical and the portfolio can be represented as linear combinations of risk factors. The normal t-distribution and two-point mixture of normals are elliptical, so this does cover many situations. Subadditivity comes under the wider concept of coherent risk measures, originally introduced by Artzner et al. (1997, 1999). See McNeil, Frey, and Embrechts (2005, section 6.1 and theorem 6.8).
8. I am working with the profit, and the figure shows the density of the profit. McNeil, Frey, and Embrechts work with the loss, the negative of the profit.
9. Expected shortfall is the conditional expectation only for a continuous distribution—see McNeil, Frey, and Embrechts (2005, 44 ff). For a general P&L distribution
10. But see Jorion (2007, section 4.5.2) for a discussion of scaling if serial correlation is present.
11. For so-called elliptical distributions, which includes normal, Student t-distribution, simple mixture of normals, and many others, the sum, or convolution, is also elliptical. (The random variables must be independent and have the same dispersion matrix [variance].) See McNeil, Frey, and Embrechts (2005, 95). The new elliptical distribution may not be the same form as the original. The normal is the exception. As is well known, the sum of independent (marginally) distributed normals is also normal, and the sum of jointly normal variables (independent or dependent) will be normal.
12. According to "Risk Management" by Joe Nocera, New York Times magazine section, January 4, 2009.
13. In reality, the "statistically worst-case loss" is the destruction of our world as we know it. Possibly by a large asteroid, or nuclear cataclysm, or something else that is totally unforeseen. Unfortunately, the word worst is commonly applied to VaR. Crouhy, Galai, and Mark (2001, 187) wrote: "Value at risk can be defined as the worst loss that might be expected from holding a security or portfolio...given a specified level of probability." Jorion (2007, 106) wrote: "VAR is the worst loss over a target horizon such that there is a low, prespecified probability that the actual loss will be larger." I am not criticizing these texts generally (they provide good treatments of the topic), only the misleading application of the word worst to VaR.
14. A graph of the P&L for the worst out of 100 days is shown in Appendix 8.1, Figure 8.13.
15. For example, the USD PV (in $M) of a one-month forward contract to sell $100M at an agreed 1.42 forward rate will be:

where
F = pre-agreed forward rate (USD/EUR, in this case 1.42)
X = current spot rate (USD/EUR, such as 1.40)
re, ru = one-month euro and U.S. rates (before day-count adjustment, such as 0.43 percent and 0.26 percent)
Changes in the present value (PV) will be most strongly affected by the spot rate X, with changes pretty close to one for one with X. There will be a smaller contribution from the interest rates. (The PV in this case would be $-1.45M.)
16. An FX contract will in fact also depend on the interest rates or interest rate differential for the two currencies, so that a full risk analysis would be done not with the spot FX rate alone but with the small set of factors: {spot rate, currency 1 yield curve, currency 2 yield curve}. The interest rate risk, however, is negligible relative to the spot FX risk.
17. I say "generally" because sometimes it seems that the market moves almost overnight from tranquil to panic mode.
18. In this case, the FX risk dominates. For a contract to sell $100 million 30 days forward with F = 1.42, X = 1.40, ru = 0.43 percent, and re = 0.26 percent, the PV will be $1.39 million. The sensitivity to the spot FX rate will be about $0.9857 million for every 1 percent weakening in the FX rate (going from 1.393 to 1.407). The volatility of the FX rate is about 0.79 percent per day, so the volatility of this position (due to changes in FX rates) will be about $780,000 per day. The sensitivity to the U.S. interest rate ru will be about $833/bp, and the volatility of rates might be 2.5 bp per day, so the interest rate volatility will be about $2,000 per day.
19. See Coleman (1998a) and Coleman (2011a) for a discussion of building yield curves and calculating DV01s from yield curves.
20. There is a subtle issue for full valuation when we look at changes in risk factors, particularly when we use the historical approach. We need to use the historical market risk factors but apply them as if they applied to the assets today. I discuss the issue more fully when we turn to the example in Chapter 9.
21. This closing-time problem is discussed in Kahya (1998) and Coleman (2007), together with some estimation strategies.
22. Jorion (2007, 247 ff) calls this a local-valuation method, meaning that sensitivities are measured using local derivatives.
23. Beirlant, Schoutens, and Segers (2005) show that daily volatility estimated over a three-year horizon is usually somewhat less than 25 percent, so 25 percent is a high but not outlandish estimate.
24. This result goes back to the case of Bernoulli trials discussed in Chapter 2. We have 12,500 Bernoulli trials (days), with the probability of success (move worse than −3.68σ) being 0.0117 percent. The distribution of multiple successes will be binomial, with the probabilities as quoted in the text.
25. See McNeil, Frey, and Embrechts (2005), Chapter 3, for a detailed technical discussion of alternative distributions.
26. Confusingly some statistics texts such as Kmenta (1971, 143) use n for the number of observations, and ν = n - 1 for the degrees of freedom or t-distribution shape parameter, and give the variance as (n - 1)/(n - 3) = ν/(ν - 2).
27. This is a simple example of the more general class of normal mixture distributions. See section 3.2 of McNeil, Frey, and Embrechts (2005). Interestingly, the multivariate t distribution is a normal variance mixture, and many other distributions of interest in finance can be generated as normal mean-variance mixtures.
28. A rough approximation that is robust and simple can be quite valuable: remember that a 90 percent correct answer in the hand is more useful than a 99 percent correct answer that is not available.
29. The t-distribution has degrees-of-freedom ν = 9 and the mixture of normals has α = 1.25%, β = 2.5. We discuss these parameter choices shortly when we turn to the Dow Jones data for 1954 to 2004.
30. See McNeil, Frey, and Embrechts, (2005, 39-40, 45-46) for the normal and t-distribution. My formulae differ slightly from McNeil et al.: first the expression  appears because a standard t-variate with ν degrees of freedom has variance ν/ν - 2, in contrast to a standard normal which has variance 1. In my expressions, the term σ is the volatility of the P&L distribution for both the normal and the t-distribution; the σ in McNeil et al. eq. (2.20) is not the standard deviation of the P&L but the scale parameter. Second, my Z is the probability losses will be worse than VaR (e.g., 1 percent or 5 percent STET) while McNeil et al.'s α is the probability losses will be better (e.g., 99 percent or 95 percent).
31. In this particular case, we use ν (degrees of freedom) = 9, but more generally, lower values of ν on the order of 3 to 6 appear to match reasonably well with the tails of financial data (see Jorion 2007, 130).
32. We could, of course, extend the mixture of normals to a three-point mixture. A mixture with α1 = 1.25%, β1 = 2.5, α2 = 0.02%, β2 = 30 would fit these Dow data reasonably well. Although maybe too ad hoc in the present context, with regard to liquidity risk and behavior during a liquidity crisis, considered in Chapter 12, such an approach may be useful.
33. Much of this discussion is based on McNeil, Frey, and Embrechts (2005, ch. 7) but also see Embrechts, Klüppelberg, and Mikosch (2003). Following McNeil, Frey, and Embrechts, this section discusses the upper tail of the distribution.
34. The law of large numbers says that the average will converge to a constant, the mean. The central limit theorem says the average, scaled up by the root of the number of observations, converges to a random variable with normal distribution. For {Xn}, an independent sequence of random variables with the same distribution and E[Xn] = m and Var[Xn] = σ finite, then:
strong law of large numbers: n−1ΣXk → m with probability 1.
central limit theorem: Sn = X1 + X2 + ... + Xn, then (Sn - nm)/σ√n = √n(n−1ΣXk - m)/σ ⇒Normal(0,1)
See Billingsley (1979) Theorem 22.4 and Theorem 27.1. The conditions can be weakened from those stated here.
35. "In financial modeling, it is often erroneously assumed that the only interesting models for financial returns are the power-tailed distributions of the Fréchet class [GEV with ξ > 0 where the tail decays slowly, like a power function]. The Gumbel class [GEV with ξ = 0, where the tails decay exponentially] is also interesting because it contains many distributions with much heavier tails than the normal, even if these are not regularly varying power tails" (McNeil, Frey, and Embrechts [2005], 269).
36. See McNeil, Frey, and Embrechts (2005, ch. 3) for a discussion of alternative distributions, both univariate and multivariate.
37. See McNeil, Frey, and Embrechts (2005, section 5.1). Copulas are most appropriate for continuous distributions.
38. McNeil, Frey, and Embrechts (2005, ch. 4) is devoted to financial time series. Box and Jenkins (1970) is the classic text on time series analysis, and Alexander (2001) wrote a more modern text devoted to financial time series.
39. For nonnormality, the problems are more complex, but many of the same issues arise.
40. Independence requires that squared changes as well as changes are uncorrelated. Technically, independence requires that P[Δrft & Δrft+1] = P[Δrft]×P[Δrft+1]—that is, the probability of any statement about Δrft and Δrft jointly equals the product of the separate probabilities, and because of volatility clustering, this is not true. Nonetheless, it is true that for most practical purposes the change Δrft+1 is unrelated to the change Δrft.
41. This assumes we do not have the closing-time problem mentioned under Section 8.3, discussed earlier.
42. When we observe 100 trading days, the empirical 1 percent quantile is indeterminate between the first and second-smallest observations and the average between the two is a reasonable estimate.
43. This will be a process of repeated draws, Bernoulli trials with p = .01, q = .99. P[no draw > 1%VaR] = .99101 = 0.3624. P[k draws] = Comb(n,k)·pk·qn-k, ⇒ P[1 draw] = 0.3697, ⇒ P[2 or more] = 0.2679. This result will hold generally for the 1%/99% VaR and not just when returns are normally distributed, as long as the VaR is correct.
44. Analytic formula for distribution of the maximum from n normals is Φ(x)n and density fn is n·ϕ(x)·Φ(x)n-1. P[P&L < 3.283*σ] = 0.999487. P[Max from 100 < 3.283*σ] = 0.99487100 = 0.95 ⇒ P[Max from 100 > 3.286*σ] = 1 - .95 = 0.05. But P[Standard Norm > 3.283] = 0.000513.
45. If n*Z is an integer, then the quantile is indeterminate between xnz and xnz+1. For example, with Z = 0.01 and n = 100, n*Z = 1 and the 1 percent quantile is indeterminate between the first and second observations. The average of these two makes a good choice, but I do not know any easy distribution for this average.
46. For the first and second observations, the density (8.4) is integrated numerically. For the average of the two, I simulated with 1 million draws from a pseudo-random number generator.
47. The fifteenth observation is the empirical quantile, but it is also biased with mean 1.055. The average of the fifteenth and sixteenth observations, with mean 1.03, is also shown, calculated by simulation. The sixteenth observation has mean 1.012 and confidence bands 0.767/1.265.
48. The average for the second observation is 2.501 (instead of 2.326). The average for the second and third is 2.412.
49. Crouhy, Galai, and Mark (2001, 245-246) do not carefully distinguish between the sampling distribution for VaR estimated by way of volatility versus VaR estimated by the empirical quantile. As a result, their comparison of the VaR estimated by way of volatility and by way of the empirical quantile is not correct, and their statement that the test based on the standard error of the quantile is less powerful than the chi-square test is not correct.
50. To quote Litterman (1996, 53): "Many risk managers today seem to forget that the key benefit of a simple approach, such as the linear approximation implicit in traditional portfolio analysis, is the powerful insight it can provide in contexts where it is valid. With very few exceptions, portfolios will have locally linear exposures about which the application of portfolio risk analysis tools can provide useful information."
51. I have seen discussion of using second derivatives to improve estimates of the variance (for example, Crouhy, Gailai, and Mark 2001, 249 ff and Jorion 2007, ch. 10) and mention of using an asymptotic Cornish-Fisher expansion for the inverse pdf to improve the VaR critical value. I have not seen the combination of using the skew and kurtosis together with the Cornish-Fisher expansion to examine the effect of nonlinearity on the P&L distribution. This is nonetheless a straightforward idea and may have been addressed by prior authors.
52. This essentially says we ignore terms with third and higher derivatives, even though they would enter with the same order in f as terms we are otherwise including.
53. cf. Abramowitz and Stegun (1972, 935, 0. 238) under "Cornish-Fisher asymptotic expansion," where they express it in terms of Hermite polynomials.









Chapter 9
Using Volatility and VaR
We discussed in Chapter 8 the standard tools used in quantitative risk measurement—primarily volatility and VaR. In this chapter, we apply these tools to measuring the market risk of a simple portfolio of two assets—one government bond and one equity futures. The idea is to show how the tools are applied in practice by way of an example. We will roughly parallel the structure of Chapter 8, providing simple examples of the topics.
9.1 Simple Portfolio
Let us consider a portfolio made up of a government bond and an equity index futures (the same portfolio considered in Chapter 1):

 Own $20M U.S. Treasury 10-year bond.
 Long €7M nominal of CAC futures (French equity index).

We can take this as a simple example or analogue of a trading firm, with the bond representing a fixed-income trading desk or investment portfolio and the futures representing an equity trading desk or investment portfolio. In a real firm, there would be many positions but the simplicity of the portfolio allows us to focus on the techniques and tools without taking on the complexity of a real portfolio. We turn in Chapter 10 to a more complex portfolio, where the quantitative techniques bring value. Nonetheless, even this simple portfolio exhibits multiple risks:

 Yield risk—U.S. Treasury curve.
 Equity risk.
 Operational risk.

 Delivery risk for bond.
 Position-keeping and reconciliation for the futures.


9.2 Calculating P&L Distribution
In Chapter 8, we discussed volatility and VaR, using as an example a normal distribution with volatility $130,800. This is actually the P&L distribution for the U.S. Treasury bond position (treated on its own) estimated using the parametric or delta-normal approach. I will try to use this as an example to illustrate how we calculate the P&L distribution and from that the VaR and volatility.
We will follow the four steps outlined in Section 8.3 "Methods for Estimating the P&L Distribution":

1. Asset to Risk Factor Mapping—Translate from assets held into market risk factors.
2. Risk Factor Distributions—Estimate the range of possibilities for the market risk factors.
3. Generate the Portfolio P&L Distribution—Generate risk factor P&L and sum to produce the overall portfolio P&L distribution.
4. VaR, Volatility, and so on—Estimate the VaR, volatility, or other desired characteristics of the P&L distribution.

We will estimate the P&L distribution (and the VaR and so on) using the parametric, historic, and Monte Carlo approaches.
Calculating Volatility and VaR for Single Bond Position
The U.S. Treasury position is long $20 million notional of the 10-year U.S. Treasury bond (the 3.75 percent of November 15, 2018). Given just this information, it is hard to have a firm idea of the risk for this position. After we estimate the P&L distribution, however, we will end up with a good idea of the risk under normal trading conditions.
The portfolio is evaluated as of January 27, 2009, so that all prices, yields, volatilities, and so on, are taken as of January 27, 2009. The goal is to find the distribution of the P&L for the portfolio, which in this case is just the single bond. We will consider the P&L over one day, going from the 27th to the 28th. The bond price on the 27th was 110.533 and so we need to find some way to estimate possible prices and price changes for the 28th. The most likely outcome is probably no change, and I will assume the average P&L is zero so that the P&L distribution has mean zero. But we still need to estimate the range of values around the mean to get the distribution. As is usually the case for estimating the P&L distribution, we can conceptually separate the problem into two parts: the distribution of market risk factors (market realizations that are independent of the firm's actions) and the mapping or transformation of the portfolio positions to those market risk factors.
Step 1—Asset to Risk Factor Mapping
Step 1 is to map from assets to risk factors. In this example, the mapping is very simple. We have one asset, the 10-year U.S. Treasury. We will use one market risk factor, the yield on the 10-year U.S. Treasury. The mapping is one to one, with the transformation being the standard yield-to-price calculation. We could use the bond price instead of yield but it is more convenient to use yields, since they standardize (at least partially) across bonds with different coupons and maturities.1
We will implement the mapping or transformation differently depending on the particular estimation approach we use—parametric, historical, or Monte Carlo. At this stage, however, there is no huge difference in the three approaches. For all three, the aim is to translate from the actual positions we hold—the 10-year bond—to some recognizable market risk factor—the market yield in this case. For all approaches, we will use the bond yield-to-price function:

For historical and Monte Carlo we will use the full function, while for parametric we will use the first derivatives and a linear approximation: assume that yield changes transform linearly into price changes and P&L. For small yield changes, price changes are roughly proportional to yield changes:2

The DV01 is the first derivative of the yield-to-price function, called delta in an option context; thus the alternative term delta-normal for the parametric approach.
The DV01 of the 10-year bond is about $914/bp for $1 million nominal, so for a $20 million holding, the portfolio DV01 or sensitivity will be about $18,300/bp. In other words, for each 1bp fall in yields we should expect roughly $18,300 profit and for each 5bp roughly $91,500 profit (since prices and yields move inversely).
For historical or Monte Carlo estimation, we will use the actual yield-to-price function. In other words, say the yield goes to 2.53 percent from 2.58 percent:

Step 2—Risk Factor Distributions
Now we turn to Step 2, where we have to determine the distribution of the market risk factor, in this case bond yields. Nobody can say with certainty what the distribution truly is, but examining history is always a good start. Figure 9.1 shows the empirical distribution for daily changes in bond yields for 273 trading days (roughly 13 months). The daily changes range from −20 basis points (bp) to +27bp, but most changes are grouped around zero.4

Figure 9.1 Distribution of Yield Changes, 10-Year U.S. Treasury, One Year of Daily Data
Note: The histogram is for one year of daily data. The line represents a normal distribution with the same volatility (7.15bp per day).

We could use this history as our distribution and have some confidence that we were not far off the truth. For historical estimation, that is exactly what we will do. To start, however, we consider the parametric approach and work with a parametric functional form (the normal distribution) rather than with the empirical distribution. The empirical distribution looks roughly normal and a normal distribution has been overlaid in Figure 9.1. Although the normal distribution does not fit the data perfectly, it does capture the major characteristics, including the concentration around zero and the wide dispersion of changes, both positive and negative.5 (I discuss non-normality and fat tails more further on.)
A normal distribution is simple—characterized by just the mean and the volatility—and the normal is easy to use, being programmed into all mathematical and statistical packages. If we want to assume normality, the best-fit normal distribution has mean zero and a standard deviation of 7.15bp per day.
Step 3—Generate P&L Distributions
To obtain the overall portfolio P&L distribution, we must now translate the distribution of market yields into portfolio P&L. This is where the three approaches—parametric, historical, and Monte Carlo—begin to differ substantially.
Parametric
The parametric approach uses the approximation that, for small changes, price changes are roughly proportional to yield changes:

As noted earlier, the portfolio DV01 or sensitivity will be about $18,300/bp. In other words, for each 1bp rise in yields, we should expect roughly $18,300 loss (since prices and yields move inversely).
The linearity of the transformation from yields to portfolio P&L, and the assumed normality of the yield distribution means that the portfolio P&L will also be normal. The P&L distribution will be the same as the yield distribution, only blown up or multiplied by the DV01 of 18,300. Figure 9.2 shows the price or P&L distribution translated from yield changes. (Note that the axis is reversed relative to Figure 9.1, since large falls in yield mean large rises in price.) Since we assume the yield distribution is normal, the P&L distribution will also be normal. The translation is:



Figure 9.2 Distribution of P&L, Parametric Approach, One Year of Daily Data
Note: This assumes that the market risk factors (yields) are normally distributed and the bond P&L is linearly related to yields.

We have thus arrived at a reasonable description of the P&L distribution for our $20M 10-year bond position—normal with a standard deviation of about $130,800. This is the same distribution as in Chapter 8, Panel A of Figures 8.2 and 8.4.
Historical
Calculating the historical P&L distribution is conceptually straightforward, and in this case quite simple:

 Choose the historical period for the market risk factors. In this example, it is the sample of 272 yield observations (slightly over one year) shown in Figure 9.1.
 Calculate the bond P&L from the market yields.

There are two important respects in which the historical approach will (or may) differ from the parametric approach. The first and most obvious is the distribution used for the market risk factors. Refer back to Figure 9.1, which shows the empirical and a fitted normal distribution. The historical uses the empirical (summarized by the histogram), while the parametric uses the fitted normal (the solid line).
The second point is how the portfolio P&L is calculated. The parametric approach uses a linear (first derivative or delta) approximation:

and we could do exactly the same for the historical approach. If we used the linear approximation, then differences in the resulting distributions between the parametric and historical approaches would be due to the distribution of the market risk factors.
For the historical approach, however, the P&L is generally calculated using full revaluation, in this example, using the yield-to-price function. So, for example, the 10-year yield on January 26 was 2.65 and on January 27 2.53, a fall of 12bp. We calculate the bond price at these two yields and take the difference:

This gives a slightly different P&L from using a linear approximation (the result would be $219,500), but for a bond such as this the difference is really trivial.
There is a subtle issue here that is not obvious when we use the linear approximation but does become apparent when doing full revaluation. The historical yield of 2.65 percent is for January 26, but we actually want to calculate what the bond price would be at that yield for January 27. In other words, we need to use the historical market risk factors but apply them as if they applied to the asset today.
The difference is only minor for yields on the 26th versus the 27th, but becomes important when we look far back in time. Consider the 5bp fall in yields from 3.90 percent on January 3, 2008, to 3.85 percent on January 4. Our 10-year Treasury was not even issued in January 2008, but if it were, it would have been some 10 years 10 months in maturity (versus 9 years 91/2 months on January 27, 2009). At a yield of 3.85 percent on January 3, 2008, it would have been trading at $99.12 and the 5bp fall in yields would have corresponded to an $87,400 profit. In fact, on January 27, 2009, the bond price was $110.526 and a 5bp change in yield would correspond to a $91,200 profit.
The point is that we want to know the impact of changes in historical market risk factors on today's holdings of today's assets, not the impact on historical holdings or on the assets at the time. In the current example, the market risk factor is the bond yield, and more specifically, changes in the yield. To assess the impact, we start with today's yield (2.53 percent) and apply the historical changes to arrive at a hypothetical new yield. Going from January 3, 2008, to January 4th, the yield fell by 5bp—applied to today (January 27, 2009, when yields are 2.53 percent), this would have meant a fall from 2.58 percent to 2.53 percent—a profit of $91,200.
The issue becomes even more dramatic when we consider short-dated instruments, options in particular. Say we held a two-week option as of January 27, 2009. In January 2008, the actual option would be one year and two weeks. The point of looking at the historical market risk factors is to try to estimate how a two-week option would behave under different market conditions, not the difference between a one-year versus two-week option.
When we do apply the actual yield changes to today's bond holding (the bond holding as of January 27, 2009), we arrive at the distribution of P&L shown in Figure 9.3. The parametric distribution (generated assuming the yield change distribution is normal and the bond prices linear in yields) is also shown. The two distributions do differ, and the difference is almost entirely due to the difference in the distribution of yields (the market risk factor) rather than the linearity of the parametric approach versus full revaluation of the historical approach. The differences will be explored more shortly.

Figure 9.3 Distribution of P&L, Parametric and Historical Approaches, One Year of Daily Data
Note: The solid line shows the distribution for the parametric approach (assuming yields are normal and bond P&L is linear) and the histogram shows the historical approach (using the historical distribution of yields and the fullyield-to-price function).

Monte Carlo
Monte Carlo estimation for the single bond position is also straightforward, and parallels the historical approach.

 Assume some distribution for the market risk factor, in this case, yield changes. For now, we will assume normal mean-zero, but other distributions could be chosen.
 Estimate the parameters for the parametric distribution. In this case calculate the volatility from the 272 observations shown in Figure 9.1. (In other words, we will make exactly the same distributional assumption as for the parametric approach—yield changes are normal with volatility 7.15bp per day.)
 Generate a Monte Carlo finite-sample distribution for the risk factors by simulating a large number of draws from the parametric distribution.
 Calculate the bond P&L from the market yields, just as for the historical approach.

In a sense, the Monte Carlo approach is a combination of the parametric approach (assuming a particular parametric form for the distribution) and the historical approach (calculating the actual P&L from yield changes). In this particular example, there is little benefit to the Monte Carlo over the parametric approach because the distribution and portfolio are so simple. If the portfolio included more complex assets, such as options that had highly nonlinear payoffs, then the benefits of the Monte Carlo approach would come to the fore.
As an example of using the Monte Carlo approach, I simulated 1,000 yield changes assuming a normal distribution. Figure 9.4 shows the histogram for the simulated yield changes, with the appropriate normal distribution overlaid. One thing to note is that for a finite sample, the empirical distribution will never be exactly normal with the originally assumed volatility. For the 1,000 yield changes used in this example, the calculated volatility was 7.32bp per day instead of the assumed 7.15bp per day. The dotted line in Figure 9.4 shows a normal distribution with volatility 7.15bp. The volatility for the empirical distribution is slightly, but not dramatically, different from what we originally assumed.

Figure 9.4 Distribution of Yield Changes, Parametric, and Monte Carlo Approaches
Note: The dotted line shows the assumed distribution for the yields—normal with volatility 7.15bp per day; the solid line shows a normal with the Monte Carlo volatility of 7.32bp per day; the histogram shows the Monte Carlo realization (1,000 simulated yield changes, assuming yields are normal).

The P&L is then calculated, as for the historical approach, usually using full revaluation. Figure 9.5 shows the histogram of the resulting P&L, with a normal curve overlaid.

Figure 9.5 Distribution of P&L, Monte Carlo Approach
Note: The histogram shows the P&L for a particular realization for the Monte Carlo approach (1,000 simulated yield changes, assuming yields are normal). The dotted line shows the P&L assuming yields are normal with volatility 7.15bp per day; the solid line shows the P&L with the Monte Carlo volatility of 7.32bp per day.

Step 4—Extract VaR, Volatility, and so on
From the distribution of P&L, we can get the volatility, VaR, expected shortfall, or whatever is our preferred risk measure. We should remember that although such measures, VaR, for example, are often talked about as if they were the primary goal in risk measurement, the P&L distribution is really the object of interest. The VaR is simply a convenient way to summarize the distribution (and specifically the spread of the distribution). We can use the volatility, the VaR, the expected shortfall, or some other measure (or combination of measures) to tell us about the distribution. But it is the distribution that is the primary object of interest and the VaR is simply a measure or statistic that tells us something about the P&L distribution.
When we use the parametric approach, we have an analytic form for the distribution, almost invariably normal with some volatility. In the preceding example, the distribution is:

The VaR is easy to calculate with this distribution. We simply ask what is the level Y of P&L such that there is a probability Z of experiencing worse:

where

We can look up the VaR off a table for the normal distribution, such as Table 8.1. We see from those that the 1%/99% VaR is 2.326 times the volatility, which means that for our example the 1%/99% VaR is $304,200.
For the historical approach, we have a set of values rather than a parametric functional form for the distribution. The histogram of the values is shown in Figure 9.3. For our example, there are 272 P&L values. If we want the volatility of the distribution, we simply apply the formula for the standard deviation:

In our example, the volatility is $130,800.
If we want the VaR, we need to get the quantile. We sort the observations and pick the nth from the smallest (most negative). Table 9.1 shows the four largest increases in yields and the resulting four most-negative P&Ls. For 272 observations, the 1%/99% VaR will be the third observation from the bottom (see the appendix to Chapter 8 for definition of the quantile). From Table 9.1, we see that this is −$296,000, so the VaR is $296,000.
Table 9.1 Largest Yield Changes for 1/08 to 1/09, and P&L for 10-year U.S. Treasury.

For Monte Carlo, we have a set of values, just as for the historical approach. In our example, we have generated 1,000 observations, but we usually would generate many more. The volatility and VaR are calculated just as for the historical approach. For this particular example, the volatility is $134,000 and the 1%/99% VaR is the 10th-smallest P&L, in this case −$315,100.
Table 9.2 shows the volatility and VaR for our example bond for the three approaches. There are some similarities and differences between them.
Table 9.2 Volatility and VaR Estimated from Three Approaches.




Volatility
1%/99% VaR




Parametric
130,800
304,200


Historical
130,800
296,000


Monte Carlo
134,000
315,100



 The volatility for the parametric and historical are the same because we fitted the parametric (normal) distribution to the historical volatility.
 The volatility for the parametric and Monte Carlo approaches are different because the Monte Carlo is for a finite number of draws. With only 1,000 draws, we should not be surprised at a difference of this magnitude.6
 The 1%/99% VaR for the parametric and historical are different because the historical distribution is not, in fact, normal. In this case, the VaR is smaller, indicating that for this particular example, the lower tail of the historical distribution does not extend as far out as the normal distribution.
 The VaR for the parametric and the Monte Carlo are different, but we should expect this, since the volatilities are different.

I want to focus for a moment on the difference in the VaR estimate between the parametric and historical approaches. The two distributions have the same volatility (we chose the parametric distribution to ensure that), but different VaRs. The historical distribution is probably not normal, but even if it were, there would still be random variation in the historical VaR, since it involves a finite number of observations. Whenever we use a limited number of observations to estimate a tail measurement such as the VaR, there will be sampling variation. Because we have so few observations in the tail, the estimate may have a fair degree of variability.
We can examine the random variability we should expect to see in the VaR using the distribution of order statistics given in the appendix to Chapter 8. For 272 observations the 1%/99% VaR will be the third-smallest observation. Using numerical integration to evaluate the formulae in the appendix, the 95 percent confidence bands for the third-smallest observation out of 272 from a normal distribution would be −2.836σ to −1.938σ (versus the true quantile for a continuous normal distribution of −2.326). Using the volatility in the table, this gives 95 percent confidence bands of −$371,000 to −$253,000. These are quite wide bands on the historical VaR, reinforcing the idea that we have to use tail measures such as VaR with care.
Using Volatility and VaR for Single Bond Position
We have just estimated the P&L distribution and summary measures (volatility and VaR) for a portfolio of a U.S. Treasury bond. Now we turn to how we might use this information. Focusing on the parametric approach, Table 8.1, reproduced here as Table 9.3, shows various combinations of Z and Y for the normal distribution.
With these data we can say some useful things about the bond position. The first and most important is that, knowing the volatility is roughly $130,800, we should expect to see P&L of more than ±$130,800 about one day out of three, since the probability of daily P&L lower than −130,800 or higher than +130,800 are each about 16 percent. This effectively calibrates the P&L for normal trading conditions: it is roughly $130,800, not $13,080 and not $1,308,000. This is considerably more than we knew before we started.
We could extend our understanding somewhat. Assuming normality of risk factors, as we have here, the volatility is $130,800 and the 5%/95% VaR is $215,100. What about VaR for lower probabilities? We could use Table 9.3, but we should have less confidence in VaR for low probabilities using the normal distribution. We have two simple alternatives. First, we could apply Litterman's rule of thumb that 4-sigma events occur about once per year. This would translate into saying that the 0.39%/99.61% VaR is about $523,200 (four times $130,800).
Table 9.3 Various Combinations of Probability (Z) and P&L (Y) for Normal Distribution (cf. Table 8.1)

We could alternatively assume that the risk factors are distributed according to a mixture of normals (1 percent chance of a high, five-times volatility day; that is, α = 1 percent, β = 5). Assuming all risk factors are simultaneously either low or high volatility, the portfolio distribution will also be a mixture of normals. Referring back to Table 8.4, we see that the 0.39%/99.61% VaR would be $356,700 (2.727σ), only slightly larger than implied by the simple normal distribution.
There are a few issues to emphasize at this point.

 First and most important, these numbers, as for all risk numbers, should be used respectfully. They are approximations to reality and one must recognize their limitations. I said earlier "the volatility is roughly $130,800" because one never truly knows the distribution of tomorrow's P&L. We might be confident with the order of magnitude (the volatility is not $13,080 nor is it $1,308,000) but one should not trust $130,800 absolutely. In Chapter 8, and again further on, we discuss uncertainty in the estimates. Basic common sense and market experience encourage care in using such estimates.
 Second, the volatility and VaR calculated here are summary measures based on history and simply summarize that history in one way or another. This is virtually always the case. That is no bad thing since knowing the past is the first step toward informed judgments about the future. It should, however, encourage humility in using the numbers, since the VaR and volatility tell us no more than what happened in the past, albeit in a concise and useful manner.
 Third, in estimating the volatility and VaR, we have made various assumptions. For example, with the parametric approach, we assumed the distribution of market risk factors was normal. For the historical approach, we assumed that the (relatively small) number of historical observations was a good estimate of the true distribution. For the Monte Carlo approach, we assumed market risk factors were normal. Such assumptions are necessary, but we must remember that our results depend on our assumptions and we should not put too much belief in our results unless we have absolute confidence in our assumptions—and anybody with experience in the markets knows the assumptions are never perfect.
 Fourth, we must assess how reasonable the assumptions are for the use we make of the results. In this case, the normality assumption for the parametric approach is reasonable for the central part of the distribution. I have considerable confidence in using the volatility and making judgments about standard trading conditions, that is, about the central part of the distribution. On the other hand, I would have much less confidence that the 0.1%/99.9% VaR is actually $399,500 (it is probably larger) since the tails are most likely fat and our assumption of normality will not capture this well.
 Fifth, and related to the preceding, it is always easier to estimate characteristics of the central part of the distribution than to estimate characteristics of the tails. The central part of the distribution can provide considerable insight, and this should be exploited, but when moving to the tails and extreme events, extra caution is necessary.
 Finally, I have focused heavily on the volatility. Volatility is appropriate and useful as a summary measure in this case because the P&L distribution is more or less symmetric. In the case of nonsymmetric distributions (for example, short-dated options with high gamma) volatility will be less appropriate.

Uncertainty in Volatility and VaR Estimates
The true value for volatility and VaR are never known with certainty. There will be a number of sources of uncertainty:

 The usual statistical uncertainty in the estimate due to the finite number of observations used to estimate the value of volatility or VaR.
 Erroneous assumptions about the underlying statistical model. For example, we usually make an assumption about the functional form of P&L distribution but the assumption may not be correct. (The statistical uncertainty mentioned earlier assumes the functional form is correct.)
 The world is nonstationary, with circumstances changing constantly, and so estimates based on the past may not be fully representative of the present, much less the future.

Let us examine the parametric volatility estimate for the U.S. Treasury bond, which is $130,800. We first consider the statistical uncertainty (the first of the preceding sources), assuming that the P&L distribution is in fact normal. The estimate is based on 272 observations. The appendix to Chapter 8 gives the formula for confidence intervals for the variance, and from this we can calculate the statistical uncertainty in the volatility estimate. The 95 percent confidence bands (2.5 percent on either side of the estimate) are shown in Table 9.4.
Table 9.4 Confidence Bands for Volatility and VaR Estimates.

Statistical uncertainty in the VaR will be the same as for the volatility (in percentage terms) since we are assuming normality and the VaR is just a multiple of the volatility.
The other sources of uncertainty are harder to evaluate. We can, however, calculate what would be the error if the P&L distribution were a mixture of normals (with α = 1%, β = 5) instead of a simple normal. Table 8.4 in Chapter 8 shows the VaR for a normal and mixture of normals, and these are reproduced in Table 9.5.
Table 9.5 VaR Levels for Normal and Mixture of Normal Distributions (α = 1%, β = 5)

For a moderate probability level (the 5%/95% VaR) there is little difference between the normal and mixture. For a low probability level, say 0.1%/99.9% VaR, there is a large difference, 38 percent difference between $404,100 (normal) and $752,600 (mixture). If the true distribution were a mixture of normals and we assumed normal, or vice versa, we would arrive at a 0.1%/99.9% VaR quite far from reality. This is an example of the large degree of uncertainty, particularly in the VaR for low probability levels, which can result from uncertainty in the true functional form of the P&L distribution.
In sum, even the best estimates of volatility and VaR are subject to uncertainty, sometimes considerable uncertainty.
9.3 Summary Measures to Standardize and Aggregate
Summary risk measures are used, primarily, in two related but conceptually distinct ways:

1. To standardize, aggregate, and analyze risk across disparate assets (or securities, trades, portfolios) under standard or usual trading conditions.
2. To measure tail risk or extreme events.

In this section, we discuss using volatility (or VaR) to standardize and aggregate. We turn to tail events in the next section.
Standardize under Normal Trading Conditions
Using volatility and VaR as tools for comparing across disparate assets under standard or normal trading conditions is relatively straightforward. To understand this use better, consider our simple portfolio, and say that a bond trader with experience in the U.S. government bond market is promoted to manage our hypothetical portfolio, which includes both U.S. Treasuries and French equities. From long experience in the bond market, the trader knows intuitively what the risk is of $20M in 10-year U.S. Treasuries (or any other U.S. bond, for that matter). Were this trader to manage only U.S. Treasuries, he would know from long experience how much particular trades might make or lose during a normal trading period, how trades would interact together in a portfolio, and have a good idea of how positions might behave during extreme conditions. But the trader has little experience with equities, does not have the same depth of experience and intuition, and needs some way to compare equity positions with something he knows. For example, how risky is a €7 million position in CAC futures?
The volatility (or alternatively, the VaR) is the simplest and most immediately informative tool for providing the manager with a comparison. By calculating the volatility for the equity trade, the manager can quickly gain insight into the riskiness of the equity and calibrate the equity versus familiar U.S. bond trades.
For the €7M CAC futures, an estimate of the volatility is $230,800 per day. This is the parametric estimate. We arrive at it in the same way we did for the aforementioned U.S. bond, following the four steps:

1. Asset to Risk Factor Mapping—Translate from assets held into market risk factors.

Here the mapping is as beta-equivalent notional, using the CAC index itself as the equity index. Since the instrument and the index are the same, the mapping is one to one.

2. Risk Factor Distributions—Estimate the range of possibilities for the market risk factors.

Assume that percent changes in the CAC equity index are normally distributed and estimate the volatility from data. The estimated volatility is 2.536 percent per day.

3. Generate the Portfolio P&L Distribution—Generate risk factor P&L and sum to produce the overall portfolio P&L distribution.

The mapping from asset to risk factor is one to one so, given the risk factor (the CAC index) has volatility 2.536 percent per day, the position will have volatility 2.536 percent per day. On €7 million or $9.1 million, this is $230,800.

4. VaR, Volatility, and so on—Estimate the VaR, volatility, or other desired characteristics of the P&L distribution.

We have the volatility, $230,800, already from step 3.


The volatility for $20 million of the U.S. 10-year bond is $130,800 per day; in other words, the equity position is substantially riskier than the U.S. bond position even though the equity notional is smaller, at €7 million or $9.1 million.
Here the volatility is used as a summary measure to allow a reasonable comparison of the distributions of P&L. This comparison of the P&L distributions works even though the securities are quite different. The bond is a U.S. bond requiring an up-front investment; the equity is a euro-based futures, a derivative requiring no up-front investment. Still, money is money and we can compare the profits and losses of the two positions. Like any summary measure, the volatility does not tell everything, but it does provide a valuable comparison between these two securities.
Aggregating Risk
We would also like to aggregate the risk across these two disparate securities. The volatility of the combined portfolio will not be the sum of the separate volatilities because the two securities provide some diversification. When the bond goes down, sometimes the equity will go up and vice versa. This incorporates the idea of portfolio or diversification effects. The next chapter covers portfolio-related information that can be mined from the P&L distribution and the portfolio volatility, but for now we simply ask what would be the volatility of the combined portfolio.
We turn again to the four-step process for generating the P&L distribution. Steps 1 and 2 are unchanged from before—we do the mapping and estimate the risk factor distributions for the bond and the equity separately. It is Step 3—generating the portfolio P&L distribution—that is now different. We first generate the distributions of yield and equity index P&L. This is the same as for the two assets on their own. These separate P&L distributions are shown in Figure 9.6.

Figure 9.6 P&L Distribution for Bond and Equity Futures
Reproduced from Figure 5.7 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

Now, however, we need to combine the distributions. It is very important that we are not simply adding the volatilities. We are combining the two distributions themselves. The easiest way to explain is to examine the P&L for the bond and equity as if we were doing historical estimation. A few of the historical observations might be as displayed in Table 9.6. For the first date, the yield fell by 2.7bp, leading to a bond profit of $49,450. The CAC index fell by 1.81 percent, for a loss of $164,400. For this date, the two assets moved in opposite directions and they net off for a portfolio loss of $114,900. For the second date, the bond and equity both showed profits.
Table 9.6 Sample Observations for Bond and Equity Risk Factors and Portfolio P&L.

We go through each day and calculate the overall portfolio P&L from the constituent bond and equity P&Ls. Some dates the assets move together, some dates opposite. The net portfolio P&L is the result of all the co-movements between assets.
Figure 9.7 shows the overall portfolio distribution with the separate bond and equity distributions overlaid. The dispersion of the overall portfolio P&L distribution is more than either the bond or the equity but less than the sum of the individual dispersions. Figure 9.7 shows both the portfolio volatility ($291,300) and the VaR ($479,200).

Figure 9.7 P&L Distribution for Portfolio of Bond and Equity Futures

Calculating the portfolio P&L day by day and combining to get the portfolio P&L is conceptually simple but computationally intensive. It is exactly what we do for historical and Monte Carlo estimation, but for parametric estimation, the bond and CAC distributions are normal, and normal distributions can be combined more easily. In fact, the sum of normals is normal with volatility that combines according to the rule:

In this case,

Table 9.7 summarizes the volatility for the individual assets and the portfolio assuming that the distributions are all normal (using parametric estimation). The portfolio volatility of $291,300 is more than either the bond or equity, but less than the sum of the stand-alone volatilities.
Table 9.7 Volatility for Government Bond and CAC Equity Index Futures.

There are a few points we need to emphasize regarding how we are using volatility here:

 The hypothetical manager is using VaR or volatility to compare one trade versus another and analyze the effect of aggregating trades under usual or normal trading conditions. As a result, it makes sense to focus on the central part of the distribution and to use volatility.
 The comparison or calibration of the equity trade versus U.S. trade is a useful guide but not the final word. Apart from other considerations, the volatility estimates are based on history, and particular circumstances may make the history more or less representative in one market versus the other.
 The comparison is focused primarily on normal trading conditions. To measure extreme events, the manager should consider additional information or alternative approaches. For example, the manager might want to extrapolate from knowledge of and experience in the U.S. market, or rely on other more detailed analysis of the French equity market.

The idea of using summary risk measures as tools for comparison across disparate trades is straightforward but quite powerful. The example of comparing a single 10-year U.S. trade versus a single equity trade is simple but it captures the essence of the approach. Realistic portfolios will consist of many trades. Such complexity would make it difficult for even an experienced manager to grasp the portfolio risk based on intuition alone. Using volatility or VaR to compare trades is particularly compelling in this example because the products are so different—different asset class, different currency, one a cash bond and the other a derivative. Most managers will not have the intimate familiarity with such a variety of products that they can dispense with these quantitative tools. When introducing new products or new sectors, with risk where the manager has little familiarity, using tools such as volatility or VaR for comparison becomes even more valuable.
9.4 Tail Risk or Extreme Events
The second important use of summary risk measures is in evaluating tail risk or extreme events. VaR and expected shortfall are specifically intended to capture the tail of the P&L distribution.
We might use the 1%/99% VaR to get an idea of what a large P&L might be. The 1%/99% VaR for the U.S. bond is −$304,200, which means we have roughly a 1 percent chance of seeing a loss worse than $304,200. In a period of 100 trading days, we should expect to see a loss worse than $304,200. This is not a worst case, merely a regularly occurring nasty event with which one should be comfortable.
Most usefully, the 1%/99% VaR gives an order of magnitude to the P&L. One should be very surprised to see a loss worse than $3,042,000 (10 times the VaR estimate), and equally surprised if there were no losses worse than $30,420 during a period of 100 days.
But we should not rely on the figure $304,200 absolutely—there are many sources of uncertainty and error in the estimate of $304,200. We must use the VaR with caution. In particular, the further we move out in the tail, the more difficult it is to estimate anything with confidence. We usually have two alternatives, both of which give imprecise estimates, although for somewhat different reasons:

1. Use all observations to estimate the P&L distribution. We will have a large number of observations, lowering the statistical error. Unfortunately, the estimated distribution may conform to the central part of the distribution (with the bulk of the observations) at the cost of poor fit in the tails.
2. Use tail observations to fit the tail. But then, we have only a handful of observations, and the statistical error will be high.

For parametric estimation, we have taken the first course and assumed that the distribution is normal. The tails do fit less well than the central part of the distribution. We could instead assume that the distribution was a Student-t rather than normal. This would give a 1%/99% VaR equal to −$335,600 instead of −$304,200.
Alternatively, we could take the second course and use tail observations. With only 272 observations, however, the bottom 1 percent consist of only two or three observations—hardly enough to say anything with confidence.
It is often difficult to measure the tails with confidence except by incorporating external information. Such information might be the general shape of tails taken on by financial returns, inferred from past studies or other markets. Such issues are discussed in somewhat more depth next.
Simple Parametric Assumptions—Student-t and Mixture of Normals
The first and simplest approach, discussed in Section 8.4, is to replace the assumption of normality in estimating parametric or Monte Carlo VaR with the assumption of a Student-t or a mixture of normals. Both a Student t distribution and a mixture of normals have fat tails relative to the normal but is still relatively simple.
The three distributions (normal, t distribution, and mixture of normals) are characterized by the parameters shown in Table 9.8 (assuming mean zero for each):
Table 9.8 Parameters for Normal, Student t Distribution, and Mixture of Normals.

A simple approach for fitting the t distribution and mixture distributions is as follows:

 Choose a value for the nonscale parameters.

For the t distribution degrees-of-freedom values of 3 to 6 seem to be reasonable (cf. Jorion, 2007, 130).
 For a two-point mixture of normals, a high-volatility probability (α) of around 1 percent/5 percent and high-volatility magnitude (β) of around 3 to 5 seem reasonable.

 Conditional on these parameters, calculate the scale parameter by equating the sample (observed) variance and the distribution variance.

Remember, as discussed in Chapter 8, that the t distribution does not add as the normal distribution does: the sum of two t distributions is not t distributed. This makes the t distribution less useful in a portfolio context. For the normal distribution, we can assume that the individual risk factors are normally distributed and the portfolio P&L (the sum of individual asset P&Ls) will also be normal. We can calculate the portfolio variance from the individual risk factor variance-covariance matrix by a straightforward matrix multiplication. This mathematical simplicity will carry over to a mixture of normals but not to the Student t distribution.
Single Asset
For the U.S. Treasury bond considered earlier, the observed standard deviation was $130,800. Assuming the t distribution degrees of freedom is n = 6 and that the mixture high-volatility probability and magnitude are α = 0.01 and β = 5, this gives values for the parameters for a normal, t distribution, and mixture of normals as shown in Table 9.9.
Table 9.9 Values for Parameters for Normal, t Distribution, and Mixture of Normals.

These parameters produce the densities shown in Figure 9.8. The densities do not look dramatically different and they are not very different in the central part of the distribution. In the tails, however, the t distribution and the mixture of normals diverge substantially from the normal.

Figure 9.8 P&L Distribution for Bond and Equity Futures
Note: The t-distribution has degrees of freedom ν = 6 and the mixture of normals has α = 1%, β = 5.

Table 9.10 (reproducing some of Table 8.4) shows the levels VaR (in dollar terms and as multiples of the volatility or standard deviation). The tails of the Student t and the mixture of normals are fat relative to the normal. The difference is not substantial at the 5 percent or 1 percent level, but for the 0.1%/99.9% VaR, the Student t value of $556,100 is 1.4 times larger than the normal, while for the mixture of normals, it is 1.9 times larger. This compares reasonably with Litterman's rule of thumb, which is that for a probability of 0.39 percent, the actual VaR is 1.5 times larger than predicted by normality.
Table 9.10 VaR for Normal, Student t, and Mixture of Normals.

Multiple Assets
The Student t distribution does not work well with multiple assets because the sum of t variates will generally not be t distributed. We cannot, as a result, apply the simple parametric approach using the t distribution, although we could still consider using it for Monte Carlo.
The mixture of normals, however, does work well with multiple assets and the parametric approach, since jointly normal variates sum to a normal variate. We can demonstrate this by using our example of the U.S. Treasury bond and the CAC futures. We must assume that the low volatility and high volatility regimes occur simultaneously for all assets, that is, both the U.S. Treasury and the CAC index are in the low volatility regime, or they are both in the high volatility regime. This is a reasonable assumption, since extreme events and crises in the financial markets tend to affect all markets together, not each market separately and independently.
The assumption that all assets are in the same regime means that, conditional on the regime, the P&L distributions are jointly normal. This makes the mathematics simple since the sum of normals itself is normal. For the example of the U.S. Treasury and the CAC, we assume that each is a mixture of normals, with α = 0.01 and β = 5. But there is an important point here. The assumption that all assets are in the same regime means that α is the same for all assets. But there is no necessity that β, the ratio of the high-to-low volatility regime, be the same for all assets. Furthermore, the correlation between assets need not be the same in the low and high volatility regimes. In a multivariate context, this means that we can allow the variance-covariance matrix to differ between the low and the high volatility regimes. What is important is that assets are jointly normal in each regime with some given variance-covariance matrix, and that all assets are in either the low or the high volatility regime simultaneously.
Table 9.11 shows how the distributions combine for our simple two-asset portfolio. Across a row (that is, in a particular regime) the P&L for multiple assets combine as a multivariate normal. In other words, the combined P&L will be normal with volatilities (standard deviations) combining according to the standard rule, as, for example, in the low volatility regime:

Down a single column, the P&L distribution for an asset (or the overall portfolio) is a two-point mixture of normals.
Table 9.11 Details for Portfolio Volatility Calculation Assuming Mixture of Normals.

The individual asset distributions will be mixture of normals, and will have fat tails relative to a normal distribution. This carries over to the overall portfolio, which is also a two-point mixture of normals. When the correlation is the same across regimes and the ratio of high-to-low volatility regimes is the same for all assets (βT = βC = β), then the portfolio P&L mixture distribution has the same parameters as the individual asset distributions (specifically the same β). In the more general case, however, the ratio of high-to-low volatility for the portfolio (σhi/σlo) will be some complicated function. Nonetheless, the calculation of this ratio and thus the P&L distribution is straightforward. It will require two separate portfolio calculations by the parametric approach, separately for the low and the high volatility regimes, but each of these is computationally simple.
Table 9.12 shows that the 0.1%/99.9% VaR is substantially higher than for the normal distribution with the same volatility.
Table 9.12 Results for Portfolio Volatility Assuming Mixture of Normals.

The two-point mixture of normals is simple but it does capture fat tails, the most important feature that is missed when assuming normality. Since it is based on normality, it is suitable for extending the parametric estimation methodology to capture some aspects of fat tails. We should not underrate the value of an approach that can build on the simplicity and computational speed provided by the parametric approach while also modeling fat tails.
As pointed out earlier, we can allow in the multivariate context for a dependence structure that differs between the high and low volatility regimes. For a standard multivariate normal, the dependence does not vary with the size of the P&L, but experience indicates that correlations increase under extreme conditions: the rule of thumb is that in a crisis, correlations move to one (plus or minus, against the portfolio). The correlation in the high volatility regime can be chosen closer to one (larger in absolute value). This will produce greater dependence (higher correlation) in the tails than in the central part of the distribution, but do so in a computationally simple manner.
Extreme Value Theory—Fitting Tails
We discussed extreme value theory (EVT) in Section 8.4. With EVT, rather than choosing a distribution with appropriate tail behavior and then fitting the entire distribution, we fit only the tails of the distribution, the maximum (or minimum) values. The generalized extreme value (GEV) distribution provides a limiting distribution for maximums and minimums. Whatever the form of the P&L distribution (under mild regularity conditions and suitably normalized), the distribution of the maximums converges asymptotically to the GEV.
In practice, it is generally better to use the generalized pareto distribution (GPD) and threshold exceedances rather than the maximums. Threshold exceedances are values that exceed some chosen high threshold. Remember from Section 8.4 that the excess distribution function gives the probability conditional on the random variable (the P&L) exceeding some specified level u.
Let X be the variable representing the random P&L. We focus on the exceedance X − u, the amount by which X exceeds the level u, and on the size of the exceedance y (which will be non-negative). The probability that the exceedance X − u is less than an amount y (conditional on X exceeding u) is the excess distribution:

The GPD is given by:

where β > 0 and y ≥ 0 for ξ ≥ 0 and 0 ≤ y ≤ − β/ξ for ξ < 0
The GPD is useful for modeling the excess distribution function because the excess distribution is simple for the GPD: the excess distribution for GPD is also GPD:

We assume that if we choose a high but finite threshold u, the observed excess distribution function will actually be GPD.
We can illustrate the process using the U.S. Treasury bond we have been considering. We have 272 observations and we will choose the exceedance level u to be 18bp (aproximately -$329,256) a level that includes five observations, or less than 2 percent of the observations.7 Maximum likelihood estimation is straightforward (assuming independence). The log-likelihood function is simple:

The five largest-yield changes are shown in Table 9.13, together with the contribution to the likelihood (at the optimum).
Table 9.13 Five Lowest Observations for Fitting GPD Parameters (Using Yield Changes)

Maximizing the likelihood function gives, approximately,

so that the excess loss distribution, the distribution of yield changes conditional on the change being larger than 18bp, is:

(Remember that y is the size by which the yield change exceeds the threshold, 18bp, and x is the size of the yield change. Table 9.13 shows the variable x.)
As the value of the shape parameter, ξ, tends to zero, the tails become less fat (zero corresponds to exponential as for a normal distribution), while for larger ξ, the tails become increasingly fat: for ξ = 0.5, the variance is not finite, and for ξ = 1, the mean is not finite. The value of ξ = 0.14 estimated here should not be taken seriously given the insignificant number of observations. It is somewhat low relative to many studies of financial returns, which find values in the range of 0.2 to 0.4 for stock market data (see, for example, McNeil, Frey, and Embrecht 2005, 280 ff; Jorion 2007, 130).
Figure 9.9 shows the estimated GPD excess loss distribution, together with that for a normal, mixture of normals (α = 1%, β = 5), and Student t (degrees-of-freedom 6). The normal tails off very fast (meaning low probability of large losses) while the other distributions show larger probability of large losses.

Figure 9.9 Excess Loss Distribution for Fitted GPD and Other Distributions

Using the definition of the excess distribution and the assumption that the excess loss distribution is GPD (and noting that here the variable x measures loss level, while the preceding y measures the exceedance), one can show that for any level of loss beyond u:8

The VaR (or any other risk measures) cannot be calculated from the fitted excess loss distribution alone, since the probability F(u) is required. The fitted GPD distribution can be combined with the empirical estimator of the threshold probability:

Calculating the VaR for the chosen level u itself provides no benefit, as it just reproduces the empirical quantile. The benefit of the GPD or tail exceedance approach is in extrapolation to more extreme tail probabilities based on the fitted GPD; the fitted GPD should be a better (and smoother) fit to the tail data than the empirical distribution.
For VaR levels beyond u (1 − Z ≥ F(u)):9

and the expected shortfall (assuming ξ < 1) is:

Tables 9.14 and 9.15 show the estimated VaR and expected shortfall for the four functional forms (normal, GPD, Student t with 6 degrees of freedom, and mixture of normals with α = 1% and β = 5) for more extreme levels of Z: the 1%/99% VaR and the 0.1%/99.9% VaR (see Section 8.4).
Table 9.14 VaR and Expected Shortfall (Yield, in bp) for Alternative Functional Forms.

Table 9.15 VaR and Expected Shortfall (Dollars) for Alternative Functional Forms.

As expected, for the more extreme tail probability (0.1%/99.9%) the thinner tail of the normal relative to the other distributions generates lower VaR and expected shortfall.
Copulas—Non-Normal Multivariate Distributions
We have used the two-point mixture as a simple model for a non-normal multivariate distribution. Copulas provide the mathematical structure to do this more generally, although we will see that the two-point mixture is still quite useful. Copulas are most suitable for Monte Carlo applications since the distributions generated with copulas are often easy to simulate but have no simple closed-form or analytic expressions.
The essence of the copula approach is that it allows one to specify the marginal distributions and the dependence structure (the copula) separately, then combine the two to produce a multivariate distribution. Alternate marginals and copulas can be mixed and matched to produce both standard distributions (for example, multivariate normal) and hybrid or what McNeil, Frey, and Embrechts (2005, 192) call meta distributions.
Here we will use the copula approach and Monte Carlo to calculate the VaR for our example portfolio of $20 million of 10-year U.S. Treasury bond and €7 million of the CAC index futures. We will make five distributional assumptions for the risk factors (yields and equity index):

1. Bivariate Normal—Normal marginals and normal copula—neither fat tails nor high probability of joint extreme events.
2. Hybrid Student/Normal—Student t distributed marginals (3 degrees of freedom) and normal copula—produces fat tails but normal copula means variables start to behave independently for joint extreme events.
3. Usual Bivariate Student t—Student t distributed marginals and usual Student t copula (again 3 degrees of freedom)—produces fatter tails and Student t distribution copula produces increased probability of joint extreme events.
4. Alternate (Product) Bivariate Student t—Student t distributed marginals and a Student t copula that reduces to a product distribution (independence) for correlation zero. This distribution also behaves independently for joint extreme events.
5. Two-Point Mixture of Normals (α = 2%,β = 4)—Looks similar to the bivariate Student t distribution with many joint extreme observations.

The difference between the usual bivariate t distribution and the product bivariate Student t requires comment and is elaborated upon shortly.
First, however, it is worth taking a moment to review exactly how and why a multivariate distribution is used in calculating the portfolio P&L distribution. The portfolio distribution is usually of primary interest and the individual asset distributions only intermediate steps. The only way to obtain the portfolio P&L distribution, however, is to build up from the individual assets (as discussed in Sections 8.3 and 9.2, and outlined in Figure 8.7). The four steps that produce the P&L distribution are:

1. Asset to Risk Factor Mapping—Calculate transformation from individual assets to risk factors
2. Risk Factor Distributions—Estimate the range of possible levels and changes in market risk factors
3. Generate P&L Distribution—Generate risk factor P&L and sum to produce the portfolio P&L distribution
4. Calculate Risk Measures—Estimate the VaR, volatility, or other desired characteristics of the P&L distribution

Joint normality is popular because if the mapping in Step 1 is linear and the risk factor distributions in Step 2 are multivariate normal, then the summation in Step 3 can be done analytically—it requires only a matrix multiplication to calculate the portfolio volatility. In such a case, the individual risk factor P&Ls will be normal, the sum of normals is normal, and the resulting portfolio P&L will be normal. This reduces mathematical and computational complexity enormously.
If the risk factors are non-normal or the P&L functions are far from linear, the summation in Step 3 becomes laborious and the P&L distribution will not be a simple form. In such cases the overall portfolio distribution has to be estimated using Monte Carlo, a long and computationally intensive process.
When the distribution is not jointly normal (for the hybrid Student/normal, the usual bivariate Student t, and the alternate Student t) the portfolio distribution will not be Student t or normal even if we assume the transformation in Step 1 is linear, since the sum of risk factor P&Ls or convolution of the distributions will give something new and analytically intractable.10 Monte Carlo will be the only feasible computational approach.
I briefly review the computational approach for simulating the meta distributions we use here, although for a more detailed discussion, the reader should go to McNeil, Frey, and Embrechts (2005, 193, 66, and 76). For the simulation of Student t marginal combined with normal copula, the two-step process for each draw of our bivariate random variable is:

Step 1. Generate a normal copula:

For each draw, generate a standardized bivariate normal with mean zero, unit variances, and the desired correlation matrix:

Calculate

where Φ(·) is the normal CDF.
This will be a normal copula. More precisely, the random vector U will have a normal copula distribution with correlation matrix R.
The numbers Ui will be in the interval [0,1] and will look like probabilities.

Step 2. Generate the joint distribution with the desired marginals:

Calculate

where (·) is the univariate Student t distribution inverse CDF or quantile function.
This will now have a marginal Student t distribution with normal dependence structure.


The process for other copulas and marginals should be obvious. For example, a Student t copula would use the t distribution CDF instead of the normal CDF in Step 1, and normal marginals would use the normal inverse CDF in Step 2.
To demonstrate the use of copulas in Monte Carlo estimation of the portfolio distribution, we will consider a portfolio of $20 million of the10-year U.S. Treasury bond and €4 million of the CAC equity index futures.11
Figure 9.10 shows the bivariate distribution for our five risk factor distributions. Each point represents one realization (5,000 in total) for the joint bond P&L (on the horizontal) and the equity futures P&L (on the vertical). The dashed lines are 3-sigma bars. The volatility of the bond P&L is $130,800, and the equity P&L is $131,900.

Figure 9.10 Monte Carlo Results for Two-Asset Portfolio, Alternate Meta Distributions through Copulas

Some of the highlights that we should note include:

 The normal-normal (jointly normal or normal marginals and normal copula) shows virtually no extreme observations, outside 3-sigma. This is as we would expect for a normal distribution since a normal has few extreme values.
 The Student-normal (Student t marginals 3 degrees of freedom, normal copula) shows observations outside the 3-sigma bars, as we would expect, given the Student t distributed marginals. With the normal copula, however, there are virtually no joint extreme observations. With the normal dependence structure inherited from the normal copula, there are many individually extreme events but virtually no jointly extreme events. This argues against a normal copula to model financial markets, in spite of the familiarity we have with the simple dependence structure (linear correlation) inherent in the normal copula.
 The Student-Student (usual bivariate Student t distribution, 3 degrees of freedom, with Student t marginals and Student t copula) shows many joint extreme events. This matches what we seem to see in markets, with simultaneous large moves in both assets.
 The alternate Student (discussed more further on), like the preceding Student-normal, shows virtually no joint extreme events.
 The mixture (two-point mixture of normals with α = 2%, β = 4) shows many joint extreme events, and in this respect looks much more like the bivariate Student t than any of the other distributions.

We now turn to a short digression on the Student t distribution. For the t distribution there is not a unique multivariate form, unlike the joint normal distribution. Shaw and Lee (2007) discuss the issue in some detail. The easiest way to see this, and to understand the tail dependence embedded in the usual t distribution, is to examine the construction of a multivariate Student t distribution by Monte Carlo (see McNeil, Frey, and Embrechts 2005, section 3.2, particularly 75 and 76, as well as Shaw and Lee 2007, 8 ff). The usual multivariate Student t (all marginals the same degrees of freedom, ν) is a normal mixture distribution, generated as:


Y  Nk(0, Ik) is a multivariate standardized normal random variable.
 is a (univariate) chi-squared random variable with ν degrees of freedom.

This construction shows immediately why a Student t variable has fat tails relative to a normal: small draws for the chi-squared variable blow up the normal variables. This produces a small chance of values that are large relative to the normal distribution. As the degrees of freedom gets large, the ν in the numerator offsets the potential small values of the chi-squared variable and the t tends to the normal. This construction also shows why this multivariate t has joint extreme values. Each and every normal is divided by the same chi-squared variable. A small draw for the χ2 will inflate all instances of the normal at the same time, producing joint extreme values.
As Shaw and Lee point out, however, this is not the only possible construction for a multivariate Student t distribution. Instead of using a single χ2 applied to all the dimensions of the multivariate normal, we could apply a separate χ2 to each dimension. In fact, this is a convenient way to produce a multivariate Student t with marginals having different degrees of freedom. Such a construction also has the benefit that zero correlation will produce independence. For the usual multivariate t distribution, zero correlation does not imply independence. Again, the preceding construction shows why. For the usual multivariate Student t distribution (with a single χ2), the tails will be correlated because the single χ2 applies to all dimensions. As we move further out in the tails, this effect dominates and even zero correlation variables become dependent.
The "Alternate Student" shown in Figure 9.10 is constructed using a separate χ2 for each of the yield and equity index variables. As we see in Figure 9.10, however, such a construction is probably not useful for a joint market risk factor distribution. It does produce extreme events but virtually no joint extreme events, no events that produce large changes for both yields and equity indexes. The dependence behavior in the tails seems to be similar to that for the normal copula. We return to this alternate Student t distribution again in Chapter 11. In the application there, however, we will see that the independence of tail events embedded in the alternative Student t is more appropriate than the usual Student t.
Returning to Figure 9.10 and the exercise of comparing copulas, we can draw three conclusions. First, joint normality is problematic for modeling tail behavior. This goes beyond the well-known result that normal marginals have thin tails. There is a more subtle effect related to the dependence structure embedded in joint normality. The scatter plot for the Student-normal distribution highlights this—joint extreme events are exceedingly rare. Second and related, we need to think carefully about joint dependence in the tails. Although this is not easy, copulas can provide some tools to aid in this. Third, the two-point mixture of normals seems to work well as a computationally simple approximation. The Student t distribution, although popular, does not have the benefit of simple analytic tools available for the normal or mixture of normals.
For completeness, Table 9.16 shows the volatility and VaR for the bond and the overall portfolio (the equity is not displayed in the interests of clarity).
Table 9.16 Volatility and 0.1%/99.9% VaR for Bond and Overall Portfolio, Alternative Joint Distributions.

9.5 Conclusion
Chapter 9 has demonstrated the use of volatility and VaR for measuring risk, using our simple portfolio of a U.S. Treasury bond and CAC equity index futures. Volatility and VaR are exceedingly valuable for measuring risk but they do not tell us very much about the sources of risk or the structure of the portfolio.
We now turn, in Chapter 10, to the portfolio tools that help untangle the sources of risk and provide some guidance toward managing risk. Like all these tools, however, we have to remember that the numbers produced provide only guidance. Nature can always come up with unanticipated events. We need to use these tools combined with common sense. Risk management is first and foremost the art of managing risk.
9.6 Appendix 9.1: Parametric Estimation Using Second Derivatives
We now go through an example of using second derivatives for parametric estimation, as laid out in the appendix of Chapter 8. As discussed there, second derivatives can be used with an asymptotic Cornish-Fisher expansion for the inverse CDF to provide a flag for when nonlinearities are large enough to make a difference in quantile (VaR) estimation.
For parametric or linear estimation (for a single risk factor, using first derivatives only and assuming μ = 0) the portfolio mean and variance are:

1st moment: zero
2nd moment: δ2σ2

Using second derivatives, the first four central moments are:

We can examine this for the $20M position in the 10-year U.S. Treasury. A bond is a reasonably linear asset and so we should expect no nonlinear effects from including the second derivatives. The delta and gamma (first and second derivative, or DV01 and convexity) are:

while the volatility of yields is 7.15bp per day. This gives the following moments:




Linear
Quadratic




Mean
0.0
450


Volatility
130,789
130,791


Skew
0.0
0.0206


Excess Kurtosis
0.0
0.0006


The skew and kurtosis are so small they clearly will have no discernible impact on the shape of the P&L distribution.
In contrast, an option will have considerable nonlinearity (gamma or second derivative) and thus we should expect to see considerable nonlinear effects in the P&L distribution. Consider short $20M of a six-month option on a five-year par bond or swap.12 The delta and gamma will be:

while the volatility of yields is 7.0735bp per day. This gives the following moments:




Linear
Quadratic




Mean
0.0
−3,399


Volatility
21,392
21,864


Skew
0.0
−0.9178


Excess Kurtosis
0.0
1.1321


These are reasonably large. To see whether they are large enough to alter the quantiles of the P&L distribution, we can use the Cornish-Fisher expansion (from Appendix 8.2) to calculate approximate quantiles.

where
y = μ + σw is solution to inverse pdf: F(y) = prob, that is, approximate Cornish-Fisher critical value for a probability level prob.
x = solution to standard normal pdf: Φ(x) = prob, that is, the critical value for probability level prob with a standard normal distribution (note that this is the lower tail probability so that x = −1.6449 for prob = 0.05, x = 1.6449 for prob = 0.95).
m3 = skew
m4 = excess kurtosis
γ3 = κ5/σ5
κ5 = 5th cumulant
Table 9.17 shows the approximate quantiles calculated from the preceding expression. (The first order includes only the m3 term in the first square brackets, the second order includes m4 and  in the second square brackets but excludes all the terms in the third square bracket.) The approximate quantiles show that the nonlinearity in the option payoff substantially alters the P&L distribution relative to the normal, with the lower tail being substantially longer (the 1 percent lower quantile being below −3 versus the −2.326 normal quantile) and the upper tail shorter.
Table 9.17 Approximate Quantiles for Short Option Position.

The final row shows the actual quantile. One can see from this that using only the leading terms in the Cornish-Fisher expansion does not provide a particularly good approximation for the quantiles far out in the tails. Although the second-order approximation is good for the 84.1 percent and 15.9 percent quantiles, it is increasingly poor as one moves further out to the 99 percent and 1 percent quantiles in the tails.
Examining Table 9.17 one might think that the first-order expansion works better than the second order in the tails but such a conclusion is not justified. The fact is that the expansion may be nonmonotonic, particularly when the skew and kurtosis are large. Consider the same option but with only one month rather than six months until maturity. Such an option will have higher gamma and thus more skew. Table 9.18 shows the resulting quantiles and approximations. For both the first- and second-order approximations, the approximate quantiles in the upper tails decrease rather than increase. The approximation with a finite number of terms is simply not very good. (See Chernozhukov, Fernandez-Val, and Galichon 2007 for a discussion of some methods for improving the approximation.)
Table 9.18 Approximate Quantiles for Short Option Position—One-Month Option (high gamma)

Nonetheless this approach does exactly what is intended—provides a flag for when the distribution deviates substantially from normality due to nonlinearity in the asset payoff. The conclusion is that using the second derivatives together with the Cornish-Fisher expansion provides a flag for when nonlinearity becomes important, even though it may not itself provide an adequate approximation.
Notes
1. I will assume that readers are familiar with basic finance and investments, such as that covered in Bailey, Sharpe, and Alexander (2000).
2. See Coleman (1998b) for an overview of bond DV01 and sensitivity calculations.
3. The accrued interest on the 27th would be 0.756. Note that using the linear approximation would give a P&L for a 5bp change in yields of about $91,440, not very different.
4. These data are synthetic but correspond roughly to the period January 2008 through January 2009.
5. The fit is not perfect but the empirical distribution is actually not too far from normal. Statistically, we can reject normality but we cannot reject a Student-t with six degrees of freedom.
6. Remember from the appendix to Chapter 8 that the variance s2 for a normal sample with variance σ2 is distributed such that . This means that for 1,000 draws there is a 5 percent probability the sample (Monte Carlo) variance may be 8.6 percent lower or 9.0 percent higher than the true variance (so the volatility would be 4.4 percent lower or 4.4 percent higher).
7. This is just for illustrative purposes. In practice, one would need to use a much larger sample size. See McNeil, Frey, and Embrechts (2005, 278 ff) for discussion of estimation and the likelihood function.
8. See McNeil, Frey, and Embrechts (2005, 283) for derivation of this and the following formulae.
9. Note that my Z = 1 percent (to denote the 1%/99% VaR) corresponds to α = 99 percent for McNeil, Frey, and Embrechts.
10. When assuming the risk factor distributions are two-point mixtures of normals, the portfolio P&L will also be a two-point mixture and can be calculated analytically, as discussed earlier.
11. This is slightly modified from the portfolio we have been using to better highlight the differences across risk factor distributions. The amount of the CAC futures is less to make the volatilities of the bond and futures positions more equal, and I assum that the correlation between yields and the equity index is 0.5 instead of 0.24 as in all other examples considered in this book.
12. A six-month out-of-the money option struck at 1.68 percent with the forward rate at 1.78 percent (put on rates or call on the bond or receiver's swaption) with the short rate at 0.50 percent and volatility 20 percent.









Chapter 10
Portfolio Risk Analytics and Reporting
Managing risk requires actually making decisions to increase, decrease, or alter the profile of risk. Making such decisions requires knowing not just the level of risk (the dispersion of the P&L distribution) but also the sources of risk in the portfolio and how changes in positions are likely to alter the portfolio risk. Risk measurement, to support this, must not only measure the dispersion of P&L (the primary focus for Chapters 8 and 9), but also the sources of risk. Litterman (1996, 59) expresses this well:
Volatility and VaR characterize, in slightly different ways, the degree of dispersion in the distribution of gains and losses, and therefore are useful for monitoring risk. They do not, however, provide much guidance for risk management. To manage risk, you have to understand what the sources of risk are in the portfolio and what trades will provide effective ways to reduce risk. Thus, risk management requires additional analysis—in particular, a decomposition of risk, an ability to find potential hedges, and an ability to find simple representations for complex positions.
In this sense, risk management merges into portfolio management. The present chapter discusses some of the tools and techniques suitable for such portfolio risk analysis. I particularly focus on:

 Volatility and triangle addition as an aid for understanding how risks combine.
 Marginal contribution to risk (also known in the literature as risk contribution, VaR contribution, delta VaR, incremental VaR, component VaR) as a tool for understanding the current risk profile of a portfolio.
 Best hedges and replicating portfolios as a tool for picking hedges to alter the risk profile and understanding the risk profile of a portfolio.
 Principal components as a data reduction and risk aggregation technique.

Many of the ideas in this chapter are based on Robert Litterman's Hot Spots and Hedges (Litterman 1996), some of which also appeared in Risk magazine March 1997 and May 1997. The idea of contribution to risk was developed independently by Litterman and M. B. Garman (Risk magazine 1996).
These techniques are most suitable for measuring and understanding risk under standard trading conditions (as opposed to tail events). That is, these techniques are most suitable when applied to the volatility (or the VaR for a large value of Z, such as the 16%/84% VaR). This is not a weakness—remember that risk must be managed every day and most trading days are standard conditions. Furthermore, many of the techniques are based on linear approximations (in other words assuming that the driving risk factors are normal and the positions linear in these factors). These techniques will not capture nonlinearities. This is important to remember, as nonlinearities have become increasingly important over the past 20 years with the increasing use of options and other nonlinear instruments. But the issue of nonlinearity should not be blown out of proportion. The linear approach has great utility in situations in which it is applicable. A simple approach can provide powerful insights where it is applicable and many, even most, portfolios are locally linear and amenable to these techniques. Again, Litterman (1996, 53) summarizes the situation well:
Many risk managers today seem to forget that the key benefit of a simple approach, such as the linear approximation implicit in traditional portfolio analysis, is the powerful insight it can provide in contexts where it is valid.
With very few exceptions, portfolios will have locally linear exposures about which the application of portfolio risk analysis tools can provide useful information.
10.1 Volatility, Triangle Addition, and Risk Reduction
As a guide for understanding how the risk of assets combine to yield the total portfolio risk, volatility and linear approximations are extremely useful. Volatility may not be the perfect measure of risk, but the intuition it builds regarding the often-nonintuitive aggregation of risk is effective, even invaluable.
The volatility of a portfolio is calculated from the volatilities of two assets according to:1
(10.1a) 
The cross term 2ρσ1σ2 means that the portfolio variance and volatility are not the simple addition of the position variances or volatilities. In fact, position volatilities combine like the legs of a triangle:2
(10.1b) 
The two expressions will be equivalent when cos θ = - ρ:

Consider the portfolio of $20M U.S. Treasury bond and €7M nominal of CAC equity index futures considered in Chapter 9, shown in Table 10.1.
Table 10.1 Volatility for Government Bond and CAC Equity Index Futures (reproduced from Table 9.7)
Based on Table 5.2 from A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

In Figure 10.1, Panel A shows the combination of the two volatilities as the two legs of a triangle. For the triangle, the A-leg is shorter than the sum of B + C because the angle is less than 180° (the correlation is less than 1.0). In terms of volatilities, the portfolio volatility is less than the sum of the UST and CAC volatility because the correlation is less than 1.0. If the angle were 180° (the correlation were 1.0), then the resulting leg (the portfolio volatility) would be the straightforward sum.

Figure 10.1 Volatilities Combine as Legs of a Triangle (Vector Addition)
Reproduced from Figure 5.12 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

Panel B shows the situation when the CAC position is reversed: - €7M nominal of CAC futures instead of + €7M. The individual volatilities are the same, but the portfolio volatility (the A-leg of the triangle) is now shorter—the portfolio volatility is only $236,400. Were the angle to be 0° (correlation -1.0), the length of the A-leg (the portfolio volatility) would be much reduced (in fact, only $100,025).
Table 10.2 Risk Reduction Potential for Various Levels of Correlation.



Correlation
Angle θ
Risk Reduction Potential




−0.99
8.1°
85.9%


−0.90
25.8°
56.4%


−0.80
36.9°
40.0%


−0.50
60.0°
13.4%


−0.25
75.5°
3.2%


Note: This shows the proportional reduction in volatility, {Vol(no hedge) - Vol(hedge)}/Vol(hedge), that is possible with various values for the correlation between the original asset and the hedging asset. Reproduced from Table 5.3 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.



Correlation and Risk Reduction Potential
The triangle addition for volatilities can be used to understand the potential for risk reduction and how this varies with the correlation between assets. In Figure 10.1, the combinations considered are +$20M UST and ±€7M nominal of CAC futures. Alternatively we could take the +$20M UST as fixed and consider the CAC as a hedge, varying the amount of the futures. We could ask by how much the UST volatility can be reduced through hedging: What is the potential for risk reduction? Precisely, we could calculate the percentage reduction in volatility that we could achieve by optimally hedging the U.S. bond with the CAC futures.
Panel B in Figure 10.1 shows + $20M in UST and - €7M nominal of CAC, with an angle of θ = 76° between them (cos 76° = 0.24 = -ρ). Hedging the UST with the CAC means keeping the amount of UST fixed (the base line B), while varying the amount of the CAC (length of line C), with the angle between them determined by the correlation (θ = arccos(-ρ)). If we wish to minimize the resulting combined volatility (the line A), then it should be clear that A must make a right angle with C, as shown in Figure 10.2. But in that case, we have a right triangle with hypotenuse B, and A = B sin θ. The reduction in volatility is B - A and the proportional reduction, or the risk reduction potential, is (B - A)/B:
(10.2a) 

Figure 10.2 Triangle Addition and Risk Reduction Potential
Note: This shows side C (the amount of the CAC futures in this case) chosen to provide maximum risk reduction or optimal hedge for side B (U.S. bond in this case). Reproduced from Figure 5.13 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

An alternative but longer way of deriving the same result is to use equation (10.1a). Take the amount of the UST as fixed at $20M and let the amount of the CAC be α (in units of €7M). The correlation between them is ρ (0.24 in this case). Then the volatility of the hedged position, σh, will be given by

This will be minimized when α = -ρ σ1/σ2. The hedged volatility as a proportion of the original (UST) volatility will be σh/σ1 = . This means that when the correlation between two assets is ρ, the maximum proportional reduction in volatility will be:
(10.2b) 
For the UST and CAC, where correlation is 0.24, using either (10.2a) or (10.2b), the risk reduction potential is only 3 percent. This is very low, and means that using CAC futures as a hedge for the U.S. bond would be almost completely ineffective. Table 10.2 shows the risk reduction potential for various levels of correlation.
As Litterman (1996, 62) points out, "Many traders and portfolio managers may not be aware of how sensitive risk reduction is to the degree of correlation between the returns of the positions being hedged and the hedging instruments." Litterman (1996, Exhibit 17) also has a useful diagram showing the potential risk reduction as a function of the correlation and angle.
The example here was using two individual assets, but the result carries over when we consider a composite portfolio hedged by one single asset.
10.2 Contribution to Risk
Volatilities and variances do not add and equation (10.1) does not, on the surface, provide a decomposition of portfolio volatility into contributions due to individual assets or groups of assets. Nonetheless, there are two useful ways we can define the contribution a position makes to the volatility or VaR:

1. Infinitesimal: change in volatility or VaR due to an infinitesimal change in a position.
2. All-or-nothing: change in volatility or VaR due to complete removal of a position.

In my view, the infinitesimal, or marginal contribution to risk, and the decomposition it provides, is one of the most powerful but underappreciated tools for risk analysis. Such a contribution to risk provides a useful decomposition of the current risk profile by showing how the current positions affect the current portfolio, aiding in the understanding of the portfolio. Positions in a portfolio are usually adjusted little by little rather than by complete removal of a position, and the marginal contribution provides a good estimate of this for a large portfolio with many small positions. I find the infinitesimal, rather than the all-or-nothing measure, to be far the more useful. Although the change due to complete removal of an asset (setting to zero position) is valuable information, I think the best hedges analysis, discussed further on, is generally more useful.
Unfortunately, there is no agreement in the literature, and considerable confusion, regarding nomenclature, and this provides a barrier to better understanding of contribution to risk. Particularly confusing is that RiskMetrics uses the word marginal for the all-or-nothing measure (even though the word marginal is commonly used to denote small changes at the margin and not large, finite changes) and uses the word incremental for the infinitesimal measure (again somewhat contrary to common usage of the word incremental). Most of the literature uses the reverse terminology. Nor are texts always clear in their explication of the concept.
Table 10.3 is a quick guide to the various terms used by different writers.3
Table 10.3 Terms Used in the Literature for the Infinitesimal and All-or-Nothing Decompositions of Risk.




Infinitesimal
All-or-Nothing




This monograph
marginal contribution or contribution to risk
all-or-nothing contribution to risk


Litterman (1996)
contribution to risk



Crouhy, Galai, and Mark (2001)
delta VaR
incremental VaR


Marrison (2002)
VaR contribution



Mina and Xiao/RiskMetrics (2001)
incremental VaR
marginal VaR


Jorion (2007)3
marginal VaR and component VaR
incremental VaR


Reproduced from Exhibit 5.2 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.



Marginal Contribution to Risk
The idea of marginal contribution to risk was introduced independently by Robert Litterman in Hot Spots and Hedges (Litterman 1996) and M. B. Garman (Risk magazine 1996). We start by considering the marginal contribution to volatility. It will be shown, however, that the concept of contribution to risk is also applicable to most commonly used risk measures (e.g., VaR, expected shortfall, but not probability of shortfall).
To start, consider equation (10.1a) for the variance of two assets, but now include explicitly the weights of the asset holdings so that σi is the volatility for a unit holding of the position, and ωi is the amount of the holding (measured in dollars, number of bonds, percent of the portfolio, whatever the appropriate unit is). This can be written as:

and the volatility as:

This suggests a simple ad hoc decomposition of the volatility into constituent parts;4 the term

being defined as that portion attributable to asset 1, while a similar term gives the contribution for asset 2, so that:
(10.3a) 
That is, the volatility can be decomposed into additive contributions from the two assets. So far, this is just an ad hoc decomposition of the volatility. The wonderful thing is that we arrive at exactly the same additive decomposition if we consider an infinitesimal or marginal change in the volatility resulting from infinitesimal changes in the asset holdings or weights. First, we rewrite the expression for volatility (using the column vector ω′ = [ω1,..., ωn] and the variance-covariance matrix Σ) as:
(10.3b) 
It will be convenient to write this in a way that keeps track of the individual components ωi. I will use the notation [ω′Σ]i to denote the ith element of the row vector ω′Σ and [Σω]i to denote the ith element of the column vector Σω. That is,




Note that [ω′Σ]i and [Σω]i are the covariance of asset i with the portfolio.
Using this notation,

so that σp can be written as:
(10.3c) 
This gives an additive decomposition, with the terms [ω′Σ]iωi/σp summing to σp; essentially the same as equation (10.3a). We can divide by an additional σp to get an additive decomposition in proportional terms:
(10.3d) 
where now the terms [ω′Σ]iωι/ sum to 1.00.
Alternatively, we can start with the volatility σp and take a total differential:

If we consider infinitesimal percent changes in the ωi, then we divide through by an additional factor of ω and we arrive at:
(10.3e) 
This gives exactly the same additive decomposition as (10.3d), with the terms [ω′]iωi/σp summing to σp. Also, we can take an infinitesimal percent change in volatility:
(10.3f) 
which gives the proportional decomposition of (10.3d) with the terms [ω′Σ]iωι/ summing to 1.00. In other words, equations (10.3c) and (10.3d) provide a useful additive decomposition of the total volatility, which is the same as the decomposition of an infinitesimal change in the volatility.5 We can call the terms in the decomposition the marginal contribution (levels) and marginal contribution (proportional):
(10.4a) 
(10.4b) 
These terms give the contribution to the infinitesimal change in volatility (levels or proportional) due to a small percent change in position (equations (10.3e) and (10.3f)), and also provide an additive decomposition of the volatility (equation (10.3d)).
The derivation for the decomposition of volatility was based on the algebraic definition of the volatility (and variance) and made no assumptions about the functional form of the P&L distribution. It should therefore be clear that the decomposition will hold for any P&L distribution—normal or non-normal.6
Deeper insight into the decomposition can be gained by noting that the decomposition is a consequence of the linear homogeneity of the volatility. Linear homogeneity means that scaling all positions by some scalar factor λ scales the risk by the same factor. If σp = Vol(ω) is the volatility of a portfolio with weights ω, then scaling all positions by a constant λ means:

Euler's law (Varian 1978, 269) states that any linearly homogenous (differentiable) function R(ω) satisfies:

The additive decomposition of volatility (10.3c) follows from this directly. Similarly, the marginal decompositions (10.3e) and (10.3f) follow directly:


The terms ∂R(ω)/∂ωi ωi will sum to R(ω), and the terms ∂R(ω)/∂ωi ωi/R will sum to 1.00.
More importantly, Euler's law means we can apply a similar additive and marginal decomposition to any risk measure R(ω) that is linearly homogeneous. In fact, most risk measures used in practice (including volatility, VaR, and expected shortfall, but not probability of shortfall) are linearly homogeneous, so that a marginal decomposition can be applied to each of these.7
This also means that the concept of marginal contribution to risk does not depend on the particular estimation method used to calculate volatility or VaR. Contribution to risk can be calculated using the parametric (delta-normal) approach, the Monte Carlo approach, or the historical simulation approach.
McNeil, Frey, and Embrechts (2002, equations 6.23, 6.24, 6.26) give formulae for contributions for volatility, VaR, and expected shortfall. Say that the portfolio is made up of investment in n assets, the P&L for one unit of asset i being denoted by Xi, and the amount invested in asset i is ωi. Then the total P&L is ∑iωiXi, the Z% VaR is VaRz = {Y s.t. P[∑iωiXi ≤ Y] = Z} and the expected shortfall is ESz = E[∑iωiXi | ∑iωiXi ≤ VaRz]. The contributions are:

volatility:  MCLi = ωi cov(ωiXi,∑iωiXi)/√variance(∑iωiXi)
VaR:  MCLi = ωi E[Xi|∑iωiXi = VaRz]
ES:  MCLi = ωi E[Xi|∑iωiXi ≤ VaRz]8,9

See the appendix for explicit formulae for simulations.
It is also important to note that the marginal contribution can be calculated for groups of assets and for subportfolios, and explicit formulae for marginal contribution to volatility are given in the appendix.
For an example of using the contribution to volatility, consider the holdings of the U.S. Treasury and the CAC futures discussed earlier, and consider a small (infinitesimal) percent change in each holding. Table 10.4 shows the result, using equation (10.4), and shows that the marginal contribution to the portfolio volatility is much higher for the equity futures than for the bond.
The situation changes when the CAC futures position is short €7M, with now the CAC providing an even larger proportional contribution to the portfolio volatility (although the overall portfolio volatility is now lower; see Table 10.5).
Table 10.4 Volatility for Simple Portfolio—With Contribution to Risk.

We could also ask: What is the CAC position for which the futures make no contribution to the volatility? Zero is an obvious but not very valuable answer. Generally there will be some nonzero CAC position such that the contribution to the overall portfolio volatility will be zero. In the present case, having a small short futures position will provide zero equity futures contribution. Specifically, for a holding of -€950k, small changes in the holdings of the CAC futures will have almost no impact on the portfolio volatility.
Table 10.5 Volatility for Simple Portfolio—Contribution for Short CAC Futures.

The triangle addition of volatilities helps to illustrate what is happening, and the situation is actually the same as shown in Figure 10.2. The CAC position is chosen so that the resultant portfolio volatility (side A) forms a right angle with side C (CAC volatility). Triangle addition also helps show how and why such a position has zero contribution. Figure 10.3's top panel shows a change in side C (CAC volatility—for clarity, a large change rather than infinitesimal). In this case, leg A (portfolio volatility) changes in length by almost nothing. The bottom panel shows a change in side B (U.S. Treasury volatility), and here the length of side A changes virtually one for one with side B.

Figure 10.3 Triangle Addition of Volatilities for +$20M UST, -€950k CAC futures
Reproduced from Figure 5.14 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

The decomposition or marginal contribution to volatility is useful for the insight provided into how the volatility will change for small changes in a single position, all other positions held fixed. It is particularly useful for large and complex portfolios, exactly the situation in which both intuition and aids such as the triangle diagrams (applicable only for two assets) break down.
Correlation with Portfolio
It is also possible to calculate, from the marginal contribution, the correlation of an asset with the portfolio:

This can also be extended to groups of assets or subportfolios using the partitioning discussed in the appendix.
All-or-Nothing Contribution to Volatility or VaR
The marginal contribution gives the change in risk for a small (infinitesimal or marginal) change in position, but we will also find it useful to measure the change if the position is entirely removed. This is the all-or-nothing contribution (called marginal contribution by RiskMetrics 1999, 2001, and incremental VaR by Galai, Crouhy, and Mark 2001 and Jorion 2006).
The formula for the volatility at a zero position is simple (see the appendix for its derivation):
(10.5) 
The all-or-nothing contribution to volatility for asset k is the reduction in volatility moving to a zero position:

10.3 Best Hedge
When considering finite or large changes in an asset holding, it is very useful to consider the change that would optimally hedge the rest of the portfolio. We can call this the best hedge—the position size that reduces the volatility as much as possible, or hedges the rest of the portfolio as effectively as possible.
To work out the best hedge position, consider that the marginal contribution attributable to a particular position may be either positive (adding to the portfolio risk) or negative (lowering the portfolio risk—acting as a hedge). At some point, the marginal contribution will be zero. This will be the position size that optimally hedges the rest of the portfolio.10 We can calculate the position size for asset k for which the marginal contribution is zero, given no changes in any other asset holdings. This means finding  that satisfies

The point of zero marginal contribution is the point at which portfolio risk is minimized with respect to the size of asset k since the marginal contribution is the derivative of the volatility with respect to its position. This will be a best hedge in the sense of being the position in asset k that minimizes the portfolio volatility (all other positions unchanged).
Earlier, for a portfolio with two assets, we used the triangle diagrams in discussing risk reduction and contribution to risk. The best hedge is where the contribution is zero and where the length of the leg is such that the third leg (side A, or the portfolio volatility) is minimized or makes a right angle with the leg under consideration.
For the preceding U.S. Treasury and CAC futures example (Figures 10.2 and 10.3) we varied the size of the CAC position, keeping $20M of the U.S. Treasury. The marginal contribution of the CAC is zero when the CAC position is -€950 k, as seen in Table 10.6.
Table 10.6 Volatility for Simple Portfolio—Zero Contribution for CAC.

Figure 10.3 shows that the resultant volatility (triangle leg A) forms a right angle with the leg representing the CAC volatility. P&L for the U.S. Treasury and the CAC are positively correlated, so that the CAC best hedge position is actually a short position, hedging the $20M long U.S. Treasury position.
The portfolio volatility at the best hedge position (see the appendix for its derivation) is given by:
(10.6) 
Examples for Marginal Contribution, All-or-Nothing Contribution, and Best Hedges
The concepts of marginal contribution to volatility, all-or-nothing contribution, best hedge position, and best hedge volatility are actually very simple, but when first encountered they can be confusing. It can take some time to recognize how they relate and when different measures are useful. So I will now turn to a simple example through which we can examine the different measures.
We will continue with the example of the two-asset portfolio used before: $20 million of the U.S. Treasury and €7 million of the CAC equity futures. Tables 10.7 and 10.8 show the same portfolio as in Table 10.4 together with additional measures. The volatility of the U.S. Treasury (considered on its own) is about half that of the CAC futures even though the notional is about double and the Treasury contributes about one-third of the total volatility. Tables 10.7 and 10.8 show the following additional measures:
Table 10.7 Volatility for Simple Portfolio—With Marginal Contribution and Correlation.

Table 10.8 Volatility for Simple Portfolio—With All-or Nothing Contribution and Best Hedges.


 Marginal contributions to volatility in both proportional and level form
 Correlation with the portfolio
 All-or-nothing contribution to volatility
 Best hedge positions
 Replicating position
 Volatility at the best hedge position

The marginal contribution in proportional and level form show the same thing, only reported in different ways. I personally prefer the proportional contribution. I look to the total volatility first to gain an estimate of the overall portfolio variability. I then turn to the marginal contribution (proportional) to see how different risk factors, asset classes, or subportfolios contribute to that overall volatility. Note, however, that my preference for the proportional contribution is just that—a personal preference. Either measure is equally valid. Both marginal contributions sum to the total (to 1.00 for the proportional, to the overall portfolio volatility for the levels) and this additive decomposition of the overall volatility is the strength of either form of the marginal contribution.
The all-or-nothing contribution for a specific asset or risk factor is how much the actual holding contributes to the volatility. It is the change in the portfolio volatility that occurs when we go from zero holding to the actual holding in that asset or risk factor. For this simple two-asset portfolio, the all-or-nothing contribution is almost trivial and can be easily worked out from the stand-alone volatilities in Table 10.7 or 10.8. The total portfolio volatility is $291,300. Say we did not hold the U.S. Treasury. Only the CAC futures would remain, and so the portfolio volatility would be the CAC futures stand-alone volatility. The U.S. Treasury all-or-nothing contribution is the difference between the portfolio volatility and the CAC stand-alone volatility ($291,300-$230,800). Similarly, the CAC all-or-nothing contribution is the difference between the overall portfolio volatility and the U.S. Treasury stand-alone volatility ($291,300-$130,800).
For anything more than a simple two-asset portfolio, the calculation of the all-or-nothing contribution is not so trivial. Say we added $40 million of a U.S. five-year Treasury. Tables 10.9 and 10.10 show the resulting volatilities and contributions. The five-year Treasury has a stand-alone volatility roughly the same as the 10-year (it is about half the duration but otherwise behaves similarly to the 10-year Treasury). Now the all-or-nothing contribution must be calculated rather than inferred directly from the stand-alone volatilities.
Table 10.9 Volatility for Portfolio with Added 5-Year U.S. Treasury—Marginal Contribution and Correlation.

Table 10.10 Volatility for Portfolio with Added 5-Year U.S. Treasury—All-or-Nothing Contribution and Best Hedges.

One method for calculating the all-or-nothing contribution, which is straightforward but cumbersome, would be to revalue the portfolio multiple times, each time leaving out one position. When working in a variance-covariance framework, however, equation (10.5) provides a simple way to calculate the all-or-nothing contribution. The calculation is particularly simple because the term [Σω] shows up in virtually all the calculations we are discussing—marginal contribution, all-or-nothing contribution, and best hedges. The term [Σω] denotes the vector that results from the dot-product of the market variance-covariance matrix and the vector of positions or deltas. In other words:

This is the covariance of asset or risk factor i with the portfolio, and is the central element of all the calculations. It goes into the calculation of the marginal contribution:

the correlation of asset or risk factor i with the portfolio:

the all-or-nothing contribution (see equation (10.5)) and the best hedge volatility (see equation (10.6)). Table 10.10 shows the results of applying equation (10.5) for the calculation of the all-or-nothing contribution.
In my experience, the all-or-nothing contribution is the least useful of the measures discussed here. The marginal contribution, providing an additive decomposition of the portfolio volatility and how the volatility changes for small changes in holdings, provides useful information about small adjustments to the portfolio. The best hedges, to which we now turn, provide insight into the composition of the portfolio and useful information on how the volatility changes for large adjustments in holdings.
Turn back to Table 10.8, the portfolio with only the 10-year Treasury and the CAC futures. The "Best Hedge Position" is the holding that provides the best hedge to the rest of the portfolio. For these two securities, we discussed earlier the CAC holding that provided the best hedge to the U.S. Treasury. Table 10.8 shows this position—short €950k of the CAC futures. Holding this amount of the CAC futures provides the best hedge to the rest of the portfolio (the rest of the portfolio in this case is just $20 million of the 10-year Treasury).
In Section 10.2, we focused on the risk reduction potential of one asset versus another individual asset, and the relevant correlation was that between the CAC and the 10-year Treasury. Looking at individual assets is fine when we have only two but does not work well when we have a multi-asset portfolio, as in Tables 10.9 and 10.10. We need to change our focus. We need to consider the CAC futures as one asset and the portfolio as a second asset. We can measure the correlation between the CAC futures and the whole portfolio (which includes some amount of the CAC futures). Table 10.8 shows that this correlation is 0.90, and referring back to Table 10.2, we can see that the risk reduction potential (using the CAC futures against the whole portfolio) is 56 percent—we can reduce the portfolio volatility 56 percent by optimally choosing the CAC futures position. Looking at Table 10.8, we see that the CAC best hedge reduces the portfolio volatility by this: (291,300 - 126,900)/291,300 = 56 percent.11
The idea of best hedges and correlation with the portfolio carries over to large portfolios. Tables 10.9 and 10.10 show three assets and the best hedges for each of the 5-year Treasury, 10-year Treasury, and CAC futures. Now the CAC best hedge is short €2.01M but the portfolio volatility is reduced by only 36.2 percent. The portfolio has much more bonds and behaves more like a bond portfolio than the earlier portfolio. We can see, in fact, that both the 5-year and the 10-year Treasury provide slightly better hedges to the portfolio than the CAC futures.
One final point regarding the concepts of marginal contribution, best hedges, and so on. We must always exercise caution and judgment in using such measures. They cannot provide definitive answers, only helpful guidance. They show us how our portfolio might have behaved in the past, but the future is always uncertain. Tomorrow may bring not only random fluctuations in the markets, but more fundamental changes in market relationships.
For the portfolio described in Table 10.10, the five-year Treasury is the best possible hedge assuming that markets behave in the future generally as they have in the past. But there are two large cautions, for two fundamentally different reasons. First, the correlation between the five-year Treasury and the portfolio is only 0.80, which means that on a particular day, there is still a good chance the two will not move together. In other words, even if markets behave as they have in the past, on a particular day, there is still a good chance the hedge will not work perfectly. As a user of such tools, it is incumbent on us to understand that best hedge does not mean perfect hedge and in particular circumstances may not even be good hedge. We have to understand how well such a hedge works, for example, by looking at the correlation or the potential risk reduction.
The second caution in using such tools is that the future may bring fundamental changes in market relations. The 5-year and 10-year Treasuries generally move very much together, and this co-movement is what leads to the five-year Treasury being the best hedge. But were this relation to break down, then the five-year Treasury might no longer be the best hedge. We have to understand that the results of an exercise such as in Table 10.10 provide guidance but not definitive answers.
10.4 Replicating Portfolio
Representing a complex portfolio in terms of a simpler portfolio is both useful and relatively straightforward. For a single asset k, the best hedge position  minimizes the portfolio variance when changing asset k only. This implies that the difference between the original and best hedge position is a mirror portfolio,

in the sense that the variance of the difference between the original and mirror portfolio is minimized:

It is simple to calculate the best single-asset mirror portfolio across all assets in the original portfolio: for each asset, calculate the volatility at the best hedge position and choose from among all assets that single asset with the lowest best-hedge volatility. It is natural to call this best mirror portfolio a replicating portfolio because it best replicates the portfolio (best using single assets chosen from the original portfolio).
The idea becomes clearer when we focus on an example. Table 10.10 shows the best hedges, both the holdings and the volatilities at those holdings, for the simple portfolio consisting of long $40 million of the 5-year Treasury, $20 million 10-year Treasury, and €7 million CAC futures. The best hedge using the 10-year Treasury would be to replace the long $20 million holding with a short of about $26 million. In other words, the best hedge would require shorting about $46 million of the 10-year Treasury versus the existing portfolio. A new portfolio made up of the original portfolio combined with short $46 million of the 10-year Treasury would have the volatility shown under "Volatility at Best Hedge."
When we examine the volatility at the best hedge holdings, we see that the five-year Treasury (a change of $94 million versus original portfolio) actually gives the lowest volatility among all choices. This implies that a holding of long $94 million in the 10-year Treasury is a mirror portfolio that most closely replicates the existing portfolio, out of all possible single-asset portfolios using assets in the existing portfolio. In this sense, such a holding in the five-year Treasury is the best single-asset replicating portfolio. We can think of this in two ways. First, the portfolio behaves most like a five-year U.S. Treasury, and such a position explains 40 percent of the volatility ((385,700 - 230,400)/385,700 = 40 percent). This gives a simple, concise view of the portfolio. Second, shorting $94 million of the U.S. Treasury will provide a simple hedge, although in this case, it will not be very effective since the hedge only reduces the volatility by 40 percent.
One really important point we need to take from Table 10.10 is the difference between what we can learn from the marginal contribution versus the best hedges. In Table 10.9, we see that the CAC futures has by far the largest marginal contribution. The CAC is the asset that contributes the most to the portfolio volatility. But it will not provide the best hedge; the five-year Treasury actually provides the best hedge. The 10-year and 5-year Treasuries are similar and the 5-year Treasury will hedge not only the 5-year Treasury in the portfolio but also the 10-year Treasury.
Multi-Asset Replicating Portfolio
Such a single-asset replicating portfolio gives a simple representation of how the full portfolio behaves, but it will usually be too simple to be useful on its own. Fortunately, the replicating-portfolio idea extends in a straightforward manner to multiple assets, to provide a replicating portfolio that is still simple but more informative.
For two particular assets, j and k, the best hedge positions  and  are given by:
(10.7) 
This extends to more than two assets in the obvious manner. (See the appendix for its derivation.)
The volatility of the best-hedge portfolio with the positions for assets j and k set equal to the best hedge positions is:
(10.8) 
Again, this extends to more than two assets in the obvious manner. (See the appendix for its derivation.)
The two-asset replicating portfolio is found by first defining the two-asset mirror portfolio for assets j and k as

The replicating portfolio using two assets is that two-asset mirror portfolio with the lowest variance. Relatively small replicating portfolios, using 3, 5, or 10 assets, can provide useful information and insight into the full portfolio. The replicating portfolio can serve as a proxy, summary, or approximation of the full portfolio, with the percent variance explained by the replicating portfolio providing a measure of the quality of the approximation.
One straightforward way to calculate the replicating portfolio using n assets is by brute force: Calculate the volatility (or variance) reduction resulting from all possible combinations of mirror portfolios using assets taken n-at-a-time and then choose the best. The problem is simplified because the best hedges variance and variance reduction can be calculated quickly using the preceding formulae.
Such an approach is feasible when the number of assets to be searched over, m, is relatively small (say, 40 or less) but becomes problematic when the number of assets in both the replicating portfolio and the original portfolio get large. For example, searching for the three-asset replicating portfolio when the original portfolio contains 20 assets involves only 1,140 cases. As Table 10.11 shows, however, the best 10-asset replicating portfolio for an original 50-asset portfolio requires searching more than 10.3 billion cases. In this case, some ad hoc strategy can be employed, such as searching over only the top-40 assets measured by single-asset best hedge variance reduction, or removing similar assets (for example, removing 10-year and leaving 30-year U.S. Treasury bonds) from the assets to be searched over.
Table 10.11 Number of Portfolios Searched When Choosing Best n of m Assets.

Alternatively, for a large portfolio, a strategy analogous to stepwise regression—building up the replicating portfolio assets one at a time—can be employed. The simplest procedure is to add additional assets one at a time, without any checking of earlier assets:

 Choose the first replicating portfolio asset as the volatility-minimizing single best hedge.

 That is, calculate  for all k. This is the best-hedge volatility for all one-asset best hedges or mirror portfolios.
 Choose as the first replicating portfolio asset, 1*, the asset k, which produces the smallest .

 Choose the second replicating portfolio asset as that asset which, combined with the first, produces the largest incremental reduction in portfolio variance.

 That is, calculate  for all k = {all assets excluding the first replicating portfolio asset}. This is the best-hedge volatility for all two-asset best hedges that include the first replicating portfolio asset.
 Choose as the second replicating portfolio asset, 2*, the k for which  is the smallest (or the variance reduction  −  is the largest).

 Choose as the third replicating portfolio asset that asset that, combined with the first two, produces the largest reduction in portfolio variance.

 That is, calculate  for all k = {all assets excluding the first and second replicating portfolio assets}. This is the best-hedge volatility for all three-asset best hedges that include the first two replicating portfolio assets.
 Choose as the third replicating portfolio asset, 3*, the k for which  is the smallest (or the variance reduction  −  is the largest).

 Continue adding single replicating portfolio assets until the number of desired replicating portfolio assets is obtained.

A more complex procedure, looking back at earlier replicating portfolio assets at each step to ensure they should still be included, is outlined in the appendix.
The discussion so far has focused on choosing a replicating portfolio from the assets within a portfolio. Alternatively, an externally specified set of assets can be used. The replicating portfolio weights can be chosen by using linear regression analysis.12
10.5 Principal Components and Risk Aggregation
Principal components is a data-reduction technique that can reduce the effective data dimensionality and provide a summary view of risk intermediate between the very granular level of individual trading desks and the very aggregate level of portfolio volatility or VaR. It takes the original returns for assets or risk factors, represented by a vector Y = [y1,..., yn]′ (for example the yields for 1-year, 2-year,..., 30-year bonds), and transforms it into a new set of variables, F, by means of a linear transformation:
(10.9) 

The trick is that we can choose A so that the new variables fi are orthogonal (statistically uncorrelated). The orthogonality is particularly nice because it means that different factors in a sense span different and independent dimensions of the risk. The benefit is that the separate fi will contribute independently to the portfolio variance or VaR (assuming that the original Y are normally distributed or close to normal). Furthermore, the fi can be ordered in terms of size or contribution to the variance. In many practical cases, the first few fs contribute the lion's share of the variance and also have an easily understood meaning (for example, yield curves, with level, twist, hump). In this case, the new variables can be particularly useful for reducing dimensionality and aggregating risk from a large number of variables (for example, 20 to 30 yield curve points) to a small number of orthogonal factors (for example, level, twist, hump, residual).
The new principal components can be used as an aid in summarizing risk, in aggregating risk from disparate sources into a set of consistent factors (for example, trading desks that use different yield curve points), and to decompose realized P&L into components due to independent factor changes.
Principal Components—Concepts
Principal component analysis is detailed in the appendix. The main point is that the new variables F, defined by 10.9 are uncorrelated (and, assuming the Y are multivariate normal, independent). A simple example will help fix ideas. Consider 2-year and 10-year rates. Assume that the rates have the following volatilities:

and that the correlation between 2-year and 10-year rates is 80 percent. Then (see the appendix for details) the matrix A will be:

The first column gives the transformation to f1, and the second the transformation to f2:


Given the history on y1 and y2, these new variables are statistically uncorrelated (and independent, assuming Y is multivariate normal).
We can also transform from F to Y according to:

and we can ask what the changes are in Y corresponding to 1-σ moves in the new factors. (See the appendix for the exact formula). The change in Y due to a 1-σ change in the fi are called the factor loadings, FL. Remember that f1 and f2 are independent, so we can think of such moves as occurring, on average, independently. In this example, the change in y2 and y10 for a 1-σ move in f1 and f2 are (see appendix):

This is roughly a parallel shift (up 6.11 and 4.28 basis points for 2-year and 10-year) and a curve twist (down 1.36 for the 2-year and up 1.94 basis points for the 10-year). That is, for the history of 2-year and 10-year yield movements summarized in the preceding covariance matrix, such a parallel shift and curve twist have occurred independently.13
We can go further, and ask what is the risk with respect to the new factors? Say that the sensitivity, measured as the P&L for a one basis point move in 2-year and 10-year yields, are -1 and 2:

Using the moves in 2-year and 10-year yields resulting from 1-σ moves in f1 and f2, we can calculate the P&L for 1-σ moves in the new factors:

These are independent, so that the overall portfolio variance is just the sum of the component variances:

Not only is the variance simple to compute, but we can treat the portfolio as being exposed to the parallel and twist risks operating independently. Even more importantly, we can immediately see that the twist risk (component f2) is by far the more important risk.
Principal Components for Risk Aggregation
Principal components can be used to aggregate risk, either to summarize disaggregated risk or to aggregate across disparate trading operations.
Although portfolio volatility and VaR provide a valuable high-level summary of the magnitude of risk (that is the power of VaR and quantitative-based risk measurement) they do not provide a view into the sources and direction of the risk. At the other end of the spectrum, individual trading units calculate and report risk at a very granular or disaggregated level, as required for the micromanagement of the trading risk, but such a micro view is too detailed for intermediate management and aggregation. An intermediate level, between the micro view used by the individual trader and the top-level view provided by VaR, is valuable, and it can often be aided by principal components analysis.
More specifically, the P&L due to 1-σ moves can provide an intermediate-level view. Principal components combine a history of market risk factors with the portfolio's sensitivity to produce a description of probable P&L scenarios.14 This is particularly effective when the principal components have an easily understood interpretation, as usually occurs with developed-economy fixed-income markets (where the first three principal components can usually be interpreted as shifts in yield curve level, slope, and curvature).
The simple preceding example, using just two yield curve points, shows the first two components as being roughly shifts in yield curve level and slope. With the yield curve made up of only two elements (2- and 10-year yields) the principal components provide little additional insight. In practice, a realistic yield curve will often be made up of 20 or more elements (cf. Coleman 1998a for a description of building a forward curve from market data). The sensitivities with respect to detailed yield curve points provide a granular view of the risk, but it can often be difficult to summarize, and reducing them to three or four important components can indeed be useful.
Consider Table 10.12, which shows, for two hypothetical trading desks, sensitivities with respect to yield curve points. Such a sensitivity report is necessary for managing risk at a granular level, where a trader must hedge small market moves and where the trader is intimately familiar with market behavior. Such a report, however, is not useful at a level one or more steps removed from minute-by-minute management of the risk. It does not adequately summarize the risk, both because it has too much detail and, more importantly, because it does not provide any indication of the size and type of yield curve moves that are likely to occur. For a division manager not watching the markets hour by hour, having a summary that incorporates market history can be invaluable. It becomes absolutely necessary when considering risk across multiple markets.
Table 10.12 Sample Sensitivity for Two Trading Desks.
Note: This is the sensitivity to a 1bp fall in the appropriate yield; that is, a positive number means long the bond, making money when bond prices rise.

Turn now to Table 10.13, which shows the factor loadings, or the response to a 1-σ move in the first three principal components. Figure 10.4 shows these graphically. We can see that the first component is a shift in the level of the yield curve (a larger shift at the short end of the curve, and a shift down in yields giving a shift up in price). The second is a twist centered at about seven years maturity. The third is a hump, again centered at about seven years maturity.
Table 10.13 Factor Loadings (Sensitivity to 1 − σ move in principal components)
Note: The "level" factor is written as a fall in yields, which denotes a rally in bond prices.

Figure 10.4 First Three Components—Rally, Flattening, Hump

The sensitivity to principal components can be calculated from the yield curve sensitivity shown in Table 10.12 by transforming with the factor loadings FL:15

Table 10.14 shows the transformed sensitivity to principal component factors. Desk one is primarily exposed to a fall in the level of the curve (rally in bond prices) making $10,700 for every 1-σ shift down in level of the yield curve (1-σ rally in bond prices). Desk two is primarily exposed to a flattening of the curve, losing money when the curve flattens (desk two is short the long end of the curve in price terms, so loses money as the long yield falls and long prices rally relative to the short end). The two desks combined have roughly equal sensitivity to the first two components, with very little to the third or higher components.
Table 10.14 Sensitivity to Principal Component Factors.

When the first few factors account for a large proportion of the portfolio variance, the factor risk provides a concise summary of the likely P&L; that is, a concise summary of the probable risk and a useful aggregation of the risk to a small number of factors.
These ideas can be applied to individual trading units or subportfolios even when they do not measure sensitivity at the same granular level. Say that the second trading desk actually measured their sensitivity and managed their risk based on the yield curve points shown in Table 10.15. This makes direct comparison and aggregation of the risk across the units more difficult.
Table 10.15 Sample Sensitivity for Two Desks with Different Yield Curve Points.
Note: This is the sensitivity to a 1bp fall in the appropriate yield; that is, a positive number means long the bond, making money when bond prices rise.

Using the factors specific to desk one (shown in Table 10.13) and desk two (in Table 10.15), one can transform the sensitivities for the two desks separately, but to the common yield curve components. Table 10.16 shows the sensitivity with respect to the level, flattening, and hump components.
Table 10.16 Sensitivity to Principal Component Factors.

Principal Components and P&L Decomposition
The factor loadings and exposures can also be used after the fact to provide a straightforward P&L attribution in terms of observed market movements. Say the observed changes in the yield curve for a particular day are written as the column vector (Δy.25,..., Δy30)′. These observed changes can be decomposed into factor moves, either using the factor loading matrix FL or by regressing the observed changes against a subset of the factor loadings:16

The {Δf1, Δf2, Δf3} give the particular day's market movement in terms of the first three factors: if Δf1 is 1.5, this means that the observed market move is composed of 1.5 factor one moves plus amounts for the other factors and a residual. The P&L due to these factors should be:

where di is the sensitivity with respect to factor i.
Table 10.17 shows an example. Columns 1 and 2 show the change in yields for a particular day—the vector:

Table 10.17 Changes in Yields, Changes in Factors, and P&L Due to Factor Changes.

Regressing this against the first three columns of the factor-loading matrix in Table 10.13 gives the coefficients shown in the fourth column. In this case, the yield changes shown in the second column correspond to +0.99 of factor 1, -2.00 of factor 2, and +1.11 of factor 3. Multiplying the estimated factor changes by the risk for desk one shown in Table 10.14 then gives the P&L due to factors shown in the final column. In this case, the residual is very small; we can explain most of the total P&L due to changes in the first three factors. For another day, we may not explain such a large portion of the total P&L.
User-Chosen Factors
Much of the analysis using principal components (for example, calculation of risk with respect to factors, P&L attribution to factors) can be applied with user-chosen factors. For example, one might prefer that the first yield-curve factor be a parallel shift with rates all moving by one basis point.
Consider an arbitrary (but full-rank) transformation of the yield curve points:

The total portfolio variance is:

The analysis of stand-alone volatility, contribution to risk, and so on, can all be performed using ΔG and ΣG. One challenge is to determine the matrix B when, in general, only the first few columns will be chosen by the user, with the remaining columns representing a residual. This can be accomplished by Gram-Schmidt orthogonalization to produce the additional columns. I think that, because the residual factors are constructed to be orthogonal to the user-chosen factors, the variances of the two groups will be block-independent so that it will make sense to talk about the proportion of the variance explained by the user-chosen versus residual factors.
10.6 Risk Reporting
Effective, intelligent, and useful risk reporting is as important as the underlying analysis. Human intuition is not well adapted to recognize and manage randomness. Risks combine within a portfolio in a nonlinear and often highly nonintuitive manner. Even for the simplest case of normal distributions, the volatility (standard deviation) and VaR do not add so that the volatility or VaR of a portfolio is less than the sum of the constituents—this is diversification. Various tools, techniques, and tricks need to be used to elucidate the risk for even relatively standard portfolios.
To illustrate and explain the techniques for analyzing portfolio risk, I focus on a small portfolio with diverse positions and risks, and on a sample risk report that includes the marginal contribution, best hedges, and so on. The intention is not only to explain what the measures are and how to calculate them, but also to provide insight into how to use them and why they are valuable.
Risk Reporting—Bottom Up versus Top Down
The risk reports discussed here and the analytics behind them are based on a detailed view of the portfolio, aggregated up to a top level for summary purposes. This is a bottom-up process. Alternatively one could view risk reporting as a top-down process, the idea being that senior managers need a big-picture overview of firm-wide risk, and do not need to be concerned with details of individual desks or units. A top-down approach is often driven by the number and complexity of assets and positions held by a large firm; a top-down approach allows many shortcuts and approximations that might be important at the micro level but do not matter much at the firm level.
Nonetheless, a bottom-up approach has important benefits to recommend it. First, even senior managers need to be concerned about risk at a relatively granular level. Not every day, not for every part of the portfolio, but there are times and places when a manager needs to be able to drill down and examine the risk at a more detailed level. A risk-reporting process should be like an onion or a matryoshka doll (babushka doll)—multiple layers that can be peeled back to display risk at a more disaggregated level. Drilling down is a natural part of a bottom-up approach but often difficult to do in a top-down approach.
A second benefit of a bottom-up approach is that reporting built by aggregating lower-level risk can be more easily compared and reconciled against reporting used by those lower-level units. Reconciliation of risk numbers from disparate sources and using alternative methodologies can consume considerable resources at a large firm. Such reconciliation is important, however, because discrepancies can vitiate the usefulness of summary risk reports—lower-level managers distrust the summary reports because they do not match the risk they know from their daily management of the risk, and top-level managers cannot access a reliable view of the lower-level risk when necessary.
Sample Portfolio
I will consider a portfolio made up of four subportfolios (individual portfolio managers or trading desks):

 Government subportfolio

 Long $20 million U.S. Treasury 10-year bond
 Long £25 million U.K. Gilt 10-year
 Short $20 million-notional call option on a 5-year U.S. Treasury

 Swaps subportfolio:

 Short $20 million 10-year swap plus
 Long outright $30 million U.S. Treasury exposure
 Net result is long swap spreads and long some residual U.S. Treasury exposure

 Credit subportfolio:

 Long £55 million corporate bond spread (credit default swap or CDS on France Telecom)

 Equity subportfolio

 Long €7 million CAC futures
 Long €5 million French company (France Telecom)


This is not a large portfolio in number of positions, only seven or eight, but it is diverse and complex in terms of products and risk exposure. This is an example of quantitative risk measurement techniques starting to bring some transparency to an otherwise complex and opaque situation.
The risks in this portfolio include:

 Yield risk

 U.S. Treasury curve
 U.K. Gilt curve
 Swap curve or swap spread risk

 Volatility risk for call option
 Credit risk

 Traded credit spread for the CDS and issuer risk for the equity
 Counterparty risk for the interest rate swap and CDS

 Equity risk

 Both index risk (exposure to the CAC, a broad market index) and company-specific risk (France Telecom)

 FX risk
 Operational risk

 Processing for futures (remember Barings)
 Processing and recordkeeping for IRS, CDS, and option
 Delivery risk for bond and equities

 Model risk

 IRS, CDS, call option


Here we will focus on market risk (yield, volatility, traded credit spread, equity, FX). The primary focus will be on the sample risk report shown in Table 10.18. The report is intended to detail not just the levels but also the sources of the portfolio's risk exposure. In this case, there are only seven positions, and it might be possible to manage such a small portfolio without the risk reporting technology laid out here, but even here comparing and contrasting exposures across disparate asset classes and currencies is not trivial.
Table 10.18 Sample Portfolio Risk Report—Summary Report.

Summary Risk Report
Tables 10.18 and 10.19 show sample risk reports for this portfolio. This is based on delta-normal or parametric estimation of the volatility and VaR. The report is the top-level report for the portfolio and summarizes the overall exposure and major sources of risk. A good risk-reporting program, however, is a little like an onion or a set of Russian dolls—each layer when peeled off exhibits the next layer and shows more detail. This is the top layer; I discuss more detailed reports in a following section, which parallel Tables 10.18 and 10.19 but zero in on a specific subportfolio.
Table 10.19 Sample Portfolio Risk Report—Top Contributors and Replicating Portfolios Report.

One note before turning to the analysis of the portfolio: most of my discussion is qualified with "probably," "roughly," "we can have reasonable confidence," and other similar terms. This is quite intentional. The measures and statistics in any reports such as these are based on estimates and past history. They are good and reasonable estimates, but anybody who has spent time in markets knows that uncertainty abounds and one should always treat such reports, measures, and statistics carefully. They provide a view into what happened in the past and what might happen in the future, but the markets always provide new and unexpected ways to make and lose money.
Volatility
The first thing to note is the overall volatility: The daily or expected volatility is around $616,900. We mean by this that the standard deviation of the daily P&L distribution is roughly $616,900. When considering the daily volatility, we are examining everyday trading activity and not tail events, and so we can have some confidence that assuming normality is probably reasonable. Using this, we can infer that the daily losses or profits should be more than ±$616,900 about one day out of three, based on a normally distributed variable being below -1σ or above +1σ with roughly 30 percent probability.
The observation on likely P&L immediately provides a scale for the portfolio. For example, if this were a real-money portfolio with capital of $10 million, we would expect gains or losses roughly 6.2 percent or more of capital every three days—a hugely volatile and risky undertaking. On the other hand, if the capital were $500 million, we would expect a mere 0.1 percent or more every three days, or roughly 2 percent per year (multiplying by  to annualize)—an unreasonably low-risk venture with probably correspondingly low returns.
The daily volatility gives a scale for the portfolio at a point in time, but even more importantly provides a reasonably consistent comparison across time. Were the daily volatility to rise to $1.2 million next week, we could be pretty confident that the risk of the portfolio, at least the risk under standard day-by-day trading conditions, had roughly doubled.
The volatility also provides a reasonably consistent comparison across asset classes and trading desks. The report shows that the daily volatility for fixed income products (bonds and swaps) is about $346,000 and equity is about $296,000. These statistics are the daily volatility of these products considered in isolation: the P&L distribution of fixed-income products alone has a volatility of about $346,000. The similar scale of risk in these two products is valuable information, because there is no way to know this directly from the raw nominal positions: the notional in fixed income ($20 million in U.S. Treasuries, £25 million in U.K. Gilts, $20 million in swap spreads) is many times that in equities (€7 million in CAC futures, €5 million in France Telecom stock).
Volatility by asset class naturally does not sum to the overall volatility: the sum by asset class of $990,000 versus the overall of $616,900 shows the effect of diversification.
VaR
The next item to note is the daily VaR. The VaR is calculated at a 0.4 percent level. This means the probability of a worse loss should be 0.4 percent or 1 out of 250. The probability level for VaR is always somewhat arbitrary. In this case, 0.4 percent was chosen because it corresponds to roughly one trading day per year (1 out of 255). Such a value should not be considered an unusual event; in Litterman's words (1996, 74): "Think of this not as a 'worst case,' but rather as a regularly occurring event with which [one] should be comfortable."
As with the volatility, the VaR provides a scale, in this case, the minimum loss one should expect from the worst day in a year. It is important to remember that this is the minimum daily loss one should expect from the worst trading day in the year. Purely due to random fluctuations, the actual loss may of course be worse (or possibly better) and there could be more than one day in a year with losses this bad or worse.
Five values for the VaR are shown. The first is derived from the normality assumption and is just 2.652 × the daily volatility—the probability that a normal variable will be 2.652σ times below the mean is 0.4 percent. The second is based on an assumption that the overall P&L distribution is Student t-distribution with six degrees of freedom. This allows for fat tails—the Student t-distribution has the same volatility but fatter tails than the normal. The third is based on an assumption that each asset's P&L distribution is a mixture of normals (99 percent probability volatility = σm, 1 percent probability volatility = 5σm), and again allows for fatter tails relative to normal. The fourth is based on Litterman's rule of thumb that a 4 − σ event occurs roughly once per year, so that the VaR is just four times the volatility. These four alternate values for VaR are useful and adjust for the possibility that the distribution of market risk factors may have fat tails.
These VaR values should be used with care, more care indeed than the volatility. One might want to examine whether assets such as those in this portfolio have exhibited fat tails in the past, and whether and to what extent assets in the portfolio have generated skewed or fat-tailed distributions. The estimates here are based on assumptions of normality for risk factors and linearity for asset sensitivities (the estimates are delta-normal or parametric). The portfolio contains a put option that is nonlinear and will generate a skewed P&L distribution. The delicate nature of estimating and using VaR estimates really argues for a separate report and more detailed analysis.
In the end, I think the common-sense approach said to be used at Goldman (Litterman 1996, 54) has much to recommend it: "Given the non-normality of daily returns that we find in the financial markets, we use as a rule of thumb the assumption that four-standard deviation events in financial markets happen approximately once per year." Under normality, once-per-year events are only 2.65-standard deviations, so a 4-σ rule of thumb is substantially higher, as seen from the report.
Marginal Contribution to Volatility and Correlation
The marginal contribution to volatility is one of the most useful tools for decomposing and understanding volatility and risk. Table 10.18 shows the MCP—proportional (or percentage) marginal contribution—so terms add to 100 percent. The marginal contribution by asset class shows that fixed income and equities are the biggest contributors, each contributing roughly one-third of the risk. Because portfolio effects are paramount but often difficult to intuit, the marginal contribution is a better guide to understanding portfolio risk than is the stand-alone volatility. In this simple portfolio, fixed income and equities have roughly the same stand-alone volatility and roughly the same contribution, but for more complex portfolios, this will often not be the case.
The tables show a breakdown of marginal contribution by asset class and subportfolio. Depending on the institutional structure, different classifications and breakdowns may be more useful. The table by asset class shows the risk for fixed-income instruments independent of where they are held. The swaps desk holds some outright rate risk, as we shall see, so that the volatility and contribution for swap spread and for the swaps desk itself are different. Examining the contribution by subportfolio shows that the government desk contributes most to the overall portfolio volatility. Much of the FX risk is held by the government desk (in the form of a partially hedged U.K. bond), and this leads to the large contribution by the government desk.
Swap spreads actually show a small but negative contribution to the overall volatility. The negative contribution does not mean that there is no risk in swap spreads—on a particular day, swap spreads may move in the same direction as the rest of the portfolio, thus leading to larger gains or losses, but it does give a reasonable expectation that over time the exposure to swap spreads will not add very much to the overall portfolio volatility.
The correlation of the swap rates with the full portfolio helps elucidate why swaps have a negative contribution. The correlation is slightly negative, and so the swaps position hedges (slightly) the overall portfolio, and for small increases, the swaps position hedges the overall portfolio. Turning back to the contribution and correlation by asset class, we see that equities are the most highly correlated with the portfolio, which explains why equities contribute so much to the volatility even though the stand-alone volatility is less than for fixed income.
Depending on the size and complexity of the portfolio, examining contribution to risk by individual assets may be useful. For a large and diverse portfolio, there will generally be many assets, and contributions by individual assets should be left to a more detailed next reporting level, below the top-level summary. For a smaller portfolio, examination of all assets is valuable.
For most any portfolio, however, the top contributors provide useful insight into the portfolio. For this sample portfolio, the top three contributors give a succinct summary of the major risks faced by the portfolio: equity index (CAC) and U.S. and U.K. yields. The top negative contributor shows those assets that reduce risk or hedge the portfolio. For this sample portfolio. there is only one asset five-year U.S. yields—that has a negative contribution.17
Best Single Hedges and Replicating Portfolios
The marginal contributions show the contribution to risk for the existing portfolio and provides a guide to how the volatility will likely change for small changes in holdings. But the marginal contributions are not the best guide to the likely effect of large changes in asset holdings, or what the best hedging assets might be. For this, the best hedges and replicating portfolios are useful.
For any particular asset, the best hedge position is that position which minimizes the expected volatility. This involves a finite, possibly large, change in position. The top best hedge will often differ from the top marginal contributor; for the sample portfolio shown in Table 10.18, the Equity Index (CAC) is the largest marginal contributor but the second-top best hedge.
The top contributors and the top single hedges measure different characteristics of the portfolio. The top contributor to risk is the top contributor, given the current positions. It tells us something about the composition of the current portfolio. The best single hedge, in contrast, is that asset that would give the largest reduction in volatility if we bought or sold some large amount. It tells us what would happen for alternate positions. We can also treat the best hedge as a mirror or replicating portfolio.
For the sample portfolio in Tables 10.18 and 10.19, the CAC Equity Index is the top contributor, but GBP 10-year yields is the top best hedge. The GBP 10-year yields position is the best hedge because it is highly correlated with USD 10-year yields, and together, these contribute 27 percent of the risk. A hedge using GBP 10-year will hedge both the existing GBP 10-year and the USD 10-year positions.
The top best hedge can be thought of as a replicating portfolio, in the sense that it is the single asset that best replicates the portfolio. For the GBP 10-year yield, the trade from the current holding to the best hedge is a sale of 56 million pounds' worth, which means that a buy of £56.2 million would be the best single-asset replicating portfolio. Such a replicating portfolio would explain 27.1 percent of the volatility.
Replicating portfolios can provide a useful proxy or summary of the actual portfolio, but the single-asset portfolio is often too simple. The three-asset and five-asset portfolios provide a much richer summary, and explain far more of the portfolio volatility. The five-asset portfolio explains 87.5 percent of the volatility and provides a valuable summary of the portfolio. The portfolio largely behaves like:

1. Long GBP 10-year yields (long 10-year bond, £26 million).
2. Long CAC Equity index (€11.9 million).
3. Long GBP FX (£19.4 million worth of FX exposure due to holding foreign currency bonds and equities).
4. Long company-specific equity exposure (€6.1 million).
5. Long U.S. 10-year yields ($24.1 million equivalent).

Reporting for Subportfolios
The report in Tables 10.18 and 10.19 show the top-level summary for the full portfolio. According to that report, the government portfolio contributes almost half the risk to the overall portfolio. Someone managing the overall risk needs the ability to drill down to examine the government portfolio in more detail. An effective way to drill down is to provide the same summary information, plus additional detail.
Tables 10.20 and 10.21 simply mimic the top-level report shown in Tables 10.18 and 10.19: expected volatility by asset class, top contributors, and top best hedges. In this case, the subportfolio is so simple—$20 million in a U.S. Treasury, £25 million in a U.K. Gilt, and $20 million in an option—that the summary is hardly necessary. (The replicating portfolios are not shown because they are trivial—the portfolio itself only contains three positions.) The summary does, nonetheless, show that the government portfolio incorporates both fixed-income risk (to changes in yields) and FX risk (due to a dollar-based portfolio holding a sterling-denominated bond).
Table 10.20 Summary Report for Government Subportfolio.

Table 10.21 Sample Portfolio Risk Report—Top Contributors.

Table 10.22 shows details by risk factor. For this subportfolio, the risk factors are yields (par bond rates) and FX rates. The top panel shows the contribution by risk factor. The holdings for this portfolio are U.S. Treasuries and U.K. Gilts, and Table 10.22 shows that roughly one-third of the risk arises from each, with the balance resulting from the FX exposure of holding a sterling bond in a dollar portfolio.
Table 10.22 Individual Position Report for Government Subportfolio.

The bottom panel shows the sensitivity of the portfolio to a 1-σ move in each particular risk factor. This is essentially the stand-alone volatility of each risk factor except that it is signed (positive if the P&L is positive in response to a downward move in rates, negative if otherwise).18 This provides a very detailed or granular view of the portfolio, possibly too granular for someone managing the full portfolio but necessary for someone managing the details of the subportfolio. The risk is expressed as sensitivity to a 1-σ move instead of more traditional measures such as sensitivity to a 1bp move or 10-year bond equivalents because the sensitivity to a 1-σ move allows comparison across any and all risk factors, asset classes, and currencies.
The sensitivity report is just the view of this simple portfolio that is made up of:

 Long U.S. 10-year bond
 Long U.K. 10-year bond
 Short U.S. option on 5-year bond
 Long sterling from owning the U.K. bond

The value of examining the detailed report does not show up using such a simple portfolio, but does when additional positions are added. Tables 10.23, 10.24 and 10.25 show the summary and detailed reports for a government subportfolio holding more, and more complex, positions.19
Table 10.23 More Complex Government Subportfolio—Summary Report.

Table 10.24 More Complex Government Subportfolio—Top Contributors and Replicating Portfolios Report.

Table 10.25 Individual Position Report for Government Subportfolio, More Complex Portfolio.

The summary report shows

 Most of the risk is actually contributed by FX exposure.

The top contributors and replicating portfolios report shows

 Euro FX exposure is by far the largest contributor.
 Euro FX is the only single hedge that reduces the portfolio volatility to any extent.
 Yields are important, but mainly in combination, as spreads.

The individual position report provides insight into what is producing this pattern of exposures.

 In the United States and United Kingdom, short 5-year bonds versus long 10-year bonds (a yield curve flattening position that benefits when the yield curve flattens).
 In Europe, long 5-year bonds and short 10-year bonds (yield curve steepening position), in roughly the same size as the sum of the United States and United Kingdom taken together.

10.7 Conclusion
This chapter has focused on risk reporting and portfolio risk tools applied to market risk. These tools help us understand the structure of the portfolio and how risks interact within the portfolio. All the examples are based on parametric estimation and delta-normal or linear approximations. Although many of the concepts (marginal contribution, for example) can also be applied when volatility is estimated by historical simulation or Monte Carlo, it is easiest to use these tools in a linear or delta-normal framework.
We now turn from our focus on market risk to considering credit risk. The fundamental idea remains—we care about the P&L distribution—but the tools and techniques for estimating the P&L distribution will often be different enough that we need to consider credit risk as a separate category.
Appendix 10.1: Various Formulae for Marginal Contribution and Volatilities
Marginal Contribution for Subportfolios—Partitioning
The marginal contribution can be calculated not just for single assets but also for groups of assets or for subportfolios. (See also Marrison 2002, 142.) For the full portfolio, the weights are the column vector:






with

These vectors can be formed into a matrix (which will have n rows and as many columns as partitions):

The partition might take the form of grouping assets together, for example, grouping assets 1 and 2 in partition a and all other assets on their own:




or it may take the form of subportfolios, so that the components ω1a,...ωna represent subportfolio a, ω1b,...ωnb represent subportfolio b, and so on, with the subportfolios adding to the total: ω = ωa + ωb ++ ωz.
Whatever partition we use, the expressions
(10.4a) 
(10.4b) 
will each produce a single-column vector with as many rows as partitions. Each element of this vector will be the marginal contribution for the corresponding partition.
This partition can also be used to calculate the stand-alone variance due to each group of assets or subportfolio. The expression

will give the stand-alone variances.
Volatility for Single-Asset Zero Position
Equation (10.5) gives the portfolio volatility at the zero position for asset k as:
(10.5) 
The logic of this is that the zero position in k means ωk = 0, and in the original expression for the variance, the row

gets multiplied by ωk and thus must be zeroed out. This is accomplished by subtracting it from the original variance. Also, because ωk = 0, the column ∑i ωkσik must be zeroed. By the symmetry of Σ, this will also be equal to [Σω]k, so we must subtract it twice. But this will duplicate the entry ωk·σkk·ωk, so it must be added back once.
Volatility for Single-Asset Best Hedge Position
Equation (10.6) gives the portfolio volatility at the best hedge position as:
(10.6) 
This works because (considering the best hedge for asset k):


But [Σω*]k = 0,  = ωi for i≠k, so

Now the only element different between ωi[Σω]i and ωi[Σω*]i for each i is ωiσki(ωk - ), which taken all together is (ωk - )(Σω)k, so that

But note that

so we end up with equation (10.6).
Volatility for Multiple-Asset Best Hedge
The best hedge for two assets, j and k,  and  are the solution to:


But


Which means
(10.7) 
(Note that the expression for the mirror portfolio coefficients is essentially the least-squares normal equations. Calculating a mirror portfolio or replicating portfolio is effectively regressing the portfolio return against the selected assets.)
Equation (10.8) gives the portfolio volatility at the best hedge position as:
(10.8) 
The variance at the best hedge is:

But [Σω*]j,k = 0,  = ωi for i ≠ j, k, so

Now the elements different between ωi[Σω]i and ωi[Σω*]i for each i is ωiσji(ωj - ) + ωiσki(ωk - ), which taken all together is (ωj - )[Σω]j + (ωk - )[Σω]k so that


Contribution to Volatility, VaR, Expected Shortfall
As discussed in the text, the properties of the marginal contribution to risk derive from the linear homogeneity of the risk measure and do not depend on the particular estimation method. As a result, the marginal contribution to risk can be calculated for volatility, VaR, or expected shortfall, using the delta-normal, Monte Carlo, or historical simulation approach.
McNeil, Frey, and Embrechts (2002, equations 6.23, 6.24, and 6.26) give formulae for contributions for volatility, VaR, and expected shortfall. Repeating from the main text, say that the portfolio is made up of investments in n assets, the P&L for one unit of asset i being denoted by Xi, and the amount invested in asset i is ωi. Then the total P&L is ∑iωiXi, the Z% VaR is VaRz = {Y s.t. P[∑iωiXi ≤ Y] = Z}, and the expected shortfall is ESz = E[∑iωiXi | ∑iωiXi ≤ VaRz]. The contributions (in levels) are:

volatility:  MCLi = ωi cov(Xi,∑kωkXk)/√variance(∑kωkXk)
VaR:  MCLi = ωi E[Xi | ∑kωkXk = VaRz]
ES:  MCLi = ωi E[Xi | ∑kωkXk ≤ VaRz].

First, let us examine the formulae if the P&L distribution were normal. In this case, the contributions to volatility, VaR, and expected shortfall are all proportional. Using the formulae in Section 8.1, we see that


In other words, the marginal contribution to VaR and expected shortfall are proportional to the marginal contribution to volatility. McNeil, Frey, and Embrechts (2005, 260) show that the proportionality for volatility, VaR, and ES holds for any elliptical distribution (and any linearly homogeneous risk measure).
We turn next to using Monte Carlo or historical simulation to estimate risk measures (whether volatility, VaR, or expected shortfall) and contribution to risk. Use a superscript q to denote a particular scenario, and continue to use a subscript i to denote asset i. The formula for marginal contribution to volatility will be:

That is, the covariance and variance will (naturally) be estimated by the usual sum of cross products and sum-of-squares. (For notational convenience, I assume in that formula that all the Xi are measured as deviations from means.)
For VaR:20

That is, we simply choose the appropriate scenario q that is the VaR (in other words, the scenario q that is the appropriate quantile, say the 50th out of 5,000 scenarios for the 1 percent/99 percent VaR). The contribution to VaR for asset i is then simply the P&L for asset i from that scenario. (This will clearly be additive, since ∑kωk = VaR.)
The problem with this is that the estimate for MCLi will have considerable sampling variability, since it uses only the single observation ωi · [ | ∑kωk = VaR]. There will be many possible combinations for the values {X1,..., Xn} that all give the same ∑kωk = VaR, and thus many possible realizations for ωi · [ | ∑kωk = VaR].
To see the problem with using the P&L observation from a single scenario as the estimate of the contribution to VaR, consider the following simple portfolio:

Two assets, X1 and X2.
Each normally distributed with mean zero and volatility σ and correlation ρ.
Each with weight ω = ½.

The portfolio P&L will be the sum:

with

We can write

with

We can thus write

The contribution to VaR (in levels) for asset 1 is

For Monte Carlo, the estimate of the contribution to VaR would be the random variable variable ω1Z:

We can calculate

and

These will be the probability that the estimated contribution in levels is below βY and above zero, respectively (remember Y will be negative, and that β = 1). For proportional marginal contribution, these two boundaries are proportional contribution above 1.0 or below 0.0 (the sign changes because Y is negative). In other words, for Monte Carlo there will be a 24.6 percent probability that the marginal contribution estimate is outside the range [0,1], when in fact we know the true contribution is 0.5. This is a huge sampling variability.
Heuristically, the problem is that for a particular Monte Carlo simulation we cannot average over multiple scenarios since there is only one scenario for which ∑iωi = VaR. To average over multiple observations for a particular asset i (that is, to obtain multiple ωi· for an asset i) we would need to carry out multiple complete simulations, say indexed by m, taking one observation ωi· from each simulation m.21
For expected shortfall:


Appendix B: Stepwise Procedure for Replicating Portfolio
In Section 10.5, I laid out a simple procedure to build up a replicating portfolio by sequentially adding hedges. A more complex procedure, more closely analogous to stepwise regression, is to go back to consider earlier best-hedge assets, one at a time, to ensure that they produce a greater reduction in portfolio variance than the newest asset.

 Choose the first replicating portfolio asset as the volatility-minimizing single best hedge.

 That is, calculate (k) for all k. This is the best-hedge volatility for all one-asset best hedges or mirror portfolios.
 Choose as the first replicating portfolio asset, 1*, the asset k, which produces the smallest (k).

 Choose the second replicating portfolio asset as that asset which, combined with the first, produces the largest reduction in portfolio variance.

 That is, calculate (1* & k) for all k = {all assets excluding the first replicating portfolio asset}. This is the best-hedge volatility for all two-asset best hedges that include the first replicating portfolio asset.
 Choose as the second replicating portfolio asset, 2*, the k for which (1* & k) is the smallest (or the variance reduction [1*] - [1* & k] is the largest).

 Choose as the third replicating portfolio asset that asset which, combined with the first two, produces the largest reduction in portfolio variance, but also check that the earlier assets still produce a sufficiently large reduction in variance.

 That is, calculate (1* & 2* & k) for all k = {all assets excluding the first and second replicating portfolio assets}. This is the best-hedge volatility for all three-asset best hedges that include the first two replicating portfolio assets.
 Choose as the third replicating portfolio asset, 3*, the k for which (1* & 2* & k) is the smallest (or the variance reduction [1* & 2*] - [1* & 2* & k] is the largest).
 Go back and check the first two replicating portfolio assets to make sure they produce a large reduction in variance when combined in a portfolio.

 Calculate (1* & 3*) and (2* & 3*), the variances sequentially excluding one of the earlier chosen replicating portfolio assets.
 Compare the variance for these new potential two-asset portfolios versus the already-chosen portfolio. That is, calculate: (1* & 3*) - (1* & 2*) and (2* & 3*) - (1* & 2*).
 If either or both are negative, replace either 1* or 2* with 3*, choosing the most negative if both are negative. (In reality, [1* & 3*] > [1* & 2*] always because 2* is chosen to minimize the portfolio variance when combined with 1*.) Then go back and choose a new third asset.


 Choose as the fourth replicating portfolio asset that asset which, combined with the first three, produces the largest reduction in portfolio variance.

 That is, calculate (1* & 2* & 3* & k) for all k = {all assets excluding the first, second, and third replicating portfolio assets}. This is the best-hedge volatility for all four-asset best hedges that include the first three replicating portfolio assets.
 Choose as the fourth replicating portfolio asset, 4*, the k for which the reduction from (1* & 2* & 3*) to (1* & 2* & 3* & k) is the largest.
 Go back and check the first three assets to make sure they produce a large reduction in variance when combined in a portfolio.

 Calculate (1* & 2* & 4*), (1* & 3* & 4*), and (2* & 3* & 4*), the variances sequentially excluding one of the earlier chosen replicating portfolio assets.
 Calculate (1* & 2* & 4*) - (1* & 2* & 3*), (1* & 3* & 4*) - (1* & 2* & 3*), and (2* & 3* & 4*) - (1* & 2* & 3*).
 If any are negative, replace the appropriate earlier asset with 4*, choosing the most negative if more than one are negative. Then go back and choose a new third asset.



Appendix C: Principal Components Overview
The factors obtained by principal components analysis are new random variables, which are linear combinations of the original variables:
(A10.9) 

We want the variance-covariance matrix of the factors to be diagonal (so factors are uncorrelated):
(A10.10) 
Principal components analysis sets the columns of the matrix A to the eigenvectors (characteristic vectors) of the variance-covariance matrix, with columns ordered by size of the eigenvalues. The eigenvectors are a convenient choice. They work because by the definition of the eigenvectors of the matrix ΣY:22
(A10.11) 
where Diag(λ·) is the matrix with zeros off-diagonal and λi in the diagonal element (i,i). This diagonalization gives a diagonal matrix for the variance-covariance of the variables F, E[F·F′]:
(A10.12) 

The reverse transformation from factors to original variables is:
(A10.13) 
The matrix can easily be expressed in terms of the original A, using

to get

giving

or

One well-known result from principal components (eigenvectors) is that it provides a decomposition of the total variance, defined as tr(ΣY):

That is, the eigenvalues sum to the total variance (sum of variances), and since the eigenvectors are orthogonal, the components are orthogonal components that explain the total variance. Traditionally, eigenvectors/eigenvalues are sorted from largest to smallest, so that the first eigenvector accounts for the largest proportion of the total variance, the second for the second-largest, and so on.
In looking at a portfolio, however, we are generally less concerned with the sum of the variances (diagonals of the variance-covariance matrix) and more concerned with the volatility or variance of the portfolio, which is a combination of the components of the variance-covariance matrix. It turns out that here the diagonalization in equations (A10.11) and (A10.12) is also valuable.
The portfolio variance is the quadratic form:

Premultiplying Δ by A-1 will allow us to diagonalize and decompose the portfolio variance into a sum of independent principal components:
(A10.14) 
where di ith component of Δ′ · A
This is a really valuable decomposition:

1. The term A · diag(√λi/si) in the third line is an n × n matrix, which we can call FL, the factor loading. Column i gives the change in the original yields due to a 1-σ move in principal component or factor i (the factor loading for factor i).
2. The term Δ′ · A · diag(√λi/si) (or Δ′ · FL) is a row vector. Element i provides the P&L because of a 1-σ move in principal component or factor i.
3. The full expression (Δ′ · A) · diag(λi/si) · (A′ · Δ) is the portfolio P&L variance. It is the dot product of the vectors Δ′ · A · diag(√λi/si) and diag(√λi/si) · (A′ · Δ), and so is the sum-of-squares of the P&L, resulting from 1-σ moves in principal components. As a sum-of-squares it decomposes the overall portfolio variance into a sum of components due to separate, uncorrelated, principal components.

In other words, when we work with the principal components, we do have a simple additive decomposition of the overall variance into elements due to the separate principal components. This is in stark contrast to working with the original variables, where the overall variance does not have any additive decomposition—the best we can do is an additive decomposition of the infinitesimal change in the volatility, the marginal contribution discussed earlier.
As mentioned before, the eigenvectors are determined only up to an arbitrary constant (the si). There are three convenient choices:

1. si = 1. This is the standard normalization (used, for example, by MatLab and Gauss). This gives E[F·F′] = diag(λi) and A′-1 = A, A′ = A-1.
2. si = 1/λi. This gives E[F·F′] = I.
3. si = λi. This means a change in Y due to 1-σ move in Fi (factor loading) is given by the matrix A, so this can be read directly from the columns of A.

Example
Consider 2-year and 10-year rates. Assume that the rates have the following volatilities:

If the correlation between 2-year and 10-year rates is 80 percent, then the variance-covariance matrix will be (measured in daily basis points):

The eigenvectors are:

The eigenvalues are 55.69 and 5.595. (The eigenvectors are calculated using the normalization A′A = I or si = 1.) The change in Yi due to a 1 − σ change in the new factors or the factor loading FL are given by the columns of A · diag(√λi/si), in this case A · diag(√λi). These are the columns in the following matrix:

This is roughly the parallel (up 6.11 and 4.28 for 2-year and 10-year) and twist (-1.36 and +1.94 for 2-year and 10-year) factors commonly found as the first two factors for yield curve movements.
The sum of rate variances is 61.27 (the sum of the diagonals of the variance-covariance matrix, 39.22 + 22.06) and the first component accounts for 90.9 percent of this. This is the standard result for principal components analysis.
More interesting for the current context is to examine the principal components analysis applied to a portfolio. For this, we need the portfolio sensitivity Δ so that we can calculate terms such as Δ′ · A · diag(√λi/si). Now assume that the portfolio sensitivity was

That is, when 2-year rates increase by 1 bp, the P&L is -1, while when 10-year rates increase by 1 bp, the P&L is +2. This can be translated into sensitivities to the principal components using:




The overall portfolio variance is 33.32 and decomposes into 33.32 = 5.97 + 27.35 (= 2.442 + 5.232). In contrast to the sum of yield variances (diagonals of the yield variance-covariance matrix), where the first factor is most important, only 5.97 (out of 33.32) or 17.9 percent of the portfolio variance is accounted for by the first factor while the second factor accounts for 82.1 percent. This can be seen directly from the P&L due to 1-sigma moves. Since the factors are orthogonal, the portfolio variance is the sum of the component variances (covariance terms are zero):

Notes
1. This assumes the volatilities σi are measured in dollar terms as in the following example. If they are measured as volatilities per unit or in percentage returns, and the asset portfolio weights are ωi, the expression will be 
2. I know of this analogy between volatilities and triangles from Litterman (1996) but Litterman notes that it may have been used earlier. For those with a background in vectors and complex numbers, the volatilities add as vectors and the vectors (volatilities) can be conveniently represented as complex numbers in polar notation (remembering that the convention for complex numbers is to measure the angle from the x-axis so that the angle ϕ for the complex representation is ϕ = 180 - θ or cos ϕ = ρ). This analogy does not to extend conveniently to three volatilities because of the difficulty in defining the angles between vectors.
3. Jorion's explication of these ideas is unfortunately somewhat confusing—his marginal VaR is not additive and he fails to point out the marginal nature of his component VaR (my marginal contribution). See the discussion in a subsequent footnote.
4. Marrison (2002, ch. 7) has a nice explication of the marginal contribution (Marrison calls it VaR contribution) with clear formulae in both summation and matrix notation. Unfortunately, Marrison does not point out the marginal nature of the measure discussed next (that it gives the infinitesimal change in volatility for an infinitesimal percent change in asset holding) but otherwise the discussion is very useful.
5. Jorion (2006), while sound in most areas, falls down here. Jorion (2006, section 7.21) introduces marginal VaR as dσp = ∑i[ω′Σ]i/σp dωi (a function of dω) rather than dσp = ∑i[ω′Σ]i ωi /σp d ln ωi (a function of dlnω). Such a definition is valid but of limited usefulness; it does not provide an additive decomposition of volatility or VaR since the components of [ω′Σ]i /σp do not add up to the total volatility. Jorion's section 7.2.3 then introduces the term component VaR for [ω′Σ]i ωi/σp (what I call marginal contribution to risk). Component VaR is presented as a partition of the VaR "that indicates how much the portfolio VAR would change approximately if the given component was deleted." This misses the point that [ω′Σ]i ωi /σp (Jorion's component VaR, my marginal contribution) provides the same marginal analysis as [ω′Σ]i/σp (Jorion's marginal VaR) while also providing a valuable additive decomposition of the volatility or VaR. Furthermore, using the component VaR as an approximation to the change in VaR upon deletion of a particular asset is of limited usefulness; the approximation is often poor, and the actual volatility change to a zero position (the all-or-nothing contribution) is easy to calculate.
6. The volatility itself may be more or less useful depending on the form of the P&L distribution, but when one does use the volatility as a risk measure, the marginal decomposition applies for any distribution.
7. Litterman (1996, 64) and Crouhy, Gailai, and Mark (2001, 255) use Euler's law and the linear homogeneity of volatility or VaR, or both, to prove the marginal decomposition. Embrechts, McNeil, and Frey (2002, section 6.3) have a detailed discussion of Euler's law and application to capital allocation (and the full allocation property, which is equivalent to the additive decomposition discussed here). Such a decomposition will apply to any coherent risk measure, since linear homogeneity is one of the defining axioms for coherent risk measures. See Artzner et al. (1999) and Embrechts, McNeil, and Frey (2002, section 6.1) for a discussion of coherent risk measures.
8. For a normal distribution, the contributions to volatility, VaR, and expected shortfall are all proportional. Using the formulae in section 8.4, we see thatMCLi(VaR, normal) = ωi [cov(ωiXi,∑iωiXi) / √var(∑iωiXi)] · Φ-1(z)MCLi(ES, normal) = ωi [cov(ωiXi,∑iωiXi) / √var(∑iωiXi)] · ϕ[Φ-1(z)]/z.McNeil, Frey, and Embrechts (2005, 260) show that the proportionality for volatility, VaR, and expected shortfall holds for any elliptical distribution (and, furthermore, for any linearly homogeneous risk measure).
9. Note that Marrison's (2005, 143-144) method for estimating contribution to VaR for Monte Carlo seems to apply to expected shortfall, not VaR. The correct formula for VaR should be: MCLi = [Lossi | Lossp = VaR] = ωi · [Xi | ∑iωiXi = VaR], that is, the contribution to VaR for asset i is the loss due to asset i. (This will clearly be additive since ∑iωiXi = VaR.) The problem naturally arises that the estimate for MCLi uses only the single observation ωi · [Xi | ∑iωiXi = VaR]. For a particular Monte Carlo simulation, we cannot average over multiple scenarios since there is only one scenario for which Lossp = ∑iωiXi = VaR. To average over multiple observations for a particular asset i (that is, to obtain multiple ωi·[Xi | ∑iωiXi = VaR] for an asset i) we would need to carry out multiple complete simulations, taking one observation ωi·[Xi | ∑iωiXi = VaR] from each simulation.
10. Trivially, for a zero position, the marginal contribution will be zero, but there will also generally be a nonzero position such that the marginal contribution is zero, and this is what we are interested in. This assumes that the position may be long or short (positive or negative). If a portfolio is constrained to be long only, then the best hedge position may not be obtainable but it is nonetheless useful for the insight it can provide.
11. In Section 10.2, the correlation between the CAC futures and the 10-year Treasury was 0.24, which gave risk reduction potential of only 3 percent. In that section, we were considering how much the 10-year Treasury volatility could be reduced, and from 130,800 to 126,900 is indeed only 3 percent. Here we are asking a different question—how much the portfolio volatility can be reduced.
12. Examination of equation (10.7) for calculating best hedge weights shows that it is essentially the least-squares equations, using assets already included in the portfolio. For assets not in the portfolio, the history of the desired assets can be regressed against a synthetic history of the daily portfolio P&L (ω′Y, where Y′ = [y1y2...yn], which is a vector of the historic changes in assets). The subset of constituent assets are chosen from the full set either by brute-force search or by a stepwise regression approach, just as before.
13. It is a stylized fact that for many currencies, yield curves decompose into (approximately) parallel, twist, and hump components.
14. More accurately, scenarios that occurred frequently in the past. The principal components analysis is based on history and past results may not be representative of future results. As I have stressed before, however, having a clear view of how the portfolio would have behaved in the past is the first step to better judgment about how it may behave in the future.
15. In this example, there is a minus sign: Δpc = -FL · Δyld, because the sensitivities are measured as $ per 1bp fall in yield, so that a positive denotes long with respect to a rally in bond prices.
16. The change in Y and F are related by ΔY = FL·ΔF, so ΔF = FL−1·ΔY, but if one wishes to use only a subset of the components, then regression is a convenient way to calculate a subset of the Δfs.
17. To my knowledge, Goldman Sachs pioneered the use of reporting top contributors and they have trademarked the term Hot Spots for such a report—see Litterman (1996).
18. Note that the sensitivity for the 10-year U.S. yield position is the same as the stand-alone volatility of the 10-year bond discussed in the preceding chapter because the current portfolio is just an expanded version of that portfolio.
19. The additional positions are short $30M worth of USD and GBP 5-year bonds, long $60M worth of EUR 5-year bonds, and short $40M worth of EUR 10-year bonds.
20. Note that Marrison's (2005, 143-144) method for estimating contribution to VaR for Monte Carlo seems to apply to expected shortfall, not VaR, but see the following footnote.
21. Noting the result quoted in McNeil, Frey, and Embrechts (2005, 260) showing the proportionality of contributions for volatility, VaR, and expected shortfall for elliptical distributions, one strategy might be to estimate the proportional contribution for volatility (or expected shortfall), then multiply by the VaR to obtain a contribution (in levels) to VaR. This would justify Marrison's (2005, 143-144) method but it is an ad hoc approach.
22. Assuming that the variance-covariance matrix is full rank.









Chapter 11
Credit Risk
11.1 Introduction
Credit risk is ubiquitous in modern finance: "Credit risk is the risk that the value of a portfolio changes due to unexpected changes in the credit quality of issuers or trading partners. This subsumes both losses due to defaults and losses caused by changes in credit quality" (McNeil, Frey, and Embrechts 2005, 327).
In many ways, the analysis of credit risk is no different from risk arising in any other part of a firm's business. The focus is on the distribution of gains and losses (P&L) and how information about gains and losses can be used to manage the business of the firm.1
Although the underlying idea is simple, particular characteristics of credit risk mean that the techniques used to estimate and analyze credit risk are often different and more complex than for market risk. The distribution of P&L can be very difficult to estimate for a variety of reasons (see McNeil, Frey, and Embrechts, 2005, 329):

 Most credit risks are not traded and market prices are not available, so the distribution of gains and losses must be constructed from first principles, requiring complex models.
 Public information on the quality and prospects for credit risks are often scarce. This lack of data makes statistical analysis and calibration of models problematic. (Additionally, the informational asymmetries may put the buyer of a credit product at a disadvantage relative to the originator.)
 The P&L distribution for credit risks is often skewed, with fat lower tails and a relatively larger probability of large losses. Such skewness is difficult to measure but particularly important because the economic capital required to support a portfolio is sensitive to exactly the probability of large losses—the shape of the lower tail drives the economic capital.
 Dependence across risks in a portfolio drives the skewness of the credit risk distribution, but dependence is difficult to measure with accuracy.

Credit risk modeling has the same underlying goal as market risk analysis—build the distribution of P&L over some horizon and use that distribution to help manage the business activity. For market risk, the distribution of P&L can usually be measured directly from the market, by looking at history. For credit risk, in contrast, the distribution must often be built from scratch, using limited data and complicated models, each with their own specialized methodology and terminology.
Varieties of Credit Risk
Credit risk shows up in many areas and pervades modern finance. This section provides a brief overview of the main instruments and activities where credit risk shows up.
The standard approach to credit risk traces back to commercial banks and their portfolios of loans (cf. Jorion 2007, 454-455). It is easy to see that a major risk, indeed the dominant risk for a loan, is that the issuer will default: credit risk in its quintessential form. Although credit risk analysis and modeling may have originated in banks to cover loans, credit exposure actually permeates finance:

 Single-issuer credit risk such as for loans and bonds. The default of the issuer means nonrepayment of the principal and promised interest on the loan or bond.
 Multiple-issuer credit risk such as for securitized mortgage bonds. Such bonds are issued by a bank or investment bank but the underlying assets are a collection of loans or other obligations for a large number of individuals or companies. Default of one or more of the underlying loans creates credit losses.
 Counterparty risk resulting from contracts between parties, often over-the-counter (OTC) derivatives contracts. OTC transactions, such as interest rate swaps, are contracts between two parties, and if one party defaults, it may substantially affect the payoff to the other party. Other contracts, such as letters of credit, insurance, financial guarantees, also entail counterparty credit risk because there is potential for loss upon default of one party.
 Settlement risk. Associated with delivery and settlement of trades, the possibility that one side fails to settle a trade after being paid.

Data Considerations
Credit risk is as much about data as it is about quantitative tools and analysis. One of the biggest challenges in the practical implementation of a credit risk system is the basic task of developing an effective database of both external and internal data.
The data required to analyze credit risk falls into two broad categories. First, what might be termed external data, cover the credit quality and prospects of counterparties and other credit exposures. As mentioned earlier, public information on credit quality, indeed on all aspects of a counterparty, are often difficult to acquire and so make statistical analysis and credit modeling difficult.
The second category is what might be termed internal data, internal to the firm: the details concerning exactly who are a firm's counterparties and other credit exposures. Collecting, collating, cleaning, and using these internal data is often challenging. Such internal data are under the control of the firm, and so it is often assumed that it is accessible. Unfortunately, such data are often scattered throughout different units of an organization, in separate legacy systems, collected and stored for reasons unrelated to credit risk analysis, and all too often difficult to access and unusable in the original form. These internal data can be intrinsically complex and difficult to collect.
As an example of the potential complexity of internal data, consider a firm's possible exposure before Lehman Brothers' collapse in 2008. One unit might hold a Lehman bond, another hold an OTC interest rate swap with Lehman, and a third might be settling an FX trade through Lehman as prime broker. All of these are at risk when Lehman goes into bankruptcy. Simply collecting information on the existence of such disparate exposures is not trivial, particularly given their heterogeneity in terms of duration, liquidity, and complexity of underlying assets.
11.2 Credit Risk versus Market Risk
Earlier chapters focused primarily on market risk, so it is useful to highlight some differences between credit risk and market risk. The difference center around specific issues: first, the time frame over which we measure the P&L distribution—it is much longer for credit risk; second, the asymmetry and skew of the P&L distribution—credit risk leads to highly skewed distributions; third, the modeling approach—P&L for credit risks must usually be modeled from first principles rather than from observed market risk factors; finally, data and legal issues become relatively more important. We review each of these briefly before turning to a simplified model of credit risk.
Liquidity and Time Frame for Credit versus Market Risk
Although the P&L distribution is the primary focus for both market risk and credit risk, the time frame over which P&L is evaluated is often substantially longer for credit risk than for market risk. This is primarily a result of the illiquidity of most credit products. Credit products, loans being the classic example, have traditionally not been traded, and institutions have held them until maturity. Furthermore, credit events tend to unfold over a longer time horizon than market events. Information on credit status changes over weeks and months, not minutes and hours as for market variables. For these reasons, measuring P&L for credit risk over a period of days or weeks is usually inappropriate because there is no practical possibility that the P&L could be realized over such a short period, and in many cases could not even be realistically measured over the period.
One result of considering a much longer time period for the P&L distribution is that the mean matters for credit risk while it generally does not for market risk. For market risk, distributions that are measured over days, the volatility of market returns swamps the mean. For credit risk, in contrast, the P&L distribution is often measured over one or more years, and over such a long period, the mean will be of the same order as the volatility and must be accounted for in using any summary measures, whether VaR or other.
Asymmetry of Credit Risk
The distribution of P&L for credit risks will often be asymmetric, highly skewed with a fat lower tail. Figure 11.1 shows results for a stylized model for the returns from a simple loan portfolio (discussed in more detail further on).

Figure 11.1 P&L Distribution for a Simple Model of a Loan Portfolio
Note: This is the one-year income (in dollars) from holding a portfolio of 1,000 homogeneous loans of face value $1,000, each with average probability of default of 0.01 and a default correlation across loans of 0.4 percent (roughly representative of BB-rated loans). Loss given default is 50 percent, promised interest income is $65. The model is discussed in Section 11.3, together with the specifics of the dependence structure. Reproduced from Figure 5.15 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

It is often argued that the distribution of credit risk is asymmetric because a credit portfolio will have many small gains and a few large losses, for example, due to infrequent defaults on loans causing a complete loss of principal. In fact, there are more fundamental reasons for asymmetry in credit risk. The idea of small gains versus large losses certainly applies, but cannot be the whole story, as we will see in Section 11.3. Dependence across defaults, for example, defaults clustering during times of general economic stress, is a prime candidate for why defaults, and credit risks generally, exhibit asymmetry.
Whatever the cause of asymmetry or skewness, it is more prevalent in credit risk than market risk. This makes credit risk inherently more difficult to measure than market risk. The degree of skewness will have a particularly large impact on the lower tail, and since the degree of skewness is hard to determine exactly, the lower tail will be difficult to measure. Since it is exactly the lower tail that is of most interest in credit risk, for example, in determining the reserves or economic capital required to sustain a business, asymmetry and skewness make the assessment of credit risk more complex than market risk.
Constructivist (Actuarial) versus Market Approach to Modeling the P&L Distribution
Market risks are, by their very nature, actively traded in the market. The availability of market prices means that estimates of the distribution of P&L over a given time horizon (usually short) can usually be derived from observed market risk factors, market prices, yields, rates. Although there is considerable thought and effort directed toward estimation of the appropriate distribution (for example, ensuring that the tails are appropriately measured), the distribution is invariably based on observed market prices, and market risk is solidly grounded in market pricing.
In contrast, credit risks are often not actively traded, so market prices are not available and the distribution of P&L cannot be taken from observed prices. As a result, the P&L from credit-related products must be constructed from a granular model of the fundamental or underlying causes of credit gains and losses, such as default, ratings changes, and so on. I call this a constructivist approach to modeling the distribution of P&L.2
The contrast between the market-based approach used for market risk and the constructivist approach applied to credit risk is a primary distinguishing characteristic of market risk versus credit risk. Much of the complication surrounding credit risk is a result of the necessity of building the distribution from first principles, constructing the distribution from underlying drivers.
While the approach taken to modeling market and credit risks are different, this arises not from a fundamental difference between market and credit risk but rather from the type of information available. The distribution of P&L for IBM, for example, could be constructed from a fundamental analysis, considering IBM's market position, pipeline of new products, financial position, and so on. In fact, this is what equity analysts do to make stock recommendations. In risk measurement, there are many reasons for using market prices for IBM rather than constructing the P&L based on underlying variables. Probably the best reason is that the market price distribution incorporates the estimates of a multitude of investors and traders regarding IBM's future prospects—there needs to be a pretty strong reason to ignore market prices.
In a perfect world in which the distribution of future outcomes can be appropriately modeled, the constructivist approach will give the same answer as the market or price approach. In practice, credit modeling must often take the constructivist approach because the underlying risk is not traded and market prices are not available.
Data and Legal Issues
Data issues were touched on earlier—credit risk involves substantial demands for both external and internal data. The data for credit risk are often low frequency (monthly, quarterly, annual) versus the high frequency data common for market risk, but collecting and collating data is often difficult because public data on credit risks are often not available.
By legal issues, I mean matters such as the legal organization of counterparties, details of contracts (netting, collateral), or priority and venue in the event of bankruptcy. Such issues generally do not matter for market risk. Market risks usually depend on changes in prices of standardized securities rather than arcane details of legal contracts. In contrast, legal matters are paramount when considering credit risk: exactly what constitutes a default, and how much is recovered upon default, critically depends on legal details.
11.3 Stylized Credit Risk Model
Introduction
My treatment of credit risk and credit modeling diverges from the approach usually taken in risk management texts. The current section lays out a stylized model to provide a framework for understanding how credit risk models are used. Section 11.4 provides a taxonomy of models (largely following McNeil, Frey, and Embrechts 2005, ch. 8 and ch. 9). Section 11.5 then briefly discusses specific models (Merton's [1974], KMV, CreditMetrics, CreditRisk+) and puts them into context, using the stylized model of the current section.
Most treatments, in contrast, start with a discussion of what is credit risk, industry practice for analyzing and categorizing credit risk, and detailed description of one or more specific credit models, such as Merton's (1974) option-theoretic model of default or industry-developed models such as KMV or CreditMetrics. I will refer to other texts for background on actual credit risk practice and models. Crouhy, Mark, and Galai (2000, ch. 7 through 12) is a particularly good review of banking industry practice and models. (Crouhy, Mark, and Galai 2006, ch. 9 through 12 provides a somewhat less detailed overview.) McNeil, Frey, and Embrechts (2005, ch. 9 and particularly ch. 8) provides a good treatment of the technical foundations for credit models. Marrison (2002, ch. 16 to 23) also has an extensive discussion of industry practice and modeling, with chapter 17 providing a particularly nice overview of the variety of credit structures that a bank faces. Duffie and Singleton (2003) is a more advanced and technical reference.
The aim of the present section is to demonstrate the characteristics of credit risk modeling, not to build a realistic credit model. One important aim of this section will be to point out that the concept behind credit risk models is simple, but also to explain why realistic models are complex and difficult to build.
Stylized Credit Risk Model
The stylized model is built to analyze a particular portfolio, a portfolio that contains 1,000 identical loans. The time horizon over which we measure the P&L distribution is one year, as we wish to determine an appropriate level of annual reserves. One year happens to be the same as the loan maturity. The loans are made to a variety of businesses, but all the businesses have the same credit quality so that the chance of default or other adverse event is the same for each loan, and the chance of default for a single loan is 1 percent. All the businesses are assumed to be independent of the other loans, so defaults are independent. If a loan defaults, there is virtual certainty that recovery, from liquidation of the business or assets held as collateral, will be 50 percent of the loan's face value.
These characteristics are summarized in Table 11.1.
Table 11.1 Characteristics of Loans, Credit Analysis, and Credit Quality.



Loans
Credit Quality
Output




$1,000 initial investment
All identical credit quality
Require 1yr P&L distribution


One year final maturity
Recovery upon default is 50 percent



Promised interest at year-end: 6.5 percent
Probability of default of an individual loan: 0.01




Individual loans independent



Reproduced from Exhibit 5.4 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.



The initial portfolio value is $1 million. The value in one year depends on the repayment and default experience. If an individual loan is in good standing, the repayment is $1,065 (income $65). If a loan defaults, the recovery is $500 and the loss is $500. These payments are shown schematically in Figure 11.2.

Figure 11.2 Schematic of Initial Investment and Final Repayment of Individual Loans and Overall Portfolio
Reproduced from Figure 5.16 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

The actual income (less initial investment) is:

We know that the average probability of defaults is 1 percent, so, on average, 10 loans will default. Thus the average actual income will be:

Beyond the average performance, we need to know how the portfolio is likely to behave in adverse circumstances, and how much the bank making the loans should set aside in reserves to cover the contingency that more loans than expected go into default. We can answer such a question if we know the full distribution of the P&L.
Before turning to the solution of this model, let me highlight a critical assumption: the independence of loans across borrowers. Loans are assumed to be independent and there is no correlation across borrowers (no change in probability of default because other borrowers do or do not go into default). Furthermore, the probability of default does not change with conditions in the economy or other factors—the probability is indeed constant at 0.01 for every borrower.
Under this assumption, the distribution of defaults is actually very simple: a binomial distribution, since the outcome for each of the 1,000 loans is a Bernoulli trial, default (probability 0.01) versus not-default (probability 0.99). The probability of having k defaults out of 1,000 firms is (by the binomial distribution):

Figure 11.3 shows the distribution of defaults in Panel A and the distribution of income in Panel B.

Figure 11.3 Number of Defaults for Portfolio of 1,000 Homogeneous Loans
Note: Panel A: The number of defaults for a portfolio of 1,000 homogeneous loans, each with probability of default of 0.01. This is a binomial distribution with 1,000 trials, probability of default 0.01. Panel B: The one-year income from holding such a portfolio with loss given default 50 percent, promised interest 6.5 percent. Reproduced from Figure 5.17 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

A couple of points with respect to the distribution of defaults: first, the distribution for this situation is easy to write down analytically, but that is not generally the case. Simulation is usually necessary. Simulation would be easy in this case: simply draw 1,000 uniform random variables (rv) between 0 and 1, compare each with the probability of default (0.01). If the rv is above 0.01, the firm does not default; if below 0.01, the firm does default. Simulation in more complex cases is similar, and often very simple conceptually.
The second point to note is that the distribution of losses and income is symmetric. This is hardly surprising given the well-known result that the binomial distribution converges to the normal for large n, and n = 1,000 is large. It does, however, demonstrate that distributions from credit risk are not of necessity asymmetric and that asymmetry does not necessarily arise from "small gains, large losses." The portfolio has many small gains ($65 for each of roughly 990 performing loans) and a few large losses ($500 for each of roughly 10 nonperforming loans) but the distribution is still symmetric; it is not simply small gains and large losses that produce an asymmetric distribution. Credit loss distributions are indeed often asymmetric, but usually due to dependence in defaults across firms. We return to this and consider the asymmetry of the distribution and alternative dependence assumptions further on.
Using the distribution displayed in Figure 11.3, we could provide some reasonable answers to questions regarding how much we might lose in adverse circumstances, but these questions will be delayed for a short time.
Credit Risk Modeling—Simple Concept, Complex Execution
This model is very simple, but it does contain many, even most, characteristics of more realistic credit risk models. (More accurately, static or discrete-time models as discussed in the taxonomy in Section 11.4.) There are four risk factors that contribute to credit risk, and this model highlights three (correlation is discussed further on):

1. Default—Probability that counterparty defaults and some or all of the value is lost. Termed probability of default (PD) or expected default frequency (EDF).3 (In this example, the probability of default is 0.01.)
2. Correlation—Dependency across firms in default probability. As discussed further on, this has a huge impact on the shape of the distribution of credit losses. (In this example, the correlation is zero.)
3. Exposure—The amount the firm has exposed to a particular counterparty or is at risk to a particular credit, also termed the exposure at default (EAD). (In this example, the exposure for each loan is $1,000.)
4. Recovery—The amount recovered upon default, since rarely is the whole amount lost. Also expressed as the loss given default (LGD), where recovery = 1 - LGD. (In this example, recovery is 50 percent or $500 out of $1,000 investment.)

Technically, credit risk depends on the three factors: default, recovery, and exposure (see, for example, Jorion 2007, 454-455). Loss amount is the product of these:

where Y = default indicator = 1 if default occurs and = 0 if no default
δ = percentage recovery (in the current model, 50 percent)
e = exposure, the dollar amount at risk if default occurs (in the current model, the loan amount, $1,000)
Correlation or dependence across defaults is simply a characteristic of the joint default probability and is subsumed under the default factor. Nonetheless, dependence across defaults is such an important element, one that has such a huge impact on the shape of the distribution of defaults in a portfolio context, that I include it as a risk factor in its own right. It is particularly important to highlight it alongside default because the primary focus when estimating default probability is often on a firm in isolation (the marginal probability) rather than the dependence across firms (the joint probability).
The current stylized model highlights the simple concept behind credit risk models (more precisely static or discrete-time models). This particular model makes simple assumptions about defaults (the probability of default is the same for all loans and there is no correlation across loans), recovery (it is fixed at 50 percent), correlation (there is none), and exposure (the fixed loan amount at maturity, it does not vary with, say, interest rates or FX rates). This model would be perfectly suitable and realistic if these underlying assumptions were realistic. The problem is that the assumptions about the underlying defaults, recoveries, and exposures are not realistic for a real-life portfolio. These assumptions are what make the model unrealistic, not the model itself.
This model also highlights why real-world credit risk modeling is so difficult. The difficulty arises not so much from solving the credit model once risk factors are known, as from estimating the risk factors underlying the credit process itself. Each of the four factors must be parameterized and estimated:

1. Default probability (or, in a more general model, transitions between states) must be estimated for each and every firm. Almost by definition default is not observed historically for a solvent firm, so one cannot naively use history to estimate the probability of default; it must be built for each firm, appropriately accounting for each firm's particular circumstances, circumstances that will include elements such as current credit status, current debt level versus assets, future prospects for their region and business, and so on.
2. The correlation or dependence structure across firms must be estimated. Again, because default is rarely observed, one cannot naively use history to observe default correlation directly. Estimates of correlation must be built indirectly using models that make reasonable projections, accounting for firms' particular circumstance. Since correlation has a huge impact on the shape of the loss distribution, it is critically important, but such dependence is difficult to measure and estimate.
3. The exposure upon default must be calculated. This can sometimes, but not always, be done using market pricing models. It can be particularly difficult for derivative products such as interest rate swaps with which the exposure varies with market variables such as interest rates.4
4. The recovery upon default or loss given default must be projected. Recovery clearly has a big effect, as it can vary from 100 percent recovery, which implies that default has little monetary impact, to zero percent recovery. Recovery is very difficult to project, and actual recovery rates can be quite far from expectations.5

Estimating these is a major analytical, data collection, and data analysis project. With realistic assumptions for defaults, dependence, recovery, and so on, the stylized model discussed here would be very realistic. The difficulty is that arriving at such realistic assumptions is a complex undertaking.
Note that, just as for market risk, we can conceptually split the modeling into two components. The first is external: default and recovery. The second is internal: monetary exposure to the default. Jorion (2006, 247) puts the situation as follows when discussing market risk: "The potential for losses results from exposures to the [market] risk factors, as well as the distribution of these risk factors." In the present case, the analogue of market risk factors are defaults and recoveries or the drivers underlying them, while the monetary exposure at default is the exposure to those risk factors.
Models discussed further on, such as KMV, CreditMetrics, CreditRisk+, produce more realistic results for the probability of default of individual loans, the dependence structure (correlation across loans), and so on. In fact, a useful way to view such models is as methods for deriving realistic default probabilities, dependence structures, and so on, in the face of limited current and past information on counterparties and their credit status. Solving for the loss distribution, once the default probabilities and so on are known, is not conceptually difficult (although it will often be computationally demanding).
The fundamental problem in credit risk, highlighting the contrast with market risk, is that good history on defaults and recoveries is rudimentary, knowledge of current status is incomplete, and projections of future states (default probabilities, and so on) are very difficult. One must turn to modeling the underlying economic and financial drivers to try to derive realistic estimates. In practical applications, one must spend much time and effort on both making the assumptions reflect reality, and building and solving the model. The actual implementation of a credit model is very demanding, requiring substantial resources devoted to analytics, data, and programming:

 Analytics: Working through the analytics of building and solving the model.
 Data: Collecting and analyzing data to measure how risks differ across loans and counterparties, categorizing according to different default probabilities, measuring and modeling dependence structure, quantifying exposures, and estimating recoveries. Data can be split into external and internal data:

 External data

 Default probability of individual issuers—requires collecting large amounts of data on each individual risk to estimate likely default probability. Much of what KMV and CreditMetrics do is this, even before getting to the modeling.
 Dependence or correlation across defaults—critical to get right but intrinsically difficult because there are not much data (default is a rare event) and because dependence is certainly nonstationary, changing with the state of the economy and time in ways difficult to measure (again, because of data limitations).
 Recovery—this depends on many uncertainties, but even just getting the legal priorities right (in the event of default) is data-intensive.

 Internal data

 What are the exposures? This is not always a trivial exercise, partly because the data are often dispersed around the firm, and partly because credit exposure will include not just homogenous loans, but heterogeneous exposures such as loans, bonds, counterparty exposure on swaps, and settlement exposures, all across disparate units within the organization.


 Programming: Solution of credit models usually requires large-scale simulation, since analytic methods are not feasible. Getting this to work means substantial programming (separate from the systems work to manage data).

The stylized model I have introduced here provides a framework for understanding how credit models work, and a foil for illustrating how and why the concepts are simple, while realistic implementations are complex. Later sections will survey some of these more realistic models, but for now we return to this stylized model.
VaR and Economic Capital
We now return to using the distribution shown in Figure 11.3 to answer how much a firm might lose in adverse circumstances. Table 11.2 shows the cumulative probability, defaults, and income for part of the lower tail (distribution function rather than the density function displayed in Figure 11.3). The average income is $59,350. We can see from Table 11.2 that the 1%/99% VaR is a loss (compared to average) between $4,520 and $5,085, while the 0.1%/99.9% VaR is a loss between $6,215 and $6,780.6
Table 11.2 Statistics for Income Distribution for Portfolio of 1,000 Homogeneous Loans.

Marrison (2002, 229) has a succinct description of what the CFO of a bank might require from a credit risk modeling exercise such as we have conducted:

 Provisions—Amounts set to cover the expected losses over a period—this would be the expected losses.
 Reserves—Amount set to cover losses for an unusually bad year—may be set at the 5 percent quantile (5%/95% VaR) of the distribution.
 Economic Capital—Loss level for an extraordinarily bad year—may be set at the 0.1 percent quantile (0.1%/99.9% VaR) of the distribution.

The expected income is $59,350, as calculated earlier. We might want to set reserves, an amount in case defaults are higher than expected, at the 5%/95% VaR level, between $2,825 and $3,390. We might set capital at $6,500, roughly the 0.1%/99.9% VaR.
Setting economic capital is a difficult problem. Economic capital is distinguished from regulatory capital because it is set in response to economic circumstances rather than regulatory or accounting rules. Economic capital supports a firm's risk-taking activities, providing the buffer against losses that would otherwise push the firm into bankruptcy. McNeil, Frey, and Embrechts (2005, section 1.4.3) lays out the following process for determining economic capital:

 First determine a "value distribution," which is the result of quantifying all the risks faced by the firm, including but not limited to market, credit, operational risk. (For the current simple model, if we assume that the portfolio of 1,000 loans is the total of the firm's business, the P&L distribution shown in Figure 11.3 and Table 11.2 is this value distribution.)
 Second, determine an acceptable probability of default (solvency standard) appropriate for the institution and horizon. A useful basis is company ratings and associated default rates. For example, a firm might target a Moody's Aa rating. Historical analysis of Moody's-rated Aa institutions shows a one-year default frequency of 0.03 percent.7 The firm would want a level of capital high enough so that losses would be worse (implying bankruptcy) only with probability 0.03 percent.
 Finally, calculate economic capital as the appropriate quantile (buffer needed to ensure bankruptcy with probability chosen in the second step). For a 0.03 percent probability of bankruptcy, that would be the Z = 0.03 percent/99.97 percent quantile. (For the current simple loan portfolio example, it would be roughly $7,300.)

Although the conceptual process for calculating economic capital is straightforward, the practical issues are challenging.
Dependence, Correlation, Asymmetry, and Skewness
The stylized model of this section has intentionally been kept simple but it is important to extend it in one particular direction, the dependence across loans. As noted earlier, asymmetry or skewness is an important characteristic of credit risk and dependence across loans is a major reason for asymmetry.
The model as formulated so far produces a symmetric default distribution and loss distribution—virtually no asymmetry. This occurs for even moderate numbers of loans and the reason is easy to see: loans are assumed independent, the default distribution is binomial, and the binomial distribution tends to the normal for large n.
It is easy to produce asymmetry, however, by the very natural mechanism of dependence across defaults. That is, probability of default for a given loan is higher when other loans or firms also default. The phenomenon of firms defaulting together is both easy to understand and often observed. Probability of default may go up and down for one of two reasons: common factors to which firms respond, or contagion. Common factors would be something to which all firms respond, such as an economic recession that makes default more likely for all firms. Contagion would be something that alters perceptions or behavior following an initial default, such as heightened investor scrutiny of corporate accounts following Enron's collapse—which might lead to the uncovering of malfeasance (and default) at other firms.
We will concentrate for now on common factors. Dependence here arises because probability of default changes systematically for all firms. In the simplest example, there might be two states of the world: low probability and high probability of default. We might be measuring probability of default in a particular year. The low probability regime corresponds to a year when the economy is growing. The high probability regime corresponds to a year when the economy is in recession.
Dependence across firms arises not because one default causes another, but because when we look at the future we do not know whether the next year will be a low or high probability year. If, however, a default were to occur, then it is more likely that it is a high default regime and thus more likely there will be other defaults. When we are in a low default or high default regime, there is no correlation across defaults, but today we do not know if next year will be low or high. This means the unconditional distribution (sitting here today and not knowing whether next year will be a low or high default regime) defaults next year look correlated.
This correlation or dependence across defaults will generate skewness as defaults cluster together. There will be relatively few defaults most of the time, but default probabilities are periodically higher and will create an episode with many defaults. Defaults may not happen often but when they do there will be many—the upper tail of the default distribution (the lower tail of the P&L distribution) will be fat with defaults.
To fix ideas, let us look at a simple example. Consider a two-state world. There is a low-default regime in which the default probability is 0.07353, and a high-default regime in which the default probability is 0.025. Firm defaults are independent in each regime so that in each regime the distribution will be binomial (symmetric). In each of these two regimes, the income for our stylized portfolio of 1,000 loans will be as shown in Panel A of Figure 11.4.

Figure 11.4 Mixing Distributions: Income Distributions for Low- and High-Default Regimes (Panel A) and Mixture (Panel B)
Notes: Panel A shows the distribution for the one-year income from holding a portfolio of 1,000 homogeneous loans, each with average probability of default of 0.02 (high-default regime) and 0.008235 (low-default regime). Loss given default is 50 percent, promised interest is 6.5 percent. Panel B shows the income distribution for a mixture that is 15 percent high-default and 85 percent low-default. Reproduced from Figure 5.18 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

We now consider the overall situation, which is a mixture of the two regimes. We will assume that at any point in time there is an 85 percent probability we are in the low-default regime, and a 15 percent probability that we are in the high-default regime. At a particular time we are in one or the other, but we don't know which beforehand. With this set-up, the overall average default probability is 0.01, just as it was originally. But now we have correlation across firm defaults, correlation of 0.004. If one particular firm defaults, it is more likely we are in the high-default regime, and thus more likely that other firms will also default—not because of the default of the first firm but simply because defaults for all firms are likely to be higher.
The overall distribution of income will be a mixture of the two distributions for the individual regimes. This is shown in Panel B of Figure 11.4 and we see that it is, naturally, skewed or asymmetric, with a fat lower tail. The asymmetry arises because the overall distribution is composed of a large part of the high-income (low default) distribution and a smaller part of the low-income distribution, and the low-income distribution skews the lower tail of the overall distribution.
The mixing of good (low default) and bad (high default) worlds naturally produces correlation across defaults and skewed distributions; the correlation and skewness go hand-in-hand. In either the good or the bad world, defaults will tend to be symmetric. But at some times we are in the high default world and will thus have many defaults, while at other times we are in the low default world and will have few defaults. The larger number of defaults during bad times produces both the skewness, or fat upper tail of the default distribution (fat lower tail of the income distribution), and the correlation (since defaults tend to happen together).
This simple model also helps to explain why credit losses can be so devastating to a firm. The simple story that credit losses are asymmetric because there are "many small gains and a few large losses" is manifestly not true—we saw earlier that the default and P&L distribution quickly becomes symmetric for defaults independent across firms. In a more subtle form, however, the story contains some truth. When unusually bad events occur, they involve, naturally, an unusually large number of defaults. These defaults mean large losses. Correlation across defaults means that when things go bad, they go really bad—in a bad state of the world, losses look like the left line in Panel A of Figure 11.4 where the world is really bad.
Mixing is a natural mechanism in credit risk to produce default correlation. It also helps to explain why credit risk can be difficult to model and manage. When things go bad, they can go very bad. If the distributions in Figure 11.4 are for the P&L over a year, then most years will be pretty good—the low default regime shown in Panel A. Periodically, however, things will go bad and losses will look like the high-default regime in Panel A.
The default correlation for this example is only 0.004 but produces substantial skewness. If we compare with Figure 11.3, where the overall probability of default is also 0.01 but there is no default correlation and little skewness, the 1%/99% VaR for the losses is between 17 and 18 defaults, or roughly $54,600 ($4,750 below the mean). For Figure 11.4, the 1%/99% VaR is between 33 and 35 defaults, or roughly $46,000 ($13,350 below the mean). Even the low default correlation of 0.004 produces substantial skewness. There is only a small chance of the bad world (many defaults), but when it occurs, it produces substantially lower income, and it is exactly the low-probability left end of the tail that determines the VaR. It requires only a tiny default correlation to produce substantial skewness or asymmetry.
This model of mixing just a good and a bad state is clearly too simplistic, but it does illustrate two points. First, how correlation can be produced not because firms depend on each other, but because all firms respond to the same underlying factors (in this case, either high or low defaults). Second, that it takes only a very low level of correlation to produce a substantial degree of skewness or asymmetry in the default and loss distribution. (This differs from market price loss distributions, where we usually develop our intuition, and where small changes in correlation do not dramatically change the shape of the distribution.)
A more realistic model for the dependence structure, a simplified variation of that used by credit models such as KMV, is the threshold model and factor structure. Default is assumed to occur when some random critical variable Xi falls below a critical threshold di:

Each loan or firm may have its own critical variable Xi and its own critical threshold di. For the simple homogeneous model considered here, where all loans are identical, all the di will be the same and all the Xi will have the same distribution.
Credit models of this form (based on the Merton [1974] approach and discussed more further on) build the relationship between the critical variable and the threshold from realistic economic and financial relationships, based on historical data and company analysis. For example, the Xi might be the value of the firm (which may go up or down randomly) and di the notional value of loans the firm has taken out. The firm goes into default when the value of the firm falls below the value of the loans, so default occurs when Xi < di. The important point for now, however, is simply that there is some reasonable story that justifies the relationship between the random variable Xi and the fixed threshold di, where default occurs when Xi < di.
If we assume that Xi is normally distributed with zero mean and unit variance, then the probability of default is:
(11.1) 
If all the Xi are independent, then the probabilities of default are independent and the model is exactly what we have discussed so far. It is, however, easy to build dependence across defaults by introducing correlation across the critical variables {Xi} for different loans or firms. Take the extreme, where X1 and X2 are perfectly correlated. Then firms 1 and 2 will always default together (or not default together). In the general case, X1 and X2 will be less than perfectly correlated and the default correlation will be determined by the correlation of the Xi. The higher the correlation across the Xi, the higher the correlation in defaults.
We can calculate the default correlation, given the individual firm default probabilities and the critical variable correlation. Define Yi as the firm-i default indicator (that is, Yi = 0 when no default, Yi = 1 for default, see McNeil, Frey, and Embrechts, 2005, 344), then:



(11.2) 
The term E[YiYj] is the joint probability that both firms default. In the critical variable framework, this translates into a statement about the joint distribution of Xi and Xj. Since we commonly assume that Xi and Xj are jointly normal, this will be a probability statement about a bivariate standard normal with a given correlation.
Say we have a default probability . This implies di = −2.3263 (Xi is a standard normal variable and the probability that a normal variable is below −2.3263 is 0.01). Say the correlation across the critical variables Xi and Xj is ρ = 0.05. Then E[Yi·Yj] = P[Xi < di & Xj < dj] = 0.0001406 (the joint probability that two standard normals with correlation 0.05 will both be below −2.3263 is 0.001406—see, for example, the approximation in Hull 1993, appendix 10B, or the functions built into Wolfram's Mathematica).8 Inserting these values into the expression 11.2 gives:

The critical variable correlation and the default correlation are both low, but these are typical values for observed credit defaults. McNeil, Embrechts, and Frey (2005, table 8.8) provide estimates of pairwise correlations from default data for 1981 to 2000. They find one-year default probability for BB-rated issuers of 0.0097 and pairwise default correlation of 0.0044.9
There are many ways to introduce such correlation but, as argued earlier, mixing through a so-called common factor model is a particularly useful form. We split the critical variable into two components, a common and an idiosyncratic factor:

with

F = common factor, random variable  N(0, 1)
εi = firm-specific independent idiosyncratic variable, N(0, 1)
B, c = coefficients chosen to ensure Xi remains N(0, 1)

The common factor F represents elements that affect all firms together. These might be common economic factors such as economic growth or the level of interest rates, or common industry conditions such as when airline companies are financially pressured as the relative price of energy rises. The firm-specific variable εi represents factors that are specific to the individual firm and are independent across firms.
A common factor structure such as 11.3 is often used in practice, and it represents a reasonable and practical representation for how firms' defaults might move together. It says that conditional on the state of the variable F (the state of the economy or industry) firms are independent, but that firms default together because they are all affected by the common factor.10 The default correlation is induced through the common factor F, but conditional on a particular value of F firms are independent.
In summary:

 The common factor (F) is the same for each firm.
 The firm-specific component (εi) affects only the particular firm i and is completely independent of any other firm.
 The correlation in the critical variable is controlled by the relative importance of the common factor (F) and the firm-specific component (εi).

The simplest form for a common factor model, where all loans or firms are assumed identical, is the equicorrelation factor structure:
(11.4) 
with

F = common factor, random variable N(0, 1)
εi = firm-specific independent idiosyncratic variable, N(0, 1)
ρ = proportion of variance attributable to common factor. This will also be the correlation in the critical variable across firms.

The ρ term is the link between the common and the idiosyncratic components, determining how much of the overall variance for the critical variable Xi is due to the common factor versus the idiosyncratic factor.
Even a low value for ρ can give quite substantial asymmetry. Figure 11.5 shows the default and loss distribution for the model with ρ = 0 and ρ = 0.05 (producing default correlation 0.004). The ρ = 0 case is what we have discussed so far: no dependence across loans and a binomial distribution for defaults, resulting in a symmetric distribution for defaults and P&L. The ρ = 0.05 represents dependence across loans. The dependence is not high but it still produces substantial skewness.11 The skewness is obvious in Figure 11.5. We can also measure it by calculating the VaR. For the ρ = 0 case, the 1%/99% VaR is between 18 and 19 defaults (out of 1,000 loans) and a P&L between $4,520 and $5,085 below the mean. For ρ = 0.05, the 1%/99% VaR is dramatically higher: defaults between 34 and 35 and P&L between $13,560 and $14,125 below the mean.

Figure 11.5 Number of Defaults for a Portfolio of 1,000 Homogeneous Loans—Alternate Dependence Assumptions
Note: Panel A is the number of defaults from holding a portfolio of 1,000 homogeneous loans, each with average probability of default of 0.01. The probability of default is as in (A) with the common factor structure as in (B). The Independent case has the threshold correlation ρ = 0, the Dependent case has critical variable correlation ρ = 0.05, and default correlation 0.004. Panel B is the one-year income from holding such a portfolio with loss given default at 50 percent, and a promised interest rate of 6.5 percent. Reproduced from Figure 5.19 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

The example so far has assumed that all loans are identical, so that the default probability is equal across loans and the distribution (for no correlation) is binomial. Introducing heterogeneity in the loans (while maintaining independence) does not change anything significantly. When independent, the default distribution tends toward symmetry, while correlation breaks the symmetry and produces a skewed distribution.
As discussed in Section 11.6, the threshold framework discussed here can be translated to a Bernoulli mixture framework. In a Bernoulli mixture framework, the default process is conditionally independent, with correlation structured as mixing across independent realizations. This helps clarify the mechanism that produces the skewed distribution seen in Figure 11.5: Each realization (conditional on a value of the common factors F) is binomial, or more generally, Bernoulli, and so will tend to be symmetric for a large number of loans. The correlation introduced by the factors F means that some realizations (states of the world) will have high default rates, producing many defaults, while other states of the world will have few defaults. The tendency for firms to default or not together produces the fat upper tail of the default distribution (fat lower tail of the income distribution).
In this example, the correlation across the threshold variables is 0.05 but as pointed out earlier, for pi = pj = 0.01, this translates into a correlation across defaults of only 0.0040. Defaults are rare, so default correlations are by their nature low. This highlights one of the difficulties of credit risk modeling: default correlations are small and can be difficult to measure in practice, but the distribution of defaults will be substantially affected by correlation. In Figure 11.5, the distribution for 1,000 loans with correlated defaults is substantially more fat-tailed than the independent distribution. The correlation across defaults is tiny, however, only 0.004.
One final note before closing the discussion on distributions and asymmetry: The asymmetry in the default distribution arises naturally out of the dependence and mixing of Bernoulli (default) processes, even with the underlying threshold variables and mixing distributions being normal and completely symmetric. For credit risk, the fat tails and asymmetry arise almost of necessity. Furthermore, there does not seem to be any substantial impact from using distributions that are themselves fat-tailed.12
Diversification and Correlation for Credit Risk
The analysis of dependence and credit risk is challenging. Much of our everyday experience and understanding of correlation and diversification comes from the arena of market risk, as applied to correlation across prices and returns. This experience and knowledge does not always carry over well to defaults and credit risk. Default is a rare event. Correlation across defaults will usually be close to zero, but even low correlations can have quite a dramatic impact. The degree of skewness and the sensitivity of the distribution to small changes in the degree of correlation provide yet another example of why credit risk modeling is so difficult: measuring the degree of dependence in the real world is difficult, but dependence makes such a large difference in the shape of the distribution that it is especially important to measure precisely.
There are two underlying reasons intuition developed around diversification of stocks does not carry over well to diversification of defaults and credit risks:

1. Correlations for credit defaults are very low (close to zero) and small changes in correlation have a large impact on the degree of diversification.
2. Correlation fundamentally changes the shape of the returns distribution, making it skewed with a fat lower tail.

There are two rules of thumb regarding portfolio diversification that we learn from traded stocks: first, that a moderate number of assets (on the order of 10 to 30) is sufficient to reap most diversification benefits; and second, that small differences in correlation do not have a large impact on the benefits of diversification. These rules of thumb are valid for moderate correlations between assets (say ρ ≈ > 0.3) but do not hold when correlation is close to zero, as it invariably is for credit portfolios. Default is a rare event, and so correlations between defaults will be low; for single-A credits, correlation might be 0.001 or less, while for single-B credits, it might be on the order of 0.015. Diversification for credit risks requires a large number of assets, and small differences in correlation can have a dramatic impact on risk, particularly in the tails.
The portfolio and diversification effect of low correlation does not depend on discrete losses and can be easily demonstrated for a portfolio of assets with continuous returns. Consider an equally weighted portfolio of n identical assets with continuous, normally distributed returns. For each asset, the mean return is μ, the standard deviation σ, and the pairwise correlation ρ. The average return for an equally weighted portfolio will be μ and the standard deviation σ√[ρ + (1 − ρ)/n]. The overall portfolio return will also be normal.
For a large portfolio, that is, as n → ∞, the portfolio standard deviation will go to σ which we can call the systematic, or nondiversifiable, component of the portfolio volatility. For a portfolio of size n, the residual or diversifiable component of the volatility will be σ.
Figure 11.6 shows the effect of correlation and portfolio size on diversification by way of two examples: correlation ρ = 0.3, which might represent market assets, and ρ = 0.01, which might represent credit default correlations (for something between BB and single-B corporate bonds). For the ρ = 0.3, the portfolio standard deviation falls quickly as n increases and most of the diversification effect is apparent with only 20 assets—the residual is only 6 percent of the systematic standard deviation. For the lower correlation, the systematic component is far lower but the residual component falls off much more slowly—for 20 assets, the residual is still 144 percent of the systematic component.13

Figure 11.6 Diversification Effect (Relative Standard Deviation) for an Equally Weighted Portfolio of Identical Assets
Note: This shows the standard deviation as a function of the number of assets in a portfolio of identical assets on a standard scale.

Low correlation also has the effect of making the systematic portfolio volatility very sensitive to small changes in the level of correlation. Consider a portfolio of assets with correlation ρ = 0.3. A single asset has volatility σ, while a fully diversified portfolio has systematic volatility 0.55·σ. Correlations may be hard to estimate, so possibly the true correlation is ρ = 0.35 instead of 0.3, which gives a systematic volatility of 0.63·σ, or 15 percent higher.
Now consider a credit portfolio with default correlation ρ = 0.001 (roughly that of BBB credits), where a fully diversified portfolio has systematic volatility of only 0.032·σ. This is a huge diversification benefit, as it should be, since joint defaults will be rare for low correlation. But default is a rare event and default correlations difficult to estimate, meaning they will have a large standard error. Say the true correlation were in fact ρ = 0.002—a difference of only 0.001. In this case, a fully diversified portfolio would have systematic volatility of 0.045·σ, 41 percent higher. A very small error in estimating the correlation has a very large impact on the diversified portfolio's volatility.
Figure 11.7 shows the impact of changes in correlation graphically. Panel A shows the normal distribution for correlation ρ = 0.3 and for 0.1 higher (ρ = 0.4). The distributions are not very different. Panel B shows the normal distribution for ρ = 0.012 and for 0.1 higher (ρ = 0.112). The distributions are quite different.

Figure 11.7 Impact of a 0.1 Change in Correlation for High and Low Correlation
Note: This shows the one-year income for equally weighted portfolios of 1,000 assets. All assets have the same mean return (3.675 percent) and standard deviation (11.9 percent). Returns are assumed log-normally distributed with return for asset i as Ri = F√ρ + εi√(1 − ρ) with F a common factor and εi the idiosyncratic component (independent of other ε and F). Normalizing the variance of F and ε to both be σ (the variance of an individual asset, 11.9 percent here) the parameter ρ is the correlation across assets.

Credit default correlations tend to be quite low. McNeil, Embrechts, and Frey (2005, table 8.8) provide estimates of pairwise correlations from one-year default data for 1981 to 2000. They find a pairwise default correlation for BBB-rated issuers of 0.00149 and for B-rated issuers of 0.01328. As a result, credit portfolios will gain diversification benefits from large portfolio sizes, larger than one would expect based on experience from portfolios of standard market-traded assets. Note that, as is apparent from Figure 11.7, gains from diversification for a credit portfolio with low correlation can be quite substantial.
Low correlation for credit portfolios also means that the P&L distribution will be sensitive to small changes in correlation. A change of 0.1 will not have a huge effect when the correlation is 0.3 (as it might be for market-traded assets) but will have a large impact when the correlation is 0.01 (as it will be for default correlations).
The effects of diversification and correlation that we have been considering are a result of the correlation being close to zero and have nothing to do with the all-or-nothing aspect of defaults. The effect is the same for a portfolio of continuous market-type assets and for a portfolio of defaultable assets.
The important distinction between continuous and all-or-nothing defaultable assets arises in how the distribution's shape changes with correlation. Market-traded assets are usually normal or close to normal and changing the correlation will change the spread of the portfolio return (the standard deviation) but not the shape. For credit risk, in contrast, introducing the correlation across credit risks induces asymmetry and a substantially fat upper tail to the default distribution (fat lower tail in the P&L distribution).
Figure 11.8 compares a normal distribution with a simulated default distribution. Both examples are a portfolio with 1,000 identical assets with an expected return of 3.675 percent and standard deviation 11.9 percent. For Panel A, each asset has a continuous (normal) return with an expected return of 3.675 percent, and the overall portfolio has a normal return. Panel A shows both independent assets and assets with correlation ρ = 0.012; in both cases, the distribution is normal. For Panel B, in contrast, each asset either does not default (probability 99 percent, return 6.5 percent) or does default (probability 1 percent, return −50 percent). The solid line shows the portfolio P&L for independent assets, which produces a distribution very much like the market assets—hardly surprising, since the independent case is binomial, which tends to normal for large n. The dashed line is a portfolio with default correlation 0.012. The distribution has substantial skew. The standard deviation is close to that for market-traded assets, but the distribution itself has a fat lower tail and is decidedly not normal.

Figure 11.8 Income for a Portfolio of Market Assets (Panel A) versus Loans or Credit Assets (Panel B)—Showing How Correlation for Credit Assets Produces a Skewed Distribution Note: Panel A is the one-year income for an equally weighted portfolio of 1,000 market assets. All assets have the same mean return (3.675 percent) and standard deviation (11.9 percent). Returns are assumed log-normally distributed, with return for asset i as Ri = F√ρ + εi√(1 − ρ) with F a common factor and εi the idiosyncratic component (independent of other ε and F). Normalizing the variance of F and ε both to σ (the variance of an individual asset, 11.9 percent here), the parameter ρ is the correlation across assets. The variance of an equally weighted portfolio of n assets is [ρ + (1 − ρ)/n]σ2. Panel B is the one-year income from holding a portfolio of 1,000 homogeneous loans, each with average probability of default of 0.05. The probability of default is as in Figure 11.1, with the common factor structure as in Figure 11.3. The Independent case has zero correlation across threshold variables (ρ = 0), and the Dependent case has ρ = 0.05, leading to a default correlation of 0.012.

Default Process versus Default Parameters
The default and dependence parameters are the most important aspect of a credit model. The detailed mechanism generating defaults (conditional on the average probability of default and dependence across defaults) is less important than the probability and dependence. The next section will discuss various credit default models. There is a wide variety of models with many important differences. The most important differences arise in how they estimate the probability of default for individual firms and dependence across firms. When these are roughly the same, the results are also roughly similar.14
11.4 Taxonomy of Credit Risk Models
Two Areas of Modeling Credit Risk
There are two areas of application and two ways in which "credit risk modeling" is used. The first is credit risk management—measuring and using the P&L distribution for a portfolio or business activity over some (usually long) period. Such credit risk modeling is the primary focus of this chapter, and models are usually static in the sense that they focus on the distribution for a fixed time, and are concerned less with the time process of default and loss or the details of when default occurs.
The second application of credit risk modeling is the pricing of credit-risky securities, whether new developments such as credit default swaps or traditional securities such as corporate bonds. This is a large area, addressing how to price instruments such as bonds, loans, CDS, or other credit derivatives. Models for pricing such instruments are usually dynamic in the sense of modeling the time that default or other loss occurs—that is, modeling the stochastic process of losses. Such models are not the primary focus of this chapter.
Recognizing the two types of modeling and the distinction between them is useful for two reasons. First, the techniques used in pricing "credit-risky" securities are often related to those used in credit risk measurement. Second, understanding the distinction between pricing "credit-risky" securities and credit risk measurement clarifies the types of models and approaches used.
This chapter focuses mainly on credit risk management and the models used for credit risks that are not market-traded. Risk management for securities that are market-traded can often dispense with complicated modeling because market risk factors, and the distribution of those factors, are available from the market.15
Basic Modeling Taxonomy
Credit models can usefully be classified according to two separate criteria: on the one hand, whether they are static (fixed, discrete, time period) versus dynamic; and on the other hand, whether they are structural versus reduced form.16
Static models are more often applied to credit risk management. This aspect of credit risk modeling is focused on determining the P&L distribution for a portfolio of risks over a fixed, usually long, time period. The P&L distribution in turn is used to compute risk measures such as VaR or economic capital, and to make risk allocation and other management decisions. The primary question is the distribution of defaults or other credit events over a fixed period, and modeling is static in that the focus is on the probability of default or change in credit status during the period, with the timing of default being decidedly secondary. These models usually work with the physical probability measure, in a sense to be discussed more fully further on.
Dynamic models are usually applied to the pricing of credit-risky securities, where the focus is primarily on the stochastic evolution of risk and default probability. The exact timing of default (or other credit events) matters and so must be modeled explicitly. Such models are usually formulated in continuous time, usually work under the equivalent martingale or risk-neutral measure, and are usually calibrated directly to market observations.
The structural versus reduced form categorization applies to both static and dynamic models, in that both static (fixed period) and dynamic (continuous time) models may be formulated as either structural or reduced form. Structural models, which might also be called firm value models, detail the specific financial and economic determinants of default, usually considering assets versus liabilities and the event of bankruptcy. Such models provide both qualitative and quantitative underpinnings for the likelihood of default. Reduced form models, in contrast, do not describe the precise determinants of default but rather model the default time directly as a function of economic or financial variables.
Structural models trace back ultimately to Merton (1974), which considers default in terms of a firm's assets relative to liabilities at the end of a fixed time period. Assets are treated as a random variable, and default occurs when (random) assets are below liabilities at the end of the period. That is, default occurs when the random variable (assets) cross a threshold (liabilities). Structural models can be more generally termed threshold models, since default occurs when a stochastic variable (or stochastic process in dynamic models) crosses a threshold. (See McNeil, Frey, and Embrechts 2005, 328)
Table 11.3 shows a basic taxonomy for credit models.
Table 11.3 Taxonomy for Credit Models.




Structural or firm-value focus on mechanism of firm default—usually relation between firm-level assets and liabilities
Reduced form precise mechanism generating default not specified—default time modeled as random variable




Static (discrete/fixed time-period)Application: primarily credit risk management—measuring the P&L distribution over a fixed periodModeling methodology: usually physical probability measure with risk premium
Paradigm is Merton (1974).Threshold models such as KMVCredit migration such as CreditMetrics
CreditRisk+CreditPortfolioView


Dynamic (continuous time)Application: primarily pricing of credit risky securitiesModeling methodology: usually risk-neutral probability measure calibrated to market observations
Dynamic structural models are not widely used
Single instruments (for example, bond, loan, CDS)—modeling using default time and hazard rates (intensities)Portfolio instruments (for example, CDO, basket swaps)—default times and hazard rates (intensities) modeled; various assumptions concerning dependence or independence of default times and hazard rates



11.5 Static Structural Models
Static structural models trace back to Merton (1974).17 Merton observed that a risky bond from a company that issues both debt and equity is equivalent to a risk-free bond plus a put option on the company's assets. This may seem simple with the benefit of hindsight, but it is a profound insight that provides a powerful approach for thinking about determinants of default.
Merton's Model
Take a very simple one-year framework in which a firm has assets, V, which are random (will go up and down in value over the next year), and the company issues equity, S, and a one-year bond with a promise to pay a fixed amount, B. Default is very simple: When the value of the assets are below the bond payment (V < B), the bond is in default. Otherwise the bond is paid and shareholders receive the excess. From this, we can see that the equity is a call option on the value of the assets, with a strike equal to the promised bond payment, B.
Setting up notation:

Firm value (assets): V0,: value at beginning, end of period.  is the driving variable in this model, assumed to be random, generally log-normal.
Bond: B0, , B: bond value at beginning, end of period.  is random (since bond may default), B is promised fixed payment.
Equity (shares): S0,: share value at beginning, end of period.  is random.
Relation between bond, equity, firm value: V0 = B0 + S0.

Shareholders have the right to walk away, so . In other words, the equity price at T has the payout of a call option on the firm's assets:
(11.5) 
and the equity value at the beginning can be evaluated as a standard call option.
The bond at T will either be paid or default, depending on whether assets are greater or less than the promised payment B. If paid, the value is B, if defaulting, the value is . This can be written as:
(11.6) 
which is the value of a fixed payment B less the payout of a put. This means that the risky bond at the beginning is equivalent to a risk-free bond (the discounted value of the fixed payment B) less a put option. Figure 11.9 shows the payout for the bond and the stock at the end of the one-year period.

Figure 11.9 Payout Diagram for Bond and Stock at End of One-Year Period for Simple Merton Model

The wonderful thing about this framework is that it provides the two most important pieces of information regarding credit risk: the probability of default and the loss given default. The probability of default is just the probability that the asset value is below the promised payment B:

Assuming that the asset value  is log-normally distributed, so that
(11.7a) 
then the default probability is
(11.7b)
This is a reasonable (although simplified) framework that provides an estimate of default probability based on company characteristics that are or might be observable:

 Promised payment on the bond, B
 Current value of assets, V0
 Volatility of the company assets, σ
 Average growth rate of company assets, μ
 Time to maturity of the bond, T

This is now a pretty complete description (although in a very simplified framework) of the eventual payment for this bond: the probability of default is given by equation (11.7a) and the actual amount paid is given by equation (11.6), either B if no default or  if default. This provides exactly what is necessary to start making the model of Section 11.3 more realistic—the probability of default and the loss given default (or recovery, if the loan goes into default). In particular, it provides a reasonable estimate for the probability of default based on variables we might be able to measure. It thus addresses the biggest challenge in credit risk modeling: how to estimate characteristics such as probability of default when default itself is rare and direct information on default is not available. equation (11.7a) is essentially a way to arrive at estimates of default probability based on other, observable, characteristics of the firm.
The structure of the Merton model can be exploited further to price the equity and bond, and derive expressions for credit spreads. The price of the equity and risky bond can be obtained from equations (11.5) and (11.6) (the terminal conditions) together with assumptions sufficient to apply Black-Scholes-type option pricing formulae. The values today will be the expected discounted value under a risk-neutral measure, assuming that the asset value  is log-normally distributed but with mean return equal to the (deterministic) risk-free rate r,18 so that:

At the moment, however, I will not pursue such pricing further (I will return to the issue in Section 11.8, and good presentations can be found in the references at the beginning of this section). The important point for current purposes is that equation (11.7a) provides probability of default (and equation (11.6) loss upon default), based on economic and financial fundamentals. For credit risk management, the P&L distribution over a period of time is the primary focus and the probability of default is the key item.
In equation (11.7a), default occurs when the random assets  (critical variable) cross the critical threshold B. This is the same form as the threshold structure introduced in equation (11.1) in Section 11.3, but the important point is that there are now economic and financial foundations to both the critical variable and critical value.
The form of the threshold structure can be outlined diagrammatically, shown in Figure 11.10. Default is the event in the lower tail of the distribution when the firm assets fall below the promised bond payment, B.

Figure 11.10 Distribution of Firm Assets at Bond Maturity, Showing Default

Equation (11.7a) and Figure 11.10 show that the important variables determining default are:

 Current asset value V0.
 The distribution of the asset value  at time T, parametrized by the mean and volatility, μ and σ.
 Promised debt repayment or the book value of liabilities, B.
 The time horizon, T.

From Figure 11.10, one can easily see how the probability of default will change. For example, it will increase as the volatility goes up (the distribution becomes more spread out with more probability mass below B), and decrease as V0 increases (the default point B becomes further away from the starting point V0 and mean ).
Moody's KMV (MKMV)
The Merton model as presented here provides a framework for thinking about default probability, but it is not realistic enough for practical application. The Moody's KMV product is a commercial model descended from Merton and was developed in the 1980s and 1990s.19 One of MKMV's most important contributions is in the collection and analysis of a huge proprietary database of public and private company default and loss data. The data are used for development, testing, and implementation of a practical and realistic model that follows the ideas outlined earlier.
The discussion here is brief, focusing particularly on three aspects of MKMV's implementation:

1. Estimation of firm assets from observable equity prices.
2. Alternate default probability function, instead of (11.7a).
3. Realistic factor structure to capture dependence across firms, broadening the simple equicorrelation factor structure in equation (11.4)).

These are three key components in MKMV's transformation of the theoretical ideas outlined in the preceding section into a functional product. My discussion follows the MKMV working paper by Crosbie and Bohn (2005) and the detailed discussion in Crouhy, Galai, and Mark (2000, ch. 9 section 5), also with reference to McNeil, Frey, and Embrechts (2005, section 8.2.3). I do not set out to present a comprehensive review of MKMV's implementation; more detail can be found in these citations.
Unobservable Assets
The probability of default is given by equation (11.7) and depends on the firm's assets: the current level of overall assets (V0), the average growth rate (μ), the volatility (σ), and how much of assets are promised as debt (B). equation (11.7a) is reproduced here as (11.7′). We will see shortly that MKMV actually uses a somewhat different function than equation (11.7′), but the idea is the same, and default still depends on assets:20
(11.7′)
Equation (11.7′) raises a problem because a firm's overall level of assets and the volatility of assets are generally not observed, and therefore (11.7′) cannot be used directly.
Assets consist of the market value of equity plus debt (V0 = B0 + S0 from Section 11.5). Market value of equity (S0) is usually available. The book value of debt (promised amounts, B earlier) is also usually available, but the market value (B0) rarely is. Debt is usually composed of bonds, bank loans, and other liabilities such as accounts receivables. Although some bonds may be traded, most debt is not, and because different classes of debt all have different seniority and payment provisions, it is generally not possible to obtain market values for all the debt.
MKMV overcomes the problem by a rather neat trick. equation (11.5) shows that the terminal stock price is the payout for a call option, and so today's price will be the current value of such a call. Continuing in the Merton framework:
(11.8) 

Both V0 and σ are unknown, but S0 is known. If we knew σ, then we could use (6) to back out V0, or if we knew V0, we could back out σ. Let us assume that σ is stable over time, pick some initial guess σ0, and apply equation (11.8) over a period of history to back out a time-series of asset values . From this time series we can calculate a new guess for volatility, σ1, and then calculate a new time-series of asset values. This apparently leads to a stable pair }. Using the σ* we obtain, together with today's S0, gives us V0 and the probability of default from equation (11.7′).
Default Probability Function (Expected Default Frequency)
Default occurs in the Merton model when the random level of assets  is below the default threshold B at maturity T, as shown in (11.7a) and Figure 11.10. The expression
(11.9) 
that forms the argument of (11.7a) provides a concise description of what determines the likelihood of default.
MKMV does not use equation (11.9) exactly, but rather defines a variable called Distance to Default:
(11.10z)
Expressions (11.9) and (11.10) are actually closely related. Consider (11.9) for T = 1: (11.10) is an approximation to (11.9) for T = 1, since μ and σ2 are small and ln V0 − ln B ≈ (V0 − B)/V0.21 Both measure the distance (measured roughly as a percentage) between the current asset level (V0) and a default threshold (B), scaled by the asset volatility. The distance to default DD is often spoken of as the "number of standard deviations away from the default threshold B."
The exact form of the default relationship (11.7a) in the Merton framework may be too simplistic for a working default model. MKMV has studied many firms and observed that default occurred when the asset value was less than the total liabilities, but more than short-term debt. In other words, default does not occur exactly when assets fall below total liabilities. To account for this, MKMV has made two adjustments. The first is to take the default threshold B as a combination of short-term and long-term debt; roughly debt to be serviced over the chosen horizon, plus one-half long-term debt.
The second, and more complex, adjustment is to dispense with the normal distribution function Φ() as the function relating the distance to default (Expression (11.10) or (11.9)) to the probability of default. MKMV then builds a function

based on an empirical analysis of a large number of companies and many events of default. The relationship between default probability and distance to default appears to be stable across industry, time, and size of company; that is, the distance to default is the important variable and differences in distance to default across industry, and so on, account for the observed differences in frequency of default. The function f[·] will obviously share important characteristics with the normal distribution function Φ[·]: In particular, it will be increasing in the argument, and it will lie between zero and one.
Factor Structure and Dependence across Firms
The focus so far has been on determining the probability of default for a single firm in isolation. This is no small undertaking, and has addressed one major issue regarding the stylized model of Section 11.3: estimating the probability of default for each firm, based on specific information pertaining to that particular firm. But the dependence structure across firms is critical for understanding the P&L distribution for a portfolio, as argued in Section 11.3.
Extending the Merton model to two (or more) firms is straightforward—each firm has its own asset value and its own promised bond payment. Default still occurs when the asset value is below the promised bond payment:

default of firm 1: 
default of firm 2: 

Default is governed by the bivariate random variable . When the asset values are independent, then the defaults are also independent, and when the asset values are correlated, then the defaults are correlated. The Merton model provides a useful framework for thinking about the default correlation and dependence across firms: it is determined by correlation across the firm asset values.
When the asset values are positively correlated, then the probability of joint default is greater than the product of individual defaults:

and the default correlation is positive:

(writing p1 for ).
In Section 11.3, we had a very simple factor model for correlation across critical variables in the form of a simple threshold model: equation (11.4), reproduced here:
(11.4)
The Xi are critical variables for different firms i, with default occurring when Xi < di. In the Merton model the random asset value at maturity  is the critical variable, and the promised payment Bi is the critical value. We can generalize the simple factor structure in (11.4) by allowing F to be multidimensional (instead of just a single F), and allowing each firm to have its own sensitivity to factors (instead of the same ρ for all firms). An example with three factors would be:
(11.11) 
In this example, factor F1 could be an overall macroeconomic factor such as GDP growth, while F2 and F3 could be industry-specific factors. The εi is a random variable independent of the Fi representing variability in the firm's asset level around the average level determined by the common factors.
In such an example, the macro factor would affect all companies. Most firms would do worse during a recession than an expansion; the coefficient αi would be positive for most firms so that a low value for F1 would lower the critical variable  (making default more likely), and vice versa. The industry variables would affect only companies within the particular industry; for example, the coefficient β would be zero for a firm not in industry represented by F2.
The common factor structure of equation (11.11) can accommodate an arbitrary number of common factors (although, to be useful, the number of factors should be much less than the number of firms). It is not the only correlation structure one could use, but it does have a number of benefits. First, it produces a parsimonious correlation structure, allowing relatively flexible correlation across firms but with a small number of parameters. Second, it ensures that the dependence across firms and default probabilities arises solely from the dependence on the common factors: conditional on a realization of the factors Fi, defaults across firms are independent (remember the assumption that the factors Fi are independent of the idiosyncratic components εi).
The conditional independence of defaults has substantial benefits, as discussed further on and in McNeil, Frey, and Embrechts (2005, section 8.4). In particular, it means that the threshold model can be recast as a Bernoulli mixture model, with advantages in simulation and statistical fitting.
For a practical application such as MKMV, the common factor structure is slightly modified from equation (11.11). The firm asset return rather than the level is modeled, and the common factor structure is multilayered, depending on global, regional factors, country, and industry factors. Crouhy, Galai, and Mark (2000, chap. 9 section 7) discusses this in some detail.
Credit Migration and CreditMetrics
In firm-value threshold models such as Merton and MKMV, default is determined by an asset variable crossing a default threshold. An alternative approach is to estimate the default probability of a firm by an analysis of credit migration; that is, the migration of a firm through various credit-rating categories.22 This is the approach taken by CreditMetrics, a commercial product developed by JPMorgan and the RiskMetrics Group, and first published in 1997.23
Single-Firm (Marginal) Migration Matrixes
The goal of credit migration modeling is to understand a specific firm's probability of default, and its relation to other firms' default probability, using historical credit migration data. The focus is on the transition or migration matrix: First, a firm is categorized according to some credit rating scheme, and then the probabilities of transiting between categories over some period (usually one year) is applied to estimate the probability of default and changes in credit status. The transition matrix is usually estimated by measuring the behavior of a large population of firms over a long period.
To start, and fix ideas, consider a trivial case with only two categories, solvent (not-in-default) and default. The migration is pretty simple—moving from solvent to default. The credit migration matrix is trivial:
(11.12)
with values that might be something like:

This says that for a solvent firm, the probability of default over the next year is 0.01, while the probability of staying out of default is 0.99. Once in default, a firm stays—the probability of moving out of default is zero.
This migration model, in fact, is exactly the default model of Section 11.3, where all firms had the same probability of default, 0.01. Such a migration model is easy to understand but not very useful. A major aim of any practical credit risk model is to distinguish between firms and to differentiate risks: to arrive at firm-specific estimates of risk factors such as the probability of default. A migration model that makes no distinction across firms except in or out of default adds almost nothing toward the goal of estimating firm-level parameters.
This simple migration model can easily be extended, however, simply by categorizing the probability of default according to credit rating. Credit ratings are publicly available for traded companies and are estimated by firms such as Standard and Poor's, Moody's Investors Services, and Fitch Ratings. Quoting S&P, "Credit ratings are forward-looking opinions about credit risk. Standard & Poor's credit ratings express the agency's opinion about the ability and willingness of an issuer, such as a corporation or state or city government, to meet its financial obligations in full and on time" (www.standardandpoors.com/ratings/en/us/). A credit rating can be considered a proxy estimate of the probability of default and possibly severity of loss, estimated using objective data, subjective judgment, and experience.24
The preceding migration matrix could be extended by categorizing companies by credit rating (in this case, S&P ratings) and measuring the probability of default conditional on the rating:25
(11.13)
This matrix differentiates across credit ratings, showing that the one-year default probability for a BB-rated company is 0.011 (and the probability of not defaulting is 0.989). Assume for now that credit ratings do appropriately differentiate across companies in terms of likelihood of default, then such a matrix goes far toward addressing the first risk factor identified in Section 11.3: estimating each firm's probability of default. Assigning a probability of default according to a borrower's credit rating would make the stylized model of Section 11.3 substantially more realistic.
The matrix 11.13 considers only migration into default, but there is far more information available. Credit rating agencies take many years of data on firms and ratings, observe the ratings at the beginning and end of the year, and calculate the relative frequency (probability) of moving between ratings categories. A firm rarely moves from a high rating directly to default, but rather transits through intermediate lower ratings, and this information on transitions between ratings is usually used in an analysis of credit risk. Table 11.4 shows a full transition matrix showing the probability of moving from the initial rating (at the beginning of the year) to the terminal rating (at the end of the year). For a firm initially rated AA, the probability of being in default at the end of the year is essentially zero, but the probability of being downgraded to A is relatively high, at 7.79 percent.
Table 11.4 One-Year Transition Matrix.

Using a multistate transition matrix as in Table 11.4 has a few advantages. First, the probability of default can be estimated over multiple years by straightforward matrix multiplication. When the initial state is represented by a vector with 1 in the appropriate column, and the migration matrix in Table 11.4 is denoted M, then the probability of being in credit rating states after one year is (with MT denoting the matrix transpose of M):

The probability after two and three years is:

If the initial state is AA:

Note that the one-year default probability is zero, but the two-year probability is 0.0002. This is because after one year there is some probability the firm will have fallen to A or BBB, and from there it has a positive probability of being in default a year later.
The second and even more important benefit is that changes in credit status beyond mere default can be modeled. For a firm rated AA, the probability of default in one year is zero according to Table 11.4, but the probability of moving to a lower rating such as A is relatively high—0.0779 according to Table 11.4. The market value of liabilities for a firm that was downgraded from AA to A would certainly fall in value, and such a fall should be taken into account when modeling credit risk.
The stylized model of Section 11.3 modeled default only and did not account for such changes in credit status. That was appropriate because we assumed the loans matured at the end of one year, so the only states they could end in was default or full payment. In reality, the world is not so neat and the impact of changes in credit status, represented here by changes in rating, should generally be incorporated. Crouhy, Galai, and Mark (2000, ch. 8) and Marrison (2002, ch. 18) discuss the mechanics of modeling losses incorporating credit migration beyond default.
Joint Probabilities, and Migration as Threshold Model
The credit migration framework has addressed probability of default, the first of the risk factors discussed in Section 11.3. The second factor, correlation or dependence across defaults, is equally important but is not addressed by the migration matrix shown in Table 11.4. The matrix shows the marginal probability of migration; that is, the probability of a firm considered in isolation. If defaults were independent (which they are not), then the joint probability of two firms migrating would be the product of the individual (marginal) probabilities. Consider two firms, one initially rated A and the other BB, and the probability that after one year they both end up B-rated. The probability if they were independent would be:
(11.14) 
The joint probability will generally not be equal to the product because migration and default across firms will not be independent. One natural mechanism creating dependence might be when firms in the same region or industry are affected by common factors—all doing well or poorly together—but we discuss that further on.
For now, equation (11.14) helps explain why the historical data collected and published by rating agencies, which lend themselves naturally to the creation of the marginal migration matrix in Table 11.4, are not as useful for analyzing the joint probability of migration and default. Even though not perfectly accurate (events are not independent), equation (11.14) gives the right order of magnitude for the joint probability. It is apparent that joint events such as described by (11.14) are rare, and so there will be very few to observe. Furthermore, there are a huge number of possible joint outcomes.
Consider Table 11.4, for which there are eight categories, AAA through default. For the single-firm (marginal) probability analysis, there are seven starting by eight ending, or 56 possibilities (the 56 entries of Table 11.4). For a joint two-firm analysis, there will be 49 starting categories (7 × 7 consisting of AAA&AAA, AAA&AA, AAA&A,...) and 64 ending categories, making 3,776 possibilities. Most of these possibilities will be quite rare, and it would take a huge sample of firms and a long period to obtain any solid estimates.
The solution chosen by CreditMetrics is to embed the credit migration process in a threshold-model framework. Consider the simple two-state migration matrix (11.12), with migration only from solvent to default. This is in fact equivalent to the Merton or KMV threshold model discussed earlier: default occurs when some random critical variable Xi falls below a critical threshold di:

Diagrammatically, this is shown by Figure 11.11, with a normally distributed X and a critical threshold d, to the far left. The firm remains solvent if the variable X is above the threshold d, and defaults if X is below d. This matches the migration matrix in (11.12) as long as d is chosen so that the area to the left of d is equal to the probability of default.

Figure 11.11 Critical Variable with Single (Default) Threshold

The way to generalize this and match a full migration matrix such as Table 11.4 should be clear. First, assume a critical variable for each initial rating, so we have a set of critical variables:

Second, for each critical variable, choose a set of critical threshold levels, chosen to match the migration probabilities. For an A-rated company, this will be:

Figure 11.12 shows what this will look like diagrammatically. The thresholds are simply chosen to match the (marginal) migration probabilities in the migration or transition matrix such as Table 11.4.

Figure 11.12 Threshold Variable for Migration of A-rated Company, Multiple Critical Thresholds

The migration approach has now been embedded in a threshold framework. The joint probability of defaults is not modeled directly, but rather indirectly through the mechanism of the underlying threshold variables, X. For practical application, the threshold variable is assumed to be the assets of the firm, normally distributed, as in the Merton and KMV models. The correlation in assets across firms then induces dependence in the defaults. For example, the joint probability of migration, from equation (11.14), no longer assuming independence, will be:
(11.4′)
This is the joint probability for a bivariate normal, with the correlation of the asset variables  and .
This is the standard approach for modeling the dependence or joint migration probability for a migration model (the approach originally proposed in RiskMetrics 1997/2007): The model is embedded in a threshold framework. Because CreditMetrics can be treated as a threshold model, and correlations are actually determined by a structural model of asset correlations across firms, it is classified as a structural model in the taxonomy of Table 11.3.
The correlation in assets across firms is usually modeled using a common factor structure, as in (11.11):
(11.15) 
where Xi = threshold variable, usually asset return, for firm i (written as Vi in (11.11) earlier)
F = common factors (may be single or multiple factors)
βi = firm i's sensitivity to the factors (usually called factor loadings)
εi = firm i's idiosyncratic component contributing to asset variability; independent of the factors F
With this structure, dependence across firms arises from the impact of common factors represented by F. These might be observable macroeconomic economic factors (such as GDP or aggregate unemployment) or industry factors (such as whether a firm is a member of a particular industry). They could also be unobservable or latent factors that are shared across firms, say in the same region or industry. The factors are common across firms, although the response of individual firms might be different. (For example, an automaker might be hurt by low GDP growth or high unemployment as people buy fewer cars, while Wal-Mart might be helped as people switch to discount retailers.)
MKMV and CreditMetrics Compared
McNeil, Frey, and Embrechts (2005, section 8.2.4) compare some of the pros and cons of MKMV versus CreditMetrics's credit migration approach. In brief:

 MKMV approach advantages

 MKMV's methodology should reflect changes more quickly than ratings agencies. Rating agencies are slow to adjust ratings, so that the current rating may not accurately reflect a company's current credit status. Since credit migration modeling depends on appropriate categorization of firms, this can lead to incorrect estimates of default probabilities.
 MKMV's expected default frequency (EDF) should capture dependence on the current macroeconomic environment more easily than historical transitions, which are averaged over economic cycles.

 Credit migration approach advantages

 Credit migration transition rates should not be sensitive to equity market over- or under-reaction, which could be a weakness for MKMV's EDFs.
 Credit ratings (either public or internal bank ratings) are often available even for firms that do not have publicly traded equity. The original MKMV model was dependent on history for equity prices to estimate asset levels and volatilities, although MKMV has developed methodologies for private companies.


11.6 Static Reduced Form Models—CreditRisk+
The threshold models from the previous section construct the default process from underlying financial and economic variables, and can thus be called structural models. An alternative is simply to assume a form for the default distribution rather than deriving the parameters from first principles, and then fit the parameters of the distribution to data. Such models are termed reduced form models. The reduced form approach has some advantages: the default process can be flexibly specified, both to fit observed data but also for analytical tractability. CreditRisk+, developed by Credit Suisse Financial Products in the 1990s (see Credit Suisse Financial Products 1997) is an industry example of a reduced form model.
CreditRisk+ concentrates on default (not credit rating migration) and the default and loss distribution for a portfolio. The mathematical form assumed for the individual-firm default process is reasonable and, importantly, leads to a convenient and tractable default distribution. Unlike the MKMV and CreditMetrics approaches outlined earlier, which require time-consuming simulation, the CreditRisk+ model can be solved relatively easily without simulation. This is a considerable advantage.26
The CreditRisk+ model focuses on two attributes of the default process:

1. Default rate or intensity, the analogue of probability of default in the models discussed earlier.
2. Variability in default intensities, although it is really the common or covariability in default intensities that matters because it is the dependence across firms (common variability generating default correlation) that is important, not idiosyncratic or firm-specific variability. The variability in default intensities can also be expressed as the mixing of underlying distributions.

The outline presented here follows McNeil, Frey, and Embrechts (2005, section 8.4.2) rather than the original Credit Suisse Financial Products (1997). Although the original presentation is comprehensive, I find it somewhat impenetrable, which is unfortunate because the techniques are so useful.
Poisson Process, Poisson Mixture, and Negative Binomial Default Distribution
Default for a single firm is approximated as a Poisson random variable with intensity λi. In reality, default is a Bernoulli variable, a variable that can take the value zero (no default) or one (default). The Bernoulli variable can be approximated, however, using a Poisson variable and there are substantial benefits to such an approximation.
A Poisson random variable is a counting variable that, in contrast to a Bernoulli variable, can take values j = {0, 1, 2,...}. When the event of default is rare, as it usually will be, the Poisson process can provide a useful approximation to the default process. The value j counts the number of events during a period. We can identify no default with j = 0, and default with j ≥ 1. This leaves the possibility that j = {2, 3,...}, but when default is rare for any particular firm, the probability of multiple defaults for a single firm will be very rare.
A Poisson variable is governed by an intensity λ, and the probability for the number of events j will be:
(11.16)
For various values for the Poisson parameter λ, the probabilities of zero, one, and two events will be as shown in Table 11.5.
Table 11.5 Probability of Multiple Events for Poisson Random Variable with Various Intensity Parameters.

We can see that when the probability of default is rare (as for everything except the lowest-rated issuers), the probability of multiple defaults is very rare.
For the Poisson model, as for the Bernoulli model, we can define a random vector  where  now counts the number of events or defaults (and we hope the number of multiple defaults for a single firm will be low). We define the random variable , which is now the sum of the number of events. The sum M* will approximate the number of defaults when the intensity and the probability of multiple events are low.
The benefit of the Poisson framework versus the Bernoulli framework, and it is a substantial benefit, arises when considering a portfolio of multiple firms. For firms that are independent, the sum of the Poissons across the individual firms is itself Poisson. This means that the total number of defaults has a simple form:
Independence across Firms
(11.17a)
Contrast with the Bernoulli process used in MKMV or CreditMetrics, where the total number of defaults will not have a simple form (for example binomial) unless all firms have the same default probability, a case that never occurs in practice.
In the real world, defaults across firms are not independent and so Expression (17a) cannot be used directly. The model can be expanded, however, to allow a firm's default intensity, λi, to vary randomly, a function of variables F; in other words, λi = λi(F). What is important here is not just that each individual firm's default intensity λi be random, but that the default intensities for firms vary together: the variables F must be common across firms.27,28 Now, if we impose the assumption that, conditional on the realization of the common variables F, the Poisson processes across firms are independent, then we can use
Conditional Independence across Firms
(11.17b) 
The distribution of total number of defaults conditional on F will be Poisson, and the unconditional distribution will be a mixture across the Poi(Λ(F)), mixing with the distribution of F. The variables F are common factors that affect some or all of the firms in a common way, in the same manner as for correlation across assets in the threshold models. The F serve to mix independent Poisson distributions, resulting in a non-Poisson distribution.
The simplest example might be F = f as a single macroeconomic factor representing the state of the economy, with default intensity for firms higher when the state of the economy is low. Conditional on the state of the economy, however, firms' default processes are independent.
Expression (11.17b) is still the conditional probability, conditional on the common factors F, whereas we need the unconditional distribution. The F will be random variables for which we must choose a distribution, and then take the expectation over the distribution of F. Assumptions for F that produce a gamma-distributed intensity λ are particularly convenient, because then the unconditional distribution will be related to the negative binomial distribution. When λ is univariate gamma, then the unconditional distribution of the sum M* will be negative binomial.
To see how this works, take the case where F is univariate and the intensity for all firms is a linear function of a gamma-distributed f:29
(11.18a)
With this assumption,

We can use these expressions (and the definition of the gamma) to see that ∑λi will be distributed Ga(a, b/∑ki). Now, according to McNeil, Embrechts, and Frey (2005, 357) and proposition 10.20, for (M* | f) distributed Poisson and ∑ λi distributed Ga(α, β), M* will be distributed negative binomial:
(11.18b)
(11.18c)
writing p = β /(1+β)

This is written in terms of (α, β), the parameters of the distribution of the sum ∑ λi, what we could call the portfolio intensity gamma distribution. It is also useful to express everything in terms of (a, b), the parameters of the gamma distribution of the factor f. This simply requires that we substitute α = a and β = b/∑ki = p/(1 − p) or p = (b/∑ki)/(1 + b/∑ki) to get:
(11.18d)

This approach has huge benefits: A Gamma-Poisson mixture produces a simple form (negative binomial) for the distribution of portfolio defaults, M*. The default distribution is now a well-known distribution that can be calculated analytically rather than by time-consuming simulation.
Details of CreditRisk+ Assumptions
So now we turn to the specific assumptions for the CreditRisk+ model.

 Default for an individual firm is approximated by a Poisson random variable.
 Default intensity of the Poisson process for an individual firm is λi(F), a function of the common variables F.
 Default, conditional on F, is independent across firms.
 The distribution of F is gamma.

Specifically, the intensity for firm i is:
(11.19) 
where ki = average default intensity (and approximate average default rate) for firm i
 = p-dimensional vector of weights for firm i, (wi1,..., wip), with the condition that the sum is one: ∑jwij = 1
F = p-dimensional independent random vector, each element distributed Ga(aj, bj) (using McNeil, Frey, and Embrechts's notation for the parameters of the gamma distribution, so that E(Fi) = aj/bj, ) and choosing 
These assumptions assure that E(Fj) = 1,  and E(λi(F)) = ki·E(F) = ki. In other words, the average intensity for firm i is ki. This is also approximately the default probability. The default probability is given by

These assumptions also ensure that the number of defaults, conditional on F, is Poisson, by equation (11.17b). The elements of F are gamma-distributed, and we saw earlier that a gamma mixture of a Poisson is related to the negative binomial.
For F univariate, the intensity for all firms is:

and ∑λi will be distributed Ga(1/σ2, 1/(σ2∑ ki)) (mean ∑ ki, variance σ2(∑ ki)2). This will give:
(11.20) 
As stated earlier, this is a huge benefit, since M* is a well-known distribution and can be handled without simulation.
When the common factors F are multidimensional (independent gammas), M* will be equal in distribution to the sum of independent negative binomial random variables. The distribution will not be as simple as in the univariate case but there are recursive formulae for the probabilities P(M* = k) (see Credit Suisse Financial Products 1997; McNeil, Frey, and Embrechts 2005, section 8.4.2; Panjer recursion in section 10.2.3).
Default Distributions for Poisson and Negative Binomial
Returning to the univariate case, we compare the mean and variance of this negative binomial (mixture) versus the Poisson with no mixing (σ2 = 0, no gamma mixing and no common variability in default intensities, which means independence of default processes across firms). The mean is the same: ∑ki. The variance of the Poisson is ∑ki, while for the negative binomial, it is increased by the factor (1 + σ2∑ki) and is also skewed.
Figure 11.13 shows the distribution for Poisson (no mixing) and negative binomial (mixed Poisson, dependence across firms). The portfolio is for 1,000 identical firms with parameter values that are representative of firms rated single-B: E(λi(F)) = 0.05 (corresponding to roughly 0.05 probability of default) and σ2 = 0.22. The mean for both distributions is 50 defaults. The standard deviation for the Poisson distribution is 7.1 defaults and the distribution is symmetric. The negative binomial is substantially spread out relative to the Poisson (standard deviation more than three times higher at 24.5 defaults), and substantially skewed.

Figure 11.13 Number of Defaults for a Portfolio of 1,000 Homogeneous Loans—Alternate Dependence Assumptions
Note: This is the number of defaults from holding a portfolio of 1,000 homogeneous loans, calculated using a Poisson default model, each firm with average default intensity of 0.05. The Poisson Mixture is a mixture of Poisson-intensities with identical intensities λ = 0.05, univariate mixing variable f  Ga(a = 1/σ2, b = 1/σ2), σ2 = 0.22, producing a negative binomial distribution, Nb(1/σ2, 1/(1 + 50σ2)). This corresponds to pairwise default correlations between firms of 0.012.

The analytic results and simple distributions considerably simplify the calculation of the default distribution and properties of the distribution such as VaR or economic capital.
Intensity Volatility and Default Correlation
Credit Suisse Financial Products (1997) takes the fundamental parameters of the model to be mean and standard deviation of the random default intensity λi(F), and calibrates these against observed data. (Remember that for the univariate case λi = ki·f and f  Ga(1/σ2, 1/σ2), so the mean is ki and the standard deviation is kiσ.) It is pretty straightforward to calibrate or estimate the mean of λi from observables. Ratings agencies follow firms and report the number of defaults from the pool of followed firms. The number of defaults (Mt) and number of firms that are being followed (mt) are available annually by ratings category. From this, it is easy to calculate the average observed default rate (separately for each ratings category):
(11.21) 
This observed default rate is an estimate of the mean default probability and can be equated to the mean of λi since for the Poisson model the mean of λi is approximately the mean default probability.
The standard deviation is not so straightforward. It is important here to distinguish between default intensity (with an assumed distribution whose standard deviation is a parameter of the model) and the observed or finite-sample default rate (which will have a finite-sample distribution with some different standard deviation). The distinction is important but somewhat subtle. Consider the case of n identical firms, each with fixed Poisson intensity, λ (in other words, the standard deviation of the default intensity distribution is zero). The Poisson intensity for the collection of firms will be nλ and the standard deviation of the default count will be ). The observed or finite-sample default rate is the count divided by n, and it will have a standard deviation of ). In other words, even when the intensity is constant (standard deviation of the default intensity distribution is zero) the observed average default rate standard deviation will be positive because of random sampling variability.30
The standard deviation of the random default intensity λi(F) can be extracted from the standard deviation of observed default rates p* but to do so is not trivial. As just argued, finite-sample (observed) default rates will fluctuate because of sampling variability even if the default intensity is constant. This finite-sample variability will vary with n or √n (sample size). Observed default rates will also fluctuate if the default intensity is random, and this will not vary with n in the same way. The trick is to distinguish between the two.
For ease of illustration, assume that the number of firms being followed each year is the same: mt = n. When the intensity is constant (the same for all firms, λi(F) = k = constant) the distribution of the count Mt will be Poisson and the variance of the observed average default rate p* will be k/n. This expression goes down as n increases.31 In contrast, for λi(F) distributed gamma (λi = ki·f and f  Ga(1/σ2, 1/σ2)), the variance of the observed default rate is k/n + k2σ2; the second term in the expression does not go down as n increases.32
Figure 11.14 demonstrates how the volatility of observed default rates falls as the number of firms in the sample increases. Panel A is fixed intensity, an unmixed Poisson distribution, for 100 and 1,000 firms. For fixed intensity, the width of the observed average default rate distribution shrinks substantially as the number of firms rises from 100 to 1,000; volatility goes like √(k/n). Panel B is variable intensity, a mixed Poisson with intensity gamma-distributed (λ = k*f = 0.05*f and fGa(1/σ2, 1/σ2)), producing a negative binomial. Here, the width of the default rate distribution does not shrink very much as the number of firms rises from 100 to 1,000 because the volatility behaves like √(k/n + k2σ2) and the term k2σ2 dominates. The bottom line is that the standard deviation of observed default rates must be used carefully to estimate the standard deviation of the default intensity. Gordy (2000) discusses estimation of the standard deviation and McNeil, Frey, and Embrechts (2005, section 8.6) discuss estimation more generally.

Figure 11.14 Decrease in Width of Distribution of Observed Default Rates as Sample Size Increases, Fixed versus Variable Default Intensity
Note: Distribution of observed default rates (Equation (11.21)). Panel A is for default count Poisson-distributed with constant default intensity λ = k = 0.05; the variance of the default rate distribution is k/n. Panel B is for default count negative binomial-distributed, a mixture of Poissons, with intensity λ = k*f = 0.05*f and fGa(1/σ2, 1/σ2), σ2 = 0.22. The variance of the default rate distribution is k/n + k2σ2.

The usual approach for CreditRisk+ is to calibrate the standard deviation of the intensity using the standard deviation of observed default rates. Alternatively, one could calibrate against the observed pairwise default correlations. The default correlations are a fundamental aspect of the credit risk problem, and focusing on default correlations makes this explicit. It is the common or covariability of default intensities across firms that is important in producing asymmetric default distributions. (It is possible to show, by simulation, that idiosyncratic variability in default intensity does not have an impact on overall portfolio variability as the portfolio grows.) Since it is the covariability that matters, focusing specifically on default correlations seems to be appropriate, particularly when modeling individual firm relationships. Nonetheless, since default is rare and joint default doubly rare, calibrating against default correlations can be difficult.
To calculate the (approximate) pairwise default correlation, remember that  counts the number of events and the event of default is Y* > 0. For the case of univariate common variables, f, default probability is given by:

Joint default probability is:

The default correlation will be (approximately)
(11.23) 
The mixing by the gamma variable f both increases the variance of the total default distribution (to ∑ki(1 + σ2∑ki) from ∑ki) and induces correlation across defaults (approximately [ki·kj·σ2]/[√(()·())] versus zero). We can view the gamma mixing as either increasing the variance or creating correlation—they are equivalent and both are valid.
Specific Factor
As mentioned at the beginning, the outline presented here follows McNeil, Frey, and Embrechts (2005, section 8.4.2), which presents the model as a mixture of Poissons. I believe this approach simplifies the exposition. For example, Credit Suisse Financial Products (1997) introduces a specific factor (appendix A12.3). For the mixture of Poissions outline presented here, the default intensity for a given firm i is the sum over gamma-distributed variables, indexed by j. Repeating equation (11.19) from before:
(11.19)
where ki = average default intensity (and approximate average default rate) for firm i
wij = weights for firm i, applied to common factor j, with the condition that the sum is one: ∑jwij = 1
fj = independent random variables distributed Ga(aj, bj) (using McNeil, Frey, and Embrechts's notation for the parameters of the gamma distribution, so that E(Fi) = aj/bj, ) and choosing 
A specific factor corresponds to defining f0 as a constant, that is, a degenerate gamma variable with σ0 = 0. In the case of a single common factor (no degenerate factor), the mean and standard deviation of the intensity are in a fixed ratio for all levels of default intensity:

By introducing the degenerate f0, this ratio can vary across levels of intensity:

The resulting unconditional distribution for the default count M* will now be the convolution of a Poisson (intensity ∑iwi0ki) and a negative binomial (M*  Nb(1/σ2, 1/(1 + σ2∑iwi1ki)), E(M*) = ∑iwi1ki, var(M*) = ∑iwi1ki·(1 + σ2∑iwi1ki)). The convolution makes the distribution slightly more difficult computationally than the negative binomial with no constant wi0, but still orders of magnitude less computationally intensive than simulation as for the Bernoulli case. We will see that the introduction of the constant wi0 will be important in fitting to observed data.
Loss Distribution
The discussion so far has covered only the default distribution. Losses depend on both the event of default and the loss given default:

The loss given default depends on the exposure and the recovery upon default:

In the CreditRisk+ model, the exposure and recovery are subsumed into the loss given default, which is treated as a random variable. The loss distribution will be the compounding of the default distribution and the distribution of the loss given default. This is discussed in detail in Credit Suisse Financial Products (1997). The loss distribution will be a compound distribution (see McNeil, Frey, and Embrechts, 2005, section 10.2.2). For the assumptions in the CreditRisk+ model, where the default distribution is a Poisson mixture, the loss distribution will be a compound mixed Poisson distribution, for which there are simple recursion relations, detailed in Credit Suisse Financial Products (1997).
11.7 Static Models—Threshold and Mixture Frameworks
Threshold and Bernoulli Mixture Models
The static (fixed time period) structural models discussed in Section 11.5 were formulated as threshold models: default (or ratings transition) occurs when a critical variable X crosses below a critical threshold d. Joint default for two firms is determined by the joint probability that both threshold variables are below their respective critical thresholds:

In many cases, the Xi are assumed jointly normal so that this is a statement about a bivariate (or for more than two, multivariate) normal distribution.
When the threshold variables are formulated using the common factor structure of (11.15), the model can alternatively be represented as a Bernoulli mixture model. Bernoulli mixture models have a number of advantages, particularly for simulation and statistical fitting (cf. McNeil, Frey, and Embrechts 2005, section 8.4).
The definition for the common factor structure is equation (11.15), reproduced here.
(11.15)
Conditional on F, the threshold variables Xi are independent because the εi are independent. This means the joint default process is independent, conditional on F:

where the final-but-two equality follows because ε1 and ε2 are conditionally independent.
The upshot is that the probability of default is independent across firms (conditional on F), with the probability for each firm being a function pi(F). For the preceding threshold models, the function p is the normal CDF:
(11.23) 
but other choices are equally good—see McNeil, Embrechts, and Frey (2005, 354).
The important point is that, conditional on the common factors F, each firm's default is an independent Bernoulli trial with probability given by pi(F). As a result, working with the distribution of defaults for a portfolio becomes more straightforward. For simulation, this boils down to the following: instead of generating a high-dimensional multivariate distribution {X1,..., Xn}, we generate a univariate F and then perform independent Bernoulli trials (by generating independent uniform random variates).
We can define a random vector Y = (Y1,..., Yn)′ where Yi = 1 means firm i has defaulted and Yi = 0 means it has not defaulted. We can define the random variable M = ∑iYi, which is the sum of the Yi, that is, the number of defaults. If all the firms are identical so that all the pi are the same, say p*(F), then the distribution of the sum M (conditional on F) will be binomial and the probability of k defaults out of n firms will be:
(11.24a) 
In the general case, each pi(F) will be different. We can define a vector y = (y1,..., y1)′ of zeros and ones to represent a particular configuration of defaults, and the probability of such a configuration (conditional on F) is:
(11.24b) 
This is a sequence of Bernoulli trials; each firm is subject to a Bernoulli trial determining whether it is in default or not. The total number of defaults, M, will now be a sum of Bernoulli rvs (still conditional on F) but each with potentially different pi. This will not be binomial and does not have any simple distribution.33
To complete the Bernoulli mixture framework requires a distribution for the random variables F. The MKMV and CreditMetrics models considered earlier were originally formulated as threshold models with common factors F normally distributed. Now we are treating them as Bernoulli mixture models and the F is the mixing distribution, which is normal (possibly multivariate normal). Normal is the usual choice but not the only choice.
The unconditional distribution is found by integrating (11.24b) over the distribution of F. This is now a mixture of Bernoulli processes, with F serving as the mixing variable. The distribution for the total number of defaults, M, will not tend to a normal, and as seen in Section 11.3 will certainly not be symmetric. (See McNeil, Frey, and Embrechts [2005, section 8.4] for a complete discussion.)
The mixing produces dependence across defaults. The conditional default probability is pi(F), given by (11.23) for the preceding threshold models. The fact that firms' default probabilities share the common variables F produces dependence. Say that F is univariate and that all βi are the same, β > 0. Then when F is below average, it will affect all firms in the same way, and default for all firms will be higher. This is dependence across firms because the joint probability of default is higher when F is below average, and lower when F is above average.34 The strength of the dependence depends on the variance of F relative to ε and the size of β and σ.
One significant benefit of working in the Bernoulli mixture framework is in simulation. If we knew the common factors F, then simulating the process would be very simple:

1. Determine the probability of default for each firm i, pi(F), as a function of F. In many applications, the function is pi(F) = Φ [(di − βiF − μi)/σi].
2. Perform a sequence of Bernoulli trials: for each firm, draw an iid uniform random variate, and compare to pi(F); the firm is in default or not depending on whether the uniform rv is above or below pi(F).

In fact, we do not know the value of F but simulating the unconditional process is only slightly more complex. All that is required is that, for each trial, we first draw a random realization for F. The F will generally be multivariate normal, but with dimension far lower than the number of firms. (F might be on the order of 10 dimensions, while there can easily be thousands of firms. In contrast, working in the threshold framework means simulating a multivariate normal with dimension equal to the number of firms, making simulation computationally more difficult.)
The simulation scheme is a slight extension of that shown earlier:

1. Draw a realization for F.
2. Determine the probability of default for each firm i, pi(F), as a function of F. In many applications, the function is pi(F) = Φ [(di − βiF − μi)/σi].
3. For each firm, draw an id uniform random variate, and compare versus pi(F); the firm is in default or not depending on whether the uniform rv is above or below pi(F).

Most implementations of threshold models (and in particular MKMV and CreditMetrics) can be formulated as Bernoulli mixture models because the correlation across firms is modeled using a common factor structure as in equation (11.15). Writing the model's stochastic structure as a Bernoulli mixture model simplifies thinking about how and why the default distribution behaves as it does.
Another important implication of the Bernoulli mixture approach is that under this framework average default rates will vary over time as F varies. Consider a homogeneous pool of firms or loans and a conditional default probability given by equation (11.23) with X following the equicorrelation structure as given in 11.4:

Conditional on a realization of F, defaults will be binomial with mean default rate p(F) = Φ[(d − F√ρ)/√(1 − ρ)]. The median will be Φ [d/√(1 − ρ)], and the ±1σ values will be Φ[(d ± √ρ)/√(1 − ρ)]. Say we are considering default over a one-year period. Then in any given year, the default distribution will be binomial, but from one year to the next, the default rate will vary, and when considering the distribution over multiple years, the distribution will be skewed.
One final note concerning Bernoulli mixture models. The threshold models considered earlier assume the probability of default pi(F) depends on F through the normal CDF Φ, as in equation (11.20). Alternative assumptions could be used, and are discussed further on.
Poisson Mixture Models
The Bernoulli mixture framework is very useful, but as discussed in Section 11.6 with reference to the CreditRisk+ model, it can be convenient to model the event of default by a Poisson rather than a Bernoulli random variable. This is an approximation, but a very useful and convenient one. The convenience arises because the sum of independent Poisson random variables remains Poisson, while the sum of Bernoulli variables does not have any simple distribution.35 The total number of defaults over a period is the sum of the individual firm default variables, so when default is modeled by independent Poisson variables, the total number of defaults is immediately available as a Poisson distribution.
Unconditional independence across firms is not a realistic assumption, but as with Bernoulli mixture models, it is often reasonable to assume that default processes are independent when conditioning on some set of random variables F. When default is modeled by conditionally independent Poisson variables the sum or total number of defaults, conditional on F, will be Poisson. The unconditional default distribution is the integral over the distribution of F; in other words, a Poisson distribution mixed with F. When F is gamma-distributed, the resulting distribution with be a gamma-Poisson mixture, which is negative binomial.
The CreditRisk+ model of Section 11.6 was presented as a gamma-Poisson mixture model. Firm default intensity is assumed to be (repeating equation (11.19)):
(11.19)
with ∑jwij = 1 and F an independent multivariate gamma. Conditional on F, firms' default processes are independent. The analytic and semianalytic results for the gamma-Poisson mixture considerably simplify the calculation of the default distribution and properties of the distribution such as VaR or economic capital.
One fruitful way to view the Poisson mixture is as an approximation to a Bernoulli mixture, an approximation that is computationally tractable. The distribution for a Bernoulli and Poisson mixture model are quite similar, given appropriate choices of parameters. Consider the one-factor Bernoulli mixture model from Section 11.3 (although it was discussed there as a threshold model, it can also be treated as a Bernoulli mixture model).
Reasonable parameters for a Bernoulli mixture model of identical single-B-rated issuers would be average probability of default = p* = 0.05 and threshold variable correlation (assuming the equicorrelation structure of equation (2)) ρ = 0.05. This will produce a default correlation of 0.012. Matching parameters for a gamma-Poisson mixture model would be λ = 0.05 and σ2 = q = 0.22. For a portfolio of 1,000 firms, this gives M*  Nb(10, 0.0833), mean of 50, standard deviation of 24.5, and pairwise default correlation of 0.012.
The simulated Bernoulli mixture and the analytic negative binomial distributions are very close. Both distributions have mean 50, default correlation 0.012, standard deviations of 24.5 (Poisson mixture) and 24.7 (Bernoulli mixture), and 1%/99% VaR of −$41,000 (Poisson mixture) and −$43,000 (Bernoulli mixture), compared with −$9,300 for the unmixed distribution with no correlation. Figure 11.15 shows both the unmixed distributions (binomial and Poisson) and the mixed distributions. They are shown separately (the binomial/Bernoulli mixture in Panel A and the Poisson mixture/negative binomial in Panel B) because they would be virtually indistinguishable to the eye if drawn in the same chart. Furthermore, the Bernoulli and Poisson mixtures are close for small portfolios as well as large.

Figure 11.15 Comparison of Poisson and Bernoulli Mixture Distributions—Portfolio of 1,000 Firms
Note: This shows the default distribution for 1,000 identical loans. For the Bernoulli distributions (Panel A), the probability of default is p = 0.05, while for Poisson distributions (Panel B), the intensity is λ = 0.05. The Independent Bernoulli and Poisson distributions are unmixed. The Bernoulli Mixture is mixed with a normal (probit-normal) using an equicorrelation structure with ρ = 0.05 (see 11.3). The Poisson Mixture is mixed with a Ga(1/0.22, 1/0.22), which produces a Nb(1/0.22, 1/(1 + 0.22*0.05*1,000)). All distributions have mean 50, the mixtures both have pairwise default correlation 0.012, the standard deviation of the Poisson mixture is 24.5 and the Bernoulli mixture is 24.7.

Generalized Linear Mixed Models
Both the Bernoulli and the Poisson mixture models discussed so far fit under the generalized linear mixed models structure (see McNeil, Frey, and Embrechts 2005; McCullagh and Nelder 1989). The three elements of such a model are:

1. A vector of random effects, which are the F in our case.
2. A distribution from the exponential family for the conditional distribution of responses. In our case, responses are defaults (either Yi for the Bernoulli or  for the Poisson). The defaults are assumed independent conditional on the random effects F. The Bernoulli, Poisson, and binomial distributions are from the exponential family.
3. A link function h() linking the mean response conditional on the random effects, E(Yi | F), to a linear predictor of the random effects μ + β + F. That is, a function h() such that E(Yi | F) = h(μ + β + F). Here, the  represent observed variables for the ith firm (such as indicators for industry or country, or balance sheet or other firm-specific financial measures) and μ and β are parameters.

Table 11.6 shows various Bernoulli and Poisson mixture models. The probit-normal and the gamma-Poisson are used in commercial products as noted.
Table 11.6 Various Mixture Models.

Parameters for Bernoulli and Poisson Mixture Models
McNeil, Embrechts, and Frey (2005, section 8.6.4) fit a probit-normal Bernoulli mixture model to historical Standard and Poor's default count data from 1981 to 2000.36 They assume that any firm is one of the five rating classes (A, BBB, BB, B, CCC) and that all firms within a ratings class have the same probability of default, pr. The probability of default varies with a single common factor, f. The equivalence between Bernoulli mixture models and threshold models will be useful, so we write out the notation for both.



Probit-Normal Mixture
Threshold




pr(f) = Φ(μr + σf) f  N(0,1)
Critical variable X = f√ρ + ε √(1 − ρ) as in (11.4) f  N(0,1) and ε  N(0,1) ⇒ X  N(0,1) default occurs in rating r when X < drpr(f) = Φ[(dr − f√ρ)/√(1 − ρ)]



In the mixture representation,

But the equivalence between the Bernoulli mixture and threshold formulations gives:

Since default occurs when X < dr and X  N(0,1), then the average probability of default is P[X < dr] = Φ(dr). It also is true that ∫Φ(μr + σz)ϕ(z)dz = Φ[μr/√(1 + σ2)] = Φ[μr √(1 − ρ)].)

The threshold formulation will generally be more useful for computation while the Bernoulli mixture formulation is more useful for estimation and simulation. The pairwise default correlation is from equation (11.2):
(11.25) 
Table 11.7 summarizes the results, from McNeil, Frey, and Embrechts (2005, Table 8.8). These results provide a valuable resource for calibrating parameters of simple default models. Importantly, McNeil, Frey, and Embrechts also fit a simple extension to the model that allows the variance of the systematic factor, σ2, (that is, the scaling applied to the common factor f) to differ by rating category: pr(f) = Φ(μr + σrf). This additional heterogeneity, however, does not provide substantial improvement to the fit, indicating that the simple model is adequate.
Table 11.7 Parameter Estimates for Bernoulli Mixture Model—from McNeil, Frey, and Embrechts (2005, Table 8.8)

As mentioned earlier, under the Bernoulli mixture framework (common factor structure) the defaults for any uniform pool of firms or loans will vary from one period to the next as the common factor f varies. The default rate for firm type r conditional on f is Φ[(dr − f√ρ)/√(1 − ρ)]. The median will be Φ[dr/√(1 − ρ)] while the ±2σ default rates will be Φ[(dr ± 2√ρ)/√(1 − ρ)]. Table 11.8 shows the mean, median, and the ±2σ default rates implied by the estimates in Table 11.7.
Table 11.8 Variation in Default Rates Under Bernoulli Mixture Framework Implied by Estimates from Table 11.7.

The data in Table 11.8 show striking variability in default rates. Averaging across years the default probability for single-A rated firms is 0.044 percent but roughly once every six or seven years the probability will be more than 0.073 percent. A diversified portfolio will do nothing to protect against this risk, since all firms are responding to the same common factor. This highlights why credit risk is such a difficult issue: credit risks either all do well (low default rates) or all badly (high default rates).
The cross-correlations shown in Table 11.7 are calculated from equation (11.25) and are critically dependent on the structure of the model. The event of default is rare and simultaneous default is doubly rare.37 It is therefore difficult to estimate cross-correlations directly from the data (nonparametrically), particularly for higher-rated issuers. The structure of the probit-normal, in particular the functional form pr(f) = Φ(μr + σf), and the assumption of a single common factor with homogenous scaling (same σ applied to all ratings categories) imposes the cross-correlation structure exhibited in Table 11.8. The size of the scaling factor σ determines the level of the correlations. The primary feature in the data that will determine σ will be the variance of the count distribution relative to that for an unmixed Bernoulli distribution. In other words, the variability of the annual counts or default rates, rather than default correlations across firms, will be the primary determinant of σ. In this sense, CreditRisk+'s focus on default rate variability is justified.
The model's dependence on functional form to determine the correlation structure is both a strength and a weakness of the model. The strength is that it provides a structure that produces cross-correlations in the face of scarce data. The weakness is that the structure of the model is difficult to test given data limitations. Given the paucity of data, however, there is probably little alternative. One must trust that the threshold model, with default determined by a critical variable crossing a threshold, is appropriate, and that modeling correlation across the underlying critical variables appropriately captures the cross-correlation of defaults.
The data in Table 11.7 can also be used to calibrate a Poisson mixture (CreditRisk+ type model). For a single-factor model (with no constant term so that wr0 = 0 and wr1 = 1 and writing q instead of σ2 for the gamma variance), the approximate default correlation, repeating equation (11.23) is:
(11.22)
There is, in fact, no single q that even comes close to reproducing the correlations in Table 11.7. The implied values are shown in Table 11.9, and these vary by a factor of more than eight.
Table 11.9 Gamma Variance Parameters for Poisson Mixture Model Implied by Table 11.8.

The Poisson mixture model with a single common factor

can be extended by introducing a constant term wr0:
(11.26) 
For a portfolio with n identical firms, this means λ will be the sum of a constant (n·kr·wr0) and a gamma-distributed random variable (mean n·kr·wr1, variance , implying that it is Ga(1/q, 1/n·kr·wr1·q)). This will produce a random variable that is, in distribution, equal to the sum (convolution) of a Poisson (with parameter n·kr·wr0) and a negative binomial (Nb(1/q, p) with p = (1/n·kr·wr1·q)/(1 + (1/n·kr·wr1·q)) = 1/(1 + n·kr·wr1·q)) (See McNeil, Frey, and Embrechts 2005, 357.)
When we do this, normalizing by wA1 = 1 (wA0 = 0 and thus using q = 0.9101), we get the weights shown in the first row of Table 11.10, and the correlations shown in the bottom of the table.38 These correlations match those shown in Table 11.7 quite well.
Table 11.10 Weights and Default Correlations for Single-Factor Poisson Mixture Model.

The benefit, and it is a substantial benefit, of formulating the model as a Poisson mixture in Table 11.10 rather than a Bernoulli mixture as in Table 11.7, is that the default distribution is simple to compute. The distribution will be negative binomial for the single-A, where w0 = 0, and a convolution of Poission and negative binomial for the others. The convolution is computationally simple relative to the simulation required for calculating the Bernoulli mixture.39 Note, however, that while the convolution of the Poisson and negative binomial matches the correlation of the mixed Bernoulli probit-normal, it does not always match the shape of the distribution. The negative binomial and mixed probit-normal are essentially the same when wr0 = 0, wr1 = 1, but differ when wr0 > 0.
Figure 11.16 shows both the Bernoulli probit-normal mixture and the Poisson mixture (convolution) for a portfolio of 10,000 BBB firms and 200 CCC firms. Panel A shows that the Poisson mixture distribution for BBB, where wr1 = 0.8460, is not too far from the mixed Bernoulli. Panel B shows, however, that as wr1 falls (so the Poisson mixture becomes more weighted toward a Poisson versus negative binomial), the shape of the Poisson mixture diverges from the probit-normal mixture.40 A pure negative binomial (with wr0 = 0 and q = 0.1066) does match the Bernoulli mixture—the pure negative binomial is not shown in Figure 11.16 Panel B because it is virtually indistinguishable from the Bernoulli mixture.

Figure 11.16 Comparison of Shape of Bernoulli Probit-Normal Mixture versus Poisson Mixture (Convolution of Poisson and Negative Binomial) Note: Panel A shows the distribution for a portfolio of 10,000 BBB firms. The Bernoulli is a probit-normal mixture with μ = −3.426 and σ2 = 0.2430 (probability of default 0.00227, critical level −2.837, critical variable correlation ρ = 0.05576). The Poisson/negative binomial is the mixed Poisson with common factors given by equation (11.24) with wr0 = 0.1540, wr1 = 0.8460, and q = 0.9101 (convolution of Poisson with intensity 3.4958 and negative binomial with alpha = 1.09878, p = 0.054119). Panel B shows the distribution for a portfolio of 200 CCC firms. The Bernoulli is a probit-normal mixture with μ = −0.838 and σ2 = 0.2430 (probability of default 0.20776, critical level −0.814, and critical variable correlation ρ = 0.05576). The Poisson/negative binomial is the mixed Poisson with common factors given by equation (11.24) with wr0 = 0.6577, wr1 = 0.3423, and q = 0.9101 (convolution of Poisson with intensity 27.3302 and negative binomial with alpha = 1.09878, p = 0.071701).

There do not appear to be a single set of parameters for the Poisson mixture that simultaneously matches the correlation structure and also reproduces the shape of the Bernoulli mixtures. There is, however, nothing sacred about the shape of the Bernoulli probit-normal mixture. The tails of the distributions cannot be fit well because of the paucity of data, so it would be difficult to discriminate between the two on the basis of observations.
Further Comparisons across Credit Models
The MKMV and CreditMetrics models can be reduced to the same Bernoulli mixture framework, and we have just seen that the Poisson mixture used in CreditRisk+ can often be a close approximation to the Bernoulli mixture distribution. It should therefore come as little surprise that, when parameters are calibrated to be roughly the same, the models produce roughly the same results. Crouhy, Galai, and Mark (2000, ch. 11) go through the exercise of calibrating these three models (plus a fourth, CreditPortfolio View, which can also be formulated as a Bernoulli mixture model but with a logit-normal link function—see Table 11.7). They apply the models to a large diversified benchmark bond portfolio and find that "the models produce similar estimates of value at risk" (p. 427).
Gordy (2000) compares CreditRisk+ and CreditMetrics (more accurately, a version of CreditMetrics that models default only, just as we have implicitly done). He shows the similarity of the mathematical structure underlying the two models. He also compares the results for a variety of synthetic (but plausible) bank loan portfolios, and shows that the models are broadly similar.
11.8 Actuarial versus Equivalent Martingale (Risk-Neutral) Pricing
The focus for credit risk so far has been on building the distribution of defaults and losses. There has been little or no attention on pricing credit risks or using market prices to infer the distribution of credit losses because we have assumed that market prices are not readily available. The focus has been on building the distribution of defaults and losses from first principles, often using complicated models and limited data. We have, naturally, used the actual probability of defaults and losses, the probability we actually observe and experience in the world—what we would call the physical probability measure.
We are going to turn in the next section to market pricing of credit securities, and what are termed dynamic reduced form models. In doing so, we need to introduce a new concept, the equivalent martingale or risk-neutral probability measure.
The distinction between physical and equivalent martingale probability measures can be somewhat subtle but in essence it is straightforward. The physical measure is the probability that we actually observe, what we experience in the physical world. All the credit risk distributions we have been discussing so far have been using the physical measure (which we will call P), the probability we actually experience. The equivalent martingale or risk-neutral measure (which we will call Q) arises in pricing market-traded securities. It is an artificial probability measure, but one that is nonetheless incredibly useful for pricing securities.
The natural question is: Why use anything other than the physical, real-world probabilities? The answer is that pricing securities using the physical probability measure is often difficult, while pricing with the equivalent martingale measure reduces to the (relatively) simple exercise of taking an expectation and discounting; for market-traded instruments, the risk-neutral approach is incredibly powerful.
Physical Measure and the Actuarial Approach to Credit Risk Pricing
To see how and why pricing under the physical measure can be difficult, we will go back and consider the simplest, stylized credit model outlined in Section 11.3—a portfolio of 1,000 loans that mature in one year and pay 6.5 percent if not in default. The distribution of income is binomial and shown in Figure 11.3. The mean income is $59,350, which means the average income per loan, accounting for losses due to default, is 5.935 percent. In Section 11.3, we briefly outlined how a firm might set reserves for such a portfolio. But we can consider the problem from a different perspective: Given the default behavior, what should be the price? More specifically, instead of taking the 6.5 percent promised interest as given, what interest rate should a firm charge? Is 6.5 percent high or low considering the risk that, on average, 10 loans out of 1,000 will default?
This seemingly straightforward question actually raises some deep and difficult problems. Assume for now that these loans are not traded and so there is no market price available, so we must work without the benefit of reference to outside prices. One standard approach is to set the interest rate at a spread relative to a default-free bond of the same maturity, with the spread set as:

This is referred to as an actuarial approach because the expression has the same structure as standard actuarial premium principles (see McNeil, Frey, and Embrechts 2005, section 9.3.4). The expected loss and risk premium are the focus (administrative costs are not the prime interest here). The expected loss is generally straightforward. In our example, it is simple, just the product of the probability of default (0.01) times the expected loss given default (50 percent), giving 0.5 percent.
The risk premium is more difficult, as it depends fundamentally on risk preferences. A common approach is to apply a hurdle rate (return on equity) to the economic capital held against the loan. Economic capital is determined from the distribution of income, Figure 11.3, as discussed in Section 11.3. It will be the buffer to protect the firm against unexpected losses from the overall portfolio, a buffer intended to protect against default and ensure some prespecified (low) probability of default. As such, economic capital will be a tail measure such as VaR or expected shortfall.
There is no correct choice of hurdle rate; it depends on risk preferences and attitudes toward risk. Whose preferences? Maybe the firm's management, maybe investors, but the answer is not trivial or obvious. For our example, let us choose a 20 percent return on equity. On our economic capital of $7,300 (from Section 11.3), this gives an aggregate risk premium of $1,460. As a percent of the portfolio investment ($1M), this is 0.146 percent.
The economic capital, and thus the risk premium, is determined for the overall portfolio, not on a security-by-security basis, and so must be allocated to individual securities.41 The risk premium allocation is itself nontrivial. In realistic portfolios, some loans may be highly correlated with the overall portfolio and thus contribute substantially to the overall risk, requiring substantial capital and entailing a large risk premium. Others may be uncorrelated with the portfolio, contribute little to the overall risk, and thus require little capital and entail a low risk premium. The allocation may be done using the analogue of the contribution to risk discussed in Chapter 10. McNeil, Frey, and Embrechts (2005, section 11.3) discuss various capital allocation principles. For our example, where all loans are identical, the spread would be 0.646 percent.
Note the difficult and somewhat tricky steps to arrive at the loan spread:

 Calculate the expected loss for each loan (0.5 percent).
 Calculate the economic capital for the overall portfolio ($7,300).
 Calculate a firm-wide risk premium by applying a hurdle rate to the economic capital (20 percent, $1,460, or 0.146 percent).
 Allocate the overall risk premium back to each loan (0.646 percent).

Moving down, the steps become more complex with more subjective components.
These loans have now been priced in a reasonable manner, but the process is not trivial and partly subjective. For loans such as these, for which there are not comparable or reference market prices, such an approach may be the best that can be done.
Equivalent Martingale or Risk-Neutral Pricing
For traded assets or securities, when market prices are available, the equivalent martingale or risk-neutral pricing approach is very powerful. The easiest way to understand the difference is by an example.42 The Merton model of 11.5 posits a firm funded by bonds and stock. The firm's total assets are assumed to follow a log-normal process, (11.7a), which gives us the probability of default (11.7b). The asset process specified in (11.7a) is the physical process and the probability of default in (11.7b) is the physical or actual probability of default. This is exactly what we wanted in Section 11.5, and what we used in Sections 11.5 through 11.7.
At no point, however, did we attempt to actually price the bond or the equity. We could have done so by taking expectations over the future payouts (the payouts given by equations (11.5) or (11.6)), using the true distribution (11.7a). The problem is that we would need to know investors' preferences—their attitude toward risk—to ascertain the relative value of the upside versus downside. This is not a trivial exercise, the equivalent of (but more difficult than) choosing the hurdle rate and the allocation of aggregate risk premium in the preceding example.
Under certain conditions, however, future cash flows can be valued by simply taking the discounted expectation of those cash flows, but taking the expectation over the artificial equivalent martingale probability measure Q rather than the true measure P.43 For the Merton model, it turns out that the martingale measure Q simply requires replacing the mean or average growth rate for the asset process by the risk-free rate. Instead of μ in equation (11.7a), we substitute r:

asset process under physical measure P; log-normal with mean μ:

asset process under equivalent martingale measure Q; log-normal with mean r:


The true default probability is given by (11.7b), reproduced here:

while the default probability under the equivalent martingale measure is given by:

The difference between μ and r will mean that the two probabilities are different. For the Merton model, it is possible to express q in terms of p:

Generally, q will be larger than p since usually μ > r. (This expression is only valid for the Merton model, although it is often applied in practice to convert between physical and risk-neutral probabilities.)
The beauty of the equivalent martingale measure is that now the price of the bond and stock can be calculated as simply the discounted expected value of the future payout. For the stock, this is:

The asset value VT is log-normal and so the integral is, in fact, just the Black-Scholes formula for a European call:
(11.27a)
The bond will be the discounted value of the promised payment, Be−rT, less a put:
(11.27b) 
The beauty of the equivalent martingale or risk-neutral approach is the simplicity of the formulae (11.27). Using the risk-neutral measure, we can price the securities as if investors were risk-neutral (and the mean were r rather than μ). That is not to say that the true distribution has mean r, or that investors are actually risk-neutral. Rather, when markets are complete so there are enough securities that we can replicate the payouts (11.5) and (11.6) through dynamic trading of existing securities, we get the right answer by simultaneously using the risk-neutral measure (mean r) and treating investors as risk-neutral. The risk-neutral argument is a relative pricing argument—it works because we can dynamically hedge or replicate the payouts.
The risk-neutral approach opens a whole arena of possibilities. We now have the price of the risky bond, equation (11.27b), as a function of the relevant underlying variables. For example, the term Be−rT is the value of a risk-free or default-free bond, and we can use (11.27b) to obtain the yield spread between the risk-free and risky bond. (Note, however, that the Merton model is not ideal as a model of credit spreads, as it implies the short-dated spread tends toward zero. See McNeil, Frey, and Embrechts [2005, section 8.2.2] and Crouhy, Galai, and Mark [2000, section 9.2]. We will encounter more useful models for risky bonds and credit spreads shortly.) The term σ is the volatility of the firm's underlying assets, and we can use (11.27b) to examine exactly how the risky bond price varies with asset volatility.
Pricing the risky bond has now become easy. The probability of default is no longer the true probability, but if our primary concern is the price of the risky security, we really don't care.
Actuarial and Risk-Neutral Pricing Compared
McNeil, Frey, and Embrechts (2005, section 9.3.4) have an excellent summary contrasting actuarial pricing (using the physical probability measure) with risk-neutral pricing:
Financial and actuarial pricing compared. We conclude this section with a brief comparison of the two pricing methodologies. The financial-pricing approach is a relative pricing theory, which explains prices of credit products in terms of observable prices of other securities. If properly applied, it leads to arbitrage-free prices of credit-risk securities, which are consistent with prices quoted in the market. These features make the financial-pricing approach the method of choice in an environment where credit risk is actively traded and, in particular, for valuing credit instruments when the market for related products is relatively liquid. On the other hand, since financial-pricing models have to be calibrated to prices of traded credit instruments, they are difficult to apply when we lack sufficient market information. Moreover, in such cases, prices quoted using an ad hoc choice of some risk-neutral measure are more or less "plucked out of thin air."
The actuarial pricing approach is an absolute pricing approach, based on the paradigm of risk bearing: a credit product such as a loan is taken on the balance sheet if the spread earned on the loan is deemed by the lender to be a sufficient compensation for the risk contribution of the loan to the total risk of the lending portfolio. Moreover, the approach relies mainly on historical default information. Therefore, the actuarial approach is well suited to situations where the market for related credit instruments is relatively illiquid, such that little or no price information is available; loans to medium or small businesses are a prime case in point. On the other hand, the approach does not necessarily lead to prices that are consistent (in the sense of absence of arbitrage) across products or that are compatible with quoted market prices for credit instruments, so it is less suitable for a trading environment.
The authors also point out that as markets develop, more credit products are priced using market prices and the risk neutral methodology. This raises issues of consistency and uniformity across an institution, with the possibility that the same product may be priced differently by different units of a firm. Managing these issues requires a good understanding of the differences between market-based (risk-neutral) valuation and actuarial valuation.
The financial versus actuarial pricing distinction highlights an important dividing line for credit risk, maybe the most important for credit risk measurement. When a credit risk is traded, it makes sense to measure risk using those market prices and the distribution of prices. One should only use complex, default-based models when instruments are not traded, for example, for loans, some corporate bonds, counterparty exposure on derivatives, and so on.
11.9 Dynamic Reduced Form Models
We now turn to pricing credit-risky securities. The analysis of credit risk in this chapter has focused on credit risk management—measuring and using the P&L distribution for a portfolio or business activity over some (usually long) period. In this section, we change gears to focus on market pricing of credit-risky securities. We will see that these models apply to credit risk when such risk can be traded. As such, it moves away from the tools and techniques we have discussed in this chapter and moves more toward the arena of market risk that we discussed in earlier chapters.
The goal of this section is to introduce the idea, not to provide a comprehensive overview. The pricing of credit-risky securities is a large and growing area. Duffie and Singleton (2003) wrote a textbook devoted to the topic. McNeil, Frey, and Embrechts (2005) devote chapter 9 of their book to the topic. This section will do no more than provide the briefest introduction.
There have been two important changes in the markets for credit-risky securities over recent years. First, an increasing variety and volume of credit risks are being actively traded. Thirty years ago few credit-risky securities beyond corporate bonds were traded, and many bonds were only thinly traded. Loans, receivables, leases, all were held to maturity by institutions and virtually never traded. Now there is a wealth of derivative securities (credit default swaps prime among them), collatoralized structures, and loans that are traded. There have been huge transformations in the markets.
The second change has been in the pricing of credit risks. The development of the risk-neutral or equivalent martingale paradigm for pricing credit-risky securities has allowed investors to value credit risks, separate from other components such as interest rates. The breaking out of a security's component parts has made the pricing of credit more transparent, and has been a major factor facilitating the increase in trading of credit risks.
The growth of markets in credit risk has seen disruptions, most spectacularly during the 2007-2009 financial crisis that was related to the securitized mortgage markets. Such credit-related disruptions should not be blamed entirely on innovations and changes in the credit markets, however. Financial markets have managed to go through crises for ages, many credit-related and well before modern derivative securities. Barings Brothers went bust (the first time, in 1890) from over-exposure to Argentine bonds (particularly the Buenos Ayres [sic] Drainage and Waterworks Company—see Kindleberger (1989, 132) and Wechsberg (1967, ch. 3)). Roughly 1,400 U.S. savings and loans and 1,300 banks went out of business from 1988 to 1991 because of poor lending practices and particularly overexposure to real estate. (See Laeven and Valencia 2008 and Reinhart and Rogoff 2009, appendix a.4.)
Credit Default Swaps and Risky Bonds
I will explain the idea of dynamic reduced form models by developing a simple version of a model for pricing a single-name credit default swap (CDS). Although quite simple, this model gives the flavor of how such models work.
Outline for CDS
A CDS is the most basic credit derivative, one that forms the basis for various securities and is in many ways the easiest credit-risky security to model. (A more detailed discussion can be found in Coleman [2009]. See also McNeil, Frey, and Embrechts [2005], section 9.3.3.) Although CDS are often portrayed as complex, mysterious, even malevolent, they are really no more complex or mysterious than a corporate bond.
We discussed CDS in Chapter 3 where we showed how a standard CDS is equivalent to a floating-rate corporate bond (a floating rate note, or FRN) bought or sold on margin. We will cover some of the same material before we turn to the mathematics of pricing.
First, to see why a CDS is equivalent to a floating rate bond (FRN), consider Figure 11.17, which shows the CDS cash flows over time for a firm that sells protection. Selling protection involves receiving periodic payments in return for the promise to pay out upon default. The firm receives premiums until the maturity of the CDS or default, whichever occurs first. Since the premiums are paid only if there is no default, they are risky. If there is a default, the firm pays 100 - recovery (pays the principal on the bond less any amount recovered from the bond).

Figure 11.17 Timeline of CDS Payments (Sell Protection)
Reproduced from Figure 3.2 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

Now we can use an elegant trick. With any swap agreement, only net cash flows are exchanged. This means we can insert any arbitrary cash flows we wish, so long as the same amount is paid and received and the net is zero. Let us add and subtract LIBOR payments at each premium date, and also 100 at CDS maturity, but only when there is no default. These LIBOR payments are thus risky. But since they net to zero, they have absolutely no impact on the price or risk of the CDS. In Figure 11.18, Panel A shows the original CDS plus these net-zero cash flows. Panel B then rearranges these cash flows in a convenient manner.
The left of Panel B is exactly a floating rate bond (FRN). If no default occurs, then the firm selling protection receives coupon of (LIBOR + spread) and final principal at maturity. If default occurs, the firm receives the coupon up to default and then recovery. The combination in the right of Panel B looks awkward but is actually very simple: it is always worth 100 today. It is a LIBOR floating bond with maturity equal to the date of default or maturity of the CDS: payments are LIBOR + 100 whether there is a default or not, with the date of the 100 payment being determined by date of default (or CDS maturity). The timing of the payments may be uncertain, but that does not affect the price because any bond that pays LIBOR + 100, when discounted at LIBOR (as is done for CDS), is worth 100 irrespective of maturity.

Figure 11.18 CDS Payments plus Offsetting Payments = FRN - LIBOR floater
Reproduced from Figure 3.3 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

In other words, we have just proven, rather simply and without any complex mathematics, that a CDS (sell protection) is just a combination of long an FRN and short a LIBOR floater (worth $100):

By reversing the signs, we also have

This is extraordinarily useful because it tells us virtually everything we want to know about the broad how and why of a CDS.44
Pricing Model for CDS
We can now turn to pricing the CDS. A model for valuing a CDS is relatively straightforward. The cash flows for a CDS (sell protection) are:

 Receive

 Fixed coupon c as long as there is no default.

 Pay

 $100 less any recovery when (and if) default occurs.xs


Both sets of cash flows are risky in the sense that how long and whether they are paid depend on whether default occurs, and when, exactly, that default occurs.
These cash flows are as shown in Figure 11.17. If default were known to occur at a fixed time τ then valuation would be quite simple: Discount the fixed cash flows (receive c until τ, then pay 100 - recovery) using the equivalent martingale measure. The problem is that the time τ is random and not known. So we assume a distribution for the random default time, τ, and discount back, again using the equivalent martingale measure.
This is a reduced form model in the sense that the process governing default (the random time τ) is assumed rather than default being modeled as a result of underlying financial or economic processes. It is dynamic in the sense that the default time is modeled as a stochastic process in continuous time. The benefit of the reduced form approach is the substantial flexibility in the stochastic process governing default, and the simplicity of the relative pricing (risk-free or equivalent martingale) framework.
For this example, we assume that the random default time τ is a constant-hazard process. This will make the mathematics particularly simple. The constant hazard assumption means the probability of default in the next instant of time, conditional on not yet having defaulted, is constant and does not change over time. In other words, under the risk-neutral measure, the default time τ is exponentially distributed with constant hazard α:

If we assume that the risk-free rate is constant at r, then the present value of receiving the coupons c up to the random time τ is:

This assumes that coupons occur annually. If not, then we would have c·df, where df = day fraction = (days between payment)/360 or /365 depending on the currency and appropriate money market convention.
The PV of paying the loss upon default is the expectation of the loss (net of recovery) over the random default time. Say the loss is 100 and the recovery rate is fixed at δ. Then the loss net of recovery is 100(1 − δ) and the expected value is:

The total value of the CDS is

(11.28) 
Where df = day fraction (for example, ≈ 92.5/360 for quarterly USD, A/360)
This is a very simple formula. In fact, one that can be evaluated in a spreadsheet without difficulty. This assumes that when default occurs between coupon payment dates, no partial coupon is paid.45
Pricing Model for Risky Bond
The real power of this approach, however, is that it puts a pure credit derivative such as this CDS in the same framework as a more traditional corporate bond. Figure 11.19 shows the cash flows for a traditional bond that is subject to default: coupons at periodic times, payment of recovery upon default, and payment of principal if no default. These are not exactly the same cash flows as shown in Figure 11.17 (although close) but whether exactly the same or not, we can value them using the same framework.

Figure 11.19 Timeline of Payments for Risky Bond

We again assume that the risk-free rate is constant at r so that the present value of receiving the coupons c is, again:

The PV of recovery upon default is the expectation over the random default time of the recovery amount, 100·δ:

The PV of the principal is 100 times the probability default occurs after T, discounted at r:

The total value of the bond is

(11.29) 
Where df = day fraction (for example, ≈ 92.5/360 for quarterly USD, A/360)
This is a very simple formula. In fact, one that can be evaluated in a spreadsheet without difficulty.
Equation (11.28) gives the CDS and (11.29) the bond as functions of the underlying parameters. The underlying parameters are:

r = risk-free rate
α = default intensity
 δ = recovery rate

Example—Applying Market Pricing to CDS and Bond
Both the CDS and the bond depend on the same parameters, the same underlying process. This means that if we can value one instrument, we can automatically value the other. (The coupon and maturity date are characteristics of the particular instrument.) The risk-free rate r depends on wider market conditions, but the default intensity α and the recovery rate δ are specific to the particular firm, the particular issuer that we are looking at.
Corporate bonds are traded in the market and so we can get a market price for the PV. Consider a five-year bond with annual coupon 5 percent when the risk-free rate is 3.50 percent. If the bond is trading at par ($100) then we can use equation (11.29) to calculate values of parameters α and δ that would be consistent with this market price.46 If we calculate α, assuming δ = 40 percent, then we arrive at α = 2.360 percent.
Now let us turn to a CDS, say, a five-year CDS on the same issuer with annual coupon 1 percent. Equation (11.28) gives us the value of the CDS (receiving fixed coupon, paying out upon default), which in this case turns out to be −$1.8727.
The beauty and power of what we have done is to take a theoretical framework (the dynamic reduced form model that discounts uncertain cash flows under an equivalent martingale measure) and apply it to two different but related instruments (the CDS and the risky bond on the same issuer). By treating both instruments using the same pricing framework, we can take the market prices from the bond and apply this market pricing to the CDS. Using equation (11.29), we have separated out and separately priced the pure discounting (due to the risk-free rate r) and the risky discounting (due to the default and recovery parameters α and δ). We can then apply these to a related but different set of cash flows, the cash flows for the CDS.
What we have done is to convert a nontraded credit security, the CDS, into a market-priced security. Essentially, we have used a relative pricing paradigm to move the CDS into the market pricing and market risk category. In this sense, dynamic reduced form credit models should be thought of in the same category as pricing models for other traded instruments such as models for pricing swaps or options. They take market risk factors and translate to the P&L for the particular securities held by the firm. They apply to Step 1 ("Asset to Risk Factor Mapping") of the process for generating the P&L distribution discussed in Section 8.3 of Chapter 8. Although the instruments are credit sensitive, they do not require the techniques discussed in this chapter.
11.10 Conclusion
As I said early on, this chapter does not take a standard approach to discussing credit risk. I have focused heavily on the mathematics and the modeling required to build the P&L distribution, much less on the traditional techniques of credit measurement and management. I think this approach is justified on two grounds. First, the modeling required to build the P&L distribution for nontraded credit risks is simple in concept but difficult in practice. I have tried to lay out the conceptual framework and highlight the simplicity of the concepts while also stressing the difficulties and subtleties of building and implementing a practical credit risk system. Second, there are many texts that do a good job of discussing the more traditional approaches to credit risk. Readers can remedy any omissions without undue difficulty.
I do want to highlight, however, the wide range of credit risk topics not covered.
In general, credit risk management is composed of three components:

1. Measurement.
2. Setting reserves, provisions, and economic capital.
3. Other management areas: setting limits, portfolio management, managing people and incentives.

The primary focus of this chapter has been on determining the distribution for defaults, which is only the first component of measuring credit risk. Measurement means determining the profit and loss (P&L) distribution. The loss itself depends on default, exposure, and recovery:

Defaults have taken center stage because default modeling is the most complex component of credit risk models, and models differ primarily in their modeling of defaults and the process underlying defaults, not their modeling of exposures and recovery.
Measurement: Traditional Credit Analysis and Ratings
Traditional credit analysis is devoted to analyzing individual firms, loans, and other credit risks with the goal of assessing the likelihood of default and how costly it would be were it to occur. It usually takes the form of assigning a credit rating to a credit risk. The credit rating may reflect only the likelihood of default or a combination of the probability of default and the severity of loss. In this sense, traditional credit ratings map to the default probabilities of the more formal models discussed in this chapter, or a combination of probability and loss given default. In some cases, the mapping is explicit, as in CreditMetrics, where a firm's ratings category determines the transition (and default) probability, and in the default probability estimates by rating category from McNeil, Frey, and Embrechts (2005), discussed earlier in Section 11.7.
Most rating systems are based on both quantitative and qualitative considerations, but usually not formal models of the type discussed in this chapter. Traditional credit analysis generally focuses on individual names and not portfolio interactions directly, and thus could be termed single-name credit analysis.
In practice, there are a huge variety of methods and an extensive literature devoted to single-name credit analysis and ratings systems. There are a number of ratings agencies that rate publicly traded issues, with Standard and Poor's, Moody's, and Fitch being the most well known. Private sector issuers pay ratings agencies to rate a bond issue, and the ratings agencies then make the ratings available to the public. The ratings are relied on by many investors and regulators. Almost all public issues in the United States are rated by one or more of the ratings agencies, and many international issues and issuers (including sovereign issuers such as the United States or the Greek government) are also rated.
Many issues and issuers that a bank is exposed to will not have public ratings, and so financial institutions often develop their own internal ratings to supplement the publicly available ratings. Crouhy, Galai, and Mark (2000) devote a full chapter (chapter 7) to both public and internal credit rating systems while Crouhy, Galai, and Mark (2006) split the topic into two chapters, one covering retail credit analysis and the other commercial credit analysis.
Measurement: Exposure and Recovery—Types of Credit Structures
Exposure and recovery are critical to measuring credit losses but have not been covered extensively in this chapter. Exposure refers to the amount that can potentially be lost if default were to occur, and recovery to the amount (or proportion) of the potential loss that is recovered. They combine to give the loss given default (LGD):

The current exposure is often itself difficult to measure. For example, simply collecting data on current exposures can be challenging (as mentioned in Section 11.1). The problem becomes even more difficult, however, because what matters is the exposure at the time of default, not the current exposure. Since default is in the future and itself uncertain, exposure at default can be doubly difficult to measure.
There is wide variation in the types of exposure. Marrison (2002, ch. 17) discusses various credit structures:

 Credit exposures to large corporations

 Commercial loans
 Commercial credit lines
 Letters of credit and guarantees
 Leases
 Credit derivatives

 Credit exposures to retail customers

 Personal loans
 Credit cards
 Car loans
 Leases and hire-purchase agreements
 Mortgages
 Home-equity lines of credit

 Credit exposures in trading operations

 Bonds
 Asset-backed securities (embodying underlying exposures to corporations or retail customers from things such as loans, leases, credit cards, mortgages, and so on)
 Securities lending and repos
 Margin accounts
 Credit exposures for derivatives (noncredit derivatives such as interest rate swaps)
 Credit derivatives
 Trading settlement


For many instruments, exposure will vary over time and with changes in markets. Consider an amortizing corporate bond with five-year final maturity. Because of amortization, the notional value of the bond will go down over time in a predictable manner. For any notional, however, the value of the bond (and thus the exposure or amount at risk of loss) will vary with the level of market risk-free interest rates: lower interest rates mean lower discounting and higher present value. A common way to represent this is by measuring the expected exposure and the maximum likely exposure (MLE). For the bond, whose value depends on interest rates, the expected exposure could be taken as the value implied by the forward curve (or possibly the notional). The MLE could be taken as the exposure at the 95th percentile of the interest rate distribution. The situation for an amortizing bond might be as shown in Figure 11.19, Panel A.

Figure 11.20 Expected and Maximum Likely Exposure for Amortizing Bond and Two Interest Rate Swaps
Reproduced from Figure 5.21 of A Practical Guide to Risk Management, © 2011 by the Research Foundation of CFA Institute.

For an interest rate swap, and other derivatives such as options, the credit exposure will be more complicated. The present value for a new at-market swap will be zero and so there is no credit exposure—if the counterparty defaulted and walked away, there would be no loss in market value. Over time and as interest rates change, however, the market value of the swap may become positive or negative. If negative, then again, there is no credit exposure—if the counterparty walked away, there would be no loss in market value. When the market value is positive, however, the credit exposure will equal the market value—if the counterparty disappeared, the loss would be equal to the market value of the swap.
The exposure for an interest rate swap will start out at zero but may then become positive, or remain at zero. The exposure will be random over time, moving between zero and some positive value. It is still possible, however, to calculate the expected and the maximum likely exposures. The expected exposure could simply be taken as the value of the swap traced out along the forward curve. This might be either positive (shown in the left of Panel B of Figure 11.19) or negative (the right of Panel B of Figure 11.19—note that the exposure will actually have discrete jumps on coupon dates but these are not shown in the figures). The maximum likely exposure could be taken as the 95th percentile of the forward curve distribution. This would be positive for virtually any swap, as shown in Panel B of Figure 11.19.
Marrison (2002, ch. 17) discusses the concept of maximum likely exposure more extensively, and has useful diagrams for many credit structures.
The expected or the maximum likely exposure could be used with the stylized default model discussed in Section 11.3 to produce a distribution of losses. Indeed, commercial products often do something akin to this. (CreditMetrics uses something close to the expected credit exposure. MKMV has the option to use market prices [forward prices] to calculate exposures, and this gives roughly the expected exposure.)
Using the expected and maximum likely exposure, however, is only an inexact approximation. In reality, the exposure at default will generally be random. Considering an interest rate swap again, the actual exposure may be zero or positive, and will change as default-free interest rates change randomly over time. Combining random default processes with random variation in underlying market variables is difficult and not commonly done.47 This is a major issue to be addressed in future credit risk model development. The problem is particularly important for instruments such as interest rate swaps in which the exposure changes substantially with market variables (interest rates for swaps). The issue will be less important for instruments such as short-dated loans, in which the exposure is primarily due to principal at risk.
Reserves, Provisions, and Economic Capital
Once the distribution of defaults and losses (the P&L distribution) has been measured, it can be used. The first place it can be used is in the determination of reserves, provisions, and economic capital. This was discussed in briefly Section 11.3. In fact, the topic deserves a deeper discussion, but it also should be integrated with overall firm risk, not limited to credit risk alone.
Other Credit Risk Management Topics
Beyond the specific issues of reserves and economic capital, there are the wider issues of risk management—how to use the information on risk to manage the business. Issues such as setting limits, capital allocation, managing people, setting compensation, and other incentives are not specific to credit risk. It would be a mistake to discuss such issues in the context of credit risk alone.
Credit Mitigation
There is a large area of credit enhancement, mitigation, and hedging techniques. These range from traditional techniques such as bond insurance and mark-to-market to recent innovations such as credit default swaps. Crouhy, Galai, and Mark (2000) devote chapter 12 of their book to the topic; Crouhy, Galai, and Mark (2005) also cover it in chapter 12 of that book.
In the end, credit risk is a huge task with many components. Ernest Patakis is indeed correct to say that one of the most dangerous activities of banking is lending. This chapter has introduced many of the topics but this treatment cannot be taken as definitive.
Appendix 11.1: Probability Distributions
Binomial
The binomial distribution counts the number of successes in a sequence of independent yes/no or succeed/fail (Bernoulli) trials. With p = probability of success, q = 1 − p = probability of failure, the probability of k successes out of n trials is:

where 

For q = 0.01, n = 100, P[k = 0] = 0.366, P[k = 1] = 0.370, P[k = 2] = 0.185, P[k ≥ 3] = 0.079
Poisson
The Poisson distribution gives the probability of observing j events during a fixed time period, when events occur at a fixed rate per unit of time and independently over time. If the intensity (or average rate per unit of time) is λ, then the probability that j events occur is:


Gamma
A gamma random variable is a positive random variable with density

Negative Binomial
The negative binomial is a discrete distribution (like the binomial taking values 0, 1, 2,...). The initial definition arises, like the binomial, when considering Bernoulli trials each of which may be either a success (probability p) or failure (probability 1 − p). Unlike the binomial (within which we consider a fixed number of trials), for the negative binomial, we keep counting until there have been r successes. Then the probability of k failures (before r successes) is:

where

The definition in various places can differ:

 It may be stated in terms of k successes before r failures
 It may be stated in terms of the total number of trials (k + r) before a fixed number of successes or failures
 The binomial coefficient may be expressed as  instead of  (examination of the definition of the binomial coefficient will show that these two expression are in fact identical)

For our purposes, however, we use an extended version of the negative binomial, sometimes called the Polya distribution, for which r, which we will now call α, is real-valued. (For the original negative binomial, r must be an integer > 0.)
This definition of the negative binomial is essentially the same:

except that the coefficient is the extended binomial coefficient defined by:

Notes
1. An example of using the P&L distribution in managing a business would be the CFO of a bank setting the following (cf. Marrison 2002, 229):

 Provisions—expected losses over a period—the mean of the distributio
 Reserves—loss level for an unusually bad year—may be set at the 5 percent quantile (VaR) of the loss distribution
 Capital (also termed economic capital to distinguish it from regulatory capital)—loss level for an extraordinarily bad year, required to ensure a low probability of default—may be set at the 0.1 percent or 0.03 percent quantile (VaR) of the loss distribution.

2. I would call this a structural approach except that McNeil, Frey, and Embrechts (2005) have used the term to highlight a useful distinction between types of credit risk models, as can be seen in Section 11.4
3. More generally, this would be extended to include more general transitions between credit states, with the transition from solvent to default being a simple special case. This is discussed more in the section titled "Credit Migration and CreditMetrics."
4. Marrison (2002, ch. 17) has a nice discussion of exposures from a variety of products that a bank might deal in.
5. Following Lehman's 2008 default, recovery on CDS contracts covering Lehman bonds was about 10 cents on the dollar. Before the settlement of those contracts, it was usually assumed that recovery would be on the order of 40 percent, not 10 percent.
6. Note that the probability of actually losing money outright from this portfolio is low (if the assumptions about the underlying loans were valid). It might be reasonable to measure income relative to costs, where costs might be the original loan plus some cost of funds. If the cost of funds were 5 percent (versus promised interest of 6.5 percent), then average actual income less costs would be $9,350.
7. See, for example, Crouhy, Galai, and Mark (2003, table 8.3) in which they cite Carty and Lieberman (1996); or see Duffie and Singleton (2003, table 4.2).
8. The approximation in Hull, which Hull credits to Drezner 1978 (with Hull correcting a typo in Drezner's paper) produces slightly different values from Mathematica's Binormal Distribution, particularly in the tails. I presume the Hull and Drezner approximation is less accurate. See http://finance.bi.no/bernt/gcc_prog/recipes/recipes/node23.html for an implementation of the Hull and Drezner algorithm in C.
9. For BBB, p* = 0.0023 and default correlation = 0.00149, while for B, p* = 0.0503 and default correlation = 0.0133.
10. This is a very simple form of a factor structure, with a single common factor F that has the same effect on all firms. In practical applications, there may be more than one common variable F, and individual firms may be affected by the common factors in different ways (each firm may have its own coefficient B). The important point, however, is that there will be a small number of factors that are common to a large number of firms. Such a common factor structure models common economic or industry factors (either observed or latent) but does not capture contagion that alters perceptions following an initial default (e.g. heightened investor scrutiny of corporate accounts following Enron's collapse). Nonetheless, most practical model implementations use a form of common factor structure and do not model contagion directly.
11. The "dependent" line in Panel B of Figure 11.5 reproduces Figure 11.1 and is close to McNeil, Frey, and Embrechts (2005, fig. 8.1).
12. This appears to conflict with the conclusion of McNeil, Frey, and Embrechts (2005, section 8.3.5), which appears to show substantial differences between using a multivariate normal versus Student-t critical variable. This is due to the embedded (nonlinear) dependence of the usual multivariate Student-t. Such dependence is better treated as additional dependence, or mixing, in addition to that explicitly modeled by the common factors, rather than a result of the functional form of the threshold variable.
13. One can measure the portfolio size at which one-half and three-quarters of the logarithmic reduction in standard deviation has been realized by calculating n* = (1 − ρ)/(ρ0.5 − ρ) and n* = (1 − ρ)/(ρ0.75 − ρ). For ρ = 0.3, this is 2.8 and 6.6 assets, while for ρ = 0.01, it is 11.0 and 45.8 assets.
14. We will discuss more fully an exercise conducted by Crouhy, Galai, and Mark (2000, ch. 11) that compares a variety of industry models.
15. I need to clarify a bit. For credit risk, we need to use complicated models to generate the distribution of defaults, the underlying factor that drives the loss distribution. For market risk, we generally do not need to use complicated models to generate the market risk factors—those can be observed. We may, however, need to use complicated pricing models to translate those market risk factors into the prices of the instruments we actually own. For example, we would need to use some sort of option model to price a bond option given the underlying yields.
16. Much of this distinction draws on McNeil, Frey, and Embrechts (2005, section 8.1.1).
17. The Merton model is laid out in McNeil, Frey, and Embrechts (2005) section 8.2, and Crouhy, Galai, Mark (2000, ch. 8 appendix 1, and ch. 9 sections 2 to 4).
18. The importance and implications of using the physical measure, with μ, versus the risk-free measure, with r, is discussed in Section 11.8.
19. KMV started as a private company named after its founders Kealhofer, McQuown, and Vasicek, and was subsequently acquired by Moody's and is now named Moody's KMV. cf. www.moodyskmv.com.
20. MKMV does not use the simple one-period Merton model but rather a continuous time extension due to Oldrich Vasicek and Stephen Kealhofer (two founders of the original KMV) known as the Vasicek-Kealhofer (VK) model. The firm's equity is a perpetual option with the default point acting as the absorbing barrier for the firm's asset value. The ideas are the same, however. Also note that instead of probability of default, MKMV uses, and has trademarked, the term Expected Default Frequency (EDF).
21. In practice, MKMV adjusts the expression 11.10 for time by including average growth in assets (μT) and scaling volatility by √T, so, in practice, the two formulations are virtually the same.
22. We will see shortly, however, that a credit migration model can be reformulated as a threshold model.
23. See CreditMetrics—Technical Document, published originally by RiskMetrics in 1999, republished 2007, at www.riskmetrics.com. Crouhy, Galai, and Mark (2000, ch. 8) discuss CreditMetrics in some detail. McNeil, Frey, and Embrechts (2005, section 8.2.4) have a concise discussion and also show how a credit migration model can be embedded in a firm-value model. Marrison (2002, ch. 18) discusses credit migration and migration matrixes.
24. Crouhy, Galai, and Mark (2000, ch. 7) and Crouhy, Galai, and Mark (2006, ch. 10) provide a particularly detailed and useful explanation of public credit ratings provided by S&P and Moody's, listing the ratings categories and definitions. They also discuss internal ratings systems often used by banks or other financial institutions. Marrison (2002, ch. 19) discusses credit ratings. Information on ratings categories and definitions can also be found on ratings agencies' websites: www.standardandpoors.com/ratings/en/us/, www.fitchratings.com.
25. From Standard & Poor's CreditWeek (April 15, 1996) quoted in RiskMetrics (1997/2007).
26. It is interesting to consider that, since the default distribution in CreditRisk+ provides a good approximation to that from MKMV and CreditMetrics, the distribution and techniques used in CreditRisk+ could have wider applicability as a computationally efficient method for solving credit risk models.
27. In fact, not all firms have to depend on all the same variables, but there must be some common variables across some groups of firms.
28. Credit Suisse Financial Products (1997, appendix A2) asserts that default rates cannot be constant. They point out that the standard deviation of observed default counts (for a portfolio of firms) is higher than that predicted by a Poisson model with (independent) fixed default rates, what is called over-dispersion in the actuarial literature. They then claim that the assumption of fixed default rates is incorrect and default rates must be variable. This is not the best way to state the issue. Over-dispersion does indeed imply that default rates (in a Poisson model) cannot be constant, but variable default rates alone do not imply over-dispersion. The important issue is dependence versus independence of the default process across firms. Default rate variability that is common across firms will produce dependence and over-dispersion in the default count distribution, but variability in default rates that is idiosyncratic or firm-specific quickly averages out and produces little over-dispersion for even modest-size portfolios. Also, it is important to carefully distinguish between default intensities (unobserved parameter of the Poisson process, also termed default rates) and default counts (observed counts of defaults), and Credit Suisse Financial Products (1997) does not always do so. Default counts for a portfolio may be expressed as a percent of the number of firms in the portfolio and termed default rate, and such observed quantities may be used to estimate default intensity parameters, but counts and intensities are conceptually distinct.
29. This follows the description of CreditRisk+ in McNeil, Frey, and Embrechts (2005 section 8.4.2), but simplified to a univariate factor.
30. Credit Suisse Financial Products (1997) and other authors do not always distinguish between the default intensity (which is a parameter of the model or an input) and the finite-sample default rate (which is a finite-sample statistic of the model or an output) and this can lead to confusion. Crouhy, Galai, and Mark (2000) is an example. On pp. 405-406 (using table 8.3, 326) they claim that the standard deviation of the observed default rate is higher than would be implied by a Poisson process with fixed intensity. While their conclusion may be right for other reasons, their analysis is wrong, with two fundamental flaws. First, there is an outright computational error. For single-B obligors, from their table 8.3, 326, E(default rate) = 7.62 percent = 0.0762. If this were the fixed Poisson intensity λ, then √λ = √(7.62 percent) = √0.0762 = 0.276 = 27.6 percent; they claim instead √λ is 2.76 percent. (Their mistake is in taking √7.62 = 2.76 and applying the percentage operation outside the radical rather than inside. This elementary error is not, however, representative of the overall high quality of Crouhy, Galai, and Mark [2000].) Second, and more subtly, they compare the standard deviation of finite-sample default rates (a finite-sample statistic) to the standard deviation of the Poisson intensity λ (a parameter of the model). For a single firm with Poisson-distributed defaults and fixed intensity λ, the standard deviation of the count (number of defaults) is √λ. For n identical firms, it is √(nλ). The standard deviation of the observed default rate (count divided by sample size n) is √(λ/n). Because the observed default rate is a finite-sample statistic, its standard deviation will vary with n (here as 1/√n) as for any finite-sample statistic. The bottom line is that the standard deviation of the observed finite-sample default rate is not √λ. Table 8.3 does not give the sample size and we therefore cannot calculate what would be the standard deviation of the finite-sample default rate for a Poisson model; their comparison is meaningless. (As an exercise, we can calculate what the standard deviation of the finite-sample default rate would be for various sample sizes. For single-B obligors, from their table 8.3, 326, E(default rate) = 0.0762. For a sample of 20 and a Poisson with fixed intensity λ = 0.076, the standard deviation of the default rate would be √(0.0762/20) = 0.062, while for a sample of 100, it would be √(0.0762/100) = 0.0276. The observed standard deviation of the finite-sample default rate is actually 0.051 [again, from their table 8.3]. This tells us that for a sample size of 20, the observed standard deviation would be too low relative to a Poisson with fixed intensity, while for a sample of size 100, it would be too high. Without knowing the sample size, however, we cannot infer whether 0.051 is too high or too low.)
31. The variance of the count is kn, so the variance of the default rate is kn/n2 = k/n.
32. The variance of the count is nk · (1 + σ2nk), so the variance of the observed rate is k/n+k2σ2, cf. Equation 11.21. Ignoring the finite-sample nature of the observed default rates does make a difference. Data in table 8 of Gordy (2000) derived from Standard and Poor's published data show that for single-B issuers, the average number of issuers is about 240, the average default rate is about 0.0474, and the variance of observed default rate is about 0.000859. This implies k/n is about 0.000195, k2σ2 about 0.000664, implying σ2 ≈ 0.296. Ignoring the k/n term would give σ2 ≈ 0.382.
33. This is the reason for using Poisson models, because the sum of Poissons does have a simple distribution. Nonetheless, since the pi will all be roughly the same size (all small since the probability of default is low) the distribution will tend toward normal as n gets large (by the law of large numbers).
34. This is a single common factor that affects all firms in exactly the same manner. An example might be a recession that makes the business conditions worse and increases the probability of default for all firms. In general, there can be more than one factor and the βi can be different across firms, so that some firms could be positively affected, others negatively affected, and some not affected at all.
35. Unless all firms have the same default probability, in which case the distribution is binomial, but this will never be the case in practical applications.
36. Default data are reconstructed from published default rates in Brand and Bahr (2001, table 13, pp. 18-21).
37. For single-A issuers, there should be less than one default per year for a sample of 1,000 issuers, and from table 8 of Gordy (2000), it appears the annual sample is on the order of 500.
38. Gordy (2000) normalizes by setting q = 1, but also investigates q = 1.5 and q = 4.0. I do not have any intuition for what is the appropriate choice and simply pick wA1 = 1 for convenience.
39. Both the Poisson and the negative binomial distributions are analytic, and the convolution involves a simple looping over possible number of defaults. For example, to calculate the probability of two defaults for the Poisson/negative binomial convolution, we sum the following terms: P[Poiss = 0]*P[NB = 2] + P[Poiss = 1]*P[NB = 1] + P[Poiss = 2]*P[NB = 0]. The number of terms in the sum becomes larger as the number of possible defaults becomes larger, but the number of calculations is orders-of-magnitude less than for simulating a Bernoulli mixture.
40. The standard deviations of the distributions are close - BBB is 19.2 for the Bernoulli mixture and 18.9 for the Poisson; CCC is 14.7 for the Bernoulli and 15.0 for the Poisson mixture.
41. In our example, all loans are identical, so all loans contribute equally to the economic capital, but for realistic applications, this will not be the case.
42. McNeil, Frey, and Embrechts (2005, section 9.3), have a nice alternative example.
43. The most important condition is that markets are complete in the sense that future payouts (say the payouts for the stock and bond in equations (11.5) and (11.6)) can be replicated by trading in current assets. See McNeil, Frey, and Embrechts (2005, section 9.3); Duffie (2001); and Bingham and Kiesel (1998).
44. The equivalence is not exact when we consider FRNs that actually trade in the market, because of technical issues regarding payment of accrued interest upon default. See Coleman (2009).
45. In fact, CDS traded in the market often involve partial payment of coupons—see Coleman (2009).
46. In fact, it is not possible to separate α and δ. The standard practice is to fix δ, say at 30 percent or 40 percent, and then calculate α conditional on the value of δ.
47. Crouhy, Galai, and Mark (2000) emphasize this more than once—see pp. 343, 411.









Chapter 12
Liquidity and Operational Risk
Liquidity and operational risk are extremely important, but in some respects more difficult to analyze and understand than market risk or credit risk. For one thing, they are both hard to conceptualize and difficult to quantify and measure. This is no excuse to give them short shrift but it does mean that the quantitative tools for liquidity and operational risk are not as developed as for market risk and credit risk. This also means that judgment and experience count—it reinforces the idea that risk management is management first and foremost.
I cover liquidity and operational risk in less depth than market and credit largely because they are at an earlier stage of development, and not because they are any less important. In fact, both are critically important. Issues around liquidity risk come to the fore during periods such as the crisis of 2007-2009. The events during that period reflected the bursting of an asset bubble, but the events were combined with, or more correctly generated, a consequent liquidity crisis.1
12.1 Liquidity Risk—Asset versus Funding Liquidity
When we turn to liquidity risk, we find that there are actually two quite distinct concepts. First there is asset liquidity risk (also known as market or product liquidity). This "arises when a forced liquidation of assets creates unfavorable price movements" (Jorion 2007, 333). The second is funding liquidity risk (also known as cash-flow liquidity). This "arises when financing cannot be maintained owing to creditor or investor demands" (Jorion 2007, 333); funding liquidity risk can also be thought of as a maturity mismatch between assets and liabilities.
Although asset and funding liquidity go by the same name, they are fundamentally different, and it is truly unfortunate that they are both called liquidity. They are related in the sense that when funding liquidity becomes an issue, then asset liquidity is invariably important. But this is no different from, say, market risk and asset liquidity risk; when there are big market movements we may need to rebalance the portfolio, and then asset liquidity becomes important and possibly contributes to further market losses.
Although going by the same name, the sources of asset and funding liquidity risks, the methods of analysis, the responses and management of the two, are so different that it I think it is more fruitful to treat them as distinct. At the end, we can return to examine the connections between them. In fact, these connections will be easier to understand after we have treated them as separate and distinct.
For both asset and funding liquidity risk, we need to examine some of the institutional and operational details of the portfolio and the firm. As Jorion (2007) says: "Understanding liquidity risk requires knowledge of several different fields, including market microstructure, which is the study of market-clearing mechanisms; optimal trade execution, which is the design of strategies to minimize trading cost or to meet some other objective function; and asset liability management, which attempts to match the values of assets and liabilities on balance sheets" (p. 335).
Before discussing asset and funding liquidity on their own, we need to think about a fundamental point: "What questions are we asking?" In earlier chapters, I have emphasized the P&L distribution—stressed that measuring and understanding risk means measuring and understanding the P&L distribution. This is still true for liquidity risk but we have to examine our assumptions, think a little more about what we are looking at and why.
In earlier chapters, we implicitly assumed that we are interested in the day-by-day P&L for the ongoing business (or week by week, or whatever). This is appropriate and correct. Consider our sample portfolio where we hold $20 million of the U.S. 10-year Treasury and €7 million of futures on the CAC equity index. In calculating the value today, in examining the history of market yields and prices, in estimating potential future P&L, we are always considering this as an ongoing business, a continuing and relatively stable portfolio. We are holding the portfolio for a period, not liquidating it every day and reconstituting it the next morning. Using midmarket prices, ignoring bid-offer spreads and the market impact of selling a position have been reasonable approximations. That does not mean such considerations are unimportant, simply that they have not been the primary focus of our attention. The questions we have been asking (even if we have not been explicit about this) are: What is the day-by-day P&L? How high or low could it be? Where does it come from, and what contributes to the variability of the P&L? We have been focused on the ongoing running of the business, not on winding down the portfolio.
Such questions, however, questions about the P&L if we were to wind down the portfolio, are important. We should ask questions such as: Under what conditions might we wish to substantially alter the composition of the portfolio? Under what conditions might we be forced to wind down the portfolio? What would be the cost of altering or winding down the portfolio? What would be the source of those costs, and would those costs change depending on the trading strategy employed for altering or winding down the portfolio?
Asking these questions impels us to look at liquidity issues. We also need to change focus somewhat. For asset liquidity, we will focus on questions such as how much it might cost to completely unwind the portfolio, how long such an unwind might take, and what are optimal methods for executing changes in the portfolio. Our focus is still on the P&L but it is possibly over a different time horizon, under circumstances different from the day-to-day, normal operations of the business. We are still asking questions about the P&L distribution but the questions are different from those we ask about standard market risk. It is hardly surprising, therefore, that both the tools we use and the answers we get will be somewhat different.
For funding liquidity risk, we focus on questions such as how the asset-liability structure of the firm might respond to different market, investor, or customer circumstances. Once again, we are still interested in the P&L, but we are not asking what the P&L might be during standard operations but how it might be affected by the liability and funding structure of the firm.
We could conceive, theoretically, of building one complete, all-encompassing model that would include the P&L when the firm is an ongoing business with no big changes in the portfolio, how asset liquidity considerations enter when there are big changes or unwinds in the portfolio, and how all this affects or is affected by the asset-liability structure of the firm. Such a goal may be commendable, but is highly unrealistic. It is better to undertake a specific analysis that focuses on these three issues separately, and then use these analyses to explore and understand the interaction of the three types of risk. Essentially, I am arguing for analyzing asset and funding liquidity risk with a different set of measures and tools from what we use for standard market or credit risk. It is more fruitful to develop a different set of measures rather than trying to adjust standard volatility or VaR. At the same time, we want to clearly delineate the relation with standard volatility and VaR measures.
12.2 Asset Liquidity Risk
When we turn to asset liquidity risk, the central question is: What might the P&L be when we alter the portfolio? Most importantly, what is the P&L effect due to liquidity of different assets?
In earlier chapters, when we examined the P&L distribution, we ignored any effect of bid-offer spread, the impact on market prices of buying or selling our holdings, or over what period we might execute a transaction. We assumed that all transactions were done instantaneously and at midmarket. This was fine because our main focus was market movements and we could afford to ignore transactions costs. Here we change gears and focus primarily on those transactions costs.2
Costs and Benefits of Speedy Liquidation
Liquidity and transactions costs generally affect the P&L through two mechanisms. First is the bid-offer spread. Virtually any asset will have a price at which we can buy (the market offer) and a lower price at which we can sell (the market bid). When we liquidate, we go from the midmarket (at which we usually mark the positions) to the worse of the bid or offer. Illiquid assets will be characterized by a wide spread. Furthermore, spreads may vary by the size of the transaction and the state of the market. A transaction that is large relative to the normal size or that is executed during a period of market disruption may be subject to a wider bid-offer spread.
The second mechanism through which liquidity and transactions costs enter is the fact that market prices themselves may be affected by a large transaction. Trying to sell a large quantity may push down market prices below what the price would be for a small or moderate-size transaction.
These two effects may be summarized in a price-quantity or price impact function:
(12.1) 
where q is the quantity bought or sold (say, the quantity transacted in one day). The function (12.1) might look like Figure 12.1 Panel A, where the bid-offer spread is $0.50 for quantity up to 50,000 shares but widens for larger-size transactions—the bid price goes down and the offer price goes up as the quantity transacted increases. (The change is shown as linear, but this, of course, is not necessary.)

Figure 12.1 Price Impact Function—Price and Percent Terms

Panel A shows the change in bid and offer prices as symmetric but this need not be the case. It may be that a large sale pushes the market price down enough to have a significant price impact on both the bid and the offer. This is shown in Panel B, where both the bid and the offer go down in response to a large sale. We can think of Panel A as a widening of the bid-offer spread, and Panel B as an actual change in the market price. It does not really matter in the end whether we think of the impact of a large transaction as a widening of the spread or a changing of the price. If we are selling, it is only the price at which we sell, the market bid, that we care about. Whether a change in price is the result of market makers widening the spread or altering a midmarket price is irrelevant from the perspective of what price we face.
For actual use we may wish to express the price impact in percentage terms, and as a function of the quantity measured in dollars or euros rather than number of shares:
(12.2) 
Such a change, however, is simply a matter of units.
Once we have the price impact function, (12.1) or (12.2), we can examine the cost of liquidating part or all of the portfolio. If we hold W dollars of the asset (and using (12.2)), the cost of liquidating the portfolio in one day is W k(W). If we liquidate over n days, selling W/n per day, the cost per day is (W/n)k(W/n), and the total cost is:

If the price impact function were linear, k(W) = k0W, then the cost of liquidating would be:
(12.3) 
Clearly, the cost of transacting over a shorter period will be higher than over a long period, but there is a trade-off. Transacting over a longer period means that market prices may move against us, generating market losses. The key is to assess the trade-off between transacting quickly versus slowly. If we transact quickly, we pay a high cost but we avoid the risk that market prices will move against us and generate a loss on the portfolio. If we transact slowly, we pay a lower cost but there is a higher chance markets will move against us.
To measure how much markets may move against us we need to think about the P&L distribution over one day, two days, and so forth. As usual, we assume that P&L is independent from one day to the next, so we can add variances (which gives the standard square-root-of-t rule for the volatility). But here we do not add the original portfolio variances because, if we are selling off part of the portfolio each day, the portfolio is decreasing in size day by day.
Let us assume that we transact at the end of a day, so that if we liquidate in one day, we are subject to one day's worth of volatility; if we liquidate in two days, we are subject to two days' volatility, and so on. If we are liquidating over n days equally, we will be adding:

day one:          Variance = σ2
day two:          Variance = (1 - 1/n)σ2
          ...
day n:            Variance = (1 - (n - 1)/n)σ2

where σ is the original portfolio volatility. These terms sum to:
(12.4) 
Equation (12.4) shows us that the volatility of a portfolio that is being liquidated uniformly over a given horizon grows more slowly than the volatility of the original portfolio. A portfolio that is liquidated over 30 days has roughly the same volatility as the original portfolio held for 10 days.
This is the volatility assuming the portfolio is liquidated evenly over a period. We could examine alternate assumptions but the idea remains the same: the portfolio variance falls over the period, and the total variance over the period is the sum of the daily variances. In the following I will assume even liquidation. This is clearly a simplification but is valuable for illustration and for building intuition.
Evaluating Liquidation over Various Horizons
We have laid out the cost-of-liquidation equation (12.3), and the effect on the portfolio volatility equation (12.4) for various horizons. We now have the building blocks we need and we can turn to evaluating the trade-offs of liquidation over various horizons. But there is a fundamental issue here. We cannot directly compare the cost from equation (12.3) versus the volatility from equation (12.4). The cost shifts down the mean of the distribution. The volatility measures the spread of the distribution. One of the first things we learn in finance is the difficulty of evaluating the trade-off between changes in mean returns (in this case, the cost of liquidation over various horizons) versus variance or volatility (the increase in volatility due to more leisurely liquidation).
In other areas of risk measurement, we invariably compare P&L distributions by comparing their volatilities or VaRs—higher volatility or VaR means higher risk. We cannot do that here. In other situations, the distributions have zero mean (or close enough that it doesn't matter much). We can ignore the mean. Here we cannot because the whole issue is that the liquidation shifts down the mean of the distribution. We need to look at the distributions themselves to appropriately evaluate the trade-off between speedy versus leisurely liquidation.
Simple Example—€7 million CAC Position
Let us consider a simple example portfolio—a single position of €7 million futures on the CAC equity index ($9.1 million at the then-current exchange rate). The CAC futures is actually very liquid and so not a good example, but let us say, for the purposes of argument, that we actually have an illiquid total return swap (TRS). Let us also assume that we have estimated the price impact function (in percentage terms, equation (12.2)) as:

In other words, if we sell $910,000 in one day, the price impact is 1 percent and the cost is $9,100. If we sell $9,100,000, the price impact is 10 percent and the cost is $910,000. The cost of selling an amount w is:

The cost of selling the full position in n days, from equation (12.3) is:

The original daily volatility of this position (the full €7 million or $9.1 million) is $230,800. The volatility of liquidating the position over n days is given by equation (12.4) with σ = 230,800.
Table 12.1 shows the costs and volatilities for liquidating this position over selected horizons. The cost of liquidating in one day is $910,000 while the cost over two days is half that, $455,000. The volatility grows according to equation (12.4), from $230,800 for one day to $258,000 for two days, and so on.
Table 12.1 Costs and Volatility for Liquidating Hypothetical Portfolio over Various Periods.



No. Days
Cost $
Volatility $




1
910,000
230,800


2
455,000
258,000


5
182,000
342,300


10
91,000
452,900


31
29,350
759,800


Remember that the cost represents a shift down in the mean of the P&L distribution and the volatility measures the dispersion of the distribution. Figure 12.2 shows the distributions corresponding to the numbers in Table 12.1. In Panel A, we start with the P&L distribution with no liquidation costs. The dotted line in Panel A is the P&L distribution for the ongoing business—simply the usual distribution with the usual one-day volatility ($230,800 in this case) and zero mean. We are interested, however, in the solid line in Panel A that shows the distribution for one-day liquidation. The distribution is shifted to the left by the single-day liquidation costs, giving a distribution with a lower mean. The introduction of these liquidation costs shifts the whole distribution to the left, so we have a distribution with the same volatility but a mean of -$910,000. (A vertical line is drawn in to show the mean at -$910,000.)

Figure 12.2 P&L Distributions for Various Liquidation Periods

Now that we have the distribution for one day, we can start to compare across days. Panel B shows the distributions for one, two, and five days. The distribution for one-day liquidation is shifted far to the left (low mean) but with a relatively small volatility. The distribution for two days is shifted less to the left, but with a somewhat higher volatility (wider distribution). The distribution for five days is shifted even less to the left but is wider (more dispersion) than for the distributions for either one or two days.
With Figure 12.2, we can ask the question: What is the optimal liquidation horizon? What trade-off should we choose between the high costs of speedy liquidation versus the increased dispersion of leisurely liquidation?
In fact, there is no definitive answer. There is no single number that gives us the answer, and the definition of a "liquidity adjusted VaR" is simply not appropriate. (See Jorion 2007, 344 for an attempt to define such a liquidity adjusted VaR.) This is a classic situation in which the answer depends on the user's trade-off between mean and volatility. Although we cannot say definitively, however, we can see in Panel B that in this particular case the one-day distribution is shifted so far to the left relative to the five-day distribution that it would be hard to imagine anyone preferring the one-day over the five-day liquidation. Comparing two days to five days, it would also seem that the five-day liquidation would be preferable—the density for five days is almost all to the right of the two-day density.3
We can also see from Figure 12.2 that simply comparing VaR across distributions is not appropriate. The vertical lines in Panel B show one standard deviation (1σ) below the mean for each of the distributions—the cost less 1.0 times the volatility. This will be the 16 percent VaR (assuming for now that the P&L distribution is normal). In Figure 12.2, the 16 percent VaR for the two-day liquidation is lower than for the five-day liquidation. In other words, relying on the 16 percent VaR would point us toward a five-day liquidation, which seems to be the right answer in this case.
Comparing the 16 percent VaR gives the right answer in this case but we can always go far enough out in the tail of the distribution to get a VaR for the two-day distribution that is better than the VaR for the five-day distribution. Simply put, because the means of the distributions are different, we cannot blindly rely on VaRs to compare distributions across days. Table 12.2 shows the cost less 3.09 times the volatility. This is the 0.1 percent VaR. This 0.1 percent VaR is virtually the same for the two-day and five-day liquidation. Relying on the 0.1 percent VaR would seem to indicate that the two-day and five-day liquidation horizons are equally good, when examination of Figure 12.2 shows that is clearly not the case.
Table 12.2 Costs and Volatility for Liquidating Hypothetical Portfolio over Various Periods, with 1 Percent VaR.

Issues around Asset Liquidity
Thinking about, calculating, and evaluating asset liquidity risk is difficult. For market risk and for credit risk, there is a well-developed framework. We are concerned with the P&L distribution. We are interested primarily with the spread or dispersion of the distribution. We can summarize the dispersion in various ways—using, say, the volatility or VaR or expected shortfall—but these single-number measures generally give a pretty good idea of the dispersion.
For asset liquidity risk, in contrast, we run into two important issues. First, the framework for thinking about asset liquidity risk is not as well developed and there is not the same depth and range of developed practice. Also, the evaluation of asset liquidity and liquidation over various horizons cannot be easily summarized into a single summary number—we have to consider the thorny issue of trade-offs between mean and variance. Second, practical issues with implementation, data collection, and calculation are substantial. The balance of this section reviews these two issues, in reverse order.
Practical Issues around Implementation
The central issue for asset liquidity risk is equation (12.1) or (12.2), the price impact function, from which we calculate costs of liquidation. I stated that equation (12.1) reflected bid-offer spreads and response of market prices to large trades. For the application of equation (12.1), the actual mechanism that generates changes in price with transaction size matters less than the values of the function itself.
The essential problem, however, is that estimating any kind of equation like (12.1) is very difficult, particularly for the illiquid securities where it is most critical. For market risk, historical prices for securities and market risk factors are relatively easy to find, and these historical prices are what we need for estimating the standard market risk P&L distribution, as detailed in Section 8.3 of Chapter 8. For asset liquidity, it is much harder to find data with which we can estimate a price impact function.
The first problem is that only a subset of assets have good publicly available data on prices and trades. Exchange-traded assets such as equities have good data on volumes, trades, bid-offer spreads, prices, and all at various frequencies from tick-by-tick to daily, weekly, monthly. Over-the-counter instruments, however, make up a large portion of many portfolios and trade data on such markets is limited.
The second problem is that even when we do have trade data, determining when a price change is due to a large trade versus occurring for other reasons related to more fundamental factors is difficult. Say we observe a trade that is larger and at a lower price than trades just before it. The lower price may be result from a seller trying to unload a large position, pushing the price down. Alternatively, both the trade and the lower price may be a result of news or information that both pushes down the market price and induces some market participant to sell their position.
There are, however, a few simple things that we can do, and which can have a large effect on our estimates of asset liquidity costs. For exchange-traded assets, we can often estimate the first part of function (12.1), the bid-offer spread for small-to-moderate size (represented in Figure 12.1 by the flat line for 50,000 shares and less) without too much difficulty. We can get statistics on the bid-offer spread and on the average or median daily trading volume. For example, Table 12.1 shows the statistics for trading in common shares of IBM (NYSE ticker IBM) and in ING 6.125 percent perpetual preferreds (NYSE ticker ISG). (Data are as of October 2011.)
Table 12.3 Market Statistics for IBM and ISG.




IBM
ISG




Market Price
$ 185
$ 18


Bid-Offer Spread—$
$ 0.07
$0.14


Bid-Offer Spread—%
0.04%
0.78%


Average daily volume (3mth), shares, '000s
7,100
104


Average daily volume (3mth) ($ million)
$ 1,313.5
$ 1.9


Shares outstanding (million)
1,194
28


Shares outstanding ($ million)
$221,000
$504


Note: "IBM" is the common shares of International Business Machines. "ISG" is the 6.125 percent perpetual preferred debentures for ING.



Just this limited amount of information provides valuable insight. ISG is far less liquid than is IBM—we already know that—but these figures provide quantitative measures for this. The bid-offer spread for ISG is much larger (in percentage terms) than for IBM. This gives us a start to estimate the cost of liquidating a position and immediately tells us that transacting in ISG will be more costly than transacting in IBM. Both the daily volume and the shares outstanding for ISG are tiny relative to that for IBM. This indicates the size of position we might expect to be able to transact easily versus with more difficulty. Trying to sell a position of $1 million in ISG should probably not be a big issue, but $20 million would be. For IBM, of course, selling $20 million would not be an issue.
Developing such information on normal bid-offer spreads and normal trade size for nonexchange-traded assets is more difficult. Doing so will generally require a firm to exploit either internal data sources (firm trade records if those are rich enough) or the judgment and experience of traders.
We can think of data on the normal bid-offer spread and the normal trade size as giving us the first part of the price impact function shown in Figure 12.1—the flat section for 50,000 shares or fewer. With this first part of the price impact function, we can examine the portfolio and determine whether asset liquidity issues are likely to arise. If all the holdings are less than the normal daily trade size and bid-offer spreads are relatively narrow, then liquidating the portfolio in a single day is unlikely to have a large price impact. In fact, using the bid-offer spreads we can make a first (minimum) estimate for the cost of single-day liquidation.
If, on the other hand, there are significant holdings that are large relative to normal daily trade size, then we have to tackle the problem of extending the price impact function and evaluating liquidation across different horizons.
In many cases, estimating the section of the price impact function beyond the flat, bid-offer section (in Figure 12.1, the section for more than 50,000 shares) will be a matter of substantial approximation and judgment. The exercise of putting numbers to a price impact function should not lull us into thinking that we have solved the asset liquidity issue. It should, instead, push us toward making our assumptions about liquidity more concrete while also critically examining those assumptions.
One final issue regarding the price impact function (12.1) or (12.2). We have been treating the function as deterministic with no random variation in costs. This is of course too simplistic. It is fine for a start, but ideally we would want the costs to be random. We could think of the equation as being:

In the first equation,  could be assumed log-normally distributed (so that the percent cost would always be positive, with mean and variance a function of w) or in the second equation,  could be assumed normally distributed (as long as z is small relative to k, there would be a low chance of the cost going negative, and here z would be normal with mean zero and variance depending on w). When the cost is random, the cost will alter the volatility of the P&L as well as shifting the mean.
Framework for Evaluating Asset Liquidity
I argued earlier that a reasonable way to think about asset liquidity is to treat the cost of liquidation as shifting the P&L distribution to the left. Faster liquidation imposes costs that shift the distribution further to the left, while leisurely liquidation reduces costs but widens the distribution and leads to larger potential trading losses. The problem reduces to choosing a trade-off between costs versus volatility.
The price impact function provides the cost data that form the foundation for this analysis. The biggest problem, of course, is that there is considerable uncertainty in most estimates of price impact functions. This means that we have to be careful in interpreting and using any asset liquidity analysis. Nonetheless, just the exercise of estimating the functions and analyzing the portfolio can shed considerable light on any asset liquidity issues. If nothing else, it can point out whether asset liquidity is likely to be an issue for the portfolio under consideration.
I argued earlier that an understanding of the trade-offs between quick versus slow liquidation requires considering the full distributions, as shown in Figure 12.2. Nonetheless, considering just the mean (cost) and volatility, numbers such as shown in Table 12.4, can provide considerable insight. We must, however, keep in mind that we are really thinking about the distributions such as shown in Figure 12.2.
Table 12.4 shows the cost and volatility for the example discussed earlier, but also shows the cost and volatility as a percent of assets. Liquidating in one day is clearly expensive, with the cost substantially higher than the volatility. Liquidating over two days dramatically reduces costs while not increasing volatility dramatically. To me, liquidating over something like 5 or 10 days seems reasonable, while waiting for 31 days seems to increase volatility too much relative to the reduction in costs.
Table 12.4 Costs and Volatility for Liquidating Hypothetical Portfolio over Various Periods.

A further benefit of having the costs laid out as in Table 12.4 is that we could estimate the reserves or capital that might be required to withstand losses from liquidating. The cost would be an expected loss, while we would need to add an additional amount to protect against unexpected losses—say the 1 percent VaR. Note that here we are adding the expected costs to the 1 percent VaR, but the interpretation is not a liquidity-adjusted VaR. As I argued before, such a concept is not sensible. We are asking instead what reserve we might want to take, accounting for both expected losses (the costs) and unexpected losses (the VaR as a deviation from the mean).
One final issue deserves mention regarding calculation of volatilities over a liquidation horizon. In deriving equation (12.4) we assumed that the portfolio was unchanged during liquidation, apart from being reduced by 1/n each day. This may be a reasonable assumption, and certainly is useful as a base case and to help build intuition. In practice, however, more liquid assets would probably be liquidated more rapidly. This could easily be accommodated by calculating the volatility of the projected portfolio day by day, accounting for what assets would be liquidated quickly versus slowly.
Such an exercise could be quite valuable in its own right by potentially highlighting problems with unwinding offsetting hedges. Say, for example, that in the original portfolio, a long position in an illiquid equity is hedged with a liquid equity index futures. If the equity index futures were sold off early and the stock itself sold off slowly there might be a large and unintended increase in portfolio volatility due to the illiquid equity being left unhedged.
Conclusion
Asset liquidity focuses on the asset side of the balance sheet, and particularly on the cost of liquidating positions. These costs can be quite different across assets, but these costs can be quite difficult to estimate.
This section has argued that the appropriate way to assess asset liquidity risk is to compare liquidation strategies across different horizons. Fast liquidation leads to high costs but avoids potential losses resulting from market movements. Leisurely liquidation reduces costs but leaves the portfolio open to possible losses if the markets move against the portfolio. To properly evaluate fast versus leisurely liquidation, we need to recognize that we have to decide on a trade-off between expected costs versus the volatility of market movements; simply calculating a liquidity adjusted VaR as the sum of standard VaR plus costs mixes apples and oranges.
12.3 Funding Liquidity Risk
We now turn from a focus on the asset side of the balance sheet to look at the liability side. Quantitative risk measurement is mostly concerned with statistics, probability, and mathematics. But as I have tried to emphasize throughout this book, risk management is about managing the firm, doing whatever it takes, using whatever tools and techniques are available and necessary, to understand and manage the risk. Funding liquidity risk is a prime case of when we do not necessarily need fancy mathematics; we need instead common sense and attention to details.
Funding liquidity focuses on the sources of funds. Risk management and risk measurement generally focus on the uses of funds, the investments, and assets held. Funding and the liability side of the balance sheet are not the natural province of most risk professionals. Funding more naturally falls under the CFO or Treasury function rather than trading. Having said that, funding liquidity is critically important. During a crisis, it is often the liquidity issues that bring down a firm. To mention only a few instances, LTCM, Metallgesellschaft, and Askin Capital Management were all subject to severe liquidity issues. Such problems become paramount and industry-wide during a liquidity crisis such as the U.S. subprime-triggered crisis of 2007-2009 and the eurozone crisis that began in 2011.
What is funding liquidity risk? Simply stated, it arises from mismatches between assets and liabilities. Not mismatches in the value (when assets are worth less than liabilities that becomes an issue of solvency) but rather mismatches in timing. It is often hard to separate solvency issues from liquidity issues, and liquidity problems can morph into solvency issues, but conceptually we want to keep them distinct.
The history of banking and finance is the story of aggregation and intermediation. We can go back to Bagehot's Lombard Street from 1873 to see that finance has long been a means for taking funds from depositors or investors and channeling those funds to more profitable uses, to entrepreneurs or companies.
A million in the hands of a single banker is a great power....But the same sum scattered in tens and fifties through a whole nation is no power at all....Concentration of money in banks...is the principal cause which has made the Money Market of England so exceedingly rich.
In this constant and chronic borrowing, Lombard Street [London's 19th-century Wall Street] is the great go-between. (Chapter I)
But the aggregating and channeling of funds invariably entails a mismatch between the liabilities owed to investors or depositors and the assets invested. This is the case whether we are looking at a traditional bank or a hedge fund, but it is easiest to see with traditional banking. A bank aggregates retail deposits and channels these deposits to mortgage loans, commercial loans, or whatever other assets in which it invests. The deposits are demand deposits, redeemable upon demand. The loans are long-term, months or years in duration.
We are abstracting from solvency issues, so we assume that assets are good and there is no excessive risk of default or other losses on the loans. But say that, for some reason, depositors all demand repayment. The bank is ruined in such a situation. There is no possibility that a bank can pay back all depositors immediately because the assets do not mature for a considerable time and the assets are not liquid enough to be quickly sold. There is a fundamental mismatch between assets and liabilities.
Financial firms other than banks are exposed to similar funding or asset-liability mismatches. An investment firm takes investor funds and invests in market securities. These market securities will generally be more liquid than bank loans, but the redemption terms for investors will often have a shorter term than the duration or term of the assets, and the assets will not be liquid enough to allow immediate liquidation.
Leveraged investments, wherever they are housed, will always be subject to funding liquidity risk. The money borrowed to fund a leveraged position will be short term while the assets are longer in duration. Consider a bond repurchase agreement or repo—funding the purchase of a bond by borrowing the purchase price and posting the bond itself as security for the loan. The repo agreement, and thus the funds borrowed, will almost always be short-term: overnight, maybe monthly. Repo arrangements will usually require a so-called haircut, in which only a fraction of the bond price can be borrowed. The haircut might be 2 percent or 5 percent so that a firm can borrow 98 or 95 percent of the purchase price. During times of market disruption, or when a firm comes under pressure, the haircut might be raised. Since the repo agreement is short-term, this can be done quickly. If the haircut goes from 5 percent to 10 percent, then the cash required to maintain the bond position doubles—a classic funding liquidity problem.
Measuring and managing funding liquidity issues reduces to good asset-liability management. This is not the central focus of this book but we can learn something by focusing on how such asset liability analysis might work for a commercial bank.
Framework for Funding Liquidity—Traditional Banking
Funding liquidity risks arise regularly in traditional banking. This section follows Marrison (2002, ch. 14) in the discussion of asset liability management within a commercial bank. This discussion provides a framework for thinking about and measuring funding liquidity risk.
As discussed before, a bank has a preponderance of short-maturity liabilities. These will be demand deposits such as checking deposits but will usually also consist of short-term funding raised on the wholesale markets, from institutional investors. Banks take these short-term funds and invest the bulk in long-dated and illiquid assets such as commercial loans. There will be random variations in the demands by retail investors for cash, and minor fluctuations in the price and availability of wholesale funds, but these fluctuations will generally be minor. A bank will hold cash and other reserves to satisfy these random fluctuations.
At rare times, however, customer demands for cash or the availability of wholesale funds will change dramatically. This might be because there is a rumor that the bank is in trouble or it could be a systemic problem that pushes a large proportion of customers to demand cash and counterparties to stop supplying wholesale funds. For whatever reason it occurs, such a change will push the bank into a funding liquidity crisis. The funding problem will then become self-fulfilling, since once a funding problem starts, more customers will demand cash and fewer counterparties will lend in the wholesale markets.
Cataloging Sources and Uses of Funds
Measuring and managing funding liquidity comes down to measuring and managing the bank's inflows (sources of funds) and outflows (uses of funds).4 We go about this in two stages. First, we define and classify the bank's sources and uses of funds. This gives us a framework for measuring the net cash position and for identifying the likely size and sources of fluctuations. Second, we consider three regimes, or sets of conditions, that lead to three sets of funding requirements: normal conditions with normal fluctuations that lead to expected funding requirements; unusual conditions with large fluctuations that lead to unusual funding requirements; and extreme conditions with extraordinary fluctuations that lead to crisis funding requirements and Economic Capital.
To lay out the framework for sources and uses of funds, we classify payments into four categories: scheduled payments, unscheduled payments, semidiscretionary payments, and discretionary or balancing transactions. Typical flows for a bank falling into these four categories might be:

 Scheduled payments—previously agreed or contracted payments that cannot be changed easily or quickly. Examples would include:

 Outflows or uses of cash = OS: loan disbursements; repayments to customers such as maturing CDs; loan repayments to other banks; bond coupons.
 Inflows or sources of cash = IS: payments from customers such as loan repayments.

 Unscheduled payments—arising from customer behavior

 Outflows or uses of cash = OU: repayments to customers, such as checking-account withdrawals; loan disbursements on things like credit cards and lines of credit; payments to corporations such as standby lines of credit.
 Inflows or uses of cash = IU: payments by customers such as deposits into checking accounts.

 Semidiscretionary payments—payments that occur as part of the bank's trading operations but which can be changed without undue difficulty

 Outflows or uses of cash = OSD: purchases of securities; outgoing cash collateral.
 Inflows or sources of cash = ISD: sale of trading securities; incoming cash collateral.

 Discretionary or balancing transactions—carried out by the funding unit to balance daily cash flows.

 Outflows or uses of funds = OB: lending in the interbank market; increase in cash reserves.
 Inflows or sources of cash = IB: borrowing in the interbank market; calls on standby lines of credit with other banks; drawdown of cash reserves; borrowing from the central bank (the Federal Reserve) at the discount window (only in grave circumstances).


Using this classification, we can write down the net balancing transactions necessary to balance the bank's daily cash sources and uses. The net balancing transactions (measured as the cash that must be raised) will be the sum of outflows less the sum of inflows:
(12.5) 
The scheduled terms are known and it is useful to group the random components:

so that we can write the net balancing transactions as:
(12.6) 
We can model the random term R as normally distributed with mean μR and standard deviation (volatility) σR.
The Funding Distribution and Funding Requirements
So far, we have done nothing more than define accounting relations. Doing so, however, organizes the data and focuses attention on the critical aspects of funding liquidity. It also allows us to think about funding liquidity risk in exactly the same way we have thought about market or credit risk: here focusing on the funding distribution. In equation (12.6), we are treating the net funding requirements or balancing transactions as a random variable, and so we can think about the distribution of the net funding. We can use the same tools and techniques that we applied previously: estimate the distribution and then use the distribution to calculate how likely we are to have a large positive or negative funding requirement.
For funding, it is useful to think about the funding requirements under different conditions: expected funding requirements, unusual funding requirements, and crisis funding requirements.
Expected funding requirements: This is easy conceptually, simply the scheduled payments plus the average (expected value) for all other payments:

This will include scheduled payments such as promised loan payments (both incoming from corporate loans and outgoing repayments on loans the bank has taken out), coupon payments, new loan originations, and so forth. It will also include expected levels and changes in unscheduled items (such as checking-account balances) and semidiscretionary items (such as purchases of government bonds).
One important point—the analysis of expected funding requirements must extend out for some period into the future. Cash inflows and outflows will vary nonsynchronously over time. For example, a large new loan disbursement occurring on a specific date will imply a large cash outflow that must be funded. Tracing the expected funding requirements out into the future will highlight potential cash flow mismatches, in either size or timing or both.
This exercise will not, of course, be easy. It requires considerable data collection and analysis. Marrison points out that a detailed daily model of checking-account balances would probably show that personal checking balances would vary over a month as individuals draw down during a month, then replenish as wages are paid. This approach does show us where we should direct our effort: toward measuring the scheduled and expected cash flows.
In thinking about the distribution of the funding requirement, NB, it is useful to consider Figure 12.3. The actual funding requirement will be random. The expected funding requirement is shown in Panel A—the mean of the distribution. In Figure 12.3, this is above zero, but it could be above or below.

Figure 12.3 Distribution of Net Funding for Hypothetical Bank

One important consideration regarding expected funding is that it may vary considerably day by day, since there may be big incoming or outgoing cash flows on particular days. For example, a new loan could be scheduled, and this would involve a large cash outflow. Treating such issues is part of standard asset liability or cash flow management.
Unusual funding requirements: The next step is easy conceptually, simply going out into the tail of the distribution:

Here we go two standard deviations, and this should cover roughly 98 percent of the cases—the funding should be this high or worse roughly two days out of 100. There is nothing sacred about two standard deviations, but it is a reasonable assumption for unusual funding requirements. Figure 12.3 Panel B shows what this looks like in terms of the funding distribution.
As pointed out earlier, estimating the distribution of the net funding is not an easy task. Analyzing the cash inflows and outflows and estimating the distribution, however, provides valuable information on exactly how and why the funding requirements may vary. It also provides information on the amount and type of discretionary funding that might be necessary to support unusual funding requirements.
Crisis funding requirements and economic capital: The final step, estimating funding requirements during a liquidity crisis, is more difficult. The natural inclination would be to go further out in the tail of the normal distribution, so going out maybe 3 or 3.5 standard deviations. This would be assuming that the factors driving funding requirements during a crisis are the same as during normal times, just more severe. This is often not the case. Thinking in regard to Figure 12.3, moving out further into the tail would be assuming that the distribution is normal, whereas in reality the distribution for extreme events is probably very non-normal—probably a skewed and fat upper tail.
Marrison (2002, 207 ff) provides an alternative, and very reasonable, approach to analyzing the funding requirement during a crisis. There are two steps. First, we go back to the underlying cash flows and modify those to reflect how customers and counterparties might behave during a crisis. This would give a better estimate of what cash requirements might actually be during a crisis. Second, based on this cash requirement, we work out how the bank would have to respond, what liquid and illiquid assets would have to be sold to generate this cash, and how much of a loss this would generate. This loss would then provide a guess at the economic capital that would be necessary to survive such a funding liquidity crisis.
The first step is to modify the cash flows. During a crisis, it would be reasonable that the bank will make all scheduled payments. Most scheduled inflows will occur but there will be some proportion of defaults. There would probably be no unscheduled inflows (customers will themselves be hoarding cash) and unscheduled outflows will be some multiple of the usual standard deviation.5 Such modifications might give a reasonable estimate of the cash required during a crisis.
The second step is to work out how the bank would generate this cash, generally by selling assets. Liquid assets can be sold first, but eventually illiquid assets will have to be sold at a discount to the book or current market value, generating a loss for the bank. The key step here is to make a list of the assets that might be sold, together with an estimate of the discount at which they would sell during a forced liquidation. Such estimates may be subjective and open to error, but they at least provide some basis for estimating the potential loss.
Table 12.5 shows such a list for a hypothetical bank, together with the loss that might be suffered from a forced liquidation. Cash suffers no discount, Treasuries a small discount, while additional assets suffer increasingly steep discounts for liquidation. If the analysis in the first step showed that $15.65 billion of additional cash would be required during a crisis, the bank could expect to suffer a $350 million loss during such a crisis. This would be an estimate of the economic capital required to sustain the business through such a crisis.
Table 12.5 Losses Due to Asset Liquidation in a Liquidity Crisis.

Liquidity Risk Management
Everything so far has focused on liquidity risk measurement, not management. The measurement is extraordinarily valuable but is only the first step. As I have emphasized throughout this book, the goal of risk management is actually managing the risk. It is to that task that we now briefly turn.
The risk measurement is important, and it is important for two reasons. First and most obviously it provides concrete and usable information that we can use to manage the risk. But second, and equally important, it provides the foundation and framework for digging deeper into funding liquidity risk. The data behind a table like 12.5, the classification of sources and uses embedded in equations (12.5) and (12.6) and the data behind that classification, provides the details necessary to build the contingency plans for managing funding liquidity before a crisis and the action plans to manage during a crisis.
With these data and the framework a bank can make better decisions. Marrison (2002, 209) lays out some of the steps a bank could take to alter its funding liquidity profile:

 Increase the proportion of long-term versus short-term funding by borrowing long-term funds in the interbank market or issuing bonds.
 Borrow additional funds long-term, investing the proceeds in liquid assets that could be sold or pledged during a crisis.
 Establish standby lines of credit that could be called upon during a crisis.
 Limit the proportion of funds lent long-term in the interbank market.
 Reduce the liquidity of liabilities, for example, by encouraging customers to deposit into fixed-term deposits rather than on-call savings accounts or demand deposits (say, by paying a higher return on fixed-term deposits).

All of these actions, however, come at a price. The yield curve is usually upward-sloping so that borrowing a larger proportion of funds long-term rather than short-term would increase costs while lending a larger proportion short-term will decrease income. The increased cost has to be traded off against the benefits of more stable funding, and potentially lower economic capital held against the potential of a liquidity crisis.
Funding Liquidity for Other Organizations
The general ideas outlined here for a commercial bank can be applied to most other organizations. For example, a long-only investment manager could follow the same classification of sources and uses of funds, but much simplified:

 Scheduled payments—would not apply since there would be no analogues of loan disbursements or loan payments.
 Unscheduled payments—arising from customer behavior.

 Outflows or uses of cash = OU: redemptions by customers.
 Inflows or uses of cash = IU: new investments from existing or new customers.

 Semidiscretionary payments—the bulk of cash flows since most of the firm's activity is trading that can be changed without undue difficulty.

 Outflows or uses of cash = OSD: purchases of securities.
 Inflows or sources of cash = ISD: sale of trading securities.
 Discretionary or balancing transactions—to balance daily cash flows, borrowing or lending from a bank to balance daily redemptions.


Estimating customer inflows and outflows can be quite difficult, but from this perspective there is no conceptual difference between the framework we would apply to a bank and that applied to other organizations.
There are three issues, however, that we have not discussed but that have a substantial impact on funding liquidity. First, leverage adds a new dimension to the analysis. Second, derivatives will add additional future cash flows, and these cash flows will often be contingent, and consequently more difficult to estimate. Third, mark-to-market and collateral issues (usually for derivatives contracts) introduce complications and an interaction between market movements and funding liquidity. We now turn to these issues.
Leverage
Leverage is the other major source, apart from the duration transformation of traditional banking, for funding liquidity problems. Traditional banking involves taking short-duration deposits and transforming them into long-duration assets. Put simply, funding liquidity problems arise when the short-duration deposits disappear and the long-duration assets cannot be funded. With leverage, long-term assets are bought using (usually short-term) borrowed money, and funding liquidity problems arise for very much the same reason. If the short-term borrowed money disappears, the assets cannot be funded and there is a funding liquidity problem.
To assess the impact of leverage and funding liquidity risk, we can undertake much the same asset-liability analysis as for a bank. We go through the exercise of classifying sources and uses of funds. Short-term borrowing used in financing longer-term assets is routinely renewed or rolled over; funding liquidity problems arise when it is not renewed. The crux of the issue is that the repayment of an expiring short-term loan is a scheduled cash outflow—the payment is an obligation. The renewal of the loan, in contrast, is not an obligation and so is an unscheduled cash inflow—to be treated as a scheduled payment, it would have to be guaranteed, in which case it would not in fact be a short-term loan.
The analysis of funding requirements under crisis conditions outlined earlier involves continuing all scheduled payments but setting unscheduled inflows to zero. In the present context, this means assuming some or all of the short-term loans do not renew. Projecting this stressed cash flow analysis into the future shows how much and at what dates the funding shortfalls would be expected to occur.
In practice, the leverage in financial businesses is often in some form other than unsecured borrowing. One particularly common form is through secured funding or repo transactions.
The legal details of bond repurchase (repo) transactions can be somewhat complicated, but the end product is equivalent to a secured loan. A firm agrees to borrow cash and give the bond as security for the loan. (A repo transaction involves borrowing cash and lending the security and, confusingly for our purposes, is often referred to as a lending transaction—the security is lent.) Repo transactions commonly incorporate a haircut through which the cash borrower cannot borrow the full value of the bond—the cash borrowing is overcollateralized. The haircut for a U.S. Treasury bond, the least risky form of repo transaction, might be 2 to 3 percent (so 98 to 97 percent of the value of the bond can be borrowed), while for corporate bonds it could range much higher, on the order of 20 percent.
The repo market is huge and is a major source of funding for the securities industry. The market for U.S. Treasury repos is deep, liquid, and active, but there are also markets for other securities, from corporate bonds to mortgage securities to equities (where the market is termed securities lending rather than repo). A large portion of the agreements are overnight repos, in other words, one-day borrowing, but a repo may be agreed for term, anywhere from a few days to months.
A repo transaction is a secured loan, and as such, safer than an unsecured transaction. As a result, repo agreements are less likely to be canceled or not renewed. During a systemic liquidity crisis, lenders will often increase the haircut (which increases the security of the loan) rather than cancel the repo.
We can apply this insight to the analysis of crisis funding conditions discussed earlier. In the framework of cash inflows and outflows we might want to consider a repo more akin to long-term funding, with the borrowing and repayment both considered scheduled payments. During a crisis, a change in haircut would be an unscheduled cash outflow. We could estimate possible changes in haircut on a security-by-security basis. The haircut on U.S. Treasuries would change little. The haircut on a low-grade corporate bond could change substantially. The estimated changes in haircuts and resulting increased cash outflows under crisis funding conditions would give a better estimate of the possible changes in funding.
To summarize how leverage affects funding liquidity, we can still apply the framework Marrison lays out for commercial banks. We take leverage into account by determining the dates of cash inflows and outflows for short-term borrowing. For nonsecured funding the restriction or cancellation of funding would involve a fall in unscheduled (but expected) cash inflows. For repo funding, the increase in haircuts would involve an increase in unscheduled cash outflows. Once we have delineated the expected funding requirements (under normal and crisis conditions), we can consider whether it is necessary or worthwhile to alter the profile.
Derivatives
Derivatives introduce two complications to considerations of funding liquidity. First, future cash flows can be difficult to estimate. Second, mark-to-market and collateral can complicate funding liquidity calculations and introduce a connection between market movements and funding liquidity. Cash flows are discussed here, mark-to-market and collateral in the next section.
Derivatives produce future cash flows, and so in many respects, are no different from a bond or other security. The cash flows would fall in the scheduled payments category of the payments classification scheme laid out earlier. The complexity introduced by derivatives is that the cash flows are often unknown or contingent, making the estimation of the cash flows difficult.
Consider an interest rate swap to receive fixed 5 percent and pay floating for two years. The fixed payments will be $2.5 every six months (for $100 notional) and are known. They are shown in Figure 12.4, represented by the upward-pointing arrows. The floating rate payments are set equal to LIBOR and are reset every three months; beyond the first reset, the exact amounts will be unknown (although they can be estimated from the forward yield curve). Such a swap is shown in Figure 12.4. This swap presents two issues: first, the cash flows are mismatched and so will produce sequential inflows and outflows (floating outflows occur every three months versus fixed inflows every six months); and second, future floating payments are not known today.

Figure 12.4 Two-Year Swap with Semiannual Fixed and Quarterly Floating Cash Flows

More exotic derivatives such as options and credit default swaps are even more difficult, as the amount and possibly the timing of the cash flows are unknown and can vary dramatically as market conditions change.
It is often said that derivatives are leveraged instruments. In a sense, this is true but the leverage is not the form discussed in the earlier section—short-term borrowing supporting purchase of long-term assets—that is subject to the withdrawal of short-term funding and funding liquidity problems.
Derivatives are contracts whose payout depends on (is derived from) other market prices or events. Derivatives generally do not involve an up-front purchase or investment. Derivative contracts, by their very nature, usually cannot. Consider the swap above, or a classic agricultural futures contract such as wheat or a financial futures contract such as an equity index. A trader can go either long or short, can either buy or sell the wheat or the equity index. Although there is a natural underlying notional amount, there is no up-front value, so no payment to be made from buyer to seller or vice versa. Buying or selling does not actually involve doing either. The futures contract is simply the agreement to buy or sell in the future at a price agreed today. In the interim there may be the obligation to pay the mark-to-market value of the difference between the originally agreed price and the current market price, but this may be either positive or negative, and at initiation, the expectation is this mark-to-market will be zero.
Mark-to-Market and Market/Cash Volatility
Derivatives contracts do embed leverage in the sense that an investor can obtain market exposure without investing the notional amount.6 The impact of this leverage on funding liquidity is quite different from that of leverage discussed earlier. The impact for derivatives comes about through mark-to-market cash calls and collateral calls. This produces a linkage between market movements and funding liquidity that can sometimes be quite dramatic. Analysis of such market moves requires thinking about what we might call the market/cash distribution.
Exchange-traded derivatives such as futures contracts require that any profit or loss be paid daily (through what is called variation margin). This mechanism helps control counterparty credit exposure and has been integral to futures markets since their inception. Over-the-counter (OTC) derivatives such as interest rate swaps or credit default swaps have, to date, generally not involved regular mark-to-market payments. Market practice has evolved, however, so that almost all OTC derivatives involve the posting of collateral to cover mark-to-market movements. The collateral provides the means by which the party who is owed money can collect should the other side default.
Whatever the exact mechanism, whether variation margin or posting collateral, most derivatives contracts entail cash inflows or outflows in response to movements in market prices. Repo contracts, discussed in a preceding section on leverage, also generally involve regular mark-to-market or collateral calls in response to price changes, and so will respond in the same way.
The result is that in the framework for classifying cash flows discussed earlier, the unscheduled payments, both inflows and outflows, will depend on market movements. The first inclination might be to include the P&L volatility, estimated as discussed in Chapter 8, Section 8.3, into the volatility of the random term R in equation (12.6).7 This is not appropriate because only some market price movements generate cash inflows and outflows. We need instead to define a new measure, what we might call the market/cash distribution. This is the distribution of cash flows generated by the distribution of market risk factors. It differs from the P&L distribution we have worked with in prior chapters because it is only the cash generated by market moves that enter.
To build the market/cash distribution, we need to build the distribution of cash flows resulting from market movements in a manner similar to that of Chapter 8, Section 8.3. Remember that in Section 8.3 there were four steps, the first being asset/risk factor mapping, which transformed from individual assets to risk factors. This first step is all that is changed in building the market/cash distribution.
In Section 8.3, the transformation from assets to risk factors involved calculating the mark-to-market P&L that resulted from changes in market risk factors. The change here is that we need to calculate the cash flow resulting from changes in market risk factors rather than the mark-to-market. This requires a somewhat different focus from that for standard mark-to-market. We need to go through all instruments, derivatives contracts in particular, and determine which will generate cash flows and under what conditions.
In analyzing contracts to determine cash flows resulting from market movements, futures contracts are relatively simple: a new contract requires initial margin up front, and existing contracts generate cash flows equal to mark-to-market profit or loss. OTC contracts are more difficult, since different contracts and counterparties usually have different terms and conditions. A contract will sometimes involve two-way margining (collateral passed from each counterparty to the other) and sometimes only one-way margining.8 There are often thresholds so that collateral is passed only when the mark-to-market is above the threshold. The details of each contract must be collected and the effect of changes in market prices on cash flows modeled.
One issue that is particularly important and deserves special mention is when one set of contracts involve cash flows upon mark-to-market while another, similar or related set of contracts, do not. This might occur when an OTC contract that does not entail collateral calls is hedged with a futures contract with cash flows (variation margin). The example of Metallgesellschaft discussed further on highlights this issue.
Once we have the market/cash distribution, we can combine this into the random cash flows R in equation (12.6) and then evaluate the funding requirements under the three sets of conditions: expected requirements, unusual requirements, and crisis funding requirements.
Additional Remarks Regarding Funding Liquidity
The analysis of funding liquidity is difficult. Theory and practice are not as well developed as for market risk, credit risk, or even operational risk. The topic is critically important nonetheless. I will conclude with short remarks on two topics. First, an anecdote related to the trading losses of Chapter 4 that emphasizes the importance of funding liquidity. Second, some remarks on the systemic nature of liquidity crises, which highlights why understanding and managing liquidity risk is particularly difficult.
Metallgesellschaft
Funding liquidity problems played a central role in Metallgesellschaft's $1.3 billion loss in 1993. Most important was the variation margin, or mark-to-market cash calls, from one side of a hedge strategy that were not matched by cash or collateral calls on the other side.
Metallgesellschaft was a German industrial conglomerate, Germany's 14th-largest industrial company with 58,000 employees. The American subsidiary, MG Refining & Marketing (MGRM), offered customers long-term contracts for oil products. MGRM undertook a strategy to hedge the long-dated fixed-price oil delivery contracts using short-dated futures and OTC swaps (essentially buying a stack of near-contract futures).
Although problematic, providing only a partial hedge, the strategy was not fatally flawed as a hedging strategy, per se. It did suffer from potentially severe funding liquidity problems. Oil prices moved in such a way that the value of the long-dated customer contracts moved in MGRM's favor. There were no collateral arrangements for those contracts so MGRM made unrealized profits but generated no cash. The short-dated futures, however, lost money and those losses had to be settled up front through cash payments to the futures exchanges.9 To make matters worse, German accounting standards did not allow the offset of unrealized profits on the customer contracts against realized losses on the futures.
When MGRM called for cash from the parent, the parent replaced senior management at MGRM and liquidated the futures contracts. There is debate about how much of the ultimately reported $1.3 billion loss was a result of the poorly designed hedge versus untimely unwinding of the strategy. What is absolutely evident, however, is that even if the hedge were perfect, it required such large cash payments that the strategy was probably not viable.
The Systemic Nature of Liquidity Crises
Managing liquidity risk is particularly difficult because liquidity issues are so often and so closely associated with systemic or macroeconomic credit and liquidity crises. Liquidity crises have occurred and recurred over the centuries. When reading reports of the panic and market disruption associated with such crises, the events of the 1700s, 1800s, and 1900s sound little different from the flight to quality that we see in today's liquidity crises—everybody wants cash or liquid assets:
This...occasioned a great run upon the bank, who were now obliged to pay out money much faster than they had received it...in the morning. ("South Sea Bubble," September 28, 1720; Makay 1932, 69, originally published 1841)
Everybody begging for money—money—but money was hardly on any condition to be had. (Thomas Joplin regarding the panic of 1825, quoted in Kindleberger 1989, 127)
A crop of bank failures...led to widespread attempts to convert demand and time deposits into currency....A contagion of fear spread among depositors. (The first U.S. banking crisis of the Great Depression, October 1930; Friedman and Schwartz 1963, 308)
The morning's New York Times [August 27, 1998] intoned "The market turmoil is being compared to the most painful financial disasters in memory."...Everyone wanted his money back." (After Russia's effective default in August 1998; Lowenstein 2000, 153-154)
Liquidity crises appear to be recurring episodes for our capitalist economic system. It may be the paradox of credit and banking, that banking and finance are built on trust and confidence and yet such confidence can be overstretched and when overstretched is apt to quickly disappear.
For an individual institution to protect against or manage such risk is difficult. When "everyone wants his money back" the fine distinctions between well-managed and poorly managed firms get blurred and all firms suffer.10 Liquidity risk is among the most difficult of all problems for managers.
12.4 Operational Risk
Over the past few years there has been an explosion of research and development in operational risk measurement. To some extent, this has been driven by regulatory demands. Basel II included a charge for operational risk in calculating regulatory capital (see Basel Committee on Banking Supervision 2006 [originally published in 2004] and 2011 [originally published 2003]). The industry has also recognized the benefits of better management of operational risk—many of the trading losses discussed in Chapter 4 were directly or indirectly related to operational failures.
The mathematical sophistication of the field has grown substantially, aided by transfer of knowledge and techniques from the actuarial models applied to nonlife insurance. We need to remember, however, that the end goal is the management of risk. This is true for all areas of risk management but is particularly true for operational risk management. The mathematical modeling is important and there have been and there will be further strides going forward, but the modeling is only part of the overall management of operational risk.
And there are indeed real business benefits to operational risk management. More than one author claims that "operational risk has no upside for a bank" (McNeil, Frey, and Embrechts 2005, 464) or that "operational risk can only generate losses" (Jorion 2007, 497). This is not the case. To quote Blunden and Thirlwell (2010):
Operational risk management is not just about avoiding losses or reducing their effect. It is also about finding opportunities for business benefit and continuous improvement. (p. 33)
A simple example should suffice to make the point that focus on operations and operational risk management can have business benefits. Many hedge funds execute interest rate swaps as part of a trading strategy, and trading often starts with a small number of swaps (say, 1 to 10), traded infrequently and held to maturity. With such a small number of swaps and infrequent activity, the daily operational and settlement activities can be managed in a spreadsheet. A largely manual process can make sense from both a cost and an operational risk perspective: Costs can be controlled by avoiding investment in a costly new back-office system and risks can be controlled by suitable procedures and monitoring.
When the volume and frequency of trading increases, however, the operational tasks required in such a spreadsheet environment can be managed only by throwing people at the problem—and these people need to be skilled, careful, and responsible. In a spreadsheet environment, higher volumes involve increased operational risks. The alternative, an alternative that reduces operational risks but can also reduce costs at the same time, is to automate the process. The automation can be tailored to the scale of the operation, say, with an Access database and simple user interfaces to handle moderate volumes or a large-scale commercial back-office system for high volumes. Such automation not only reduces error rates but also allows trading volumes to grow without adding costs, thus increasing profit potential. This results in both a better operational risk profile and lower costs, a clear business benefit.
Operational risk management is important and growing. The topic is covered in many books. McNeil, Frey, and Embrechts (2005, ch. 10) provide a nice introduction to the technical modeling and probability theory. Jorion (2007) has a good general introduction. Chernobai, Rachev, and Fabozzi (2007) is a book devoted to the mathematics and probability theory for modeling loss distributions. My favorite overall treatment, however, is Blunden and Thirlwell (2010). They focus less on the mathematical details of modeling loss events and more on the management of operational risk. They emphasize the necessity for an overall risk management framework and plan, with the buy-in of senior management. This is right; since the goal of operational risk management is to manage the risks, it needs to be driven by senior management.
The remainder of this section provides an overview of operational risk management. This will be a high-level overview rather than a detailed account for two reasons. First, operational risk is an area that is changing rapidly and whatever I would write will likely be quickly outdated. Second, readers can turn to extensive treatments of the topic, some just mentioned, that have been published recently.
The approach I lay out for operational risk differs somewhat from that applied to market and credit risk. Here I focus more on the process of identifying risks, on qualitative assessment, and on analyzing business processes, with less attention attention paid to the quantitative measurement and probabilistic modeling.
The approach to operational risk can be summarized as falling into four stages:

1. Define: Define operational risk.
2. Identify and Assess: Identify and assess the sources and magnitude of risks associated with particular lines of business and activities. Identify risk indicators that are associated with the sources of risks.
3. Measure and Model: Use operational risk events to quantify losses and model the distribution of such losses.
4. Manage and Mitigate: develop and implement plans to manage, control, and mitigate the risks identified and measured in the first two stages.

The difference between operational and market or credit risk is more apparent than real, however. For measuring risk, we are still concerned with the P&L distribution—what are the possible outcomes for profit or loss?
But there are some aspects of operational risk that do set it apart. First, operational risk management is a newer discipline, and so it is natural that we need to focus relatively more effort on the first stages of defining and identifying operational risk.
Second, relative to market or credit risk, measurable data on operational risk causes and risk events are scarce, difficult to interpret, and heterogeneous. Operational risk events are internal to and specific to a company. Data are generally not reported publicly and even when they are, data for one company are often not relevant for another. (Consider incorrectlybooked trades, a classic operational risk. The frequency of such errors and the severity of any resulting losses depend on a firm's particular process, systems, and personnel.) As a result of the data paucity and heterogeneity, identifying and quantifying operational risks requires relatively more attention than for market or credit risks.
Finally, there is one fundamental difference between operational and other risks that we need to highlight. Market and credit risk are the reason for doing business; operational risk is a coincidental result of doing business. Market and credit risk are a central aspect of the business. When a portfolio manager buys a bond or when a bank makes a loan, the market or credit risk is actively solicited in the expectation of making a profit that compensates for assuming the risk. There may be many problems in measuring and managing it but the risk is front and center in deciding to undertake the business. Operational risk is different; it is an aftereffect, a result of doing business rather than the reason for the business. Nobody actively solicits the risk of wrongly booking a trade—the necessity to book trades is a result of doing the business and is not central in the way that price risk is central to the business of investing in a bond.
Operational risk is embedded in the business process rather than intrinsic to the financial product. Operational risk may be an unavoidable consequence of trading a bond but it is not intrinsic to the bond; the operational risk depends on the details of how the business is organized. The details of the operational risk will differ from one firm to another, even for firms in the same line of business. The details of the business must be examined to both measure and manage the risk. This requires more attention to the minutiae of the business processes, relative to market and credit risk where risk analysis applies generally to all instruments of a particular class, independent of which firm owns the instruments.
Stage 1—Define Operational Risk
Definitions matter. By giving something a name we give it a reality; we can speak about it with others. Until we decide what to include within the definition of operational risk, we have no hope of measuring it, and little prospect of managing it effectively.
A few years ago, definitions of operational risk were rather narrow, restricted to risk arising from operations: transactions processing and settlement, back-office systems failures, and so on. These areas are, obviously, crucial, but such a definition is too restrictive. It would exclude, for example, fraud perpetrated by a trader.
Following consultation with industry, the Basel Committee on Banking Supervision (BCBS) promulgated the following definition:
Operational risk is defined as the risk of loss resulting from inadequate or failed internal processes, people, and systems or from external events. This definition includes legal risk, but excludes strategic and reputational risk. (BCBS 2006, 144)
This definition was developed for commercial banks but it is a reasonable definition that could equally apply to virtually any organization.
This definition includes a wide variety of risks outside what we would usually consider financial risks. Losses related to people would include a trader's fraudulent trading, but also the loss of key personnel or breaches of employment law. Such events might be quite far from the market risk of our U.S. Treasury bond, but a loss is a loss and when $5 million walks out the door, it really doesn't matter if it is due to a fall in a bond price or a legal settlement on a claim of unfair dismissal. In fact, the settlement on the employment claim may be more irksome because it is not central to managing a portfolio—the price risk of buying a bond is inherent in a financial business but a well-managed firm should be able to avoid or minimize employment contract risks.
This is a high-level definition but we need to move to specifics, to specific risks. A good start is the categorization of losses that the BCBS provides (2006, annex 9). Table 12.6 shows the Level 1 categories for losses, categories that provide an exhaustive list of losses that would fall under the preceding definition.
Table 12.6 Basel Committee on Banking Supervisions (BCBS) Loss Event Type Categorization (Level 1)



Event-Type Category (Level 1)
Definition




Internal fraud
Losses due to acts of a type intended to defraud, misappropriate property or circumvent regulations, the law, or company policy, excluding diversity or discrimination events that involves at least one internal party


External fraud
Losses due to acts of a type intended to defraud, misappropriate property, or circumvent the law, by a third party


Employment Practices and Workplace Safety
Losses arising from acts inconsistent with employment and health or safety laws or agreements from payment of personal injury claims or from diversity or discrimination events


Clients, Products, and Business Practices
Losses arising from an unintentional or negligent failure to meet a professional obligation to specific clients (including fiduciary and suitability requirements), or from the nature or design of a product


Damage to Physical Assets
Losses arising from loss or damage to physical assets from natural disaster or other events


Business Disruption and System Failures
Losses arising from disruption of business or system failures


Execution, Delivery, and Process Management
Losses from failed transaction processing or process management, from relations with trade counterparties and vendors


Source: BCBS (2006) Annex 9.



These Level 1 loss event categories are still high level and the BCBS annex provides further granularity with Level 2 (event categories) and Level 3 (examples of activities associated with loss events). Table 12.7 shows Levels 2 and 3 for internal fraud and employment practices—the full table can be found on the Web.
Table 12.7 Basel Committee on Banking Supervisions (BCBS) Detailed Categories for Two Selected Level 1 Categories.

There is a very important point we need to highlight here: the distinction between loss events and operational risks. Blunden and Thirlwell emphasize the difference (2010, 15) and it is actually critical for managing risk.
The items categorized by the BCBS and shown in the two preceding tables are loss events—incidents associated with financial loss. We obviously care about such events and they rightly take center place in most analysis of operational risk. But for managing the risk, for taking remedial action, it is the cause of the event that we need to focus on. The cause is really the operational risk, or at least the focus for managing operational risk.
We want to think of operational risk or an operational event such as those categorized here as:

We might best explain the difference between these by means of the following example:



Event
Trader fraudulently hides a mistake when executing and booking an OTC option (option strike incorrectly booked). The mistake entails an unexpected (but moderate) loss on the option. The trader subsequently tries to trade his way out of the loss.


Effect
The original mistake plus losses on subsequent trading is several times the budgeted annual profit of the trading desk.


Cause
Two causes. First, poor user interface on options pricing screen makes it easy to confuse entry of $16/32 and $0.16. Second, back-office booking and reconciliation process and procedures fail to thoroughly check the deal as booked against counterparty confirms.


Focusing strictly on the loss event or the effect of the event (the monetary loss) would miss the underlying source of the event—poor software design and inadequate back-office process and procedures for reconciliation of trade confirms. For managing and mitigating this risk, we need to go back to ultimate causes. Concentrating on the fraudulent behavior of the trader is important but insufficient. Doing so could lead to misplaced or insufficient remedial action. A rule that traders must take mandatory holidays would help protect against fraud, but in this case, the real solution is to address the root cause of poor software interface and inadequate back-office procedures.
This distinction between observed events and underlying operational risk causes adds to the difficulty of measuring and managing operational risk. Loss events are already difficult to measure in a comprehensive manner. The added difficulty of tracing events back to root causes adds another layer of difficulty.
Stage 2—Identify and Assess the Risks in the Business
The goal here is to identify the sources of risk and prioritize them according to the impact they are likely to have on the business. This will involve at least some subjective and qualitative evaluation of the source as well as the impact of such risks. This information may be less precise than the objective and quantitative data to which we turn in the next section, but it is nonetheless valuable, even critical. Operational risks are embedded in the business itself, part of how the business is run. Managing a business relies on successfully using subjective and qualitative information, and so it is natural that in managing operational risk we should seek to exploit such information.
The specific risks of the business have to be identified, their impact assessed, and the data collected and catalogued. The word assess is commonly used instead of measure to reflect that the impact of operational risk is hard to quantify and will usually be less precise than for market or credit risk. The information developed here may be subjective and qualitative but it can still be collected, catalogued, and used. Organizing such information helps to identify key risks and points the direction toward managing such risk.
There are a variety of ways to get such data but they all rely on developing the information from within the business itself. This may take the form of interviews, questionnaires, or workshops. Whatever the form, there are a few broad considerations regarding the information being developed.
First, we want to separate the overall P&L impact of an operational risk into two components: the probability or frequency of occurrence and the size or severity. These two components, the frequency and severity, will not always be independent, but they are conceptually distinct and so it is far easier to estimate and measure them separately. Combining these two variables gives us the overall loss, and in the next section we examine how we do this mathematically.
For the present purpose of identifying and assessing risk, the frequency and severity might be represented in a minimal and qualitative manner. For example, frequency and severity might be estimated using a three-point score:

1. Low
2. Medium
3. High

The overall impact, the combination of frequency and severity, would then be the product, with a score from 1 to 9. This is a simplistic and subjective approach (and in this case would give only an estimate of the expected impact and not any unexpected or tail effect) but it may be sufficient to start with. The scheme might be extended to estimate average and awful cases.
A second consideration in identifying risks is that we will often want to examine operational risks at different levels of the organization. At the top level, we will be concerned with strategic risks. These will be issues that have an impact across business lines and potentially affect the business as a whole. They are related to high level goals and functions within the organization. Examples of such strategic risks could be:

 Failure to attract and retain key staff.
 Failure to understand and adhere to the relevant law and regulations.
 Weakness in information security systems.
 IT infrastructure that is inadequate to support business objectives.

At the middle level, there will be risks associated with processes and business lines. As an example, consider the back-office or mid-office function for a hedge fund, where daily or monthly net asset value is calculated and reconciled with the fund administrator, and trades are transmitted to and reconciled with the prime broker. Examples of such process or business line risks could be:

 Loss of key staff.
 Failure to coordinate holidays among key staff (leading to lack of coverage for crucial activities).
 Lack of adequate data backup and offsite systems replication to allow disaster recovery.
 Staff turnover at administrator, leading to a drop in reliability of producing the NAV.
 Errors at administrator in collecting prices leading to incorrect NAV reported to customers.

At the lowest, granular level there will be risks associated with specific business activities. Continuing with the example of the back-office and mid-office function for a hedge fund, the end-of-day reconciliation of positions versus prime broker holdings is a specific activity. Examples of activity risks for this could be:

 Failure to transmit trades to prime broker in a timely manner.
 Failure to properly allocate trades across multiple portfolios.
 Interruption of telecommunications link with prime broker for automated transmission of futures tickers.
 Late delivery of futures traded at other brokers to prime broker.
 Trader forgets to enter ticket into system.

There are other important aspects to consider in identifying operational risks. For example, the risk owner, the person managing the business unit or activity responsible for the risk, should be identified. Controls are usually built around operational risks. These controls are meant to eliminate or reduce the frequency or severity of risk events. Such controls should also be identified in the risk assessment because controls are a critical element in managing operational risk.
This discussion is only a brief overview of the issues. Blunden and Thirlwell (2010, ch. 4) is devoted to risk and control assessment and delves into these issues in more detail. Before continuing, however, it may help to fix ideas if we examine the output from a simple hypothetical risk assessment exercise. Table 12.8 shows the risk assessment for the risks mentioned earlier for the back-office or mid-office unit of a hedge fund. The highest two operational risks are failure to coordinate holidays among key staff, which leads to lack of coverage for critical functions, and errors at the administrator level that can lead to incorrect NAV being delivered to customers. The owner of the risk and the controls implemented to reduce these risks are also shown.
Table 12.8 Sample Risk Assessment.

The final issue I mention here is the development of risk indicators. These are usually called key risk indicators (KRIs) but would be better called indicators of key risks. The goal is to identify a set of measurable indicators that can tell us something about the current state of key risks and controls. For the risks shown in Table 12.8, an indicator of the risk of errors at the administrator leading to incorrect NAV might be the time required to reconcile the fund's internal NAV against the administrator NAV. More errors by the administrator would usually mean that the internal reconciliation (undertaken by the hedge fund) would take longer. This would not be a perfect indicator (there could be other sources for longer times) but it would be an indicator that attention should be directed at that area.
In closing the discussion of risk assessment, we should note the close connection between risk assessment as discussed here and the arenas of six sigma and continuous product improvement. This is hardly surprising, since operational risk is so closely connected with the running of the business. Financial firms are not manufacturing firms but many of the methods and ideas developed for removing defects in manufacturing processes can nonetheless be applied. We should also remember that the information developed here complements and supplements rather than substitutes for the quantitative information of the next section.
Stage 3—Measure and Model Losses
We now turn to the quantitative measurement and modeling of risk events and losses. This is the area that has benefited from the attention of mathematicians and statisticians, and there have been considerable advances over the past few years. As for all areas of risk management, however, we have to remember that the goal is managing risk, not mathematical rigor or sophisticated models per se. Blunden and Thirlwell (2010) state it well:
Much has been written about the mathematical modelling of operational risk. Unfortunately, almost all of the writing has been very mathematical and with very little focus on the business benefits. It is almost as though the modelling of operational risk should be sufficient in itself as an intellectual exercise. (p. 146)
Modeling is important—Blunden and Thirlwell go on to make clear that they are not arguing against modeling—but modeling must be in the service of an overall framework that harnesses such modeling to benefit the business.
The goal here is to model the distribution of losses resulting from operational risks. The approach is termed the actuarial approach or the loss-distribution approach (LDA). The loss we focus on is the total loss over a period of time, say, over a year. During the period, a random number of events may occur (zero, one,...) and for each event the loss may be large or small. The aggregate loss during the year results from combining the two component random variables:

The aggregate loss for the year is the sum of the loss amounts, summing over a random number of events:
(12.7) 
The random variable SN is called a compound sum (assuming the Xk all have the same distribution, and the N and X are independent).11 For a typical operational loss distribution, there will be a handful of events in a year. When an event does occur, it will most likely be a small or moderate loss but there will be some probability of a large event. Figure 12.5 shows a hypothetical distribution. Panel A is the frequency or probability of events during a year, with an average of two in a year and virtually no chance of more than seven. Panel B shows the severity or the probability of loss when an event occurs—high probability of small loss, small probability of large loss. Finally, Panel C shows the dollar losses during the year—the sum of losses over a year or the compound sum in equation (12.7).

Figure 12.5 Hypothetical Operational Risk Loss Distribution

The important point from Figure 12.5 is that the distribution is substantially skewed with a very long tail. (Note that as is conventionally done when talking about operational loss distributions, the sign is changed and losses are treated as positive numbers.) The long upper tail is one of the fundamental issues, and challenges, with modeling operational losses. There is a mass of high frequency low impact events, events that occur often but with low losses, and a small number of low frequency high impact events. These large losses are what cause significant damage to a firm, what keep managers awake at night. The large losses are particularly important for operational risk, but because they are so infrequent, they are particularly hard to measure and model.
The mathematics for working with the compound sums of equation (12.7) can be complex. But that is not the major hurdle facing quantitative modeling of operational risk. Data are the major issue. To quote McNeil, Frey, and Embrechts (2005):
The data situation for operational risk is much worse than that for credit risk, and is clearly an order of magnitude worse than for market risk, where vast quantities of data are publicly available. (p. 468)
Building a model using distributions such as in Figure 12.5 is appealing but daunting. A firm would have to collect data for many years, and even then would not have very many observations or even confidence that all events had been captured. Some public databases using pooled industry data are becoming available but significant challenges remain.
Even with the challenges that exist, the discipline imposed by a quantitative approach can be valuable, from both challenging and enriching how we think about the problem and from forcing us to confront real data.
Before turning to managing and mitigating operational risk, we need to review the Basel II capital charges for operational risk. The capital charges are important for two reasons. First in their own right because commercial banks have to hold capital calculated in this manner. Second and equally important, capital charges and the Basel II approach have spurred development of the field. The ideas behind the capital calculations provide a good starting point for data and modeling efforts.
Basel II provides for three tiered sets of calculations. The first two, called the basic-indicator (BI) and the standardized (S) approaches use gross income as an indicator of activity: "gross income is a broad indicator that serves as a proxy for the scale of business operations and thus the likely scale of operational risk exposure within...business lines." (BCBS 2006, par. 653). The difference between the basic-indicator and standardized approach is that the basic-indicator approach uses gross income for the business as a whole while the standardized approach uses gross income by business line, as defined by the BCBS (2006, annex 8) and shown in Table 12.9.
The basic-indicator approach uses gross income over three years (positive values only) and sets capital equal to a percentage of income (15 percent). The standardized approach uses gross income in each of the business lines shown in Table 12.9, with the factors shown applied to that business line (and allowing some offset across business lines).
Table 12.9 Business Lines and Standardized Capital Factors—Basel II.



Business Lines
Beta Factors




Corporate finance (β1)
18%


Trading and sales (β2)
18%


Retail banking (β3)
12%


Commercial banking (β4)
15%


Payment and settlement (β5)
18%


Agency services (β6)
15%


Asset management (β7)
12%


Retail brokerage (β8)
12%


Source: Basel Committee on Banking Supervision (2006, par. 654 and annex 8).



It is when we turn to the third, most sophisticated, approach that the modeling and data come to the fore. The advanced measurement approach (AMA) allows a bank to calculate capital according to its own internal risk measurement system. To qualify for the AMA, a bank must collect loss data by the eight business lines shown in Table 12.9, and within each business line according to the loss event types shown in Table 12.6. The bank cannot use AMA until they have collected five years of such data. There are additional criteria, as detailed in BCBS (2006).
The main point, however, is that AMA points banks in a useful direction, toward collecting and using loss data. By providing some standardization of the categories and criteria for collecting loss event data, the BCBS has provided a major impetus for development of operational risk modeling. Just collecting data on and monitoring losses is often a major step forward.
Stage 4—Manage and Mitigate the Risks
The final stage for operational risk management is to manage and mitigate the risks. The earlier stages have provided the necessary background, both qualitative and quantitative, for making informed strategic and tactical decisions.
With the sources of risks identified, and the size of actual and potential losses estimated or modeled, informed decisions can be made. Corrective and preventive actions can be undertaken. These might take the form of loss reduction (reducing the severity of losses when they occur); loss prevention (reduction in the frequency of occurrences); exposure avoidance (simply avoiding the activity, an extreme form of loss prevention); or mitigation (insurance).
The link between good operational risk management and continuous process improvement and six-sigma ideas was highlighted earlier. In the end, it is competent managers and an appropriate corporate culture that provides the best protection against operational risk.
12.5 Conclusion
Operational and liquidity risk are the poor cousins of market and credit risk. Progress has been made, particularly in the arena of operational risk, but much more work needs to be done. Market and credit risk are more developed partly because they are easier, higher profile, and more amenable to quantitative analysis, with data readily available. Losses from liquidity and operational events are just as painful, however.
There are additional risks that a firm will face. Strategic and reputational risk is explicitly excluded from the BCBS definition of operational risk, but failures in these areas can be the most damaging to a firm in the long run. Yet it might be right to exclude them, as they fall so entirely in the realm of traditional management, with quantitative and mathematical techniques having little to offer.
Notes
1. The combination of a bursting asset bubble and a liquidity crisis has been quite common over history—for the United States, think about 1873, 1893, 1907-1908, and 1929-1933 (the Great Depression).
2. Also see Jorion (2007, section 13.2) for a discussion of these asset liquidity issues.
3. From Figure 12.2 it might appear that a longer liquidation period is always better. This is not the case. As we liquidate over longer periods the volatility eventually rises much more than the cost decreases. This can be seen in Table 12.1
4. As mentioned earlier, this section follows Marrison (2002, ch. 14).
5. This shows the benefit of defining and analyzing the cash flows as in equations (12.5) and (12.6). By collecting and analyzing the cash flows for normal times, we have at hand estimates of the usual flows plus estimates of the usual variation.
6. The initial margin required for a futures contract is not a payment for the contract but a sum held by the exchange to ensure and facilitate the payment of daily mark-to-market amounts. This initial margin does serve to limit the leverage an investor can obtain through the futures contract, but the initial margin is a mechanism to manage and control counterparty credit exposure rather than an investment in or payment for an asset.
7. Remember that R is the sum of unscheduled and semidiscretionary cash flows: R = (OU + OSD) - (IU + ISD), assumed to be random, for example, normal with mean μR and volatility σR.
8. On swaps contracts broker-dealers generally require commercial customers to post collateral from the customer to the dealer, but often insist that the dealer not be required to post collateral to the customer.
9. The hedge strategy was by no means perfect so that the losses on the futures were not fully offset by gains on the customer contracts. The main issue here, however, is the asymmetry of the up-front cash paid on the exchange-traded futures versus no cash or collateral transfers for the customer contracts.
10. Nocera (2009) relates how Goldman cut back on exposure to mortgages in 2006 and 2007, anticipating problems with the mortgage markets. And yet when the liquidity crisis hit in late 2008 and early 2009, Goldman suffered along with other banks and investment banks. They protected themselves, and survived better than others, but were still caught in the turmoil.
11. This approach is called actuarial because much of the mathematical theory comes from the actuarial and insurance arena. See McNeil, Frey, and Embrechts (2005, section 10.2) for a discussion of the mathematical details.









Chapter 13
Conclusion
With this book we have taken a tour through risk management in its majesty. We have covered much, but there is also much that we have not covered. Risk, management, and financial markets are all evolving. That is good but provides challenges for any manager who takes his responsibilities seriously.
In closing, I simply reiterate what I see as the central, in fact, the only, important principle of risk management: Risk management is managing risk. This sounds simple but it is not. To properly manage risk, we need to understand and use all the tools covered in this book, and even then we will not be able to foretell the future and will have to do the best we can in an uncertain world.
Risk management is the core activity of a financial firm. It is the art of using what we learn from the past to mitigate misfortune and exploit future opportunities. It is about making the tactical and strategic decisions to control risks where we can and to exploit those opportunities that can be exploited. It is about managing people and processes, about setting incentives and implementing good governance. Risk management is about much more than numbers. "It's not the figures themselves, it's what you do with them that matters," as Lamia Gurdleneck says.1
Risk measurement and quantitative tools are critical aids for supporting risk management, but quantitative tools do not manage risk any more than an auditor's quarterly report manages the firm's profitability. In the end, quantitative tools are as good or as poor as the judgment of the person who uses them. Many criticisms of quantitative measurement techniques result from expecting too much from such tools. Quantitative tools are no substitute for judgment, wisdom, and knowledge. A poor manager with good risk reports is still a poor manager.
Managing a firm, indeed life itself, is often subject to luck. Luck is the irreducible chanciness of life. The question is not whether to take risks—that is inevitable and part of the human condition—but rather to appropriately manage luck and keep the odds on one's side. The philosopher Rescher has much good advice, and in closing, it is worth repeating his recommendations:
The bottom line is that while we cannot control luck through superstitious interventions, we can indeed influence luck through the less dramatic but infinitely more efficacious principles of prudence. In particular, three resources come to the fore here:

1. Risk management: Managing the direction of and the extent of exposure to risk, and adjusting our risk-taking behavior in a sensible way over the overcautious-to-heedless spectrum.
2. Damage control: Protecting ourselves against the ravages of bad luck by prudential measures, such as insurance, "hedging one's bets," and the like.
3. Opportunity capitalization: Avoiding excessive caution by positioning oneself to take advantage of opportunities so as to enlarge the prospect of converting promising possibilities into actual benefits. (2001, 187)

Notes
1. From The Undoing of Lamia Gurdleneck by K. A. C. Manderville, in Kendall and Stuart (1979, frontispiece).









About the Companion Web Site
Much of this book is technical and quantitative. We have provided supplementary material on an associated web site (www.wiley.com/go/qrm) to aid in the use and understanding of the tools and techniques discussed in the text. The material falls into two broad categories.
The first is a set of routines, written in matlab that implements the parametric estimation of portfolio volatility, together with basic portfolio tools such as contribution to risk and best hedges. These routines demonstrate the practical implementation of a risk measurement system. We assume that market history and portfolio sensitivities are supplied externally. The routines then calculate the portfolio volatility, volatility for various sub-portfolios, and best hedges and replicating portfolios. The objective is to provide routines that demonstrate the ideas discussed in the text. We do not aim to provide a working risk measurement system but instead to show how the ideas in the book are translated into working code.
The second set of materials is appendixes that expand on ideas in individual chapters in the form of interactive digital documents. For example, Figure 8.4 in the text explains VaR by means of the P&L distribution for a US Treasury bond. The digitally-enhanced appendix to Chapter 8 discusses the volatility but makes the discussion interactive. Using Wolfram's Computable Document Format the user can choose the VaR probability level, the instrument (bond, equity futures, etc.), the notional amount, and the assumed distribution (normal, Student-t, mixture of normals). The document dynamically computes the VaR and draws the P&L distribution, allowing the user to see how the VaR varies as assumptions or various aspects of the portfolio change.









References
Abramowitz, Milton, and Irene A. Stegun. 1972. Handbook of Mathematical Functions. New York: Dover Publications.
Aczel, Amir D. 2004. Chance: A Guide to Gambling, Love, the Stock Market, & Just About Everything Else. New York: Thunder's Mouth Press.
Adler, David. 2009. Snap Judgment. Upper Saddle River, NJ: FT Press.
Alexander, Carol. 2001. Market Models: A Guide to Financial Data Analysis. New York: John Wiley & Sons.
Artzner, P., F. Delbaen, J. M. Eber, and D. Heath. 1999. "Coherent Measures of Risk." Mathematical Finance 9: 203-228.
Bailey, Jeffrey V., William F. Sharpe, and Gordon J. Alexander. 2000. Fundamentals of Investments. 3rd ed. New York: Prentice Hall.
Basel Committee on Banking Supervision. Undated. About the Basel Committee. www.bis.org/bcbs.
_____________. 2003. Sound Practices for the Management and Supervision of Operational Risk. BIS. www.bis.org/publ/bcbs96.htm.
_____________. 2004. Basel II: International Convergence of Capital Measurement and Capital Standards: a Revised Framework. BIS. www.bis.org/publ/bcbs107.htm.
_____________. 2006. Basel II: International Convergence of Capital Measurement and Capital Standards: A Revised Framework—Comprehensive Version. BIS. www.bis.org/publ/bcbs128.htm.
_____________. 2011. Principles for the Sound Management and Supervision of Operational Risk. BIS, June. www.bis.org/publ/bcbs195.htm.
Beirlant, Jan, Wim Schoutens, and Johan Segers. 2005. "Mandelbrot's Extremism." Wilmott Magazine, March.
Bernstein, Peter L. 2007. Capital Ideas Evolving. Hoboken, NJ: John Wiley & Sons.
Billingsley, Patrick. 1979. Probability and Measure. New York: John Wiley & Sons.
Bingham, N. H., and R. Kiesel. 1998. Risk-Neutral Valuations. New York: Springer.
Blunden, Tony, and John Thirlwell. 2010. Mastering Operational Risk. Harlow, UK: Pearson Education Ltd.
Box, G. E. P., and G. M. Jenkins. 1970. Time Series Analysis: Forecasting and Control. San Francisco: Holden-Day.
Brand, L., and R. Bahr. 2001. Ratings Performance 2000: Default, Transition, Recovery, and Spreads. Standard & Poor's.
Carty, L. V., and D. Lieberman. 1996. Defaulted Bank Loan Recoveries. Special Report. Global Credit Research. Moody's Investors Service.
Chernobai, Anna S., Svetlozar T. Rachev, and Frank J. Fabozzi. 2007. Operational Risk. Hoboken, NJ: John Wiley & Sons.
Chernozhukov, Victor, Ivan Fernandez-Val, and Alfred Galichon. 2007. Rearranging Edgeworth-Cornish-Fisher Expansions, September. www.mit.edu/vchern/papers/EdgeworthRearranged-posted.pdf.
Coleman, Thomas S. 1998a. Fitting Forward Rates to Market Data. January 27. http://ssrn.com/abstract=994870.
_____________ 1998b. A Practical Guide to Bonds and Swaps. February 20. http://ssrn.com/abstract=1554029.
_____________ 2007. Estimating the Correlation of Non-Contemporaneous Time-Series. December 13. http://ssrn.com/abstract=987119.
_____________ 2009. A Primer on Credit Default Swaps (CDS). December 29. http://ssrn.com/abstract=1555118.
_____________ 2011a. A Guide to Duration, DV01, and Yield Curve Risk Transformations. January 15. http://ssrn.com/abstract=1733227.
_____________ 2011b. Probability, Expected Utility, and the Ellsberg Paradox. February 26. http://ssrn.com/abstract=1770629.
Coleman, Thomas S., and Larry B. Siegel. 1999. "Compensating Fund Managers for Risk-Adjusted Performance." Journal of Alternative Investments 2(3): 9-15.
Cramér, Harald. 1974. Mathematical Methods of Statistics. Princeton, NJ: Princeton University Press. First published 1946.
Credit Suisse Financial Products. 1997. CreditRisk+—A Credit Risk Management Framework. Credit Suisse Financial Products.
Crosbie, Peter, and Jeff Bohn. 2003. Modeling Default Risk. Moody's KMV, December 18. www.moodyskmv.com.
Crouhy, Michel, Dan Galai, and Robert Mark. 2001. Risk Management. New York: McGraw-Hill.
_____________ 2006. Essentials of Risk Management. New York: McGraw-Hill.
Drezner, Z. 1978. "Computation of the Bivariate Normal Integral." Mathematics of Computation 32 (January): 277-79.
Duffie, Darrel. 2001. Dynamic Asset Pricing Theory. 3rd ed. Princeton, NJ: Princeton University Press.
Duffie, Darrel, and Kenneth J. Singleton. 2003. Credit Risk: Pricing, Measurement, and Management. Princeton Series in Finance. Princeton, NJ: Princeton University Press.
Eatwell, John, Murray Milgate, and Peter Newman, eds. 1987. The New Palgrave: A Dictionary of Economics. London: Macmillan Press Ltd.
Ellsberg, Daniel. 1961. "Risk, Ambiquity, and the Savage Axioms." The Quarterly Journal of Economics 75 (4, November): 543-669.
Embrechts, Paul, Claudia Klüppelberg, and Thomas Mikosch. 2003. Modelling Extremal Events for Insurance and Finance. corrected 4th printing. Berlin: Springer Verlag.
Epstein, Larry G. 1999. "A Definition of Uncertainty Aversion." Review of Economic Studies 66 (3, July): 579-608.
Feller, William. 1968. An Introduction to Probability Theory and Its Applications, Volume I. Third Edition, Revised Printing. New York: John Wiley & Sons.
Felsted, Andrea, and Francesco Guerrera. 2008. "Inadequate Cover." Financial Times, October 7.
Felsted, Andrea, Francesco Guerrera, and Joanna Chung. 2008. "AIG's Complexity Blamed for Fall." Financial Times, October 7.
Friedman, Milton, and Anna Jacobson Schwartz. 1963. A Monetary History of the United States, 1857-1960. Princeton, NJ: Princeton University Press.
Frydl, Edward J. 1999. The Length and Cost of Banking Crises. International Monetary Fund Working Paper. Washington DC: International Monetary Fund, March.
Gardner, Martin. 1959. "Mathematical Games." Scientific American, October.
Garman, M. B. 1996. "Improving on VaR." Risk 9(5): 61-63.
Gigerenzer, Gerd. 2002. Calculated Risks: Learning How to Know When Numbers Deceive You. New York: Simon & Schuster.
_____________ 2007. Gut Feelings: The Intelligence of the Unconscious. New York: Penguin Group.
Gladwell, Malcolm. 2005. Blink. New York: Little, Brown and Company.
_____________ 2009. "Cocksure: Banks, Battles, and the Psychology of Overconfidence." The New Yorker, July 27.
Gordy, M. B. 2000. "A Comparative Anatomy of Credit Risk Models." Journal of Banking and Finance 24: 119-149.
Hacking, I. 1990. The Taming of Chance. Cambridge, UK: Cambridge University Press.
Hacking, Ian. 2001. Probability and Inductive Logic. New York: Cambridge University Press.
Hacking, I. 2006. The Emergence of Probability. 2nd ed. Cambridge, UK: Cambridge University Press.
Hadar, J., and W. Russell. 1969. "Rules for Ordering Uncertain Prospects." American Economic Review 59: 25-34.
Hald, A. 1952. Statistical Theory with Engineering Applications. New York: John Wiley & Sons.
Hanoch, G., and H. Levy. 1969. "The Efficiency Analysis of Choices involving Risk." Review of Economic Studies 36: 335-346.
Hoffman, Paul. 1998. The Man Who Loved Only Numbers: The Story of Paul Erdos and the Search for Mathematical Truth. New York: Hyperion.
Holm, Erik, and Margaret Popper. 2009. "AIG's Liddy Says Greenberg Responsible for Losses." Bloomberg website, March 2.
Hull, John C. 1993. Options, Futures, and Other Derivative Securities. 2nd ed. Englewood Cliffs, NJ: Prentice-Hall.
Isserlis, L. 1918. "On a Formula for the Product-Moment Coefficient of Any Order of a Normal Frequency Distribution in Any Number of Variables." Biometrika 12: 134-139.
Jorion, Philippe. 2007. Value-at-Risk: The New Benchmark for Managing Financial Risk. 3rd ed. New York: McGraw-Hill.
_____________ 2000. "Risk Management Lessons from Long-Term Capital Management." European Financial Management 6(3): 277-300.
Kahneman, Daniel, and Amos Tversky. 1973. "On the Psychology of Prediction." Psychological Review 80: 237-251.
Kahneman, Daniel, Paul Slovic, and Amos Tversky, eds. 1982. Judgment under Uncertainty: Heuristics and Biases. Cambridge, UK: Cambridge University Press.
Kaplan, Michael, and Ellen Kaplan. 2006. Chances Are.. .Adventures in Probability. New York: Viking Penguin.
Kendall, Maurice, and Alan Stuart. 1979. Advanced Theory of Statistics. Fourth. Vol. 2. 3 vols. New York: Macmillan.
Keynes, John Maynard. 1921. A Treatise on Probability. London: Macmillan.
Kindleberger, Charles P. 1989. Manias, Panics, and Crashes: A History of Financial Crises. Revised Edition. New York: Basic Books.
Kmenta, Jan. 1971. Elements of Econometrics. New York: Macmillan.
Knight, Frank. 1921. Risk, Uncertainty and Profit. Boston: Houghton Mifflin Co.
Laeven, Luc, and Fabian Valencia. 2008. "Systemic Banking Crises: A New Database." IMF Working Paper.
Lakatos, Imre. 1976. Proofs and Refutations: The Logic of Mathematical Discovery. Cambridge, UK: Cambridge University Press.
Langer, Ellen. 1975. "The Illusion of Control." Journal of Personality and Social Psychology 32(2): 311-328.
Langer, Ellen, and Jane Roth. 1975. "Heads I Win, Tails It's Chance: The Illusion of Control as a Function of Outcomes in a Purely Chance Task." Journal of Personality and Social Psychology 32(6): 951-955.
LeRoy, Stephen F., and Larry D. Singell Jr. 1987. "Knight on Risk and Uncertainty." Journal of Political Economy 95 (2, April): 394. doi:10.1086/261461doi:10.1086/261461
Litterman, R. 1996. "Hot Spots™ and Hedges." Journal of Portfolio Management (Special Issue) (December): 52-75.
Lleo, Sébastien. 2008. Risk Management: A Review. London: CFA Institute Publications.
Lowenstein, Roger. 2000. When Genius Failed: The Rise and Fall of Long-Term Capital Management. New York: Random House.
Mackay, Charles. 1932. Extraordinary Popular Delusions and the Madness of Crowds. New York: Farrar Straus Giroux.
Mahajan, Sanjoy, Sterl Phinney, and Peter Goldreich. 2006. Order-of-Magnitude Physics: Understanding the World with Dimensional Analysis, Educated Guesswork, and White Lies. March 20. www.stanford.edu/class/ee204/SanjoyMahajanIntro-01-1.pdf.
Markowitz, Harry M. 1959. Portfolio Selection. Malden, MA: Blackwell Publishers.
_____________ 2006. "de Finetti Scoops Markowitz." Journal of Investment Management 4 (3, Third Quarter). Online only, and password protected, at www.joim.com.
Marrison, Chris. 2002. Fundamentals of Risk Measurement. New York: McGraw-Hill.
Maslin, Janet. 2006. "His Heart Belongs to (Adorable) iPod." New York Times, October 19.
Mauboussin, Michael, and Kristin Bartholdson. 2003. "On Streaks: Perception, Probability, and Skill." Consilient Observer (Credit Suisse-First Boston), April 22.
McCullagh, P., and J. A. Nelder. 1989. Generalized Linear Models. 2nd ed. London: Chapman & Hall.
McNeil, Alexander, Rudiger Frey, and Paul Embrechts. 2005. Quantitative Risk Management. Princeton, NJ: Princeton University Press.
Merton, Robert C. 1974. "On the Pricing of Corporate Debt: The Risk Structure of Interest Rates." Journal of Finance 29 (2, May): 449-470.
Mirrlees, J. 1974. Notes on welfare economics, information, and uncertainty. In Contributions to Economic Analysis, ed. M. S. Balch, Daniel L. McFadden, and S. Y. Wu. Amsterdam: North Holland.
_____________ 1976. "The Optimal Structure of Incentives and Authority within an Organization." Bell Journal of Economics 7(1): 105-131.
Mlodinow, Leonard. 2008. The Drunkard's Walk: How Randomness Rules Our Lives. New York: Pantheon Books.
New School. Riskiness. http://homepage.newschool.edu/het//essays/uncert/increase.htm.
Nocera, Joe. 2009. "Risk Mismanagement." New York Times, January 4, Magazine sec. www.nytimes.com/2009/01/04/magazine/04risk-t.html?_r=1&ref=business.
Press, William H., Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2007. Numerical Recipes, 3rd ed. New York: Cambridge University Press.
Reinhart, Carmen M., and Kenneth S. Rogoff. 2009. This Time Is Different: Eight Centuries of Financial Folly. Princeton, NJ: Princeton University Press.
Rescher, Nicholas. 2001. Luck: The Brilliant Randomness of Everyday Life. New York: Farrar Straus Giroux.
RiskMetric Group, Greg M., Greg M. Gupton, and Christopher C. Finger. 1997. CreditMetrics—Technical Document. RiskMetrics Group. www.riskmetrics.com/publications/techdocs/cmtdovv.html.
Rosenhouse, Jason. 2009. The Monty Hall Problem: The Remarkable Story of Math's Most Contentious Brainteaser. New York: Oxford University Press.
Ross, Stephen. 1973. "The Economic Theory of Agency: The Principal's Problem." American Economic Review 63 (2, May): 134-139.
Rothschild, M., and J. E. Stiglitz. 1970. "Increasing Risk I: A definition." Journal of Economic Theory 2(3): 225-243.
_____________. 1971. "Increasing Risk II: Its economic consequences." Journal of Economic Theory 3(1): 66-84.
Schmeidler, David. 1989. "Subjective Probability and Expected Utility Without Additivity." Econometrica 57 (3, May): 571-587.
Selvin, S. 1975a. "On the Monty Hall Problem." American Statistician 29: 134.
_____________. 1975b. "A Problem in Probability." American Statistician 29: 67.
Shaw, W. T., and K. T. A. Lee. 2007. Copula Methods vs. Canonical Multivariate Distributions: The Multivariate Student T Distribution with General Degrees of Freedom. Kings College, London, April 24.
Stiglitz, J. E. 1974. "Incentives and Risk Sharing in Sharecropping." Review of Economic Studies 41 (April): 219-255.
Stiglitz, J.E.. 1975. "Incentives, Risk, and Information: Notes Toward a Theory of Hierarchy." Bell Journal of Economics 6(2): 552-579.
Taleb, Nassim. 2004. Fooled by Randomness. New York: Random House.
_____________. 2007. The Black Swan: The Impact of the Highly Improbable. New York: Random House.
The Economist. 2008. "AIG's Rescue: Size Matters." The Economist, September 18. www.economist.com/finance/displaystory.cfm?story_id=12274070.
Tremper, Bruce. 2008. Staying Alive in Avalanche Terrain. 2nd ed. Seattle WA: The Mountaineers Books.
Tversky, Amos, and Daniel Kahneman. 1974. "Judgment under Uncertainty: Heuristics and Biases." Science 185(4157): 1124-1131.
_____________. 1983. "Extensional versus Intuitive Reasoning: The Conjunction Fallacy in Probability Judgment." Psychological Review 90 (4, October): 293-315.
Valencia, Mathew. 2010. "The Gods Strike Back." Economist, February 11.
Varian, Hal R. 1978. Microeconomic Analysis. W. W. Norton & Company.
vos Savant, Marilyn. 1990a. "AskMarilyn." Parade, September 9.
_____________. 1990b. "AskMarilyn." Parade, December 2.
_____________. 1996. The Power of Logical Thinking. New York: St. Martin's Press.
Wechsberg, Joseph. 1967. The Merchant Bankers. London: Weidenfeld and Nicolson.
WillmerHale. 2008a. Rogue Traders: Lies, Losses, and Lessons Learned. Wilmer Hale, March. www.wilmerhale.com/files/Publication/738ab57a-ba44-4abe-9c3e-24ec62064e8d/Presentation/PublicationAttachment/a5a7fbb0-e16e-4271-9d75-2a68f7db0a3a/Rogue%20Trader%20Article%20FINAL%20for%20Alert.pdf.
Young, Brendon, and Rodney Coleman. 2009. Operational Risk Assessment. Chichester, UK: John Wiley & Sons.









About the Author
Thomas S. Coleman has worked in the finance industry for more than 20 years and has considerable experience in trading, risk management, and quantitative modeling. Mr. Coleman currently manages a risk advisory consulting firm. His previous positions have been head of quantitative analysis and risk control at Moore Capital Management, LLC (a large multi-asset hedge fund manager), and a director and founding member of Aequilibrium Investments Ltd., a London-based hedge fund manager. Mr. Coleman worked on the sell side for a number of years, with roles in fixed-income derivatives research and trading at TMG Financial Products, Lehman Brothers, and S. G. Warburg in London.
Before entering the financial industry, Mr. Coleman was an academic, teaching graduate and undergraduate economics and finance at the State University of New York at Stony Brook, and more recently he has taught as an adjunct faculty member at Fordham University Graduate School of Business Administration and Rensselaer Polytechnic Institute. Mr. Coleman earned his PhD in economics from the University of Chicago and his BA in physics from Harvard. He is the author, together with Roger Ibbotson and Larry Fisher, of Historical U.S. Treasury Yield Curves and continues to publish in various journals.









Index
Activity-related operational risks
Actuarial approach
operational loss measurement
and risk measurement
risk-neutral approach vs.
and risk pricing
Aczel, Amir D.
Advanced measurement approach (AMA), capital charges
Aggregating risk, summary measures
AIB/Allfirst Financial trading loss (2002)
AIG Financial Products (FP) trading loss (2008)
All-or-nothing contribution to risk
Amaranth Advisors trading loss (2006)
Ambiguity aversion. See uncertainty/randomness 
American Alpine Club
Aracruz Celulose trading loss (2008)
ARCH (autoregressive conditionally heteroscedastic) model
Ars Conjectandi (Bernoulli)
Askin Capital Management trading loss (1994)
Asset liquidity risk
costs/benefits of liquidation
defined
evaluating
Asset-to-risk factor mapping
conceptual models
FX example
for single bond position
Asymmetric (skewed) distribution
and credit risk modeling
credit vs. market risk
and volatility
Asymmetric information
historical vs. future data
principal-agent issues
Autoregressive conditionally heteroscedastic (ARCH) model
Avalanche response, as risk management model

Back-office procedures
effective risk management
as source of operational risk
Bagehot, Walter
Bank for International Settlements (BIS)
Bankhaus Herstatt trading loss (1974)
Bank of Montreal trading loss (2007)
Banks
commercial, regulations governing
corporate structure and risk management
defining assets, capital holdings
and funding liquidity risk
measuring and managing liquidity
operational risk
Barings Bank failure (1995)
Barings Brothers failure (1890)
Basel Committee on Banking Supervision. See BCBS 
Basel II/Basel III rules
Basic-indicator (BI) approach, capital charge calculation
Basis point value (BPV)
BAWAG trading loss (2000)
Bayes, Thomas
Bayes' rule/Theorem
BCBS (Basel Committee on Banking Supervision)
Bear Stearns (2008) failure
Beliefs/belief inertia
Belief-type probability
Bayes' rule
de Finetti game
with frequency-type probability
logical probability
Berger, Michael
Berkeley, George
Bernoulli, Jakob
Bernoulli mixture models
applications
parameters
Poisson mixture comparison
using Poisson variable in
Bernoulli Probit-Normal Mixture
Bernoulli's Theorem
Best hedge position calculations
Best practices
Beta-equivalent notational
Binning for fixed-income instruments
Binomial distribution
as analytic approach
Bernoulli trials
for defaults
negative
BIS (Bank for International Settlements)
Blink (Gladwell)
Blunden, Tony
Board of directors, role in risk management
Bonds
asset/risk factor mapping
comparing multiple assets
corporate, default example
and credit risk modeling
DV01/BPV
floating-rate, and CDS behavior
liquidation costs
portfolio analysis involving
and rate swaps
risks associated with
risky, pricing model for
and share value
tail events associated with
volatility contributions/comparisons
BPV (basis point value)
Brazil currency markets, and trading loss events
Breast cancer risk calculations

CAC index futures
best hedge positions/replicating portfolios
beta-equivalent position
estimating volatility of
liquidity of
marginal contribution calculations
and normal distribution
in parametric estimates
portfolio analysis example
volatility contributions/comparisons
Calyon trading loss (2007)
Capital asset pricing model
Capital charges (Basel II), calculating
Capital holdings
Cash flow. See also Funding liquidity risk
cash-flow mapping
credit default swaps
future, valuing
and interest rate swaps
and liquidity risk
and market/cash distribution
risky bonds
Cash flow mapping
Cayne, Jimmy
CDSs (credit default swaps)
and AIG Financial Products failure
applying market pricing to
behavior of and risk calculations
equivalence to floating rate note
pricing model
Central limit theorem
Central tendency
CEO (chief executive officer), risk management responsibilities
China Aviation Oil (Singapore) trading loss (2004)
CITIC Pacific trading loss (2008)
Citron, Robert
Closing-time problem
Codelco trading loss (1993)
Cognitive biases
Coleman, Thomas S.
Collateral calls
Commodity price risk
Common factor structure
Communications. See Risk communication/reporting
Compensation and incentives
Constructivist (actuarial) approach
Contribution to risk tools
Convolution
Copulas
Corporate structure. See also Board of directors; CEO (chief executive officer)
Correlations
assets within a portfolio
correlation matrix estimates
credit risk modeling
daily, and portfolio risk estimates
and diversification
and joint default probability
over time
and risk reduction potential
Counterparty risk
Covariance. See also Variance-covariance distribution
Cramér, Harald
Credit analysis
Credit default correlations
Credit default swaps. See CDS (credit default swaps)
CreditMetrics
Credit migration
Credit risk
data inputs
defined
legal issues
limits, implementing
market risk vs.
operational risk vs.
and P&L distribution estimates
and risk-weighted assets
varieties of
CreditRisk+ model
assumptions
conditional independence across firms
CreditMetrics model comparison
credit risk pricing vs.
intensity volatility and default correlation
loss distribution
overview
parameters
Poisson process, Poisson mixture, and negative binomial default distribution
specific factor
static vs. dynamic models
Credit risk modeling
Bernoulli vs. Poisson mixture models
equivalent Martingale/risk-neutral pricing
reduced form approach
risk pricing approach
static/structural approach
stylized approaches
taxonomy, overview
technical challenges
Credit structures, types of
Credit Suisse Financial Products
Crisis funding requirements
CRO (chief risk officer)
Cross-currency settlement risk
Crouhy, Michel

Daily volatility
Daiwa Bank trading loss (1995)
Damage control
Data
for asset liquidity risk estimates
for bank funding liquidity risk estimates
for credit risk estimates
historical vs. future
internal vs. external
and IT infrastructure
for operational risk estimates
Default probability
De Finetti, Bruno/ De Finetti game
Delta normal distribution. See Parametric approach
Dependence
across defaults
across firms
copulas
credit risk distributions
credit risk modeling
multivariate analyses
tail dependence
Derivatives, second
and funding liquidity risk
parametric estimation using
Desk-level traders, view of risk
Dexia Bank trading loss (2001)
Dimensionality
Disasters, financial. See Financial risk events; Tail (extreme) events 
Dispersion. See Scale
Dispersion/density functions
Diversification
Dollar duration
Dow Jones Industrial Average
Dynamic reduced form risk pricing

Econometrics
Economic capital
and credit risk modeling
crisis funding
Elliptical distributions
Ellsberg, Daniel/Ellsberg paradox
Embedded options
Embrechts, Paul
Employer-employee relations
Equity price risk
Equity traders
Equivalent Martingale/risk-neutral approach to risk pricing
Erdös, Paul
ES (expected shortfall)
Exponential weighting
Exposure, measuring
Extreme events. See Tail (extreme) events
Extreme value theory (EVT)

Factors, factor loadings, principal component analysis
Failure to segregate, as cause of trading loss event
Familiarity and effectiveness
Fannie Mae/Freddie Mac
Fat tails
Feller, William
Finance unit (risk management group)
Financial times series
5%/95% VaR
Fixed-income traders
Foreign exchange. See FX (foreign exchange) speculation
Franklin, Benjamin
Fraudulent trading
and financial loss events
fraud without
and operational risk management
preventing
tangential fraud
types of fraud
Fréchet-class distribution
Frequency-type probability
Frey, Rudiger
FRN (floating-rate notes)
Front-back office separation, and trading loss events
Funding liquidity risk
defined
and derivatives
leveraged instruments
market-to-market payments and market/cash volatility
Metallgesellschaft trading loss (1993)
risk management using
FX (foreign exchange) speculation
as cause of trading losses
forward contracts, risks associated with
risk estimates, valuation model

Galai, Dan
Gamma random variable
GARCH (generalized autoregressive conditionally heteroscedastic) model
Gardner, Martin
Garman, M. B.
General Electric
Generalized linear mixed credit risk models
Generalized pareto distribution. See GPD 
GEV (generalized extreme value) distribution
Gigerenzer, Gerd
Gladwell, Malcolm
Global financial crisis
Goldman Sachs
Gordy, M. B.
GPD (generalized pareto distribution)
Groupe Caisse d'Epargne trading loss (2008)
Gumbel-class distribution

Hacking, Ian
Haldane, Andrew
Hedge funds
loss events
operational risk
performance fees
Herstatt. See Bankhaus Herstatt
Heuristics (rules of thumb)
High-water mark
Historical approach
asset to risk factor mapping
modeling
P&L distribution
parametric and Monte Carlo approaches vs.
summary
volatility and VaR
Hot Spots and Hedges (Litterman)
Human factor
Hyperinflation
Hypo Group Alpe Adria trading loss (2004)

Idiosyncratic risk
system risk vs.
trading loss events-2008
Iguchi, Toshihide
Incentive schemes
Incremental VaR. See All-or-nothing contribution to risk
Infinitesimal contribution to risk. See Marginal contribution to risk
Inflation
Innumeracy, statistical, overcoming
Interest rate risk
Intuition, human
and probability
role in risk management
IRSs (interest rate swaps)
IT (information technology) infrastructure needs

Japan, banking crises
Jett, Joseph
Jobs, Steve
Jorion, Philippe
JPMorgan

Kahneman, Daniel
Kashima Oil Co. trading loss (1994)
Kealhofer, Stephen
Kerviel, Jérôme
Keynes, John Maynard
Kidder, Peabody & Co. trading loss (1994)
Kindleberger, Charles P.
Kluppelberg, Claudia
Kmenta, Jan
Knight, Frank
Kolmogorov, A. N.
KRIs (key risk indicators)

Langer, Ellen
Laplace, Pierre-Simon
Law of large numbers
Lee, David
Lee, K. T. A.
Leeson, Nick
Legg Mason Value Trust performance
Legitimate practices, trading losses from
Lehman Brothers' trading loss (2008)
LeRoy, Stephen F.
Let's Make a Deal (TV show)
Leveraged instruments. See also CDS (credit default swaps); Hedge funds
defined
and liquidity risk
speculation in
LGD (loss given default)
Limits, implementing
"Linda the Bank Teller" example
Linear mixed models, generalized
Line managers
Liquidating assets, costs
Liquidity risk
asset liquidity risk
asset vs. funding liquidity
credit vs. market risk
funding liquidity risk
and systemic failures
Litterman, Robert
Lleo, Sébastien
Local-valuation method
Location, in distribution measurements
Logical probability
Lombard Street (Bagehot)
London Interbank Offered Rate (LIBOR)
Loss-distribution measurements for operational loss
Losses, anticipating. See also P&L (profit and loss)
Loss event categories
Lowenstein, Roger
LTCM (Long-Term Capital Management) fund collapse (1998)
Luck
Luck (Rescher)

Mackay, Charles
Macroeconomic financial crises. See Systemic risk
Managers
collaborations tandem with risk professionals
contribution to trading loss events
incentivizing
overconfidence
responding to shareholders/owners
risk understanding, importance
training to use measurement tool
Manhattan Investment Fund trading loss (2000)
Manias, Panics, and Crashes: A History of Financial Crises (Kindleberger)
Marginal contribution to risk
calculating
definitions and terms used for
reporting
subportfolios, partitioning approach
volatility estimates
multiple-asset best hedge position
simple portfolio
single-asset best hedge position
single-asset zero position
Margin calls
Mark, Robert
Market/cash distribution
Market risk
credit risk vs.
defined
estimating
historical approach
Monte Carlo approach
parametric approach
limits associated with, implementing
modeling
operational risk vs.
and P&L
reporting
sample portfolio
subportfolios
risk categories
risk factor distribution estimates
and risk-weighted assets
terminology for
Market-to-market payments
Markowitz framework
Marrison, Charles (Chris)
McNeil, Alexander
Mean-variance Markowitz framework
Merrill Lynch trading loss (1987)
Merton, Robert C.
Meta distributions
Metallgesellschaft trading loss (1993)
MF Global Holdings trading loss (2008)
Migration modeling, for credit risk estimates
Mikosch, Thomas
Miller, Bill. See also Legg Mason Value Trust Fund
Mirror portfolios. See Replicating portfolios
Mixture of distributions
for credit risk modeling
two-position example
Mixture of normals assumption
MKMV (Moody's KMV) credit risk model
CreditMetrics model vs.
data sources
default probability function
factor structure and dependence across firms
implementing
unobservable assets
Mlodinow, Leonard
Monte Carlo approach to risk estimation
asset-to-risk-factor mapping
copula/multivariate approach
marginal contribution to risk calculations
overview
P&L distribution
parametric and historical approaches vs.
volatility and VaR calculations
Monty Hall problem
Morgan Grenfell trading loss (1997)
Mortgage bonds. See also Tail (extreme) events
and credit risk modeling
liquidation costs
repo market for
subprime
Multiple asset portfolios. See also Covariance
analytic challenges
analyzing tail events, parametric assumptions
calculating marginal contribution to risk
mixture of normals approach
replicating portfolios for
Multiple-issuer credit risk
Multivariate distributions. See also Copulas

National Australia Bank trading loss (2004)
Natural frequencies
NatWest Markets trading loss (1994)
Negative binomial distribution
Newton, Isaac
New York Times
Nocera, Joe
Non-normal multivariate distributions
Nonsymmetrical distribution
Normal distribution
analyzing tail events
calculating risk factor distributions
determinants
and marginal contribution to volatility
overview
P&L distribution estimates
predicting tail events
Normal mixture distributions
Normal-normal distribution
Normal trading, and trading loss events
Norway, systemic banking crisis (1987-1993)

Objective probability. See Frequency-type probability
One-off events, probability of. See Belief-type probability
Operational risk
capital charges and
loss events vs.
managing and mitigating
market risk/credit risk vs.
measuring and modeling losses
overview
sources and types
Operations/middle office (risk management group)
Opportunity, capitalizing on
Options, embedded
Orange County, CA trading loss (1994)
"Order of Magnitude Physics" (Sanhoy, Phinney, and Goldreich)
OTC (over-the-counter) transactions
Other mapping/binning
Overconfidence, problem of

P&L (profit and loss) distribution
ambiguity of
and asset liquidity risk
asset/risk factor mapping
as basis for financial risk management
constructivist vs. market approach
and costs of liquidation
and credit risk
day-by-day P&L
estimating, general approach
location and scale (dispersion)
and operational risk
and risk factor distribution
sources of variability
and static credit risk modeling
time scaling
volatility and VaR and
when comparing securities or assets
Paradoxes, and ambiguity
Parametic approach/parametric distribution
asset-to-risk-factor mapping
historical and Monte Carlo approaches vs.
modeling
overview
P&L distribution estimates
risk factor distribution
second derivatives
tail events
volatility and VaR
Partitioning
Past/future asymmetry
Pentagon Papers
Physical measure/actuarial approach to risk pricing
Poisson distribution
Poisson mixture models
Poisson random variable
Popper, Karl
Portfolio allocation. See also P&L (profit and loss) distribution; Risk management; Risk measurement
diversification and
manager responsibilities
Markowitz framework and
and P&L
Portfolio analysis. See also specific risk measurement approaches
all-or-nothing contribution to risk
asset liquidity risk estimates
best hedge position calculations
comparing positions, summary measures
contribution to risk calculations
and correlation
day-by-day P&L
liquidation costs, simple CAC portfolio
multi-asset replicating portfolio
principal components analysis
risk reduction potential calculation
simple replicating portfolios
understanding and communicating risk
using copula and Monte Carlo approach
using parametric approaches
volatility and VaR measures
zero position contribution to risk
Price, Richard
Principal-agent problems
Principal components analysis
application to P&L estimates
basic concepts and approach
risk aggregation using
user-chosen factors
using
Probability
assumptions and
Bayes' rule (Theorem)
belief-type probability
binomial distribution
combining belief-type and frequency-type probability
of default, modeling
defined
frequency-type probability
gamma random variable
joint, in credit risk modeling
negative binomial distribution
nonintuitive approaches to
and past/future asymmetry
Poisson distribution
probability theory, history
probability paradoxes
and randomness
and runs, streaks
uses for
Process/business line operational risks
Procter & Gamble trading loss (2007)
Profits. See P&L (profit and loss) distribution
Proxy mapping

Quantile distributions
Quantitative risk measurement. See Risk measurement

Ramsey, Frank Plumpton
Randomness. See Uncertainty and randomness
Random walks
Reckoning with Risk: Learning to Live with Uncertainty (Gigerenzer)
Regulation
Reinhart, Carmen M.
Replicating portfolios
multi-asset portfolios
reporting
stepwise procedure for
using
volatility estimates
Rescher, Nicholas
Reserves
Risk
ambiguity/uncertainty vs.
defined
idiosyncratic vs. systemic
importance of managers' understanding of
luck vs.
multifaceted nature of
sources of
types of, overview
upside vs. downside
Risk advisory director
Risk aggregation
Risk assessment (operational risk)
Risk aversion
Risk communication/reporting
best hedges and replicating portfolios
bottom-up vs. top-down approach
consistency in
daily data reporting
data inputs
importance
IT systems
marginal contribution to risk
risk management group
for sample portfolio, summary
for subportfolios
Risk events. See Tail (extreme) events
Risk factor distributions, estimating
Risk management. See also P&L (profit and loss) distribution; Portfolio analysis; Risk communication/reporting; Risk measurement
as core competence
credit risk management
fraud-preventing policies and systems
goals and importance
heuristics, cautions about
infrastructure/programming needs
judgment and expertise needed for
liquidity risk assessments
managing people
managing processes and procedures
and operational risk
organizational structure/culture and
parties responsible for
processes and procedures for
probabilistic intuition
risk measurement vs.
understanding day-by-day P&L
understanding tail events and systemic failures
using risk professionals
Risk measurement. See also P& L (profit and loss) distribution; Risk management and specific measurement tools and approaches
best hedge position calculations
frequency-and belief type probabilities
comparing securities and assets
consistent measurements, tools for
contribution to risk calculations
credit risk vs. market risk
data needs and sources
distributions/density
expected shortfall calculations
funding liquidity risk
identifying sources and direction of risk
interest rate swaps (IRS)
importance and goals
independence of within corporate structure
information technology infrastructure needs
language of quantification
limitations
market approach
measuring extreme (tail) events
portfolio management tools
principal components analysis
risk management vs.
standard vs. extreme conditions
summary measures
uniform foundation for
using approximation, simple answers
RiskMetrics
Risk-neutral risk pricing
Risk pricing
Risk reduction potential calculation
Risk unit (risk management group)
Risk-weighted assets
Rogoff, Kenneth S.
Rogue trading
Rosenhouse, Jason
Rubin, Howard A.
Runs, streaks
Rusnak, John
Russia, systemic crises in

S&P500, average daily volatility and return
Sadia trading loss (2008)
Sampling distribution
Santayana, George
Savage, Leonard J.
Vos Savant, Marilyn
Scale (dispersion)
Schoutens, Wim
Second derivatives
Segars, Johan
Selvin, Steve
Senior managers
Sensitivity, measuring
Settlement risk
Shareholders
Share value, bonds
Shaw, W. T.
Shortfall, expected. See VaR (Value at Risk)
Showa Shell Sekiyu trading loss (1993)
Siegel, Larry B.
σ (sigma). See Volatility 
Single assets
analyzing tail events
calculating marginal contribution to risk
Single-firm (marginal) migration matrixes
Single-issuer credit risk
Skewness
Social proof (herding instinct)
Société Générale trading loss (2008)
South Sea Bubble
Spain, systemic banking crisis (1977-1985)
Speculation, failures associated with
Standard error
Standardizing positions, summary measures for
State of West Virginia trading loss (1987)
Static structural risk models
Statistical approaches and randomness, uncertainty. See also Distributions; P&L (profit and loss) distribution; Risk measurement; VaR (value at risk); Volatility and specific statistical approaches
Statistical or empirical factor mapping
Staying Alive in Avalanche Terrain (Tremper)
Stochastic dominance
Stock prices
application of frequency probability to
risks associated with
Strategic operational risks
Structural credit risk models
Student-normal distribution
Student-Student distribution
Student t distribution
alternate student distribution
Stylized credit risk model
Stylized financial time series
Subadditivity
Subjective probability. See Belief-type probability
Subportfolio analysis
Sumitomo Corporation trading loss (1996)
Summary measures
for aggregating risk
distribution/density functions
limits of
for standardizing and comparing positions
for tail events
for volatility and VaR
Summary risk report
Supervision, lax
Swap rates and spreads
Sweden, systemic banking crisis (1991-1994)
Symmetric distributions
Systemic risk
costs
idiosyncratic risk vs.
and managing liquidity crises
systemic financial events

Tail (extreme) events
analytic tools and techniques
copulas
distribution, order statistics
extreme value theory
idiosyncratic
and limits of quantitative approach
measuring
1974-2008, summary
parametric analysis for single asset
Student t distribution
Two-point mixture of normals distribution
understanding, importance
use of VaR for
variability among
Taleb, Nassim
Temporal factors
Thirlwell, John
Threshold models. See Structural credit risk models
Time scaling
Time-series econometrics
Titanic disaster example
Traders, compensation approaches
"trader's put," 70-71
Trading in excess of limits
Trading loss events-2008
categories of loss
failure to segregate and lax supervision
from fraudulent practices
from legitimate business practices
lessons learned from
loss accumulation periods
main causes
from non-fraudulent or tangentially fraudulent practices
size and description of loss
summary table
A Treatise on Probability (Keynes)
Tremper, Bruce
Triangle addition for volatility
Tversky, Amos
Two-point mixture of normals distribution

Uncertainty/randomness
ambiguity aversion, need for control
and human intuition
and past/future asymmetry
and people management
and risk management
risk vs.
runs, streaks
sources, overview
and volatility, VaR
Union Bank of Switzerland (UBS) trading loss (1998)
Unique risk ranking
United States
S&L crisis (1984-1991)
Treasury rates
U.S. Treasury bond
calculating DV01/bpv
distribution and tail behavior
marginal contribution calculations
P&L distribution example
time scaling example
volatility

Valuation model for asset mapping
Value Trust Fund winning streak
VaR (Value at Risk)
for aggregating risk
all-or-nothing contribution calculations
calculating
conditional VaR/expected shortfall
contribution to risk calculations
credit risk modeling using
defined
interpreting, cautions
for liquidation cost estimates
probability expressions
relation to volatility
reporting risk estimates using
for single bond position
small-sample distribution
for standardizing and comparing positions
and subadditivity
and symmetric vs. asymmetric distribution
for tail events
with two-point mixture of normal distribution
using effectively
variability in over time
Variance
Variance-covariance distribution. See also Parametric approach to risk estimation
Variance-covariance matrix 
Vasicek, Oldrich
Venn, John
Volatility (; standard deviation)
aggregating/summarizing risk using
best hedge positions/replicating portfolios
contribution to risk calculations
estimating
exponential weighting
interpreting, cautions
liquidity risk estimates
low vs. high dispersion
marginal contribution calculations
market/cash distribution
relation to VaR
reporting risk estimates using
for single bond position
for tail events
triangle addition for
using effectively
variability of over time
variance-covariance distribution estimates
volatility estimates for simple portfolio
Volatility (standard deviation), measurement uncertainties
Volatility point
Voltaire
Von Mises, Richard

Weather predictions
Weatherstone, Dennis
WestLB trading loss (2007)
Worst-case situations

Yates, Mary

Z% VaR




