












STATISTICALMETHODS
in Online A/B Testing
Statistics for data-driven business decisionsand risk management in e-commerce
Georgi Z. Georgiev







"If you're running online experiments, you absolutely need this book. It will help you avoid costly testing mistakes stemming from misuse and misunderstanding of statistics."
- Peep Laja, Founder and Principal, CXL Institute
"This is simultaneously the most comprehensive and the most accessible book on A/B testing statistics I've encountered. Most companies are running A/B tests now, but the level of statistical malpractice is surprisingly high. If you run tests, then, this book should be required reading. Read it, and then gift it to the rest of your team to read, and watch your program's maturity rise in front of your eyes."
- Alex Birkett, Senior Growth Marketing Manager, HubSpot
"Statistical Methods in Online A/B Testing transforms the often-complex area of statistical modeling into an understandable and useful framework for those will little or no math background. Starting from the ground up, Georgiev introduces the reader to important statistical concepts with the goal of making the science behind A/B testing interpretable and useful. CRO professionals who lack the statistical background necessary to properly setup, run and interpret A/B tests will find this book a must have tool in augmenting their base of knowledge."
- Barry Garner, Sales Development Manager, SiteGainer
"Finally, a comprehensive reference on statistics for my own industry! No longer do I need 8 different books on statistics that were each bought for only one or two chapters each, because Georgi has delivered a text whereby each and every chapter is applicable to success in my daily work. Thank you for making such a complex subject both interesting and easier to understand."
- Philip Cross, CEO & Conversion Rate Optimizer, Premeni Services Company
„One of the few books that would actually make you a better marketer and bring you more sales. Period. No fluff, straight to the point, practical manual you need on your desk. Your business runs on data. Admit it. Pick up a copy of this book and finally understand how to properly evaluate, analyze and use your data when running A/B tests. All masterfully delivered by a true practitioner and an excellent teacher of the somewhat complicated world of statistics."
- Hristian Kambourov, Chief Conversion Craftsman, Revise







STATISTICAL METHODS IN ONLINE A/B TESTING
This book is dedicated to explaining the tools of statistical inference and estimation through online controlled experiments; a.k.a. A/B tests. It views them in a risk-management context of balancing the risks and rewards of innovation. With the help of this text, user experience and conversion rate optimization practitioners will be able to harness the power of data-driven decision-making, and enable their business to innovate, while controlling the risk to which it is exposed.
An issue with much of the current statistical theory and practice in online A/B testing is that it is misguided and frequently misinterpreted. It is often applied without a good understanding of the role and limitations of statistical methods, instead blindly copying scientific applications without due consideration of many of the unique features of online business. This book approaches this problem by laying solid statistical foundations, providing clear definitions, and examining practical scenarios, while constantly keeping an eye on the overarching business goal of A/B testing. By making constant use of the business context, and ample practical examples, this text presents A/B testing statistics in a uniquely useful way.
Georgi Z. Georgiev is the Managing Director of Web Focus LLC, and a veteran web marketer and web developer. His diverse 15-year experience includes owning, developing, and managing dozens of successful online projects, working as an SEO, Google AdWords, Google Analytics, and statistics consultant, as well as delivering training and lectures on multiple seminars and events, including in his capacity as a Google Regional Trainer. He is also developer of statistical tools at Analytics-Toolkit.com, as well as the author of numerous articles and white papers on the topic of statistics in online A/B testing. His most notable works have been "Efficient A/B Testing in Conversion Rate Optimization: The AGILE Statistical Method", and the first extensive glossary of statistical terms in online A/B testing. His vast experience with online business and statistics positions him uniquely to deliver an accessible book on the topic of statistics applied to online A/B tests.








Statistical Methods in Online A/B Testing: Statistics for data-driven business decisions and risk management in e-commerce
Georgi Z. Georgiev
Sofia, Bulgaria
Copyright © 2019 by Georgi Z. Georgiev
This publication is in copyright. All rights are reserved by the Author, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction in any physical way, and transmission of information storage and retrieval, electronic adaption, computer software, or by similar or dissimilar methodology now known or hereafter developed.
Trademarked names, logos, and images that may by appearing in this book are used in an editorial fashion.
While the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.
Cover & interior design: Red Raven Book Design
Editor: Christopher Morris
Graphics: Georgi Georgiev
First published 2019
For information on translations, please e-mail georgi@webfocus.bg
Visit the book's website at www.abtestingstats.com








TABLE OF CONTENTS
MOTIVATION
WHO IS THE READER OF THIS BOOK?
ACKNOWLEDGMENTS
NOTATIONS
1. USING STATISTICS IN BUSINESS
1.1. When are statistics useful?
1.2. Statistical inference in business
1.3. Primary uses of statistics in online A/B testing
1.4. The necessity for counterfactual reasoning
1.5. Establishing causality
1.6. Ruling out alternative explanations for the data
1.7. Statistical methods and efficient use of data
1.8. Caveats in using statistical methods in business
2. ESTIMATING UNCERTAINTY. STATISTICAL SIGNIFICANCE, P-VALUES, OTHER ESTIMATES.
2.1. Substantive hypotheses
2.2. Statistical hypotheses
2.3. Standard deviation and Z-Scores
2.4. p-value and type I errors
2.5. p-value: utility and interpretation
2.6. Confidence intervals
2.7. Misinterpretations of p-values and confidence intervals
2.8. p-values and confidence intervals in decision-making
2.9. Maximum likelihood estimate
2.10. Severity
3. STATISTICAL ASSUMPTIONS. ASSESSING MODEL ADEQUACY
3.1. The importance of statistical assumptions
3.2. Probabilistic assumptions of statistical models
3.3. Probabilistic assumptions in different practical cases
3.4. Assumptions imposed by the design of the experiment
3.5. Assessing statistical adequacy through statistical tests
3.6. Assessing statistical adequacy through A/A tests
4. STATISTICAL POWER AND SAMPLE SIZE CALCULATIONS
4.1. Errors of the second kind (type II errors)
4.2. Statistical power
4.3. The role of statistical power in A/B testing
4.4. Minimum effect of interest vs. minimum detectable effect
4.5. Underpowered and overpowered tests
4.6. Sample size calculations
4.7. False positive and false negative rates
4.8. Misunderstandings about statistical power
5. TYPES OF STATISTICAL HYPOTHESES
5.1. One-sided tests and confidence intervals
5.2. Misconceptions about one-sided tests
5.3. Strong superiority tests
5.4. Non-inferiority tests
5.5. p-value notation
6. TESTS WITH MORE THAN ONE VARIANT
6.1. Type I error in an A/B/n test
6.2. p-value and CI corrections for testing multiple variants
6.3. Sample size calculations for A/B/n tests
6.4. Dynamically dropping or adding variants
6.5. Do factorial designs make sense in online A/B testing?
6.6. Testing the perfect shade of blue
7. SEGMENTATION AND MULTIPLE PERFORMANCE INDICATORS
7.1. The Šidák correction
7.2. Analyses of segments of the sample of an online experiment
7.3. Designs with more than one primary parameter of interest
7.4. Designs with secondary parameters of interest
8. WORKING WITH CONTINUOUS DATA
8.1. Standard deviation, p-values, and confidence intervals
8.2. Statistical power and sample size calculations
8.3. Is the Normal distribution assumption adequate?
8.4. A workaround for incomplete ARPU data
9. PERCENTAGE CHANGE
9.1. Percentage change (lift) vs. absolute change
9.2. Confidence intervals for percentage change
9.3. p-values for percentage change
9.4. Sample size calculations for percentage change
10. SEQUENTIAL TESTING: CONTINUOUS MONITORING OF DATA
10.1. The issue of repeated significance tests on accumulating data
10.2. The sequential probability ratio test
10.3. Fixed analysis time group sequential trials
10.4. Alpha-spending functions and efficacy boundaries
10.5. Beta-spending and futility boundaries
10.6. Expected sample size and efficiency of sequential A/B tests
10.7. Estimation following a group sequential A/B test
11. OPTIMAL SIGNIFICANCE THRESHOLDS AND SAMPLE SIZES
11.1. Defining "success" for a business experiment
11.2. Costs, benefits, risks, and rewards in A/B testing
11.3. Test parameters and their relationship to costs and benefits
11.4. Distribution of expected effect sizes
11.5. Calculating risk/reward ratios and key points
11.6. Testing with 50% confidence threshold?
11.7. Inherent cost of A/B testing
11.8. Limitations of Risk/Reward calculations
12. EXTERNAL VALIDITY a.k.a. GENERALIZABILITY OF A/B TEST RESULTS
12.1. What is external validity (generalizability)?
12.2. Threats to the external validity of online experiments
12.3. Improving the generalizability of A/B test results
12.4. Representative samples and sequential tests
12.5. Running multiple concurrent A/B tests
13. MISCELLANEOUS TOPICS
13.1. Equal or unequal allocation between test groups?
13.2. Holdout groups
13.3. Time to event analysis. Hazard ratio
13.4. Meta-analyses of A/B test results
13.5. Adaptive Designs
13.6. A word on multi-armed bandits
13.7. Bayesian methods
14. COMMUNICATING STATISTICAL RESULTS
14.1. Changing the perception of data variability
14.2. Translating business questions into statistical models
14.3. Presenting statistical results
EPILOGUE
REFERENCES
INDEX







MOTIVATION
The most straightforward way to explain why this book exists is via a brief description of my journey from a statistical know-nothing to an author of a book on statistics.
Some years back, I set out to learn more about the application of state-of-the-art scientific methods to the business world of data-driven decision-making. Starting with analyses of observational data, I quickly shifted my focus to online controlled experiments. These are commonly referred to as A/B tests, or split tests.
At the time, I had no formal training in statistics, and only college-level understanding of mathematics, so, to be honest, I didn't even know where to start! Available books on A/B testing barely had anything to say about statistics, so I started reading online blog posts and educational resources from universities, such as online lectures and courses, as well as the odd scientific paper.
In doing so, I had to face an entirely new jargon full of counterintuitive terms, such as 'statistical significance', which has little to do with significance, 'statistical power' which has nothing to do with power in the casual sense, 'confidence interval', which has nothing to do with any kind of confidence, and so on. And, above all else, I had to familiarize myself with a notation full of small and capital Greek letters (α, β, γ, δ, θ, µ etc.), which would sometimes mean different things in different contexts, while different letters would also denote the same concept.
To make things worse, there were ample examples of vague or conflicting information. There were dozens of definitions for what a p-value is and how it should be interpreted. Almost nobody seemed to care to define what a family of hypotheses is supposed to be when discussing the Family-Wise Error Rate. One source would claim one-tailed tests are preferable, while others would swear by two-tailed tests, and scare you with the heavens coming down on you if you were so reckless as to consider a one-tailed test.
Practitioners and academics alike were battling over which approach is best overall, or squabbling over the merits of particular applications - frequentist inference vs. decision-theoretic vs. Bayesian approaches. To make matters even more confusing, there seemed to be noticeable schisms within each school of thought.
Most confusing of all, statistics as such turned out to be very context-dependent - it meant different things in different scientific and business fields. Practitioners in those fields had, over time, developed somewhat separate branches of statistics. Therefore, statistics would mean something different for you depending on whether you come from physics, medicine, social studies, econometrics, environmental studies, or industrial quality control.
It was simply a nightmare attempting to navigate this fractured jungle of jargon, conflicting stances, and math-heavy explanations. Yet, I persevered! And through painstaking reading, practice, implementing/coding methods, and countless simulation runs, I was able to garner a good enough understanding of the matter to begin writing methodological white papers and in-depth articles, to start delivering lectures and courses on statistics in A/B testing, and to become a developer of statistical tools.
From my current position, I see both the immense value of statistical methods applied to business risk management, estimation and prediction problems, and the immense harm done by improper applications or misguided understanding of those same methods. Thus, in-depth explanations of the practical application of statistical methods, as well as common errors and how to avoid them, are key elements of this work.
Furthermore, in 2019 the difficulties that I went through are about as severe as they were a few years before, despite the valiant efforts of some in the statistics and A/B testing communities. Addressing common mistakes, misconceptions, and misapplications of statistical methods is, therefore, a central part of this work.
My aim with this book is to carve a clear path through the statistical jungle, and thus save the reader weeks, months or even years of wandering around in circles, falling into gorges, and crossing rivers, metaphorically speaking! While the book does use the established jargon, each term is explained with painstaking detail and accuracy using the simplest language possible. Math and formulas are kept to a sanitary minimum in order to facilitate reading, while also satisfying the needs of technically-inclined readers, who will also find the detailed references supporting each chapter to be particularly useful. Since this is a book on statistical methods, and not on decision theory per se, the text also sticks to the frequentist error-statistical approach, and only briefly touches on current decision-theoretic and Bayesian methods.







WHO IS THE READER OF THIS BOOK?
This book aims to introduce the complex topic of statistical estimation and inference to readers with somewhere between little and no mathematical and statistical background. The text makes few assumptions, and builds each topic from the ground up, explaining the rationale behind each concept, and following it with a multitude of practical examples from the world of online A/B testing. It contains detailed explanations, so that one can understand the statistical methods deeply enough in order to correctly put them into practice, but steers clear of some of the difficult parts of set theory, calculus, etc., which are typical of many other books on statistics.
A background in conversion rate optimization, operating an online business or mobile app, design of user experiences, or similar, would be helpful for an easier reading, as the primary audience for this work is conversion rate optimization professionals who design, execute, and analyze A/B tests in an online environment. However, due to the similarities with other fields where controlled experimentation is possible and valuable, the book can be a useful guide to A/B testing in areas other than website and mobile application development. Product managers and growth experts should benefit from it regardless of the particular product or service they are focused on.
The overall framing of the presentation is always mindful of the topic of business objectives achieved through statistical methods. This will be useful for those readers who have some experience of statistics in other disciplines, and who are now looking to understand the use of statistical tools in facilitating decision-making through online A/B testing.
However, I must emphasize that the unique research presented in this book, in terms of ideas, models, and simulation results, will be valuable to all readers, regardless of background.







ACKNOWLEDGMENTS
I would like to thank my parents, Maya Maneva and Zdravko Georgiev, for encouraging, each in their unique way, my exploratory nature, curiosity, and longing for the truth, without which I would not have ventured into the field of experimentation.
Key ideas in the book were influenced significantly by the work of Deborah G. Mayo, Aris Spanos, and Daniel Lakens. I'm also particularly grateful to Deborah G. Mayo for providing her constructive feedback on the parts of the book dealing with Severity.
I would like to thank industry professionals Chad Sanderson, Ronny Kohavi, Barry Garner, and Daniel Gonzalez for providing stimulating discussions and exchanges, out of which arose valuable ideas for some of the topics in the book. I'm also thankful to all users of my statistical software, and all of my consulting clients, who helped me indirectly both in financing this project, as well as by posing interesting practical questions.
I'm indebted to the reviewers of early drafts of this book - Philip Cross, Hristian Kambourov, and Antoaneta Getova for their feedback. And I'm thankful to my sister, Diana Georgieva, for her comments, and for making these early drafts more readable. I would also like to thank my editor, Christopher Morris, for his excellent work on improving the book's readability.
My deepest gratitude is reserved for my partner in life, Vesela G. Gavrailova, for her continual encouragement and indirect support of all my efforts, leading to and resulting in this book becoming a reality.







NOTATIONS
α (Greek lower-case letter alpha) -type I error rate / type I error probability of a procedure
β (Greek lower-case letter beta) -type II error rate / type II error probability of a procedure
θ (Greek lower-case letter theta) - an unknown random variable, a parameter to be estimated, e.g. the absolute difference between two conversion rates
µ (Greek lower-case letter mu) - arithmetic mean of a population
x̄ (x-bar) -sample mean
δ (Greek lower-case letter delta) - an observed difference between two parameters
σ (Greek lower-case letter sigma) - standard deviation. Can refer to the known sample standard deviation or to the population standard deviation, usually unknown and estimated from a sample.
Δ (Greek upper-case letter delta) - the difference between two parameters
Θ (Greek upper-case letter theta) - the parameter space, all possible values of a parameter
ℳ - statistical model
P - probability, usually P(...)
p-value - the probability of the testing procedure producing an outcome as extreme or more extreme than the one observed, under a specified the null hypothesis
H0 - null hypothesis, default claim or statement to would fall back on if no data is available
H1 - alternative hypothesis







Chapter 1
USING STATISTICS IN BUSINESS
Usage of statistics is now widespread in the business world. However, understanding the exact role and limitations of applied statistical methods appears to be lacking, resulting in poor decisions, business loses, wasted resources, and missed opportunities.
In this book, I will be addressing this by exploring when reasoning based on statistics is admissible and helpful, and what limitations data-driven decision-making faces. This will help eliminate poor and misleading usage of statistics in business decision-making. I will then lay the foundation for proper usage of statistical methods in business, and then introduce various approaches for dealing with the many practical scenarios encountered in online A/B testing.
To do that, we will first take a step back and examine the foundational needs addressed by statistics, as well as the prerequisites for employing statistical methods.
1.1 When are statistics useful?
On a philosophical level, statistical inference provides us with the important ability of 'speaking up' - be it to a senior colleague, an expert, or a consultant. Using statistics enables us to hold them accountable, and to defend our knowledge and convictions to the extent to which they are supported by data. It is a scientifically rigorous way to reason with integrity about any question or issue (Mayo 2018, 11).
Unless you are already at the top of the business hierarchy, or a well-renowned expert, you may encounter problems convincing others of your way of addressing a business issue, even though you might be fully convinced that your approach is superior. Without statistics, it is just you and your opinion, hunch, or informed hypothesis, with few compelling reasons for anyone to even consider it. However, armed with good statistics you can argue successfully, even if facing the world's biggest expert on a certain subject, or the CEO of a billion-dollar company.
That said, statistics is no magic bullet, and 'letting the facts speak' is not as easy as many would like it to be, as you will realize by the end of this book, if not this chapter.
Despite the distinct edge one acquires by aligning themselves with reality, statistical methods and experimentation may not be the preferred tool for every situation. Several prerequisites are key for applying statistical methods in an effective manner.
Firstly, there has to be room for argument and appreciation of facts. There should be clear business goals, and commitment to achieving them, not by brute force, but by careful study of the environment. Furthermore, there has to be understanding that even the best decision-making process will fail miserably if it is fed inaccurate information, or if the uncertainty in the data is underestimated. The smaller role that emotions, favoritism, and personal attachment to ideas, claims, or proposed actions play in decision-making, the greater the benefit of using statistics.
Bringing hard facts to the table when the decision has already been made based on the considerations that we have just mentioned may actually be harmful in the short-term. This is especially true if a decision maker has not pre-committed to a data-driven approach, and is emotionally or otherwise invested in a particular outcome. In such a case, no amount of data will be sufficient to change their mind(s), so bringing data to bear on the decision may be viewed as an obstacle, or even an attack, rather than an honest effort to inform interested parties of a business decision. Therefore, securing a commitment from stakeholders for accepting the data, whatever it may reveal, is a prerequisite for the ultimate success of any A/B test.
Secondly, there must be a sound overall approach to measurement. This includes smart choice of metrics and indicators, as well as a good understanding of what decisions these can and cannot inform. Adding statistics to poor or irrelevant measures will hardly make for better decisions.
For example, conversion rate is often a poor measurement for business revenue, and an even worse one for profits. Making a decision based off a perceived increase in conversion rate, while thinking it is equivalent to an increase in revenue or, worse, profits, is an almost sure-fire way to make a bad call. Obviously, revenue per conversion needs to be considered to estimate the effect on revenue, and the profit margin needs to be considered in order to estimate the effect on the bottom line.
A third prerequisite for successful use of statistical methods is the presence of data prone to measurement error and/or phenomena which exhibits cross-subject heterogeneity and/or chance fluctuations over time. If the data is always 100% accurate, and if every 20th visitor to a website purchases exactly one item worth $20, then there is no need for statistical methods. Even the slightest deviation from this pattern can easily be attributed to whatever change was introduced. However, if sometimes there are orders or leads which do not get tracked, or if the conversion rate is 5% on average but varies from hour to hour and day to day, or if the purchase amount is $20 on average but varies from order to order, then statistical methods are of great assistance in making sense of such noisy data.
A clarification is needed for the third point - there has to be chance regularity to the errors and deviations in the data; in other words, they cannot be 'random' in the colloquial sense of the word. If that were the case, statistics would be useless. Luckily, errors and deviations are not at all random in this sense, therefore statistics is quite useful in estimating them.
A culture of data-driven decision-making, sound measurements, and data exhibiting chance regularity are all necessary to make statistical methods relevant.
To sum up, a great environment for employing statistics consists of the presence of a culture of decision-making based on data, a sound overall approach to measurement, and data exhibiting chance regularity. Given that the last condition is almost always true, it is the first two which can be missing, or lacking and in need of improvement.
A company which has already integrated data-driven decision-making and solid measurement processes into their core business is usually said to be in possession of a measurement culture, or a culture of experimentation. Any positive change we can make that drives the enterprise we are part of closer to this ideal will increase the utility of all statistical inferences.
In such circumstances, statistical inference can be an incredibly powerful tool that allows the uncovering of and establishing of facts, and, therefore, the speaking of truth to any kind of authority. It is also as close to foreseeing the future as one can get.
One of the earliest examples of applying statistics to a seemingly intractable prediction problem was demonstrated in Scotland way back in mid-1500s, when the Church of Scotland was reformed to allow its ministers to marry. This made the church responsible for the fate of ministers' families, who could be left destitute in the unfortunate event of the death of the head of the house.
By the mid-1700s, this had become a seemingly unsolvable 200-year-old problem, since one-time payments were insufficient to provide for the families of deceased ministers, while overtime payments were too tricky to predict in order for adequate financing to be ensured upfront.
At this point, Robert Wallace and Alexander Webster, who shared a love for math were commissioned to solve the problem of the "Scottish widows". Their idea was to work out a way for a perpetual insurance fund to be established which would never go under. The hard thing was to figure out what the premium needed to be, in order to allow sufficient ongoing payouts for families of the deceased. And this was obviously made more difficult by their not knowing in advance how many ministers would die, if they would leave a widow upon their death, and how many children under 16 they would have at the time of their demise.
However, in 1744 Wallace and Webster were already aware of the recently proven law of large numbers. What the Swiss mathematician Jakob Bernoulli established in 1713 meant that while one cannot predict the death of a particular minister and their circumstances upon it happening, one can very reliably predict the expected death rate and related information for a relative large population of such ministers over a large span of time. At the time, the ministers numbered just over 900 at any given time, and given the timeframe of the fund, the law of large numbers could be adequately applied.
Using publicly available data, the duo drew up the first so-called 'life tables', and predicted that in the year 1765 the fund would have at its disposal 58,348 pounds. This prediction was made in 1744, about 20 years prior. When the balance of the Scottish Ministers' Widows' Fund they helped establish was tallied in 1765, it was 58,347, just 1 pound short of the prediction. This is how good their data and statistical methods were. The fund operates to this day, now open to anyone who wants to sign up, and manages upwards of 100 billion pounds. (sources: (Russell 2016), (Harari 2011))
Note how all three prerequisites were in place for the Wallace and Webster endeavor to succeed. Firstly, there was a clear need and incentive to use data to establish the insurance premiums for the fund. Secondly, there was fairly reliable historical data on the number of ministers employed, deaths, and the families that they left behind. And, thirdly, the data was subject to variation, and could be modeled as a random variable.
Statistical methods, data collection, and data processing equipment have come a long way since then, with the latest advances in so-called Artificial Intelligence (AI) now being part of this evolution. The discipline had a significant boost in the early 20th century when it began to be applied to manufacturing, agriculture experiments, and other areas of science and business. At this time, its theory was developed by great minds such as Karl Pearson, Ronald Fisher, William Gosset (Student), Jersey Neyman, Egon Pearson, and others.
A further wave of improvements came during the Second World War when statistical quality control in manufacturing, as well as testing of various weapons systems and ammunition, played a significant role in the economic side of this great conflict. In fact, some statistical methods developed at that time were initially classified; e.g. Abraham Wald's sequential probability ratio test (SPRT) was only declassified in May 1945 (Wald 1947). The invention of the electronic computer significantly accelerated all kinds of complex and laborious statistical calculations, and made the then commonly used tables, with pre-calculated values of distribution functions (quantile functions) becoming almost obsolete while theoretical work continued and branched out further.
Nowadays, statistical models are more widespread, and more trusted than ever, both online and offline. A/B testing may be a small niche in the broader application of statistics, but online experiments serve as a final check against misuse and misinterpretation of data. This is a great power, and it comes with great responsibility. Wielding statistical inference must be conducted with the utmost care, lest it causes irreparable harm to ourselves and any organization of which we are part.
1.2 Statistical inference in business
The prerequisites for proper and reliable use of statistics remain the same now as they were back in the 16th century. Once the conditions for employing statistical methods just described are fulfilled, one will benefit greatly from applying the methods of statistical inference to business problems. Statistical inference consists of the application of statistical methods to uncover facts about the process or mechanism generating certain noisy data.
For example, if the goal is to estimate the relative performance of one version of a checkout page over another, of one ad creative over another, of a certain call-to-action over another, it can be achieved by modeling the data generating process, and then applying statistical methods based on that model. The model incorporates information about the chance regularity of the process. Therefore, statistical inference makes it possible to learn something about the workings of the mechanism producing data despite the variability of its output, be it checkout conversion rate (CR), click-through-rate (CTR), etc., or the relative performance of one over the other - e.g. the relative difference in conversion rates between the proposed new checkout page, and the current one.
Statistical inference is the process of applying statistical methods to uncover the characteristics of a data-generating mechanism which can be used to support causal claims and predictions.
Engaging in online controlled experiments usually aims to produce causal claims and predictions about the future performance of a given intervention (treatment) based on a limited sample of users exposed to it. Such a claim is a statement about a population based on a sample. Making this in a rigorous way, and including a measurement of the uncertainty involved in the process, necessitates the use of statistical inference.
For example, statistical inference is making the claim that the 10% improvement in average revenue per user observed during an A/B test with a lower 99% confidence bound of 2% would most likely lead to an increase in revenue by 10% over the next year, and would almost certainly lead to at least a 2% increase in said revenue. In this case, the sample consists of the users exposed to the A/B test, while the population inferred about consists of all future users of the website.
Note that the above process involves a certain unstated assumption - that the mechanism behind the numbers will remain the same, or nearly the same, after the completion of the A/B test. This might not always be granted so easily, and is discussed in detail in Chapter 12 on external validity of statistical inferences.
Contrast the above to applying only basic descriptive statistics. If that were the case, one would only be able to say that the observed relative improvement during the A/B test was 10%. Saying anything about the reliability of this estimate, or making a prediction is not possible without engaging in statistical inference.
Some inferences, such as the one used by Wallace and Webster to operate their insurance fund, are based on observational data, and only provide the ability to make a prediction. Eliciting a causal link between life circumstances and the death rate of ministers would be hard to justify. On the other hand, inferences based on controlled experiments make it possible to make predictions and extract causal links between an intervention and observed effects on whatever particular variable is of interest.
The powerful ability of statistical methods to make use of noisy data for these purposes is why online controlled experiments are practiced in most, if not all, of the leading technology giants of the day, such as Google, Microsoft, and Amazon, along with online services such as Netflix, Booking.com, and others. Indeed, many of these firms reportedly run thousands of experiments on an annual basis, across their multitude of products and services (Gupta, et al. 2019).
With this in mind, let us examine the function of statistical methods in online A/B testing in more detail.
1.3 Primary uses of statistics in online A/B testing
Statistical methods in general serve a dual function - they allow estimation of the uncertainty inherent in any measurement or set of measurements (data), and make it possible to refute hypotheses (claims, statements) based on experimental data. Statistics such as p-values and confidence intervals can be interpreted as estimates of unknown parameters, as well as error probabilities in the context of hypothesis testing.
Imagine having just completed an experiment on an e-commerce website comparing the current checkout experience (A) with a new, hopefully improved one (B). The observed improvement in experience B versus A is 10%. A 99% confidence interval for the relative difference between A and B is constructed, and it covers the values from +2% to +∞.
An interpretation of this result from an estimation standpoint is that the true improvement is most likely 10%, given the data (10% is the maximum likelihood estimate). The interval bound would be interpreted as such - had the true improvement been lower than 2% we would have seen such an interval with probability 1% or less (100% - 99% = 1%). Values of under 2% improvement are thus poorly supported by the data at hand, given the stark coincidence required for such an observation to happen if the true value is under 2%.
This is an example of estimating a range of effects which can be ruled out for practical purposes. We usually care about ruling out negative effects and constructing a one-sided interval bounded from below, but sometimes we also want to know what positive effects are unrealistic, in which case we can construct a one-sided interval bounded from above. Bounding our predictions in this way limits the risk of having expectations for the future performance, which are overly optimistic given the data at hand.
From a hypothesis-testing standpoint one can consider that same example data and the following hypothesis: "our new checkout would perform worse than our current one", then see how it fares, given the information from the A/B test. It turns out that the hypothesis fares quite poorly, since observing the data at hand is highly improbable if it were true. Therefore, we can consider the hypothesis rejected, and have grounds to take action under the assumption that the new experience will improve our website's business performance.
In other words, the upper limit of the risk of making a poor decision has been estimated through statistical methods. Therefore, one can proceed in full awareness of the risk to which the business is exposed.
Different hypotheses such as "our new checkout B works at most 10% better than our existing one A" can also be considered. This hypothesis cannot be refuted with high probability since there is a 50% probability of observing the data at hand, even if B is in fact less than 10% better than A (more on these calculations in the next chapter). Note that even though a 10% lift is the most likely true value, a claim to the effect of B being more than 10% better (the opposite of the hypothesis we want to refute), would still be poorly supported by the statistics, as it has not passed a rigorous (or severe) test.
The above examples show that the same set of experimental data can be used to perform two functions - manage business risk in decision-making and estimate the uncertainty surrounding a measurement.
Statistical methods in online A/B testing can be used as a risk-management tool as well as for estimating the uncertainty surrounding different measurements.
Even at this early stage, it should be clear that the purpose of statistical tests is not to "identify winners" or to "figure out what works". It is to estimate the effects of different interventions with a satisfactory degree of precision. Whether the effect is substantial enough to "declare a winner" and implement a tested solution is a different question altogether. In other words, A/B testing allows businesses to innovate and improve their product or service in a sustainable manner by limiting the business risk of the changes necessary to stay on par with, or outpace, the competition.
An example of how the risk-management aspect of A/B testing works in the longer run is to consider the fate of a company with a set of 100 ideas for changes to its website. In this hypothetical scenario, we are going to assume the very unfavorable situation that all of the ideas would harm the business if they are implemented, for illustrative purposes.
In one hypothetical world the company simply implements the ideas one after another, monitoring their performance metrics as they go. They will experience worse and worse results with some variance, which will preclude them from telling which, if any, of the changes they are introducing is causing the decline. They may then roll back certain changes, after which particularly bad drops may be observed, but they would have a hard time connecting the effects to the actions which caused them. This difficulty would be caused by noisy data and the difficulty in differentiating effects caused by their actions and those caused by changes in the rest of the business environment.
In another hypothetical world, the same company uses p-values and confidence intervals to test each idea before it is implemented. Further, for some strange reason they apply the same 5% confidence threshold, and use 95% confidence intervals for estimation of the true effect across all tests.
Luckily, this makes the math quite easy, as it means that the testing team will likely reject about 95% of the changes (they will reject more than 90% of them with 99% probability) before they are even implemented. This limits their downside to these 5 to 10 ideas which will result in false positives. The process of testing the ideas will certainly slow them down somewhat, which in this situation is a good thing, but it will also reduce the maximum amount of risk the business is exposed to by over 90%.
Of course, the above example is an extremely pessimistic, worst-case scenario for the purpose of illustration.
Statistical quantification of uncertainty through p-values, confidence intervals, and others will be explored in detail once we discuss the counterfactual nature of statistical inference, and why it is a necessity in A/B testing. This will make it easier to understand the seemingly unnecessary hoops we must jump through, and the somewhat weird wording encountered in statistical inference and estimation. In fact, both of these are crucial if we are to understand the rationale behind using statistical tests as a tool for inference and estimation.
1.4 The necessity for counterfactual reasoning
If you have not dealt with statistical inference before I'm sure that you have noticed the unusual language in the example in 1.3. It would probably have been the 'ifs' which caused an uneasy feeling: "if it were true", "if [...] is in fact worse than...", as well as the "we would have seen" part.
This type of reasoning: "what would have happened if" is not the way people perceive probabilities in situations where one would use words such as 'chance', 'likely' and 'probably' in their everyday meaning. For example, when saying it is likely to rain tomorrow or, more specifically, that there is a 90% probability that it will rain tomorrow, there is no reference to any counterfactuals. Similarly, saying the chance to win the lottery is 1 in 292,201,338 (2019 US Powerball jackpot odds) there are no 'ifs' added to that statement. The odds are fixed, resulting in no need to add "if it were true"-type qualifiers to it. Dice rolling and games of chance of all sorts are other examples of situations with fixed odds.
However, reasoning about the real world of physics, biology, psychology, stock markets, and business is a different situation since the odds are unknown. What are the odds of you investing in the next Google? What are the odds of your next invention or business project being a success? What are the odds of the redesign of your website resulting in an increased revenue per user? We simply do not know.
The obligatory disclaimer on many financial products that "past performance may not be indicative of future results" is there for a good reason. Even if one possesses prior experience with relevant situations, this may not translate well going forward.
What is worse is that due to this lack of prior knowledge, even observing an infinite amount of data cannot confirm a hypothesis. Ever.
Consider the hypothesis that a new checkout experience (B) works better than an existing one (A) in terms of some relevant metric. Running an A/B test on 1,000,000 users might show that B is performing 10% better than A. However, this does not logically lead to the conclusion that B is better than A due to the so-called problem of induction examined famously by Karl Popper (and many others before him).
The problem can be stated as such: assuming P, then Q; observe Q and therefore conclude that P is true (∴ means "therefore").

For example, let P1 be the proposition that "B is better than A" and Q1 is "Conversion Rate of B > Conversion Rate of A". Upon observing Q1 there is a temptation to conclude that P1 is true. However, this is an invalid logical chain. This is a well-known invalid argumentative form called 'affirming the consequent', also known as the 'converse error' and as the 'confusion of necessity and sufficiency'.
It may very well be that a particular observation over, say, two weeks, is indeed "Conversion Rate of B > Conversion Rate of A", but it does not lead to concluding that P1 is true since it is possible that if data is collected for another two weeks the effect might disappear or completely reverse its direction. This can happen if the initial result was due to variation in the data. It is also possible that something other than P also results in Q, thus an observation of Q cannot be treated as unconditional evidence for P.
To avoid making the invalid argument of affirming the consequent the question can be posed differently which turns the logic around.
For example, let P2 be the proposition complementary to P1 which is, in this case, that "B is worse than or equal to A". If P2 is true, then "Conversion Rate of B ≤ Conversion Rate of A" (Q2) necessarily follows. However, upon completing an experiment ¬Q2 (not-Q2) is observed. Since Q2 follows from P2 being true and the opposite of Q2 is observed, it can be argued that P2 is not true.
This is the so-called modus tollens argument form:

The two arguments, compared:
P1: "B is better than A" and Q1 is "Conversion Rate of B > Conversion Rate of A"
Q1: "Conversion Rate of B > Conversion Rate of A"
Upon observing Q1, conclude "B is better than A" and Q1 is "Conversion Rate of B > Conversion Rate of A" (invalid).
P2: "B is worse than or equal to A"
Q2: "Conversion Rate of B ≤ Conversion Rate of A"
Upon observing the opposite of Q2 , conclude "B is worse than or equal to A" is not true (valid).
When using statistical methods as part of an online A/B test, this argument takes a probabilistic form instead. The reason is that we cannot measure the actions of all current and future users of a website or application, which is the population we want to infer about. We are limited to a sample of them measured throughout the duration of a test. Therefore, the argument form becomes something like the following - if during a test which has a high probability of producing Q were P true, we nevertheless observe ¬Q, we will consider ¬P as truth.
Probabilistic form or not, the essence is the same, and it is the cornerstone of establishing anything as a corroborated fact in business and science alike. Corroboration here means that it has survived one or more rigorous falsification attempts - experiments which had high probability of refuting the claim, were it false. Since in the valid argumentative form the procedure corroborates ¬P, it should have a high probability of resulting in Q instead of ¬Q, were P true.
Methods and procedures which allow ensuring such high probability are covered in Chapter 2, but for now it is important to remember that we cannot logically confirm what we want to be true. We can only refute what we do not want to be true - we pose "if P then Q" and hope to see "not-Q". This logical difficulty leads to adoption of counterfactual constructs, and is the reason a hypothesis can be rejected, but cannot be accepted, in the strict sense of the word.
While the above might be a bit confusing, this line of reasoning is actually very powerful. In a statistical hypothesis test, our opponent's argument is posed as the null hypothesis (H0). An experiment is constructed in such a way that it can produce, with low probability under H0, data which refutes the argument. If the observed data goes against H0 despite this, it shifts the burden of proof to anyone who would defend H0, due to the low probability of the event happening were H0 true.
A statistical test allows shifting the burden of proof to whomever is defending the tested null hypothesis.
Even though disproving P2 (B ≤ A) is not equivalent to proving P1 (B > A) due to the problem of inference discussed above, P1 remains as the more viable option. A manager opposing the decision, a designer not happy with the new look, or a salesperson disapproving of the removal of some checkout feature, are all examples of stakeholders defending the null hypothesis that our intervention is not working, or worse. A good statistical test can provide evidence against the null hypotheses.
One can also consider a slightly different angle to the argumentative form of statistical inference. Prof. D. Mayo (Mayo 2018) sees statistical inference from experimental data as a strong argument from coincidence. The argument is to treat the data (Q) as evidence that it has not been observed due to error to the extent to which a statistical procedure with very high capability of signaling an error, if and only if it is present, nevertheless detected no error. If this way of presenting the logic of testing works better for you, feel free to use it in communicating results from A/B tests.
1.5 Establishing causality
Having a basic understanding of counterfactuals, we can now explore the way online controlled experiments help us establish causal effects. The claims (hypotheses, propositions) made in business are mostly causal, such as "changing the call-to-action on this button leads to an increase in sales", "our new checkout user experience results in an increase in purchases per user", "this new email delivery software increased our retention rate by 5%", and so on. These are all claims of causal effects - we did X, and Y followed because of that.
Simple logic such as "if the new checkout works better than the old one, we will see an increase in average revenue per user; we see an increase in average revenue per user, therefore our new checkout works better than the old one" is invalid, as discussed above.
It is doubly invalid if it comes from observational data - for example, comparing the average revenue per user before and after the implementation of a new checkout flow. A positive or a negative change in a key performance indicator can be caused by a potentially infinite number of factors, most of which are not measured, or outright unknown.
Examples of such possible factors include - an improvement to the loading speed of the site which was released at about the same time, actions of major competitors, changes in credit card interest rates, the weather, changes in the general economic environment and consumer confidence, a tracking issue or resolving a tracking issue, changes in customer support staff, etc. The above is encompassed by the famous saying that correlation does not necessitate causation.
It is a simple statement which rests on the fact that a correlation between observations of A and B can be due to:
•   A causing B (direct causation);
•   B causing A (reverse causation);
•   A and B being consequences of a common cause, but not causing each other;
•   A and B both causing C, which is (explicitly or implicitly) conditioned on;
•   A causing B and B causing A (bidirectional or cyclic causation);
•   A causing C which causes B (indirect causation);
Finally, it could just be a 'lucky' coincidence, and there is no causal link at all!
A/B tests, or more generally 'randomized controlled experiments' were developed to address these problems. They are often referred to as 'randomized controlled trials' (RCTs) in the scientific literature.
The second word is easier to explain. 'Controlled' refers to the fact that some users or sessions are set aside as a 'control group'. That group would not be subjected to any changes whatsoever.
Having a control group is very useful, as it serves as a baseline to which to compare any changes we introduce. Since the data on its performance is gathered alongside the data for the treatment groups, any time-varying factors influencing one group also have a probability of influencing the other. For example, a competitor announcing a major discount offer one week into our test will have an effect on the conversion rate of both our control and treatment groups.
Compare this to a situation in which, as with observational data, the baseline is from before the changes are introduced. The data has already been gathered, so the baseline will not be affected at all by changes occurring after its collection. If these changes in the environment are known and their effect can be estimated, it can be compensated for in the analysis by adjusting the baseline post-factum. However, what about the multitude of unknown and/or unmeasured changes?
Here comes the 'randomized' part of the term. It refers to the fact that the assignment of users or sessions to treatments is done using a randomization device which guarantees that each user or session has a specified probability of being assigned to any of the treatment arms. With equal allocation between two groups (probability is 0.5/0.5) a user arriving on a site for the first time since the A/B test was started should have equal chance of experiencing both the new checkout flow and the old checkout flow in a properly set up experiment.
This has the pleasing effect of resulting in a tendency to distribute the plethora of factors influencing the outcome approximately evenly between groups. The larger the number of users or sessions included in a test, the more this tendency should be pronounced when examining measurable factors.
However, the utility of randomization is that it allows us to ignore unobserved factors that might have an effect on the outcome of the test, such as some of the ones mentioned above with regards to observational data (competitor actions, technological changes such as browser updates, the weather, general economic trends). By randomizing the allocation of users between the test groups, whether or not they end up in one group or another becomes an independent variable with regards to the test outcome. This allows modeling of the outcome in terms of difference between the groups as a random variable, which is an important prerequisite for constructing many fundamental statistical tests.
Using a control group and random assignment allows ignoring unobserved factors and modeling the parameters as random variables.
Randomization also saves us from proving and defending the effect of any observed factors (a.k.a. covariates) that we might otherwise condition the assignment upon. For example, if from other tests one concluded that the user's browser, or the speed of their internet connection, have a certain effect on conversion rate, they might be tempted to condition on these factors. However, this would result in the experimenter having to defend the vulnerable position of asserting that this same effect is present in the new situation, without having any data to back this assertion up, since past experience might not apply to the case at hand.
Choosing to do so regardless results in a blocked randomized controlled trial. In such experiments, blocks of users are defined based on certain factors, and randomized across treatments within each block. Such designs can demonstrate some efficiency gains when the available sample size is extremely small. When larger sample sizes, such as the ones usually available in online A/B testing, are used there is usually no improvement in statistical efficiency. In this scenario, a blocked randomized controlled test may be less efficient than a simple randomized controlled approach due to the added infrastructure cost and complexity of the statistical calculations.
It is important to understand that while randomization tends to produce balanced groups in terms of measured factors, such as geographical location, browser, internet speed, time from first visit, demographic factors, traffic source, etc., such a balance is not a prerequisite for proper statistical analysis and conclusions from the data (Senn 2012). Statistics like the p-value actually take into account all the possible combinations of effects from such factors, balanced or not, so the results are statistically valid even if an egregious imbalance is observed in a certain A/B test. In fact, it would be wrong, as in inadequate and inefficient, to use a statistic which is based on a random assignment model for a situation in which, for example, users are re-randomized several times until a certain balance of factors is achieved, or if they were assigned based on a set of factors in the first place.
The above works since while there can be an infinite number of unobserved factors, their summed influence is bounded. In a sense, the only factor which matters in the end is the potential outcome of the test, and statistical calculations make use of this.
Proper randomization neither guarantees nor requires balanced allocation between groups.
This book deals exclusively with simple randomized controlled trials due to the inefficiency of blocked randomized controlled trials in the context of most online A/B testing scenarios.
1.6 Ruling out alternative explanations for the data
A final challenge remains to use data in order to arrive at conclusions and inform decision-making.
Even when using data from a proper randomized controlled experiment and the deductively valid logic of statistical inference, all results are still subject to an issue familiar to most who have attempted to explain anything using data - the so-called Duhem-Quine problem of underdetermination of theory by data. It states that no experiment, no matter how well-designed, can deliver decisive evidence for or against a hypothesis, since no experiment can test a hypothesis directly.
The problem can be best understood by considering that any kind of experiment tests not only the main proposition P, but also a countless number of auxiliary propositions A1, A2, ... An. Any of them, or a combination of them, might be true or false and result in an outcome which can be interpreted in favor of, or against, a hypothesis, without being related to its truth or falsehood.
Consider some of the auxiliary hypotheses in the examples from 1.3 - one is that there is no bias in the randomization procedure, another is that the A/B testing software is working as expected, yet another is that the tracking is accurate or, at best, that any inaccuracies are not biased against one of the hypotheses, that the statistical model is adequate, and so on. Any of these Duhemian problems can be the reason behind whatever outcome is observed at the end. Issues with randomization, software bugs, biased tracking and statistical model inadequacies regularly invalidate experiments, and there is an infinite number of unknown factors which might have the same effect.
Luckily, statistical analyses can help detect and, therefore, eliminate, minimize, or take into account some of the possible alternative explanations, due to these auxiliary hypotheses. Statistical methods can be used to check the output of the randomization algorithm, in order to ensure that it is distributing users or sessions according to specification. Biased tracking can be detected with statistical methods during a so-called A/A test where treatment and control are the same - no intervention. Tests of statistical adequacy can validate our statistical model vis-à-vis the data without focusing on any specific issue. These methods are covered in Chapter 3.
Of course, these are no substitutes for performing regular quality assurance on our testing and tracking setup.
1.7 Statistical methods and efficient use of data
In a brief sidestep from the logic and use of statistical inference, let us consider a few facts about the efficiency of statistical methods used in A/B testing. Understanding them will help prevent certain temptations for unnecessary attempts at statistical innovation.
Frequentist statistics, which this book is mostly concerned with, were developed in the early 20th century with initial applications in agricultural experiments and industrial production experiments (R. Fisher at Rothamsted Experimental Station (Fisher 1925) (Fisher 1935) and William Gosset (Student) at the Guinness brewery R & D department (Student 1908)). They were later improved on by countless others, most notable being the early additions by E. Pearson & J. Neyman whose' lemma (Neyman and Pearson 1933) defined optimally efficient tests for certain scenarios, as well as A. Wald who developed procedures for sequential testing with application in quality control at mass production factories during the Second World War.
One consideration that these early statisticians shared was how to make the most efficient use of the available data (Fisher 1922). Performing experiments was costly in all the above scenarios both in terms of capital and time. Therefore, the methods they developed were guided by the principle of efficiency.
Most of the statistical estimators covered in this book are 'optimal' in the sense that they are the 'best possible' in extracting information from data. Generally, the most efficient estimator is the one which has the least variance while remaining unbiased with regard to the true value of the parameter of interest. In other words, an optimal estimator deviates as little as possible from the true value (θ*) which is to be estimated.
Statistical tests based on fully efficient estimators are the best possible tests upon which no improvements in efficiency are possible. The technical term is UMP-tests - Uniformly Most Powerful. Statistical power is covered in detail in Chapter 4.
Furthermore, many statistics used in this book are also sufficient statistics, meaning that they capture all the relevant information in a set of data.
What this means from a practical standpoint is that when using such statistical methods, based on sufficient and fully efficient estimators in online A/B testing, there is nothing further to be learned from the data about the parameter of interest, and there is a more efficient method for analyzing this particular dataset does not exist.
This is important to understand since otherwise there is the temptation of attempting to augment, or otherwise alter the methods, or to begin searching for ways to analyze the data more efficiently. Each of these would prove fruitless and likely harmful. If one starts finding what appear to be shortcuts, despite already using the data in the most efficient way, then it's important to start asking where any perceived gains come from, and at what cost. There is no free lunch in life, and there is no free lunch in statistics.
Examples of things not to do include:
•   Trying to invent proxy metrics (surrogate metrics) only due to perceived reduction of variance which, if it was real, would lead to faster testing. A proxy metric is by definition noisier, and thus less informative, completely negating perceived advantages of this kind.
•   Attempting to calculate a 'baseline variance' or 'background noise' metric on a per-project basis and to judge tests based on whether the outcome is within its bounds. Instead, use the statistical estimates obtained during the test as these are based on the most recent, and thus most representative sample.
•   Inventing new rules for stopping tests early. Optimal rules have already been invented.
•   Attempting to reinvent the p-value. I've seen many attempts which stem from failure to understand what it does and how well it does it.
•   Using pretest data or 'warm-up' periods as a kind of out-of-sample control. Instead, combine all of the data in one sample in order to extract the maximum information from it.
Concepts such as 'variance' and 'p-value' are introduced in the next chapter, but for now it suffices to rely on what we have already established, if we are using statistics based on fully efficient and sufficient estimators. These cannot be improved upon.
1.8 Caveats in using statistical methods in business
This chapter began by narrowing the field of application of statistical methods to situations in which statistical estimation and measurement of uncertainty can assist data-driven decision-making. But there are a few other considerations that need to be discussed as well.
The first is that the everyday understanding of both uncertainty and certainty should be abandoned in any exploration of statistics. During an everyday conversation, if someone states that they are 99% certain something will not happen, most understand that said thing won't occur. When a statistic says that there is just 1% probability of something happening, it is saying that it is certain said thing will happen given enough tries or observations. It actually states exactly how often one can expect it to happen, on average; i.e. 1 in 100.
In other words, using 99% confidence intervals or a p-value of less than 0.01 as a threshold for deciding whether to undertake a certain set of actions will result in action based on wrong information in up to 1% of cases.
A second important caveat is that even under the assumption of perfect alignment between measurements and business goals, meeting a certain statistical threshold is still not a call to action, regardless of how little uncertainty the statistic reports. Statistics should not be confused with decision-making tools. Decision-making is a process which can certainly be improved by basing data interpretation on statistical estimates, but it is not synonymous with statistics.
One can think of a statistical measure as a guide to interpretation, the same way that a car speedometer is a guide on how to interpret our current driving. The same speedometer reading will be interpreted differently in different situations. Going 30 miles per hour (45km/h) might be fine in a city, but it would be too slow on an interstate highway. Similarly, a 95% confidence interval which excludes zero improvement might result in adopting the tested user experience in one situation, but in rejecting it in another, depending on a host of external factors.
Finally, statistical estimates are only as good as the data they are based on. The Duhem-Quine problem was already briefly explained, but a restatement might be useful - statistical methods do not possess the magical ability to decipher what is true and what is not when being fed with bad input. If tracking is broken, or if an A/B testing software is biased, performing a statistical analysis on the data will not miraculously fix this. Data quality assurance checks should be an integral part of any experimentation program.
Similarly, performing a statistical test or estimation without checking if the prerequisite assumptions hold means that the result could be thrown off target, or in the wrong direction entirely. There are different methods for addressing such mis-specification problems, including statistical procedures, and these are covered in significant detail in Chapter 3.







Chapter 2
ESTIMATING UNCERTAINTY. STATISTICAL SIGNIFICANCE, P-VALUES, OTHER ESTIMATES.
This chapter covers the concepts of p-value, statistical significance of an outcome, and confidence intervals for different parameters. These see daily usage in the design and evaluation of A/B tests. Proper understanding of these concepts depends on differentiation between a data-generating mechanism, a statistical model, and a statistical hypothesis. Therefore, this chapter beings by going through each of them in detail, starting from the more familiar concept of a substantive hypothesis, and going all the way through to the data-generating mechanism.
As some of these are somewhat counterintuitive for people not already familiar with statistics, this chapter includes common fallacies in using and interpreting them. Examining these fallacies will serve the dual purpose of understanding them on a deeper level, while preventing you from misusing them in the future.
2.1 Substantive hypotheses
One of the first things one encounters in testing is the scary word 'hypothesis'. For example, it is common advice that one must always have a defined hypothesis before conducting an A/B test, or any kind of experiment. Most of the time this refers to a substantive hypothesis (a.k.a. 'research hypothesis' or 'experimental hypothesis'), which, interestingly enough, is not a requirement for performing a statistical test.
Substantive hypotheses come quite naturally as result of specific business queries. We have already discussed examples of substantive hypotheses such as 'changing the call-to-action on this button leads to an increase in sales', 'our new checkout user experience results in an increase in purchases per user', and 'this new email delivery software increased our retention rate by 5%' in the first chapter. What is common between these is that they pose a certain causal link between an action and a measurable effect.
Note that an even broader type of substantive hypothesis exists in which one adds additional qualifiers related to the mechanism which caused the effect. For example, if the claim is that "the new checkout experience results in an increase in purchases per user due to our use of persuasion techniques X & Y", then testing the part before "due to" is possible through an A/B test while testing the second part is near-impossible, especially with a single experiment. This is due to the myriad of other mechanisms through which the effect might be explained.
Substantive hypotheses are usually claims of improvement, but it doesn't mean that a substantive hypothesis can't be a claim of a detrimental effect. However, in practice such claims are usually reserved for the substantive null hypothesis, as opposed to the alternative hypothesis. And due to the counterfactual nature of statistical inference claims cannot be tested directly.
A claim complementary to what we hope to establish is posed as a null hypothesis which can hopefully be refuted through a statistical test. A rejection of the null hypothesis can serve as indirect evidence for the alternative hypothesis as it shifts the burden of evidence to anyone looking to defend the rejected claim.
For example, if the claim put forth is that "the new checkout experience results in an increase in purchases per user", what needs to be rejected is its complementary - "the new checkout experience results in no effect or a decrease in purchases per user". The latter becomes the hypothesis to be tested; the null hypothesis. The former is called the 'alternative hypothesis', as in the alternative to adopt if the null can be rejected. Referring to the modus tollens argumentative form (explained in Chapter 1.4.), the null hypothesis is the proposition P, while the alternative is ¬P.
Formulating the substantive hypothesis that will be tested is just the beginning. There is still an issue to be solved since noone can say how to perform a statistical hypothesis test and arrive at a p-value or confidence interval, with just the substantive claim at hand.
The reason is that when the null hypothesis is phrased only in terms of improvement on some outcome such as conversion rate, then it does not provide enough information in order to calculate the probability of observing a certain set of outcomes due to random variability. In the probabilistic version of a modus tollens, we need to know the probability of observing a set of outcomes ¬Q if the null proposition P is true. The argument can work if the probability of observing ¬Q is known to be sufficiently low to turn the burden of proof on defenders of P, should ¬Q be observed. Stating an explicit statistical model is required to achieve this. And it should be emphasized that positing a statistical model requires first knowing more about the data-generating mechanism.
2.2 Statistical hypotheses
A statistical hypothesis is the framing of a substantive hypothesis in terms of a statistical model. It consists of a description of a data-generating mechanism which could have resulted in the observed data.
For example, under the Simple Normal Model the substantive null hypothesis can be translated from "the new checkout experience results in an increase in purchases per user" to "the true difference in purchase rate between the tested version B and the control version A is less than or equal to zero, the error of the observed purchase rate is normally-distributed, independent and identically distributed (NIID). Furthermore, N users would be measured in each group and the statistical analysis will be performed once the data on all N users has been collected."
While the above is a verbose statement of the statistical hypothesis, it still needs to be translated to a statistical model - a mathematical model which reflects a set of statistical assumptions regarding the process governing the generation of sample data. It is an idealized form of the true data-generating mechanism (DGM), and all statistical estimators and hypothesis tests are based on such a model.
In the above example, some of the assumptions behind the proposed statistical model are for the distribution of the error of the mean (Normal), for the dependence of the separate observations (Independent), and for the persistence of the distribution of the error across observations (Identically Distributed), which for a normal distribution means constant mean and variance.
Since these assumptions might not apply in a particular case, the model might need to be redefined to better reflect the inferential premises. Other variants for translating the above substantive hypothesis to a statistical hypothesis are to change the number of users to observe, to allow for dependence and/or non-ID data, or to perform sequential analysis of the data as it gathers. Any such alteration would lead to a change in the data-generating mechanism, and result in a different statistical hypothesis, even though the tested substantive claim remains the same.
For the more technically inclined, a generic description of a statistical model is (Spanos 1999):

where  is the joint distribution of the sample that encapsulates the probabilistic structure of the sample . Θ (theta) denotes the parameter space, or all possible values of the unknown parameter θ, while X is the sample space or all possible values of the statistic.
The act of specifying a statistical model allows us to assign probabilities to all events of interest. These probabilities are referred to as the sample space. Consequently, this act enables calculating error probabilities with which to assess the optimality and reliability of methods for learning from data.
A statistical test tests a statistical hypothesis expressed as a statistical model of the true data-generating mechanism.
Since what is tested is the statistical model, it is crucial that the model is adequate vis-à-vis the data. The importance of the full and correct specification of a model, and the ways we can check its adequacy with regards to the observed data, is discussed in Chapter 3.
Having a statistical model defined for θ and a given sample space, we can introduce the null and alternative statistical hypotheses, which are by definition complementary sub-spaces of the overall parameter space defined by the model. A textbook null hypothesis states that two samples are coming from the same population, so they share the same statistical model, while a textbook alternative is that they come from different populations, and thus observe a different statistical model. This can be expressed in terms of the parameter space as:

under the condition that they fully partition the sample space Θ:

The question then posed under such a setup is whether an observed data accords better with one of the above subsets.
The null and alternative can also be defined in terms of a particular variable, such as the difference between two means Δ, so the null would state that the two samples come from the same model and the difference in means is zero. Conversely, the alternative will posit the same model, but a non-zero mean.

The alternative is by necessity:

There are other, more practical types of null, and, therefore alternative hypotheses, discussed in Chapter 5.
Note that the full statistical model includes not only the value or range of values falling under the null and alternative hypotheses, but also the assumptions about the shape of the distribution, the dependence of observations, and the persistence of the distribution across observations.
The translation of a business question into a statistical model and statistical estimates, then test results back to business decisions, can be visualized as such:

We are yet to examine many of these concepts, but this overview should provide a clearer picture of the role of statistics in the business decision process and the step at which a statistical model needs to be defined.
2.3 Standard deviation and Z-Scores
Once there is a specified statistical model, we need a convenient and efficient way to calculate the discrepancy or 'distance' between a given observation and a known, expected, or hypothetical centrality measure. One such measure is variance, denoted σ2(sigma square) which is calculated as the arithmetic mean of the squared differences between each data point and the arithmetic mean of the data set:

Squaring the differences is carried out in order to punish larger errors much more severely, as well as to ensure that errors in both directions do not cancel each other out. However, it is not easy to work with variance since it is expressed in square units.
A more convenient statistic is the standard deviation σ2 which is a measurement of central tendency quantifying the amount of dispersion in a given set of data in the original measurement unit. A low standard deviation means that the data points are clustered neatly around the mean, while a high value means they are spread out over a wider range of values.
Figure 2.1 below features two identically shaped distributions, both centered at zero percent with probability on the y-axis and the range of values on the x-axis. The first distribution has a standard deviation of 3%, while the second has a standard deviation of 6%, therefore the latter spans double the range of the former. Note that the respective variances are 9 and 36 in percentage square, but how do you work with that?


Figure 2.1: Identically shaped distributions with equal mean and different variance and standard deviation.

Mathematically, the standard deviation is calculated by taking the square root of the variance:

The fact that the unit of standard deviation is the unit of the original measurement makes it easy to work with. For example, if the original unit is a proportion, such as 0.1, the standard deviation will also be a proportion. If it is a percentage, the standard deviation will also be in percentage form. If it is in U.S. dollars, the standard deviation will also be expressed in U.S. dollars.
Now imagine an observed test outcome of 12% with a distribution with standard deviation 6%. An intuitive way to communicate how far apart from the distribution mean 12% would be is to say that it is 12%/6% = 2 standard deviations from the mean.
By dividing the observed discrepancy by the standard deviation, we acquire a standard score. Expressing the distance from a model as the number of standard deviations, instead of as a relative difference, absolute difference, percentages, dollars, etc., has the benefit of making it easily comparable across measurement units.
A generic equation for a distance function producing a standard score is (Mayo and Spanos 2011, 4):

X̄ is the arithmetic mean of the observed values, µ0 is a hypothetical or expected mean to which X̄ is compared; e.g. zero percent improvement when using a simple superiority alternative hypothesis. σx is the standard deviation of the mean of x, a realization of X, also called the standard error of the mean.
When the distribution of the variable of interest is normal, the standard score is called a Z-score. The Z-score is a measurement of the difference between observed data and a statistical model expressed as the number of standard deviations away from the null. It is the most commonly used measurement of this type. If the variable has a different distribution, such as T-distributed or X2-distributed, we talk of a T-score or a X2-score instead.
The Z-score is a distance measures expressing the difference between expected and observed data in number of standard deviations.
Since a Z-score is expressed as the number of standard deviations away from the mean of a normal distribution, lines drawn perpendicular to the x-axis at the point of the Z-score always cut out a specified percentage of the probability density. Figure 2.2 shows some Z-scores and their corresponding quantiles:


Figure 2.2: Z-scores and their corresponding percentages.

When the standard deviation is calculated from a sample, as it usually is, a correction is introduced wherein we divide by n-1 instead of n, which obviously has much less of an effect the larger n is, which is why it may sometimes be omitted without practical consequences.

Conveniently, when the metric is binomial - a conversion rate or drop-out rate of any kind - then the standard deviation is simply:

where p is the proportion of observations with events to those with no events. For example, if the conversion rate is 2%, then p is 0.02 and the standard deviation of a sample of 100 observations is the square root of 0.02 times 0.98 divided by 100, or 0.014.
As we are often interested in working with means, it is useful to know that the standard deviation of the mean is simply the standard deviation estimated from the sample divided by the square root of the number of observations in the sample:

The standard deviation of the mean is often called standard error of the mean (SEM).
When calculating the standard deviation for absolute difference in proportions or other means, the standard deviation estimate is the pooled standard deviation of the two groups. Remember that under a standard null hypothesis the data-generating mechanism behind the data in each group is the same, so the best estimate for the actual standard deviation is the pooled standard deviation.
For difference in proportions, the pooled standard deviation of the means is calculated using the formula:

where  is the pooled proportion calculated using the equation:

where p1 and p2 are the proportions in the two groups and n1 and n2 are the total observations in each group. Therefore, p1 times n1 gives us the number of events in group 1, which can be a direct input into the formula, if known.
The Z-score for difference in proportions is then computed as:

For example, for an A/B test with 1,000 users in each arm, and proportion of events to total observations (conversions vs. users, purchases vs. users, etc.) of 0.1 in A and 0.15 in B, the Z-score is computed as:

For difference in means other than proportions, the pooled standard deviation of the means is calculated using the formula:

where σ1 and σ2 are the estimated standard deviations of the two groups, calculated using the formulas provided above.
The Z-score* is, accordingly:

* this is technically a T-distributed variable, but for sample sizes encountered in A/B testing it is equivalent to a Z-distributed one - see end of subchapter.
Note that in practice in both cases the observed baseline mean (p1, µ1) is used in place of µ0 as it is the best estimate for the true baseline performance.
Important Observations
From the formulas given above it can be seen that the Z-score is positively related to the observed discrepancy from the mean, meaning that the Z-score will be larger if the observed difference is larger, everything else being equal. Z-score will be higher if the absolute difference is 0.05 than if it is 0.02, if in both cases 1,000 users are measured and the standard deviation is the same.
Due to the denominator in the standard deviation equation, the Z-score is also positively related to the number of observations - the more observations there are, the larger the Z-score will be, given the same observed difference. Z-score will be larger with an observed difference of 0.05 in an A/B test with 10,000 users, compared to observing the same difference with only 1,000 users.
From the numerator in the standard deviation formula, we can observe that the Z-score is inversely related to the standard deviation of the data - the same outcome in terms of sample size and observed difference will result in a lower Z-score if the standard deviation of the data is higher.
If the observed difference is 0.05 with 1,000 users and a standard deviation of 0.03 then the Z-score will be 0.05/0.03 = 1.667, while if the standard deviation was 0.02, then the Z-score would be 2.5. It conforms with the intuition that if the data are less spread out, the same observed value will produce a higher Z-score. The higher score reflects the fact the observations is much more extreme relative to the expected probability.
For example, calculating a Z-score of 1.64 means that the observed value is 1.64 standard deviations away from the mean (1.64σ). When the distribution is normal, a value of 1.64σ cuts a slice from the distribution to the right of it, which contains about 10% of the total cumulative probability density. This will prove to be important shortly.
A Z-score is positively related to the observed difference and the sample size while being negatively related to the standard deviation.
A brief note on the T-score and T-distributed statistics. A T-score is much like the Z-score, except it is applied when the standard deviation of the population being sampled from is unknown, which is the standard case in A/B testing practice when using non-binomial variables. A T-score follows a distribution from the T family, with specified degrees of freedom, and can be converted to percentiles using the cumulative distribution function for that distribution. T-scores will not be discussed in any significant detail, since for sample sizes encountered in online A/B testing the T-score and the Z-score can mostly be assumed equivalent. For large sample size n the T-distribution of n-1 degrees of freedom is equivalent to the normal distribution: the two distribution types are considered equal for practical purposes for n larger than 30.
2.4 p-value and type I errors
It is time to introduce the main character of this chapter - the p-value. The p-value is usually defined as the probability of observing an outcome as 'extreme' or more extreme than the one observed under the assumption that the statistical null hypothesis is true. Phrasing this in terms of the distance function introduced earlier, the p-value is the probability, under the assumption of the null hypothesis being true, of the distance function producing a score as extreme or more extreme than the observed.
A preferred definition which makes it explicit that the p-value is a characteristic of the testing procedure relative to a specific hypothesis is:
A p-value is the probability of the testing procedure producing an outcome as extreme or more extreme than the one observed, under a specified null hypothesis.
An even more explicit but slightly more tedious definition is: "a p-value is the probability of the testing procedure producing an outcome as extreme or more extreme than the one observed, under a specified statistical model corresponding to the substantive null hypothesis".
Some practitioners might be tempted to 'abbreviate' the above definitions, and state instead that "the p-value is the probability of the data given the null hypothesis". However, this statement is false as can be seen by the greater than or equal sign in the proper notation below.
The p-value formula in proper notation is the marginal probability

where P stands for probability, d(X) is a test statistic (distance function) of a random variable X, x0 is a typical realization of X and H0 is the selected null hypothesis. The semi-colon means 'assuming'. The distance function often comes in the form of a T-score or Z-score function, which means that the p-value is calculated via the cumulative distribution function (CDF) of the T-distribution, or Z-distribution, respectively.
Table 2.1 features a list of commonly encountered standard deviation / Z-score cut-offs for normally distributed variables.



Selected Z-score cut-offs for normally distributed variables



Z-score

p-value / Percentile (One-Sided)

p-value / Percentile (Two-Sided)



0.3186σ

0.500 / 50.00%

0.750 / 25.00%



0.5000σ

0.309 / 69.15%

0.617 / 38.29%



0.6745σ

0.250 / 75.00%

0.500 / 50.00%



0.8416σ

0.200 / 80.00%

0.400 / 60.00%



1.0000σ

0.159 / 84.13%

0.317 / 68.27%



1.2816σ

0.100 / 90.00%

0.200 / 80.00%



1.6448σ

0.050 / 95.00%

0.100 / 90.00%



1.9599σ

0.025 / 97.50%

0.050 / 95.00%



2.3263σ

0.010 / 99.00%

0.020 / 98.00%



2.5758σ

0.005 / 99.50%

0.010 / 99.00%



3.7190σ

0.001 / 99.99%

0.002 / 99.98%




Table 2.1: Selected Z-score cut-offs and their corresponding quintiles.

For example, if the observed outcome of d(x0) is a Z-score of 1.96 then, assuming a one-sided alternative hypothesis of superiority, the cumulative probability function can be computed for all values of Z from 1.96 to +∞ to arrive at a p-value of approximately 0.025. From the inequality in the above notation it becomes clear that a p-value calculation is not the probability of observing exactly 1.96, nor an infinitesimal interval around it.
In the example for calculating a Z-score in the previous subchapter, the obtained Z-score was 1.429 for an A/B test with 1,000 users per arm, 0.10 conversion rate in the control, and 0.12 in the variant. To convert it into a p-value, it is necessary to calculate the cumulative distribution function of the Z-distribution to get a p-value of 0.0765. This means that the probability of observing a Z-score of 1.429 or larger is 0.0765 - it would happen in 7.65% of identical tests from a hypothetical infinite run. Again, it is not the probability of the particular outcome.
The reason we are not interested in the probability of observing the particular observed outcome, but instead calculate the probability of the observed outcome - or any value more extreme than the observed (shaded area in Figure 2.3) - is that we want the p-value to act as a bound on the probability of committing the error of rejecting the null hypothesis when it is in fact true. This is what is known as an error of the first kind, or, more-commonly, a type I error. It is denoted by the Greek lower-case letter alpha (α).


Figure 2.3: The relation between p-values, significance thresholds, and an error distribution.

Since the null would have been rejected if the p-value was lower than the observed, and since this could only happen with this sample size if the observation was more extreme, we include that probability in our uncertainty measurement. This then enables the p-value to act as a bound on the type I error rate.
It should be noted that the probability of observing exactly the outcome which happened to be realized, no matter what it is, is zero, and there is a reasonable amount of probability density only if computing the integral over a reasonably-sized interval around the observed value. This follows from the property of the p-value to have a uniform distribution under the null hypothesis. Each possible range of p-values is equally as likely to be observed as any other range of the same size, assuming the null hypothesis is true.
This property holds regardless of sample size since frequentist methods maintain their qualities for any finite sample of two or more observations. This is unlike other types of inferential tests or estimation methods, which offer only asymptotic guarantees (the statistic converges to the specified limit with infinite sample size), and may have unstable small-sample performance.
The procedure of computing a p-value and comparing the observed value to a predetermined threshold of interest, referred to as 'significance threshold', is widely known as a 'test of significance' or a 'statistical significance test'. Another frequently used term is a Null Hypothesis Statistical Test or a Null Hypothesis Significance Test, which are equivalent in most cases and are both abbreviated to NHST. The presence of 'null hypothesis' in the term emphasizes the fact that a p-value can only be defined relative to a specific null hypothesis, and can only be interpreted through it. The significance threshold is often referred to as the 'size' of the test, although this notation is not common in the online A/B testing world.
For example, it might be decided that observing any p-value less than 0.01 will result in rejecting the null hypothesis. This, then, would be a significance test of size 0.01 since the significance threshold is set to 0.01. If the null is that of no or negative effect, then rejecting it means we continue to act as if the opposite claim is warranted and we implement the tested version.
2.5 p-value: utility and interpretation
The utility of the p-value over other presentations of the input of the data comes from the fact that it is a summary statistic which incorporates information about the size of the observed difference between the test groups and the sample size, as well as the characteristics of the frequency distribution of the error of a parameter of interest. If we know the p-value, and we know it has been calculated using an adequate statistical model, we don't need to know all the details of said model in order to properly interpret and use it, which is very convenient.
The p-value is a convenient expression of the distance function and incorporates counterfactual information about the observed parameter, the sample size, as well as the frequency distribution of the error of a parameter of interest.
An important observation about the definition of the p-value which should help in its proper interpretation is that it mentions the null hypothesis as an assumption only. The p-value is thus not a quantity one can attach to either the null or the alternative hypothesis. The p-value and other error-probabilities in frequentist inference are characteristics of the testing procedure and do not directly or indirectly characterize the probability of a hypothesis being true or false, nor are conditional on that being so. They are counterfactuals.
The counterfactual nature of the p-value is clear from the fact that it is the probability of a certain observation under a hypothetical assumption. It is not the probability of the outcome being 'due to chance', the probability of the null hypothesis, the probability of the alternative hypothesis, or the probability of making a wrong decision. In fact, such concepts are not defined in frequentist statistics and only make sense with inverse probability; a.k.a. Bayesian inference. Inverse probability was widely taught before the invention of the p-value and the Null-Hypothesis Statistical Test in the early 20th century, after which it was mostly displaced by frequentist methods in both business and the sciences (Zabell 1989).
Examining the logical consequences of observing a certain low p-value such as 0.001 should make it easier to grasp the above.
There are three possible realities under which a p-value of 0.001 can be observed, all entirely logically valid:
1.   The null hypothesis is not true.
2.   The null hypothesis is true, but a very rare outcome has been observed.
3.   The statistical model is inadequate hence the calculated nominal p-value is not an actual p-value.
Most of the time we want to interpret a low p-value as evidence that the null hypothesis is not true. However, the other two possibilities can never be ruled out entirely.
There is always the possibility of observing a very rare outcome. With more and more evidence it should become a more and more unlikely coincidence, but it can never be ruled out with 100% certainty. The role of the p-value is to quantify this uncertainty, however small it may be.
The third possibility refers to the fact that a p-value might be calculated under a mis-specified statistical model - a model which doesn't reflect the real data generating mechanism well enough. Since a mathematical equation doesn't care if it is a part of a well-specified model or not, it will always output a (nominal) p-value. However, a p-value from a mis-specified model is, in fact, uninterpretable, at least in relation to the question at hand. Attempting to apply it to reality will have an unpredictable outcome.
However, there are statistical methods which help us distinguish adequate models from inadequate ones. These are the family of so-called misspecification tests (M-S tests), which are covered in Chapter 3.
Being an error probability of the testing procedure related to a counterfactual claim in the way described above, the p-value speaks to the quality of the data relative to that claim, but does not speak to the claim directly. In a sense, an error-probability is like the error-tolerance on a scale or another measurement device - the accuracy and precision of the instrument remains the same regardless of whether what is being weighed is a person, an animal, or an inanimate object. Weighing a person and observing a scale measurement of 70kg does not mean that we can automatically argue that the person in question does not weigh 69kg or less. However, if the scale is known to be accurate to 0.1kg in 99% of measurements, that accuracy estimate can be used to argue that it is extremely unlikely that the person weighs 69kg or less, given the measurement of 70kg obtained from a scale accurate to 0.1kg in 99% of cases.
In this sense, a p-value is a measure of how surprising a result is under the assumption that the null hypothesis is true. The more surprising it is, the harder it becomes for anyone to argue for the null hypothesis, since it seems to be a more and more preposterous coincidence that the null hypothesis is true, and we instead experienced a form of misfortune by observing such a rare outcome.
For example, consider the substantive null claim "the new checkout experience results in no effect or a decrease in purchases per user" as in prior examples. After observing a p-value of 0.001, anyone wishing to claim that the new checkout had, has, or will have a bad or neutral effect on purchase rate would have to explain why, if that is the case, we observed something which would have only happened once in 1,000 such A/B tests. That is the equivalent of observing 10 heads in a row when flipping a fair coin, on the first attempt!
Sure, rare events do (rarely) happen. But if a person refuses to accept that there is a level of rarity that, when breached, must force them to reconsider and potentially forgo their position, then they are simply admitting that no amount of evidence can ever move them from their stance. This would be incompatible with operating a business through data-driven decisions.
Understanding uncertainty is key for implementing a data-driven approach in the real world. While prior to running an experiment, we must agree that if we observe a p-value lower than a certain (usually "low") significance threshold we will reject the null hypothesis, it doesn't mean that a conclusion reached through the procedure is irrefutable, certain, or unquestionable. Statistics is the science of estimating uncertainty. It cannot lead to using qualifiers such as the above, it can only suggest how close we are to having irrefutable evidence.
2.6 Confidence intervals
A confidence interval is a tool for presenting the uncertainty associated with a given measurement of a parameter of interest. Confidence intervals are a sister concept of the p-value, but they aim to serve more of an estimation purpose, rather than operating as a hypothesis testing tool. However, since there is duality between estimation and hypothesis testing for all statistics, there is also duality between p-values and confidence intervals. One can usually be easily converted to the other.
But first, what is a confidence interval? A confidence interval is a random interval on the real line that when constructed over repeated tests of the same type with different data it covers the true value of the parameter of interest a specified proportion of the time. This proportion is called a confidence level, and is usually expressed as percentages resulting in 90%, 95%, 99%, etc. confidence intervals. The term 'confidence' is technical and refers specifically to that proportion, also known as coverage probability.
A confidence interval is a random interval on the real line which covers the true value of the parameter a specified proportion of time, when constructed over repeated tests of the same type.
Customarily, confidence intervals are constructed with two bounds - one from above and one from below, lower and upper, also called confidence limits. However, an interval can also have a limit on just one side only, spanning to minus or plus infinity in the other direction, as shown on figure 2.4.
The generic formula for calculating the bounds a confidence interval is:

where P is the parameter we are measuring and ME is the desired Margin of Error for the parameter, usually based on variability estimates from the data.
In online A/B testing the parameter is usually either absolute difference in proportions or relative difference in proportions, but can also be differences in means of continuous metrics. The margin of error is calculated from the estimated standard deviation of the parameter, multiplied by a Z-score corresponding to the chosen confidence level.


Figure 2.4: Confidence intervals: two-sided and one-sided.

For example, the bounds of a confidence interval for absolute difference between two means are calculated with the following equation:

where μ1 is the mean of the baseline or control group, μ2 is the mean of the treatment group, n1 is the sample size of the baseline or control group, n2 is the sample size of the treatment group, and σp is the pooled standard deviation of the two groups. Z is the score statistic corresponding to the desired confidence level. The formula for absolute difference of proportions - e.g. conversion rates of all kinds) - is the same as they are just a special type of mean.
Examining the formula, we can see why it is a random interval: the interval bounds depend both on the random error introduced by the observed difference in the means, and the error in estimating the standard deviation of said difference. The construction of either the upper or the lower bound is equal to adding or subtracting a certain number of standard deviations, multiplied by the standard error of the mean to the observed difference in means.
Visualized as a distribution of the error of the mean, an interval bound cuts a certain percentage of a distribution that is centered on the observed value - either to the left, to the right, or on both sides of it.
Further examining what goes into the estimation of the margin of error we can see that the larger the sample size is, the narrower the width of the interval gets. This happens since we are dividing the pooled standard deviation by a larger number which ultimately results in a smaller number being added or subtracted from the observed parameter value. With infinite sample size an interval collapses into a single point on the real line. This follows our intuition that the more data we have, the less uncertainty an estimate of the parameter of interest should have.
We can also observe that requiring a higher confidence level means a larger value for Z is used, resulting in a wider interval, and vice versa. A 99% confidence interval will always be wider than a 95% confidence interval, all else being equal.
Note that, the Z-score corresponding to a two-sided interval at level α (e.g. 0.90) is calculated for Z1-α/2 revealing that a two-sided interval, similarly to a two-sided p-value, is calculated by conjoining two one-sided intervals with half the error rate. For example, a Z-score of 1.6448 is used for a 0.95 (95%) one-sided confidence interval and a 90% two-sided interval, while 1.956 is used for a 0.975 (97.5%) one-sided confidence interval and a 0.95 (95%) two-sided interval. The issue of one-sided vs. two-sided CIs is covered in more detail in Chapter 5.
Interpretation of confidence intervals
Like p-values and other frequentist distance measures, a confidence interval is a characteristic of the testing procedure and not the hypothesis being tested. However, unlike a p-value, a confidence interval does not reference any particular null hypothesis, but it can easily be translated into claims regarding different null hypotheses, as already pointed out.
While confidence intervals (CIs) are quite convenient as a visual tool, they do face certain difficulties in interpretation. Once a CI is realized from the data, we can no longer use the probabilistic language in which the procedure is framed.
For example, it would be incorrect to state that the true value lies within the confidence bounds with probability XX% (XX% being the confidence level of the CI). The true value is either within the realized interval or not. The probability guarantee is for the procedure, not the particular interval we happen to observe. An interpretation from the point of passing a severe test can bring back meaning to an interval (discussed in detail in 2.10 "Severity").
From a severity standpoint, the correct way to interpret a confidence interval relies on counterfactual reasoning similar to that applied in interpreting p-values. For example, upon observing a 95% interval spanning from 0.02 to 0.05 we can say that claims that the true value lies outside of that interval are not supported by the data since, had the true value been outside of the interval, the statistical procedure would have, with probability XX% or greater, resulted in an interval different than the one observed.
When one is interested in making claims like "we can rule out values less than X with 95% confidence", where X is some minimal value of interest, one-sided confidence intervals are called for. For example, we may want to implement the tested variant if we can rule out values less than 0.01. In such cases it would be incorrect to use the lower bound of a two-sided 95% confidence interval - a one-sided 95% interval should be used instead. By using a two-sided interval to make such a claim we would be overstating the uncertainty since the 95% two-sided interval would exclude values less than 0.01 with 97.5% probability, if the true value is indeed greater than 0.01. However, a one-sided 95% confidence interval will have a lower limit which will be greater than 0.01 with 95% probability, if the true difference is greater than or equal to 0.01, which is exactly what we want.
Duality between confidence intervals and p-values
Comparing the formula for computing confidence interval bounds to the formula for computing a p-value for absolute difference in proportions, we can see where the duality between confidence intervals and p-values stems from.
The duality results in that a confidence interval bound defines a set of values, which, if contained as part of the null hypothesis, would not be rejected with the given data at a significance threshold corresponding to the interval confidence level. To put this in other words, we would reject any null hypothesis in a Null Hypothesis Statistical Test defined over a set of values, all of which are outside the confidence interval.


Figure 2.5: Confidence interval and relation to hypothesis testing.

For example, if the lower bound of a one-sided 95% interval is -0.01, then H0: Δ ≤ 0 would not be rejected with a p-value less than 0.05, since 0 is greater than -0.01. On the other hand, any H0: Δ ≤ µ0 for µ0 < -0.01 can be rejected.
There is direct correspondence between confidence intervals and p-values: any p-value can be expressed as a corresponding confidence interval and vice versa.
Converting from a confidence threshold to a significance threshold (p-value threshold) is easy: simply subtract the confidence threshold from 100% and divide by 100 to get rid of the percentage: 95% confidence level is equivalent to 100% - 95% = 5% / 100 = 0.05 significance threshold.
Note that all prerequisites for proper usage of p-values also apply to using confidence intervals - the statistical model they are based on is the same and any issues with its adequacy would affect both estimates.
2.7 Misinterpretations of p-values and confidence intervals
In common with any other complex, but widely applied tool both p-values and confidence intervals are often misinterpreted and misapplied. Common misapplications are examined in our discussion of nominal versus actual p-values in the next chapter. Here we will discuss misinterpretations, which would plague our inferences, and thus our decisions, should we fall into them.
Most misinterpretations stem from the fact that error probability is often confused with the probability of a certain substantive hypothesis, whatever the latter means. This is why the p-value is described as a measure of how unexpected a given observation is assuming the null statistical hypothesis, and explicitly distanced from any conclusions about both the null and alternative substantive claims under investigation.
A common misinterpretation is to treat a high p-value, a highly non-significant outcome, as evidence in support of the null hypothesis. This is an example of a fallacy of acceptance (as in accepting the null hypothesis). We can see why this is not a valid argument1 if we return to point 1.3 and recall that for the probabilistic modus tollens argument to hold we need to ensure the procedure we use has a high probability of resulting in Q instead of ¬Q, were P true. In this case P is the null hypothesis then the decision rule is "reject P if p is high", therefore the statistical method should have a high probability of resulting in a low p if P is true. By definition, getting a low p-value is hard in case P is true, therefore we can discard the use of the rule "reject P if p is high" as inconsistent with the logic of testing.
If, following an A/B test, we want to make the argument for the null hypothesis then we need to use confidence intervals (or severity, discussed at the end of the chapter). For example, if a 95% significance threshold was agreed upon and the upper boundary of a 95% CI is lower than zero, we can argue for H0: Δ ≤ 0. Additionally, if a 95% CI is just above zero, say, +0.5% relative lift, we can then argue for H0: Δ ≤ 0.5%. It should be noted that a test should have high statistical power for it to have a high probability of corroborating a true null hypothesis to a satisfactory extent. If power is low, then the test will only rarely produce evidence for the null hypothesis, even when it is true.
Another variant of this misinterpretation is to say that a high p-value means that the true effect size is likely small. This is not necessarily true, though, since with a small sample size even very large true effects can produce a non-significant outcome with high probability. This is evident from the formula for calculating the Z-score - it will produce non-extreme values if either the sample size or the effect size is small.
Another frequent mistake is to confuse statistical significance with practical significance. For example, following a statistical test with a sample size of 1 million users we observe a difference statistically significant at the 0.01 level, while our threshold is 0.02. However, the size of the difference is so tiny that implementing the tested variant will barely cover the cost of testing over the next couple of years and is, therefore, of little or no practical importance.
This situation happens when one uses default null hypotheses instead of informed ones. Consequently, the rejection of the null hypothesis can have any degree of practical significance. If the null hypothesis is chosen wisely (more in Chapter 5), then this should be less of a problem for the practitioner.
Yet another misinterpretation is to treat 1 minus the p-value as the probability that the observed improvement is the actual improvement. For example, if a p-value is 0.02 this is then interpreted as 98% (1-0.02 in %) probability that the observed difference of 3% is the actual difference between variant and control.
While in most scenarios in A/B testing the observed improvement is also the most likely value of the parameter of interest (maximum likelihood estimate), its likelihood is not represented by 1-p-value, and the data certainly does not support concluding that the true value is equal to, or larger than, the observed one.
This can easily be verified by calculating the p-value for the null hypothesis H0: Δ ≤ δ1 where δ1 is the observed difference. This calculation will result in a p-value of 0.5 if the error is symmetrically distributed, as is the case for most statistics used in online A/B testing. A p-value this large will rarely be under the specified significance threshold.
Another misinterpretation stemming from attaching the error probabilities to the tested hypothesis, instead of to the testing procedure, results in treating the p-value as the likelihood that the alternative hypothesis is true or false. For example, thinking that a p-value of 0.01 means that the alternative hypothesis, say H1: Δ > 0 is 99% likely to be true, or thinking that a p-value of 0.99 means that it is 99% likely to be false. Both these claims would require the involvement of prior probabilities, and would thus become subjective. The concept of a 'probability of a hypothesis' makes no sense in frequentist inference and estimation.
Most misinterpretations of p-values and confidence intervals result from detaching their meaning as a characteristic of the testing procedure or from the null hypothesis under which they are computed.
Another type of misinterpretation of p-values is to treat 1-p as the probability of seeing the same outcome if a test is to be repeated with the same parameters. It comes in two forms. In the first, one thinks that a p-value of 0.01 means that there is 99% (1-0.01 in %) probability of seeing the same value (e.g. difference in conversion rates) in a repeat A/B test. For example, if the current test produced a relative difference of 2% with a p-value of 0.01, this means that if the test is to be repeated, there is 99% of seeing a relative difference of 2%.
Knowing that any particular value on the real line has a near zero probability of being observed makes dismissing such a claim easy, as it would contravene this basic fact. Still, a further discussion can be useful.
The error comes both from dropping the qualifier 'under a specified null hypothesis', as well as from forgetting the inequality expressed as "...outcome as extreme or more extreme than the one observed...", which is in the p-value definition to prevent such misinterpretations. Even if these qualifiers are preserved, such a statement would only hold for a one-sided test where the true value is at the point of the null hypothesis parameter space closest to the alternative hypothesis parameter space, and for no other scenario.
Another reading of the misinterpretation preserves the inequality part of the definition, and refers to the conclusion that we would draw. It goes something like this: "a p-value of 0.01 means that in a repeat experiment there is 99% probability of seeing an outcome which would lead to the same conclusion". If the p-value is 0.01 and our significance threshold is 0.05, we reject the null hypothesis in our test. This is interpreted to mean that there is 99% probability of rejecting the null hypothesis in a repeat A/B test.
As explained, this misinterpretation is due to forgetting the 'under a specified null hypothesis' part of the p-value definition, as well as not specifying the only scenario in which it holds - in a one-sided test if the true value is exactly at the point of the null hypothesis parameter space closest to the alternative hypothesis parameter space. In such a case, it is true by definition. Making the above statement without this crucial qualification makes it false, since it is false for any other true value of the parameter of interest.
If the true value is in the alternative parameter space, the probability of rejecting the specified null would be greater than 1-p, while if it is in the null parameter space it will be less than 1-p. This probability is, in fact, what we call 'statistical power', and it will be introduced formally in Chapter 4.
Many of the above misinterpretations are often transferred to confidence intervals, but I think by far the most common misinterpretation is to think that the values included in an interval are values with XX% probability of being the true ones. This was discussed in detail in the previous section.
2.8 p-values and confidence intervals in decision-making
How do p-values produced from A/B tests help us with data-driven business decision-making and risk management?
From the standpoint of decision-making, using a certain significance threshold puts a conservative upper bound on the risk of mistaking the direction of the effect. The above assumes using one-sided hypotheses, and calculating one-tailed p-values, which is usually the case in online A/B tests (more on one-sided versus two-sided hypotheses, composite versus point null hypotheses in Chapter 5). Otherwise the bound is exact, but difficult to interpret in a practical setting.
A p-value is therefore a bound on the maximum risk of making a decision which worsens our business results, assuming the null hypothesis is defined as equivalent to maintaining the status quo, as it often is. If we were to use the same significance threshold, say 0.01, over a large number of A/B tests, then we are limiting the number of directionally wrong decisions we would make to 1%, since even if all the variants that we test have no real positive effect, we would adopt no more than 1% of them.
To operationalize it, one needs to agree on a tolerable level of measurement error which is applicable to each individual A/B test. In some tests, one can require a very strict error control, while in others one might only seek to guard against the most egregious of errors. In this sense, a significance threshold expresses how improbable a coincidence needs to occur, at a minimum, in order for us to make a conclusion in error, while the p-value is the level of improbability actually achieved, assuming the null hypothesis is true.
If there is no level of error which can be tolerated, then it is obvious that data has no bearing on the decision-making process, since there will be some room for error in even the largest datasets.
The limit on business risk imposed by a decision rule based on a significance threshold is conservative, since if some or all of the tested variants are in fact worse than the control, we would adopt only a proportion of them less than one minus the threshold value. If the threshold is 0.01, we would adapt less than 1% of tested variants in such a situation. This comes from the fact that in one-sided tests we calculate the p-value for the point of the null parameter space closest to the alternative hypothesis parameter space. The result is that for any true value different from that point, the p-value is controlled conservatively.
For example, if the null hypothesis is that the difference in purchase rate is less than or equal to zero, the p-value is calculated for a purchase rate of exactly zero. If the p-value under H0 : Δ ≤ 0 is 0.01, it is even smaller for any other H0 where Δ is smaller than some negative percentage. E.g., if the p-value is exactly 0.01 for H0 : Δ ≤ 0% then it could be 0.001 for H0 : Δ ≤ -6%, meaning that there is ten times less risk the true difference in conversion rate between control and variant is greater than 6% in the negative direction than the risk that it is in the negative direction at all.
Some statisticians have proposed the calculation, not of a single p-value, but of p-value curves as a function of the possible effect size and sign. Others have proposed the equivalent for confidence intervals - calculating interval bounds for all possible confidence levels.
Both of these approaches are appealing in theory as we might want to consider a whole bunch of theoretical null hypotheses. However, in practice, we are usually interested in a certain claim that is defined based on external considerations, before we even begin the test. For example, if we know our investment in shooting more photographs for each product will only pay off if then we can get an improvement in our purchase rate of more than 0.5%, we would be interested in testing the null hypothesis H0 : Δ ≤ 0.5% and we would then not care about H0 : Δ ≤ 0% or H0 : Δ ≤ -2% all that much. Similarly, if our risk/reward analysis suggests an optimal confidence threshold of 95% why would we be interested in having CI bounds at the 90%, 98%, 99%, 99.5% etc. levels? This topic is expanded on in Chapter 5.
Admittedly, there are cases where agreement on the risk/reward analysis is hard to obtain, and there might be competing models to be assessed. If that is the case, such approaches are valuable tools for presenting statistical information.
Finally, it has to be said that in reality it is rarely justified to use the same significance level across all A/B tests, therefore, the above strict decision rules are just hypothetical illustrations in order to avoid the complications of a realistic example with, say, 100 A/B tests; most of which would have different significance thresholds and observed p-values.
Due to the duality between p-values and confidence intervals, the latter are just another way to examine the data with the same consequences - they can serve as bounds on the maximum risk taken with a given decision, as well as a measure of the uncertainty surrounding any estimate of interest.
2.9 Maximum likelihood estimate
While p-values are mostly employed in decision-making, and confidence intervals offer range estimates corresponding to them, the maximum likelihood estimate (MLE) is the preferred choice when a point estimate for a parameter of interest is needed. The MLE is an estimate of the point at which the likelihood function reaches its maximum value. In other words, it is the point with highest plausibility based on our specified statistical model and data x0. The concept was first used by the likes of C. Gauss, P.S. Laplace, T.N. Thiele, and F.Y. Edgeworth, but was made prominent by its fervent proponent R. Fisher in the early 20th century. This happened even before it was proven asymptotically by Wilks in 1938 (Wilks 1938).
For each parameter of interest there can be multiple estimators which maximize the likelihood function. What defines a good MLE is that it has all the desirable properties of an estimator - asymptotic consistency, finite-sample unbiasedness, efficiency, and sufficiency. For some variables there is no maximum of the likelihood function, and therefore, no MLE can be defined.
In practice, one often calculates the natural logarithm of the likelihood function (log-likelihood) due to its convenience as being easier to differentiate. The fact that a logarithm is strictly increasing is useful when calculating maximum likelihood - log-likelihood reaches the maximum at the same point as the likelihood.
In A/B testing, where we are usually interested in differences between means of which proportions is a special case, the MLE coincides with the observed difference of means. So, if the parameter of interest is Δ then MLE(Δ) for each observed δ is equal to δ.
In fixed sample size tests of difference in proportions or means the maximum likelihood estimate is the observed difference.
While in the simple case of an independent and identically distributed data the MLE seems trivial and intuitive, it is not so in other cases. For example, the difference in means might not be the only random variable in the statistical model - which is the case in sequential testing where the stopping time is a random variable as well. In such cases there might be several competing MLEs with different properties, with none being optimal across all properties.
2.10 Severity
A different approach to arguing from data and presenting statistical inferences is proposed in the error statistics approach of Deborah Mayo (Mayo 1983) (Mayo 1996) (Mayo and Spanos 2006), (Mayo and Spanos 2011), (Mayo 2018). Mayo puts forth the severity principles based on which various inferences can be assessed as severe or non-severe. The notion of severity has similarities to Karl Poppers' notion of corroboration as a measure of how well probed a certain claim is.
The weak severity principle states that:
"One does not have evidence for a claim if nothing has been done to rule out ways the claim may be false. If data x agree with a claim C but the method used is practically guaranteed to find such agreement, and had little or no capacity of finding flaws with C even if they exist, then we have bad evidence, no test (BENT)." (Mayo 2018, 5)
It serves to block unwarranted claims with respect to a parameter of interest based on some observed data and a statistical test.
The strong severity principle lets us judge what claims are warranted:
"We have evidence for a claim C just to the extent it survives a stringent scrutiny. If C passes a test that was highly capable of ﬁnding ﬂaws or discrepancies from C, and yet none or few are found, then the passing result, x, is evidence for C." (Mayo 2018, 14)
Following this principle, a claim about a parameter of interest based on certain data is warranted to the extent to which the procedure we used had high capacity for producing evidence against it, were the claim false. We say for such a claim that it has passed a "severe test".
The main benefit of using severity logic and presentation is that it offers a coherent measure of how well-tested a statistical hypothesis and its corresponding substantive claims are, allowing users to easily avoid both fallacies of rejection and fallacies of accepting the null. Using a severity rationale also breaks the mold of black and white thinking and strict yes/no decisions by naturally allowing uncertainty to be considered in a spectrum.
Severity is also the name of the corresponding measure of error-detection capacity of a test to which a given (statistical) hypothesis had been subjected. Mathematically, severity has a lot in common with p-values and confidence intervals. A formal expression is SEV(Tα, x0, H), which translates to "the severity with which claim H passes test T with outcome x0" and from this follows the formulation in the case of a superiority claim:

The formulation can be easily modified to correspond to an inferiority claim. Severity is, therefore, mathematically equivalent to shifting a composite null hypothesis so that its most extreme point is µ1 and then calculating 1-p for that null hypothesis when we are analyzing a test of superiority. For an inference in the opposite direction SEV = p assuming H0 defined by µ1.
For example, observing SEV(δ > 0.02) = 0.99 means that our testing procedure had very high capacity to produce an outcome less extreme than the one observed if δ ≤ 0.02 was true. Since it did not, the claim δ > 0.02 is warranted.
In another example, consider a test stopped with a non-significant p-value of 0.2 (under H0: δrel ≤ 0%) and an observed relative difference δrel = 2%. Such a high p-value can be mistaken as guidance to accept the null hypothesis H0. However, once we compute SEV(δrel ≤ 0%) = 0.2 we have low severity for the claim δrel ≤ 0% so it is not warranted by the data. With the same data we can compute SEV(δrel ≤ 7%) = 0.982 which is very high, and the claim δrel ≤ 7% is well warranted.
As briefly demonstrated, severity can be assessed for any claim about a parameter of interest. Severity curves (examples in the last chapter) are very helpful if one wants to assess the test's capacities versus a set of possible values at a glance. Due to these qualities, severity is especially useful in preventing misguided interpretations of the outcome of an A/B test by stakeholders, both in cases where the null hypothesis has been rejected, and in cases where we failed to reject it.

1 A deductive argument is said to be valid if and only if it takes a form that makes it impossible for the premises to be true and the conclusion nevertheless to be false. Otherwise, a deductive argument is said to be invalid. -Internet Encyclopedia of Philosophy








Chapter 3
STATISTICAL ASSUMPTIONS. ASSESSING MODEL ADEQUACY
3.1 The importance of statistical assumptions
Statistical tools such as p-values, confidence intervals, and severity were introduced with the intention of assessing the input of data and its error probabilities. While doing so, it was noted that one of the logically valid conclusions that can be drawn from observing a low p-value is that the statistical model is inadequate, hence the calculated nominal p-value is not an actual p-value. What this means in practice is that the mathematical result is uninterpretable, and therefore unusable for statistical inference or decision-making.
When the model does not reflect reality, its outputs are not applicable to said reality. Therefore, understanding model adequacy is crucial, regardless of whether one uses p-values, confidence intervals, or Bayes factors and posterior distributions.
Testing with an inadequate statistical model renders any result inadmissible for guiding action in the real world.
To illustrate the importance of probabilistic assumptions, consider a simple example from physics. The task at hand is to calculate the maximum velocity which can be achieved by jumping from a plane at 30,000 feet. There is an easy formula:  where g is the gravitational acceleration of the Earth and t is the time elapsed. Assuming g to be 9.81 m/s2, we can use this formula and get a result: in 10 seconds one can reach a velocity of 98.1 m/s. However, the question is - is this result of any real use?
If the assumption that the force of gravity is much greater than the air resistance experienced by the falling object holds, the formula will work reasonably well for most practical purposes. However, if it doesn't then its result can be vastly misleading, and thus unusable. For example, if we drop a feather it will certainly not reach 98.1 m/s in 10 seconds. Adding information about the air density at 20,000 feet and below, as well as the drag coefficient of the body we are dropping, will make our results much more reliable. Furthermore, we should take into account that while gravity is ~9.81 m/s2 at sea level, it is somewhat lower at the starting position of 30,000 feet above sea level. Specifying our model in a way which makes it an adequate reflection of reality by accounting for all these factors will improve the practical applicability of our calculation.
While terminal velocity can be calculated using the simpler formula with stricter restrictions, its result is not applicable unless these assumptions hold. Using a more complex formula which more adequately models the actual circumstances will produce a result applicable to those circumstances.
Similarly, in statistics, assumptions about the stochastic process generating the data can be piled on, leading to simpler calculations and/or tests with higher statistical power. However, if these assumptions are made frivolously, and do not hold for the case at hand, our result will be of no practical use.
In fact, I would argue that such results would damage the decision-making process of a business, since they would appear to give credence to conclusions which are in fact unwarranted. Having a statistically significant result or a confidence interval calculated under such conditions will in fact make erroneous conclusions less likely to be challenged further down the road, even if post-experimental data heavily contradicts predicted results. This false aura of certainty can have even worse consequences if there is no culture of retesting hypotheses.
Due to the importance of differentiating between statistical tests performed under valid assumptions and those performed under invalid ones, the terms 'actual p-value' and 'nominal p-value' were introduced in the literature. Similarly, one can talk about nominal versus actual confidence intervals and other statistics. A nominal p-value is one calculated without regard for the underlying statistical assumptions. Nominally, we calculate a p-value, but it does not reflect how surprising a result actually is under the conditions of interest. Just to clarify, an actual p-value is a p-value calculated under a statistical model adequate to the situation at hand.
3.2 Probabilistic assumptions of statistical models
The key probabilistic assumptions of any statistical model fall into three broad categories: Distribution, Dependence, and Homogeneity (Spanos 1999).
Each probability we compute is based on assumptions along those three dimensions. For example, the distribution can be Normal, Poisson, Binomial, Exponential, Beta, Gamma, Cauchy, etc.
In terms of dependence the observations are either independent or dependent on each other. There are different types of independence, e.g. Markov (conditional independence: given the present, future observations are independent of past observations) and dependence, e.g. Martingale (difference dependence). A special type of independence is asymptotic independence, which is achieved when observations become independent, as the distance between them increases to infinity.
Two events, A and B, are independent if the probability of one occurring does not depend on the probability of the other occurring:

Consider two random variables, X and Y, and their density functions. X and Y are said to be independent if their joint density equals the product of their marginal densities:

A result from this is that if X and Y are independent then conditioning on either one does not affect the marginal density of the other. The same logic extends to the n-variable case.
In terms of homogeneity, a model can assume a data-generating process with complete homogeneity (all observations come from identical distributions, ID) or alternatively adopt a non-homogenous one. For example, a homogenous data generating process with a normal distribution is such a process that the underlying distribution has the same mean and standard deviation for each observation. Change in either of the two across observations will result in a heterogeneous process - if the mean is different, it is referred to as 'first moment heterogenous', while if the standard deviation is different it is called 'second order heterogenous'.
All parametric statistical models make assumptions for the distribution, dependence, and homogeneity of the data.
Take the canonical example for a primary key performance indicator in online controlled experiments - conversion rate. Whether it's e-commerce conversion rate, or lead conversion rate of some kind, this is usually calculated as the number of conversion events divided by the number of users over a certain timeframe. If an e-commerce website sees 10,000 users in a given week and 100 of them make a purchase, the purchase conversion rate (CR) is 100/10,000 = 0.01 (1%). Posing a statistical model for this variable under the strictest possible restrictions would mean that the stochastic process it is based on will have the following characteristics:



Example of Probabilistic assumptions



Distribution

Binomial



Dependence

Independent



Homogeneity

Homogenous





Table 3.1: Probabilistic assumptions of statistical models
Modeling the conversion rate of a single group of users is usually not of interest. But modeling the absolute or relative difference between the conversion rates of two or more groups often is. Assuming the A/B testing software used to randomize users and deliver experiences splits users properly, the model for the absolute difference in conversion rate and the model for the relative difference in conversion rate have the same assumptions, since the difference of two binomially-distributed variables is also binomially-distributed. Therefore, the distributional assumption will be the same. Same goes for dependence and homogeneity.
Note that when working with the large sample sizes used in many A/B tests, the distribution can be assumed approximately Normal for most practical cases by invoking the Central Limit Theorem.
3.3 Probabilistic assumptions in different practical cases
Let us examine a few common cases where the usual probabilistic assumptions might be questioned for one reason or another.
Some practitioners have a concern that the distributional assumption of Normality (in the case of a large sample size) does not hold when performing statistical calculations on non-binomial (continuous) metrics, such as revenue per user. However, such a concern fails to take into account that we are not performing a statistical test on revenue per user - we instead test differences in average revenue per user (ARPU). As with any arithmetic mean, ARPU is asymptotically normally distributed, so given a sufficiently large sample size, which we usually have in practice, the Normal distribution assumption holds well for both ARPU and difference in ARPU between the control and variant.
Another practical threat to the distributional assumption is posed by improper randomization of users, and often manifests itself as unequal randomization between the groups of users assigned to the control and variants. This can happen due to various 'creative solutions' for delivering test interventions to end users, or simply due to software bugs. If a large sample size test with planned equal allocation finishes with significant differences in the number of users allocated to one or more variants or the control, it could be a sign of such issues. This is especially true if the difference does not disappear when the number of users increases. If such discrepancies from the expected allocation ratio are noted across several tests then it is almost certain that there is some issue in need of addressing.
Finally, there could be an issue where a model specifies just one user action per user; for example, a single transaction per user in measuring purchase conversion rate per user, but the data actually contains users with multiple orders. The underlying variable is no longer Bernoulli (yes/no, event/non-event), and so the purchase conversion rate cannot be modeled as Binomial which can, by extension, become an issue for the normality assumption.
Concern for the independence assumption is present when users cannot be consistently identified throughout the duration of an experiment. It could be due to issues such as cookie churn, users having more than one account, and others besides. Such issues lead to one person being seen as several users by the testing software. Naturally, the actions of groups of these 'users' will be dependent on the mindset and perceptions of the person behind them, so the independence assumption no longer holds for any user-based metrics. The more such groups of 'users' there are, the worse the departure from independence of observations will be.
The above issue is of greatest magnitude when one tracks users based on cookies (as opposed to logins), and it gets worse the longer that a test continues.
A similar, and usually greater, dependence issue presents itself when one opts to use session-based metrics instead of user-based ones; for example, using session-based conversion rate instead of user-based. The reason is that a user can have multiple sessions, and behavior in future sessions will depend on behavior in past ones, invalidating the assumption of independence of observations.
Working with dependent data presents a difficulty for the modeler. The first step in tackling it would be to plot the empirical distribution and compare it to a normal one, with special attention to the tail areas of both distributions. If there is no difference of practical significance, then the normal distribution can be used for convenience. If, however, the difference is significant, one may need to use the empirical distribution (probably with kernel smoothing and other techniques) for calculation of p-values and confidence intervals. This presents a difficulty when planning a test since the power of the test can only be approximately estimated.
Concerns for the homogeneity of the data generating mechanism can be raised, especially when the data exhibits trends, trend reversals, shifts, or other kinds of non-stationarity in the course of the experiment. This would be a valid concern when performing some kind of time series analysis, but for as long as the primary variable of interest is the difference (relative or absolute) between two averages, there should be no concern about the homogeneity assumption, at least for fixed sample size tests.
The same likely holds for tests which evaluate data in a sequential manner, but verifying this through simulations remains future work for the author.
3.4 Assumptions imposed by the design of the experiment
On top of the purely probabilistic assumptions, there are further requirements for the calculation and interpretation of statistical measures which are imposed by the very design of the online controlled experiment. Due to the logic of statistical inference, all aspects of the data generating mechanism need to be considered, not just the sum of users and user actions. Actions of the experimenter are of great interest as well.
A prominent example for such an assumption is the assumption in a fixed sample test that the data is only assessed once, at a predetermined moment of time, or when a predetermined number of users had been exposed to the test. This is the most basic and simple scenario with great practical utility, despite some obvious drawbacks. In such a statistical test, the sample size: number of users, sessions, emails, etc., is fixed at the beginning, and a statistical calculation is performed only when it is reached. Looking at the data with intent to stop the test if some event occurs... if the variant is performing 10% worse than the control, or if the result is nominally statistically significant, then this violates this assumption.
If data is examined at multiple points in time during the experiment, then the probability of observing an extreme value should take this data into account, making a fixed sample test no longer applicable due to violated assumptions. The nominal p-value it will result in, will significantly overestimate how unexpected the result is under the null hypothesis. Computing the actual p-value would requires us to include the data evaluations at different times and the decision rule used to potentially stop the test into the statistical model.
Other assumptions are related to the number of tested variants, whether any variants were added or removed mid-experiment, whether the proportion of users being assigned to each experience changed during the duration of the experiment, and so on. Not taking these into account can result in statistical estimates which vastly over or underestimate the significance of the outcomes.
Conducting experiments which properly take these design decisions into account is covered in the chapters ahead.
3.5 Assessing statistical adequacy through statistical tests
There are a number of statistical tests developed for checking statistical assumptions of all three categories. All of them result in p-value measures, which help us detect notable departures from our assumptions vis-à-vis the data at hand.
Such tests are Fisherian by nature. Unlike Neyman-Pearson type tests, which are performed within a defined statistical model, Fisherian ones test outside the statistical model (Spanos 1999). This means that while such tests can refute the null hypothesis, they don't lead to the acceptance of a specific alternative hypothesis.
For example, if a test for normality rejects the null hypothesis, it does not lead us to adopt another underlying distribution - it only allows us to say the adequate distribution is not Normal. Even if the test is known to be most powerful against a certain set of alternative distributions, it is not warranted to conclude that one of them is the distribution of the data. Similarly, if a test for independence rejects the null hypothesis of independence of observations, it does not lead us to the conclusion that a specific type of dependence is present.
Tests for Normality which can be used include the Shapiro-Francia test (Shapiro and Wilk 1965) (Shapiro and Francia 1972), the Cramer-von Mises test (Cramer 1928) (von Mises 1931), the Anderson-Darling test (Anderson and Darling 1954), as well as more exotic ones such as the d'Agostino-Pearson (D'Agostino and Pearson 1973), and Jarque & Bera tests (Jarque and Bera 1987).
Different normality tests have greater sensitivity towards different types of departures from normality, therefore, it is recommended to use them as a battery. For example, the Anderson-Darling test shows greatest sensitivity when the departure is towards a Laplace or Uniform distribution, while the Shapiro-Francia test is perhaps the most well-rounded among them (Mbah and Paothong 2014), as it provides significant statistical power versus the widest range of departures from normality, compared to other tests. The Shapiro-Francia test can also be calculated relatively easily for n > 50 using the Royston method (Royston 1993).
One would think that the Shapiro-Francia test should, therefore, be preferred in most situations. However, it should be noted that the test may result in a low p-value even if the underlying distribution is normal due to its assumptions of independence and homogeneity. This leads to the need to perform separate tests for independence and heterogeneity, in order to check if departures there may have led to the low p-value from the Shapiro-Francia test.
A significant drawback of statistical tests of normality is their relatively low power. Interpreting a high p-value from such a test as evidence that the distribution can be safely treated as normal will likely lead to a high false-negative rate, especially if the tests are performed with small sample sizes or the departures from normality which matter are relatively small. Unfortunately, such an unwarranted conclusion is often drawn even by some professional statisticians.
3.6 Assessing statistical adequacy through A/A tests
Another tool assisting the practitioner in checking for statistical adequacy is the A/A test, specifically performing multiple A/A tests with live data. An A/A test is a test in which there is no actual change between the variant and the control. Since this is the default null hypothesis, the percentage of such tests which are expected to result in a statistically significant outcome at any significance level is known. Similarly, it is known what proportion of confidence intervals should cover the null hypothesis.
Thanks to the finite-sample properties of frequentist estimators we know this for any sample size. The exact proportion of times a test should result in a p-value of less than 0.05 is 0.05 or 5%, regardless of the sample size of the A/A test. Running several hundred A/A tests (which can be run simultaneously), and comparing the expected versus the observed proportion of p-values under a specified significance threshold is a good way to test for the probabilistic assumptions.
Furthermore, a series of A/A test should tend to produce a uniform distribution of p-values, the larger the number of such tests is, as shown in Figure 3.1.


Figure 3.1: Distribution of p-values under a true null hypothesis.

A series of A/A tests using an adequate statistical model should tend to produce a uniform distribution of p-values.
Similar to statistical tests for model adequacy, however, discovering an issue with the assumptions through a series of A/A tests does not necessarily lead to pinpointing the underlying issue. For example, a discovery that p-values under 0.05 are produced 20% of the time might be due to a problem with the distributional assumption, or instead due to a problem with the independence assumption. Each of these can have different specific causes as well - the actual underlying distribution can be any distribution other than the postulated, while dependence may be caused by tracking persistency issues, randomizer bias, or another technical problem.
An example of the latter would be a system for front-end testing which delivers the control as default, and then repaints the webpage with one of the tested variants. Such flicker will be present for the variants, but not for the control, biasing the results in favor of the control. Another example would be a system which, upon failing to display a test variant in a certain short timeframe, defaults to the control. This would bias the results against the control, since users who are reassigned in this manner are more likely to be on slow connections or using cheaper machines, which usually means they are less likely to convert. Ironically, such a solution might be implemented to deal with flicker issues...







Chapter 4
STATISTICAL POWER AND SAMPLE SIZE CALCULATIONS
So far, this discussion has covered only tools which help us manage the error of the first kind: rejecting a null hypothesis when it is in fact true. This included the major methods used to assess and manage this type of error rate: p-values and confidence intervals. We also talked about how one of the assumptions behind many basic statistical models is that the sample size is fixed in advance - we adhere to it by only calculating a p-value or a confidence interval once this sample size is reached.
This chapter will cover sample size computations, and what sample size can tell us about the other important type of error we can commit in decision-making - the error of the second kind.
4.1 Errors of the second kind (type II errors)
In any kind of a null hypothesis statistical test there are 4 distinct possible outcomes depending on the null hypothesis being true or false. There are 2 outcomes in which we correctly reject or fail to reject the null, and 2 outcomes in which we do so in error. Table 4.1 below presents all possible outcomes:



Types of judgements following a null hypothesis statistical test



Judgement of Null Hypothesis

Null Hypothesis (H0) is Valid/True

Null Hypothesis (H0) is Invalid/False



Reject

Type I Error (false positive)

Correct Inference (true positive)



Fail to Reject

Correct Inference (true negative)

Type II Error (false negative)




Table 4.1: Judgements following a null hypothesis statistical test

Assume a textbook A/B test trying to rule out the possibility that the variant is worse than or equal to the control. In our case, the control is, in fact, worse than a tested variant. A statistical test which ends up producing a statistically non-significant outcome at the significance threshold specified for it would lead to a type II error. In such a case there is no possibility of a type I error. However, if the control was, in fact, better than the variant then a statistically significant outcome would lead to a type I error.
The type II error rate is defined as the probability of observing a p-value which is not statistically significant when a true effect of a certain magnitude is, in fact, present. This error rate is denoted by the Greek lower-case letter beta: β.
An oft-used shorthand definition is to state that an error of the second kind is to fail to reject a false null hypothesis and thus that β is the probability of failing to reject a false null hypothesis. However, this formulation falls short since β can only be interpreted and calculated for a true effect of a specified size and in relation to a given test of significance.
This becomes clear once the proper notation is written down:

for any μ1 greater than μ0, where c(α) is the significance threshold, d(X) is a test statistic (distance function), μ is the true magnitude of the effect while μ1 is the magnitude of the effect under a particular alternative hypothesis H1). T(α) denotes a statistical test with a critical region which corresponds to a bound on the type I error rate equal to α.
For example, d(X) can be a Z-score function and if α = 0.05 with a one-sided null hypothesis then c(α) will be 1.6448. μ1 can be expressed in terms of standard deviations from the null, e.g. 0.5σ or in the original unit, e.g. 0.2 percentage points (p.p.), as relative difference (e.g. 0.01) or as percentage difference (e.g. 5%).
The type II error rate of a statistical test is the probability of observing a non-significant p-value at a certain threshold α if a true effect of a certain magnitude µ1 is in fact present.
That beta is a function of a significance test with a certain significance threshold and effect size of interest is especially important when performing risk-reward analysis. Such analysis takes into account that true differences of greater magnitude are associated with a decreased type II error rate.
Given a fixed sample size and significance threshold, the type II error (β) decreases with an increase in the magnitude of the minimum effect size of interest (MEI). Beta is inversely related to sample size - the larger the sample size of a test, the smaller the probability of a type II error for all possible true alternative hypotheses. It is also inversely related to the type I error - the type II error will decrease if the type I error is allowed to increase while the sample size and MEI remain the same.
Considering the number of variants we intend to test - a topic we will discuss in subsequent chapters, we can arrive at the relationship between these parameters as shown on Table 4.2.
The type II error is inversely related to all of the test parameters - when one increases, beta decreases, and vice versa, all else remaining fixed.
This essentially means that there is always a trade-off between the probability of mistakes of the first kind, versus mistakes of the second; between testing with more users, and increased risk of false negatives, etc. Deciding what trade-off is acceptable is the job of the A/B testing specialist, but it must take into consideration information external to the particular A/B test at hand.



Relationship between key test parameters and type II error



Test Parameter

Change Direction

Effect on β (all else fixed)



Minimum Effect of Interest

⇧

⇩



⇩

⇧



Type I Error (α)

⇧

⇩



⇩

⇧



Sample Size

⇧

⇩



⇩

⇧



Number of Variants

⇧

⇩



⇩

⇧




Table 4.2: Relationship of type II error and other test parameters

The topic of arriving at optimal significance thresholds and sample sizes is discussed in Chapter 11.
4.2 Statistical power
Understanding errors of the second kind, we can introduce the concept of statistical power. The power of a statistical test (statistical power, POW), is defined as the probability of observing a statistically significant outcome if a true effect of a certain magnitude exists. It is the probability of rejecting a false null hypothesis with a test of size α, given that a particular alternative hypothesis is true.
From this definition it is obvious that it is the inverse of the type II error rate: POW = 1 - β. Since

and POW = 1 - β, it leads to:

for any μ1 greater than μ0, where c(α) is the significance threshold, d(X) is a test statistic (distance function), μ is the true magnitude of the effect, while μ1 is the magnitude of the effect under a particular alternative hypothesis H1). T(α) denotes a statistical test with a critical region corresponding to a bound on the type I error rate equal to α.
The statistical power of a statistical test is defined as the probability of observing a p-value statistically significant at a certain threshold α if a true effect of a certain magnitude µ1 is in fact present. It is the inverse of its type II error rate.
This can be asserted since statistical power is a function of the same parameters as beta, but in the inverse, it is natural that statistical power would be positively related to all of the test parameters to which the type II error was negatively related. Therefore, increasing any of them results in an increased power, all else remaining fixed.



Relationships between key test parameters and statistical power



Test Parameter

Change Direction

Effect on Power (all else fixed)



Minimum Effect of Interest

⇧

⇧



⇩

⇩



Type I Error (α)

⇧

⇧



⇩

⇩



Sample Size

⇧

⇧



⇩

⇩



Number of Variants

⇧

⇧



⇩

⇩




Table 4.3: Relationship between statistical power and other test parameters.

In planning an A/B test, decreasing the uncertainty level by reducing the type I error rate we would accept will reduce the statistical power of the test, unless other test parameters are changed. If detecting smaller improvements is of no interest so the minimum effect of interest is increased, then the test will have greater power. If the number of users included in a test is increased, by either testing for a longer duration or by including more user segments, then the power of the test will increase, all else being the same.
It should be noted that for μ1 = 0 and a simple hypothesis (one or two-sided) power is always equal to exactly alpha: POW = α, so if the bound on the type I error rate is 0.05, then the probability of the test resulting in a statistically significant result is POW(T(α); 0) = 0.05 if the true effect size is 0.
The formula for computing the power function of a test of absolute difference between two proportions is:

Φ is the cumulative distribution function, σpooled is the estimated pooled standard deviation of the two arms, accounting for a possible difference in allocation. For difference in proportions σpooled is calculated using the formula:

where p1 is the proportion under the null hypothesis while p2 is a proportion corresponding to a hypothetical true value. r is the allocation ratio between the control and variant groups (n1/n2). With equal allocations, which is usually the most efficient and easiest to implement design, r is always equal to 1 so it can be ignored.
Figure 4.1 features a plot of the power function for a range of possible true effect sizes for alpha equal to 0.05 and a fixed sample size:


Figure 4.1: A power function and some key points on it.

The test was planned with a simple one-sided null  and power of 0.9 (or 90%, type II error of 0.1 or 10%) against a minimum effect of interest of 8.22% relative difference (lift). This is often referred to as target power. Some find it useful to contrast this with actual power, or exact power, which is the power level at the actual true effect size. However, since the actual effect size and direction are unknown, the concept of actual power is non-operational except in simulations.
Note how power quickly becomes almost zero once δ goes slightly into the parameter space of the one-sided null hypothesis. It reflects the fact that, in a one-sided test, the type I error is a conservative bound on the error rate, so actual error rate might be even smaller than nominal, if the null is in fact true, and µ is not at the boundary of the null space.
Power increases and becomes virtually 1.0 (100%) for values deeper in the parameter space of the alternative hypothesis. This is true for both one-sided and two-sided alternative hypotheses, since a two-sided test is simply two one-sided tests combined into one:


Figure 4.2: Power of two-sided test explained as the combined power of two complementary one-sided tests.

Therefore, power is lowest against the point null: exactly α, in this case 0.1, and it increases in both directions as the magnitude of the actual difference from the null increases.
Statistical power is of great interest when planning an experiment, but of little interest after it has been completed. This stems from the fact that power calculations do not use any of the data produced by the experiment. They are entirely hypothetical, using a pretest estimate for the performance of the control group and the variance of the data, as well as hypothetical possible values for the performance of the treatment group. It therefore makes little sense to perform power calculations after the fact (calculating post-hoc, a.k.a. obtained power) in the usual course of online A/B testing. Such calculations are useful only in critiquing the design of a test and, importantly, any such critique cannot use data obtained from the test. Post-hoc power calculations are not useful in discussing test results, as shall be demonstrated in more detail below.
4.3 The role of statistical power in A/B testing
Statistical power should be a consideration for any A/B test at the stage where we design the experiment, as it determines the probability of the procedure to reject a false null hypothesis in favor of an alternative of interest.
One can think of statistical power as being similar to the sensitivity of a radar - if we are monitoring a given airspace, and care very much about finding a drone flying in it, we want sensitivity to be as high as possible. If we are only interested in detecting big passenger planes, then less sensitive equipment will be just as adequate. Getting the super-sensitive equipment would be an over-investment; in an A/B test we would be waiting for too long and have to experiment on too many users so the risks will not justify the potential gains.
Another way to think of statistical power is by considering the saying that "absence of evidence is not evidence of absence" which, contrary to popular intuitions, is exactly as true as saying that "the presence of evidence is not evidence of presence". If an A/B test has very high power, meaning very high sensitivity towards a specified effect size, then not finding evidence is in fact evidence of absence exactly to the extent that the test had probability of detecting evidence for such an effect, if it exists. Similarly, the presence of evidence is only evidence for the presence of some effect to the extent to which said data would not have been produced if the effect was absent.
Designing a test to have high power towards any minimum effect of interest means that it will result in narrower confidence intervals, regardless of the true value of the parameter. A test with infinite power would result in collapsing the interval to a point on the real line.
Like p-values, statistical power is a characteristic of the testing procedure relative to a certain hypothesis of interest. Power, however, is of no use after the data has been gathered, for the simple fact that power calculations do not make use of the actual test data. Power calculations are strictly hypothetical and are informed by the minimum effect that we would be excited to detect, and a particular null hypothesis we wish to potentially disprove with a specified level of uncertainty.
Therefore, the computation of 'observed power', 'attained power', or 'power to detect the observed effect' makes no sense in most situations. In a scenario in which a statistically significant outcome was observed, we know in advance that the test was powerful enough. Calculating power adds nothing to our knowledge. Returning to the radar metaphor, it is akin to having a radar detect an object, and then calculating whether it had sufficient sensitivity in order to detect it.
Statistical power is a pretest calculation based on hypothetical values and, therefore, has no use after the data has been gathered in the regular course of analyzing the outcomes of A/B tests.
Even if the result was not statistically significant, and we wanted to estimate how good of an evidence for lack of effect the result is, a power calculation with µ1 = µobserved is still useless and would be a sign of mistaking the minimum effect of interest with an estimated effect from the data at hand. While nothing forbids us from computing such power, it adds nothing that is not already contained in the p-value, confidence interval, or severity value attained.
The radar metaphor cannot be stretched to encompass this scenario, though, as radar sensitivity is not probabilistic the same way power is. There is an even better way to illustrate that there is no need to calculate post-hoc power - we always know what the power would be, based on the p-value. For example, if the result is just below significant at whatever significance threshold was chosen, the test will always have an observed power of just below 0.5 (50%) for a one-sided null. An easy way to prove this is equivalent to reevaluating the test by redefining the null hypothesis as limited on one side by the observed value - the result will be that we will always get a p-value of ~0.5 in the case of a one-sided null. Since at the null POW = 1 - α, POW for µobserved would always be 0.5 when the result is almost significant, and higher if the p-value is lower.
When a test fails to reject the null hypothesis, and we want to know how strong the evidence provided by the data can be considered against particular discrepancies in both the null hypothesis and alternative hypothesis space, it is best to use confidence intervals, or calculate a p-value for the null hypothesis of interest. Another approach would be to plot a severity curve against all values. All these tools provide an answer to the question "how well do the data support a particular inference?" by using the data from the experiment, unlike statistical power.
4.4 Minimum effect of interest vs. minimum detectable effect
The term 'minimum detectable effect' causes much confusion among testing practitioners, researchers, and even statisticians from many fields. The keen reader should have noticed that throughout this chapter the term 'minimum effect of interest' (MEI) was used in place of the standard term 'minimum detectable effect' (MDE). The reason for introducing this departure from standard lingo is that the term 'minimum detectable effect' is often misinterpreted (or probably correctly interpreted, from the point of linguistics) as meaning that a test will not be able to detect a true difference smaller than the MDE µ1.
Practitioners sometimes think that a test in which the true value is less than the value of the MDE will never produce a statistically significant p-value. We already know that this is incorrect, since this would mean that a test procedure has a type I error rate of 0, which is impossible. For as long as type I errors are possible there will be true values under the null hypothesis which have probability of producing a statistically significant p-value greater than zero. Since the null hypothesis parameter space is by definition outside the alternative hypothesis parameter space, it is possible to get a statistically significant result even if the true difference is smaller than the MDE.
This reasoning can also be formulated in terms of the observed difference - thinking that an observed difference smaller than the MDE cannot produce a statistically significant p-value. It is a mistake, and it betrays failure to differentiate between the observed effect size and the true effect size. It can be demonstrated to be a mistake by running a set of A/A tests with a large sample size, and then setting the MDE at any value higher than the lowest observed effect size.
A more trivial way to demonstrate the error in mixing MDE with observed outcomes is to: #1 compute the sample size required for a test with parameters α, β, and a given MDE; #2 pretend the test has been ran at the target sample size, the baseline is exactly the one used for sample size calculations, and the observed effect is equal to baseline plus MDE; #3 enter these variable in a significance calculator and observe the significance level; #4 change the observed effect to be lower than MDE and see at which point it stops being significant at level α. This point will be much lower than MDE.
A way to address the above issues could be to specify the term further by saying 'minimum reliably detectable effect' (MRDE), which more correctly reflects the substance of what the term means. However, this is also questionable since if a test only has 60% power against an alternative of interest, can it 'reliably' detect it? Most people would say no. Some would say 'no' to this question even if we replace 60% with 80%, 90% or even 95%+. What is 'reliable' is context-dependent and consists primarily of considerations external to the A/B test and its statistical design. Still, the term is acceptable if what 'reliable' is has been explicitly specified. I do use the term occasionally, but only where this is the case.
My proposal to address the above issues is to introduce a new term which makes confusion very unlikely while at the same time reflecting what MDE currently stands for - the minimum effect size we would be happy to detect with a certain probability. The term I use is 'minimum effect of interest', or MEI. I believe that this phrase makes it hard for one to think that we cannot detect effects smaller than the MEI, even though it is at the cost of not referring to detectability.


Figure 4.3: Relationship between minimum detectable effect (MDE), minimum effect of interest (MEI) / minimum reliably detectable effect (at XX% power, MRDE) and the range of true values which can produce a statistically significant result.

Even more importantly, the 'minimum effect of interest' term stresses on the 'interest' of the user, which is subjective and ultimately determined by external considerations, in the same way as the power level. This hints at the fact that MEI/MDE at a certain power level is a hypothetical quantity, determined not by data or procedural necessities, but by the A/B testing the practitioner by way of tweaking any of the other test parameters, and usually the test's sample size / duration.
Using the terms MEI or MRDE means the term MDE can now be used for its true meaning - the minimum true effect which can result in a statistically significant outcome of a test, as shown in Figure 4.3.
4.5 Underpowered and overpowered tests
Understanding statistical power, MEI, MRDE, and MDE allows us to examine two suboptimal scenarios in planning A/B tests: underpowered and overpowered tests.
Underpowered tests are ones in which the sample size does not provide large enough statistical power to detect a certain minimum effect of interest. This can be due to a simple lack of calculation; for example, by making a decision to run the test for x weeks, regardless of the sample size achievable in that timeframe. More often, however, it seems to be the result of a poor choice of minimum effect of interest due to reversing the logic of said choice: first, the duration of the test is decided upon, the available sample size is calculated, and only then is the MEI chosen based on the result of the first two steps. However, this is a very poor way of deciding on a value for the minimum effect of interest, since the only interest it takes into account is how long we desire the test to run for.
While sometimes there are good external reasons to fix the duration of the test based on a deadline of sorts, in other times the above process leads to MEIs which are not minimum at all - effects of much smaller sizes would be exciting from the perspective of the business bottom-line. In some cases, the MEI decided upon is so large that it defies the realities dictated by what is being tested: it is near impossible for the intervention to result in an effect as big as the MEI, leading to power of 30%, 40% etc., against realistically achievable effect sizes.
The cost of running underpowered tests cannot be underestimated. One company reportedly stopped A/B testing after seeing only 3 out of 70 tests produce statistically significant outcomes:
At <company>, we recently stopped actively A/B testing [...] we ran more than 70 conversion tests before making this decision and had only 3 significant winners.
While details about the power of the tests were not disclosed, and company representatives did not reply to attempts to acquire this information, I consider it more likely that most of their tests were powered for unrealistically high MEIs than the possibility that all, or almost all, of their tested variants were not improving upon their baseline user experience.
This informed guess is supported by a meta-analysis (Georgiev, Analysis of 115 A/B Tests: Average Lift is 4%, Most Lack Statistical Power 2018) performed by the author on an open database of 115 A/B tests. One of the results of that meta-analysis is that roughly 70% of tests appear to have been underpowered relative to expected treatment effects and considering business logic. The average minimum detectable percentage change at 90% power was 27.8% while the median was 20.84%. Given that most of the tests were for trivial changes to button colors, wording, and minor positioning changes, such large minimum effects of interest are unreasonable, both in terms of expected effect size and in terms of business effect.
Going back to the company which mostly abandoned testing after those initial 70 tests... it should not have stopped testing in any case. Even if their tests were properly powered, observing so many negative outcomes means that they were prevented from implementing 70 changes, many of which would have hurt, or at least would not have improved their website. Therefore, either the tests were underpowered, meaning they needed to fix their testing approach, or the risk-management aspect of A/B testing was working as expected and prevented business harm. Whatever the case, abandoning testing was likely a poor choice.
Overpowered tests are ones in which the sample size was fixed at such a value, so the test has very high power against true effect sizes which are not interesting for the business bottom line. Running such tests might even be detrimental. For example, if it costs $100,000 to implement and maintain a UX intervention, and the statistical design provides 95% power against a true difference, which will only result in $25,000 increase in revenue and no side benefits, then such a test is most likely severely overpowered.
Overpowered tests are encountered much less often, compared to underpowered tests, according to the meta-analysis cited above.
Since input parameters related to power calculation might be somewhat arbitrary, I recommend performing a risk-reward analysis to determine the appropriate sample size and, therefore power and minimum effect of interest. This topic is discussed in Chapter 11.
4.6 Sample size calculations
Sample size calculations are crucial for any well-planned A/B test. When we know what risk of type I errors and type II errors can be tolerated, as well as the minimum effect which would be of practical interest, the sample size required for an online controlled experiment to achieve these characteristics can be calculated.
The minimum effect of interest is usually an absolute or relative change versus a particular baseline value. When performing sample size calculations, the baseline has to be estimated from historical data.
One way to go about this is to simply take the past couple of weeks or months - assuming that they were not subject to any extreme events or seasonality effects. If they were, and the nature of the effects is known, it can be appropriate to remove their influence on the data before using it, instead of using older data. Using data for up to a year back can be justified in certain cases, but usually the further back we go, the less connected the baseline is to current performance, and the worse our prediction for baseline during the duration of the test will be.
A more advanced way of estimating the baseline can include extrapolation or prediction for the baseline rate based on a past period of suitable length, while accounting for expected seasonality effects. Certain time series analysis methods such as ARIMA forecasting seem especially well-suited.
Deciding on the type I and type II error levels, as well as on the minimum effect size of interest for each particular test, is not a trivial task. What complicates the process is that the choice of significance threshold, power, and MEI require solving a feedback loop where changing one alters all others. Whereas scientists of various disciplines can use 'conventional' values, or threshold set by relevant journals without anyone questioning their choice, in A/B testing we are in the unique position of being able to determine these values based on highly accurate risk/reward calculations. This process is discussed in detail in Chapter 11, but for the purposes of this subchapter we can assume that a sound choice of these parameters was arrived at in some manner.
Once we have settled on these parameters, the next task is to calculate what number of users, sessions, etc. we need to include in a test, in order to match these requirements. Here we will only consider fixed sample size tests, and only in the case of a simple A vs. B test. Calculations for A/B/n tests, tests with multiple monitoring points, multiple parameters of interest, etc., will be discussed in their respective chapters.
Sample size calculations require three parameters to be set: the type I error rate, the type II error rate (or power), and the minimum effect of interest.
For an A/B test of absolute difference between a control and treatment arm (arm A, arm B), the following formula gives the sample size for a test with a type I error rate of α, type II error rate of β, and an absolute difference of interest (MEI) of δ = µ1 - µ0.

Z1-α is the Z-score corresponding to the selected significance threshold of a one-sided test and, therefore, Z1-α/2 would need to be used if the test is of a two-sided alternative. Z1-β is the Z-score corresponding to the selected statistical power. Z-scores are computed in the usual way through the inverse cumulative distribution function (a.k.a. quantile function). E.g. Z1-α would be 1.6448 for a one-sided test with a significance threshold of 0.05 and it would be 1.9599 for a two-sided test with a significance threshold of 0.05. With β = 0.2, Z1-β is 0.8416, with β = 0.1, Z1-β is 1.2816, and so on.
σpooled is the estimated pooled standard deviation of the two arms, accounting for possible differences in allocation proportions. For difference in proportions σpooled is calculated using the formula:

where p1 and p2 are the proportions in the two groups, and r is the allocation ratio between the control and variant groups (n1/n2). With equal allocations, which is usually the most efficient and easiest to implement design, r is always equal to 1 so it can be ignored.
For continuous data the variance can be estimated from relevant past data and multiplied by 2 to get an estimate of σpooled.
Sample size calculations require that the variance of the data is known or reliably estimated.
To get the total sample size of an A/B test, add n1 divided by r to n1.

For an A/B test with equal allocation this is equivalent to multiplying n1 by 2: ntotal = n1 · 2.
Note that this formula is not applicable if there is more than one treatment arm. A/B/n tests require a much more complicated sample size computation.
Limiting ourselves to the case of a simple A/B test, the formula above illustrates what we already know about the relationship between power, significance threshold, and the minimum effect of interest. Since MEI is in the denominator, increasing it decreases the required sample size, all else being equal. Since type I error, type II error, and the variance are in the numerator, increasing any of them increases the required sample size to maintain the other variables at fixed levels.
Table 4.4 presents these relationships:



Relationships between key test parameters and sample size



Test Parameter

Change Direction

Effect on sample size



Minimum Effect of Interest

⇧

⇩



⇩

⇧



Type I Error (α)

⇧

⇧



⇩

⇩



Type II Error (β)

⇧

⇧



⇩

⇩



Variance

⇧

⇧



⇩

⇩



Number of Variants

⇧

⇧



⇩

⇩





Table 4.4: Relationship between sample size and other test parameters.
Variance is not actually a test parameter, rather it is a characteristic of the data. It has been added to the table to explicitly point out that less-variable data requires a smaller sample size to estimate and draw conclusions from. Since standard deviation is simply the square root of the variance, the relationships between variance and sample size, and standard deviation and sample size, are the same.
'Number of variants' was added to the table for completeness. Sample size calculations for multiple variants will be covered in Chapter 6.
The following graphs and tables should help visualize the connections between these parameters and sample size:



Sample size required for different significance levels



Significance Level

Sample Size Per Variant Required for 80% Power



85%

141,645



90%

181,032



95%

248,286



98%

366,660



99%

403,037




Table 4.5: Sample size required for different confidence levels.




Sample size required for different MEI



Minimum Effect of Interest

Sample Size per Variant



1%

2,360,495



2%

592,902



5%

96,195



10%

24,601



20%

6,424




Table 4.6: Sample size required for different sized MEI.

Note that, while the MEI is expressed as percentage difference for easier comprehension, the calculations in these graphs are for detecting absolute difference. Baseline CR and α are fixed for all MEI levels.




Figures 4.4, 4.5, 4.6: Sample size for different effect sizes, MEI at different baseline conversion rates, sample size at different baseline conversion rates.

On all three graphs the non-linear relationship between the parameters and sample size can be observed. It gets more and more difficult to detect a smaller effect size, as at effect size of zero the sample size approaches infinity. The baseline conversion rate is, in essence, a proxy for the data variance, since the lower it is, the higher the noise to signal ratio becomes, which increases variance, and, therefore, the required sample size.
Looking at these relationships, one can be tempted to consider ways to reduce the variance of the data in order to achieve either a lower significance threshold, or higher power, or both. This is impossible, though, since variance is a property of the data and the data is what it is.
However, there is one avenue that I would suggest as being at least worth pursuing - changing the parameter of interest from conversion to the last step of a multi-step process to conversion to the next step only. Take as an example a typical e-commerce website in which the shopping funnel can be broken down as follows:


Figure 4.7: A typical shopping funnel for an e-commerce store.

The inverse pyramid shape signifies the reduction in the number of users available at each stage. Running a test on any of these steps with the same primary KPI - purchase conversion rate - would result in decreased variance as we go further down, but the decreased number of users available per time period means that a test with the same parameters will take the same time, regardless of which stage we are testing at.
However, if the assumption can be made that an improvement in the conversion rate from one step to the next, say, from the 'viewed product' stage to the 'added to cart' stage, transfers almost uniformly to an improvement in the purchase rate, then the required sample size for the same test parameters would be reduced. Equivalently, one can run the test with the same sample size and enjoy a greater power and detect smaller differences reliably. Here are calculations for one realistic scenario:
If that assumption is acceptable, we can switch to testing conversion rate to the next step, instead of purchase conversion rate, and either greatly speed up our tests, or be able to detect much smaller effect sizes with the same test duration. This assumption would not be warranted if the nature of the treatment we are introducing incentivizes behavior which does not translate to increased purchase conversion rate.


Figure 4.8: MRDE for outcomes measured as conversion rates to next step.

Using next step conversion rate is natural in many situations when the tested treatment affects only a certain step, and is not a wholesome approach aimed at increasing purchase conversion rate. Practitioners should still measure the purchase conversion rate as a second primary metric or as a secondary outcome, which might be evaluated with a non-inferiority test to make sure the treatment is not harmful.
4.7 False positive and false negative rates
Both this and previous chapters discussed two types of errors and the bounds on the probability of committing them - α and β, where alpha corresponds to the probability (or a bound on it in the case of a one-sided test) of rejecting a true null hypothesis, and beta corresponds to the probability of failing to reject a false null hypothesis with a test of specified size in favor of a particular true alternative hypothesis.
These concepts, however, are not to be confused with a different concept - the false positive rate or its inverse: the false negative rate, of an experiment program or any series of A/B tests.
Two extreme examples should quickly illustrate why conflating the two can lead to major confusion. In the first, run 100 tests, each with a significance threshold of 0.05 and in all of which the intervention is in fact better than the control. While the type I error rate is 0.05, by equating type I error rate with the false positive rate of an experiment program, we would expect 5 false positives in a group of 100 (1-10 with ~99% probability). However, we would in fact observe 0 false positives so our false positive rate will be 0.00.
In the second, run 100 tests, each with power 0.9 (90%) against a relevant minimum effect of interest, but for all of them the intervention is in fact worse than the control. Even though the type II error rate is 0.1 (1 - 0.9) and by equating this rate with the false negative rate of an experiment program one would expect about 10 false negatives (3-17 with 99% probability), we would in fact have 0 false negatives since all of the test treatments are in fact truly worse than the existing user experience. The false negative rate will be zero.
The reality will, of course, be somewhere between these two extremes, since virtually no testing program includes only true improvements, or only true failures. Since no one knows how many treatments are actually better or worse than the control, by definition, the true false positive rate and the true false negative rate cannot be computed. If we could do so, then we would simply not run these tests, as it would be counter-productive to do so when we already know the answers that these tests can guide us towards.
Therefore, it is important that even if, for some strange reason, we run all our tests at the same significance threshold and power, we are not mistaking these with the rate of false positives or false negatives in an A/B testing program.
4.8 Misunderstandings about statistical power
Staying true to the book's promise, let us examine frequently encountered misconceptions about statistical power so we can then easily avoid them.
In my opinion, the most common misunderstanding is forgetting that power is a function calculated for a specified effect of interest. Communicating the power of a particular statistical design, and forgetting to specify what size of effect it was calculated for, is a common mistake. I believe this to be mostly due to power being communicated as a single number, and not as function values over a set of possible effect sizes. Part of the issue may be also due to 'sensitivity', a synonym for 'power', being used in contexts of medical / biological / epidemiological testing to mean the average proportion of false negatives of a field or laboratory test, which is quite different from statistical power.
I believe the best way to minimize the chance for this mistake to occur is to present the power graph over a large range of the possible values, with the agreed upon minimum effect of interest singled out in some way, as opposed to communicating merely a single number.
Another set of common issues occurs due to difficulties in the interpretation of statistical power after a test has been completed. Some practitioners can try to calculate power against the observed effect, or against any effect size of interest, regardless of the outcome. In some instances, if the power is below a given threshold, they would claim a statistically significant result is not valid, as the test was not powerful enough to detect such a discrepancy. In others, they would try to use a high statistical power as evidence that the test provides good evidence for accepting the null hypothesis.
The first is an obvious contradiction - if the result is statistically significant then the test was powerful enough. There is no need to compute power in this case.
However, there is no need to compute statistical power in the case of a non-significant outcome either. Since statistical power calculations do not use the data observed during the test, there is no input from the data in their result. If we are trying to argue for the null hypothesis, usually to a given margin around it, we can use confidence intervals or severity instead, as they incorporate the data from the A/B test.
In short, the only case in which I can see a need to compute power post-hoc is if we are trying to argue that the test was poorly designed in the first place, that it didn't make sense either from a risk/reward standpoint, or from a standpoint of being adequately powered for a range of possible outcomes. For example, the present author has made the case (Georgiev, Analysis of 115 A/B Tests: Average Lift is 4%, Most Lack Statistical Power 2018) that a majority of the A/B tests part of the public GoodUI.org database seem to have been poorly designed based on a combination of post-hoc power calculations and the observed confidence intervals.
The reader will get an even deeper understanding of the role of statistical power from Chapter 11, subchapter "Calculating risk/reward ratios and key points".







Chapter 5
TYPES OF STATISTICAL HYPOTHESES
One of the biggest differences between much of textbook statistics and applied statistics seems to be how statistical hypotheses are defined. As there seems to be a significant disconnect between theory in practice, this whole chapter is dedicated to exploring different types of statistical hypotheses the A/B testing practitioner needs to be familiar with to make proper use of statistical methods.
The modern textbook example of a statistical test for the difference in means or proportions involves a point null hypothesis at zero, and a two-sided alternative hypothesis covering every value lesser or greater than zero. The textbook example of a confidence interval is also two-sided, containing the true value between its lower and upper bound XX% of the time it is constructed. This wasn't necessarily so in the writings of the fathers of modern statistical methods such as Ronald Fisher, Jersey Neyman, and Egon Pearson (Georgiev, Fisher, Neyman & Pearson: Advocates for One-Sided Tests and Confidence Intervals 2018), but it later became the norm, perhaps due to a few design flaws of the statistical tables published by Fisher in the early 20th century (Georgiev, Is the widespread usage of two-sided tests a result of a usability/presentation issue? 2018).
However, these textbook examples correspond only to a single subset of claims one can seek to support or refute with the help of testing data. With online controlled experiments, we face a multitude of questions to answer with data and therefore need to consider many different types of substantive hypotheses, and their corresponding statistical ones.
Let us explore several key types of hypotheses and the questions and claims to which they relate.
5.1 One-sided tests and confidence intervals
As established in Chapter 2, the null hypothesis is the claim we want to argue against, while the alternative hypothesis is the claim we hope remains after dismissing the null. The two are always complementary sub-spaces of the overall statistical model:

In most practical A/B testing scenarios we want to make a claim about the size and the direction of the effect, e.g. "treatment B results in an improvement in conversion rate over the control". These are called directional claims and any directional claim requires a corresponding directional null hypothesis.
Claims for the direction of the effect require directional (one-sided) null and alternative hypotheses.
The corresponding null hypothesis covers the values from minus 100 percent to zero percent:

If dealing with a continuous variable and not a proportion, then H0 would be limited by minus infinity from below, and zero from above.
The alternative is by necessity:

This is a classical one-sided test in which the error we want to avoid is not that the treatment makes no difference (true difference is exactly zero), but that it results in no difference or a negative difference. In terms of action, we would act the same if we knew the true effect to be negative, as if we knew it was zero: we would retain the current user experience and will not implemented the tested treatment. Therefore, the statistically adequate model is that of a one-sided alternative.


Figure 5.1: Two-sided vs. one-sided alternative hypothesis.

The interpretation of a p-value from a one-sided test remains unchanged from the classical definition - it is the probability that the observed discrepancy, or a more extreme one, would be observed, assuming the null hypothesis is true.
There is a slight clarification necessary. Since the null is composite, and there are many possible point values within it, the p-value is actually calculated for the most extreme point of the null hypothesis - the point closest to the alternative parameter space. Assuming H0 as specified above, the statistical calculation is performed for Δ = 0% rendering the p-value an exact probability only if δ* = 0. Since δ* is known only in simulated environments, the best option in real-world applications is to assume the worst-case scenario. If δ* < 0 then the actual error rate will be less than α, therefore in a one-sided test α and the corresponding p-value is no longer an exact probability, but a conservative bound on the type I error.
The one-sided test is sometimes also called a 'one-tailed test' since performing it is equivalent to calculating the cumulative probability density of the statistic by allocating all of the probability for rejecting the null to one of the distribution's tails.
For any symmetrical distribution, including the standard normal, the p-value from a statistical test can easily be converted from a one-tailed p-value to a two-tailed one, and vice versa, by multiplying or dividing by 2, respectively. If, for some reason, a two-tailed calculation was performed which resulted in a p-value of 0.05, all that is needed is to divide it by 2 to get the one-tailed p-value of 0.025.
Similarly, when interpreting confidence intervals, one often wants to know if a value outside of the interval would be rejected by a one-sided statistical test in which the most extreme point of the null hypothesis is at that value. This is the case when one wants to claim that 'values less than X' are not supported by the data. In such a case one should construct a one-sided confidence interval - most of the time this is an interval with an upper limit of plus infinity, but it can also be on the opposite side and have a lower limit of minus infinity.
Take, for example, a 95% one-sided interval with a lower limit of 2% and an upper limit of +∞. We can say that values below 2% are poorly supported by the data since a procedure with probability 95% of resulting in an interval limit of less than 2% in case the true value was under 2%, nevertheless resulted in said interval. However, constructing a 95% two-sided interval which covers, for example, the values between 0.5% and 35%, and claiming that only values below 0.5% are poorly supported at the 95% confidence level or higher would be overstating the uncertainty. The correct interpretation is that values below 0.5% and above 35% are poorly supported at that level. In fact, values below 0.5% are rejected at the 97.5% confidence level or higher.
The same logic applies if we prefer the reverse phrasing and argue that "the true value is above the lower confidence bound". To make this claim and add the confidence level at which the interval was constructed, we need to be using a one-sided interval. If using a two-sided interval, we need to add "and below the upper confidence bound" to our claim, changing it in a significant way.
With a one-sided confidence interval bound from below we can argue that the true value is above its lower limit, while with a two-sided confidence interval we can argue that the true value is between the two limits.
An easy way to remember this is that a two-sided interval at the 90% confidence level is the intersection of two one-sided intervals at the 95% confidence level:


Figure 5.2: A two-sided interval as the intersection of two one-sided intervals.

With the above interval, values below 0.3% are rejected at the 95% confidence level or higher.
5.2 Misconceptions about one-sided tests
Several misconceptions exist regarding one-sided tests both in online A/B testing circles, and among statisticians and researches in other areas. These are extensively documented by the author in several openly accessible articles (Georgiev, One-tailed vs Two-tailed Tests of Significance in A/B Testing 2017) (Georgiev, 12 Myths About One-Tailed vs. Two-Tailed Tests of Significance 2018), so here a brief summary will be presented.
The most common misconception is that one-sided tests are biased in favor of results in one direction or the other. This stems from mistaking the specification of the null hypothesis with an assumption or prediction about the direction of the effect which affects the sampling space, and, therefore, the evaluation of probabilities of observing certain outcomes. This is decidedly not so - the specification of the null and alternative hypotheses only partitions the parameter space (outcome space), and thus has no effect on the statistical estimation of where the true effect lies.
Its only effect, compared to a two-sided test, is that it broadens the claim we wish to reject - we move from a claim about the effect being exactly zero (which no one really makes in practice) to a claim that the effect is either negative or zero. This changes the interpretation of the results, but does not bias them in any sense of the word. Rather than 'biasing' the test, using a one-sided test where appropriate to the business question of interest aligns the statistical hypothesis test with the substantive hypothesis.
To further explain why this is a non-issue, consider a weight measurement metaphor in which there is a scale which results in a measurement that is no more than ±1 kg off the true weight 68.27% of the time. Assuming a normal error distribution, the scale's standard deviation is therefore 1 kg. Suppose the allowed risk for any weight claim we make is 5% corresponding to a critical value of 1.644 kg for a one-sided claim, and 1.96 kg for a two-sided one. The scale is obviously the equivalent of a statistical measurement.
If we measure John's weight and the scale reads 82 kg, we can reject the claim "John weighs less than or equal to 80 kg" with a p-value of 0.0227 and the claim "John weighs exactly 80 kg" with a p-value of 0.0455. This difference comes from the fact that under the second claim a scale reading of 78 kg would also count against the null claim. The same 78 kg reading, which has a chance of occurring due to random error even if the true weight is 80 kg, will not count as evidence against the one-sided null of "John weighs less than or equal to 80 kg". Note how the claim we make has no effect on the accuracy of the scale measurement, it only affects how we interpret the reading.
Another common charge against one-sided tests is that they allow for a greater proportion of type I errors compared to two-sided tests. For example, from the same set of data one can compute a one-sided p-value of 0.03 and a two-sided p-value of 0.06. This leads some to conclude that a one-sided test would produce a proportion of type I errors twice as large as that of a two-sided test.
This misconception stems from applying a one-sided p-value to a two-sided hypothesis - a clear case of model inadequacy. The p-value should always be computed under the null hypothesis to which it is applied. If the claim is for a point null, then the p-value should be two-sided, and a one-sided value should not be computed at all. The reverse is also true.
Once this error is revealed, the confusion disappears, and it becomes clear that both one-sided and two-sided tests provide the same level of error control for their respective null hypotheses. In fact, due to the conservative nature of one-sided error control, one can possibly argue that one-sided tests provide more stringent error control if it can be shown that some true null hypotheses are different than the worst-case value.
Yet another charge is that one-sided tests account for only one scenario. By committing to a one-sided superiority test, for example, we neglect to examine results in the negative direction. So, for example, if a test results in the outcome being -5% difference, we would need to refrain from making statements about its statistical significance.
Even if we accept such a charge at its face value, there is nothing wrong in not wanting to expose 20-60% more users to estimate how poorly a treatment is performing based on a certain low statistical significance threshold. Doing the opposite would, in fact, defy the usual business logic. The significance threshold is determined by business utility, and, assuming utility is to be found for positive results only, the significance should not even be calculated for a case of negative effects. Highly precise knowledge of the extent to which an effect is in the negative is redundant and wasteful, as we would act the same if we knew that the treatment effect is -1%, and if we knew it was -30%. We would not implement the treatment, assuming we would only implement experiences which improve our test KPI.
5.3 Strong superiority tests
A simple or classic superiority test has an alternative spanning from zero to plus infinity, which is appropriate when the claim we want to reject is that the true effect is not zero or negative. However, there are many situations in which it is not enough that the effect of the tested treatment is positive, but a certain size of the effect is needed to consider switching to the treatment. This could be due to external costs for implementing and maintaining the treatment, or due to perceived or estimated negative effects from implementing it.
For example, implementing an exit layover or a pop-up may have a positive impact on the number of email signups, but may result in the irritation of certain users, making them less likely to engage with the brand long-term. Similarly, aggressive cross-selling and up-selling tactics may increase short-term revenue, but legitimate concerns about negative long-term effects may be raised. If an online shop is considering adding more and higher quality photos per product, then the estimated lift in an A/B test should be larger than a certain threshold in order to justify the increased costs of the photo shoots and hosting expenses.
A strong superiority alternative hypothesis is required when a positive effect of a certain magnitude must be proven in order to justify implementation.
In general, it makes sense to use a superiority or strong superiority test when the proposed change or new solution:
•   has high implementation costs that need to be recouped;
•   incurs ongoing maintenance costs (IT infrastructure, personnel, etc.) or recurring costs of other types (e.g. 3-rd party services);
•   is costly or near impossible to reverse, once implemented for a variety of reasons: technological, marketing, PR, business relationships;
•   faces strong internal opposition in the form of HiPPO2 and other stakeholders voicing concerns for external reasons;
In case any of the above applies, a strong superiority alternative hypothesis is the appropriate statistical model for the business decision to be made. For example, if it is estimated that for the tested treatment to have a positive effect on the bottom line it must increase the conversion rate by 2% or more, the null hypothesis would need to cover the values from minus 100% to plus 2 percent:

The alternative is by necessity:

with 2% being the superiority margin expressed in the units of the parameter of interest (percentage difference in conversion rates). This would obviously alter the p-value calculation compared to a simple superiority case, since the p-value is the probability of observing the data assuming the null hypothesis is true, and the null hypothesis is now different. For example, while observing an improvement of 6% might result in a p-value of 0.03 under the null

leading to the conclusion to reject that null under a 0.05 significance threshold. However, it may result in a p-value of 0.1 under the strong superiority null defined above [-100%, 2%], and this null will not be rejected at the same 0.05 significance threshold.
In terms of the parameter space, take as an example the relative difference between two conversion rates. A classical alternative hypothesis covers differences in both directions, while a one-sided superiority alternative covers from zero to plus infinity, and a substantive superiority alternative covers from 0.25 to plus infinity.


Figure 5.3: Types of alternative hypotheses: simple superiority vs. substantive superiority

The nulls are complementary to the alternatives, as usual.
Since the addition of the superiority margin effectively decreases the difference between treatment and control, a superiority test will require a larger sample size than a classic superiority test with the same parameters. We are decreasing the effective minimum effect of interest, leading to an increase in the required sample size, if the minimum effect of interest stays the same relative to the baseline. With a large enough superiority margin, the required sample size can be multiple times larger than that of a simple superiority test. Therefore, it must be chosen judiciously.
A well-informed choice of the superiority margin by considering all costs associated with a decision to implement is key for a good test.
The term 'strong superiority test' is not yet established in the literature, but since I wasn't able to find a term for these types of hypotheses, I consider it appropriate to set these apart from the 'default one-sided null'.
In terms of calculation, there is nothing special about a strong superiority test. One simply subtracts the superiority margin from the observed difference and then proceeds as usual. If the outcomes of the A/B test as shown in Table 5.1 the p-value from the usual calculation is p(H0: Δ ≤ 0) = 0.03.



 

Users

Conversion Rate



A (control)

39500

5%



B (treatment)

39500

5.3%




Table 5.1: Example outcomes of an A/B test.

However, for the superiority test we subtract 0.1 p.p. from the difference in conversion rates. The resulting p-value p(H0: Δ ≤ 0.2) ≈ 0.1 is increased, informing us the evidential input of the data is inadequate to reject this null at the 0.05 level.
5.4 Non-inferiority tests
There are certain cases when designing an online A/B test when we do not actually need the proposed change to make a measurable positive difference: we would actually implement the change if it results in no difference in the primary metric of interest, or even if it is possible that it results in a slight decrease in said metric. In such cases, we can conduct a non-inferiority test.
Non-inferiority tests are appropriate when an absence of a negative effect of a certain size is all that is needed to implement a change.
In an article from 2018, (Georgiev, The Case for Non-Inferiority A/B Tests 2017) the author divides the cases where one can consider a non-inferiority alternative hypothesis in two distinct types - 'side benefits' and 'easy decision'.
The first type describes tests in which the intervention put forth for consideration comes with benefits which are not measurable through the primary performance indicator. Examples of side benefits are reducing the number of photos taken per product might result in savings on photoshoot costs and hosting expenses; removing a free trial period which results in lessening the involvement of support personnel; removing a manually curated section of the website, or replacing it with automatically-curated content. All of these would reduce ongoing business costs, meaning that the new experience might be worth implementing, even if it has a negative effect on the primary KPI of the test.
The 'easy decision' type of scenarios may seem quite narrow in application, since, to classify as such, a tested intervention should be all the below:
•   cheap and easy to implement;
•   have zero maintenance costs;
•   reversible with relative ease;
•   face little to no internal opposition;
•   However, examples when all these conditions are fulfilled describe a high proportion of tests conducted on a daily basis, such as trivial changes to the text of call-to-action (CTA) elements, simple copy changes or image replacements. The removal or addition of some elements of a site or a third-party functionality (such as trust signals, live chat, etc.) can also sometimes check these boxes. The reason a non-inferiority test can be a good choice for such situations is that this combination of factors means that one can choose to trade the slower and more conservative superiority test for a faster and more risky non-inferiority one.
I argue that in a side benefits situation the use of a non-inferiority test constitutes the correct modeling of the business problem into a statistical one and this means that the use of a non-inferiority test is therefore a must. If one would be happy to implement a change even if hurting the performance metric to a certain degree cannot be ruled out, then a non-inferiority test is the appropriate choice.
Things are not so clear for the easy decision type of cases where a non-inferiority test can be admissible, with the caveat that it would be a good idea to monitor results post-test. An example scenario is examined below.
In common with strong superiority tests, and the choice of a superiority margin, the choice of a non-inferiority margin is key for conducting a successful non-inferiority A/B test. If it is estimated that even with up to a 1% decrease in conversion rate for the tested treatment that there will still be a positive effect on the bottom line, the null hypothesis would need to cover the values from minus 100% to minus 1%:

The alternative is by necessity:

with 1% being the non-inferiority margin expressed in the units of the parameter of interest (percentage difference in conversion rates).
In terms of calculation, there is nothing special about a non-inferiority test. One simply adds the non-inferiority margin to the observed difference, and then proceeds as usual.
Since the addition of the non-inferiority margin effectively increases the difference between treatment and control, a non-inferiority test will require a smaller sample size than a classic superiority test with the same parameters. We are increasing the effective minimum effect of interest, which results in a decrease in the required sample size. Depending on the size of the non-inferiority margin, the required sample size of a non-inferiority design can be anywhere from double-digit percentages to multiple times smaller than that of an 'equivalent' superiority test.
To illustrate the possible increase in speed of testing, consider an example wherein the tested change is a simple CTA button text swap of 'Free Trial' with 'Start Free Trial'. Such a test checks the boxes for an 'easy decision' scenario and is therefore a candidate for a non-inferiority test. The current conversion rate is 9%. Let us say that with a classic superiority test a reasonable minimum effect of interest is 0.45 p.p., and with such a design a test would require 103,152 users in total with a significance threshold of 0.05 and power of 0.8, which can be funneled into the test in three months. If we instead choose a non-inferiority design with a 0.18 p.p. non-inferiority margin and maintain the same 0.45 p.p. MEI relative to the baseline of 9%, then we would instead need only 52,174 users in total which can be funneled into the test in about six weeks.
The trade-off is an approximately 50% faster test execution, with the potential risk of implementing a change which has no effect, or has a slightly negative effect. To minimize the risk of a negative effect, the data can be monitored for level shift after the implementation. If such a shift is observed to a degree not explained by usual variance, then the change can be easily reversed, or retested, either with another non-inferiority test, or with a stricter superiority test.
Whether such a trade-off is acceptable for any particular intervention from the 'easy decision' category is at the discretion of the practitioner.
5.5 p-value notation
As we begin using null hypotheses which are aligned with the decision-making process and, therefore, reflect possibilities we hope to reject using data, a need to make sure these p-values are communicated properly emerges. It takes us beyond the standard approach of writing the observed difference and adding the p-value in brackets, e.g.: "...the observed difference between variant and control is 0.001 (p=0.02)" or the equivalent tabular presentation.
Since the p-value may be calculated under a one-sided or two-sided null, or it may be calculated using a superiority or non-inferiority margin, the notation used should make that fact clear. This will avoid confusion in the reading of results, and will likely help avoid some of the common misinterpretations.
The suggested notation is of this form, when the p-value is presented next to an observed parameter of interest:
Parameter Value (p=p; H0: Null Expressed as Included Parameter Values)
When the null is a point hypothesis of zero difference in means, the notations would look like so:
0.001 (p=0.02; H0: Δ = 0)
If the null is composite one-sided superiority, the notation is:
0.001 (p=0.02; H0: Δ ≤ 0)
An example for strong superiority:
0.001 (p=0.02; H0: Δ ≤ 0.0005)
And a notation example for non-inferiority:
0.001 (p=0.02; H0: Δ ≤ -0.01)
Of course, the parameter can be percentage change, or any other type of variable. The idea is that the notation should guide the data user to knowing which conclusion is barred by a low p-value, without the need to seek for contextual information elsewhere in the report.

2 Highest Paid Person's Opinion








Chapter 6
TESTS WITH MORE THAN ONE VARIANT
Experiments with multiple interventions (variants) tested against a control are often referred to as A/B/n tests or multivariate tests. Since the latter term is also used to refer to tests where there is more than one outcome of interest, as well as the entirely different category of 'factorial designs', I will refrain from using 'multivariate' to refer to A/B/n tests. The chapter also discusses if factorial designs are viable and useful in a typical online A/B testing setting, but will not cover these in any considerable detail.
A/B/n tests are useful for comparing a single existing user experience with a set of distinct alternatives, in order to determine either which one of them, if any, is better than the control. In rare cases, we might want to identify all variants which are better than the control. The number of variants is usually limited by development resources, creativity, and time / sample size constraints. A typical A/B/n would include 2-4 variants tested against the control, but there is theoretically no upper limit on the number of variants. Google famously tested 41 shades of blue in an A/B/n.
6.1 Type I error in an A/B/n test
As with a simple A/B test, an A/B/n test must produce a p-value and/or confidence interval with guaranteed type I error probability, respectively coverage. A naïve approach would be to perform pairwise significance tests of each variant and the control, then report the one with the biggest observed distance score (Z-score, T-score, etc.) as the most likely one to be an improvement over the control. For example, the p-values for B vs. A, C vs. A, and D vs. A in an A/B/C/D test might be 0.3, 0.11 and 0.04. We then claim the difference between D and A is statistically significant at the 0.05 level. An equivalent operation would be to calculate 95% confidence intervals for each pairwise difference.
The issue with such an approach is that we report a statistic calculated only on a partial set of the totality of the available data. The statistical model of such a statistic will fail to include data on any of the other variants that we tested and will, therefore, fail to incorporate all information. From a statistical adequacy point of view, such a model is mis-specified with regards to the actual testing procedure.
The problem is well illustrated by the story of the Texas sharpshooter. In it, a man from Texas takes a revolver and fires 120 bullets at the broad side of a barn. He then goes to the barn with chalk in hand, finds a group of 6 shots neatly grouped together, and draws a target with the bullets in its center. He then claims he is a great shot as he was able to produce such a nice group of six bullets while 'forgetting' to mention the total number of bullets he fired at the barn, and how the target was drawn.
Obviously, the target and the group of holes in its center are poor evidence of the qualities of our shooter. Similarly, having multiple attempts at success and reporting only one of those after the fact, while fully neglecting the others, is a sure way to make any p-value or confidence interval highly misleading with regard to the true input of the testing procedure. If the sharpshooter fired at a target already drawn, and was able to say which six bullets will hit its center before he fired them, we would be much more impressed with his abilities.
Since a p-value is supposed to communicate how unexpected a set of results are, assuming a certain statistical model corresponding to the null claim, we should start defining the proper statistical model for an A/B/n test by asking ourselves what the null claim is. In this case, the default position is that the existing UX is better than all of the proposed variants. An actual p-value from such a procedure should therefore correctly report the probability of observing the outcome we observe, or a more extreme one, assuming the null is true. The null hypothesis in this case is defined as each of the variants being worse than or equal to the control, as measured by the parameter of interest (difference in conversion rates, etc.).
The need to work with such a composite null hypothesis lead to the development of procedures for controlling the 'Family-Wise Error Rate'. 'Family' refers to a set of statistical hypotheses which combined produce the substantive hypothesis that needs to be rejected. In our case, the set of null hypotheses in an A/B/C/D test could be, for example:

The family-wise error rate is then the overall error rate of this set.
This issue was well understood as early as 1936, when Bonferroni published his now famous work (Bonferroni 1936) describing it and calculating the FWER of m tests each of size α to be:

FWER describes the probability that at least one of the m tests is significant, and it equals 1 minus the probability that none of the m tests are significant. Assuming the statistical tests are independent, the probability, under the null hypothesis, that none of the m tests are significant is the product of each test's probability of not being significant, which is exactly .
With one-sided superiority tests, the above is the probability of one of the tests resulting in a statistically significant outcome at level α if all tested variants are in fact equal to, or worse, than the control. So in an A/B/C/D scenario the actual error of three tests with α = 0.05 would be 1 minus 0.953, which equals 1-0.857375 or 0.143 which is almost three times higher than the desired 0.05.
This result is conservative, as it does not consider the dependency between the tests which exists, since in each of them a different variant is compared to the same control.
The type I error rate of an A/B/n test is called family-wise error rate (FWER): the probability of falsely rejecting any of the pairwise null hypothesis if all are in fact true.
There are two ways to apply FWER control in an A/B/n test. One is to perform pairwise significance tests, and then adjust the significance threshold boundary for each of them so that its actual significance corresponds to the desired nominal significance. For example, for an A/B/C/D test the adjusted p-value threshold αadj is calculated as α/m, the nominal α divided by the number of comparisons, resulting in each test being performed with significance threshold αadj = 0.0166. Similarly, for an A/B/C/D/E test the actual p-value threshold corresponding to a nominal type I error of 0.05 would be ~0.0125. Using this threshold any pairwise p-value below 0.0125 can be declared significant at the 0.05 level.
Most do not prefer this procedure, though, as it arbitrarily shifts the significance threshold, which makes interpretation confusing, as shown above. Even more confusingly, if the nominal threshold we want to achieve is 0.05, and the modified significance threshold for the comparison is 0.013, upon observing a nominal p-value of 0.04 we have to declare the result as not significant at the 0.05 level.
A more intuitive approach is to adjust the p-value or confidence interval bounds in a way which makes the reported p-value the actual p-value, and ensures that the reported confidence interval bounds are the actual ones. If the result is 0.0499 then it is just significant at the 0.05 level, period.
6.2 p-value and CI corrections for testing multiple variants
After understanding how the type I error rate is affected by adding more variants to an A/B test, and the two approaches to dealing with this problem, we can examine methods for calculating adjusted statistical estimates in an A/B/n test.
The simplest p-value and CI adjustments are based on the simple Bonferroni equation given above. They are also the least powerful, so we would not consider them. Somewhat more powerful procedures are achieved by using rank-ordered p-values, and either adjusting them or evaluating them against a threshold adjusted in a step-down or step-up method.
A suitable method, slightly more powerful than basic Bonferroni, is the Holm-Bonferroni step-down method (Holm 1979). Step down means that we iterate the list of ranked p-values from the lowest p-value to the highest. The procedure controls the family-wise type I error rate in the strong sense and conservatively: the probability to reject a true null hypothesis is at most α, regardless of the set of true null hypotheses at hand.
Holm-Bonferroni, being more powerful than simple Bonferroni, requires a smaller sample size to achieve the same type II error probability without imposing any restrictions on the dependence of the hypotheses that compose the family of hypotheses for which FWER is to be controlled. This is unlike other even more powerful methods, such as Hochberg's procedure or the Šidák adjustment which is explained in Chapter 7.
The adjusted p-values from the Holm-Bonferroni method can be computed as such:

where m is the number of hypotheses to be tested, i is the index of the hypothesis such that 1 ≤ i ≤ m while j is an iterator j = 1,...,i . The condition on {x}1 is imposed to guarantee no adjusted p-value is larger than 1.
Consider an example application with an A/B/C/D test which produced the following unadjusted p-values from the pairwise comparisons B vs. A, C vs. A and D vs. A: p1 = 0.01, p2 = 0.04, p3 = 0.03. First, order the p-values from smallest to largest: 0.01, 0.03, 0.05. m = 3.
In the first step, adjust the p-value for B vs. A since p1 is the smallest. j = 1 so the adjusted p-value is padj1 = (3 - 1 + 1) · 0.01 = 0.03. Next, with j = 2 evaluate padj3 (D vs. A) which is the maximum of (3 - 1 + 1) · 0.01 = 0.03 and (3 - 2 + 1) · 0.03 = 0.06, which is 0.06, so padj3 = 0.06. Finally, with j = 3 evaluate padj2 = max{ (3 - 1 + 1) · 0.03, (3 - 2 + 1) · 0.03), (3 - 3 + 1) · 0.05 } = max{ 0.03, 0.06, 0.05 } = 0.06 which is the adjusted p-value for the difference between C and A.
The result is that the adjusted p-values are padj1 = 0.03, padj2 = 0.06, padj3 = 0.06. While in this particular example the end result has not changed and we would reject H1 at the 0.05 with both the adjusted and non-adjusted values, the significance of the result fell from 0.01 to 0.03 (three times higher uncertainty) and we are not able to reject H2 and H3 at the 0.05 level after the adjustment.
Given its simplicity, the Holm-Bonferroni procedure can be applied easily, but its power gains are small compared to a procedure developed specifically to deal with the case of comparing multiple treatments versus a control - the Dunnett's correction. The Dunnett's test was developed in 1955 (Dunnett 1955) and later improved (Dunnett and Tamhane 1991), and in common with Holm-Bonferroni it controls FWER in the strong sense, but is significantly more powerful as it makes use of the positive dependence between the hypotheses tested in the pairwise comparisons. This quality makes the Dunnett's procedure the preferred choice for p-value adjustments in A/B/n tests.
The Dunnett's correction is the most powerful procedure which maintains the FWER in an A/B/n test.
Dunnett published tables with adjusted p-value thresholds for a number of cases initially in 1955, and then in 1964, which cover up to nine treatments and one-sided and two-sided adjusted T-scores for the 0.05 and 0.01 significance thresholds.
These can be useful, but are tedious to apply in modern practice. A preferable approach is instead to calculate adjusted p-values which is, however, non-trivial even today as it requires the computation of multivariate normal or multivariate t distributions.
For example, in the Dunnett step-down method we let  denote the ordered test statistics with associated null hypotheses . Then we have the following stepwise procedure:
If , reject H1 and continue; else stop
If , reject H2 and continue; else stop
...
If , reject H1 and continue; else stop
...
If , reject Hm
where  denotes the  quantile of the distribution of the maximum of -distributed random variables and is computed from the corresponding multivariate z-distribution.
From this method of adjusting the significance threshold, we can also arrive at adjusted p-values for each statistical test. But this is quite involved computationally. For practical applications, the reader is advised to consider the R package 'DunnettTests' developed by Fan Xia3. Commercial tools which support it include SAS and SPSS, as well as calculators developed by the author, which can be found at Analytics-Toolkit.com.
For an example comparison of the power of the Holm-Bonferroni and Dunnett's methods, we can use the same A/B/C/D test example from above, for which the unadjusted p-values were 0.01, 0.03, and 0.05 for H1, H2, and H3. The adjusted p-values using the Holm-Bonferroni method are 0.03, 0.06, and 0.06, respectively. The adjusted p-values using the Dunnett's method are 0.0262, 0.0598, and 0.0546, demonstrating the increased power achieved by taking the known dependence into account.
An easy way to calculate adjusted confidence intervals, regardless of the procedure used to adjust the p-value, is to make use of the duality between p-values and confidence intervals. One can calculate an adjustment ratio for the confidence level based on the ratio between the unadjusted p-value and the adjusted one, and then multiply that ratio by 1 minus the desired confidence level. Finally, the interval can be computed in the usual way depending on the metric of interest but using the confidence level adjusted in this manner. For example, if the unadjusted p-value is 0.01 and the adjusted one is 0.03, the adjustment ratio is 0.01/0.03 = 1/3. Then, to calculate a 95% interval, calculate an interval with confidence level produced by multiplying 0.05 by 1/3, subtracting the result from 1 and converting it to percentages: 1 - (0.05 · 1/3) = 1 - 0.01667 = 0.983 = 98.3%. So, the confidence interval needs to be computed with a nominal confidence level of 98.3% to get 95% actual confidence level.
6.3 Sample size calculations for A/B/n tests
Due to the inflation of type I error and the consequent application of p-value adjustments to keep the FWER in check, the sample size of an A/B/n test needs to be increased accordingly, in order to maintain the same statistical power versus a given minimum effect of interest.
Sample size computations for A/B/n tests depend on the type of p-value adjustment procedure used to control FWER.
The simplest, but least efficient, way to achieve this is to compute the sample size required for a single variant in an A/B test, and then simply multiply it by the number of variants in an A/B/n. If a test takes 10,000 users per variant in an A/B test for a total of 20,000 users, then an A/B/C/D/E test will take around 43,000 users. This is inefficient, though, since it doesn't correspond to procedures which can make use of the dependency between tests to achieve more powerful tests.
If one decides to use the slightly more efficient Holm-Bonferroni method, the sample size can be reduced, while maintaining the same power versus a particular MEI. The sample size can be reduced even further by employing Dunnett's correction.
In almost all cases, sample size calculations become non-trivial and require several assumptions for the computation to be feasible in practice. The first two are that the minimum effect size of interest is the same for all variants (treatment groups), and that the alternative hypotheses are one-sided. The first of these stipulations should not be a problem if there are no external considerations specific to each variant, while the second one is standard for most online A/B testing anyway. An additional restriction about the treatment groups having equal sample size is necessary for proper numerical evaluation, which should not be a problem either, since this is usually the most efficient approach.
Additionally, the practitioner should consider whether they are interested in rejecting at least one false null hypothesis, in which case disjunctive power calculations have to be applied, or if they are interested in rejecting all false null hypotheses, in which case conjunctive power calculations are used. In most cases, one should use disjunctive power, as the sample size cost of testing with disjunctive power is usually not justified. However, there might be cases were the test needs to be well-powered against all false null hypotheses; for example, when there are considerations for the preference of one variant over another which are not measured by the primary KPI of the test.
Even with these assumptions, the computations go well beyond what can fit within these pages, not to mention the fact that simulations are sometimes the only way to compute the sample size.
Table 6.1 below presents the scaling of the sample size with increasing number of variants in a test:



Sample sizes for different numbers of tested variants



# of variants tested

Sample size per variant

Total sample size

Sample size per variant (% of A/B test)

Total sample size (% of A/B test)



1

9,735

19,470

-

-



2

8,620

25,860

-11.45%

+32.82%



3

8,163

32,625

-16.15%

+67.70%



4

7,885

39,425

-19.00%

+102.49%



5

7,723

46,338

-20.67%

+138.00%




Table 6.1: Sample sizes for different numbers of tested variants.

The above calculations are for disjunctive power - we are only interested in achieving our required power level for a single false null hypothesis.
Note how the sample size per variant decreases while the total sample size increases. Adding one variant to an A/B test means that we would need to test about 1/3 longer, even with Dunnett's correction, which is still better than using simple Bonferroni. The advantage of Dunnett's correction slowly disappears as more variants are added - at 4 variants versus a control Dunnett's only cuts down 12% of the sample size compared to Bonferroni, and it gets worse as the number increases.
In practice this means that A/B testing practitioners should not add new variants to either an A/B test or an A/B/n test without due consideration of those variants' probability to actually improve over both the control, and any of the other variants already accepted for testing. Adding a new variant which differs from an existing one by a tiny, and most likely inconsequential, detail will only slow down the test, with no real chance of a pay-off for the increased number of users exposed to the test, and the longer time required to reach a conclusion. A new variant should only be added if it is considered significantly different from all existing ones.


Figure 6.1: Power with different numbers of test variants

Another way to illustrate this is to consider the effect on the minimum reliably detectable effect of a test with the same number of users, but different number of variants. The calculations presented in Figure 6.1 are performed using the most powerful method available, which is Dunnett's, and are for a sample size of 50,000 users, with a baseline conversion rate of 10%, and a p-value threshold of 0.05 (equivalent to 95% confidence level).
To obtain the minimum reliably detectable effect of a test with a given number of variants at any given power level, trace the point where the respective curve crosses the chosen power level, then see the corresponding value on the x-axis. For example, a test with 4 variants versus a control has 80% power to detect a true effect of about 9.5%.
Note how with the same sample size (and therefore test duration) we can detect a true effect of approximately 6.7% relative difference with one variant versus a control, but this number jumps to over 10% relative difference with 5 variants against a control.
Adding a variant to an A/B test should be approached conservatively given the great cost in terms of either reduced statistical power, increased sample size requirement, or increase of the minimum reliably detectable effect.
Practitioners who are tempted to add one or more variants which are only slight variations of existing treatments should instead consider running a follow-up test with such variants based on the winning treatment. The initial A/B/n test will not be powerful enough to reliably detect the minimal actual differences in performance between these slight modifications and the treatments they are based on. A follow-up test can be more comfortably planned to run for much longer than the original as we would already be reaping the benefits of implementing a winner. We can, therefore, more readily wait for data which might point to a slightly superior shade of blue. Running a long test in which there is either a poorly performing control or, alternatively, several equally poorly performing variants, is usually not good for business.
The takeaway here is to avoid the spray-and-pray approach in which we add a variant and hope it 'sticks', without considering the cost in terms of either reduced statistical power, increased sample size, or increase of the minimum reliably detectable effect.
6.4 Dynamically dropping or adding variants
The discussion on A/B/n tests thus far was made under the assumption that the number of variants is determined before the beginning of a test, and does not change until its end. However, there is always a temptation to add a variant in the middle of the test because a very good new idea came up, or to drop an especially poorly performing variant in order to stop exposing users to it.
Doing either of these is non-trivial, as they necessitate changes to the statistical model used to calculate statistical estimators. Going back to the sharpshooter metaphor, dropping a poorly performing variant is like plugging bullet holes - if we plug enough of them then it is easy to appear to be the best sharpshooter in the Wild West. But a p-value or a confidence interval calculated without taking into account these extra attempts would not represent how surprising the result actually is.
Adding variants is also an issue, as it increases the type II error rate. Spreading the users into more groups increases the within-group variance, and makes each comparison less sensitive.
Furthermore, issues that emanate due to the time heterogeneity of the data, which were mostly dismissed in Chapter 2, suddenly become relevant.
For example, groups dropped early, or added after the beginning of the test, also contain biased data relative to the rest of the treatment groups. If there was a factor affecting all groups negatively in the beginning which later disappeared, any group that stopped early would not have experienced the effect of that change, or would have experienced it only partially. This means that comparing it with groups that ran for the whole duration is no longer fair; it is effectively like comparing apples to oranges. Similar logic can be applied to groups added later in an experiment - they might have not experienced an effect which negatively affected the rest of the groups, biasing the result in their favor, and vice versa.
There is a whole body of literature on Adaptive Trials which examines ways to make dropping or adding arms to a trial efficient, while maintaining statistical guarantees. The increased cost and complexity of these approaches, the difficulty in interpretation of results, as well as issues remaining to be addressed around them, makes it difficult to recommend adaptive designs in general. More on this in Chapter 13.
I would argue that it is a poorly justified practice for time-heterogeneous data and related external validity issues, characteristic for the online A/B testing environment which produces the data.
If one is nevertheless interested in pursuing this path, then one must make sure to take into account both the increased complexity in the statistical design and analysis of such tests, as well as the increased difficulty in correctly communicating results from them.
6.5 Do factorial designs make sense in online A/B testing?
When talking about A/B/n tests, the topic of factorial design tests often comes up, mostly due to lack of understanding what a factorial design is and what it is good for. Many practitioners seem to have an incorrect notion that a factorial design is basically an A/B/n test, in which each variant is produced by changing one of several factors which are believed to alter user behavior in some way.
For example, say there is a landing page we want to optimize, and we decide to focus on three things - the page title, the color of the main CTA, and the text of the main CTA. Imagine we create one new version for each of these elements - one new title, one new color, and a new button text. Covering all possible combinations will produce seven variants to test against a control, with each variant being a unique combination of the tested elements.
Can this be the setup for a factorial test? Not usually, since a factorial test addresses different questions and reflects different goals. From a theoretical perspective, such a test can be planned and analyzed as a full-factorial design, but I would argue that it makes little sense to do so from CRO point of view.
A test with factorial design aims to estimate the effect of each factor individually. However, there appear to be very few, if any, A/B testing situations in which we would care about the impact of an individual factor in a way that is done in a factorial design experiment. Such designs are only appropriate when the factors and different possible settings for them are of primary interest. They also make most sense when the changes introduced to each factor can be neatly plotted on a scale; e.g. temperature, speed, force, pressure, etc.
For example, a plant or factory which produces steel bearings may consider as factors the heat of the ovens, the amount of outer ring oscillation, and the type of the cage containing the balls. This makes sense, since two of the variables are on a continuum, while the third is a yes/no categorical variable, with a very limited number of options. One can learn a lot from testing a few different temperature levels, oscillation speeds, and a couple of cage designs. We can get directional information for the first two factors, as they are neatly plotted on the temperature scale and speed scale. And we can, for example, therefore conclude that increasing the temperature of the ovens by 5% increases the durability of the bearings by 10%.
In the above example, understanding the input of each variable on the outcome variable(s) is useful since it can inform the direction of future experiments; e.g. increase temperature by 10% to see if it further increases the durability, and if the relationship is maintained in a linear fashion, or if it starts to temper off, or even reverse (as is usually the case with physical properties). It is still not as good as understanding the overall effect of the combined changes on durability, but given that it requires a smaller sample size than an A/B/n test with an equivalent MEI, it is as cost-efficient as a preliminary learning effort.
Is such a situation ever encountered in A/B testing scenarios? And what is the nature of the factors that we would like to test? Most changes made in an A/B test treatment are part of an infinite set of possible variants which are not defined on any type of continuum. Different CTA texts cannot be ordered on a scale, therefore information about the direction of future changes cannot be obtained from the result of a factorial test, in the way that it could be with, for example, temperature.
Ordering CTAs by length of the text, or by some measure of complexity of used text, is possible, but not very useful due to the nature of human language and perception. Only rarely there would be a 'yes/no' type of factor, such as the presence or absence of a given element. It is not, by itself, a justification for running a factorial test.
The results from a factorial online A/B test would be much less informative than those performed in a manufacturing facility.
More important, however, is whether factorial designs answer the questions we want answers for, which are about higher order interactions. While it might be interesting to know that the absence of a certain element results in better conversion rate, it is even more interesting to know that the combination of three particular changes, one of which is the presence of said elements, results in a higher conversion rate. In a practical setting, knowing that page title 2 increases CR by 1% or more will be of little interest compared to knowing that a variant in which page title 1, orange CTA color, and a CTA text 'Free Trial' increase the conversion rate by 3% while the next best combination of page title 2, orange CTA color, and CTA text 'Free 14-day Trial' increases it by only 2%. The sample size required to test minute changes to single web page elements is rarely there. More often we want to test whole solutions involving a set of changes introduced to different elements instead.
A factorial test is typically not equipped to answer questions about higher-order interactions, since power calculations and MEI are considered at the factor level, with the factor with the highest sample size requirement determining the sample size of the whole experiment. However, the sample size is always lower than that of an equivalent A/B/n test, Therefore, upon completing the test there will not be enough data to reason about interactions of a higher order. If a factorial design is powered for the highest order of interaction, it becomes essentially an A/B/n test and is equivalent in efficiency.
However, the versatility of a set of A/B/n tests can, in fact, mean that choosing a set of such tests is always more efficient than running one big factorial test.
An illustrative example can be put forth by exploring the situation outlined above, in which we need to test a landing page, and, for the purpose of the example, are interested both in the overall effect of combining different page titles (P), CTA colors (C), and text (T), and in the effect of each individual change. There are three orders of interaction with all the possible interactions being:
First Order (lowest order): P, C, T
Second Order (higher order): P + C, P + T, C + T
Third Order (highest order): P + C + T
The number of factor levels we consider will determine the final number of possible unique variants. In this case we are considering just two 'levels' with levels in quotes since there is no scale on which to measure 'levels'. Using the typical factorial design terminology simply exposes its inapplicability.
So these are all the possible highest order interaction variants:



Possible highest order interactions in a factorial design


 

Page Title (P)

Color of CTA (C)

Text of CTA (T)



Control

1

Green

"Free Trial"



Variant 1

1

Green

"Free 14-day Trial"



Variant 2

1

Orange

"Free Trial"



Variant 3

1

Orange

"Free 14-day Trial"



Variant 4

2

Green

"Free Trial"



Variant 5

2

Green

"Free 14-day Trial"



Variant 6

2

Orange

"Free Trial"



Variant 7

2

Orange

"Free 14-day Trial"




Table 6.2: Possible highest order interactions in a factorial design.

If we care only about first order interactions, then we can design this as a factorial ANOVA test, powering it for the smallest minimum effect of interest among all three factors. For example, if we decide on a MEI of 5% relative difference for the title, and 6% for the text and the color of the CTA, we will power the test to have 90% power to detect a 6% MEI, meaning that the tests for the other two factors will be overpowered. Still, the test will require only a fraction of what an A/B/n will, while these eight variants will require at 5% MEI.
This naturally means that if we choose to analyze it for highest order interactions then the test will be severely underpowered.
Now consider how the same situations can be handled with a single A/B/n test, with seven variants and a control. Say that the baseline conversion rate is 10%, and it is designed to have 90% power at 5% MEI using the Dunnett's correction. This means a total of ~360,000 users or 45,000 users per treatment would be needed, with a 0.05 significance threshold.
Does this design preclude us from analyzing interactions of lower orders? Not in the slightest. If we are interested in first order interactions, we simply combine the relevant groups, and treat it as an A/B test. To learn which title variant works better, combine the data from the Control group and group 1 through 3, and then compare it with the data for groups 4 through 7. This means we will have 45,000 · 4 = 180,000 users per group, giving us a minimum reliably detectable effect of approximately 2.94%.
If we are interested in second order interactions, repeat the process with the relevant groups. Say we want to examine the combined effect of changing the CTA color and the CTA text. Simply group together the data for all variants which contain the same CTA color, and then perform the statistical analysis. Since there are exactly two variants with each pair of CTA color and text, we would have three groups to compare (Variant 1 & Variant 5, Variant 2 & Variant 6, Variant 3 & Variant 7) against a control (Control + Variant 4) which means 90,000 users per group. This would result in a minimum reliably detectable effect for these interactions of about 3.72% relative difference.
Obviously, once we start making so many hypothesis tests, we might need to consider FWER control based on methods like Bonferroni or Holm-Bonferroni. This will slightly decrease the statistical power of each interaction test, but it will not be worse than performing an ANOVA test at all of these orders of interactions.
Even if, for some reason, we care only about first-order interactions and want to measure the effect of changes to the title, CTA color, and CTA text in isolation, a factorial design would still not be a great choice, since it has to be powered based off the smallest MEI. Note that one cannot simply drop factors mid-test, in the same way that one cannot simply drop variants from an A/B/n test.
Changes to different factors would often warrant different MEIs, meaning that it will be more efficient to run several separate concurrent A/B or A/B/n tests, instead of a single factorial design. This is due to the fact that it would be slowed down based on the difference between the smallest and the largest MEI of a factor. As each of these can be powered based on its respective MEI, some tests will conclude earlier. This becomes especially efficient if we are using a sequential testing procedure, such as those described in Chapter 10.
For every factorial design there is an equally efficient or more efficient combination of A/B or A/B/n tests, due to their flexibility.
I believe I have made the case why factorial designs are generally geared towards questions of secondary interest at best, and due to their rigidity are outperformed in efficiency by running a set of concurrent A/B or A/B/n tests when lower-order interactions are of genuine interest. One of the main reasons for the lack of application of factorial designs and the need for other forms of factor analysis is that most changes tested in online controlled experiments are not measurable on a meaningful scale. A curious example of where the changes can be put on a scale is examined below.
6.6 Testing the perfect shade of blue
I would like to offer a humble reexamination of a famous test of this nature which was run by Google some time in 2008/2009. According to then Google executive Marissa Mayor, and a designer involved in the test (Holson 2009) (Hern 2014), two Google teams had to decide on the blue background of the toolbar across Google pages. A designer proposed one shade of blue which all designers liked. An engineer, however, tested another shade, and the data suggested that his blue worked better in driving clicks on the search results. The dispute was temporarily resolved by Mayor selecting a shade between the two shades.
However, afterwards a test was performed for each of the 41 shades of blue between the one proposed by the designer and the one proposed by the engineer. One shade was selected based on these experiments. There are conflicting accounts on what benefit this brought the company, since reports are mixing two different 'shades of blue' experiments, but the number $200 million is often cited.
While it is not entirely clear if Google ran 41 variants against a control in a single test, or whether they did 41 separate A/B tests, assume for argument's sake that they did run 41 variants against a control in a single test. Further assume that they applied correct adjustments for significance and sample size, and they were able to run the test in a timely manner. Even so, if you, dear reader, had to design the test, would you test 41 variants, knowing that this is increasing the time required to run the test by about 12 times compared to a simple A/B test between the two shades?
What if we are to consider the fact that blue is on a scale and whatever effect it has on users is likely 'dose-dependent' with a certain peak around the 'best' shade of blue. With that in mind, it is more efficient to first run an A/B/n with just three variants against the control - two variants with each of the two shades proposed by the two camps, and then a shade exactly between them, versus a control with the current color, whatever it is.
This would have increased the sample size only by about 67% compared to a simple A/B test. Then another A/B/n test with several values around the winning shade could be run as well. This would have cut total execution time about four-fold, while having a high probability of homing in on the best shade of blue.
As an added benefit, during the duration of the second test the control and most variants would have an improved conversion rate relative to the baseline, meaning that we would be exposing significantly less users to suboptimal shades of blue. In other words, we would start gaining the benefits of the test much sooner than if we'd designed a large single test with 41 variants, only a handful of which would be close to optimal.
While most practitioners would either prefer to test a more impactful set of changes, or would simply not have the sample size required to run a properly powered test of that many levels of a single scalable factor, the above is an example of employing A/B/n testing, in order to determine the optimal level of such a factor in an efficient manner.

3 It can be found at The Comprehensive R Archive Network (CRAN) or by performing a repository search in R








Chapter 7
SEGMENTATION AND MULTIPLE PERFORMANCE INDICATORS
Performing an analysis on a segment of the users included in an A/B test, or having more than one measurement for the outcome of the test, have a lot in common, since both procedures may result in us performing multiple tests of significance. As already discussed in the previous chapter, performing more than one statistical test results in the need to control the Family-Wise Error Rate (FWER) of that set of tests, if just one significant outcome can decide the outcome of the experiment.
This chapter discusses different possible experimental designs, where FWER control is required, and different strategies for providing such control where necessary.
7.1 The Šidák correction
Since we will discuss FWER control in the case of multiple significance tests, methods based on the work of Šidák (Šidák 1967) are appropriate, as they provide a nice balance between being less stringent than Bonferroni's method and providing a higher statistical power.
As a reminder, FWER describes the probability that at least one of the m tests is significant, which equals 1 minus the probability that none of the m tests are significant. Assuming the statistical tests are independent, the probability under the null hypothesis that none of the m tests are significant is the product of each individual probability of not being significant, which is exactly .
Since Šidák's work is concerned with arriving at an adjusted significance threshold αadj for each of the tests, one can use the FWER equation and solve for αadj to obtain:

The Šidák correction maintains exact FWER control if the tested hypotheses are independent, conservative control if they are positively dependent, and liberal control in case of negative dependence.
The most powerful method to correct p-values is by using a stepwise adjustment on the rank-ordered p values. This way the adjustment is altered as a function of the p-value rank and magnitude.
First, p-values are computed for each null hypothesis in the usual way. Then, the p-values are sorted in increasing order, from the smallest to the largest. Then, using the set of sorted p-values p1, ..., pm, i = 1, ... , m, calculate the adjusted value  for each test by solving:

Using the above formula, if there are five tests with the following nominal p-values: 0.011, 0.020, 0.062, 0.124, 0.350, the adjusted p-values become: 0.049, 0.078, 0.175, 0.233, 0.350. Note that the highest p-value nominal and adjusted values are the same since m-i+1=1 for i=m, therefore .
Even though there are other applicable methods, the Šidák correction using the step-down method described above will be the main tool we'll be using in going through the rest of the chapter.
7.2 Analyses of segments of the sample of an online experiment
Segment analysis can be both a great tool for learning more from our tests and informing future tests, as well as a wasteful exercise of chasing statistical ghosts if not done properly.
An ad-hoc way of doing segment analysis is almost sure to lure us into the aforementioned chase. In such a scenario, there are no predetermined segments before starting the test, nor meaningful actions related to results for particular segments. A test is completed, and the software spills out a bunch of data, with some tempting options to segment it based on various criteria. The more segments are analyzed in this fashion, the more likely it is the analysis will produce an exciting result, as we already know from our discussion of issues resulting from running multiple significance tests without appropriate adjustments.
Note that it is easy to run just a single significance test in this manner - by cherry-picking the largest segment that demonstrates an eye-catching result. This process, and the process of performing a series of significance tests, are equivalent in the outcomes they produce, unless the selection process is included in the statistical model.
A sound strategy for segment analysis rests on pre-selecting the segments to be analyzed. Segments should be based around:
•   User characteristics that can be influenced (e.g. traffic sources);
•   User or device characteristics based on which experiences can be customized (e.g. device type, location, age, subscription age);
•   Characteristics which are expected to change over time (e.g. device type, traffic sources);
The reason for this is that any outcome needs to be actionable. We can take action if we can influence the characteristic; e.g. by spending more on ads, or investing more in social channels. We can also act if we can customize the experience for a segment; e.g. by delivering a different website version for users in different locales, for users using different devices, etc. If we expect mobile traffic to rise over time, as we would have rightly done back in 2012, then the outcomes of the test can be adjusted to account for the expected increase, using an analysis of the mobile segment.
Once we have selected the segments that we wish to analyze, we then need to consider what significance threshold to use. And it is here that things get difficult. While we are yet to discuss how to perform it, a risk/reward analysis is usually needed in order to set a significance threshold which makes for an optimal business decision.
Let us suppose that such an analysis was performed, and an optimum level for the overall test was determined to be α = 0.01, due to fairly high stakes in making a bad call.
If each segment needs to be statistically significant before a final decision is made, simply perform tests of size α = 0.01, and there will be no need for adjustments. However, usually the reason that we segment is to act on a given segment. If the data suggests that we should, most of the time we would be interested in any segment which shows an interesting effect.
At first, this may lead to the conclusion that adjustments such as Šidák or Bonferroni are necessary in order to maintain a given overall level of significance. However, each segment has a separate risk/reward profile and thus can justify different significance thresholds. A decision to deploy a tested variant to U.K. customers only will not carry the same risk-related costs, nor will it bring the same benefits when compared to deploying the solution worldwide. Similarly, a change which only affects desktop users will carry a different weight versus a decision to deploy the same change for mobile users as well.
Absent any non-shareable costs part of the risk/reward calculation, and assuming we only act on a per-segment basis, both risks and rewards scale with the size of the segment. It follows that the significance threshold which needs to be met for each segment should be roughly the same, or slightly lower, than the significance threshold for the whole A/B test, meaning that no p-value adjustments would be necessary. It becomes equivalent to running separate tests for each segment, which can be more efficient if using sequential testing procedures which allow each segment's test to stop earlier than others in case of overly positive or overly negative outcomes. Running separate tests is actually recommended in case there is reason to believe different segments will be affected differently, even in opposite ways. But an estimation is still desirable for all segments, even those which are expected to show zero or negative effects.
It has to be noted how this is different than scientific contexts where significance thresholds are set by convention, and not determined by utility considerations.
I understand how some may be alarmed by the potentially high FWER in such a family of tests. The definition of a family needs to be considered here - is it a family of hypotheses, if we are acting on each result individually? I argue that it is not. Instead, it becomes the equivalent of a science-wise error rate - sure, one can calculate it, but who is arguing for the null hypothesis, and who is interested in rejecting it? As stated, the above does not apply if one wants to act on the whole website or app, based on results from a single segment. If that is the case, the Šidák correction should be used when calculating statistical significance.
Put differently, if one is worried about making too many segment-specific decisions, increasing the chance that some of them might be in error, one should also be worried about the proportion of all A/B tests performed, which will necessarily increase the number of them concluded with an erroneous decision. This would quickly paralyze any optimization program, as these arbitrary groupings of hypotheses mean that with every new test performed, the probability of committing an error increases. It is this increased probability of which one would be afraid, without any regard of balancing risk and reward, without realizing that committing one small mistake might be well worth it, if it gave them the opportunity to test ten new ideas in a relatively safe way.
7.3 Designs with more than one primary parameter of interest
A test can have a number of variables of interest, a.k.a. parameters of interest, outcome metrics, or endpoints. These are usually separated into two tiers - primary and secondary. A primary outcome metric is one which is used to decide the results of a test. By contrast, secondary outcomes of interest are not used to decide the outcome of whole tests, but may bring value due to the additional insights that they provide.
As a general approach, designs with multiple primary parameters of interest should be avoided whenever possible. When designing a test, the metric based on which the success or failure of the tested intervention is judged should be as close to the business bottom line as possible, and should lend itself to a risk/reward analysis. If there is a situation where two metrics seem to compete with each other, consider if there is a different metric which combines the information of these two metrics instead.
A simple example is that of add to cart rate and purchase conversion rate per user. Since purchase conversion rate is closer to the business bottom line, it should be a primary metric, and the add to cart rate should be a secondary metric. The only exception is if what we do is aimed at increasing the add to cart rate per se, and maintaining a level of purchase conversion rate is only a secondary consideration, in which case the two metrics flip their places. However, even then there is just one primary metric.
Another simple example is purchase conversion rate per user and average order value. While both are at the same distance from the bottom of the funnel, it is easier to use average revenue per user, as it combines the information contained in the two original metrics. If evaluating the impact on each of these metrics is of interest, then they can be given the status of a secondary outcome.
In a different scenario, a test might potentially influence purchase conversion rate and the wholesale inquiries conversion rate. This presents a difficulty, since a conversion on latter metrics would usually be weighted much more heavily than a conversion on the former. This can be avoided by combining them into average revenue per user. Converting inquiries to revenue will need to happen, regardless of whether the total effect on both of the original metrics is to be estimated.
Still, there are situations when more than one primary outcome may be of interest and of equal or nearly equal value, and there might not be a straightforward way of combining them into one coherent metric. Consider the case of an email marketing A/B test in which the primary goal is to increase average revenue per subscriber, but on the other hand it should not come at the cost of increasing our unsubscribe rate.
The first metric is short-term and the second is long-term - we gain revenue from each sale immediately but the losses which an unsubscribe action may bring will not be felt for a long time. In this combination it seems hard to come up with a straightforward combined metric.
In case that two primary metrics are justified, there are two main scenarios. In the first, all of the metrics need to be statistically significant at a specified significance threshold, in order to implement the tested variant(s). In this case there is no need for any adjustments to the calculated p-values, confidence intervals, and other estimates. The example of ARPU plus unsubscribe rate as primary outcomes fits this scenario, as we want both an increase in ARPU and no increase in unsubscribe rate in order to implement the tested variant. In such a case we would test ARPU for significance, and only if it meets the rejection threshold would we proceed to evaluate the unsubscribe rate. The overall rate of the procedure α' is ensured to be ≤ α.
In the second case, the threshold can be met by any of the metrics, and the test variant will be implemented. For example, an online media website might want to implement a solution if it either increases the duration of reader engagement, the number of pieces of content they consume, or both. In such a situation, there would be an inflation to the type I error rate similar to that discussed with regards to A/B/n testing.
The Šidák correction can be applied if there is no evidence of negative dependence between the hypotheses. In the above example there is most likely a positive dependence between the hypotheses - a person reading larger quantities of content is more likely to also spend more time on the website. The Šidák correction will therefore provide conservative FWER control. Resorting to the less powerful Holm-Bonferroni is also an option if there are concerns about the assumptions for applying Šidák.
There is a hypothetical third situation in which there are more than two primary outcomes of interest and they are grouped in some way. For example, an experiment may reject the null hypothesis if outcomes 1 & 2 are significant, if outcomes 1 & 3 are significant, or if outcomes 2, 3 & 4 are significant. Such situations call for more advanced so-called alpha-recycling methods in order to properly control FWER over a complicated decision tree. The interested reader can begin exploring them starting with (Burman, Sonesson and Guilbaud 2009), (Bretz, et al. 2009) (Mauer and Bretz 2013) (Dmitrienko, D'Agostino and Huque 2013).
Note that unlike scientific contexts, risk/reward analyses need to be performed for each possible outcome in order to determine ROI-positive statistical design parameters (Chapter 11).
If the statistical design includes an FWER control procedure then the sample size needs to be adjusted in order to maintain the statistical power of the test, much in the same way as it is done for multiple comparisons. Determination of test sample size to ensure that the test is appropriately powered is difficult in these cases, and often need to be performed using computer simulations, rather than an analytic formula.
7.4 Designs with secondary parameters of interest
Since, by definition, secondary outcomes are not used to decide the outcome of a test, they are only evaluated if at least one of the primary outcomes results in a statistically significant outcome. Secondary parameters usually measure micro-conversions, such as views of product pages, add to cart actions, start checkout actions, newsletter signups, trial signups, and so on. We still want to have error-control for statements with regard to the effects of a tested variant on secondary parameters. The difficulty is that we rarely have guidance on what an acceptable error is with regard to secondary outcomes. Most of the time, secondary parameters are evaluated at the significance threshold chosen for the primary parameters.
If there is just one primary outcome of interest, if its null hypothesis is rejected then the secondary outcome(s) can be evaluated with full alpha, without incurring any penalty in terms of error inflation. If α = 0.05 for the primary outcome, and it is rejected with p ≤ α, then the error left to "spend" on the secondary outcome(s) is also α = 0.05.
Similarly, if there are multiple primary metrics, and all of them need to be significant in order to make a certain inference and/or decision, full alpha is available for testing the secondary metric(s).
If, however, only some of the primary metrics are significant, then we are in the more complex case described briefly above - we need to consider graphical approaches, and/or gatekeeping methods or closed testing procedures, whose details go beyond the current work.
Once we know the alpha allocated for evaluating secondary outcomes, we proceed the same way as if we are evaluating primary outcomes, depending on whether we need one, all, or just some of the outcomes to be significant, before we accept a certain conclusion. In most cases, we would be happy with just one outcome being significant, and therefore need to apply the Šidák or Bonferroni correction, depending on which assumptions are met. Since in most online A/B testing scenarios the outcomes are positively dependent or independent, Šidák is usually applicable.







Chapter 8
WORKING WITH CONTINUOUS DATA
Many good business metrics come in the form of a continuous variable, so called non-binomial metrics or continuous metrics.
A non-binomial metric is any variable which can take values on a continuum, as opposed to a binomial metric which only takes one of two values: 0 or 1, true or false, present or absent. Most calculations and examples in the book so far have dealt with statistics based on binomial metrics, such as conversion rate - the mean of a binomial metric. A lot of the time we are specifically interested in the difference in conversion rates.
Continuous metrics, on the other hand are non-discrete as their values often span the entire real line. Sometimes they are limited from above or below, often from 0 to +∞, for example, in the case of revenue per user or per transaction. Examples of such metrics are revenue in any currency, number of transactions, transactions per user, revenue per user, session duration, sessions per user.
Arithmetic means of continuous metrics are also continuous metrics themselves: average transactions per user (ATPR), average revenue per user (ARPU), average session duration, average sessions per user, and so on. This results in several complications in calculating statistical estimates, and, more notably, in the planning of statistical tests.
8.1 Standard deviation, p-values, and confidence intervals
Unlike binomial metrics and statistics based on them, such as conversion rates, one needs to estimate the variance and standard deviation of continuous metrics by performing calculations on each observation from the sample. With a conversion rate, the standard deviation of the mean (a.k.a. standard error of the mean) can be estimated by knowing only two numbers - the proportion or number of events in a group and the total number of observations in the group.
However, with a non-binomial metric the standard deviation needs to be computed by first computing the mean, and then summing the squared differences between each observation and the mean. As a reminder, the formula is:

Whereas the formula for the pooled standard error of two means is:

where σ1 and σ2 are the estimated standard deviations of the two respective groups, calculated using the formula provided above.
Note that while a sample of 10,000 users with a conversion rate of 0.1 (10%) has a known standard deviation of 0.003, the standard deviation of a sample of 10,000 users with a mean revenue per user of $50 cannot be computed using a formula. If the transaction revenue is normally distributed, and 68.27% of orders are between $45 and $55, then the standard deviation will be $5, while if 68.27% of orders are between $10 and $90 then it will be $40. This translates directly into the standard error of the mean revenue per user, which is the standard deviation of the revenue per user divided by the square root of the number of observations.
Other than this slight complication, the computation of p-values or confidence intervals can continue as described in Chapter 2 for as long as the statistic in question is an arithmetic mean. The reason is that according to the Central Limit Theorem the standard error of the mean tends to be normally distributed with an increase in sample size, regardless of the underlying distribution. This applies to all of the following frequently used metrics - average transactions per user, average revenue per user, average session duration, and average sessions per user.
In case a p-value or confidence interval needs to be calculated for revenue per user, sessions per user, or similar metric, non-parametric tests or quantile tests based on the empirical distribution will be appropriate, but necessarily harder to interpret. Tests of stochastic difference between two or more groups are very rare in A/B testing, since arithmetic means usually translate well enough to effects on the business bottom line.
When faced with continuous data, one might be tempted to engage in the removal of 'outliers' - data points far off the mean in either direction - in order to reduce the variance of the data and produce a certain outcome. With ARPU these are unusually large purchases which, naturally, have an effect on both the mean and the standard deviation. There is no single definition of what 'far off the mean' actually amounts to - it is entirely subjective and thus the term 'outlier' is also subjective and situation-specific.
It must be stressed that removal of such extreme data points constitutes using less than the available data. Removal of data can only be done if it is known that the data point in question was recorded erroneously (e.g. due to tracking error, a test/fake order), or if it is determined to be a threat to the external validity of the results due to the observation being due to such an extreme event that it is very unlikely that it will be repeated in the foreseeable future, or is a 'once in X years event'.
Any removal of data points has to be well-justified and noted in the meta data accompanying the experiment data set.
8.2 Statistical power and sample size calculations
Statistical power and sample size calculations for fixed sample tests do not differ between binomial and non-binomial metrics in any categorical way. As with non-binomial metrics, the baseline mean must be estimated from historical data. If there are pronounced seasonality effects, or trends in the standard deviation over time, estimating the standard deviation for the duration of the test can also be useful.
An important observation about using non-binomial metrics is that tests based on a continuous metric will usually take longer than tests for any metric which is a component of that continuous metric due to its higher variance.
A test for a binomial metric which is a component of a continuous metric will usually require a smaller sample size to run, all else being equal.
For example, purchase conversion rate per user is a part of average revenue per user, since the purchase conversion rate is a component of the average revenue per user, with the other component being the average order value. If the conversion rate increases, the average revenue per user will increase as well, as long as the average order value stays the same or increases. The conversion rate and the average order value are the two components that can be used to fully substitute the average revenue per user since:

and

hence, after replacing Purchases in the ARPU formula and eliminating Users from the numerator and denominator results in:

If the CR is 3%, and AOV is $50, then ARPU is 3% times $50 which results in ARPU of $1,5. The variance (and standard deviation) of ARPU will consist of the combined variance of CR and AOV which is greater than or equal to the individual variance of CR and AOV by definition.
The only case where the variance of CR will not be greater than that of ARPU is if all purchases are of the same value, say $20. In this case there is no added uncertainty and the pooled variance is equal to the CR variance. In all other cases the dispersion of ARPU data is greater and hence would take longer to test for.
Running a test based on difference in CR will almost always require a smaller sample size than a test based on difference in ARPU, all else being equal. This, however, comes at the cost of such tests being less informative, since switching from ARPU to CR amounts to ignoring information. Measuring just CR fails to take into account the different value of different purchases, hence this approach treats all purchases the same.
Tests based on binomial metrics do not take all the relevant information into account if the measured events are of unequal value.
If a test shows that CR went up while AOV went down, the end business result might be positive, neutral, or even negative, depending on how much CR went up, and how much AOV decreased. This means that in many cases the primary goal of 'conversion rate optimization' is in fact optimizing average revenue per user, even when faced with the cost of increased test duration. Typically, for e-commerce websites the increase in required sample size for an ARPU test is in the 20% to 30% range compared to an A/B test of a component conversion rate.
This naturally leads to the question - are there situations in which we can safely test for CR and thus reduce the test duration without having a significant chance of misinterpreting the end result for the bottom line? A compromise which might be admissible, if the proposed intervention is considered unlikely to affect AOV, is to run the test with difference in CR as a primary metric and AOV or ARPU as a second primary metric to be evaluated only after a positive outcome on CR. In such a case, the primary metric can be tested for superiority or strong superiority, while the secondary one is tested for non-inferiority.
8.3 Is the Normal distribution assumption adequate?
Some people working in the CRO industry are afraid that working with difference in means of non-binomial metrics violates the normality assumption. I will briefly illustrate where this fear is stemming from, and why the normality assumption holds for A/B tests in which difference in means is of interest.
For most people, the obviousness of the inapplicability of the Normal distribution stems from graphs like the one below, in which we can clearly see that the data is nothing close to normally distributed. If this is average revenue per user data, then there is a huge spike of users with zero-revenue, and then there is a heavily left-skewed distribution of users per revenue level. The graph may have a more well-defined peak in the middle, or all users with non-zero revenue could be distributed among 2-3 values (e.g. if dealing with a SaaS with a couple of price tiers), but it is definitely non-Normal by all accounts. This would be contested by no one.


Figure 8.1: Simulated data for revenue per user.

However, the statistical model must adequately describe the data generation mechanism for the variable that we are measuring which is the difference in means.
To simplify our exploration, consider just a single mean, say that of the control group. Since the observations are independent random variables with bounded moments, the Central Limit Theorem (CLT) can be invoked, and it states that the average of a large number of independent random variables is approximately Normally distributed around the true population mean. A difference between two Normally distributed random variables also tends toward a Normal distribution, regardless of the distribution of the variables themselves. The difference in ARPU is just such a variable; therefore, the normality assumption is adequate.
While it would be correct to note that a convergence result is not valid for a small number of observations, the number of observations that we usually deal with in online A/B testing is in the hundreds, thousands, or hundreds of thousands, which is a very comfortable territory for applying the Central Limit Theorem. Unpublished simulations performed by the author show that it holds well with 30+ users per arm, which conforms to established results, and is well below most practical scenarios.
In fact, the Normal distribution is very often used in calculating statistics for means of binomial variables - conversion rates of all kinds. Visualizing a Bernoulli variable, it becomes clear that its distribution is decidedly non-Normal. For example, a 5% conversion rate looks like Figure 8.2 in terms of the distribution of the underlying Bernoulli observations.


Figure 8.2: Bernoulli data

Yet, a randomly generated set of proportions from Bernoulli data are approximately normally distributed with any decently large sample size.
Once we establish that means can be modelled as normally distributed random variables, it can easily be concluded that their difference is also a normally distributed random variable.
The normal distribution assumption is justified when testing for difference of means in all practical online A/B testing scenarios.
Having said that, it would not necessarily be a mistake to use something like the Mann-Whitney-Wilcoxon rank-sum test, methods from so-called robust statistics, as well as bootstrapping resampling methods, as some have suggested, but it is an unnecessary and inefficient complication. To take the MWW test, for example: it is a test of stochastic difference which is hard to interpret unless the distribution is symmetrical around the median. Interpreting it as we would, the outcome of a T-test or Z-test is only warranted if the data is approximately T-distributed or normally distributed, which is exactly the situation in which we would use a T-test or a Z-test. Given the slight edge in terms of statistical power, and the greater ease with which we can apply a T or Z test, it is unclear to me why anyone would go for the MWW test.
8.4 A workaround for incomplete ARPU data
As practitioners, we might want to perform statistical calculations on A/B tests for difference of means of non-binomial metrics, but might find ourselves in situations in which the software does not compute these for us, and does not provide us with any way to get detailed user-level or session-level data - one row per user or session. Knowing the proportion of observations with events, and the number of observations, is enough to calculate p-values and confidence intervals for difference in proportions. However, knowing the mean and the number of observations is not enough when talking about the difference of means of non-binomial variables like average revenue per user (ARPU), average sessions per user, or average pageviews per user.
For example, using Google Analytics or similar software to store our A/B testing data might mean that there are only aggregates available - number of transactions, number of users, average revenue per transaction, average transactions per user, average sessions per user, etc., but there is no way to get a list of rows, one per user, with the revenue, number of sessions, or number of transactions for each.
If the metric used is something like average sessions per user, average time on site per user, average pageviews per user, then we are out of luck. There is nothing that can be done to reconstruct the data from the available information.
However, for the particularly important metric of ARPU and its equivalents, there is a nice and easy workaround which effectively allows us to reconstruct the full set of data from a partial set. For example, we might have a list of transactions and their revenue - one transaction and its revenue per row, for example:
Transaction ID, Revenue
...
123, 25.25
124, 20.00
125, 19.00
...
Let's say there are 200 transactions. The next step is to obtain the total number of users for the period, say 10,000. Continue to create a spreadsheet and paste the transaction data in the first 200 rows. Then create another 9,800 rows in the first column with random numbers for the first column and zeroes for the second column.
The result would be something like:
...
199, 23.15
200, 21.00
201, 0.00
202, 0.00
203, 0.00
...
10000, 0.00
Voila! We've effectively reconstructed the data set for the purposes of the usual statistical calculations since we can calculate its mean, standard deviation, and number of observations from it.







Chapter 9
PERCENTAGE CHANGE
9.1 Percentage change (lift) vs. absolute change
In most online controlled experiments, the results are ultimately communicated in terms of a percentage change relative to the baseline (control group). In part of the literature this is referred to as 'lift', 'percentage lift', or 'percent change'. A statistically equivalent way to express the result is in terms of relative difference.
This way of expressing the outcome of an A/B test has many advantages over communicating an absolute change and the baseline. Firstly, it is just one number, not two, e.g. 5% instead of 0.005 and a baseline of 0.1. Secondly, it is much easier to understand the magnitude of the effect when the baseline conversion rate is tiny, and the effect is small - a result of 2% relative is more meaningful than an absolute improvement of 0.00009 and a baseline of 0.0045. Thirdly, it can be plugged directly into calculations of the expected effect on the business bottom line, either directly or through some kind of modeling, in case the measured variable does not directly translate into increased revenue.
While percentage change has many advantages, it has a practical disadvantage due to the fact that most statistical tools only support calculations for absolute change. Therefore, what ends up happening in practice is that the statistical design is for absolute difference, confidence intervals and p-values are calculated for absolute difference, but then the result is communicated as percentage change, while still reporting the CI and p-value calculated for absolute change.
Few people realize that this is a case of model misspecification, and this therefore results in reporting a nominal p-value which is different than the actual one. If one reports percentage change, then the statistical model should be built for this variable, not for the absolute change variable. The transformation from absolute change to percentage change is not invariant since the variance is different for these two variables. Performing this naïve transformation, and accompanying it with the failure to calculate adequate statistical estimates, is a very common error.
The statistical model corresponding to testing for absolute difference is different than the statistical model for testing relative or percentage difference.
In a simple A/B test, if we denote the conversion rate of the control group by µ0, and the conversion rate of the test group by µ1, then the parameter of interest is

and the variance will be simply the combined variance of µ0 and µ1. However, when calculating a statistic based on the relative difference, the parameter of interest becomes

When talking about percentage change:

The multiplication by 100 does not change anything about the statistical model, but the division by µ0 for relative and percentage change does. It increases the variance of the estimates: the greater the actual difference between µ0 and µ1 is, the more it increases the variance of Δrel and ΔrelPct compared to Δabs. Greater variance means larger error, meaning that there is a greater probability of observing a given relative difference compared to the probability of observing its equivalent absolute difference. This is reflected by the wider confidence intervals and higher p-values produced by methods which properly incorporate this information.
The difference between the absolute difference and relative difference models was confirmed by extensive simulations (100,000 per unique combination composed of type of interval, confidence level, size of true difference, and baseline) performed by the author (Georgiev, Confidence Intervals & P-values for Percent Change / Relative Difference 2018). Summary results are presented below.
Figure 9.1 shows how much the nominal confidence interval coverage of results, obtained from a naïve transformation of absolute difference intervals to percentage, difference differs from the actual coverage for a range of actual effect sizes. Results from an interval which is adequately defined for Δrel and ΔrelPct are presented for comparison.


Figure 9.1: Discrepancy between nominal and actual type I error of different confidence intervals applied to relative difference (percent change) for different true effect sizes.

We can see that the larger the true effect size is, the worse the divergence between actual and nominal type I error (α) becomes for the naïve transformation of an interval based on absolute difference. If the true relative change is below 5% α-inflation is under 10%. This goes sharply up with increasing effect sizes: for 40% true lift the type I error will be 1.72 times higher for a 95% confidence interval; it is over 2x higher for a 97.5% interval.
In contrast, the larger the true effect size is, the more conservative the proper confidence interval for percent effect becomes, providing around 20% lower type I error than nominal, compared to 2-5% lower when the effect size is below 10%.
We can observe that the error also increases in magnitude with the increase in the confidence level of the interval, however it is in the positive direction for the transformed absolute difference CIs, and in the negative for the proper relative difference ones.
Figure 9.2 shows the effect is approximately consistent across a wide range of baseline proportions.
Let us illustrate the difference between applying an adequate statistical model and an inadequate one with an example.


Figure 9.2: Discrepancy between nominal and actual type I error of different confidence intervals applied to relative difference (percent change) for different baseline rates.

We have a simple superiority hypothesis (H0: Δ ≤ 0, H1: Δ > 0) with a fixed sample size design. After observing the outcome, we see µ0 = 0.10 (10%) and µ1 = 0.12 (12%), with 1360 observations in each group. The p-value computed for the absolute difference of 0.02 is 0.049 and the corresponding confidence intervals spans from 0.003 to 0.0397. A naïve transformation of this result would suggest we can claim significance at the 0.05 level for a 20% improvement. The naively transformed CI spans from 0.3% to 39.7%.
However, an adequate model results in an actual p-value of 0.0539 or about 10% higher than the nominal one computed above. The confidence interval spans -0.5% to 43.1% and now covers the no difference value of zero percent. This means we cannot claim significance at the 0.05 level.
9.2 Confidence intervals for percentage change
There are two approaches to computing proper, although not exact, confidence intervals for percentage change. The first is to derive the standard error using the so-called delta method - a first order Taylor's expansion to calculate the variance of B/A where B is the conversion rate of the treatment group, A is the conversion rate of the control group, and µ0 and µ1 are the observed proportions. The delta method delivers an approximate result via:

Since A and B are independent by design in a randomized experiment, Cov(A,B) = 0 so the above simplifies to:

The formula, however, appears to systematically overestimate the standard error, and results in intervals which have higher than nominal coverage.
An interval with coverage much closer to the nominal was provided in Kohavi et al.'s 2008 paper: "Controlled experiments on the web: survey and practical guide" (Kohavi, et al. 2008). The interval is constructed using the following equation:

Where Δrel is the relative difference (B-A)/A, CVA is the coefficient of variation of the control group, CVB is the coefficient of variation of the treatment group, and Z is the standard Z-score corresponding to the desired level of confidence, e.g. 1.6448 for a one-sided 95% confidence interval.
The coefficient of variation, also known as the relative standard deviation, is simply the standard deviation divided by the observed parameter value:

Therefore, CVA is calculated as σA / µA and CVB is calculated as σB / µB.
The simulation results presented in Figures 9.1 and 9.2 above were achieved using this formula, while simulations performed with the delta method formula resulted in worse discrepancies between nominal and actual coverage. Therefore, this formula is recommended.
9.3 p-values for percentage change
The author of the book is not aware of an analytical method for calculating the p-value for percentage change with any high degree of precision. An approximate solution can be achieved by using the approximate solution for standard error calculation using the delta method formula, but it is not fully satisfactory.
A more accurate solution is to iteratively compute an interval using the Kohavi formula, by changing the Z-value in a way which makes the solution reach a value of Z, for which the interval just barely misses the extreme point of the null hypothesis closest to the alternative hypothesis. Such iterative approximation can produce results with very high precision, even with a fairly low number of iterations, and it is trivial to code. Execution time is in milliseconds with modern-day computers.
The resulting Z-score can be easily transformed into a p-value using the relevant cumulative distribution function (CDF).
9.4 Sample size calculations for percentage change
The author, in collaboration with Assen Tchorbadjieff, PhD., has devised a sample size computation method based on risk ratio sample size calculations, and a correction to account for the increased variance. This method is currently proprietary and used only in Analytics-Toolkit.com and GIGAcalculator.com.
Approximate conservative sample size calculations can be performed by calculating the sample size with a model for absolute difference, and then increasing the resulting sample size by 4% to achieve the same statistical power for percentage difference.
Approximate results are arguably applicable to many practical A/B testing situations where the actual sample size is dictated by considerations such as observing the data after a whole day or a whole week has passed, in which case even such a rough approximation may be good enough.







Chapter 10
SEQUENTIAL TESTING: CONTINUOUS MONITORING OF DATA
Our discussion so far considered only fixed sample size tests in which the sample size, or equivalently the duration, is fixed before the test begins, and the statistical evaluation of the data is performed after the test is completed. This provides for a fairly simple statistical model of the data generating mechanism, and a p-value simply reflects the fraction of all possible combinations which would end up producing an outcome as extreme or more extreme as the one observed. The random variables considered were all primary or secondary outcome metrics, while confidence intervals followed a similar logic.
We know that fixed sample tests are optimal in terms of making the most efficient use of the available data. However, such optimality calculations are valid only under the specified statistical model, which allows only one evaluation of the data - when it has been gathered completely. There is room for more efficient methods under models which account for evaluation of data at one or more interim points.
Remember the concepts of overpowered tests - wherein one commits a larger than necessary sample size, provided a given minimum effect of interest? This often results in a longer-than-necessary exposure to either an inferior control or an inferior variant, resulting in lost revenue, or in missed opportunities to gain revenue.
Assuming the null hypothesis was informed by business considerations, there are often cases where allowing examination of the data at an earlier point in time may result in interim results so optimistic as to leave no doubt in the rejection of the null. Similarly, there are other cases where an interim analysis suggests the tested treatment is very unlikely to outperform the control, or is indeed proving to be worse than the control by a certain margin. In the first case, we would be happy to implement a winning variant, in order to maximize the gains by exposing 100% of users to it as soon as possible. In the second case, continuing the test is futile or even harmful as it exposes a proportion of users to an inferior variant. Stopping early is ROI-positive in both cases.
This chapter will examine how to conduct tests with interim evaluation of data, while maintaining proper error control. It covers statistical models which reflect a data-generating mechanism that incorporates a decision rule to potentially stop the test before its planned end. The underlying assumptions, caveats and trade-offs involved in sequential analysis of accumulating data are also discussed.
10.1 The issue of repeated significance tests on accumulating data
Before addressing sequential monitoring of A/B tests, let us first consider the problems posed by such a practice if using tools presented in previous chapters without any modification.
As already established, calculating any statistic requires defining an adequate model, and describing the data-generating mechanism under the null hypothesis. An adequate statistical model reflects the data-generating process well enough for practical purposes. Note that this encompasses the observations themselves, but also the procedure through which the experiment is conducted and analyzed.
This was well-understood by statisticians in the 20th century. The first estimation of how much nominal p-values calculated under a fixed sample assumption differ from the actual p-values was performed by Armitage et al. in 1969 (Armitage P. 1969) and the results can be easily confirmed through simulations in a matter of minutes nowadays. Table 10.1 presents a few of these results.



Inflation of type I error during unaccounted peeking with intent to stop



Number of peeks with intent to stop

Nominal type I error probability

Actual type I error probability

Type I error inflation



0

0.05

0.050

-



1

0.05

0.083

1.7x



2

0.05

0.107

2.0x



3

0.05

0.126

2.5x



4

0.05

0.142

2.8x



9

0.05

0.194

3.9x




Table 10.1: Inflation of type I error with unaccounted peeking with intent to stop an A/B test.

Zero peeks are equivalent to a fixed sample test, while one peek means that we are looking once before the final evaluation. As can be seen, even a couple of peeks with intent to stop will result in multiple times larger type I error rate, if unaccounted for.
Unaccounted peeking with intent to stop results in a nominal p-value multiple times smaller than the actual one.
We can also observe that the error does not increase linearly - by peeking 10 times we only increase the difference by 2.3 times versus peeking once, and by 2 times versus peeking twice. There is a limit to the increase of the actual type I error rate as the number of peeks gets closer to the number of observations.
A way to visualize the cause of this difference between nominal and actual p-values is to examine two alternative sample paths which end up at the same point after 24 days, meaning that if we were to analyze the data only at that point, we would come up to the same conclusion despite the different paths it took to get there (Figure 10.1).


Figure 10.1: Comparison of two paths that result in the same end outcome.

In Scenario 1, the significance threshold line is crossed multiple times during the duration of the test. If we were using a naïve optional stopping rule, we would have stopped on day 7, and would have never reached day 24. In Scenario 2, on the other hand, we would not have stopped before day 24, even if we used the same optional stopping rule.
Now consider how the knowledge of us having used an optional stopping rule changes how we view the data on day 24. In Scenario 1, we would have never reached day 24 in the first place, so we focus on Scenario 2. If we did not peek with intent to stop, there is just a single point in the sample path at which the data may disprove the null hypothesis. Doing so at that point is fairly impressive. Compare this to the presence of an optional stopping rule in Scenario 2, where we had 23 possibilities to stop, and that we failed at each of them, until stopping at day 24. The data in Scenario 2 is worse evidence for the presence of a true effect, due to the fact that we had so many chances to stop early, and failed at each of them.
Scoring on the last of 24 attempts at any sport would not impress anyone nearly as much as scoring on the first attempt. If we go back to our Texas sharpshooter metaphor, repeated tests of significance would be like drawing the target first, then starting to shoot at it, and stopping once a bullet hits the bullseye.
Obviously, we would be somewhat impressed with a shooter that lands a bullseye on the first attempt, while not being as impressed by a shooter who does so on his fifth attempt, and we would probably mock anyone claiming to be a sharpshooter if they require 20 attempts to score a hit. If a shooter is good (there is a true difference between variant and control), we would expect them to hit the bullseye in the first couple of attempts. The more attempts that it takes them, the worse shooter they are. Or, in keeping with the metaphor - the less support the data provides for the existence of any true difference.
10.2 The sequential probability ratio test
To understand the ways in which one can incorporate the presence of an optional stopping rule in a statistically valid way, we can go back to the World War II period. It was during that time that Abraham Wald and the Statistical Research Group at Columbia University operated under a contract with the Office of Scientific Research and Development. In 1943, Wald developed the Sequential Probability Ratio Test (SPRT) (Wald 1945) as part of the group's efforts to reduce the cost of assuring the quality of the different equipment, vehicles, ammunition, etc., produced during the war effort. Since a lot of industrial testing is destructive for the tested item, and takes a fair bit of effort, their goal was to minimize the number of items inspected, in order to either clear a batch of items, or to declare it unfit.
To achieve this, a pair of statistical borders are drawn, which are then compared to the sum of the log-likelihood at any given time during the test. One boundary is used to guide the decision to reject the null hypothesis and the other to accept it. Data is obtained sequentially, and the sum of the log-likelihood is calculated, and then compared with these borders. After each measurement, one of three decisions can be made - stop by rejecting the null hypothesis, stop by accepting the null hypothesis, or continue testing. The earliest tests had two parallel decision boundaries as in Figure 10.2.


Figure 10.2: A classic sequential probability ratio test.

The measurement of the effect is traditionally expressed in terms of the sum of the log-likelihood.
The average sample size required by Wald's procedure was 40-50% smaller than equivalent fixed sample procedures. This greatly reduced the cost of industrial testing, and was considered a competitive advantage of the U.S. war effort; for which reason the method was classified until after the war.
While in theory, an SPRT will terminate by crossing a boundary with probability one, it could take an indeterminate time for it to do so. Since some applications require a decision to be reached either way after a certain number of observations, triangular stopping boundaries, or so-called truncated-sequential analysis (a.k.a. "restricted sequential tests") were developed (Armitage 1957) with the following type of boundaries as in Figure 10.3.


Figure 10.3: A triangular sequential probability ratio test.

This imposes a maximum sample size: a test may stop earlier than that, but it cannot proceed for more than the maximum sample size unless data accumulation continues governed by external factors. The maximum sample size of a sequential test is always greater than that of an equivalent fixed sample test, in order to compensate for the additional statistical evaluations which would otherwise increase the type I error rate.
A sequential test has smaller average sample size but greater maximum sample size compared to an equivalent fixed sample test.
In all cases, the stopping time t becomes a random variable to be included in the statistical model as it depends on the observed data. Both tests with parallel boundaries and truncated tests support a point-versus-point hypothesis scenario, as well as a one-sided composite alternative.
Importantly, SPRT assumes continuous monitoring of the data - a statistical analysis is performed after each observation in a single sample test, or after one observation is performed in both the control and test group in an A/B test. If an analysis is not performed after each observation, for technical or other reasons, significant improvements in the efficiency of the procedure can be introduced.
In clinical trials, as well as other experiments involving humans where participants are recruited on an ongoing basis, it is often cost-inefficient to perform a statistical analysis after each observation. Furthermore, the treatment outcome is not always amenable to immediate assessment. Much the same situation is experienced in online controlled experiments. Considerations for external validity which require the data to be evaluated on more or less equal intervals of at least a day (more often at least a week) mean that a different set of sequential monitoring algorithms are more appropriate for the case of online A/B testing.
10.3 Fixed analysis time group sequential trials
Group sequential tests (GST) were developed for analyzing observations in groups of several at a time, which is what happens when data is examined once a day or once a week. An intuitive approach to the topic might suggest the Bonferroni correction to control the FWER over a set of repeated significance tests, however such a procedure would be too conservative, and thus inefficient, as it fails to account for the dependence between past and future tests. Later statistical tests are obviously partially based on data used in earlier tests.
Instead, approaches based on SPRT were developed. The idea was to split the observations into groups, and to perform a statistical analysis after each group, making adjustments to the critical values used to reject the null hypothesis at each analysis, so that an overall type I error is maintained for the entire procedure. A set of such critical values is usually called a 'stopping boundary'.
Some of the first developments in this area in clinical trials came from work in determining more efficient stopping boundaries by Pocock (Pocock 1977), Haybittle-Peto (Haybittle 1971) (Peto, et al. 1976), and O'Brien and Fleming (O'Brien and Fleming 1979). The boundaries are usually expressed in terms of the Z-score at observation k, ck. The latter had the intuitive appeal of producing Z-scores close to the ones used in equivalent fixed sample tests, and were also more conservative early on, which provided better balance between obtaining a representative sample and stopping a test early. Therefore, the O'Brien-Fleming boundaries became the de-facto standard for later medical trials.
A group in a group sequential trial is defined as a set of consecutive independent observations. After each, a statistical analysis is performed:

where K is the number of groups n1 through nK.
Consider a case with just two groups (k=2), for simplicity, where only two analyses are planned. The decision boundaries should be constructed with critical values c1 and c2 such that the overall probability across the two statistical analyses is less than or equal to the chosen significance threshold α:

which translates to:

In the case of a standard normal distribution d(X1) = Z1, ... , d(XK) = ZK.
In the general case of having equal allocation between treatments at analysis k there are nk observations in each treatment group. The Z statistic is simply:

An equivalent way of expressing this is as a sum of the stage-wise Z-scores, each of which is obtained from data gathered from a single group, divided by the square root of the number of groups examined:

 is the test statistic constructed based on data from the k-th group, should the test reach that stage. It is known that if  is normally-distributed with a given mean Δ and unit variance, as is the case in most A/B tests, then Zk being the sum of  is also normally-distributed with mean  and unit variance. If the distribution of Z1,..., Zk is asymptotically multivariate normal the calculation of the critical boundaries c1, ... , ck can be performed using recursive integration methods developed by Armitage (Armitage, McPherson and Rowe 1969). If the sequentially computed Z-scores do not have an independent increment structure numerical integration or estimation through simulations would be involved in calculating the boundary critical values. Reboussin, DeMets, Kim, & Lan (Reboussin, et al. 2000) also developed a method for calculating boundaries of various group sequential designs.
The initial work by Pocock suggested a constant critical value for all K stages: c1 = c2 ... = cK. O'Brien & Fleming suggested that the fraction of alpha spent on each stage should depend on how close that stage is to the final analysis, in a way which ensures less spending early on and more aggressive spending as the end of the trial approaches.
A comparison between decision boundaries generated by the two approaches with 5 analyses and one-sided alpha of 0.05 (4 interim and one final) can be seen in Figure 10.4. Another comparison, this time with 12 total analyses is shown in Figure 10.5.


Figure 10.4: O'Brien Fleming vs. Pocock sequential stopping boundaries for up to 5 analyses.



Figure 10.5: O'Brien Fleming vs. Pocock sequential stopping boundaries for up to 12 analyses.

It is obvious how the O'Brien-Fleming bounds achieve critical scores closer to the fixed sample size Z-score of 1.6448 at the last stage, and this is balanced by having much higher Z-score thresholds for early stages. The final analysis is evaluated at a Z-score of 1.75 in the first case and 1.81 in the second case, where the respective values for the Pocock boundaries are 2.12 and 2.30, respectively.
What is common to both approaches is the requirement for the analyses to be evenly spaced. For example, a trial with two analyses should have its first analysis at 50% of the information gathered, and the second with 100% of information. A trial with five monitoring points would have analyses performed at 20%, 40%, 60%, 80%, and 100% of the maximum sample size.
Another significant limitation is that the number and timing of analyses should be specified in advance, and could not be altered at any point. The need to relax these limitations so tests are more applicable in clinical practice led to the development of more flexible procedures called spending functions.
10.4 Alpha-spending functions and efficacy boundaries
Lan & DeMets (Lan and DeMets 1983) (Lan and DeMets 1994) proposed functions for spending alpha on a continuous basis, in order to address the limitations of fixed analysis time approaches to sequential monitoring. The so-called alpha-spending functions are continuous functions of the information fraction and the chosen statistical significance threshold. They manage the cumulative alpha spent up to a particular moment in time, and its corresponding sample size.
The major advantage of spending functions is that they allow the number and timing of analyses to change at will, while assuring type I error control even in situations where analyses are done in rapid succession; e.g. as the Z value approaches the decision boundary (Lan and DeMets 1989)(Lan and DeMets 1994, 1348).
Error-spending functions allow flexibility in the number and timing of analyses in a sequential test.
The boundaries proposed by Pocock and O'Brien-Fleming could be approximated with alpha-spending as well. The alpha-spending functions and families of functions frequently considered are listed in Table 10.2.


Table 10.2: Spending functions and families of spending functions.

The different functions are visualized in Figure 10.6, except the Hwang-Shih-De Cani family:


Figure 10.6: Pocock vs. O'Brien-Fleming vs. Uniform spending functions.

The Kim & DeMets family (Kim and DeMets, Design and Analysis of Group Sequential Tests Based on the Type I Error Spending Rate Function 1987) allows the control of error-spending through changing the rho parameter. Larger values of ρ result in more conservative spending early on and more error being available later in the trial, while smaller values of ρ shift spending in the opposite way. A power function with ρ = 3 results in boundaries very close to the O'Brien-Fleming function, ρ = 1 results in Uniform spending while ρ = 1.5 (not pictured) is about in the middle between Pocock and O'Brien-Fleming. ρ = 0.5 results in very aggressive early spending.
The Gamma family (Hwang, Shih and De Cani 1990) allows to control where most of the alpha-spending occurs through the gamma parameter. Increasing γ results in higher proportion of alpha spent early in the test, while less is available for later analyses. Decreasing it achieves the opposite with gamma less than zero being a valid input.


Figure 10.7: Hwang-Shih-De Cani family of functions.

Due to the continuous nature of spending functions, they do not result in discrete boundaries at set information fractions, but instead provide a boundary for any sample size at which a practitioner may decide to perform a statistical evaluation of a sequential A/B test.
The choice of spending function can, in theory, be different for each A/B test. Due to considerations related to external validity (discussed in Chapter 12) the author's recommendation is for using spending functions which are quite conservative early on. A Kim-DeMets power function with ρ = 3 seems to be a good compromise between ending tests as early as possible, and achieving a representative sample. The function allocates alpha at the early stages slightly more aggressively than the O'Brien-Fleming function, which is one of the more conservative approaches.
Note that neither the alpha-spending function, nor the beta-spending function, discussed in the following paragraphs can be changed after a test has been started. Changing the function mid-way would invalidate the error-control which would otherwise be provided by the monitoring procedure.
10.5 Beta-spending and futility boundaries
By using alpha-spending an experiment can be stopped earlier than the maximum planned sample size if the results allow the rejection of the null hypothesis. However, it can be as important, and perhaps even more important, in late-stage A/B tests to be able to terminate experiments before their maximum sample size, in case the test outcomes are such that the test variant is unlikely to reject the null hypothesis, based on the remaining sample size. Even more importantly, it may be desirable to stop a test early if the results are heading in a direction favorable for the null hypothesis - the variant is performing worse than the control. In both cases, we want to construct thresholds which, if reached, would mean that the test will most likely be futile in rejecting the null, and can thus be stopped early without compromising the statistical power of the test.
Similar to how peeking with intent to stop and reject the null increases the type I error rate if not adjusted for, peeking with intent to stop and accept the null increases the type II error rate, if not adjusted for.
In order to fulfill this need, the alpha-spending approach was extended by Pampallona et al. (Pampallona, Tsiatis and Kim 2001) to the construction of beta-spending futility boundaries for early termination of the experiment in favor of the null hypothesis. This transfers the flexibility of the alpha-spending functions to the type II error control. A futility boundary allows us to end an A/B test early if there is evidence that it would be futile to continue it, without compromising its desired sensitivity towards a particular minimum effect of interest. Optional stopping for futility allows A/B tests to fail fast, therefore preventing unnecessary monetary losses, and also saving resources which can be reallocated to more promising tests.
Since introducing stopping for futility decreases the overall power and increases the type II error of the test, unless compensated by a larger sample size, sample size adjustments are needed to control the false negative rate and the statistical power at desired levels. Consequently, designs with a futility boundary require a larger amount of observations if the test is continued to its end. However, they offer a significant improvement to the average expected sample size due to the ability to stop early when the results are highly unpromising or negative. This is especially useful when performing tests on a design element, page, or process which has undergone several improvements - any further improvement is much less likely, while it is more and more likely that attempts to improve the current state would result in worse variants.
There are two types of futility boundaries which can be constructed - binding and non-binding. When binding boundaries are calculated, the efficacy boundaries are recalculated to account for the possibility that a test is stopped at a previous stage, both for efficacy and for futility. When non-binding boundaries are calculated, the efficacy boundary remains unaffected, resulting in slightly more conservative type I error control.
With a binding boundary, one commits to stop the test if the observed statistic crosses the futility boundary. Failure to follow this rule would lead to an unaccounted for increase in the type I error of the test, since the efficacy boundary was recalculated to take into account the probability of stopping for futility. In case there are external considerations guiding the decision, which may demand that a test continues despite crossing the futility boundary, this cannot happen without compromising type I error control. An example of such an external factor is reconsideration of the minimum effect of interest in a downward direction, which would mean the need to give the tested variants a larger sample size in order to maintain power. Recalculating the design is one way to handle such input, but it may be easier to ignore a futility boundary crossing instead.
Binding futility boundaries mean stopping is a must, while non-binding boundaries allow us flexibility in how we handle a boundary crossing.
With a non-binding futility boundary, a crossing of the boundary serves more as a guideline than a strict rule. We are free to decide whether to stop the test or not, based on external information and considerations. The type I error would remain unaffected, as the efficacy boundary was constructed separately from the futility one. Introducing a non-binding boundary is slightly costlier in terms of sample size required to maintain power, but the flexibility it gives is usually enough to offset this drawback.


Figure 10.8: A design with efficacy and futility stopping boundaries.

Figure 10.8 is an example of a test conducted by applying both an efficacy boundary and a non-binding futility boundary using the Analytics-Toolkit.com AGILE A/B testing calculator. The test stops for efficacy with about 65% of the sample size of an equivalent fixed sample test, resulting in a 35% faster test.
10.6 Expected sample size and efficiency of sequential A/B tests
Unlike a fixed sample size test, the sample size of a sequential test is random and depends only on the continuation regions of the test, as they determine how many stages will be reached. Therefore, we usually compare fixed with sequential tests based on statistical power. However, it is more illustrative to perform a comparison in terms of sample size, for which we need to introduce the concept of expected sample size (a.k.a. average sample number, ASN).
Denoting the sample size at analysis k by nk, where k = 1, ... , K, the sample size at the time a boundary crossing occurs by N, the efficacy boundary crossing probability at stage k by αk, and the futility boundary crossing probability by βk the expected sample size for a given true value of the parameter of interest θ can be calculated using:

In a design with both efficacy (upper) and futility (lower) boundary, denote the critical value of the upper boundary at stage k by uk and that of the lower boundary by lk. The probability of first crossing the upper bound at analysis k assuming a certain true value θ is computed as:

while the probability of first crossing the lower bound at analysis k as:

k and i are defined so the following inequality holds 1 ≤ i ≤ k ≤ K. If we denote the statistical information at stage k by Ik and the information at stage i by Ii, then for the above calculations to work, these two conditions need to hold asymptotically:

As seen, the average sample size is a function of the true value of the parameter of interest (θ), usually an absolute or relative difference between two or more groups.
Let us examine how the expected sample size changes depending on the design choices involved in a sequential trial. We will only use the Kim-DeMets family of functions, in order to keep the number of graphs to a manageable level.
A comparison between the expected sample size of two tests with the same parameters, but different stopping boundaries, is presented on the figures below. The one in Figure 10.9 has just an efficacy stopping boundary, while the test presented in Figure 10.10 has a futility stopping boundary as well.


Figure 10.9: Early Stopping for Efficacy Only, α = 0.05, β = 0.10, θmei = 0.15p.p. (10% relative lift), Baseline: 1.5%, 12 planned analyses. Kim-DeMets power function alpha-spending with boundary 3



Figure 10.10: Early Stopping for Efficacy & Futility, α = 0.05, β = 0.10, θmei = 0.15p.p. (10% relative lift), Baseline: 1.5%, 12 planned analyses. Kim-DeMets power function alpha-spending with boundary 3, Kim-DeMets power function non-binding beta spending with boundary 2

It is easily visible that at the cost of a modest increase in worst-case scenario sample size (4% to 15.6%) there is a huge gain in efficiency: from 25.27% to 56.96%, on average. This is true even for a sample of possible true relative differences that is positively skewed: from -20% to +30%, so actual results may be even better.
If there is exactly zero difference, the gain is a very significant ~44% efficiency in terms of faster testing when compared to stopping just for efficacy. The efficiency gains go to over 80% if the tested variant is, in fact, worse than the control. This is especially important in late-stage testing of already heavily optimized shopping processes, landing pages, email templates, etc.
Now, compare the impact on average efficiency of using a non-binding futility boundary in Figure 10.11 versus the bounding boundary in Figure 10.10.


Figure 10.11: Early Stopping for both Efficacy and Futility, α = 0.05, β = 0.10, θmei = 0.15p.p. (10% relative lift), Baseline: 1.5%, 12 planned analyses. Kim-DeMets power function alpha-spending with boundary 3, Kim-DeMets power function binding beta-spending with boundary 2

The cost of using a non-binding beta-spending boundary is a significant increase in the worst-case sample size from +9.90% to +15.62% when compared to a bounding one. This is offset slightly by a better average sample size expectation - from 58.10% to 56.96%.
Since averages can be misleading, it is useful to consider what the actual distribution of the stopping stages is, given different true values of θ, as well as what percentage of tests go beyond the sample size of an equivalent fixed sample test.
Simulations performed by the author (Georgiev, Efficient A/B Testing in Conversion Rate Optimization: The AGILE Statistical Method 2017) can illustrate this, with δ denoting the minimum effect of interest. The test was planned for twelve analyses, and the fixed sample size would be exceeded if the experiment goes past analysis number ten. Table 10.3 presents summary results of the simulations, revealing the average stopping stage and the percentage of runs stopped after the equivalent fixed sample size.



Average stopping stage and % of tests stopped past the equivalent fixed sample size with AGILE



True Variant Lift

Average Stopping Stage

% of Tests Stopped After the Fixed Sample Size



-15% (-1⋅δ)

2.87

0.00%



-7.5% (-0.5⋅δ)

3.99

0.18%



0% (0⋅δ)

6.22

6.99%



7.5% (0.5⋅δ)

8.26

23.14%



15% (1⋅δ)

7.27

11.37%



22.5% (1.5⋅δ)

5.23

0.84%



30% (2⋅δ)

3.84

0.01%




Table 10.3: Average stopping stage and percentage of tests stopped past the equivalent fixed sample size of an AGILE A/B test.

In line with the expected sample size distribution, in Table 10.3 we see that the latest average stopping stage is observed when the true value of the parameter is between the null hypothesis and the MEI. This is also where the largest proportion of tests exceed the equivalent fixed sample size. The benefits of using a futility boundary are confirmed by the very early average stopping stages for outcomes in the direction opposite to the alternative hypothesis. In case the true absolute difference is exactly equal to the minimum difference of interest, just north of 11% of tests are expected to take longer than a fixed-sample size equivalent, with the majority stopping at about 70% of the sample size that would otherwise be required.
Averaging over a range of possible values from -2·δ to +3·δ the minimum improvement in expected sample size is 20% (at exactly 0.7·δ), while the maximum improvement is 80% at -2·δ.
A sequential test with futility and efficacy stopping boundaries can result in significantly faster tests, ranging from 20% faster to 80% faster, in certain cases.
Let us examine the actual distribution of the stopping stages for different values of θ*. For a true effect twice as large as the MEI, the test would almost certainly stop before stage eight as shown in Figure 10.12.


Figure 10.12: Runs stopped at each stage with θ* = 2 · MEI.



Figure 10.13: Runs stopped at each stage with θ* = 1.5 · MEI.



Figure 10.14: Runs stopped at each stage with θ* = MEI.



Figure 10.15: Runs stopped at each stage with θ* = 0.5 · MEI.



Figure 10.16: Runs stopped at each stage with θ* = 0.



Figure 10.17: Runs stopped at each stage with θ* = -0.5 · MEI.



Figure 10.18: Runs stopped at each stage with θ* = -1 · MEI.

With θ* half of MEI a fair number of tests continue past the tenth analysis, while only a few did that with θ* equal to zero.
Now that we have seen the expected improvements in expected sample size from using sequential testing procedures, let us see some of the complications arising from their use.
10.7 Estimation following a group sequential A/B test
With fixed sample tests, the statistical estimates: p-value, confidence intervals, and maximum likelihood estimate are unbiased regardless of the true value of the parameter of interest θ*. When it comes to deciding to reject or fail to reject the null hypothesis based on a predefined significance threshold, a sequential analysis is no different than a fixed sample test. The difference is that the estimates are no longer unbiased, and no longer have minimum variance. p-values, confidence intervals, while the MLE are all biased depending on the stopping stage and the distance of the observed test score from the stopping boundary it crossed, which in term depend on θ* (Fan, DeMets and Lan 2004) .
The bias can be split in two distinct types - unconditional bias or bias over all possible stopping stages, and bias conditional on the stopping stage. The unconditional (marginal) bias is usually small, and can be compensated through unconditionally unbiased estimators. It is also usually not what we are interested in since, once a test is stopped, the stopping stage is known and informative of the likely effect size. This information should be incorporated, in order to address the conditional bias, which is very high in tests stopped either early or late. Figure 10.19 depicts the bias of the observed difference in proportions as estimated through simulation of a twelve-stage sequential trial, with efficacy and futility boundaries.


Figure 10.19: Absolute and relative average bias of point estimate by stopping stage.

It is obvious that reporting the observed difference in proportions from A/B tests stopped in the first six stages would result in overly optimistic expectations, while if a test stops late the true difference will be underappreciated. In fact, for tests stopped at stages 1 through 3, the observed difference has a positive bias versus the actual one measured in the multiples.
The bias of p-values, CIs, and MLE follows a similar curve. And what's worse, the statistics will become skewed in the wrong way in terms of conditional bias when an estimator which corrects for unconditional bias is used.
Many methods for producing conditionally unbiased estimates following group sequential experiments rely on a so-called 'stagewise ordering' of the sample space. Such ordering reflects the fact that tests stopped earlier suggest a larger true effect size than ones stopped later, and among tests stopped at the same stage those with higher Z scores suggest a larger true effect size than ones with lower Z scores. This ordering can be illustrated as shown in Figure 10.20.


Figure 10.20: Stagewise Z-score ordering.

A counterclockwise ordering of the sample space results in priority being given first to the boundary which was crossed, then to the stopping stage, and finally to Zk on termination of the experiment.
It should be noted that, while widely adopted, this ordering is neither the only one possible, nor the best. Since the outcome is bivariate (stopping stage & test statistic), and the likelihood ratio is not monotone for such a sample space, there is no ordering which produces a uniformly most powerful test. However, it has the advantage of ensuring the resulting p-value is less than the significance threshold α of the test, if and only if, H0 is rejected. Another advantage is that it ensures that the test does not depend on either information levels or group sizes past the observed stopping stage k.
As a p-value has to provide the probability of observing an outcome as extreme or more extreme than the one observed under the null hypothesis, in the bivariate case, encountered in sequential designs, H0 can be framed in terms of both the parameter of interest θ and the stopping stage k, so it becomes the probability of observing (θ, k) as extreme or more extreme than the observed. The p-value adjusted via stagewise ordering of the sample space is therefore the probability of stopping earlier than the observed stopping stage, combined with the probability of stopping at that stage with a difference larger than or equal to the one observed.
This is simply the cumulative exit probability for stages 1...k-1 given the specified boundaries c1... ck and the probability of the observed statistic at stage k, zk:

The Z-value sample space ordering was found to be optimal compared to several alternatives by Chang et al. (Chang, Lawrence Gould and Snapinn 1995).
In case there is a binding futility boundary the calculation of exit probabilities needs to include the probability of stopping for futility as well as for efficacy.
In terms of confidence intervals, there are several proposed methods for computing marginally unbiased intervals with exact or approximate coverage probability. Each of these attempts to reduce bias and achieve near-exact coverage with minimum variance and mean square error. Other desirable properties are the inclusion of the MLE in the interval, as well as the exclusion of the true value if and only if H0 is rejected, which is not always the case for some intervals based on stagewise ordering.
Exact confidence intervals (ECI) based on different types of sample space ordering were proposed by Tsiatis et al. (Tsiatis, Rosner and Mehta 1984), Kim & DeMets (Kim and DeMets 1987), Emerson & Fleming (Emerson and Fleming 1990), Rosner & Tsiatis (Rosner and Tsiatis 1988) and Chang (M. Chang 1989). Repeated Confidence Intervals (RCI) were proposed by Jennison & Turnbull (Jennison and Turnbull 1989), and while straightforward to compute, they result in wide bounds and are too conservative. The performance of these methods in terms of conditional bias is unknown.
To address the issue of conditional bias Strickland & Casella (Strickland and Casella 2003) proposed an Exact Conditional Confidence Interval (ECCI) which, while marginally exact and both marginally and conditionally unbiased, does not make use of all the available information. This results in intervals which are not conditionally exact and are unnecessarily wide, usually wider than those produced by unconditionally unbiased intervals. The difference between nominal and actual stagewise coverage becomes bigger as the number of planned analysis increases.
Fan & DeMets (Fan and DeMets 2006) propose a Restricted Conditional Confidence Interval (RCCI), in order to address the shortcomings of the above approaches. It is computed as the intersection of ECCI and a 1-β confidence interval, based just on the stopping stage k. Since RCCI results in intervals almost as narrow as ones produced by unconditionally unbiased interval methods, while at the same time maintaining coverage probability very close to nominal, it is recommended that such intervals are used whenever possible. It should be noted that the resulting interval is usually asymmetric about the observed value and the MLE.
When it comes to point estimation, a.k.a. maximum likelihood estimation, there were various methods proposed including the bias reduced method by Whitehead (Whitehead 1986). It is, however, dependent on future number, timing and boundary values, making it difficult to justify its use. Emerson and Fleming (Emerson and Fleming 1990) proposed an unbiased estimator, which was proven to be sufficient, but not complete by Liu & Hall (Liu and Hall 1999). This method is, however, only marginally unbiased, so it will result in an unadjusted MLE if a test stops at the first planned analysis, which is very counterintuitive as this is also the stage where the greatest conditional bias is present.
A future-independent modification of the Maximum Conditional Likelihood Estimate proposed by Fan, DeMets & Lan (Fan, DeMets and Lan 2004) addresses these concerns, and produces a conditional bias reducing estimator by taking into account both the discrete stopping rule and the overshoot of the test statistic at the point of stopping - that is how much above or below a boundary it is. Its smaller standard deviation and mean square error make it a desirable estimator, despite the slightly increased conditional bias if stopping on the very first analysis.







Chapter 11
OPTIMAL SIGNIFICANCE THRESHOLDS AND SAMPLE SIZES
Until this point in the book, there was an implicit assumption that the statistical significance threshold, the statistical power, and the minimum effect of interest (and consequently the sample size) were determined according to some sound procedure, before proceeding to calculate the sample size based on these parameters.
This is more or less the approach taken in scientific applications of statistical methods, where the 0.05 significance threshold is used by convention in many disciplines, even though there are a few somewhat misguided proposals for this threshold to be raised to a different arbitrary threshold. Some fields have more stringent standards, as is the case in physics where some discoveries are confirmed only if the result is 5 or 6 standard deviations apart from the null, which corresponds to really tiny p-values. Certain industrial quality control applications also take a similar approach with one of the common conventions; the so-called '3 sigma rule'.
The minimum effect of interest is usually quite subjectively calculated, and is often incorrectly equated to the expected effect size, instead of calculating an effect size of, for example, clinical importance in a medical trial. Similar to the standard p-value threshold of 0.05, statistical power is often required to be 80% for the chosen minimum effect of interest.
The online A/B testing community is no different, with advice in the form of "always aim for XX% significance" or "don't stop a test until it reaches YYY conversions" being common. Others advocate for coming up with as many variants to test against a control, as possible, in order to improve the chance of finding the best option. What unites such advice is the assumption that there is a one-size-fits-all solution which works in most if not all situations.
This book also used these conventional values in a number of examples to avoid questions like "but why is the significance threshold 50%/80%/90%/99%/99.9%?", "why is this a 50%/80%/90%/99%/99.9% confidence interval instead of a 95% one?", or "why we calculate the sample size aiming to achieve 90% power for the selected MEI?" before we are equipped to address them. However, it is time to break the mold of these conventions, and to take a deep look into what these thresholds actually mean and whether a 'conventional' significance threshold or power level even makes sense in a world where no two A/B tests are alike.
Business A/B testing, both online and offline, is a discipline almost uniquely positioned to employ such calculations with some rigor. The estimates of the potential overall impact of any decision can be very accurate and reliable as there is a well-defined set of involved parties and a short duration of the effect, usually no more than a decade. This is unlike applications in science where the outcome may have lasting effects on an unspecified group of businesses and individuals hundreds, and even thousands, of years into the future.
I argue that due to these circumstances the appropriate significance threshold and sample size of each experiment can be calculated based on balancing business risk and reward in an optimal way; optimizing the return on investment of a whole A/B testing program. We will start by exploring the different costs and benefits involved in A/B testing, then proceed to examine how taking these into account can lead to custom statistical design and better return from A/B tests.
11.1 Defining "success" for a business experiment
Firstly, let us properly frame the discussion by establishing what determines if a particular A/B test is successful or unsuccessful from a business perspective. While in science any properly conducted experiment can be deemed a success as it increases human knowledge, regardless of its outcome, businesses are dealing with a somewhat different situation.
At first thought, one might decide that the only possible type of question we want to get an answer to from an A/B test is "what action should we take?", e.g. should we implement this new shopping cart software or not, should we make this design change or not, should we use this copy or that other one?
However, what we should be asking instead is: "based on our finite resources, what is the action we should take to improve X in the most cost-effective, sustainable way?". X here stands for any key performance indicator we can think of, such as conversion rate, or average revenue per website user.
The question we should ask ourselves before conducting an A/B test is: "Based on our finite resources, what is the action I should take to improve KPI X in the most cost-effective, sustainable way?"
Before taking any substantial business action, we should be asking ourselves the following key questions, the answers to which should incorporate all risks involved:
What is the total revenue which could be impacted by our planned action?
What is the expected positive effect from the action?
What is the expected total cost, fixed and risk-related costs included, from that action?
By optimizing the first two, and minimizing the third we will be getting closer to an optimal return on investment, defined as:

From the above we can easily see that optimal ROI is not equal to generating the best possible improvement in a given KPI such as purchase conversion rate. It is entirely possible to achieve a large effect, but if the cost is equally large or larger, this decision might be worse than making several less costly changes which do not amount to such a big effect, but are much less costly overall. Increasing the impact of a change will produce a positive effect on ROI only if it can be achieved at a reasonable cost, including risk costs.
For example, if we can achieve an effect of 2% lift, which impacts $1,000,000 of revenue at a cost of $5,000, then the ROI of that action would be 0.02 · 1,000,000 / 5,000 = 4, or 400%. The risk/reward ratio will be Cost / Effect · Impact = 1/4 or gaining $4 for a cost of $1. To improve this, we can either increase the effect size, the impact, or decrease the cost, or a combination of these. If we can achieve 2% lift which only impacts $500,000 in revenue, but costs only $2,000 to test and implement, then ROI will be 5 or 500%, and the ratio 1/5. On the other hand, if we can achieve 4% lift which impacts $1,000,000 at a cost of $8,000, we will again get better 'bang for our buck' compared to the initial scenario, since we will achieve an ROI of 5, or 500%, R/R = 1/5.
The above might make it seem that ROI estimation is an easy task, but the only easy part is estimating the impact, which involves simply tallying the total revenue which could be impacted by an action. Estimating the effect size and the costs is much more complicated, as we need to take into account all of the different fixed and risk-related costs and benefits. The estimation of the marginal utility of a particular A/B test answers questions like "should we test this?", "should we skip testing and release immediately?", "for how long should we test this?", "are we taking on too much risk with this test?", and ultimately "how much is this A/B test worth to us as a business?".
ROI-optimal tests achieve balance between risk and innovation, allowing progress to happen under controlled risk.
If we have derived an ROI-optimal design then there are four possible outcomes of a test.
Firstly, if we fail to find a variant better than our control, and there truly isn't a better variant among the ones that we tested, we know that we tried to innovate, but our safety measures made sure we did not spend too much time testing suboptimal variants, and we can move on quickly.
Secondly, if we fail to find a better variant, but there is indeed one among the tested, then we can safely proceed knowing that it was not good enough to warrant more efforts in detecting it.
Thirdly, if we identify a better variant, but it is not truly better, then we know that even though we will not be gaining, or might even be losing, from going forward with it, the risk of that happening is tolerable given our circumstances.
And, finally, if we determine that a variant outperforms the control and it truly does, then we know we did so as quickly as possible while observing our level of risk-tolerance.
In this sense, the success or failure of an A/B test does not depend on its outcome, instead, it is determined by the fact that its design is ROI-optimal.
This does not mean that performing an A/B test is always the best solution. If the risk/reward ratio is worse than 1/1, then performing the experiment would be worse than simply taking the action which would otherwise be informed by it. However, in all cases where a test with ROI-optimal statistical design does get performed with good data and sound statistics, it is a positive for the business in the long run, regardless of its particular result.
A ROI-optimal test is a positive for the business in the long run, regardless of the outcome.
Designing ROI-optimal A/B tests is where experience really plays a big role, and a veteran conversion rate optimization specialist can shine compared to a novice. Finding the fine balance between managing risk and not stifling innovation begins by identifying all the costs and benefits involved.
11.2 Costs, benefits, risks, and rewards in A/B testing
In order to identify the different types of costs and benefits which can be accrued in the course of an A/B test, we need to examine the timeline of a typical test. The process of A/B testing can incur costs during the planning and development phase, during the test itself, as well as after a test has completed and the preferred solution has been implemented. We can dub these planning costs, execution costs and implementation or exploitation costs.


Figure 11.1: Costs on a timeline of a typical A/B test.

These costs can be further divided into two major types - fixed costs (one-time or perpetual) and risk-related costs. Consequently, when speaking of the latter in monetary terms with the risk-factor included, we will refer to them as risk-adjusted costs.
The benefits of A/B testing accrue only during the duration of a test, or after the implementation of a tested variant.


Figure 11.2: Gains on a timeline of a typical A/B test.

We can refer to these as 'execution benefits' and 'implementation or exploitation benefits', respectively. As with costs, benefits are divided into fixed benefits (one-time or perpetual) and risk-related benefits. Similarly, we will use the term risk-adjusted benefits or risk-adjusted rewards when speaking in monetary terms about benefits with a probability factor incorporated into them.
Note that fixed in 'fixed costs' refers to the fact that these costs or benefits are with fixed probability of 1, regardless of the A/B test outcome. It does not mean their amount is fixed. Obviously, a monthly subscription for a testing software or statistical analysis software is a fixed cost in terms of the probability of it being incurred. However, its total amount is variable, and depends on the length of the period for which this cost is relevant.
There are no fixed benefits, since if any benefits could be enjoyed regardless of the outcome of the test, they would be unrelated to the A/B test.
Fixed costs include, for example, the price of the testing software (whether third-party or in-house), of the analytical software, of the expenses for design, development and quality assurance for each test variant, of hours spent in meetings, of external or internal CRO and UX talent, and including the work required to analyze the results and write up reports. For trivial tests these might amount to several hundred dollars or less, while for complex tests they can reach many thousands of dollars, with no real upper limit. All the above costs would be incurred regardless of the outcome of the test.
Risk-related costs include any type of expenses which depend on the actual or perceived performance of the tested variant(s). The actual performance is never truly known, but we can use our prior experience to make informed guesses about it, which we can express as a distribution for the expected effect sizes. This can be used for hypothetical calculations. The perceived performance is only known after the test has been completed.
Examples of risk-related costs include the cost of implementing a variant which is in fact inferior to the control, as well as losses due to exposing a part of our user base to one or more inferior variants during a test. Technical expenses incurred due to the implementation of a superior variant, or an added maintenance cost incurred by the chosen variant, are also examples of typical risk-related costs.
To make it more concrete, a winning variant might, in fact, increase the conversion rate or average revenue per user. However, it might carry with it the burden of hiring more customer support staff, more development time to maintain the solution, larger costs for external providers, larger hosting costs, or a combination of the above. For example, we might think it is a great idea to have 360-degree shots of each product on our store, and an A/B test might confirm that is the case. However, making 360-degree shots means spending more on photography and photo equipment, photo processing, and website hosting, as well as requiring a third-party software with which to stick the images together for a 360 view.
Note that costs from testing or implementing inferior variants can vary a lot from test to test - a variant might only decrease conversions by a couple per week, but it might also cause them to drop by one half, or even fully stop in case of accidentally testing a variant which completely breaks the user experience due to a critical bug, resulting in a 100% decline in conversions.
An important cost which can skew the risk/reward calculation significantly is the cost of reversing a decision which is later discovered to have been made in error. Some decisions are easy and cheap to reverse, such as trivial interface, design, or copy changes. However, others may have far-reaching consequences, and may be near-impossible to reverse. For example, if the results of a test inform changes involving complicated third-party relationships, very high infrastructure investments, wide-reaching branding changes, and so on, the reversal may be impossible due to PR, technical, legal, or other reasons. In these cases, a monetary cost for reversing the decision should still be arrived at for the sake of computation, and it should be large enough to encapsulate these risk costs.
Finally, there are also opportunity costs - failure to detect a truly superior treatment thus not being able to gain from the improvement which would have been realized if the solution was implemented without an A/B test, or if it was tested with a more relaxed significance threshold. If an A/B test is underpowered then it will incur an opportunity cost with greater probability, but even a properly powered test still has some probability of resulting in such costs. Opportunity costs can be made worse by using a statistical null hypothesis which does not correspond to the substantive hypothesis. A common issue of this type is to define a point null and a two-sided alternative, when in reality the null is one-sided. This leads to a 20-60% loss of efficiency, and a corresponding increase in opportunity costs.
Risk-related benefits are of two types. Firstly, there is the increased revenue or leads during a test, or after implementing a winning variant, if it is truly superior to the control. Secondly, there might be benefits not measured by the primary KPI, such as a reduction in technical or manpower maintenance costs compared to the control, regardless of the final result on the primary KPI.
For example, we might be A/B testing the switch from a sales funnel which pushes users through a free trial period to one which does not. If the former required the constant involvement of several support staff to answer questions and to cancel the accounts of trial users, then implementing the latter means that several support staff can be let go, resulting in significant money savings.
Both exploitation costs and exploitation benefits need to be put in the context of the time period over which we expect the implemented variant to provide value. If, for example, we know that in 6 to 12 months there will likely be a full redesign of the website we are testing on, both the cost of implementing an inferior variant and the benefit of choosing a superior one are limited by that short time frame. Consequently, the level of scrutiny can be reduced. The significance threshold should not be the same for tests with vastly differing time frames of impact. For example, the approaches for a test for which the result is expected to last for six months, and for a test, for which the conclusion is expected to shape UX for the next 5-6 years, should be completely different.
11.3 Test parameters and their relationship to costs and benefits
Now that we understand how a successful A/B test is defined in terms of the ratio between risk and reward, and the different types of costs and benefits related to an A/B test, we can proceed to see what parameters we can tweak in an A/B testing design in order to achieve an optimal risk/reward ratio.
The two statistical parameters that we can adjust are the statistical significance threshold and the sample size. The sample size defines both the statistical power and the minimum reliably detectable effect, and is often synonymous with the test duration, since we have a given daily / weekly / monthly number of users coming to the site.
In terms of the design of the experiment, we can alter the definition of the null hypothesis, the number of variants we test, as well as the monitoring scheme. As already discussed, the null hypothesis can be that of superiority, strong-superiority, or non-inferiority. The monitoring scheme can be a simple fixed sample test, or a test with sequential monitoring. Increasing the number of variants past one makes it an A/B/n test, with all consequences related to that.
The parameters we can influence are:
1.   Statistical significance threshold
2.   Sample size / test duration
3.   Monitoring scheme
4.   Number of variants
5.   Null hypothesis
Other than manipulating these parameters, ROI can be improved by reducing the cost of testing by optimizing our processes, using more cost-efficient tools, having a good testing infrastructure in place, and so on. Obviously, we can also improve our probability of success by testing options which justify more optimistic distributions of the expected effect size. For the purposes of this book, however, we will only focus on the above 5 parameters.
Statistical significance is the first parameter one should consider changing on a per-test basis. The level of statistical significance has a two-fold effect. On one hand, lowering the tolerable type I error increases the required sample size, assuming we want to keep our statistical power and minimum effect of interest fixed. This leads to an increase in risk-costs during testing, as it increases the number of users exposed to potentially inferior treatments. On the other hand, it decreases the risk of exploitation losses, since the higher the threshold is, the less likely we are to adopt an intervention that will hurt our results post-implementation. For example, if a decision is very costly or near impossible to reverse, the significance threshold should be increased accordingly.
Changing the sample size, assuming the significance threshold remains the same, means that we are decreasing the power of the test for any given effect of interest. This results in an increase of risk-related costs during the test, as well as an increase in the probability of missed opportunities to adopt a superior solution. A less sensitive test means that we are much more likely to miss out on a smaller effect, if it truly exists.
The significance threshold determines the limit of post-implementation risk, while statistical power limits the potential post-implementation gains. Increasing the sample size increases the potential costs incurred during testing.
In terms of the statistical parameters of an A/B test, we can only influence risk-related costs and benefits. More specifically, we limit the risk of post-implementation losses through the level of statistical significance, and we limit the potential gains after implementation by increasing the statistical power (and thus sample size). Decreasing the significance level, or increasing the power, leads to an increase in the potential losses during testing by prolonging it due to the increased sample size.
Changing the monitoring scheme from fixed sample to sequential testing means limiting the risk of exposing users to a variant which performs significantly worse than the control, as well as the risk of running a test for too long; delaying the adoption of a better-performing variant. A sequential monitoring scheme effectively guards us against poor predictions for the treatment effect size.
On the negative side, sequential monitoring can potentially result in longer tests in certain cases. However, looking from the perspective of an entire testing program, sequential monitoring has an entirely positive effect on the ROI of testing. In many realistic scenarios it can improve the overall risk/reward ratio by 50%-100%, although more modest improvements of 20-30% were also observed.
Increasing the number of variants in a test leads to the need to increase the sample size, if power is to be maintained across the range of effect sizes of interest. This has all the usual effects associated with an increase of sample. However, more variants mean less users experience the control, so if the variants are in fact better performing, this increases gains during testing. This, however, is under the assumption that the variants have a similarly sized positive impact. If that is not the case, the effect can be near zero or negative.
Changing the null hypothesis affects the minimum reliably detectable effect and the power of the test, and therefore the sample size required to maintain them. Depending on how it is changed, this can have opposite effects. Since the null hypothesis should reflect a business question of estimation, it is usually mostly predetermined. However, changing the null hypothesis to reflect the increased fixed or risk-costs of testing can be done as part of the process of arriving at the optimal design parameters.
The trade-offs between these five variables is the main thing which needs to be balanced in order to arrive at a ROI-optimal design. Given that the optimal monitoring scheme is almost always sequential monitoring, and the last two are mostly externally informed, the two parameters to focus on are sample size and significance threshold.
The calculation of optimal values, however, results in feedback loops, necessitating an iterative approximation approach to computation. For example, decreasing the sample size reduces the costs and benefits during the test, but increases the risk of a false negative, thus reducing the benefit of the test, which in turn results in less justification for committing the sample size in the first place. That is up to a certain breakpoint where the relationship reverses.
Examining Figure 11.3 of an interface of a tool that the author developed in order to perform these calculations should help put all the necessary parameters in context (the tool can be found at https://www.analytics-toolkit.com/ab-test-roi-calculator/).


Figure 11.3: Interface of a risk/reward calculation software.

As can be seen on the screenshot, before this computation becomes possible, a distribution of the expected effect sizes, or 'Expectation distribution' as it is called in the above interface, needs to be defined.
11.4 Distribution of expected effect sizes
We have examined the different types of costs and benefits involved in A/B testing, and categorized them based on when they appear in the timeline of a typical test, as well as on whether they depend on the outcome of the test. There is one missing ingredient before the actual cost/benefits calculation becomes possible - we need to know the actual performance of the tested variant(s), which is, however, unknown. The only possible workaround is to have some expectation for the outcome of the test, expressed in the form of a hypothetical probability distribution, which can then be used to calculate risk-adjusted costs and benefits.
This means that we are faced with the incredibly unintuitive task of expressing our subjective perception of the test and our objective prior knowledge (e.g. based on testing similar solutions on the same website or on tests of the same solution on other website) in the form of a probability distribution.
In a typical A/B test for difference in conversion rates this expectation can be framed in terms of the expected relative difference between the control group and the tested variant(s); e.g. as percentages ranging from -100% to +∞, but more realistically to +100% to +300%. Let us say the task is to test the removal of a coupon field, which is standard for many ready-made shopping cart solutions. In this hypothetical scenario, there have been five such tests on different websites in the past, resulting in an increase in conversion rate of 1%, 3%, 3.4%, 4.2%, and 5%. The data is too little to assess the shape of the distribution, but assume for the sake of argument that it is roughly normal. The standard deviation can be estimated to be 1.5%, and given the mean is 3.33%, we can plot the standard Normal distribution shown in Figure 11.4.


Figure 11.4: Expectation distribution with σ=1.5% centered on 3.33%.

However, this is not taking into account the information that the website currently being worked on is very similar in terms of product category and demographics profile to the one in which we observed 1% lift. Maybe we should shift our expectation more towards 1%? On the other hand, if this site is more active with its coupon codes, and they are already floating around on coupon-sharing websites, it may make it more similar to the sites with similar coupon policies, which saw 4.2% and 5% improvement in conversion rate.
This is what is known as the reference class problem, an insightful discussion of which can be found in Hájek (Hájek 2007). Including too few factors ensures that our expectation is more generic. It is based on loosely grouped information. However, increasing the specificity also reduces the amount of available information. In the extreme case, there is no reference test as each test is unique, usually in more than one aspect. Still, a compromise must be made for a cost/benefit calculation to become possible. A distribution should be selected so that it represents the available empirical data and expertise.
A tempting, seemingly conservative approach, is to posit a uniform distribution of the expectation with the probability smeared across the whole range of possible values between -100% and +∞. Many interpret a uniform distribution as equivalent to stating "I do not know", or "I have no particular expectation", but that is wrong, since such a distribution is actually highly informative. Applied to the above example, using a uniform distribution is equivalent to saying that we have equally high expectation that the removal of the coupon field from the checkout form will cause a decline of -100%, will have zero effect, and will increase conversion rate by 100%. This is obviously not a statement any conversion rate expert is going to put their name under. Sure, these three events are all possible, but this is not the same as them being equally probable.
One way to be conservative is to construct an expectation distribution symmetrically around zero. This way we are saying that effects in both directions are equally likely. The next thing is to choose a standard deviation, so that the range of probable results is reasonable, given the intervention at hand.
For example, if testing textual or visual changes to a Call-to-Action button, we can reasonably expect a modest effect size, so maybe a standard deviation of 5% is reasonable. This results in a distribution of the shape shown in Figure 11.5.


Figure 11.5: Expectation distribution centered on 0% with a standard deviation of 5%, scaling of 1.

Note that we have a high relative probability that the true effect is exactly zero. However, if the change is more dramatic, such as redesigning the whole checkout user experience, then an effect of zero is much less likely. A higher scaling parameter value, as well as larger standard deviation of the expectation distribution, would be appropriate, resulting in a graph similar to the one in Figure 11.6.


Figure 11.6: Expectation distribution centered on 0% with a standard deviation of 7%, scaling of 2.

Such a distribution allows for much larger effect sizes in both directions, with much higher probability compared to the one above it.
Note that the term 'prior probability' does not suggest the calculation of a 'posterior probability' as in some Bayesian approaches. In the current approach, the probability distribution is used merely to determine the statistical parameters of a test which is fully frequentist, and has the usual interpretation based on the resulting p-values or confidence intervals. In other words, the expectation distribution has no influence on the statistical estimates.
The author sees the current approach as similar to Abraham Wald's decision-theoretic one, although it was not influenced by it. The similarity is that in Wald's approach, a loss function is defined on the value of the parameter of interest θ*, meaning that it is only operational in hypothetical scenarios when evaluated for all θ in Θ, since  θ* is unknown in practice. It is also the case that the expected loss assigned to each value of θ has nothing to do with the actual process of learning from data about θ* (Spanos 2017).
11.5 Calculating risk/reward ratios and key points
Now that all the factors at play have been laid out: the parameters we can influence, as well as the need to define a distribution of the expected effect size in order to make the calculations possible, let us see how they pan out mathematically.
A natural first step towards calculating ROI is to establish our fixed-cost break-even point, which is usually a very small percentage relative lift, often way below 1%, since the cost of preparing and running most tests is small relative to their potential impact over time. The break-even point is, therefore, the point at which ROI reaches 100%, and the risk/reward ratio is 1/1. It is very rarely at 0% lift because of the costs of testing to which we commit, regardless of the performance of the tested variant or the outcome of the test - the so-called fixed costs mentioned earlier. The point will be at zero only if there are side benefits of the tested treatment which exactly balance the testing costs.
We know whether we are looking at a test that produced a winner or not by comparing the estimated performance of the tested variant to the estimated break-even point. Consequently, without calculating this point, we have no idea if implementing the intervention would be a business positive, since an improvement on a certain metric says nothing about the costs associated with its implementation and maintenance, which are incorporated into the break-even calculation.
Once the break-even point is known, the test can be designed around it.
Calculations below assume testing for a binomial metric, such as purchase conversion rate, but can be adopted for metrics such as average revenue per user.
In order to calculate the total cost, one needs to know:
•   the costs of testing incurred regardless of outcome or the true effect (Costf);
•   the baseline conversion rate;
•   the number of users available for inclusion in the test in a period which is considered one business cycle (usually a week, can be day, if daily variance is negligible, or even a whole month);
•   the revenue for the same period;
The calculation of total risk adjusted cost is performed by integrating over the expectation distribution and subtracting the fixed testing costs:

Where a and b should be the lower and upper range of the expectation distribution and fc(θ) is a function returning the change in revenue during testing under a true effect θ* equal to θ multiplied by the expectation density at θ plus the post-test revenue impacted by the test outcome multiplied by θ, POW(T(α); θ), and the expectation density at θ:

Costd(t) are costs that depend on adopting a variant which do not depend on the true effect size x. t is the expected duration during which these costs will be incurred. The result of calculating fc(θ) is always less than or equal to zero. Note that the revenue during testing also depends on the sample size and therefore power.
The calculation of total risk adjusted gain requires a similar integration and initial parameters, but instead of estimating the costs depending only on the decision to adopt a variant, estimation of the gain achieved by adopting the tested variant regardless of its true effect size over an expected period of exploitation t (Gaind(t)) is required instead.

Where:

The risk/reward ratio is then simply:

In the simplest case with no fixed costs or decision-dependent costs and gains, the results can be graphically plotted, as shown in Figure 11.7.


Figure 11.7: Overall risk/reward calculations displayed as a graph.

Through integration, we calculate the area of the shape colored in dark-grey for the risk-adjusted loss, and the area of the shape in light-grey for the risk-adjusted gains. The ratio between the two areas is the risk/reward ratio of the test RR.
The calculation can be broken down by displaying only the risk/reward ratio for the duration of the test as shown in Figure 11.8 or only the risk/reward ratio for the exploitation period as shown in Figure 11.9.


Figure 11.8: Risk/reward graph for the test duration.

In this case, the risk/reward ratio during testing is 1/1 since the expectation distribution is symmetrical around zero. The post-implementation ratio is 1/42 due to the high significance threshold used, and the particulars of the expectation distribution.


Figure 11.9: Risk/reward graph for the exploitation period.

A uniform prior can also be used to explore the effect of the power function in isolation, as shown in Figure 11.10.


Figure 11.10: Risk/reward graph with uniform expectation.

The graphical display allows us to easily explore the three key points in a risk/reward calculation - the probability-adjusted break-even point is where the Revenue black line crosses the x-axis, the maximum probability-adjusted loss point is the lowest point on the Revenue line to the left of the break-even point, whereas the maximum probability-adjusted gain point is the highest point to the right of the break-even point.
Decision dependent benefits or costs which are independent of the exact value of θ* are added at the integration stage, as they still depend on the expectation density and statistical power. Fixed costs associated with testing are added to the total costs after the integration, since they are independent of both the true value θ* and the outcome of the test and its associated decision. Fixed costs of noticeable size make the graphical display more awkward, since the ratio between the two areas no longer represents the risk/reward ratio. My recommendation is to then display the output using lines only, and then present the risk/reward ratio as a separate graph, perhaps as line bars or rectangular bars. This should ensure that anyone examining the graph is less tempted to compare the areas.
The calculation becomes more complicated if a sequential monitoring scheme, such as AGILE A/B testing, is introduced, since then both the test duration and the exploitation period are not fixed, and instead depend on the unknown true value of the parameter of interest θ*. Given the expectation distribution, an average duration can be calculated and used to estimate the revenue at risk during and after testing, making the calculations possible.
Increasing the number of variants introduces additional assumptions in calculating the sample size, and may in some cases necessitate the introduction of a separate expectation distribution for each tested variant. This further complicates the math, but the logic of the computation remains the same.
A sample size and significance threshold which maximize the risk/reward ratio of the test under specified expectation distribution and null hypothesis can be estimated through iterative approximation.
The most important consequence of having the ability to compute a risk-reward ratio is that, by iteratively adjusting the sample size and significance threshold, one can arrive at an optimal set of parameters. They are optimal in the sense that they maximize the risk/reward ratio, and thus result in a ROI-optimal test.
Taking into account the particular circumstances of each experiment means escaping the 'one-size-fits-all' solution of using conventional 95% confidence (0.05 significance) threshold, and calculating sample size to achieve 80% power for a more or less arbitrarily chosen minimum effect of interest. It allows us to choose a combination of sample size and significance threshold (and therefore power and MEI) at which the A/B test has a positive effect on the business bottom line, regardless of its outcome, as explained in the first part of this chapter.
11.6 Testing with 50% confidence threshold?
To illustrate risk/reward analysis with a curious example, consider running a test with a significance threshold of 0.5 or 50% confidence threshold. This would be absolutely unheard of in a scientific context, but can it make sense in a particular business case? Even if it does not, how bad is it actually? Is it a coin flip? Is the post-implementation risk/reward ratio of a test with 0.5 significance threshold 50/50?
Assume what is tested is a brand-new intervention, which is not that drastic, so the expectation distribution is centered on 0% lift with a standard deviation of 4% and a scaling ratio of 1 for a Standard Normal distribution. The expectation distribution is thus symmetrical around zero, meaning there are 50/50 prior odds for the intervention to succeed or fail. Further assume zero cost for testing and other externalities and a fixed sample-size test for the sake of simplicity.
18,000 users per week and 10% baseline conversion rate resulting in $40,000 in weekly sales results in the graphical representation of the overall risk/reward balance shown in Figure 11.11:


Figure 11.11: Risk/reward analysis for the exploitation period for a sample test conducted at 50% significance threshold.

Some summary data is present in Table 11.1 and key points are displayed in Table 11.2.



Risk/Reward Analysis



Metric / Scope

Overall

Without Testing

During Testing

During Exploitation



Risk/Reward Ratio

1/13.57

1/1.00

1/1.00

1/27.78



Marginal Improvement

1,217.41%

-

-

-



R/R Adjusted Gain

$2,467

$2,655

$102

$2,364



R/R Adjusted Loss

-$187

-$2,655

-$102

-$86




Table 11.1: Details of a risk/reward analysis.




Risk/Reward Analysis



Metric / Scope

Probability-Adjusted Break-Even

Max. Probability-Adjusted Gain

Max. Probability-Adjusted Loss



Revenue

$0

$38,504

-$4,661



% Relative Lift

0.00%

4.07%

-1.23%



% Power

50.00%

99.46%

21.73%




Table 11.2: Key points from a risk/reward analysis.

Even though the statistical significance threshold is 0.5, corresponding to a 50% confidence interval, the risk/reward ratio is better than 1/1. In fact, it is much better at 1/13.57 in this particular case, resulting in a marginal improvement of the risk/reward ratio of 1,217% compared to a case in which this treatment is implemented without testing it. Why is that?
The answer can be found in the power function. It expresses the probability of observing a statistically significant result at the 0.5 level over a range of possible true effects, and quickly goes to 0% probability as we go into negative effects territory. This means that even with a threshold of 0.5 we are very unlikely to see a significant outcome, if the result is more than a couple of percentage points below 0.
Does this mean we should start testing with 50% significance? Probably not, as this will very rarely result in an optimal risk/reward ratio. It is not the optimal threshold in this example either, but it should help us more readily consider values different to the conventional 0.05/95%, depending on circumstances.
11.7 Inherent cost of A/B testing
Armed with the risk/reward computation algorithm provided above, we can take a sober look at the inherent costs of A/B testing, which are similar to those of any other risk management solution. If one has any experience in the financial markets, then one can think of A/B testing as hedging, in that it limits business risk at the cost of limiting the business' profit potential.
Limiting business risk always results in a reduction in the business profit potential.
Why does that happen? It happens due to the fixed and probabilistic costs associated with the experimentation process. While fixed costs are nearly self-explanatory - test design & development, testing software, analysis, man-hours, etc., - risk costs are less well-understood. Here we will examine them in some detail.
Let us look at an example scenario of a company testing a new shopping cart to checkout experience, which is expected to be in use for three years from the moment we start testing. To examine just risk costs, assume zero cost to test, implement, maintain, and reverse the decision, as well as zero savings from its introduction. For simplicity we will examine only a fixed sample test scenario with a single test variant.
In terms of the expectation distribution, assume it is centered at zero with a standard deviation of 5% lift, and a scaling factor of 1. If this e-commerce website is pushing 20,000 users to its shopping cart per week, and they have a 50% conversion rate resulting in $1,000,000 in revenue ($50 ARPU for users who reached the checkout), an optimal risk/reward ratio would be achieved if the test lasts two weeks, and has a significance threshold of 96.5% (p < 0.035). The resulting risk/reward analysis is displayed in Table 11.3.



Risk/Reward Analysis



Metric / Scope

Overall

Without Testing

During Testing

During Exploitation



Risk/Reward Ratio

1/141.02

1/1.00

1/1.00

1/4,578.38



Marginal Improvement

14,002.07%

-

-

-



R/R Adjusted Gain

$142,403

$155,588

$984

$141,411



R/R Adjusted Loss

-$1,010

-$155,588

-$984

-$1




Table 11.3: Risk/Reward analysis for a larger e-commerce website

The risk/reward adjusted loss is limited to $1,010 versus the potential loss of $155,588, which could be incurred by implementing the new checkout without testing. Most of the costs, if they occur, will be incurred during testing, while a small fraction will be due to an erroneous decision, and the exploitation losses due to it.
However, compare the risk/reward adjusted gain, which is only $141,411 if we go through testing, versus $155,588 if we implement without a test. The cost of reducing our risk by 99.36% through testing is an approximately 8.5% reduction in potential gains. While this seems like a very good trade-off, note that the above scenario is quite favorable.
Let us examine a more niche online shop which is doing the same kind of test, but only has 5,000 users per week, converting at 50% and bringing in $250,000 in revenue ($50 ARPU, same as above). The optimal risk/reward ratio can be achieved with a two-week test at a 97% significance threshold (p < 0.03), almost the same as in the first example, resulting in the risk/reward analysis in Table 11.4.



Risk/Reward Analysis



Metric / Scope

Overall

Without Testing

During Testing

During Exploitation



Risk/Reward Ratio

1/104.89

1/1.00

1/1.00

1/1,160.43



Marginal Improvement

10,388.75%

-

-

-



R/R Adjusted Gain

$28,212

$38,897

$246

$27,963



R/R Adjusted Loss

-$269

-$38,897

-$246

-$24




Table 11.4: Risk/Reward analysis for a small e-commerce website

While the risk is reduced by about the same percentage, 99.31%, the potential gain is reduced by 27.47%; a much more significant premium on risk. If we further reduce the number of people hitting the shopping cart to 2,500 per week, and the revenue to $125,000, the premium rises to a whopping 43%.
Smaller businesses pay a higher premium for reducing their risk through A/B testing.
Note that this is under the unrealistic scenario of zero cost for testing, implementation, maintenance, etc. which, if added, would further increase the inherent cost of experimentation.
The reason we see such higher costs for a smaller business is mostly due to lower probability of detecting small true effects, and there is no way around this. For example, if a shop with 5,000 users in the cart per week wants to achieve the same statistical power as the big shop with 20,000 users per week, it needs to increase the test duration from two to eight weeks, or four-fold. This would delay the release of a truly better experience to 100% of users, or would increase the amount of time a portion of the users is exposed to a truly worse experience. This leads to a decrease in the potential revenue, and increases the risk from 0.95% (1/104.89) to 2.86%; a 3-fold increase.
Due to these inherent costs of A/B testing, there are situations in which an A/B testing is risk/reward negative - it should not be performed, as far as the business bottom-line is concerned.
One scenario in which this happens is when the perceived risk of the intervention is tiny compared to the perceived benefits. Bug fixes often fall into this category, and if A/B testing is justified then it is usually conducted with a non-inferiority hypothesis, a large significance threshold, and a small sample size. This makes sure that no high-impact disaster is going to happen, while also ensuring that the fix is delivered in a prompt manner.
There are business situations where an A/B test has a negative marginal value and is therefore not worth performing.
The other situation is if there is a very small number of users. The statistical power will simply not be there, so the ROI of an A/B test will be negative for most interventions, except perhaps very drastic ones. If this is combined with a nimble team, which is happy to reverse a decision if it seems to be causing harm, justifying an A/B test becomes even more difficult.
11.8 Limitations of Risk/Reward calculations
As with any algorithm, a risk/reward calculation would only be as accurate as the input which goes into it. If the expectation distribution is way off, or if certain costs are underappreciated while benefits are exaggerated, the calculated ROI-optimal design will not really be ROI-optimal. Similarly, if the expected duration of the exploitation period is over or under-estimated, then the calculation will be off. Obviously, there is a 'good enough' level of estimation which balances the difficulty of obtaining accurate estimates and their usefulness.
However, certain limitations need to be considered when applying this risk/reward framework for designing ROI-optimal A/B tests.
The first thing, which is also a hindrance to the application of A/B tests in general, are long-term effects. Such effects, both in terms of costs and benefits, are simply difficult to measure through short-term experiments, given the changing nature of websites, technologies, user expectations, market forces, and other factors. One assumption in the calculations proposed here is that the result will be about the same during the whole exploitation period. However, that need not be the case as novelty effects in both directions die off, and changes in circumstances entirely external to the website affect the future behavior of users. Any estimation of the longevity of exploitation of an implemented variant is also necessarily uncertain.
Secondly, we should consider a little-studied effect, which is that of constantly introducing changes to the user experience. Products which undergo a lot of A/B testing may suffer from this fact regardless of the outcome of A/B tests. If the homepage looks one way today, another way a month from now, then yet another month passes and it is back to what it was initially, same for the shopping cart, same for the registration process, etc... is it not possible that this has some adverse effects on users? If there are such effects, would they be more pronounced on sites that the users frequent, such as news sites, social networks, and other properties with high frequency of repeat visits in a relatively short time frame, or would they be more surprising for users of websites with less frequent visitation patterns?
I am not aware of answers to these questions or of solid research on this topic as a whole, but it is a potential limitation to be aware of.
Finally, there is the overall opportunity cost of A/B testing. What if the time and money spent on A/B testing is invested in something else entirely? Certain businesses may be better off skipping A/B tests and proceeding with more risk while expending resources in another direction. A/B testing is a means to an end, and if that end can be achieved in other ways, without unduly exposing the business to risk from bad decisions, why not do it?
The calculation of the marginal improvement of the risk/reward ratio is of utmost relevance in answering the question of opportunity cost. It shows how much better off we are if we chose to test versus straight up implementing whatever is considered for testing. While it does not provide the whole picture, it makes for a helpful comparison.







Chapter 12
EXTERNAL VALIDITY a.k.a. GENERALIZABILITY OF A/B TEST RESULTS
In this chapter, we will consider a crucial, but often neglected aspect of A/B testing which is not intimately related to statistics - the external validity of experimental results. Statistics are not applied in an environment shielded from concerns for external validity, and these concerns have an influence over the design of online experiments, and thus, indirectly, over the statistical methods we choose to perform the analyses. Therefore, understanding of this topic is important if statistical models are to address business questions posed in complex real-life environments.
12.1 What is external validity (generalizability)?
Chapter 3 discussed the concept of statistical adequacy which ensures the internal validity of a test. If the statistical model adequately reflects the data-generating mechanism, then the obtained statistical estimates can be trusted as reflecting some facts about the processes governing the data.
However, ensuring the validity of our conclusions with regard to the test data is not enough to make sound business decisions regarding the future behavior of both the users which participated in a test, as well as future users to whom the findings are applied. Obviously, we need to consider how applicable the conclusions from our tests are to these future circumstances - undoubtedly a difficult task.
A commonly cited definition of generalizability in a scientific context is provided by Campbell & Stanley (Campbell and Stanley 1963):
"External validity asks the question of generalizability: To what populations, settings, treatment variables and measurement variables can this effect be generalized?"
Examples of issues of generalizability in science are the prolific testing on students, and the quasi-anonymous online collaborators in psychology, as well as the lack of test subjects of certain gender, ethnic, or religious and other groups in some clinical trials. The first raises the question: "do students and mturks4 represent the broader population well enough?", while the second makes us wonder if a tested clinical treatment or medicine will have the same main effects and side effects on an out-of-sample individual which was not well represented in the trial?
Generalizability is a constant issue in political polls and other surveys, including consumer research. In A/B testing, we do not have to deal with some of these issues due to the way the experiments are set up, but there is plenty else to worry about.
The external validity of an online controlled experiment is defined by the applicability of its results to populations and times different than the ones examined during the test. Therefore, the larger the population to which the results of a test are generalizable, the better. Having a representative sample from a large population is therefore a desirable characteristic of any online experiment.
In practice we rarely even know what the full population is as we can only guess how external factors such as marketing strategy, competitor actions, broad economy trends, technology trends, regulatory action, and so on will shape, for example, the future potential clients of an e-commerce store. This means that a representative sample is hard to define, but we can at least strive towards making sure that users involved in a test are as representative of our future user base as possible.
Failure to do so means that obtaining accurate statistical estimates of the effect of the tested intervention during a test will hardly be of value, as they will not provide an accurate reflection of the true effect on future visitors to our site.
Unless test results are generalizable to future potential clients any predictions we try to make about sales, revenue, conversion rates, etc. will be inaccurate and may even be opposite to the true direction of the effect.
An A/B test may be demonstrating a strong positive effect with a very low p-value, and a narrow confidence interval for relative lift, yet if it is not based on a representative sample then it will lack external validity. The business may fail to register any discernible gains, or may in fact generate losses if the effect in the general population is of the opposite sign.
12.2 Threats to the external validity of online experiments
While an unknown number of immeasurable risks are inherent to any prediction made with regard to an open and changing environment, such as consumers in a dynamic market, there are also known threats to generalizability which can be countered by acquiring more representative samples.
Threats to generalizability of A/B test results can be separated into three distinct categories, following previous work of the author (Georgiev, Representative Samples and Generalizability of A/B Testing Results 2018). These are:
1.   Time-related factors
2.   Population change factors
3.   Novelty/learning factors
A detailed examination of each of these should help us deal with them to the extent to which it is possible.
Time-related factors can be further categorized into factors which result in time-variability of user behavior and time-to-effect factors. In the latter case, there is a difference in user behavior depending on the time difference between the moment they experience the intervention, and the moment their behavior is measured.
Examples of time-variability factors are within-day differences (e.g. morning vs. evening), day-to-day differences, day of week differences, holidays versus workdays, as well as seasonal effects. Businesses operating in certain niches are more prone to such effects than others. For example, seasonal cycles in the tourism and travel industry are much more pronounced than the general food consumption industry. Ice-cream and beer would be prominent niche exceptions.
An example for within-day difference can be people looking for a new TV alone during the day, and those doing so as a couple in the evening. These two types of shoppers may have a distinctly different approach to the task, with time being a proxy measure for this.
Day of week differences may be present for people visiting a travel offers website on a Tuesday, versus those doing so on a Saturday. The former may be more likely to just browse, while the latter may be more likely to actually complete a reservation or purchase.
The above are just a few examples of how the time-period analyzed during an A/B test may affect external validity when the results are generalized to a longer time period. An obvious way to address this would be to adjust the period so that it contains users exhibiting the whole spectrum of such behaviors, instead of just one group. This is unless the treatment is time-sensitive and targets only a certain time period.
With regard to time-to-effect factors, users who act quickly after experiencing the intervention may behave differently than those who take more time to deliberate or consult third parties. Differences might be present both in terms of effect size and effect direction.
For example, something which works well for impulsive buyers may have the opposite effect on more deliberate customers, therefore negating, or even reversing the effect entirely, when the tested treatment is applied to the whole population over an extended period. A brief experiment may fail to properly capture the behavior of the latter. Note that 'brief' might refer to hours in one case and weeks in another. How it is defined depends on the typical purchase cycle of the product or service at hand.
Population change factors include any significant shifts in the composition of the population of interest which may occur either during the test period, or after the test has been completed.
For example, a major competitor may launch a huge promotion, the likes of which will not be seen for a year, or maybe several years afterwards, precisely during an A/B test which is planned to run for two weeks. The outcome of the test may be invariant to that event, but it may also be skewed due to the promotion in such a way that the conclusions are externally invalid, and backfire when implemented permanently.
An even more trivial example can be given with a test which coincides with a big advertising push from our own marketing department. Or perhaps there is an atypical volume of organic social media traffic. The test intervention might work less well for this atypical traffic, and therefore the test will be a false negative from the standpoint of our future traffic.
In yet another scenario, the primary source of website traffic may be the U.S., but for some reason during the test there could be a surge in traffic from the United Kingdom and Australia that later subsides. If the population change was a factor in the test outcome, once the event is over this factor is no longer present, and the test's predictive capacity is destroyed, or at least hindered.
Novelty and learning factors can be a significant issue when there is a solid base of returning users. This is usually the case for big e-commerce websites, but can apply to small niche sites with loyal customer bases as well. A novelty effect is a positive or negative initial reaction to a change, which is dampened at some rate over time. Such novelty effects are often called 'learning effects', as they usually involve some form of increasing familiarity leading to a corresponding increase or decrease of a KPI.
A typical example involves the way that users behave when involved with a major redesign. It is common in this scenario for users to become lost during their first few interactions with the new flow, resulting in worse KPIs reported during a test. After a while, users become accustomed to the experience, and the KPI recovers to previous levels, or even surpasses them. Unless the test encompasses a long enough period, it may incorrectly suggest that the redesign was a failure, resulting in massive losses due to the resources which are now considered wasted, as well as the lost opportunity to gain from the redesign.
As a counterexample, introducing an attention-drawing change might trigger an initial spike in a KPI, but after getting used to it users may stop responding to it as much as they did previously. This would result in the KPI returning to its baseline. Some might call this regression to the mean, but I believe this to be incorrect. Regression to the mean is a very complex phenomena with many possible explanations, which can be true separately, or at the same time.
The above examples relate to conversion rate optimization applied to websites, but the same principle is valid for other applications of A/B tests, such as tests performed as part of email marketing. Email marketing is in a slightly favorable position since people in an email list do not change on a daily basis, as occurs with the general audience of a website. However, it is by no means immune from threats to external validity.
In A/B tests in email marketing, generalizability is threatened, since an email list may grow from traffic sources vastly different from the ones that brought in the subjects we tested on. In other cases, a test only covers a single email burst where a unique one-time event might have been in play, skewing the results relative to the future performance. External factors may change in a similar fashion; for example, a while back text emails were the norm since many people used clients which could not render HTML emails or did so poorly. HTML emails have consequently become the standard form of email communication.
Note that in all of the above examples the externally invalid outcomes can result in mistakes in both directions. A non-representative sample may suppress the discovery of a genuine beneficial effect, or it might make it appear as if there is an effect, when in fact the results only apply to the atypical set of users exposed to the test.
Also note that none of the statistical adequacy tests and checks on statistical assumptions play any role in addressing generalizability threats. How small a p-value is, or how narrow a confidence interval is observed, also has no bearing on the external validity of a test outcome. The effect can be present, and of large magnitude for a non-representative sample, yet disappear or reverse its direction after the test period.
The only way to alleviate threats to external validity is to design A/B tests informed by knowledge and understanding of threats which are external to the purely statistical design.
12.3 Improving the generalizability of A/B test results
There is one solution which addresses, to an extent, all the threats to external validity at the same time - longer duration of the test, and consequently a larger sample size. In an A/B testing environment this is the single biggest factor in improving the representativeness of the sample.
Time factors that depend on a kind of seasonality are dampened the longer that the test runs. The effect of any intraday variability is normalized when the sample covers several days. Similarly, the effect of any day-of-week variability is normalized when sampling users over a whole week, or even better several full weeks. Time-to-effect factors are also partially accounted for by sampling long enough to cover at least one full buying cycle.
For many online businesses, the above means using weeks as the base time unit for planning tests. A test duration is therefore 1, 2,... n weeks and not 10, 20, or 30 days. Businesses with known variability based on the time of the month should consider switching to testing full months instead. In most practical cases, variability based on seasons of the year cannot be tested due to the unduly long duration that such tests would command. With this in mind, some businesses resort to retesting as a way of addressing external validity. For example, a travel company might test something for four weeks off-season and implement it, but then retest it during the first four weeks of high-season, just to make sure it was not a case of seasonality-specific effects.
The longer a test runs for, the less of a role short-term population change factors play. Despite this, if a significant shift in the type of population occurs after the test is completed, external validity will suffer. If regular monitoring on important metrics, such as demographics, traffic sources, user intent, etc. reveals a significant shift in such measures, then retesting of interventions which are deemed susceptible to such shifts should be considered.
Novelty/learning effects become more pronounced the more time that they are given, so a longer test duration should translate in a sample which accounts better for such effects. In 2015, several Google employees published a very interesting paper (Honhnhold, O'Brien and Tang 2015) proposing a method for estimating long-term learning effects from short-term tests, but it is far from trivial to implement, is limited in terms of applicable scenarios, and will likely be unable to capture learning effects with delayed onset. If one does not have the technical resources to implement a similar solution, though, the best option remains a longer test duration.
Unless test results are generalizable to future potential clients any predictions we try to make about sales, revenue, conversion rates, etc. will be inaccurate and may even be opposite to the true direction of the effect.
An obvious effect of this is that risks during the duration of the test are increased, and there is an opportunity cost to not releasing a true improvement sooner. Readers familiar with issues such as cookie churn would note that increasing the duration of a test may necessitate an adjustment of the statistical model to accommodate cookie deletion in cases where the tested users do not have a persistent identity. Both are trade-offs of which one should be aware.
12.4 Representative samples and sequential tests
The above threats to generalizability, and solutions for obtaining representative samples, are even more important in a test in which, instead of evaluating the data once at the end (fixed sample tests), we monitor it as it gathers, and have the opportunity to make the decision to stop the test at any of these evaluation times. Proper sequential testing procedures, such as AGILE A/B testing, fall into this category.
The issue with sequential evaluation of an A/B test is that one can get a very good, or a very bad, statistically significant result very early into a test. Assuming the significance threshold was chosen following a well-thought-out risk-reward calculation, should one stop the test early due to efficacy or futility, while risking the generalizability of the result?
The answer is "no" and "yes", depending on the type of monitoring scheme. It is "no" if a simple version of Wald's Sequential-Probability Ratio Test (SPRT), or an equivalent procedure is used. SPRT and SPRT-equivalent methods have been proposed by some statisticians and some have been implemented in A/B testing software. Most multi-armed bandit algorithms fall into this category.
The reason I am recommending against the adoption of such simple monitoring schemes is that they allow one to stop too early way too often with results which are not impressive enough (extreme enough) to justify the potential loss of generalizability. This is especially true in the case of multi-armed bandit methods in which one usually evaluates the data after every observation, and both time-variability and time-to-effect factors become an issue. While several methods have been developed to model delayed feedback, the time-variability aspect remains unresolved, to my knowledge.
Approaches in which alpha-spending functions are constructed in a way that alpha is spent conservatively early on and more aggressively on later stages should result in more representative samples. In using such methods, one can adjust the balance between fast decision-making and better external validity by tweaking the shape of the spending function. This results in demanding either a less or a more representative sample before a test can be stopped very early.
This is a quote for the justification of the choice of spending function from the author's paper (Georgiev, Efficient A/B Testing in Conversion Rate Optimization: The AGILE Statistical Method 2017): The method used by default for the construction of the efficacy boundary in our AGILE AB testing tool is a Kim & DeMets (1987) power function with an upper boundary of 3. The choice is justified by the fact that the function is a bit less conservative than the O'Brien-Fleming-like spending function in the early stages of a test, but is still conservative enough so that more error is allocated to the later stages of a test. Here by conservative it is meant that initial results must be very extreme before an early conclusion would be suggested - which is a good property as early users in a test are not always representative of later ones.".
While there is no 'best' or 'objective' way to determine how conservative one has to be in stopping early, a decision on the shape of the spending function should be informed by historical and expected time-variability and expected population changes, as well as the specific treatment's perceived likelihood of resulting in a novelty/learning effect.
12.5 Running multiple concurrent A/B tests
This is a special threat to external validity, insofar as it is caused by testing itself, unlike the other threats described above. If we are to categorize it then it will loosely fall into the category of population change factors.
But why is running tests concurrently an issue to external validity? A/B test variants in tests running partially or fully parallel to our test (T1) may be having an impact on the outcome of T1, which is an issue since they themselves may not be adopted after their corresponding test ends, resulting in loss of generalizability. Let us say that T1-A had positive outcome only because of the better performance it achieved in combination with T2-B, and was therefore kept in place instead of being replaced by T1-B. However, T2-B was judged (correctly) to be inferior and dropped after the T2 test has ended. Thus T1-A is now a poorer choice to T1-B which would have won if it weren't for T2-B.
Since it is not easy to visualize situations in which combinations of tests lead to wrong conclusions, let us examine one such scenario in which interactions between the test variants lead to missing a true improvement instead of implementing an inferior variant, as in the above hypothetical. Instead of choosing variant B from T1, and control A from T2, analyzing T1 and T2 separately leads to choosing the control A in both tests as shown in Figure 12.1.


Figure 12.1: Comparison of analyzing test results from two A/B tests without vs. with accounting for interactions.

The interaction between T1-B and T2-B is stronger than the interaction between T1-B and T2-A and is opposite in sign to the interaction between T1-A and T2-B, making the overall T1-B result worse than that of T1-A. If the interactions are not examined and tests are analyzed separately, an inferior combination will be chosen in test T1.
In simulations ran by the author with randomly generated true outcomes from a uniform distribution spanning [-50%, 50%] change, 32.3% resulted in picking a wrong variant in one of the tests due to interference. There were no cases where both tests were influenced. While this demonstrates that interaction effects such as the above example are certainly possible, it says nothing about their likely prevalence, since estimating that will require strong prior distributions about the size and magnitude of the effect of a batch of concurrent A/B tests.
When users experience several tests at the same time, strong and opposite in sign interaction effects can result in conclusions which do not hold after the tests are completed.
It has to be noted that the time overlap above was considered to be 100%: T1 and T2 start and end at the same time, which rarely happens in practice. However, the takeaways apply to any percentage of overlap. Obviously, the less overlap there is, the smaller the interference of one test over another, as we discount any interference by the difference between 100% and the overlap percentage. Nevertheless, the possibility of this interference influencing the outcome of one of the tests remains.
An intuitive solution to the problem of interaction between tests is to run just one A/B test at a time. By adopting this approach, no interaction effects are present, and it is therefore a sure way to combat the issue. But at the same time, it is very inefficient - either severely limiting the ability of the business to innovate, or resulting in releasing untested changes, which would have otherwise been tested.
Another solution is to begin planning A/B tests in parallel isolated lanes - silos of users who are going to experience just one test at any given time. In terms of efficiency, this is just as inefficient as the first solution, since by reducing the number of users available for each test (they are, by necessity, a portion of the total user pool) the duration is prolonged accordingly. Either that, or tests are run with lower than adequate statistical power.
For example, there are 100,000 users per month on an e-commerce website and we need to run four tests on them, each of which requires 100,000 users. If we run the tests one after another then it would take exactly four months (4 tests x 100,000 users), and if we split the users into four silos then each silo would get 25,000 users per month. To run one test with 100,000 in each silo would take four months again.
However, the choice to isolate users can have consequences other than severely slowing down the overall testing process, as it makes it very likely to release untested experiences. Consider the following scenario of running two tests on isolated sets of users, then releasing the winning variants to all users. This scenario is depicted in Figure 12.2.


Figure 12.2: Comparison of analyzing test results from two A/B tests ran in separate lanes.

In case the variants (B) win in both tests, the result will be users experiencing T1-B and T2-B at the same time, which is a combination that was never tested, so if there are any interaction effects between T1-B and T2-B, they would be unknown. It is similar to running a test for mobile users, and then implementing a winning variant for desktop and mobile users.
Running tests in isolation is sure to result in releasing untested user experiences.
This can be further illustrated by examining a scenario in which T1 is a test on the cart page and T2 is a test on the checkout page. With isolated lanes what happens in terms of user allocation can be visualized as shown in Figure 12.3:


Figure 12.3: Test groups users can experience with isolated lanes.

The silo effectively results in users who experience variant B on the cart page to never experience variant B on the checkout. If users where not siloed, the allocation would look as shown in Figure 12.4.


Figure 12.4: Test groups users can experience without isolation.

About one-quarter of users would experience T1-B and T2-B, giving plenty of data to assess for interaction effects.
If the first intuitive solution results in a very slow testing process, and the second results in both a very slow testing process and releasing untested variants into the wild, how can we approach the problem of interactions between concurrently running tests, which has been demonstrated to possibly result in incorrect inferences?
Since unlike other population change factors, such as geographical location or device usage, we actually control and can measure the effects of concurrent tests on any test of interest, the best approach is to perform a statistical analysis of the interaction effects, and act accordingly. This can happen similarly to how we analyze segments of the users based on other characteristics - in this case the characteristic is whether they are enrolled in a different test, and to which variant they have been exposed.
The risk/reward calculations of such analyses, and the appropriate significance thresholds, can become quite messy, since the number of possible interaction effects increases exponentially with the number of concurrently running tests. The decision-making process is itself complicated, since tests usually overlap only partially. For example, the outcome of a test T2 in which variant T2-B which demonstrated interaction effects with test variant T1-B may not be clear by the time T1 needs to be called and T1-B implemented as winner. If it turns out that T2-B is superior than the control and is to be implemented, the interaction effect may diminish or reverse the effect of T1-B. In such a case, we would obviously want to implement the combination of tests which leads to the best overall outcome. However, going back and reversing decisions on tests already completed and implemented can be very awkward, and can also induce additional costs.
One solution would be to delay the implementation of T1-B until the result of T2 is clear. This delay can be costly in terms of missed revenue. Another approach, especially viable with partial time overlap and if the interaction effect is highly uncertain, is to prolong the test T1 until the conclusion of T2. More data on the interaction would be gathered, and its direction and size will be estimated to a higher precision.
Whatever measures are taken, they need to be put in context of the actual likelihood of interactions existing in the first place. I would argue that there are some tests where the risk of interaction effects is great and rather obvious, such as when testing multiple elements on the same page, or section of a template in separate tests. Making a free delivery more obvious in the header of an e-commerce store in one test, and experimenting with charging for delivery at the checkout stage in another, would be blatant examples of how interactions are possible even when tests refer to different sections of a website.
In other cases, when concurrent tests run on separate pages, and there is no link between what is being tested, the risk can be negligible, and engaging in complicated statistical analyses may not be justified. For example, experimenting with making pricing clearer is unlikely to interfere with a test for improving the wording of a free trial sign up button.

4 (usually) low-paid remote workers hired through the Amazon Mechanical Turk crowdsourcing marketplace. Often employed in studies in psychology, sociology, economics, etc. since its launch in Nov 2005.








Chapter 13
MISCELLANEOUS TOPICS
This chapter covers topics which do not fit neatly into any of the major chapters, but do not warrant a separate chapter by themselves, at least in the current book.
13.1 Equal or unequal between test groups?
In most of the formulas provided in the book, we allowed for a test with unequal allocation of users between test variants, but we never considered such a test in any detail. In a test with equal allocation, the total sample size is split equally between the treatment and the control group. The roughly equal distribution is assured by an algorithm for random number generation, so that if we have 15,000 users to test with, it will allocate approximately 5,000 to each of the two test groups and another 5,000 to a control group.
However, it is entirely possible to design a test with a different allocation ratio; for example, 20% of users in a test group, and the remaining 80% in a control group. The reverse is also a possibility. The rationale is usually that of reducing risk of exposure to an inferior variant during the test in the former, and of accelerated exposure to a superior variant in the latter.
This concept can also be combined with different schemes for 'ramping up' the allocation to the test group(s) in case of early positive effects, or 'ramping down' in case of early negative effects. One can even start with equal allocation, but then proceed to unbalance the design by changing the proportions of traffic sent to each group. This, however, is a slightly different topic discussed in "Adaptive Designs" below.
For now, consider the effects on statistical efficiency of unequal allocation in the fixed sample size case. This effect, in the case of a simple A/B test, is to decrease the statistical power of the test. Since p-value, confidence interval, severity, and other estimators depend on comparing estimates for within-group variance with between-group differences, and the variance of one or more groups can be estimated less precisely than the others, the overall power of the procedure is decreased. This leads to higher p-values, wider confidence intervals, and lower severity.
Consider the following example test - we have 10,000 users, a baseline of 3% conversion rate and want to test for relative difference in percent. If we observe a 3.6% conversion rate for the variant (B), this will result in a p-value of 0.053, or a confidence level of 94.73%. The corresponding one-sided 95%CI spans from -0.3% to +∞ relative change in conversion rate.
If the users are split 80% to 20% between the control and variant A/B, and observe the same 3.6% CR of the variant, the p-value will be 0.097 (confidence level of 90.30%). The corresponding one-sided 95%CI spans from -5.3% to +∞ relative change in conversion rate. While we might observe a larger difference due to the smaller sample size, on average we would observe exactly the true difference which is fixed (though unknown). Unequal allocation expectedly produced estimates reflecting the greater uncertainty of the data.
Unequal allocation of the available sample size between groups significantly increases the time required to complete an A/B test when compared to equal allocation.
Another way to understand what is happening above is to plot the confidence intervals of each group separately, and examine their width. In the case of equal allocation, the two intervals are roughly of the same width. With unequal allocation the interval for the control is much narrower, while the one for the variant is much wider than before.


Figure 13.1: Standard deviation with equal vs. unequal allocation.

Achieving greater precision for one estimate comes at the cost of lesser precision for the other, resulting in a net loss due to the non-linear nature of the relationships.
In the case of an A/B/n test, unequal allocation can technically be beneficial for efficiency. For example, with four treatments versus a control, an optimal allocation would be to send 1/3 of traffic to the control and 1/6 to each of the treatments, resulting in nearly 10% smaller sample size compared to equal allocation.
However, this is only the case with using Bonferroni-corrected p-value and confidence interval calculations. The efficiency of procedures such as Dunnett's correction usually far outweigh this benefit. In certain scenarios, unequal allocation results in less than 10% improvement in power, while equal allocation with Dunnett's correction results in 30% improvement.
To my understanding, Dunnett's adjustment is only feasible in a design with equal allocation, therefore using unequal allocation will likely be counterproductive.
13.2 Holdout groups
A holdout group is a set of users who are prevented ('held back') from entering any experiments performed on a website and kept as a global control group. The purpose of a holdout group is to compare the performance of a website, app, or other software as it would be without conversion rate optimization efforts, versus the effects of those efforts. To serve its purpose it must also not be subject to any changes introduced to the website.
Note that while the terminology is also encountered in machine learning, its purpose there is entirely different. In machine learning, a part of a database would be withheld from the learning algorithm, and then used to validate its precision and recall, which is only superficially similar to the holdout group in an A/B testing context.
The idea of a holdout group is intuitively appealing, as it can provide a benchmark for the total efforts to improve a website or app over a given period. It can also serve as a check for the adequacy of the statistical models, and the platform for delivering and evaluating A/B tests. It seems to transcend external validity issues of all types - time to effect, population change, and novelty factors would all be accounted for by comparing the holdout group, versus a group which was improved upon over a sequence of A/B tests conducted over several years.
However, in practice, holdout groups are usually a poor solution to a real problem. The first issue related to holdout groups is the prohibitive cost of maintaining such a group. Any user included in a holdout would not experience any website improvement. The proportion of our total users held in a holdout group is also a proportion for which no improvements are implemented. The greater this proportion, the more one fails to capitalize on real or perceived gains. This usually keeps the holdout group small, e.g. 5% of total users, making it hard to draw conclusions from the data due to high intrinsic variance of any measurement on these 5% (this issue was discussed in the previous subchapter). This makes statistical conclusions based on comparisons to the holdout group uncertain.
To add to that, holdout groups are not immune to issues such as those experienced in any attempt to infer causality from a non-experimental situation. Firstly, unless user identity persists over large periods of time and across browsers and devices, there will be pollution of the holdout group due to cookie churn, multiple accounts for the same person, and similar issues. This can result in some of the effects of the A/B tests being expressed in the holdout group, thus reducing the difference between the holdout and the rest of the users.
Secondly, survivorship bias and selection bias are introduced by not adding new members to the group - users of the holdout group are no longer representative of the overall user base. Therefore, judging the overall effects of group of A/B tests based on the difference in performance in this group and the rest is not justified. Survivorship bias becomes more expressed as time goes on, and the less satisfied and less motivated users in the group stop using the website or app. Thus, the users that remain are no longer representative.
This cannot be countered easily by adding new users to the holdout group over time - e.g. by adding every 20th user to keep the holdout group at ~5% of total - as this would negate the logic of its existence. Some of the users added later on would have already experienced some of the experiments, meaning that they cannot be used as an out-of-sample control.
On a separate note, it should be clear that while a holdout group can be used to estimate the value of a whole set of changes to the website, and this can be judged against other potential investments, it is poorly informative in terms of potential actions. For example, discovering that the performance of the rest of the users versus the holdout group is not as great as expected based on a meta-analysis of A/B test results does not mean that we know what can be done to fix this. It remains unclear if this is due to poor external validity, or due to poor statistical modeling leading to internal validity being compromised. Unlike an A/B test, in which there is one or at most a couple of related changes, the comparison of a holdout group is not very actionable in itself.
Furthermore, once we add the complexities involved in separating users and maintaining a shielded experience for some of them, it becomes easy to see why keeping a holdout group is not recommended in most cases.
There are specific cases where a holdout group might make some sense, such as if there is strong persistent user identity, and if the period over which the group would exist is no longer than 6-12 months. This negates some of the positives of having a holdout group, but it also reduces opportunity costs. Additionally, issues of representativeness are usually less pronounced over such a time period.
13.3 Time to event analysis. Hazard ratio
With regard to most A/B tests discussed so far, we were interested in measuring the difference in means, but there are situations where this difference is not of primary interest, such as when we care about effects estimated based on a time-to-event rate. Examples of such rates are email newsletter unsubscribe rate, SaaS churn rate, account deletion or suspension rate, free trial cancellation rate, and so on. When working with such rates there is usually persistent identification, which allows running tests for prolonged periods, without fear of pollution of the data.
When we care about the relative rate of events in two or more groups, we need to begin by randomly assigning users to these groups. Then we follow them for some time, noting the presence of an event such as the cancellation of a payment subscription. 'Time' only moves forward if there is an event, in this case a cancellation. The resulting curves are called 'time-to-event curves', or 'survival curves'.
Note that in this case we are not interested in comparing the difference in the number of events divided by the number of users (relative difference, a.k.a. relative risk). If we were to follow all tested users until the end of their payment subscriptions, we would end up with zero subscribed users in both groups, and the overall relative difference would be zero. Similarly, for any time slice before reaching zero, the relative differences might be the same, and still one group might be bringing in more revenue than the other due to how events are spaced in time (Stare and Maucort-Boulch 2016). Traditional A/B test metrics do not work well in such scenarios.
Instead, in such cases we should care about the average relative hazard - the average ratio between the ratios of observed events and expected events under a null over a time slice. The resulting mean hazard ratio across all relevant time slices reflects the difference between two time-to-event curves. A hazard ratio larger than 1 means that there is an increased risk of events on average, while if it is less than 1 then there is a reduction of that risk. The above assumes that you are comparing a test group to a control group, otherwise the reverse is true.
The instantaneous hazard ratio between two groups, A and B, is calculated using:

Where

Obs stands for number of observed events, Exp stand for expected events, NAR stands for Number at Risk at the beginning of the respective time slice. All formulas are calculated for the time-slice t.
The mean hazard ratio over a period is calculated by taking the geometric mean of the instantaneous hazard ratios. The mean hazard ratio is also known as a relative event rate.
Observing a mean hazard ratio of 0.5 means that, on average, a user in the test group is half as likely to experience an event than a user in the control group, given they both reached a given point in time t. The average hazard ratio is therefore equivalent to the relative risk that a user in the group with the higher hazard will experience an event first.
It should be noted that the mean hazard ratio is usually not enough to provide information relevant to decision-making, since if the distance between the curves is not relatively stable over time the same mean hazard ratio may be computed for quite differently looking curves.


Figure 13.2: Survival curves, median and mean difference.



Figure 13.3: Survival curves, median and mean difference.

Compare Figure 13.2 with Figure 13.3. In the first graph, there was a slight delay in the cancellation event rate starting from month 6 onward, but it slows down, and by the end the two groups have nearly equal numbers of survivors. In the second graph the delay starts at month 3, but goes on for quite a lot longer, and by the end the test group has several times more users in it.
The median and the mean of the relative difference are statistics usually used to complement the mean hazard ratio (Spruance, et al. 2004) (Sashegyi and Ferry 2017). In the first case above, the median is just 3 months, compared to 5 months in the second scenario, while the mean is 1.5 months in the first and 4.6 months in the second. Despite the same mean hazard ratio of 0.5, the two graphs would lead to differing conclusions about the effect size and the expected gains, even though the direction might be the same.
When the relative difference in time-to-event is of interest, the appropriate statistic is the mean hazard ratio, accompanied by median and mean difference.
If the mean hazard ratio is of interest in a particular test, it is useful to have an estimate of its uncertainty in terms of a p-value and confidence intervals. The p-value for a X2 (Chi-square) distributed variable with 1 degree of freedom in the case of a simple A/B test and can thus be computed by calculating the cumulative distribution function of a X2 distribution with 1 degree of freedom. The X2 statistic itself is calculated by:

In this case i=1 for the control group A and i=2 for the test group B, and n=2.
The standard error of the log mean hazard ratio is approximated by:

Note that these are the cumulative expected values, not the instantaneous ones. The confidence interval for the mean hazard ratio is then simply:

Z is, as usual, the score statistic corresponding to the desired confidence level.
While hazard ratios are a bit hard to interpret on their own, they can be a powerful tool for tests comparing time-to-event rates, especially when combined with median and mean difference, as well as a graphical assessment of the corresponding time-to-event curves.
13.4 Meta-analyses of A/B test results
There are cases where a practitioner might need to present a meta-analysis of a series of A/B test results. This subchapter will briefly cover two common scenarios that require aggregating statistics from multiple tests.
One such scenario is possible when the same test is re-run for some reason, perhaps with the goal of validating the results at a future point in time. Another scenario is possible where the performance of a whole A/B testing program needs to be evaluated over a certain period, say, a year.
In the first case, where there are two or more tests of the same intervention (variant) the results can simply be combined by pooling together all observations and all events of interest, and computing the statistics with the totality of the available data.
For example, assume test T1 with 10,000 users per group resulted in observed conversion rate of 5% for the control and 5.6% for the variant. Then test T2 with another 10,000 users per group resulted in 5.0% CR for the experience, which was the control in T1, and 5.7% CR for the experience, which was the tested variant in T1. Working under the percent change model results in a Z-score of 1.844 and a p-value of 0.033 for T1, and a Z-score of 2.124 and a p-value of 0.017 for T2. Combining data leads to a total of 20,000 users per group with a 5% CR for the control and 5.65% CR for the variant leading to a Z-score of 2.783 and a p-value of 0.003.
An equivalent solution which works directly with the Z-scores without the need to recompute other data attributed to Stouffer et al. (Stouffer, et al. 1949) is given simply by:

where k is the number of tests to be analyzed as one. It results in the same outcome: (1.844 + 2.124) / √2 = 3.968 / 1.414 = 2.806 (discrepancy due to rounding). A p-value can be obtained from the resulting Z-score in the usual manner by computing the CDF.
Considerations about external validity might cause a practitioner to want to give more weight to the more recent test, which would require more involved computations.
In the second case, where we have results from a whole series of A/B tests testing entirely different variants, perhaps on different sections of a website, the goal is usually not to obtain a Z-score or a p-value, but an estimate of the combined effect size of all tests alongside an estimate of the uncertainty of such a confidence interval. This exercise is usually performed when an estimate of the effect achieved by the entire user experience team, or conversion rate optimization agency, is needed.
In such a case, we can assume the tests to be independent, so the outcome of each test is a random variable, and their variance is the sum of variances. Summing the radius of every interval and taking its square will give us the interval for the combined outcome of all tests.
Assuming one is working with relative difference or percent difference, the maximum likelihood estimate can be obtained by simply summing the observed effects of each A/B test. The lower bound of a one-sided interval can then be computed by subtracting the square root of the sum of the squared intervals.
For example, assume we have the following test outcomes:



Outcomes of 5 A/B tests



Test ID

Observed Relative Difference

95% Percent Change Confidence Interval*



Test 001

4.0%

1.0%



Test 002

10.0%

3.0%



Test 003

6.0%

1.5%



Test 004

18.0%

8.0%



Test 005

12.0%

6.0%




* one-sided, lower bound given.
Table 13.1: Example outcomes from 5 A/B tests

The estimate of their total effect would be simply:

Therefore:

The 95% lower bound of the percentage change interval would be:

So the observed total effect is 50%, with a 95% one-sided interval bound by 35.36% from below.
The above equations were confirmed by simulations performed by the author. A small, but noticeable, bias in the MLE towards positive differences was observed at baseline values in the extreme end of the binomial distribution such as 0.01 (1%) conversion rate. At 0.1 (10%) the bias was practically non-existent. This bias is likely what resulted in slight interval under-coverage.
The equations should generalize well to cases where the input consists of Z-scores produced by sequential tests, given that they were adjusted as described in Chapter 10.7.
The same equations can be used for other situations where a meta-analysis of several A/B tests is deemed useful.
13.5 Adaptive Designs
Throughout the book we have considered fixed sample designs, designs with one or more variants versus a control, and designs with sequential evaluation of accumulating data. All of them have a fixed maximum sample size, fixed numbers of treatments, fixed allocation between test groups, and more or less pre-planned numbers and timings of analyses. While some sequential designs allow us flexibility in the number and timing of interim analyses, as well as the possibility of some sample size overrun, none of the designs discussed so far allowed for sample size re-estimation, for dropping or adding variants midway, or for changing the allocation of users between arms during the experiment.
Designs which allow such design changes during the experiment, usually based on the outcome of one or more variables of interest, are called adaptive designs. It is almost as if there is no need for any design if one is using an adaptive statistical design, which is a common, but mistaken perception that results in them having an appealing façade. Unbiased and efficient inferences following such a design are even more complicated than those following sequential designs, which might make results difficult to interpret.
However, these are not necessarily unsurmountable drawbacks. The reason adaptive designs are not more prominently featured in this book is, in fact, their apparent lack of efficiency when compared to equivalent sequential testing designs, which are much simpler to execute and analyze.
With regard to the comparative efficiency of adaptive designs, Tsiatis & Mehta in their work "On the Inefficiency of the Adaptive Design for Monitoring Clinical Trials" (A. M. Tsiatis 2003, 368) arrive at the conclusion that "for any adaptive design, one can always construct a standard group-sequential test based on the sequential likelihood ratio test statistic that, for any parameter value in the space of alternatives will reject the null hypothesis earlier with higher probability, and, for any parameter value not in the space of alternatives will accept the null hypothesis earlier with higher probability."
In discussing why adaptive sequential designs can be as much as 30-40% less efficient than competing non-adaptive sequential designs, Jennison & Turnbull (Jennison and Turnbull 2010, 24) point out that due to adaptive design's use of non-sufficient statistics "...they cannot be optimal designs for any criteria. Since the potential benefits of adaptivity are slight, any departure from optimality can leave room for an efficient non-adaptive design, with the same number of analyses, to do better." The unequal weighting of observations results in loss of efficiency due to reliance on non-sufficient statistics. This conclusion is even stronger than that of Tsiatis & Mehta, which is cited above, who allow the comparator non-adaptive GSP to have additional analyzes.
Jennison & Turnbull also point out that sample size adjustments are inherently based on a highly variable interim estimator of the true effect size, making them inefficient.
"It is our view that the benefits of such [data-dependent] modifications are small compared to the complexity of these designs" add the same authors in the conclusions of the aforementioned paper.
Basically, these authors prove to a satisfactory level that adaptive designs, while having certain appeal, are less efficient than simple group sequential designs, such as the ones discussed in this book. This is not just an average or overall conclusion; it is a conclusion valid for every adaptive test. While this doesn't necessarily mean that there are no situations where adaptive designs might be preferred on certain grounds, it certainly makes it difficult to imagine such practical situations.
Lee, Chen & Yin examine the issue of fixed equal versus variable unequal randomization throughout a test and reach a similar conclusion (Lee, Chen and Yin 2012, 1): "In summary, equal randomization maintains balanced allocation throughout the trial and reaches the specified statistical power with a smaller number of patients in the trial [compared to adaptive randomization]", which further strengthens the argument against the adoption of adaptive designs in online A/B testing.
Another argument for the adoption of flexible designs might come from adaptiveness of the 'pick winner' / 'drop-loser(s)' or 'add arm' type. However, these procedures result in significant increase in the complexity of planning tests of both fixed and variable sample size as illustrated by Wason et al. (Wason, et al. 2017), who also points out difficulties in the presence of delayed effects. Further challenges in terms of specifying selection criteria for 'winner' arms and a trade-off between reduced sample size and increased duration are pointed out by Chang & Balser (Chang and Balser 2016). I was not able to uncover papers discussing the performance of such adaptive designs in the face of time-heterogeneous data, or addressing external validity concerns.
The current state of development and understanding of adaptive designs does not allow me to suggest their adoption.
13.6 A word on multi-armed bandits
Multi-armed bandit (MAB) algorithms have been around for a fairly long time, since Herbert Robbins made his break-through in 1952 in devising selection strategies for sequential experiments. The basic idea is that if there is a set of mechanisms delivering rewards with an unknown and potentially different frequency, we want to devise a strategy for switching between one mechanism and the other, which maximizes the cumulative reward. In a bandit problem, the 'player' faces a set of possible actions at each step (in time), with the objective being formalized as minimizing regret for missed rewards. For a concise review of different types of regret the reader can refer to (Rosset, Ige and Duhaime 2017).
The goal of early bandit algorithms was to maximize the reward over a potentially unlimited long run. If we think of it as an A/B or A/B/n experiment with sales or conversions as rewards, it would be like wanting to maximize the number of sales or conversions we get during the duration of the experiment by changing the allocation ratio of users across the arms. It is trying to balance exploration with what currently works.
However, this goal is not the same as identifying the best possible arm in order to continue using it after the test for a potentially much longer period. There are purely exploratory MAB algorithms which attempt to identify the best possible arm in as few observations as possible and these are the ones which most closely align with the goal of A/B testing in providing reliable estimates and risk-management.
There are currently dozens of MAB algorithms with varying optimality. The literature is rich with approaches for dealing with aspects of the problem, including delayed rewards, time-heterogeneous rewards, delayed switching, adversarial versus non-adversarial scenarios, and so on. It would not do these methods justice to attempt to go over them in any detail here.
However, I will make a few general points which are relevant to all bandit methods aiming to identify the best possible arm. These might help you understand why only a small subchapter is dedicated to them.
Firstly, note that a bandit method in terms of tools at its disposal is a sequential adaptive design, with unequal allocation between arms and sometimes also implementing 'pick winner' / 'drop loser'. As pointed out in the previous subchapter, adaptive unequal allocation is always inferior to equal allocation, while 'pick winner' / 'drop loser' strategies face certain practical issues, especially when the reward is not fully encompassed in the outcome of each action.
I was unable to find literature comparing the performance of one or more MAB algorithms and a more traditional sequential testing procedure of any kind, but the known literature comparing the best adaptive sequential designs with non-adaptive ones seems to suggest a relative inefficiency of multi-armed bandits.
Secondly, significant external considerations which would make MAB difficult to deploy are very common in A/B testing practice, and while they can be used to inform the statistical design of a test, I am not aware of ways to do so for a bandit. If possible, it would certainly be much more involved due to the vast number of possible sample paths and thus risk/reward calculations, that are necessary.
Thirdly, bandit approaches would need to address concerns for the generalizability of results achieved after a very early stopping, either by selecting a winning arm, or by dropping underperformers. In traditional experiments, we can alter alpha-spending functions in order to make them less probable to stop too early, but I am not aware of ways to do the same with a bandit algorithm.
I would be hesitant to deploy a bandit approach in case of external considerations or generalizability issues with early results, even if its expected efficiency was higher than a comparable sequential non-adaptive test. Since these are almost always present, and there is also the added complexity in deploying and analyzing MAB methods, I remain skeptical of the utility of multi-armed bandits outside of several narrow areas (such as ad serving) in which they experience most of the current usage.
13.7 Bayesian methods
This subchapter will be similar to the one on multi-armed bandits in that it is not an exposé on different Bayesian methods, but instead an argument against their use in statistical inference.
While there are competing definitions of what Bayesian inference is, most would agree that it entails starting from a prior distribution, observing certain data, and arriving at a posterior distribution. After arriving at the posterior, some suggest that it should be presented in its entirety, while others prefer presenting certain quantiles. Bayes factors are also often used with certain thresholds suggested to correspond to different levels of probability. Bayesians may also present an estimate by constructing credible intervals, and make inferences based on them.
What unites these approaches is that what is sought is to arrive at a probability estimate for the parameter of interest. For example, we might say that if a 95% credible interval excludes a zero difference in proportions, then there is, under a specified statistical model, 95% probability that the proportions are in fact different. Contrast this to frequentist inference, in which we cannot assign probabilities to any statistical or substantive hypothesis, as these are either true or false. Therefore, we speak only of probabilities of events under a specified statistical model.
Most of the appeal of Bayesian approaches stems from their ability to calculate probabilities for hypotheses. Another attractive property is the ability of Bayesian approaches to ignore stopping rules and make inferences, regardless of how the statistical information was obtained.
However, many, including leading Bayesians, argue against this claim by rightly pointing out that it is a misguided view, and insist, correctly, that the stopping rule is part of the data-generating mechanism ("data"), and, therefore, ignoring it leads to flawed inferences. See (Georgiev, Bayesian AB Testing is Not Immune to Optional Stopping Issues 2017) for a brief discussion and references.
The greatest issue with Bayesian methods lies in making that grand claim of being able to calculate probabilities for hypotheses - statistical or, worse, substantive. This is a genuine issue, since computing a posterior probability requires the presence of a prior probability. Different schools of thought insist on different ways of defining that probability - purely subjective, reflecting some expert opinion or consensus, or so-called objective, or minimally informative prior probabilities.
When using certain objective priors, we often arrive at the same statistical estimates (assuming optional stopping was taken into account). Does this mean frequentist p-values can also be interpreted as probabilities of hypotheses? Hardly anyone would agree with that suggestion, and for good reason. But then why accept the Bayesian claim that their statistic should be interpreted as such?
Subjective priors of different magnitudes raise further questions. On the one hand, why do we care about the subjective expectation of someone, be it an expert or not? And on the other, we have the reference class problem to deal with. We discussed it briefly in chapter 11.4. A short statement of the problem would be: "why should your expertise or data based on past experience apply to this particular case?".
The root of the issue seems to be that Bayesian methods are ultimately not tools for statistical inference, but tools for decision-making, either personal or in an environment where consensus or agreement on a prior distribution or, equivalently, a loss function (Spanos 2017, 11), can be achieved.
Bayesian methods take as given some initial knowledge or belief encoded as a probability curve. Then they allow us to calculate how that knowledge should change upon receiving new information. The result is another probability curve, which is only useful to someone who understands and agrees with the prior distribution. Bayesian methods are essentially a probability calculation for the easy part of the problem of making an inference. They address the question of how to make an optimal decision, given some data and a prior state of knowledge and/or a utility function.
David Cox, after remarking on the fact that statistical problems are not regarded as decision ones - as was widely expected at some points in the past - adds: "The reasons that the detailed techniques [of decision theoretic approaches] seem of fairly limited applicability, even when a fairly clear cut decision element is involved, may be (i) that, except in such fields as control theory and acceptance sampling, a major contribution of statistical technique is in presenting the evidence in incisive form for discussion, rather than in providing a mechanical prescription for the final decision. This is especially the case when a single major decision is involved; (ii) the central difficulty may be in formulating the elements required for the quantitative analysis, rather than in combining these elements via a decision rule." (Cox 1978, 45) Formalizing the decision-making process in such a way is scarcely useful, and very difficult to achieve in practice, as the reader might have realized when going through chapter 11.4. Additionally, the process only works for simple complementary statistical hypotheses (Spanos 2014).
I do not view Bayesian methods as methods for statistical inference, since they explicitly ignore the true state of nature and make no claims to the effect of pinpointing it (Spanos 2017, 7). Consequently, most Bayesians do not care about estimating error rates or limiting risk in the way that frequentists do. A Bayesian method is appropriate when asking - taking the data for granted and combining with my knowledge, beliefs and/or utility considerations, what is an optimal decision to make? This is a relatively easy question when compared to the issue of estimating the true value of a parameter, and how uncertain that estimate is.
As a result of these different goals, there are two completely different definitions, and thus formulas, for the Mean Squared Error (MSE) - a frequentist and a Bayesian (Spanos 2017, 10). A frequentist asks what the mechanism is which generated the data, and seeks rejection of certain models of such mechanisms, through examining how unlikely is it for the data at hand to have been generated by them. In the case of A/B testing, the question is: are these two/three/... sets of data generated by different mechanisms, or the same mechanism? Frequentist estimates tell us something about the data and its error probabilities, and leave decision-making to the researcher/businessman/stakeholder/etc. This is unlike Bayesian ones, which actually incorporate decision-making information into their output.
A p-value or confidence interval does not include in its derivation any prior information or decision-making considerations. Anyone is free to use the estimate and its error probabilities with their own utility function or beliefs. Specific ways to incorporate error probabilities into a risk-minimizing mathematical apparatus, which also takes into account certain prior beliefs, is explored in Chapter 11. Note that in using this tool the statistical estimates are not altered, rather the parameters of the A/B testing procedure are tweaked so that the procedure ultimately results in estimates with acceptable error.
Contrast this to the Bayesian 'counterpart': a Bayes factor, a credible interval, a quintile of the posterior distribution, or any other. It is the result of mixing together A/B test data and external considerations and beliefs in a way which makes it hard to untangle the two. The output is not something which can be used to make inferences about the objective state of nature, nor can it be used by someone with different external considerations. For this to happen, one needs to first subtract the prior information and only then can uncertainty be objectively estimated (using frequentist methods).
This is an especially critical issue for many current applications of Bayesian approaches in A/B testing, since software vendors often use priors and/or utility functions without these being communicated to the user at all (Georgiev, Issues with Current Bayesian Approaches to A/B Testing in Conversion Rate Optimization 2017) (Georgiev, The Google Optimize Statistical Engine and Approach 2018). In most cases, the user is not given the chance to alter the prior probability or utility function by incorporating their own information and considerations. I argue that the two issues above make it near impossible for end users to properly use such tools, and to interpret their output in a meaningful way in most situations.
While some of the practical issues can be fixed in time via upgrades and changes from software vendors, the conceptual issues with Bayesian methods would remain and they are the reason that I would not recommend them as a tool for statistical inference or decision-making in A/B testing. In my view, decision making starts before a test is conducted, via the process of choosing the parameters of the test, in such a way as to ensure that an optimal risk/reward ratio is achieved. Statistical methods must then provide a good estimate vis-à-vis the true data generating mechanism based on an adequate model and the observed data.
The business decision to be made after the test is inherently easier, since the heavy lifting has been done in the planning phase where the success or failure of the test were determined based on its capacity to allow exploration under controlled risk. Even if someone were to enter the process after the experiment has been completed, they can still use the resulting statistics combined with their own external considerations, in order to reach a conclusion or business decision.







Chapter 14
COMMUNICATING STATISTICAL RESULTS
All statistical methods, databases, and tools that we perform statistical analysis with would be in vain if we were also to fail to present the outcome in a way that people less proficient in statistics would find useful. The fifteen hours spent on statistical design, data cleaning, and analysis of a test would be wasted should it fail to make any impact on decision-making due to poor presentation.
It is almost inevitable that some A/B test data will not be perceived, and that some which is perceived will not be properly understood. This is expected to be the case, similarly to how it is expected from an internal combustion engine that some energy will be wasted in the process of converting heat to the rotation of its axis. Some of our data may even end up being misunderstood.
However, the more data we manage to surface in a meaningful and usable way, the more efficient we become as professionals. The less likely our presentation is to cause confusion or misunderstanding, and the better analysts we are. Unlike an internal combustion engine, our usefulness is not limited by the laws of thermodynamics, so we can, in theory, achieve 100% accuracy and efficiency in our presentation. It is therefore essential to always strive to improve our data communication skills.
This brief chapter is intended as a reminder of that fact, as well as being a primer on some of the best practices, which should be employed on every occasion that we are presenting statistics.
14.1 Changing the perception of data variability
One significant obstacle for understanding statistics is the need to appreciate variability and measurement uncertainty. In most people's deterministic worldview, there must be a reason behind every change and shift in a business metric, at which point focus then shifts to determining the source and implementing a fix.
Imagine an engineer who is looking at a machine which is not producing output with the desired parameters - this process is focused on trying to figure out what is wrong with the machine, and ultimately fix it. However, both an engineer and a manager would be in error were they to react to every deviation from the standard by intervening in some way, thinking that they know the reason behind it.
Even the biggest amounts of data are still subject to variance intrinsic to the data-generating mechanism, assuming that we are applying statistics in situations in which outcomes vary over time, and/or between objects or subjects (a prerequisite discussed in Chapter 1). If someone reacts to every change or level shift, they will most likely increase the variation or worsen the mean performance of whatever indicator they are focused on, or both, due to misidentification of the issue.
If a machine produces a part which is 0.5mm wider than specifications due to its natural variation, and in response an engineer corrects its settings so it starts producing narrower parts, many parts in the next batch will turn out narrower than specification, as this was likely to be a correction of a non-existing problem. Similarly, imagine that a manager reacts to a daily drop of the purchase conversion rate of a given test variant by immediately stopping the test. There is no consideration for its performance over the two-week duration of the test, and no recognition that it can be just part of natural variation. Acting in such a way will significantly increase the probability of failing to implement an actual improvement.
Since at any point there are a near-infinite number of factors affecting a metric, and, as we have discussed - we cannot hope to understand and measure all of them, we are not in a good position to determine the exact reason for any particular change of a parameter of interest. However, what we can do, if we are to determine what would be a beneficial intervention, is to perform an A/B test in which we model the process as the interaction of two or more random variables. The only difference behind them being the change(s) that we want to introduce.
We obviously already know that there should not be any peeking at the results, with part of this being due to the need to have statistical error control and part of it due to external validity considerations. But how do we communicate this to higher ups? While some people will understand the need to consider the whole performance of the test up to that point, and will not freak out from a single day of bad results, how do we explain the need to refrain from unaccounted peeking at the overall data due to its effect on the error probabilities?
While there is value in theoretical explanations and metaphors, a practical demonstration is always a useful tool to consider. If one does not mind being a bit sneaky, one can ensure that there is a good A/B testing setup then sneak in an A/A test without telling anyone that it is in fact an A/A test. The A/A test can be presented as testing some trivial change and reported on in daily meetings, with calculated naïve statistical significance, confidence intervals, and so on. There is a high probability that the A/A test would yield a nominally statistically significant result early on. If that happens, one can innocently ask if they should terminate the test, as it has clearly demonstrated the superiority/inferiority of the test variant.
After such an exercise, one can reveal the deception, and explain what is happening. If possible, several A/A tests can be performed in parallel, greatly increasing the probability that one of them will produce remarkable results at one point or another. The above exercise should be helpful in demonstrating the inherent variability of the data which is still deterministic, but due to the multiplicity of factors, and the very limited nature of our information, the best one can do is to model it as a noisy data stream, a random variable, and gather enough of it so that the noise is acceptable for the task at hand.
Running several hundred parallel A/A tests might be a way to demonstrate how in a worst-case scenario there will be some false positives, even though testing procedures were followed with precision. Obviously, in this case one should not try to hide their intentions and explain the purpose of the exercise up front. This would be a great way of demonstrating that 'rare', 'improbable', 'unlikely' means 'inevitable, given enough observations' in statistics, unlike the everyday meaning of 'will not happen'. This should serve as an illustration of the fact that even if we do everything right in an A/B test, there is a chance that we will make a poor decision afterwards, and that nothing is ever 100% certain.
Explaining and demonstrating data variability, as well as the inherent uncertainty of information is a key foundation in proper communication of statistical results.
The perception of data variability should ultimately correspond to the perception of risk, and should help us convey the utility of A/B tests as estimations and risk management tools which, if used correctly, always have a positive return on investment. Risk is unavoidable in the long run, but it is manageable, to an extent. The risk-management function of A/B testing thus becomes clearer - it does not eliminate the risk of poor business decisions, but it can keep it below certain acceptable levels.
One thing I'd like to mention with regard to communicating uncertainty is to avoid betting metaphors, whenever possible. While there are some similarities between betting and real life, betting is based on fixed and known odds while real-life is not. In an A/B test, we estimate the true odds, and they may change at any point, without warning, unlike the rules of a game. Therefore, betting metaphors might convey a false sense of certainty of the estimation or prediction, when it is not warranted to such an extent.
14.2 Translating business questions into statistical models
One of the other hurdles to efficient communication of statistical results from A/B tests stems from issues caused by translating business goals into measurements, and a statistical model for estimating its uncertainty. The role of A/B tests as enhancing any raw data, and providing a measure of the uncertainty involved in a business metric, needs to be explicitly defined in order to successfully translate goals to measurements. It will also help avoid confusing statistical estimation with a decision-making apparatus.
In order to facilitate good understanding of post-test results, the person responsible for the statistical design and analysis of the test should gather as much information as possible regarding the actual business question that the A/B test is planned for. This includes understanding the level of measurement accuracy which is deemed sufficient to make the relevant decision. A well-defined question is crucial for the subsequent communication of the results.
The more specific the question is, the more specific and actionable the results. Broader questions result in more difficult interpretation of the data and less precise answers. The question should be reformulated until its answer can be clearly defined in terms of one or more measurements. At this stage, the role of the statistician is very close to that of a translator or mediator. A good translation of the business needs into measurements makes the reverse process after the test is completed much easier.
A well-defined business question translated properly into a statistical model is key for communicating the results of any A/B test.
Note that specifying the question includes not only securing agreement on the primary and possibly secondary key performance indicators (what we need to measure), but also encompasses estimating the costs of testing, including design and development time and resources, as well as costs or savings which can be incurred following the success of the tested variant (these may be different for different variants, further complicating the task). Agreeing on an estimate of the cost of reversing the decision, if it is later determined as made in error, is also important. These are all related to estimating the required precision of the measurement.
Having all this information, a statistician can use the risk/reward calculation methods discussed previously to calculate an optimal balance between sample size and significance threshold. Acquiring agreement on these two parameters, and explaining what they mean in terms of the statistical power versus a range of effect sizes, is key, and so is the agreement around the monitoring scheme of the test. If there is some disagreement about inputs to the risk/reward calculation, several different calculations can be performed with different inputs and then compared for compatibility. A compromise needs to be achieved in the end.
Alternatively, agreement on significance threshold, statistical power, and a minimum effect of interest can be achieved, although it seems that this is more difficult to achieve in practice than focusing on the significance threshold and sample size, with power and MEI explored merely as resulting from the first two parameters.
If a sequential test is chosen, the number and timing of analyses should be agreed upon, and even if using spending functions, deviations from the schedule should not be introduced without good reason.
The design of an experiment and an adequate statistical model can be chosen only after acquiring agreement on what needs to be measured and the required level of measurement precision.
It should be made clear to decision-makers that an A/B test allows the business to experiment safely with a proposed change, but its outcomes can be interpreted differently depending on the overall business context. A given level of uncertainty may be perfectly acceptable if certain circumstances are present, but conversely could be completely unacceptable in other scenarios. An A/B test provides a measurement of the uncertainty, but does not automatically lead to a decision of any kind, nor should it.
14.3 Presenting statistical results
Assuming that we have been successful in the tasks outlined in the chapter so far, this part will be almost be a walk in the park. But there are still a few useful guidelines which can help it be more pleasant and useful for everyone involved.
Something to always keep in mind is that different people consume information differently. While graphical representation usually trumps all other modes in terms of speed of information transfer, there are people who find information easier to digest if it is presented in a more verbose manner. Others prefer tabular form. In many cases information is best understood when presented in two or three complementary and overlapping mediums. Therefore, it is advisable to strive to provide information in at least two of the three main forms - visual, verbal, and numeric / tabular.
For example, we can present the difference in conversion rates on a graph, and then state it verbally in a sentence. Usually, there is more data to communicate, so we can consider adding a table. A table with conditional formatting for some of the cells combines visual cues with numerical information.
Let us consider a simple A/B test as the basis for our examples below. The observed relative difference between the variant B and the control A is 5%, and we are using proper confidence intervals and p-value calculations for percentage change. We are measuring purchase conversion rate and the control converts at 2%. We want to present the results for three predefined segments - users on mobile, desktop, and tablet devices (ignoring the difficulty of a single user being part of more than one segment, for the sake of simplicity).
Graphs
Simple line or dot charts are preferable for trend data and segment analysis, while box and whisker charts fit nicely when interval estimates are presented. The simplest presentation looks something like:


Figure 14.1: Example presentation of A/B test results.

However, such a presentation can easily be improved by adding more information, in order to it to make it more self-contained. Adding axis labels and improving the title would be a good first guess. An improved graph:


Figure 14.2: Example presentation of A/B test results.

However, there are still issues. The use of notations, abbreviations and acronyms breaks the self-containment of a graph, and requires users to use information surrounding it, or worse, a search engine, in order to understand what the graph is demonstrating. It is also unclear what the dark-grey bar represents. A much-improved version looks like Figure 14.3:


Figure 14.3: Example presentation of A/B test results.

The user can understand the graph with little to no need of external information. When abbreviations and acronyms are inevitable, they should always be explained, ideally on first use. Attaching a standard abbreviations/acronyms reference to reports is also good practice.
If we want to present the outcomes of the same A/B test per segment, we can resort to a box and whisker chart as shown in Figure 14.4 or to a bar chart as shown in Figure 14.5.
When possible, avoid stacked bar graphs, as they are quite difficult to read properly, and the same applies to pie charts. 3D effects should be avoided on most chart types, as they notoriously exaggerate certain data points at the expense of others.


Figure 14.4: Example presentation of A/B test results for more than one segment.



Figure 14.5: Example presentation of A/B test results for more than one segment.

Ensure that you do not truncate axes and, if it is absolutely necessary, use proper graphical notation to denote it. However, since many end users are not aware of these notations, and may have significant difficulty correctly interpreting the data even if they are, I'd avoid axes which show just part of the data. The only exception is for data which goes to plus or minus infinity, such as the bound of a one-sided interval.
Graphical representations should help the distinction between warranted and unwarranted conclusions. Error bars / confidence intervals are a standard choice for representing uncertainty, but as they are constructed at a fixed confidence threshold, they might tempt users into providing "yes" or "no" answers with no regard for nuance.
Whenever possible, it is useful to provide more nuanced representations, instead of representation leading to binary interpretation. Tools to consider for this purpose include p-value curves and severity curves.
A p-value curve is simply the p-value calculated for each value of the parameter, as if the null hypothesis is defined at that point or terminates at that point.


Figure 14.6: p-value curve.

Above is an example for a p-value curve, if the result of our test was an observed relative difference of 5%, and it was barely significant at the 0.05 significance level. The graph is at exactly 0.05 for δ* = 0%, which is the p-value for the following null hypothesis:

and it is at 0.011 for -2%, meaning that had we considered this null hypothesis:

the p-value would be 0.011. The p-curve allows us to easily evaluate the level of significance for a range of null hypotheses. Note that with a one-sided p-value a null hypothesis of the type:

always has a p-value of 0.5.
A severity curve, on the other hand usually depends on whether the resulting p-value meets the significance threshold. If it does, then SEV is calculated for accepting a specified alternative hypothesis, and is therefore the exact inverse of a p-curve since:


Figure 14.7: Severity curve example.

Remember that higher SEV means more warranted inference. Inferring that percentage change is greater than any value below 0% can be made with high severity, while concluding that the true lift is greater than any value above 10% would be very poorly supported by the data.
If the result is not statistically significant, e.g. if the observed relative difference is just 2% with the same data set, then severity is usually calculated with regard to accepting the null hypothesis:


Figure 14.8: Severity curve example.

From this second curve we can determine that inferring the true change is 0% or less would be poorly supported by the data, while inferring that it is less than any value above 7% is relatively well supported.
Note that you can calculate both curves regardless of the outcome, if you want to explore warranted conclusions in both directions. However, given a sound choice of design parameters, only one curve should be appropriate at a time.
As already mentioned in previous chapters, severity and severity curves are a good tool for clear communication of test results and differentiation between warranted and unwarranted claims, as they block incorrect inferences, regardless of the outcome of the A/B test.
Stories
A nice quote which illustrates the importance of the verbal telling of statistics and shaping them into a story or narrative can be found in (Abelson 1995):
"The virtues of a good statistician, therefore, involve not only the skills of a good detective, but also the skills of a good storyteller. As a good storyteller, it is essential to argue flexibly and in detail for a particular case; data analysis should not be pointlessly formal. Rather, it should make an interesting claim by telling a tale that an informed audience will care about, doing so through an intelligent interpretation of data."
Any guide to storytelling in a business context should be useful for the statistician here. Adopting a journalistic-style presentation can also be very successful. More specifically, state the important points first, then methodology, supporting tables, caveats, disclaimers, and so on, follow.
Breaking the presentation into relatively self-contained sections is good practice when reporting on more complex tests; e.g. those considering multiple primary or secondary metrics, segments, and so on. By using this approach, a user can look into the part of immediate concern, without being bothered to go through a lengthy report, most of which may be irrelevant to his current needs.
Also, make use of the verbal presentation in order to change or add additional ways of presenting the same data. For example, a proportion or rate of 0.05 can be communicated as 5% or, even better, as 1 in 20. One presentation can complement the other, e.g.: "The baseline conversion rate is 5%, meaning that out of 20 users who enter the website 1 completes a purchase.". When rates and probabilities are communicated as natural frequencies then it is less likely that they will be misunderstood.
A mild caveat is that presenting data as a story or narrative can introduce a temptation to embellish it with conjectures which may not be well supported by the underlying data. Presenting data as a story is not a ticket for introducing unsupported conclusions or embellishments. It is a change in form, not in content.
Tables
Tables should follow general best practices for readability and visual design. It is important that rows and columns are clearly labeled. While abbreviations and acronyms are acceptable, they should ideally be explained in the footnotes directly beneath each table. Floating point values should be rounded to a reasonable degree, in order to facilitate easier comprehension.
Table 14.1 is an example of a table in need of improvement.



 

Conversion Rate

Revenue Per User

% Change vs. Control

p-value



Control (A)

2.2035%

$2.456789

N/A

N/A



Variant (B)

2.29492%

$2.56889

4.14838%

0.06849



Variant (C)

2.41058%

$2.713258

9.39727%

0.02248




Table 14.1: Example tabular presentation of A/B test results.

Table 14.2 is an improved version with clearer labels, a descriptive title and footnotes with some meta data.



A/B Test Results - "Checkout Improvement (May 2019)", #586 - Desktop Only



 

Conversion Rate per User

Average Revenue Per User

% Change in CR vs. Control

p-value (H0: Δ% ≤ 0%)



Control (A)

2.20%

$2.46

N/A

N/A



Variant (B)

2.30%

$2.57

4.15%

0.069



Variant (C)

2.41%

$2.71

9.40%

0.023



Data for the period Apr 1 - May 11, 2019, source: Google Analytics view #423456, custom dimension 12.




Table 14.2: Improved tabular presentation of A/B test results.

Note how one of the changes is to specify the null hypothesis under which the p-value was computed, making it clear what claim can be rejected if it falls below an agreed upon threshold.
Certain tables are excellent candidates for employing conditional formatting; a tool that which I generally recommend.
Meta data
Making sure to include meta information about the data source, methodology of gathering, definitions of metrics, as well as other notable characteristics of the data in a meta data summary or sheet is something every practitioner finds invaluable if they happen to use their own data after a while. Key statistical assumptions, and assumptions about the accuracy and meaning of data, should also be included. It is near-mandatory if one works in a larger team, and needs to communicate results to peers or others.
Annotating any changes to methodology, tooling, metric definitions, and any lost or corrupted data will be very helpful for current data consumers, as well as when any attempts are made at revisiting the report long after it has initially been prepared. The included meta information should be sufficient to arrive at correct statistical inferences.
Context and external validity
Providing relevant context for all observations is key for a good report. This information should also include caveats for the external validity of the results. For example, if this is the second test of this kind of change, it should be noted, even though it has no direct bearing on the statistics. If any background information was used to inform the duration of the test, the timing of analyses, or other important parameters, it should also be included in the report.







EPILOGUE
In this short book, we covered a lot of ground, starting with the proper role of statistical inference in business decision-making, going through the basic concepts of statistical models, statistical hypotheses, statistical estimators, before proceeding to examine different models encompassing various practical cases, and finally ended with advice on how to make it all as impactful as possible. The linking theme between all of these topics were the business objectives which can be achieved with the use of statistical methods - estimation of noisy metrics and management of business risks related to sustainable growth.
I hope that I have succeeded in my goal of making this an accessible read for readers with little mathematical and statistical background, while also being useful for the statistically savvy by covering a diverse set of methods in fair detail, and placing them in a rich business context.
While qualitative feedback is not the topic of the book, it would certainly be appreciated, so I encourage you to share your user experience with it by reaching me at https://www.linkedin.com/in/geoprofi/ or dropping me a line at georgi@webfocus.bg.







REFERENCES
Abelson, R.P. 1995. Statistics as Principled Argument. Hillsdale, NJ: Lawrence Erlbaum Associates, Inc.
Anderson, T.W., and D.A. Darling. 1954. "A Test of Goodness of Fit." Journal of the American Statistical Association 49: 765-769. doi:10.2307/2281537.
Armitage P., McPherson, C.K., Rowe, B.C. 1969. "Repeated Significance Tests on Accumulating Data." Journal of the Royal Statistical Society 132: 235-244.
Armitage, P. 1957. "Restricted Sequential Procedures." Biometrika 44: 9-26. doi:10.2307/2333237.
Armitage, P., C. K. McPherson, and B.C. Rowe. 1969. "Repeated Significance Tests on Accumulating Data." Journal of the Royal Statistical Society, Series A 132: 235-244. doi:10.2307/2343787.
Bonferroni, E. C. 1936. "Teoria statistica delle classi e calcolo delle probabilità." Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze 8: 3-62.
Bretz, F., W. Maurer, W. Brannath, and M. Posch. 2009. "A Graphical Approach to Sequentially Rejective Multiple Test Procedures." 28 (4): 586-604. doi:10.1002/sim.3495.
Burman, C.F., C. Sonesson, and O. Guilbaud. 2009. "A Recycling Framework for the Construction of BonferroniBased Multiple Tests." Statistics in Medicine 28 (5): 739-761. doi:10.1002/sim.3513.
Campbell, D.T., and J.C. Stanley. 1963. Experimental and Quasi-Experimental Designs for Research. Chicago: Rand McNally & Company.
Chang, M., and J. Balser. 2016. "Adaptive Design - Recent Advancement in Clinical Trials." Journal of Bioanalysis and Biostatistics 1 (1): 1-14.
Chang, M.N. 1989. "Confidence Intervals for a Normal Mean Following a Group Sequential Test." Biometrics 45 (1): 247-254. doi:10.2307/2532050.
Chang, M.N., A. Lawrence Gould, and S.M. Snapinn. 1995. "P-values for Group Sequential Testing." Biometrika 82 (3): 650-654. doi:10.2307/2337542.
Cox, D.R. 1978. "Foundations of Statistical Inference: the Case for Eclecticism." Australian Journal of Statistics 20 (1): 43-59. doi:10.1111/j.1467-842X.1978.tb01094.x.
Cramer, H. 1928. "On the composition of elementary errors." Skandinavisk Aktuarietidskrift 11: 13-74, 141-180.
D'Agostino, R.B., and E.S. Pearson. 1973. "Tests for Departure from Normality." Biometrika 60: 613-622. doi:10.2307/2335012.
Dmitrienko, A., R.B. D'Agostino, and M.F. Huque. 2013. "Key Multiplicity Issues in Clinical Drug Development." Statistics in Medicine 32 (7): 1079-1111. doi:10.1002/sim.5642.
Dunnett, C.W. 1955. "A Multiple Comparison Procedure for Comparing Several Treatments with a Control." Journal of the American Statistical Association 50 (272): 1096-1121.
Dunnett, C.W., and A.C. Tamhane. 1991. "Step-Down Multiple Tests for Comparing Treatments with a Control in Unbalanced One-Way Layouts." Statistics in Medicine 10: 939-947. doi:10.1002/sim.4780100614.
Emerson, S.S., and T.R. Fleming. 1990. "Parameter Estimation Following Group Sequential Hypothesis Testing." Biometrika 77 (4): 875-892. doi:10.1093/biomet/77.4.875.
Fan, X., and D.L. DeMets. 2006. "Conditional and Unconditional Confidence Intervals Following a Group Sequential Test." Journal of Biopharmaceutical Statistics 16: 107-122. doi:10.1080/10543400500406595.
Fan, X., D.L. DeMets, and K.K.G. Lan. 2004. "Conditional Bias of Point Estimates Following a Group Sequential Test." Journal of Biopharmaceutical Statistics 14 (2): 505-530. doi:10.1081/BIP-120037195.
Fisher, R.A. 1922. "On the Mathematical Foundations of Theoretical Statistics." Philosophical Transactions of the Royal Society 222: 594-604. doi:10.1098/rsta.1922.0009.
—. 1925. Statistical Methods for Research Workers. 11. Edinburgh: Oliver and Boyd.
—. 1935. The Design Of Experiments. Oxford: Oliver & Boyd.
Georgiev, G.Z. 2018. 12 Myths About One-Tailed vs. Two-Tailed Tests of Significance. Aug 6. Accessed Jun 11, 2019. https://www.onesided.org/articles/12-myths-one-tailed-vs-two-tailed-tests-of-significance.php.
—. 2018. Analysis of 115 A/B Tests: Average Lift is 4%, Most Lack Statistical Power. Jun 26. Accessed Jun 11, 2019. http://blog.analytics-toolkit.com/2018/analysis-of-115-a-b-tests-average-lift-statistical-power/.
—. 2017. "Bayesian AB Testing is Not Immune to Optional Stopping Issues." Analytics-toolkit.com. May 21. Accessed Jun 11, 2019. http://blog.analytics-toolkit.com/2017/bayesian-ab-testing-not-immune-to-optional-stopping-issues/.
—. 2018. Confidence Intervals & P-values for Percent Change / Relative Difference. June 6. Accessed Jun 11, 2019. http://blog.analytics-toolkit.com/2018/confidence-intervals-p-values-percent-change-relative-difference/.
—. 2017. "Efficient A/B Testing in Conversion Rate Optimization: The AGILE Statistical Method." www.analytics-toolkit.com. May 22. Accessed May 30, 2019. https://www.analytics-toolkit.com/pdf/Efficient_AB_Testing_in_Conversion_Rate_Optimization_-_The_AGILE_Statistical_Method_2017.pdf.
—. 2018. Fisher, Neyman & Pearson: Advocates for One-Sided Tests and Confidence Intervals. Aug 6. Accessed Jun 11, 2019. https://www.onesided.org/articles/fisher-neyman-pearson-advocates-one-sided-tests-confidence-intervals.php.
—. 2018. Is the widespread usage of two-sided tests a result of a usability/presentation issue? Aug 6. Accessed Jun 11, 2019. https://www.onesided.org/articles/widespread-usage-of-two-sided-tests-result-of-usability-issue.php.
—. 2017. "Issues with Current Bayesian Approaches to A/B Testing in Conversion Rate Optimization." Analytics-toolkit.com. May 18. Accessed Jun 11, 2019. https://www.analytics-toolkit.com/whitepapers.php?paper=issues-with-baysian-ab-testing-approaches-cro.
—. 2017. One-tailed vs Two-tailed Tests of Significance in A/B Testing. Oct 17. Accessed Jun 11, 2019. http://blog.analytics-toolkit.com/2017/one-tailed-two-tailed-tests-significance-ab-testing/.
—. 2018. Representative Samples and Generalizability of A/B Testing Results. Sep 10. Accessed Jun 11, 2019. http://blog.analytics-toolkit.com/2018/representative-samples-generalizability-a-b-testing-results/.
—. 2017. The Case for Non-Inferiority A/B Tests. Sep 12. Accessed Jun 11, 2019. http://blog.analytics-toolkit.com/2017/case-non-inferiority-designs-ab-testing/.
—. 2018. "The Google Optimize Statistical Engine and Approach." Analytics-toolkit.com. Apr 11. Accessed Jun 11, 2019. http://blog.analytics-toolkit.com/2018/google-optimize-statistical-significance-statistical-engine/.
Gupta, S., R. Kohavi, D. Tang, Y. Xu, R. Andersen, E. Bakshy, N. Cardin, et al. 2019. "Top Challenges from the first Practical Online Controlled Experiments Summit." ACM SIGKDD Explorations Newsletter 21 (1): 20-35. doi:10.1145/3331651.3331655.
Hájek, A. 2007. "The Reference Class Problem is Your Problem Too." Synthese 156: 563-585. doi:10.1007/s11229-006-9138-5.
Harari, Y.N. 2011. Sapiens: A Brief History of Humankind. Harper.
Haybittle, J.L. 1971. "Repeated assessment of results in clinical trials of cancer treatment." The British Journal of Radiology 44: 793-797. doi:10.1259/0007-1285-44-526-793.
Hern, A. 2014. Why Google has 200m reasons to put engineers over designers. 02 5. https://www.theguardian.com/technology/2014/feb/05/why-google-engineers-designers.
Holm, S. 1979. "A Simple Sequentially Rejective Multiple Test Procedure." Scandinavian Journal of Statistics 6 (2): 65-70.
Holson, L.M. 2009. Putting a Bolder Face on Google. 02 28. http://www.nytimes.com/2009/03/01/business/01marissa.html?pagewanted=3.
Honhnhold, .H., D. O'Brien, and D. Tang. 2015. "Focusing on the Long-term: It's Good for Users and Business." Conference on Knowledge Discovery and Data Mining. Sydney: Association for Computing Machinery. 1849-1858. doi:10.1145/2783258.2788583.
Hwang, I.K., W.J. Shih, and J.S. De Cani. 1990. "Group Sequential Designs Using a Family of Type I Error Probability Spending Functions." Statistics in Medicine 9 (12): 1439-1445. doi:10.1002/sim.4780091207.
Jarque, C.M., and A.K. Bera. 1987. "A test for normality of observations and regression residuals." International Statistical Review 55 (2): 163-172. doi:10.2307/1403192.
Jennison, C., and B.W. Turnbull. 2010. "From Group Sequential to Adaptive Designs." In Handbook of Adaptive Designs in Pharmaceutical and Clinical Development, by A., Chow, S-C, Pong, 496. Boca Raton: CRC Press. doi:10.1201/b10279-6.
Jennison, C., and B.W. Turnbull. 1989. "Interim Analyses: The Repeated Confidence Interval Approach." Journal of the Royal Statistical Society 51 (3): 305-361.
Kim, K., and D.L. DeMets. 1987. "Confidence Intervals Following Group Sequential Tests in Clinical Trials." Biometrics 43 (4): 857-864. doi:10.2307/2531539.
Kim, K., and D.L. DeMets. 1987. "Design and Analysis of Group Sequential Tests Based on the Type I Error Spending Rate Function." Biometrika 74: 149-154. doi:10.1093/biomet/74.1.149.
Kohavi, R., R. Longbotham, D. Sommerfield, and R.M. Henne. 2008. "Controlled Experiments on the Web: Survey and Practical Guide." Data Mining and Knowledge Discovery 18 (1): 140-181.
Lan, K.K.G., and D.L. DeMets. 1989. "Changing Frequency of Interim Analyses in Sequential Monitoring." Biometrics 45: 1017-1020. doi:10.2307/2531701.
Lan, K.K.G., and D.L. DeMets. 1983. "Discrete Sequential Boundaries for Clinical Trials." Biometrika 70: 659-663. doi:10.2307/2336502.
Lan, K.K.G., and D.L. DeMets. 1994. "Interim Analysis: The Alpha Spending Function Approach." Statistics in Medicine 13: 1341-52. doi:10.1002/sim.4780131308.
Lee, J.J., N. Chen, and G. Yin. 2012. "Worth Adapting? Revisiting the Usefulness of Outcome-Adaptive Randomization." Clinical Cancer Research 18 (17): 4498-4507. doi:10.1158/1078-0432.CCR-11-2555.
Liu, A., and W.J. Hall. 1999. "Unbiased Estimation Following a Group Sequential Test." Biometrika 86 (1): 71-78.
Mauer, W., and F. Bretz. 2013. "Multiple Testing in Group Sequential Trials Using Graphical Approaches." Statistics in Biopharmaceutical Research 5 (4): 311-320. doi:10.1080/19466315.2013.807748.
Mayo, D.G. 1983. "An Objective Theory of Statistical Testing." Synthese 57 (3): 297-340. doi:10.1007/BF01064701.
—. 1996. Error and the Growth of Experimental Knowledge. Chicago, Illinois: University of Chicago Press. doi:10.1080/106351599260247.
—. 2018. Statistical Inference as Severe Testing. Cambridge: Cambridge University Press.
Mayo, D.G., and A. Spanos. 2011. Error Statistics. Vol. 7, in Handbook of Philosophy of Science Volume 7 - Philosophy of Statistics, by D.G., Spanos, A. et al. Mayo, 1-46. Elsevier.
Mayo, D.G., and A. Spanos. 2006. "Severe Testing as a Basic Concept in a Neyman-Pearson Philosophy of Induction." The British Journal for the Philosophy of Science 57 (2): 323-357. doi:10.1093/bjps/axl003.
Mbah, A.K., and A. Paothong. 2014. "Shapiro-Francia test compared to other normality test using expected p-value." Journal of Statistical Computation and Simulation 85 (15): 3002-3016. doi:10.1080/00949655.2014.947986.
Neyman, J., and E.S. Pearson. 1933. "On the Problem of the Most Efficient Tests of Statistical Hypotheses." Philosophical Transactions of the Royal Society of London 231: 289-337.
O'Brien, P.C., and T.R. Fleming. 1979. "A Multiple Testing Procedure for Clinical Trials." Biometrics 35: 549-556. doi:10.2307/2530245.
Pampallona, S., A.A. Tsiatis, and K.M. Kim. 2001. "Interim Monitoring of Group Sequential Trials Using Spending Functions for the Type I and Type II Error Probabilities." Drug Information Journal 35: 1113-1121. doi:10.1177/009286150103500408.
Peto, R., M.C. Pike, P. Armitage, N.E. Breslow, D.R. Cox, S.V. Howard, N. Mantel, K. McPherson, J. Peto, and P.G. Smith. 1976. "Design and analysis of randomized clinical trials requiring prolonged observation of each patient. I. Introduction and design." British Journal of Cancer 34 (6): 585-612. doi:10.1038/bjc.1976.220.
Pocock, S.J. 1977. "Group sequential methods in the design and analysis of clinical trials." Biometrika 64: 191-199. doi:10.2307/2335684.
Reboussin, D.M., D.L. DeMets, K.M. Kim, and K.K. Lann. 2000. "Computations for group sequential boundaries using the Lan-DeMets spending function method." Controlled Clinical Trials 21 (3): 190-207. doi:10.1016/S0197-2456(00)00057-X.
Rosner, G.L., and A.A. Tsiatis. 1988. "Exact Confidence Intervals Following a Group Sequential Trial: A Comparison of Methods." Biometrika 75 (4): 723-729. doi:10.1093/biomet/75.4.723.
Rosset, C., M. Ige, and E. Duhaime. 2017. "Regretful Bandits: A Survey of Algorithms and Regret Bounds for Adversarial Multi-Armed Bandits." Accessed 6 11, 2019. http://corbyrosset.com/files/banditsurvey.pdf.
Royston, P. 1993. "A Pocket-Calculator Algorithm for the Shapiro-Francia Test for Normality - An Application to Medicine." Statistics in Medicine 12 (2): 181-184. doi:10.1002/sim.4780120209.
Russell. 2016. A Very Scottish History of Insurance. 9 13. http://sonsofscotland.com/scottish-history-insurance/.
Sashegyi, A., and D. Ferry. 2017. "On the Interpretation of the Hazard Ratio and Communication of Survival Benefit." The Oncologist 22: 484-486. doi:10.1634/theoncologist.2016-0198.
Senn, S. 2012. "Seven Myths of Randomisation In Clinical Trials." Statistics in Medicine 32: 1439-1450. doi:10.1002/sim.5713.
Shapiro, S.S., and M.B. Wilk. 1965. "An analysis of variance test for normality (complete samples)." Biometrika 52: 591-611. doi:10.2307/2333709.
Shapiro, S.S., and R.S. Francia. 1972. "An approximate analysis of variance test for normality." Journal of the American Statistical Association 67: 215-216.
Šidák, Z. 1967. "Rectangular Confidence Regions for the Means of Multivariate Normal Distributions." Journal of the American Statistical Association 62 (318): 626-633. doi:10.2307/2283989.
Spanos, A. 1999. Probability Theory and Statistical Inference. Cambridge University Press. doi:10.1017/CBO9780511754081.
Spanos, A. 2014. "Revisiting Bayes Rule and Evidence - Notional Events vs Real Data." Virginia Tech, working paper.
Spanos, A. 2017. "Why the DecisionTheoretic Perspective Misrepresents Frequentist Inference: Revisiting Stein's Paradox and Admissibility." In Advances in Statistical Methodologies and Their Application to Real Problems, by Spanos et.al., edited by Tsukasa Hokimoto, 3-29. Rijeka: InTech. doi:10.5772/65720.
Spruance, S.L., J.E. Reid, M. Grace, and M. Samore. 2004. "Hazard ratio in clinical trials." Antimicrobial agents and chemotherapy 48 (8): 2787-2792. doi:10.1128/AAC.48.8.2787-2792.2004.
Stare, J., and D. Maucort-Boulch. 2016. "Odds Ratio, Hazard Ratio and Relative Risk." Metodološki zvezki 13 (1): 59-67.
Stouffer, S.A., E.A. Suchman, L.C. DeVinney, S.A. Star, and R.M. Jr. Williams. 1949. The American Soldier, Vol. 1: Adjustment during Army Life. Princeton, USA: Princeton University Press. doi:https://doi.org/10.2307/2572105.
Strickland, P.O.A., and G. Casella. 2003. "Conditional Inference Following Group Sequential Testing." Biometrical Journal 45 (5): 515-526. doi:10.1002/bimj.200390029.
Student. 1908. "The Probable Error of the Mean." Biometrika 6 (1): 1-25. doi:10.1093/biomet/6.1.1.
Tsiatis, A.A., G.L. Rosner, and C.R. Mehta. 1984. "Exact Confidence Intervals Following a Group Sequential Test." Biometrics 40 (3): 797-803. doi:10.2307/2530924.
Tsiatis, A.A., Mehta C. 2003. "On the Inefficiency of the Adaptive Design for Monitoring Clinical Trials." Biometrika 90 (2): 367-378.
von Mises, R. 1931. Wahrscheinlichkeitsrechnung und Ihre Anwendung in der Statistik und Theoretischen Physik". Julius Springer.
Wald, A. 1947. Sequential Analysis. J. Wiley & Sons, Inc.
Wald, A. 1945. "Sequential Tests of Statistical Hypotheses." The Annals of Mathematical Statistics 16: 117-186. doi:10.1214/aoms/1177731118.
Wason, J., N. Stallard, J. Bowden, and C. Jennison. 2017. "A Multi-Stage Drop-the-Losers Design for Multi-Arm Clinical Trials." Statistical Methods in Medical Research 26 (1): 508-524. doi:10.1177/0962280214550759.
Whitehead, J. 1986. "On the Bias of Maximum Likelihood Estimation Following a Sequential Test." Biometrika 73 (3): 573-581. doi:10.2307/2336521.
Wilks, S.S. 1938. "The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses." The Annals of Mathematical Statistics 9 (1): 60-62. doi:10.1214/aoms/1177732360.
Zabell, S. 1989. "R. A. Fisher on the History of Inverse Probability." Statistical Science 4 (3): 247-263. doi:10.1214/ss/1177012488.







INDEX
A/A test, 67-69, 255
A/B/n test, 111-29
allocation, 233-36
alpha-spending, 167-70
Bayesian, 249-52
beta-spending, 171-73
concurrent A/B tests, 226-31
confidence interval, 44-49
decision-making, 53-55
misinterpretations, 50-53
one-sided, 100-01
percent change, 153-54
sequential test, 184-85
confidence level, 45
continuous metric, 139, 141-42
cost-benefit analysis, 191-95
Dunnett's Correction, 116-17
efficacy boundary, 167-70
external validity, 217-19
factors, 219-22
improving, 223-25
factorial design, 123-28
Family-Wise Error Rate (FWER), 111-14
futility boundary, 171-73
hazard ratio, 238-42
holdout group, 236-38
Holm-Bonferroni, 114-15
hypothesis
family of, 113
non-inferiority, 106-9
one-sided, 98-101
statistical, 27-30
strong superiority, 103-6
substantive, 25-27
maximum likelihood estimate (MLE), 55-56
minimum detectable effect (MDE), 81-83
minimum effect of interest (MEI), 73, 81-83
modus tollens, 13
non-inferiority margin, 108-9
non-inferiority test, 106-9
primary outcome, 135-37
p-value, 38-44
adjustments, 114-17, 131-32, 184
decision-making, 53-55
for percent change, 154
misinterpretations, 50-53
nominal v. actual, 60-61
null distribution, 68
randomization, 16-18
representative sample, 218
return on investment (ROI), 189-90
optimal, 190-91
risk-reward analysis, 203-15
sample size, 85-93
continuous data, 141-43
for A/B/n tests, 118-21
sequential tests, expected, 173-181
secondary outcome, 138
segment analysis, 132-35
sequential probability ratio test (SPRT), 161-63
sequential testing, 161-186
severity, 57-58
curve, 264-65
Šidák correction, 131-32
significance threshold, 41
standard deviation, 31-36, 139-140
statistical adequacy, 66-69
statistical inference, 6-8, 10-14
statistical model, 27-30
assumptions, 59-66
statistical power, 74-80
continuous data, 141-43
misunderstandings, 95-96
overpowered, 84-85
underpowered, 83-84
survival curves, 238-242
type I error, 40, 99
in A/B/n test, 111-14
in repeated tests, 158-61
type II error, 71-74
variance, 31
Z-score
cut-offs, 34
in sequential tests, 165



