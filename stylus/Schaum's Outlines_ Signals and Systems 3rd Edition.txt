



















HWEI P. HSU received his BS from National Taiwan University and his MS and PhD from Case Institute of Technology. He has published several books, including Schaum's Outline of Analog and Digital Communications, and Probability, Random Variables, and Random Processes.
Copyright © 2014 by McGraw-Hill Education. All rights reserved. Except as permitted under the United States Copyright Act of 1976, no part of this publication may be reproduced or distributed in any form or by any means, or stored in a database or retrieval system, without the prior written permission of the publisher.
ISBN: 978-0-07-182947-2MHID:       0-07-182947-4
The material in this eBook also appears in the print version of this title: ISBN: 978-0-07-182946-5, MHID: 0-07-182946-6.
eBook conversion by codeMantraVersion 2.0
All trademarks are trademarks of their respective owners. Rather than put a trademark symbol after every occurrence of a trademarked name, we use names in an editorial fashion only, and to the benefit of the trademark owner, with no intention of infringement of the trademark. Where such designations appear in this book, they have been printed with initial caps.
McGraw-Hill Education eBooks are available at special quantity discounts to use as premiums and sales promotions, or for use in corporate training programs. To contact a representative please visit the Contact Us page at www.mhprofessional.com.
Trademarks: McGraw-Hill Education, the McGraw-Hill Education logo, Schaum's, and related trade dress are trademarks or registered trademarks of McGraw-Hill Education and/or its affiliates in the United States and other countries, and may not be used without written permission. All other trademarks are the property of their respective owners. McGraw-Hill Education is not associated with any product or vendor mentioned in this book.
TERMS OF USE
This is a copyrighted work and McGraw-Hill Education and its licensors reserve all rights in and to the work. Use of this work is subject to these terms. Except as permitted under the Copyright Act of 1976 and the right to store and retrieve one copy of the work, you may not decompile, disassemble, reverse engineer, reproduce, modify, create derivative works based upon, transmit, distribute, disseminate, sell, publish or sublicense the work or any part of it without McGraw-Hill Education's prior consent. You may use the work for your own noncommercial and personal use; any other use of the work is strictly prohibited. Your right to use the work may be terminated if you fail to comply with these terms.
THE WORK IS PROVIDED "AS IS." McGRAW-HILL EDUCATION AND ITS LICENSORS MAKE NO GUARANTEES OR WARRANTIES AS TO THE ACCURACY, ADEQUACY OR COMPLETENESS OF OR RESULTS TO BE OBTAINED FROM USING THE WORK, INCLUDING ANY INFORMATION THAT CAN BE ACCESSED THROUGH THE WORK VIA HYPERLINK OR OTHERWISE, AND EXPRESSLY DISCLAIM ANY WARRANTY, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIED WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. McGraw-Hill Education and its licensors do not warrant or guarantee that the functions contained in the work will meet your requirements or that its operation will be uninterrupted or error free. Neither McGraw-Hill Education nor its licensors shall be liable to you or anyone else for any inaccuracy, error or omission, regardless of cause, in the work or for any damages resulting therefrom. McGraw-Hill Education has no responsibility for the content of any information accessed through the work. Under no circumstances shall McGraw-Hill Education and/or its licensors be liable for any indirect, incidental, special, punitive, consequential or similar damages that result from the use of or inability to use the work, even if any of them has been advised of the possibility of such damages. This limitation of liability shall apply to any claim or cause whatsoever whether such claim or cause arises in contract, tort or otherwise.







Preface to The Second Edition
The purpose of this book, like its previous edition, is to provide the concepts and theory of signals and systems needed in almost all electrical engineering fields and in many other engineering and science disciplines as well.
In the previous edition the book focused strictly on deterministic signals and systems. This new edition expands the contents of the first edition by adding two chapters dealing with random signals and the response of linear systems to random inputs. The background material on probability needed for these two chapters is included in Appendix B.
I wish to express my appreciation to Ms. Kimberly Eaton and Mr. Charles Wall of the McGraw-Hill Schaum Series for inviting me to revise the book.
HWEI P. HSUShannondell at Valley Forge, Audubon, Pennsylvania







Preface to The First Edition
The concepts and theory of signals and systems are needed in almost all electrical engineering fields and in many other engineering and scientific disciplines as well. They form the foundation for further studies in areas such as communication, signal processing, and control systems.
This book is intended to be used as a supplement to all textbooks on signals and systems or for self-study. It may also be used as a textbook in its own right. Each topic is introduced in a chapter with numerous solved problems. The solved problems constitute an integral part of the text.
Chapter 1 introduces the mathematical description and representation of both continuous-time and discrete-time signals and systems. Chapter 2 develops the fundamental input-output relationship for linear time-invariant (LTI) systems and explains the unit impulse response of the system and convolution operation. Chapters 3 and 4 explore the transform techniques for the analysis of LTI systems. The Laplace transform and its application to continuous-time LTI systems are considered in Chapter 3. Chapter 4 deals with the z-transform and its application to discrete-time LTI systems. The Fourier analysis of signals and systems is treated in Chapters 5 and 6. Chapter 5 considers the Fourier analysis of continuous-time signals and systems, while Chapter 6 deals with discrete-time signals and systems. The final chapter, Chapter 7, presents the state space or state variable concept and analysis for both discrete-time and continuous-time systems. In addition, background material on matrix analysis needed for Chapter 7 is included in Appendix A.
I am grateful to Professor Gordon Silverman of Manhattan College for his assistance, comments, and careful review of the manuscript. I also wish to thank the staff of the McGraw-Hill Schaum Series, especially John Aliano for his helpful comments and suggestions and Maureen Walker for her great care in preparing this book. Last, I am indebted to my wife, Daisy, whose understanding and constant support were necessary factors in the completion of this work.
HWEI P. HSUMontville, New Jersey







To the Student
To understand the material in this text, the reader is assumed to have a basic knowledge of calculus, along with some knowledge of differential equations and the first circuit course in electrical engineering.
This text covers both continuous-time and discrete-time signals and systems. If the course you are taking covers only continuous-time signals and systems, you may study parts of Chapters 1 and 2 covering the continuous-time case, Chapters 3 and 5, and the second part of Chapter 7. If the course you are taking covers only discrete-time signals and systems, you may study parts of Chapters 1 and 2 covering the discrete-time case, Chapters 4 and 6, and the first part of Chapter 7.
To really master a subject, a continuous interplay between skills and knowledge must take place. By studying and reviewing many solved problems and seeing how each problem is approached and how it is solved, you can learn the skills of solving problems easily and increase your store of necessary knowledge. Then, to test and reinforce your learned skills, it is imperative that you work out the supplementary problems (hints and answers are provided). I would like to emphasize that there is no short cut to learning except by "doing."







Contents
CHAPTER 1     Signals and Systems
1.1 Introduction
1.2 Signals and Classification of Signals
1.3 Basic Continuous-Time Signals
1.4 Basic Discrete-Time Signals
1.5 Systems and Classification of Systems
Solved Problems
CHAPTER 2     Linear Time-Invariant Systems
2.1 Introduction
2.2 Response of a Continuous-Time LTI System and the Convolution Integral
2.3 Properties of Continuous-Time LTI Systems
2.4 Eigenfunctions of Continuous-Time LTI Systems
2.5 Systems Described by Differential Equations
2.6 Response of a Discrete-Time LTI System and Convolution Sum
2.7 Properties of Discrete-Time LTI Systems
2.8 Eigenfunctions of Discrete-Time LTI Systems
2.9 Systems Described by Difference Equations
Solved Problems
CHAPTER 3     Laplace Transform and Continuous-Time LTI Systems
3.1 Introduction
3.2 The Laplace Transform
3.3 Laplace Transforms of Some Common Signals
3.4 Properties of the Laplace Transform
3.5 The Inverse Laplace Transform
3.6 The System Function
3.7 The Unilateral Laplace Transform
Solved Problems
CHAPTER 4     The z-Transform and Discrete-Time LTI Systems
4.1 Introduction
4.2 The z-Transform
4.3 z-Transforms of Some Common Sequences
4.4 Properties of the z-Transform
4.5 The Inverse z-Transform
4.6 The System Function of Discrete-Time LTI Systems
4.7 The Unilateral z-Transform
Solved Problems
CHAPTER 5     Fourier Analysis of Continuous-Time Signals and Systems
5.1 Introduction
5.2 Fourier Series Representation of Periodic Signals
5.3 The Fourier Transform
5.4 Properties of the Continuous-Time Fourier Transform
5.5 The Frequency Response of Continuous-Time LTI Systems
5.6 Filtering
5.7 Bandwidth
Solved Problems
CHAPTER 6     Fourier Analysis of Discrete-Time Signals and Systems
6.1 Introduction
6.2 Discrete Fourier Series
6.3 The Fourier Transform
6.4 Properties of the Fourier Transform
6.5 The Frequency Response of Discrete-Time LTI Systems
6.6 System Response to Sampled Continuous-Time Sinusoids
6.7 Simulation
6.8 The Discrete Fourier Transform
Solved Problems
CHAPTER 7     State Space Analysis
7.1 Introduction
7.2 The Concept of State
7.3 State Space Representation of Discrete-Time LTI Systems
7.4 State Space Representation of Continuous-Time LTI Systems
7.5 Solutions of State Equations for Discrete-Time LTI Systems
7.6 Solutions of State Equations for Continuous-Time LTI Systems
Solved Problems
CHAPTER 8     Random Signals
8.1 Introduction
8.2 Random Processes
8.3 Statistics of Random Processes
8.4 Gaussian Random Process
Solved Problems
CHAPTER 9     Power Spectral Densities and Random Signals in Linear System
9.1 Introduction
9.2 Correlations and Power Spectral Densities
9.3 White Noise
9.4 Response of Linear System to Random Input
Solved Problems
APPENDIX A   Review of Matrix Theory
A.1 Matrix Notation and Operations
A.2 Transpose and Inverse
A.3 Linear Independence and Rank
A.4 Determinants
A.5 Eigenvalues and Eigenvectors
A.6 Diagonalization and Similarity Transformation
A.7 Functions of a Matrix
A.8 Differentiation and Integration of Matrices
APPENDIX B   Review of Probability
B.1 Probability
B.2 Random Variables
B.3 Two-Dimensional Random Variables
B.4 Functions of Random Variables
B.5 Statistical Averages
APPENDIX C   Properties of Linear Time-Invariant Systems and Various Transforms
C.1 Continuous-Time LTI Systems
C.2 The Laplace Transform
C.3 The Fourier Transform
C.4 Discrete-Time LTI Systems
C.5 The z-Transform
C.6 The Discrete-Time Fourier Transform
C.7 The Discrete Fourier Transform
C.8 Fourier Series
C.9 Discrete Fourier Series
APPENDIX D   Review of Complex Numbers
D.1 Representation of Complex Numbers
D.2 Addition, Multiplication, and Division
D.3 The Complex Conjugate
D.4 Powers and Roots of Complex Numbers
APPENDIX E   Useful Mathematical Formulas
E.1 Summation Formulas
E.2 Euler's Formulas
E.3 Trigonometric Identities
E.4 Power Series Expansions
E.5 Exponential and Logarithmic Functions
E.6 Some Definite Integrals
INDEX







CHAPTER 1Signals and Systems
1.1 Introduction
The concept and theory of signals and systems are needed in almost all electrical engineering fields and in many other engineering and scientific disciplines as well. In this chapter we introduce the mathematical description and representation of signals and systems and their classifications. We also define several important basic signals essential to our studies.
1.2 Signals and Classification of Signals
A signal is a function representing a physical quantity or variable, and typically it contains information about the behavior or nature of the phenomenon. For instance, in an RC circuit the signal may represent the voltage across the capacitor or the current flowing in the resistor. Mathematically, a signal is represented as a function of an independent variable t. Usually t represents time. Thus, a signal is denoted by x(t).
A. Continuous-Time and Discrete-Time Signals:
A signal x(t) is a continuous-time signal if t is a continuous variable. If t is a discrete variable—that is, x(t) is defined at discrete times—then x(t) is a discrete-time signal. Since a discrete-time signal is defined at discrete times, a discrete-time signal is often identified as a sequence of numbers, denoted by {xn} or x[n], where n = integer. Illustrations of a continuous-time signal x(t) and of a discrete-time signal x[n] are shown in Fig. 1-1.


Fig. 1-1 Graphical representation of (a) continuous-time and (b) discrete-time signals.

A discrete-time signal x[n] may represent a phenomenon for which the independent variable is inherently discrete. For instance, the daily closing stock market average is by its nature a signal that evolves at discrete points in time (that is, at the close of each day). On the other hand a discrete-time signal x[n] may be obtained by sampling a continuous-time signal x(t) such as
x(t0), x(t1), ..., x(tn), ...
or in a shorter form as

where we understand that
xn = x[n] = x(tn)
and xn's are called samples and the time interval between them is called the sampling interval. When the sampling intervals are equal (uniform sampling), then
xn = x[n] = x(nTs)
where the constant Ts is the sampling interval.
A discrete-time signal x[n] can be defined in two ways:
1. We can specify a rule for calculating the nth value of the sequence. For example,

2. We can also explicitly list the values of the sequence. For example, the sequence shown in Fig. 1-1(b) can be written as

We use the arrow to denote the n = 0 term. We shall use the convention that if no arrow is indicated, then the first term corresponds to n = 0 and all the values of the sequence are zero for n < 0.
The sum and product of two sequences are defined as follows:

B. Analog and Digital Signals:
If a continuous-time signal x(t) can take on any value in the continuous interval (a, b), where a may be -∞ and b may be +∞, then the continuous-time signal x(t) is called an analog signal. If a discrete-time signal x[n] can take on only a finite number of distinct values, then we call this signal a digital signal.
C. Real and Complex Signals:
A signal x(t) is a real signal if its value is a real number, and a signal x(t) is a complex signal if its value is a complex number. A general complex signal x(t) is a function of the form

where x1(t) and x2(t) are real signals and j = 
Note that in Eq. (1.1) t represents either a continuous or a discrete variable.
D. Deterministic and Random Signals:
Deterministic signals are those signals whose values are completely specified for any given time. Thus, a deterministic signal can be modeled by a known function of time t. Random signals are those signals that take random values at any given time and must be characterized statistically. Random signals will be discussed in Chaps. 8 and 9.
E. Even and Odd Signals:
A signal x(t) or x[n] is referred to as an even signal if

A signal x(t) or x[n] is referred to as an odd signal if

Examples of even and odd signals are shown in Fig. 1-2.


Fig. 1-2 Examples of even signals (a and b) and odd signals (c and d).

Any signal x(t) or x[n] can be expressed as a sum of two signals, one of which is even and one of which is odd. That is,


Note that the product of two even signals or of two odd signals is an even signal and that the product of an even signal and an odd signal is an odd signal (Prob. 1.7).
F. Periodic and Nonperiodic Signals:
A continuous-time signal x(t) is said to be periodic with period T if there is a positive nonzero value of T for which

An example of such a signal is given in Fig. 1-3(a). From Eq. (1.7) or Fig. 1-3(a) it follows that

for all t and any integer m. The fundamental period T0 of x(t) is the smallest positive value of T for which Eq. (1.7) holds. Note that this definition does not work for a constant signal x(t) (known as a dc signal). For a constant signal x(t) the fundamental period is undefined since x(t) is periodic for any choice of T (and so there is no smallest positive value). Any continuous-time signal which is not periodic is called a nonperiodic (or aperiodic) signal.


Fig. 1-3 Examples of periodic signals.

Periodic discrete-time signals are defined analogously. A sequence (discrete-time signal) x[n] is periodic with period N if there is a positive integer N for which

An example of such a sequence is given in Fig. 1-3(b). From Eq. (1.9) and Fig. 1-3(b) it follows that

for all n and any integer m. The fundamental period N0 of x[n] is the smallest positive integer N for which Eq. (1.9) holds. Any sequence which is not periodic is called a nonperiodic (or aperiodic) sequence.
Note that a sequence obtained by uniform sampling of a periodic continuous-time signal may not be periodic (Probs. 1.12 and Probs. 1.13). Note also that the sum of two continuous-time periodic signals may not be periodic but that the sum of two periodic sequences is always periodic (Probs. 1.14 and Probs. 1.15).
G. Energy and Power Signals:
Consider v(t) to be the voltage across a resistor R producing a current i(t). The instantaneous power p(t) per ohm is defined as

Total energy E and average power P on a per-ohm basis are


For an arbitrary continuous-time signal x(t), the normalized energy content E of x(t) is defined as

The normalized average power P of x(t) is defined as

Similarly, for a discrete-time signal x[n], the normalized energy content E of x[n] is defined as

The normalized average power P of x[n] is defined as

Based on definitions (1.14) to (1.17), the following classes of signals are defined:
1. x(t) (or x[n] is said to be an energy signal (or sequence) if and only if 0 < E < ∞, and so P = 0.
2. x(t) (or x[n]) is said to be a power signal (or sequence) if and only if 0 < P < ∞, thus implying that E = ∞.
3. Signals that satisfy neither property are referred to as neither energy signals nor power signals.
Note that a periodic signal is a power signal if its energy content per period is finite, and then the average power of this signal need only be calculated over a period (Prob. 1.18).
1.3 Basic Continuous-Time Signals
A. The Unit Step Function:
The unit step function u(t), also known as the Heaviside unit function, is defined as

which is shown in Fig. 1-4(a). Note that it is discontinuous at t = 0 and that the value at t = 0 is undefined. Similarly, the shifted unit step function u(t - t0) is defined as

which is shown in Fig. 1-4(b)


Fig. 1-4 (a) Unit step function; (b) shifted unit step function.

B. The Unit Impulse Function:
The unit impulse function δ(t), also known as the Dirac delta function, plays a central role in system analysis. Traditionally, δ(t) is often defined as the limit of a suitably chosen conventional function having unity area over an infinitesimal time interval as shown in Fig. 1-5 and possesses the following properties:



Fig. 1-5

But an ordinary function which is everywhere 0 except at a single point must have the integral 0 (in the Riemann integral sense). Thus, δ(t) cannot be an ordinary function and mathematically it is defined by

where ϕ(t) is any regular function continuous at t = 0.
An alternative definition of δ(t) is given by

Note that Eq. (1.20) or (1.21) is a symbolic expression and should not be considered an ordinary Riemann integral. In this sense, δ(t) is often called a generalized function and ϕ(t) is known as a testing function. A different class of testing functions will define a different generalized function (Prob. 1.24). Similarly, the delayed delta function δ(t - t0) is defined by

where ϕ(t) is any regular function continuous at t = t0. For convenience, δ(t) and δ(t - t0) are depicted graphically as shown in Fig. 1-6.


Fig. 1-6 (a) Unit impulse function; (b) shifted unit impulse function.

Some additional properties of δ(t) are

if x(t) is continuous at t = 0.

if x(t) is continuous at t = t0.
Using Eqs. (1.22) and (1.24), any continuous-time signal x(t) can be expressed as

Generalized Derivatives:
If g(t) is a generalized function, its nth generalized derivative g(n)(t) = dng(t)/dtn is defined by the following relation:

where ϕ(t) is a testing function which can be differentiated an arbitrary number of times and vanishes outside some fixed interval and ϕ(n)(t) is the nth derivative of ϕ(t). Thus, by Eqs. (1.28) and (1.20) the derivative of δ(t) can be defined as

where ϕ(t) is a testing function which is continuous at t = 0 and vanishes outside some fixed interval and ϕ' (0) = dϕ(t)/dt|t = 0. Using Eq. (1.28), the derivative of u(t) can be shown to be δ(t) (Prob. 1.28); that is,

Then the unit step function u(t) can be expressed as

Note that the unit step function u(t) is discontinuous at t = 0; therefore, the derivative of u(t) as shown in Eq. (1.30) is not the derivative of a function in the ordinary sense and should be considered a generalized derivative in the sense of a generalized function. From Eq. (1.31) we see that u(t) is undefined at t = 0 and

by Eq. (1.21) with ϕ(t) = 1. This result is consistent with the definition (1.18) of u(t).
Note that the properties (or identities) expressed by Eqs. (1.23) to (1.26) and Eq. (1.30) can not be verified by using the conventional approach of δ(t) as shown in Fig. 1-5.
C. Complex Exponential Signals:
The Complex exponential signal

is an important example of a complex signal. Using Euler's formula, this signal can be defined as

Thus, x(t) is a complex signal whose real part is cos ω0t and imaginary part is sin ω0t. An important property of the complex exponential signal x(t) in Eq. (1.32) is that it is periodic. The fundamental period T0 of x(t) is given by (Prob. 1.9)

Note that x(t) is periodic for any value of ω0.
General Complex Exponential Signals:
Let s = σ + jω be a complex number. We define x(t) as

Then signal x(t) in Eq. (1.35) is known as a general complex exponential signal whose real part eσt cos ωt and imaginary part eσt sin ωt are exponentially increasing (σ > 0) or decreasing (σ < 0) sinusoidal signals (Fig. 1-7).


Fig. 1-7 (a) Exponentially increasing sinusoidal signal; (b) exponentially decreasing sinusoidal signal.

Real Exponential Signals:
Note that if s = σ (a real number), then Eq. (1.35) reduces to a real exponential signal

As illustrated in Fig. 1-8, if σ > 0, then x(t) is a growing exponential; and if σ < 0, then x(t) is a decaying exponential.
D. Sinusoidal Signals:
A continuous-time sinusoidal signal can be expressed as

where A is the amplitude (real), ω0 is the radian frequency in radians per second, and θ is the phase angle in radians. The sinusoidal signal x(t) is shown in Fig. 1-9, and it is periodic with fundamental period

The reciprocal of the fundamental period T0 is called the fundamental frequency f0:



Fig. 1-8 Continuous-time real exponential signals. (a) σ > 0; (b) σ < 0.



Fig. 1-9 Continuous-time sinusoidal signal.

From Eqs. (1.38) and (1.39) we have

which is called the fundamental angular frequency. Using Euler's formula, the sinusoidal signal in Eq. (1.37) can be expressed as

where "Re" denotes "real part of." We also use the notation "Im" to denote "imaginary part of." Then

1.4 Basic Discrete-Time Signals
A. The Unit Step Sequence:
The unit step sequence u[n] is defined as

which is shown in Fig. 1-10(a). Note that the value of u[n] at n = 0 is defined [unlike the continuous-time step function u(t) at t = 0] and equals unity. Similarly, the shifted unit step sequence u[n - k] is defined as

which is shown in Fig. 1-10(b).


Fig. 1-10 (a) Unit step sequence; (b) shifted unit step sequence.

B. The Unit Impulse Sequence:
The unit impulse (or unit sample) sequence δ[n] is defined as

which is shown in Fig. 1-11(a). Similarly, the shifted unit impulse (or sample) sequence δ[n - k] is defined as

which is shown in Fig. 1-11(b).


Fig. 1-11 (a) Unit impulse (sample) sequence; (b) shifted unit impulse sequence.

Unlike the continuous-time unit impulse function δ(t),δ[n] is defined without mathematical complication or difficulty. From definitions (1.45) and (1.46) it is readily seen that

which are the discrete-time counterparts of Eqs. (1.25) and (1.26), respectively. From definitions (143) to (146), δ[n] and u[n] are related by

which are the discrete-time counterparts of Eqs. (1.30) and (1.31), respectively.
Using definition (1.46), any sequence x[n] can be expressed as

which corresponds to Eq. (1.27) in the continuous-time signal case.
C. Complex Exponential Sequences:
The complex exponential sequence is of the form

Again, using Euler's formula, x[n] can be expressed as

Thus, x[n] is a complex sequence whose real part is cos Ω0n and imaginary part is sin Ω0n.
Periodicity of :
In order for  to be periodic with period N (> 0), Ω0 must satisfy the following condition (Prob. 1.11):

Thus, the sequence  is not periodic for any value of Ω0. It is periodic only if Ω0/2π is a rational number. Note that this property is quite different from the property that the continuous-time signal  is periodic for any value of ω0. Thus, if Ω0 satisfies the periodicity condition in Eq. (1.54), Ω0 ≠ 0, and N and m have no factors in common, then the fundamental period of the sequence x[n] in Eq. (1.52) is N0 given by

Another very important distinction between the discrete-time and continuous-time complex exponentials is that the signals  are all distinct for distinct values of ω0 but that this is not the case for the signals .
Consider the complex exponential sequence with frequency (Ω0 + 2πk), where k is an integer:

since ej2πkn = 1. From Eq. (1.56) we see that the complex exponential sequence at frequency Ω0 is the same as that at frequencies (Ω0 ± 2π), (Ω0 ± 4π), and so on. Therefore, in dealing with discrete-time exponentials, we need only consider an interval of length 2π in which to choose Ω0. Usually, we will use the interval 0 ≤ Ω0 < 2π or the interval - π ≤ Ω0 < π.
General Complex Exponential Sequences:
The most general complex exponential sequence is often defined as

where C and α are, in general, complex numbers. Note that Eq. (1.52) is the special case of Eq. (1.57) with C = 1 and .
Real Exponential Sequences:
If C and α in Eq. (1.57) are both real, then x[n] is a real exponential sequence. Four distinct cases can be identified: α > 1, 0 < α < 1, -1 < α < 0, and α < -1. These four real exponential sequences are shown in Fig. 1-12. Note that if α = 1, x[n] is a constant sequence, whereas if α = -1, x[n] alternates in value between + C and -C.


Fig. 1-12 Real exponential sequences. (a) α > 1; (b) 1 > α > 0; (c) 0 > α > -1; (d) α < -1.

D. Sinusoidal Sequences:
A sinusoidal sequence can be expressed as

If n is dimensionless, then both Ω0 and θ have units of radians. Two examples of sinusoidal sequences are shown in Fig. 1-13. As before, the sinusoidal sequence in Eq. (1.58) can be expressed as

As we observed in the case of the complex exponential sequence in Eq. (1.52), the same observations [Eqs. (1.54) and (1.56)] also hold for sinusoidal sequences. For instance, the sequence in Fig. 1-13(a) is periodic with fundamental period 12, but the sequence in Fig. 1-13(b) is not periodic.


Fig. 1-13 Sinusoidal sequences. (a) x[n] = cos(πn/6); (b) x[n] = cos(n/2).

1.5 Systems and Classification of Systems
A. System Representation:
A system is a mathematical model of a physical process that relates the input (or excitation) signal to the output (or response) signal.
Let x and y be the input and output signals, respectively, of a system. Then the system is viewed as a transformation (or mapping) of x into y. This transformation is represented by the mathematical notation

where T is the operator representing some well-defined rule by which x is transformed into y. Relationship (1.60) is depicted as shown in Fig. 1-14(a). Multiple input and/or output signals are possible, as shown in Fig. 1-14(b). We will restrict our attention for the most part in this text to the single-input, single-output case.


Fig. 1-14 System with single or multiple input and output signals.

B. Deterministic and Stochastic Systems:
If the input and output signals x and y are deterministic signals, then the system is called a deterministic system. If the input and output signals x and y are random signals, then the system is called a stochastic system.
C. Continuous-Time and Discrete-Time Systems:
If the input and output signals x and y are continuous-time signals, then the system is called a continuous-time system [Fig. 1-15(a)]. If the input and output signals are discrete-time signals or sequences, then the system is called a discrete-time system [Fig. 1-15(b)].


Fig. 1-15 (a) Continuous-time system; (b) discrete-time system.

Note that in a continuous-time system the input x(t) and output y(t) are often expressed by a differential equation (see Prob. 1.32) and in a discrete-time system the input x[n] and output y[n] are often expressed by a difference equation (see Prob. 1.37).
D. Systems with Memory and without Memory
A system is said to be memoryless if the output at any time depends on only the input at that same time. Otherwise, the system is said to have memory. An example of a memoryless system is a resistor R with the input x(t) taken as the current and the voltage taken as the output y(t). The input-output relationship (Ohm's law) of a resistor is

An example of a system with memory is a capacitor C with the current as the input x(t) and the voltage as the output y(t); then

A second example of a system with memory is a discrete-time system whose input and output sequences are related by

E. Causal and Noncausal Systems:
A system is called causal if its output at the present time depends on only the present and/or past values of the input. Thus, in a causal system, it is not possible to obtain an output before an input is applied to the system. A system is called noncausal (or anticipative) if its output at the present time depends on future values of the input. Example of noncausal systems are

Note that all memoryless systems are causal, but not vice versa.
F. Linear Systems and Nonlinear Systems:
If the operator T in Eq. (1.60) satisfies the following two conditions, then T is called a linear operator and the system represented by a linear operator T is called a linear system:
1. Additivity:
Given that Tx1 = y1 and Tx2 = y2, then

for any signals x1 and x2.
2. Homogeneity (or Scaling):

for any signals x and any scalar α.
Any system that does not satisfy Eq. (1.66) and/or Eq. (1.67) is classified as a nonlinear system. Eqs. (1.66) and (1.67) can be combined into a single condition as

where α1 and α2 are arbitrary scalars. Eq. (1.68) is known as the superposition property. Examples of linear systems are the resistor [Eq. (1.61)] and the capacitor [Eq. (1.62)]. Examples of nonlinear systems are

Note that a consequence of the homogeneity (or scaling) property [Eq. (1.67)] of linear systems is that a zero input yields a zero output. This follows readily by setting α = 0 in Eq. (1.67). This is another important property of linear systems.
G. Time-Invariant and Time-Varying Systems:
A system is called time-invariant if a time shift (delay or advance) in the input signal causes the same time shift in the output signal. Thus, for a continuous-time system, the system is time-invariant if

for any real value of τ. For a discrete-time system, the system is time-invariant (or shift-invariant) if

for any integer k. A system which does not satisfy Eq. (1.71) (continuous-time system) or Eq. (1.72) (discrete-time system) is called a time-varying system. To check a system for time-invariance, we can compare the shifted output with the output produced by the shifted input (Probs. 1.33 to Probs. 1.39).
H. Linear Time-Invariant Systems:
If the system is linear and also time-invariant, then it is called a linear time-invariant (LTI) system.
I. Stable Systems:
A system is bounded-input/bounded-output (BIBO) stable if for any bounded input x defined by

the corresponding output y is also bounded defined by

where k1 and k2 are finite real constants. An unstable system is one in which not all bounded inputs lead to bounded output. For example, consider the system where output y[n] is given by y[n] = (n + 1)u[n], and input x[n] = u[n] is the unit step sequence. In this case the input u[n] = 1, but the output y[n] increases without bound as n increases.
J. Feedback Systems:
A special class of systems of great importance consists of systems having feedback. In a feedback system, the output signal is fed back and added to the input to the system as shown in Fig. 1-16.


Fig. 1-16 Feedback system.

SOLVED PROBLEMS
Signals and Classification of Signals
1.1. A continuous-time signal x(t) is shown in Fig. 1-17. Sketch and label each of the following signals.
(a) x(t - 2); (b) x(2t); (c) x(t/2); (d) x (-t)


Fig. 1-17

(a) x(t - 2) is sketched in Fig. 1-18(a).
(b) x(2t) is sketched in Fig. 1-18(b).
(c) x(t/2) is sketched in Fig. 1-18(c).
(d) x (-t) is sketched in Fig. 1-18(d).


Fig. 1-18

1.2. A discrete-time signal x[n] is shown in Fig. 1-19. Sketch and label each of the following signals.
(a) x[n - 2]; (b) x[2n]; (c) x[-n]; (d) x[-n + 2]


Fig. 1-19

(a) x[n - 2] is sketched in Fig. 1-20(a).
(b) x[2n] is sketched in Fig. 1-20(b).
(c) x[-n] is sketched in Fig. 1-20(c).
(d) x[-n + 2] is sketched in Fig. 1-20(d).



Fig. 1-20

1.3. Given the continuous-time signal specified by

determine the resultant discrete-time sequence obtained by uniform sampling of x(t) with a sampling interval of (a) 0.25 s, (b) 0.5 s, and (c) 1.0 s.
It is easier to take the graphical approach for this problem. The signal x(t) is plotted in Fig. 1-21(a). Figs. 1-21(b) to (d) give plots of the resultant sampled sequences obtained for the three specified sampling intervals.
(a) Ts = 0.25 s. From Fig. 1-21(b) we obtain

(b) Ts = 0.5 s. From Fig. 1-21(c) we obtain

(c) T = 1 s. From Fig. 1-21(d) we obtain



Fig. 1-21

1.4. Using the discrete-time signals x1[n] and x2[n] shown in Fig. 1-22, represent each of the following signals by a graph and by a sequence of numbers.
(a) y1[n] = x1[n] + x2[n]; (b) y2[n] = 2x1[n]; (c) y3[n] = x1[n]x2[n]


Fig. 1-22

(a) y1[n] is sketched in Fig. 1-23(a). From Fig. 1-23(a) we obtain

(b) y2[n] is sketched in Fig. 1-23(b). From Fig. 1-23(b) we obtain

(c) y2[n] is sketched in Fig. 1-23(c). From Fig. 1-23(c) we obtain



Fig. 1-23

1.5. Sketch and label the even and odd components of the signals shown in Fig. 1-24.
Using Eqs. (1.5) and (1.6), the even and odd components of the signals shown in Fig. 1-24 are sketched in Fig. 1-25.


Fig. 1-24




Fig. 1-25

1.6. Find the even and odd components of x(τ) = ejt.
Let xe(t) and x0(t) be the even and odd components of ejt, respectively.
ejt = xe(τ) + x0(t)
From Eqs. (1.5) and (1.6) and using Euler's formula, we obtain

1.7. Show that the product of two even signals or of two odd signals is an even signal and that the product of an even and an odd signal is an odd signal.
Let x(t) = x1(t)x2(t). If x1(t) and x2(t) are both even, then
x (-t) = x1(-t)x2(-t) = x1(t)x2(t) = x(t)
and x(t) is even. If x1(t) and x2(t) are both odd, then
x(-t) = x1(-t)x2(-t) = -x1(t) [-x2(t)] = x1(t)x2(t) = x(t)
and x(t) is even. If x1(t) is even and x2(t) is odd, then
x (-t) = x1(-t)x2(-t) = x1(t)[- x2(t)] = - x1(t)x2(t) = -x(t)
and x(t) is odd. Note that in the above proof, variable t represents either a continuous or a discrete variable.
1.8. Show that
(a) If x(t) and x[n] are even, then

(b) If x(t) and x[n] are odd, then

(a) We can write

Letting t = - λ in the first integral on the right-hand side, we get

Since x(t) is even, that is, x (-λ) = x(λ), we have

Hence,

Similarly,

Letting n = -m in the first term on the right-hand side, we get

Since x[n] is even, that is, x[-m] = x[m], we have

Hence,

(b) Since x(t) and x[n] are odd, that is, x(-t) = -x(t) and x[-n] = -x[n], we have
x (-0) = -x (0) and x[-0] = -x[0]
Hence,

Similarly,

and

in view of Eq. (1.76).
1.9. Show that the complex exponential signal

is periodic and that its fundamental period is 2π/ω0.
By Eq. (1.7), x(t) will be periodic if

Since

We must have

If ω0 = 0, then x(t) = 1, which is periodic for any value of T. If ω0 ≠ 0, Eq. (1.78) holds if

Thus, the fundamental period T0, the smallest positive T, of x(t) is given by 2π/ω0.
1.10. Show that the sinusoidal signal
x(t) = cos(ω0t + θ
is periodic and that its fundamental period is 2π/ω0.
The sinusoidal signal x(t) will be periodic if
cos[ω0(t + T) + θ] = cos(ω0t + θ)
We note that
cos[ω0(t + T) + θ] = cos[ω0t + θ + ω0T] = cos(ω0t + θ)
if

Thus, the fundamental period T0 of x(t) is given by 2π/ω0.
1.11. Show that the complex exponential sequence

is periodic only if Ω0/2π is a rational number.
By Eq. (1.9), x[n] will be periodic if

or

Equation (1.79) holds only if
Ω0N = m2π       m = positive integer
or

Thus, x[n] is periodic only if Ω0/2π is a rational number.
1.12. Let x(t) be the complex exponential signal

with radian frequency ω0 and fundamental period T0 = 2π/ω0. Consider the discrete-time sequence x[n] obtained by uniform sampling of x(t) with sampling interval Ts. That is,

Find the condition on the value of Ts so that x[n] is periodic.
If x[n] is periodic with fundamental period N0, then

Thus, we must have

or

Thus, x[n] is periodic if the ratio Ts/T0 of the sampling interval and the fundamental period of x(t) is a rational number.
Note that the above condition is also true for sinusoidal signals x(t) = cos(ω0t + θ).
1.13. Consider the sinusoidal signal
x(t) = cos 15t
(a) Find the value of sampling interval Ts such that x[n] = x(nTs) is a periodic sequence.
(b) Find the fundamental period of x[n] = x(nTs) if T = 0.1π seconds.
(a) The fundamental period of x(t) is T0 = 2π/ω = 2π/15. By Eq. (1.81), x[n] = x(nTs) is periodic if

where m and N0 are positive integers. Thus, the required value of Ts is given by

(b) Substituting Ts = 0.1π = π/10 in Eq. (1.82), we have

Thus, x[n] = x(nTs) is periodic. By Eq. (1.82)

The smallest positive integer N0 is obtained with m = 3. Thus, the fundamental period of x[n] = x(0.1πn) is N0 = 4.
1.14. Let x1(t) and x2(t) be periodic signals with fundamental periods T1 and T2, respectively. Under what conditions is the sum x(t) = x1(t) + x2(t) periodic, and what is the fundamental period of x(t) if it is periodic?
Since x1(t) and x2(t) are periodic with fundamental periods T1 and T2, respectively, we have
x1(t) = x1(t + T1) = x1(t + mT1)        m = positive integer
x2(t) = x2(t + T2) = x2(t + kT2)        k = positive integer
Thus,
x(t) = x1(t + mT1) + x2(t + kT2)
In order for x(t) to be periodic with period T, one needs
x(t + T) = x1(t + T) + x2(t + T) = x1(t + mT1) + x2(t + kT2)
Thus, we must have

or

In other words, the sum of two periodic signals is periodic only if the ratio of their respective periods can be expressed as a rational number. Then the fundamental period is the least common multiple of T1 and T2, and it is given by Eq. (1.84) if the integers m and k are relative prime. If the ratio T1/T2 is an irrational number, then the signals x1(t) and x2(t) do not have a common period and x(t) cannot be periodic.
1.15. Let x1[n] and x2[n] be periodic sequences with fundamental periods N1 and N2, respectively. Under what conditions is the sum x[n] = x1[n] + x2[n] periodic, and what is the fundamental period of x[n] if it is periodic?
Since x1[n] and x2[n] are periodic with fundamental periods N1 and N2, respectively, we have
x1[n] = x1[n + N1] = x1[n + mN1]        m = positive integer
x2[n] = x2[n + N2] = x2[n + kN2]        k = positive integer
Thus,
x[n] = x1[n + mN1] + x2[n + kN2]
In order for x[n] to be periodic with period N, one needs
x[n + N] = x1[n + N] + x2[n + N] = x1[n + mN1] + x2[n + kN2]
Thus, we must have

Since we can always find integers m and k to satisfy Eq. (1.86), it follows that the sum of two periodic sequences is also periodic and its fundamental period is the least common multiple of N1 and N2.
1.16. Determine whether or not each of the following signals is periodic. If a signal is periodic, determine its fundamental period.
(a) 
(b) 
(c) 
(d) x(t) = cos t + sin  t
(e) x(t) = sin2t
(f) x(t) = ej[(π/2)t -1]
(g) x[n] = ej(π/4)n
(h) 
(i) 
(j) 
(a) 
x(t) is periodeic with fundamental period T0 = 2π/ω0 = 2π.
(b) 
x(t) is periodeic with fundamental period T0 = 2π/ω0 = 3.
(c) 
where x1(t) = cos(π/3)t = cos ω1t is periodic with T1 = 2π/ω1 = 6 and x2(t) = sin(π/4)t = sin ω2t is periodeic with T2 = 2π/ω2 = 8. Since  is a rational number, x(t) is periodeic with fundamental period T0 = 4T1 = 3T2 = 24.
(d) x(t) = cos t + sin  t = x1(t) + x2(t)
where x1(t) = cos t = cos ω1t is periodeic with T1 = 2π/ω1 = 2π and x2(t) = sin  t = sin ω2t is periodeic with T2 = 2π/ω2 =  π. Since T1/T2 =  is an irrational number, x(t) is nonperiodic.
(e) Using the trigonometric identity sin2 θ = (1 - cos 2θ), we can write

where  is a dc signal with an arbitrary period and  is periodic with T2 = 2π/ω2 = π. Thus, x(t) is periodic with fundamental period T0 = π.
(f) 
x(t) is periodic with fundamental period T0 = 2π/ω0 = 4.
(g) 
Since  is a rational number, x[n] is periodic, and by Eq. (1.55) the fundamental period is N0 = 8.
(h) 
Since  is not a rational number, x[n] is nonperiodic.
(i) 
where

Since  (= rational number), x1[n] is periodic with fundamental period N1 = 6, and since  (= rational number), x2[n] is periodic with fundamental period N2 = 8. Thus, from the result of Prob. 1.15, x[n] is periodic and its fundamental period is given by the least common multiple of 6 and 8, that is, N0 = 24.
(j) Using the trigonometric identity , we can write

where  is periodic with fundamental period N1 = 1 and  Ω2n → Ω2 = π/4. Since  (= rational number), x2[n] is periodic with fundamental period N2 = 8. Thus, x[n] is periodic with fundamental period N0 = 8 (the least common multiple of N1 and N2).
1.17. Show that if x(t + T) = x(t), then

for any real α, β, and a.
If x(t + T) = x(t), then letting t = τ - T, we have
x(τ - T + T) = x(τ) = x(τ - T)
and

Next, the right-hand side of Eq. (1.88) can be written as

By Eq. (1.87) we have

Thus,

1.18. Show that if x(t) is periodic with fundamental period T0, then the normalized average power P of x(t) defined by Eq. (1.15) is the same as the average power of x(t) over any interval of length T0, that is,

By Eq. (1.15)

Allowing the limit to be taken in a manner such that T is an integral multiple of the fundamental period, T = kT0, the total normalized energy content of x(t) over an interval of length T is k times the normalized energy content over one period. Then

1.19. The following equalities are used on many occasions in this text. Prove their validity.
(a) 
(b) 
(c) 
(d) 
(a) Let

Then

Subtracting Eq. (1.95) from Eq. (1.94), we obtain
(1 - α) S = 1 - αN
Hence if α ≠ 1, we have

If α = 1, then by Eq. (1.94)

(b) For  Then by Eq. (1.96) we obtain

(c) Using Eq. (1.91), we obtain

(d) Taking the derivative of both sides of Eq. (1.91) with respect to α, we have

and

Hence,

1.20. Determine whether the following signals are energy signals, power signals, or neither.
(a) x(t) = e-atu(t),     a > 0
(b) x(t) = A cos(ω0t + θ)
(c) x(t) = tu(t)
(d) x[n] = (-0.5)nu[n]
(e) x[n] = u[n]
(f) x[n] = 2ej3n
(a) 
Thus, x(t) is an energy signal.
(b) The sinusoidal signal x(t) is periodic with T0 = 2π/ω0. Then by the result from Prob. 1.18, the average power of x(t) is

Thus, x(t) is a power signal. Note that periodic signals are, in general, power signals.
(c) 

Thus, x(t) is neither an energy signal nor a power signal.
(d) By definition (1.16) and using Eq. (1.91), we obtain

Thus, x[n] is an energy signal.
(e) By definition (1.17)

Thus, x[n] is an power signal.
(f) Since |x[n]| = |2ej3n| = 2|ej3n| = 2,

Thus, x[n] is an power signal.
Basic Signals
1.21. Show that

Let τ = - t. Then by definition (1.18)

Since τ > 0 and τ < 0 imply, respectively, that t < 0 and t > 0, we obtain

which is shown in Fig. 1-26.


Fig. 1-26

1.22. A continuous-time signal x(t) is shown in Fig. 1-27. Sketch and label each of the following signals.
(a) 


Fig. 1-27

(a) By definition (1.19)

and x(t)u (1 - t) is sketched in Fig. 1-28(a).
(b) By definitions (1.18) and (1.19)

and x(t)[u(t) - u(t - 1)] is sketched in Fig. 1-28(b).
(c) By Eq. (1.26)

which is sketched in Fig. 1-28(c).


Fig. 1-28

1.23. A discrete-time signal x[n] is shown in Fig. 1-29. Sketch and label each of the following signals.
(a) x[n]u[l - n]; (b) x[n]{u[n + 2] - u[n]}; (c) x[n]δ[n - 1]


Fig. 1-29

(a) By definition (1.44)

and x[n]u[l - n] is sketched in Fig. 1-30(a).
(b) By definitions (1.43) and (1.44)

and x[n]{u[n + 2] - u[n]} is sketched in Fig. l-30(b).
(c) By definition (1.48)

which is sketched in Fig. 1-30(c).


Fig. 1-30

1.24. The unit step function u(t) can be defined as a generalized function by the following relation:

where ϕ(t) is a testing function which is integrable over 0 < t < ∞. Using this definition, show that

Rewriting Eq. (1.98) as

we obtain

This can be true only if

These conditions imply that

Since ϕ(t) is arbitrary, we have
u(t) = 0, t < 0 and 1 - u(t) = 0, t > 0
that is,

1.25. Verify Eqs. (1.23) and (1.24); that is,
(a) 
The proof will be based on the following equivalence property:
Let g1(t) and g2(t) be generalized functions. Then the equivalence property states that g1(t) = g2(t) if and only if

for all suitably defined testing functions ϕ(t).
(a) With a change of variable, at = τ, and hence t = τ/a, dt = (1/a) dτ, we obtain the following equations:
If a > 0,

If a < 0,

Thus, for any a

Now, using Eq. (1.20) for ϕ (0), we obtain

for any ϕ(t). Then, by the equivalence property (1.99), we obtain

(b) Setting a = -1 in the above equation, we obtain

which shows that δ(t) is an even function.
1.26. (a) Verify Eq. (1.26):

if x(t) is continuous at t = t0.
(b) Verify Eq. (1.25):
x(t)δ(t) = x (0)δ(t)
if x(t) is continuous at t = 0.
(a) If x(t) is continuous at t = t0, then by definition (1.22) we have

for all ϕ(t) which are continuous at t = t0. Hence, by the equivalence property (1.99) we conclude that
x(t)δ(t - t0) = x(t0)δ(t - t0)
(b) Setting t0 = 0 in the above expression, we obtain
x(t)δ(t) = x (0)δ(t)
1.27. Show that
(a) tδ(t) = 0
(b) sin tδ(t) = 0
(c) cos tδ(t - π) = -δ(t - π)
Using Eqs. (1.25) and (1.26), we obtain
(a) tδ(t) = (0)δ(t) = 0
(b) sin tδ(t) = (sin 0)δ(t) = (0)δ(t) = 0
(c) cos tδ(t - π) = (cos π)δ(t - π) = (-1)δ(t - π) = -δ(t - π)
1.28. Verify Eq. (1.30):

From Eq. (1.28) we have

where ϕ(t) is a testing function which is continuous at t = 0 and vanishes outside some fixed interval. Thus, ϕ'(t) exists and is integrable over 0 < t < ∞ and ϕ (∞) = 0. Then using Eq. (1.98) or definition (1.18), we have

Since ϕ(t) is arbitrary and by equivalence property (1.99), we conclude that

1.29. Show that the following properties hold for the derivative of δ(t):
(a) 
(b) 
(a) Using Eqs. (1.28) and (1.20), we have

(b) Using Eqs. (1.101) and (1.20), we have

Thus, by the equivalence property (1.99) we conclude that
tδ'(t) = -δ(t)
1.30. Evaluate the following integrals:
(a) 
(b) 
(c) 
(d) 
(e) 
(a) By Eq. (1.21), with a = -1 and b = 1, we have

(b) By Eq. (1.21), with a = 1 and b = 2, we have

(c) By Eq. (1.22)

(d) Using Eqs. (1.22) and (1.23), we have

(e) By Eq. (1.29)

1.31. Find and sketch the first derivatives of the following signals:
(a) x(t) = u(t) - u(t -a), a > 0
(b) x(t) = t[u(t) - u(t -a)], a > 0
(c) 
(a) Using Eq. (1.30), we have
u'(t) = δ(t) and u'(t -a) = δ(t -a)
Then
x'(t) = u'(t) - u'(t -a) = δ(t) - δ(t -a)
Signals x(t) and x'(t) are sketched in Fig. 1-31(a).
(b) Using the rule for differentiation of the product of two functions and the result from part (a), we have
x'(t) = [u(t) - u(t -a)] + t[δ(t) - δ(t -a)]
But by Eqs. (1.25) and (1.26)
tδ(t) = (0)δ(t) = 0 and tδ(t -a) = aδ(t -a)
Thus,
x'(t) = u(t) - u(t -a) - aδ(t -a)
Signals x(t) and x'(t) are sketched in Fig. 1-31(b).
(c) x(t) = sgn t can be rewritten as
x(t) = sgn t = u(t) - u (-t)
Then using Eq. (1.30), we obtain
x'(t) = u'(t) - u' (-t) = δ(t) - [-δ(t)] = 2δ(t)
Signals x(t) and x'(t) are sketched in Fig. 1-31(c).


Fig. 1-31

Systems and Classification of Systems
1.32. Consider the RC circuit shown in Fig. 1-32. Find the relationship between the input x(t) and the output y(t)
(a) If x(t) = vs(t) and y(t) = vc(t).
(b) If x(t) = vs(t) and y(t) = i(t).


Fig. 1-32 RC circuit.

(a) Applying Kirchhoff's voltage law to the RC circuit in Fig. 1-32, we obtain

The current i(t) and voltage vc(t) are related by

Letting vs(t) = x(t) and vc(t) = y(t) and substituting Eq. (1.04) into Eq. (1.103), we obtain

or

Thus, the input-output relationship of the RC circuit is described by a first-order linear differential equation with constant coefficients.
(b) Integrating Eq. (1.104), we have

Substituting Eq. (1.106) into Eq. (1.103) and letting vs(t) = x(t) and i(t) = y(t), we obtain

or

Differentiating both sides of the above equation with respect to t, we obtain

Thus, the input-output relationship is described by another first-order linear differential equation with constant coefficients.
1.33. Consider the capacitor shown in Fig. 1-33. Let input x(t) = i(t) and output y(t) = vc(t).
(a) Find the input-output relationship.
(b) Determine whether the system is (i) memoryless, (ii) causal, (iii) linear, (iv) time-invariant, or (v) stable.


Fig. 1-33

(a) Assume the capacitance C is constant. The output voltage y(t) across the capacitor and the input current x(t) are related by [Eq. (1.106)]

(b) (i) From Eq. (1.108) it is seen that the output y(t) depends on the past and the present values of the input. Thus, the system is not memoryless.
(ii) Since the output y(t) does not depend on the future values of the input, the system is causal.
(iii) Let x(t) = α1x1(t) + α2x2(t). Then

Thus, the superposition property (1.68) is satisfied and the system is linear.
(iv) Let y1(t) be the output produced by the shifted input current x1(t) = x(t - t0).
Then

Hence, the system is time-invariant.
(v) Let x(t) = k1u(t), with k1 ≠ 0. Then

where r(t) = tu(t) is known as the unit ramp function (Fig. 1-34). Since y(t) grows linearly in time without bound, the system is not BIBO stable.


Fig. 1-34 Unit ramp function.

1.34. Consider the system shown in Fig. 1-35. Determine whether it is (a) memoryless, (b) causal, (c) linear, (d) time-invariant, or (e) stable.


Fig. 1-35

(a) From Fig. 1-35 we have
y(t) = T{x(t} = x(t) cos ωct
Since the value of the output y(t) depends on only the present values of the input x(t), the system is memoryless.
(b) Since the output y(t) does not depend on the future values of the input x(t), the system is causal.
(c) Let x(t) = α1x(t) + α2x(t). Then

Thus, the superposition property (1.68) is satisfied and the system is linear.
(d) Let y1(t) be the output produced by the shifted input x1(t) = x(t - t0). Then
y1(t) = T{x(t - t0)} = x(t - t0) cos ωct
But
y(t - t0) = x(t - t0) COS ωc(t - t0) ≠ y1(t)
Hence, the system is not time-invariant.
(e) Since |cos ωct| ≤ 1, we have
|y(t)| = |x(t) cos ωct| ≤ |x(t)|
Thus, if the input x(t) is bounded, then the output y(t) is also bounded and the system is BIBO stable.
1.35. A system has the input-output relation given by

Show that this system is nonlinear.

Thus, the system is nonlinear.
1.36. The discrete-time system shown in Fig. 1-36 is known as the unit delay element. Determine whether the system is (a) memoryless, (b) causal, (c) linear, (d) time-invariant, or (e) stable.


Fig. 1-36 Unit delay element

(a) The system input-output relation is given by

Since the output value at n depends on the input values at n - 1, the system is not memoryless.
(b) Since the output does not depend on the future input values, the system is causal.
(c) Let x[n] = α1x1[n] + α2x2[n]. Then

Thus, the superposition property (1.68) is satisfied and the system is linear.
(d) Let y1[n] be the response to x1[n] = x[n - n0]. Then
y1[n] = T{x1[n]} = x1[n - 1] = x[n - 1 - n0]
and
y[n - n0] = x[n - n0 -1] = x[n - 1 - n0] = y1[n]
Hence, the system is time-invariant.
(e) Since
|y[n]| = |x[n - 1] ≤ k      if |x[n]| ≤ k for all n
the system is BIBO stable.
1.37. Find the input-output relation of the feedback system shown in Fig. 1-37.


Fig. 1-37

From Fig. 1-37 the input to the unit delay element is x[n] - y[n]. Thus, the output y[n] of the unit delay element is [Eq. (1.111)]
y[n] = x[n - 1] - y[n - 1]
Rearranging, we obtain

Thus, the input-output relation of the system is described by a first-order difference equation with constant coefficients.
1.38. A system has the input-output relation given by

Determine whether the system is (a) memoryless, (b) causal, (c) linear, (d) time-invariant, or (e) stable.
(a) Since the output value at n depends on only the input value at n, the system is memoryless.
(b) Since the output does not depend on the future input values, the system is causal.
(c) Let x[n] + α2x2[n]. Then

Thus, the superposition property (1.68) is satisfied and the system is linear.
(d) Let y1[n] be the response to x1[n] = x[n - n0]. Then
y1[n] = T{x[n - n0]} = nx[n - n0]
But
y[n - n0] = (n - n0) x[n - n0] ≠ y1[n]
Hence, the system is not time-invariant.
(e) Let x[n] = u[n]. Then y[n] = nu[n]. Thus, the bounded unit step sequence produces an output sequence that grows without bound (Fig. 1-38) and the system is not BIBO stable.


Fig. 1-38

1.39. A system has the input-output relation given by

where k0 is a positive integer. Is the system time-invariant?
Let y[n] be the response to x1[n] = x[n - n]. Then
y[n] = T{x1[n]} = x1[k0n] = x[k0n - n]
But
y[n - n0] = x[k0(n - n0)] ≠ y1[n]
Hence, the system is not time-invariant unless k0 = 1. Note that the system described by Eq. (1.114) is called a compressor. It creates the output sequence by selecting every k0th sample of the input sequence. Thus, it is obvious that this system is time-varying.
1.40. Consider the system whose input-output relation is given by the linear equation

where x and y are the input and output of the system, respectively, and a and b are constants. Is this system linear?
If b ≠ 0, then the system is not linear because x = 0 implies y = b ≠ 0. If b = 0, then the system is linear.
1.41. The system represented by T in Fig. 1-39 is known to be time-invariant. When the inputs to the system are x1[n], x2[n], and x3[n], the outputs of the system are y1[n], y2[n], and y3[n] as shown. Determine whether the system is linear.


Fig. 1-39

From Fig. 1-39 it is seen that
x3[n] = x1[n] + x2[n - 2]
Thus, if T is linear, then
T{x3[n]} = T{x1[n]} + T {x2[n - 2]} = y1[n] + y2[n - 2]
which is shown in Fig. 1-40. From Figs. 1-39 and 1-40 we see that
y3[n] ≠ y1[n] + y2[n - 2]
Hence, the system is not linear.


Fig. 1-40

1.42. Give an example of a system that satisfies the condition of additivity (1.66) but not the condition of homogeneity (1.67).
Consider a discrete-time system represented by an operator T such that

where x*[n] is the complex conjugate of x[n]. Then

Next, if α is any arbitrary complex-valued constant, then

Thus, the system is additive but not homogeneous.
1.43. (a) Show that the causality for a continuous-time linear system is equivalent to the following statement: For any time t0 and any input x(t) with x(t) = 0 for t ≤ t0, the output y(t) is zero for t ≤ t0.
(b) Find a nonlinear system that is causal but does not satisfy this condition.
(c) Find a nonlinear system that satisfies this condition but is not causal.
(a) Since the system is linear, if x(t) = 0 for all t, then y(t) = 0 for all t. Thus, if the system is causal, then x(t) = 0 for t ≤ t0 implies that y(t) = 0 for t ≤ t0. This is the necessary condition. That this condition is also sufficient is shown as follows: let x1(t) and x2(t) be two inputs of the system and let y1(t) and y2(t) be the corresponding outputs. If x1(t) = x2(t) for t ≤ t0, or x(t) = x1(t) - x2(t) = 0 for t ≤ t0, then y1(t) = y2(t) for t ≤ t0, or y(t) = y1(t) - y2(t) = 0 for t ≤ t0.
(b) Consider the system with the input-output relation
y(t) = x(t) + 1
This system is nonlinear (Prob. 1.40) and causal since the value of y(t) depends on only the present value of x(t). But with x(t) = 0 for t ≤ t0, y(t) = 1 for t ≤ t0.
(c) Consider the system with the input-output relation
y(t) = x(t)x(t + 1)
It is obvious that this system is nonlinear (see Prob. 1.35) and noncausal since the value of y(t) at time t depends on the value of x(t + 1) of the input at time t + 1. Yet x(t) = 0 for t ≤ t0 implies that y(t) = 0 for t ≤ t0.
1.44. Let T represent a continuous-time LTI system. Then show that

where s is a complex variable and λ is a complex constant.
Let y(t) be the output of the system with input x(t) = est. Then
T{est} = y(t)
Since the system is time-invariant, we have

for arbitrary real t0. Since the system is linear, we have

Hence,
Setting t = 0, we obtain

Since t0 is arbitrary, by changing t0 to t, we can rewrite Eq. (1.118) as

or
where λ = y (0).
1.45. Let T represent a discrete-time LTI system. Then show that

where z is a complex variable and λ is a complex constant.
Let y[n] be the output of the system with input x[n] = zn. Then
T{zn} = y[n]
Since the system is time-invariant, we have

for arbitrary integer n0. Since the system is linear, we have

Hence,
Setting n = 0, we obtain

Since n0 is arbitrary, by changing n0 to n, we can rewrite Eq. (1.120) as

or
where λ = y[0].
In mathematical language, a function x (·) satisfying the equation

is called an eigenfunction (or characteristic function) of the operator T, and the constant λ is called the eigenvalue (or characteristic value) corresponding to the eigenfunction x (·). Thus, Eqs. (1.117) and (1.119) indicate that the complex exponential functions are eigenfunctions of any LTI system.
SUPPLEMENTARY PROBLEMS
1.46. Express the signals shown in Fig. 1-41 in terms of unit step functions.


Fig. 1-41

1.47. Express the sequences shown in Fig. 1-42 in terms of unit step sequences.


Fig. 1-42

1.48. Determine the even and odd components of the following signals:
(a) x(t) = u(t)
(b) 
(c) 
(d) x[n] = δ[n]
1.49. Let x(t) be an arbitrary signal with even and odd parts denoted by xe(t) and x0(t), respectively. Show that

1.50. Let x[n] be an arbitrary sequence with even and odd parts denoted by xe[n] and x0[n], respectively. Show that

1.51. Determine whether or not each of the following signals is periodic. If a signal is periodic, determine its fundamental period.
(a) 
(b) x(t) = cos2t
(c) x(t) = (cos2πt)u(t)
(d) x(t) = ejπt
(e) x[n] = ej[(n/4)-π]
(f) 
(g) 
(h) 
1.52. Show that if x[n] is periodic with period N, then
(a) 
1.53. (a) What is δ (2t)?(b) What is δ[2n]?
1.54. Show that
δ' (-t) = -δ'(t)
1.55. Evaluate the following integrals:
(a) 
(b) 
(c) 
(d) 
1.56. Consider a continuous-time system with the input-output relation

Determine whether this system is (a) linear, (b) time-invariant, (c) causal.
1.57. Consider a continuous-time system with the input-output relation

Determine whether this system is (a) linear, (b) time-invariant.
1.58. Consider a discrete-time system with the input-output relation
y[n] = T{x[n]} = x2[n]
Determine whether this system is (a) linear, (b) time-invariant.
1.59. Give an example of a system that satisfies the condition of homogeneity (1.67) but not the condition of additivity (1.66).
1.60. Give an example of a linear time-varying system such that with a periodic input the corresponding output is not periodic.
1.61. A system is called invertible if we can determine its input signal x uniquely by observing its output signal y. This is illustrated in Fig. 1-43. Determine if each of the following systems is invertible. If the system is invertible, give the inverse system.


Fig. 1-43

(a) y(t) = 2x(t)
(b) y(t) = x2(t)
(c) 
(d) 
(e) y[n] = nx[n]
ANSWERS TO SUPPLEMENTARY PROBLEMS
1.46. (a) 
(b) x(t) = u(t + 1) + 2u(t) - u(t - 1) - u(t - 2) - u(t - 3)
1.47. (a) x[n] = u[n] - u[n - (N + 1)]
(b) x[n] = - u[-n - 1]
(c) x[n] = u[n + 2] - u[n - 4]
1.48. (a) 
(b) 
(c) xe[n] = j cos Ω0n, x0[n] = - sinΩ0n
(d) xe[n] = δ[n], x0[n] = 0
1.49. Hint: Use the results from Prob. 1.7 and Eq. (1.77).
1.50. Hint: Use the results from Prob. 1.7 and Eq. (1.77).
1.51. (a) Periodic, period = π
(b) Periodic, period = π
(c) Nonperiodic
(d) Periodic, period = 2
(e) Nonperiodic
(f) Periodic, period = 8
(g) Nonperiodic
(h) Periodic, period = 16
1.52. Hint: See Prob. 1.17.
1.53. (a) 
(b) δ[2n] = δ[n]
1.54. Hint: Use Eqs. (1.101) and (1.99).
1.55. (a) sin t
(b) 1 for t > 0 and 0 for t < 0; not defined for t = 0
(c) 0
(d) π
1.56. (a) Linear; (b) Time-invariant; (c) Noncausal
1.57. (a) Linear; (b) Time-varying
1.58. (a) Nonlinear; (b) Time-invariant
1.59. Consider the system described by

1.60. y(n) = T{x[n]} = nx[n]
1.61. (a) Invertible; 
(b) Not invertible
(c) Invertible; 
(d) Invertible; x[n] = y[n] - y[n - 1]
(e) Not invertible







CHAPTER 2Linear Time-Invariant Systems
2.1 Introduction
Two most important attributes of systems are linearity and time-invariance. In this chapter we develop the fundamental input-output relationship for systems having these attributes. It will be shown that the input-output relationship for LTI systems is described in terms of a convolution operation. The importance of the convolution operation in LTI systems stems from the fact that knowledge of the response of an LTI system to the unit impulse input allows us to find its output to any input signals. Specifying the input-output relationships for LTI systems by differential and difference equations will also be discussed.
2.2 Response of a Continuous-Time LTI System and the Convolution Integral
A. Impulse Response:
The impulse response h(t) of a continuous-time LTI system (represented by T) is defined to be the response of the system when the input is δ(t), that is,

B. Response to an Arbitrary Input:
From Eq. (1.27) the input x(t) can be expressed as

Since the system is linear, the response y(t) of the system to an arbitrary input x(t) can be expressed as

Since the system is time-invariant, we have

Substituting Eq. (2.4) into Eq. (2.3), we obtain

Equation (2.5) indicates that a continuous-time LTI system is completely characterized by its impulse response h(t).
C. Convolution Integral:
Equation (2.5) defines the convolution of two continuous-time signals x(t) and h(t) denoted by

Equation (2.6) is commonly called the convolution integral. Thus, we have the fundamental result that the output of any continuous-time LTI system is the convolution of the input x(t) with the impulse response h(t) of the system. Fig. 2-1 illustrates the definition of the impulse response h(t) and the relationship of Eq. (2.6).


Fig. 2-1 Continuous-time LTI system.

D. Properties of the Convolution Integral:
The convolution integral has the following properties.
1. Commutative:

2. Associative:

3. Distributive:

E. Convolution Integral Operation:
Applying the commutative property (2.7) of convolution to Eq. (2.6), we obtain

which may at times be easier to evaluate than Eq. (2.6). From Eq. (2.6) we observe that the convolution integral operation involves the following four steps:
1. The impulse response h(τ) is time-reversed (that is, reflected about the origin) to obtain h (-τ) and then shifted by t to form h(t -τ) = h[-(τ - t)], which is a function of τ with parameter t.
2. The signal x(τ) and h(t - τ) are multiplied together for all values of τ with t fixed at some value.
3. The product x(τ)h(t - τ) is integrated over all τ to produce a single output value y(t).
4. Steps 1 to 3 are repeated as t varies over - ∞ to ∞ to produce the entire output y(t).
Examples of the above convolution integral operation are given in Probs. 2.4 to Probs. 2.6.
F. Step Response:
The step response s(t) of a continuous-time LTI system (represented by T) is defined to be the response of the system when the input is u(t); that is,

In many applications, the step response s(t) is also a useful characterization of the system. The step response s(t) can be easily determined by Eq. (2.10); that is,

Thus, the step response s(t) can be obtained by integrating the impulse response h(t). Differentiating Eq. (2.12) with respect to t, we get

Thus, the impulse response h(t) can be determined by differentiating the step response s(t).
2.3 Properties of Continuous-Time LTI Systems
A. Systems with or without Memory:
Since the output y(t) of a memoryless system depends on only the present input x(t), then, if the system is also linear and time-invariant, this relationship can only be of the form

where K is a (gain) constant. Thus, the corresponding impulse response h(t) is simply

Therefore, if h(t0) ≠ 0 for t0 ≠ 0, then continuous-time LTI system has memory.
B. Causality:
As discussed in Sec. 1.5D, a causal system does not respond to an input event until that event actually occurs. Therefore, for a causal continuous-time LTI system, we have

Applying the causality condition (2.16) to Eq. (2.10), the output of a causal continuous-time LTI system is expressed as

Alternatively, applying the causality condition (2.16) to Eq. (2.6), we have

Equation (2.18) shows that the only values of the input x(t) used to evaluate the output y(t) are those for τ ≤ t.
Based on the causality condition (2.16), any signal x(t) is called causal if

and is called anticausal if

Then, from Eqs. (2.17), (2.18), and (2.19a), when the input x(t) is causal, the output y(t) of a causal continuous-time LTI system is given by

C. Stability:
The BIBO (bounded-input/bounded-output) stability of an LTI system (Sec. 1.5H) is readily ascertained from its impulse response. It can be shown (Prob. 2.13) that a continuous-time LTI system is BIBO stable if its impulse response is absolutely integrable; that is,

2.4 Eigenfunctions of Continuous-Time LTI Systems
In Chap. 1 (Prob. 1.44) we saw that the eigenfunctions of continuous-time LTI systems represented by T are the complex exponentials est, with s a complex variable. That is,

where λ is the eigenvalue of T associated with est. Setting x(t) = est in Eq. (2.10), we have

where

Thus, the eigenvalue of a continuous-time LTI system associated with the eigenfunction est is given by H(s), which is a complex constant whose value is determined by the value of s via Eq. (2.24). Note from Eq. (2.23) that y (0) = H(s) (see Prob. 1.44).
The above results underlie the definitions of the Laplace transform and Fourier transform, which will be discussed in Chaps. 3 and 5.
2.5 Systems Described by Differential Equations
A. Linear Constant-Coefficient Differential Equations:
A general Nth-order linear constant-coefficient differential equation is given by

where coefficients ak and bk are real constants. The order N refers to the highest derivative of y(t) in Eq. (2.25). Such differential equations play a central role in describing the input-output relationships of a wide variety of electrical, mechanical, chemical, and biological systems. For instance, in the RC circuit considered in Prob. 1.32, the input x(t) = vc(t) and the output y(t) = vc(t) are related by a first-order constant-coefficient differential equation [Eq. (1.105)]

The general solution of Eq. (2.25) for a particular input x(t) is given by

where yp(t) is a particular solution satisfying Eq. (2.25) and yh(t) is a homogeneous solution (or complementary solution) satisfying the homogeneous differential equation

The exact form of yh(t) is determined by N auxiliary conditions. Note that Eq. (2.25) does not completely specify the output y(t) in terms of the input x(t) unless auxiliary conditions are specified. In general, a set of auxiliary conditions are the values of

at some point in time.
B. Linearity:
The system specified by Eq. (2.25) will be linear only if all of the auxiliary conditions are zero (see Prob. 2.21). If the auxiliary conditions are not zero, then the response y(t) of a system can be expressed as

where yzi(t), called the zero-input response, is the response to auxiliary conditions, and yzs(t), called the zero-state response, is the response of a linear system with zero auxiliary conditions. This is illustrated in Fig. 2-2.
Note that yzi(t) ≠yh(t) and yzs(t) ≠ yp(t) and that in general yzi(t) contains yh(t) and yzs(t) contains both yh(t) and yp(t) (see Prob. 2.20).


Fig. 2-2 Zero-state and zero-input responses.

C. Causality:
In order for the linear system described by Eq. (2.25) to be causal we must assume the condition of initial rest (or an initially relaxed condition). That is, if x(t) = 0 for t ≤ t0, then assume y(t) = 0 for t ≤ t0 (see Prob. 1.43). Thus, the response for t > t0 can be calculated from Eq. (2.25) with the initial conditions
where

Clearly, at initial rest yzi(t) = 0.
D. Time-Invariance:
For a linear causal system, initial rest also implies time-invariance (Prob. 2.22).
E. Impulse Response:
The impulse response h(t) of the continuous-time LTI system described by Eq. (2.25) satisfies the differential equation

with the initial rest condition. Examples of finding impulse responses are given in Probs. 2.23 to Probs. 2.25. In later chapters, we will find the impulse response by using transform techniques.
2.6 Response of a Discrete-Time LTI System and Convolution Sum
A. Impulse Response:
The impulse response (or unit sample response) h[n] of a discrete-time LTI system (represented by T) is defined to be the response of the system when the input is δ [n]; that is,

B. Response to an Arbitrary Input:
From Eq. (1.51) the input x[n] can be expressed as

Since the system is linear, the response y[n] of the system to an arbitrary input x[n] can be expressed as

Since the system is time-invariant, we have

Substituting Eq. (2.33) into Eq. (2.32), we obtain

Equation (2.34) indicates that a discrete-time LTI system is completely characterized by its impulse response h[n].
C. Convolution Sum:
Equation (2.34) defines the convolution of two sequences x[n] and h[n] denoted by

Equation (2.35) is commonly called the convolution sum. Thus, again, we have the fundamental result that the output of any discrete-time LTI system is the convolution of the input x[n] with the impulse response h[n] of the system.
Fig. 2-3 illustrates the definition of the impulse response h[n] and the relationship of Eq. (2.35).


Fig. 2-3 Discrete-time LTI system.

D. Properties of the Convolution Sum:
The following properties of the convolution sum are analogous to the convolution integral properties shown in Sec. 2.3.
1. Commutative:

2. Associative:

3. Distributive:

E. Convolution Sum Operation:
Again, applying the commutative property (2.36) of the convolution sum to Eq. (2.35), we obtain

which may at times be easier to evaluate than Eq. (2.35). Similar to the continuous-time case, the convolution sum [Eq. (2.35)] operation involves the following four steps:
1. The impulse response h[k] is time-reversed (that is, reflected about the origin) to obtain h[-k] and then shifted by n to form h[n - k] = h[ - (k - n)], which is a function of k with parameter n.
2. Two sequences x[k] and h[n - k] are multiplied together for all values of k with n fixed at some value.
3. The product x[k]h[n - k] is summed over all k to produce a single output sample y[n].
4. Steps 1 to 3 are repeated as n varies over - ∞ to ∞ to produce the entire output y[n].
Examples of the above convolution sum operation are given in Probs. 2.28 and Probs. 2.30.
F. Step Response:
The step response s[n] of a discrete-time LTI system with the impulse response h[n] is readily obtained from Eq. (2.39) as

From Eq. (2.40) we have

Equations (2.40) and (2.41) are the discrete-time counterparts of Eqs. (2.12) and (2.13), respectively.
2.7 Properties of Discrete-Time LTI Systems
A. Systems with or without Memory:
Since the output y[n] of a memoryless system depends on only the present input x[n], then, if the system is also linear and time-invariant, this relationship can only be of the form

where K is a (gain) constant. Thus, the corresponding impulse response is simply

Therefore, if h[n0] ≠ 0 for n0 ≠ 0, the discrete-time LTI system has memory.
B. Causality:
Similar to the continuous-time case, the causality condition for a discrete-time LTI system is

Applying the causality condition (2.44) to Eq. (2.39), the output of a causal discrete-time LTI system is expressed as

Alternatively, applying the causality condition (2.44) to Eq. (2.35), we have

Equation (2.46) shows that the only values of the input x[n] used to evaluate the output y[n] are those for k ≤ n.
As in the continuous-time case, we say that any sequence x[n] is called causal if

and is called anticausal if

Then, when the input x[n] is causal, the output y[n] of a causal discrete-time LTI system is given by

C. Stability:
It can be shown (Prob. 2.37) that a discrete-time LTI system is BIBO stable if its impulse response is absolutely summable; that is,

2.8 Eigenfunctions of Discrete-Time LTI Systems
In Chap. 1 (Prob. 1.45) we saw that the eigenfunctions of discrete-time LTI systems represented by T are the complex exponentials zn, with z a complex variable. That is,

where λ is the eigenvalue of T associated with zn. Setting x[n] = zn in Eq. (2.39), we have

where

Thus, the eigenvalue of a discrete-time LTI system associated with the eigenfunction zn is given by H(z), which is a complex constant whose value is determined by the value of z via Eq. (2.52). Note from Eq. (2.51) that y[0] = H(z) (see Prob. 1.45).
The above results underlie the definitions of the z-transform and discrete-time Fourier transform, which will be discussed in Chaps. 4 and 6.
2.9 Systems Described by Difference Equations
The role of differential equations in describing continuous-time systems is played by difference equations for discrete-time systems.
A. Linear Constant-Coefficient Difference Equations:
The discrete-time counterpart of the general differential equation (2.25) is the Nth-order linear constant-coefficient difference equation given by

where coefficients ak and bk are real constants. The order N refers to the largest delay of y[n] in Eq. (2.53). An example of the class of linear constant-coefficient difference equations is given in Chap. 1 (Prob. 1.37). Analogous to the continuous-time case, the solution of Eq. (2.53) and all properties of systems, such as linearity, causality, and time-invariance, can be developed following an approach that directly parallels the discussion for differential equations. Again we emphasize that the system described by Eq. (2.53) will be causal and LTI if the system is initially at rest.
B. Recursive Formulation:
An alternate and simpler approach is available for the solution of Eq. (2.53). Rearranging Eq. (2.53) in the form

we obtain a formula to compute the output at time n in terms of the present input and the previous values of the input and output. From Eq. (2.54) we see that the need for auxiliary conditions is obvious and that to calculate y[n] starting at n = n0, we must be given the values of y[n0 - 1], y [n0 - 2], ..., y[n0 - N] as well as the input x[n] for n ≥ n0 - M. The general form of Eq. (2.54) is called a recursive equation, since it specifies a recursive procedure for determining the output in terms of the input and previous outputs. In the special case when N = 0, from Eq. (2.53) we have

which is a nonrecursive equation, since previous output values are not required to compute the present output. Thus, in this case, auxiliary conditions are not needed to determine y[n].
C. Impulse Response:
Unlike the continuous-time case, the impulse response h[n] of a discrete-time LTI system described by Eq. (2.53) or, equivalently, by Eq. (2.54) can be determined easily as

For the system described by Eq. (2.55) the impulse response h[n] is given by

Note that the impulse response for this system has finite terms; that is, it is nonzero for only a finite time duration. Because of this property, the system specified by Eq. (2.55) is known as a finite impulse response (FIR) system. On the other hand, a system whose impulse response is nonzero for an infinite time duration is said to be an infinite impulse response (IIR) system. Examples of finding impulse responses are given in Probs. 2.44 and Probs. 2.45. In Chap. 4, we will find the impulse response by using transform techniques.
SOLVED PROBLEMS
Responses of a Continuous-Time LTI System and Convolution
2.1. Verify Eqs. (2.7) and (2.8); that is,
(a) x(t) * h(t) = h(t) * x(t)
(b) {x(t) * h1(t)} * h2(t) = x(t) * {h1(t) * h2(t)}
(a) By definition (2.6)

By changing the variable t - τ = λ, we have

(b) Let x(t) * h1(t) = f1(t) and h1(t) * h2(t) = f2(t). Then

and


Substituting λ = σ - τ and interchanging the order of integration, we have

Now, since

we have

Thus,

2.2. Show that
(a) 
(b) 
(c) 
(d) 
(a) By definition (2.6) and Eq. (1.22) we have

(b) By Eqs. (2.7) and (1.22) we have

(c) By Eqs. (2.6) and (1.19) we have

since 
(d) In a similar manner, we have

since 
2.3. Let y(t) = x(t) * h(t). Then show that

By Eq. (2.6) we have

and

Let τ - t1 = λ. Then τ = λ + t1 and Eq. (2.63b) becomes

Comparing Eqs. (2.63a) and (2.63c), we see that replacing t in Eq. (2.63a) by t - t1 - t2, we obtain Eq. (2.63c). Thus, we conclude that
x(t - t1) * h(t - t1) = y(t - t1 - t2)
2.4. The input x(t) and the impulse response h(t) of a continuous time LTI system are given by

(a) Compute the output y(t) by Eq. (2.6).
(b) Compute the output y(t) by Eq. (2.10).
(a) By Eq. (2.6)

Functions x(τ) and h(t - τ) are shown in Fig. 2-4(a) for t < 0 and t > 0. From Fig. 2-4(a) we see that for t < 0, x(τ) and h(t - τ) do not overlap, while for t > 0, they overlap from τ = 0 to τ = t. Hence, for t < 0, y(t) = 0. For t > 0, we have



Fig. 2-4

Thus, we can write the output y(t) as

(b) By Eq. (2.10)

Functions h(τ) and x(t - τ) are shown in Fig. 2-4(b) for t < 0 and t > 0. Again from Fig. 2-4(b) we see that for t < 0, h(τ) and x(t - τ) do not overlap, while for t > 0, they overlap from τ = 0 to τ = t. Hence, for t < 0, y(t) = 0. For t > 0, we have

Thus, we can write the output y(t) as

which is the same as Eq. (2.64).
2.5. Compute the output y(t) for a continuous-time LTI system whose impulse response h(t) and the input x(t) are given by
h(t) = eαt u(t) x(t) = eαt u (-t) α > 0
By Eq. (2.6)

Functions x(τ) and h(t - τ) are shown in Fig. 2-5 (a) for t < 0 and t > 0. From Fig. 2-5 (a) we see that for t < 0, x(τ) and h(t - τ) overlap from τ = - ∞ to τ = t, while for t > 0, they overlap from τ = - ∞ to τ = 0. Hence, for t < 0, we have

For t > 0, we have

Combining Eqs. (2.66a) and (2.66b), we can write y(t) as

which is shown in Fig. 2-5(b).


Fig. 2-5

2.6. Evaluate y(t) = x(t) * h(t), where x(t) and h(t) are shown in Fig. 2-6, (a) by an analytical technique, and (b) by a graphical method.


Fig. 2-6

(a) We first express x(t) and h(t) in functional form:
x(t) = u(t) - u(t - 3)     h(t) = u(t) - u(t - 2)
Then, by Eq. (2.6) we have

Since

we can express y(t) as

which is plotted in Fig. 2-7.


Fig. 2-7

(b) Functions h(τ), x(τ) and h(t - τ), x(τ)h(t - τ) for different values of t are sketched in Fig. 2-8. From Fig. 2-8 we see that x(τ) and h(t - τ) do not overlap for t < 0 and t > 5, and hence, y(t) = 0 for t < 0 and t > 5. For the other intervals, x(τ) and h(t - τ) overlap. Thus, computing the area under the rectangular pulses for these intervals, we obtain

which is plotted in Fig. 2-9.
2.7. Let h(t) be the triangular pulse shown in Fig. 2-10(a) and let x(t) be the unit impulse train [Fig. 2-10(b)] expressed as

Determine and sketch y(t) = h(t) * x(t) for the following values of T: (a) T = 3, (b) T = 2, (c) T = 1.5.



Fig. 2-8



Fig. 2-9



Fig. 2-10

Using Eqs. (2.59) and (2.9), we obtain

(a) For T = 3, Eq. (2.69) becomes

which is sketched in Fig. 2-11(a).
(b) For T = 2, Eq. (2.69) becomes

which is sketched in Fig. 2-11(b).
(c) For T = 1.5, Eq. (2.69) becomes

which is sketched in Fig. 2-11(c). Note that when T < 2, the triangular pulses are no longer separated and they overlap.
2.8. If x1(t) and x2(t) are both periodic signals with a common period T0, the convolution of x1(t) and x2(t) does not converge. In this case, we define the periodic convolution of x1(t) and x2(t) as

(a) Show that f(t) is periodic with period T0.
(b) Show that

for any a.
(c) Compute and sketch the periodic convolution of the square-wave signal x(t) shown in Fig. 2-12 with itself.


Fig. 2-11



Fig. 2-12

(a) Since x2(t) is periodic with period T0, we have
x2(t + T0 - τ) = x2(t - τ)
Then from Eq. (2.70) we have

Thus, f(t) is periodic with period T0.
(b) Since both x1(τ) and x2(τ) are periodic with the same period T0, x1(τ)x2(t - τ) is also periodic with period T0. Then using property (1.88) (Prob. 1.17), we obtain

for an arbitrary a.
(c) We evaluate the periodic convolution graphically. Signals x(τ), x(t - τ), and x(τ)x(t - τ) are sketched in Fig. 2-13(a), from which we obtain

which is plotted in Fig. 2-13(b).


Fig. 2-13

Properties of Continuous-Time LTI Systems
2.9. The signals in Figs. 2-14(a) and (b) are the input x(t) and the output y(t), respectively, of a certain continuous-time LTI system. Sketch the output to the following inputs: (a) x(t - 2); (b) x(t).
(a) Since the system is time-invariant, the output will be y(t - 2), which is sketched in Fig. 2-14(c).
(b) Since the system is linear, the output will be  y(t), which is sketched in Fig. 2-14(d).


Fig. 2-14

2.10. Consider a continuous-time LTI system whose step response is given by
s(t) = e-t u(t)
Determine and sketch the output of this system to the input x(t) shown in Fig. 2-15(a).
From Fig. 2-15(a) the input x(t) can be expressed as
x(t) = u(t - 1) - u(t - 3)
Since the system is linear and time-invariant, the output y (t) is given by

which is sketched in Fig. 2-15(b).


Fig. 2-15

2.11. Consider a continuous-time LTI system described by (see Prob. 1.56)

(a) Find and sketch the impulse response h(t) of the system.
(b) Is this system causal?
(a) Equation (2.72) can be rewritten as

Using Eqs. (2.61) and (2.9), Eq. (2.73) can be expressed as

Thus, we obtain

which is sketched in Fig. 2-16.
(b) From Fig. 2-16 or Eq. (2.75) we see that h(t) ≠ 0 for t < 0. Hence, the system is not causal.


Fig. 2-16

2.12. Let y(t) be the output of a continuous-time LTI system with input x(t). Find the output of the system if the input is x'(t), where x'(t) is the first derivative of x(t).
From Eq. (2.10)

Differentiating both sides of the above convolution integral with respect to t, we obtain

which indicates that y'(t) is the output of the system when the input is x'(t).
2.13. Verify the BIBO stability condition [Eq. (2.21)] for continuous-time LTI systems.
Assume that the input x(t) of a continuous-time LTI system is bounded, that is,

Then, using Eq. (2.10), we have

since |x(t - τ)| ≤ k1 from Eq. (2.77). Therefore, if the impulse response is absolutely integrable, that is,

then |y(t)| ≤ k1 K = k2 and the system is BIBO stable.
2.14. The system shown in Fig. 2-17(a) is formed by connecting two systems in cascade. The impulse responses of the systems are given by h1(t) and h2(t), respectively, and
h1(t) = e-2t u(t)     h2(t) = 2e-t u(t)
(a) Find the impulse response h(t) of the overall system shown in Fig. 2-17(b).
(b) Determine if the overall system is BIBO stable.


Fig. 2-17

(a) Let w(t) be the output of the first system. By Eq. (2.6)

Then we have

But by the associativity property of convolution (2.8), Eq. (2.79) can be rewritten as

Therefore, the impulse response of the overall system is given by

Thus, with the given h1(t) and h2(t), we have

(b) Using the above h(t), we have

Thus, the system is BIBO stable.
Eigenfunctions of Continuous-Time LTI Systems
2.15. Consider a continuous-time LTI system with the input-output relation given by

(a) Find the impulse response h(t) of this system.
(b) Show that the complex exponential function est is an eigenfunction of the system.
(c) Find the eigenvalue of the system corresponding to est by using the impulse response h(t) obtained in part (a).
(a) From Eq. (2.82), definition (2.1), and Eq. (1.21) we get

Thus,
(b) Let x(t) = est. Then

Thus, by definition (2.22) est is the eigenfunction of the system and the associated eigenvalue is

(c) Using Eqs. (2.24) and (2.83), the eigenvalue associated with est is given by

which is the same as Eq. (2.85).
2.16. Consider the continuous-time LTI system described by

(a) Find the eigenvalue of the system corresponding to the eigenfunction est.
(b) Repeat part (a) by using the impulse function h(t) of the system.
(a) Substituting x(τ) = esτ in Eq. (2.86), we obtain

Thus, the eigenvalue of the system corresponding to est is

(b) From Eq. (2.75) in Prob. 2.11 we have

Using Eq. (2.24), the eigenvalue H(s) corresponding to est is given by

which is the same as Eq. (2.87).
2.17. Consider a stable continuous-time LTI system with impulse response h(t) that is real and even. Show that cos ωt and sin ωt are eigenfunctions of this system with the same real eigenvalue.
By setting s = jω in Eqs. (2.23) and (2.24), we see that ejωt is an eigenfunction of a continuous-time LTI system and the corresponding eigenvalue is

Since the system is stable, that is,
then       
since |e-jωτ| = 1. Thus, H(jω) converges for any ω. Using Euler's formula, we have

Since cos ωτ is an even function of τ and sin ωτ is an odd function of τ, and if h(t) is real and even, then h(τ) cos ωτ is even and h(τ) sin ωτ is odd. Then by Eqs. (1.75a) and (1.77), Eq. (2.89) becomes

Since cos ωτ is an even function of ω, changing ω to - ω in Eq. (2.90) and changing j to -j in Eq. (2.89), we have

Thus, we see that the eigenvalue H(jω) corresponding to the eigenfunction ejωt is real. Let the system be represented by T. Then by Eqs. (2.23), (2.24), and (2.91) we have

Now, since T is linear, we get

and

Thus, from Eqs. (2.93a) and (2.93b) we see that cos ωt and sin ωt are the eigenfunctions of the system with the same real eigenvalue H(jω) given by Eq. (2.88) or (2.90).
Systems Described by Differential Equations
2.18. The continuous-time system shown in Fig. 2-18 consists of one integrator and one scalar multiplier. Write a differential equation that relates the output y(t) and the input x(t).


Fig. 2-18

Let the input of the integrator shown in Fig. 2-18 be denoted by e(t). Then the input-output relation of the integrator is given by

Differentiating both sides of Eq. (2.94) with respect to t, we obtain

Next, from Fig. 2-18 the input e(t) to the integrator is given by

Substituting Eq. (2.96) into Eq. (2.95), we get
or       
which is the required first-order linear differential equation.
2.19. The continuous-time system shown in Fig. 2-19 consists of two integrators and two scalar multipliers. Write a differential equation that relates the output y(t) and the input x(t).


Fig. 2-19

Let e(t) and w(t) be the input and the output of the first integrator in Fig. 2-19, respectively. Using Eq. (2.95), the input to the first integrator is given by

Since w(t) is the input to the second integrator in Fig. 2-19, we have

Substituting Eq. (2.99) into Eq. (2.98), we get

or

which is the required second-order linear differential equation.
Note that, in general, the order of a continuous-time LTI system consisting of the interconnection of integrators and scalar multipliers is equal to the number of integrators in the system.
2.20. Consider a continuous-time system whose input x(t) and output y(t) are related by

where a is a constant.
(a) Find y(t) with the auxiliary condition y (0) = y0 and

(b) Express y(t) in terms of the zero-input and zero-state responses.
(a) Let
y(t) = yp(t) + yh(t)
where y(t) is the particular solution satisfying Eq. (2.101) and yh(t) is the homogeneous solution which satisfies

Assume that

Substituting Eq. (2.104) into Eq. (2.101), we obtain
-bAe-bt + aAe-bt = ke-bt
from which we obtain A = K/(a - b), and

To obtain yh(t), we assume
yh(t) = Best
Substituting this into Eq. (2.103) gives
sBest + aBest = (s + a) Best = 0
from which we have s = -a and
yh(t) = Be-at = 0
Combining yp(t) and yh(t), we get

From Eq. (2.106) and the auxiliary condition y (0) = y0, we obtain

Thus, Eq. (2.106) becomes

For t < 0, we have x(t) = 0, and Eq. (2.101) becomes Eq. (2.103). Hence,
y(t) = Be-at        t < 0
From the auxiliary condition y (0) = y0 we obtain

(b) Combining Eqs. (2.107) and (2.108), y(t) can be expressed in terms of yzi(t) (zero-input response) and yzs(t) (zero-state response) as

where


2.21. Consider the system in Prob. 2.20.
(a) Show that the system is not linear if y (0) = y0 ≠ 0.
(b) Show that the system is linear if y (0) = 0.
(a) Recall that a linear system has the property that zero input produces zero output (Sec. 1.5E). However, if we let K = 0 in Eq. (2.102), we have x(t) = 0, but from Eq. (2.109) we see that
y(t) = y0e-at ≠ 0       y0 ≠ 0
Thus, this system is nonlinear if y (0) = y0 ≠ 0.
(b) If y (0) = 0, the system is linear. This is shown as follows. Let x1(t) and x2(t) be two input signals, and let y1(t) and y2(t) be the corresponding outputs. That is,

with the auxiliary conditions

Consider
x(t) = α1x1(t) + α2x2(t)
where α1 and α2 are any complex numbers. Multiplying Eq. (2.111) by α1 and Eq. (2.112) by α2 and adding, we see that
y(t) = α1y1(t) + α2y2(t)
satisfies the differential equation

and also, from Eq. (2.113),
y (0) = α1y1(0) + α2y2(0) = 0
Therefore, y(t) is the output corresponding to x(t), and thus the system is linear.
2.22. Consider the system in Prob. 2.20. Show that the initial rest condition y (0) = 0 also implies that the system is time-invariant.
Let y1(t) be the response to an input x1(t) and

Then

and

Now, let y2(t) be the response to the shifted input x2(t) = x1(t - τ). From Eq. (2.114) we have

Then y2(t) must satisfy

and

Now, from Eq. (2.115) we have

If we let y2(t) = y1(t - τ), then by Eq. (2.116) we have
y2(τ) = y1(τ - τ) = y1(0) = 0
Thus, Eqs. (2.118) and (2.119) are satisfied and we conclude that the system is time-invariant.
2.23 Consider the system in Prob. 2.20. Find the impulse response h(t) of the system.
The impulse response h(t) should satisfy the differential equation

The homogeneous solution hh(t) to Eq. (2.120) satisfies

To obtain hh(t), we assume
hh(t) = cest
Substituting this into Eq. (2.121) gives
scest + acest = (s + a) cest = 0
from which we have s = -a and

We predict that the particular solution hp(t) is zero since hp(t) cannot contain δ(t). Otherwise, h(t) would have a derivative of δ(t) that is not part of the right-hand side of Eq. (2.120). Thus,

To find the constant c, substituting Eq. (2.123) into Eq. (2.120), we obtain

or

Using Eqs. (1.25) and (1.30), the above equation becomes

so that c = 1. Thus, the impulse response is given by

2.24 Consider the system in Prob. 2.20 with y (0) = 0.
(a) Find the step response s(t) of the system without using the impulse response h(t).
(b) Find the step response s(t) with the impulse response h(t) obtained in Prob. 2.23.
(c) Find the impulse response h(t) from s(t).
(a) In Prob. 2.20
x(t) = Ke-btu(t)
Setting K = 1, b = 0, we obtain x(t) = u(t) and then y(t) = s(t). Thus, setting K = 1, b = 0, and y (0) = y0 = 0 in Eq. (2.109), we obtain the step response

(b) Using Eqs. (2.12) and (2.124) in Prob. 2.23, the step response s(t) is given by

which is the same as Eq. (2.125).
(c) Using Eqs. (2.13) and (2.125), the impulse response h(t) is given by

Using Eqs. (1.25) and (1.30), we have

which is the same as Eq. (1.124).
2.25. Consider the system described by

Find the impulse response h(t) of the system.
The impulse response h(t) should satisfy the differential equation

The homogeneous solution hh(t) to Eq. (2.127) is [see Prob. 2.23 and Eq. (2.122)]
hh(t) = c1e-2t u(t)
Assuming the particular solution hp(t) of the form
hp(t) = c2δ(t)
the general solution is

The delta function δ(t) must be present so that h'(t) contributes δ'(t) to the left-hand side of Eq. (1.127). Substituting Eq. (2.128) into Eq. (2.127), we obtain

Again, using Eqs. (1.25) and (1.30), we have

Equating coefficients of δ(t) and δ'(t), we obtain

from which we have c, = -1 and c, = 1. Substituting these values in Eq. (2.128), we obtain

Responses of a Discrete-Time LTI System and Convolution
2.26 Verify Eqs. (2.36) and (2.37); that is,
(a) x[n] * h[n] = h[n] * x[n]
(b) {x[n] * h1[n]} * hJ[n] = x[n] * {h1[n] * h[2n]}
(a) By definition (2.35)

By changing the variable n - k = m, we have

(b) Let x[n] * h1[n1] = ƒ1[n] and h1[n] * h2[n] = ƒ2[n]. Then

and

Substituting r = m - k and interchanging the order of summation, we have

Now, since

we have

Thus,

2.27. Show that
(a) 
(b) 
(c) 
(d) 
(a) By Eq. (2.35) and property (1.46) of δ[n - k] we have

(b) Similarly, we have

(c) By Eq. (2.35) and definition (1.44) of u[n - k] we have

(d) In a similar manner, we have

2.28 The input x[n] and the impulse response h[n] of a discrete-time LTI system are given by
x[n] = u[n]     h[n] = αn u[n]     0 > α > 1
(a) Compute the output y[n] by Eq. (2.35).
(b) Compute the output y[n] by Eq. (2.39).
(a) By Eq. (2.35) we have

Sequences x[k] and h[n - k] are shown in Fig. 2-20(a) for n < 0 and n > 0. From Fig. 2-20(a) we see that for n < 0, x[k] and h[n - k ] do not overlap, while for n ≥ 0, they overlap from k = 0 to k = n. Hence, for n < 0, y[n ] = 0. For n ≥ 0, we have



Fig. 2-20

Changing the variable of summation k to m = n - k and using Eq. (1.90), we have

Thus, we can write the output y[n] as

which is sketched in Fig. 2-20(b).
(b) By Eq. (2.39)

Sequences h[k] and x[n - k] are shown in Fig. 2-21 for n < 0 and n > 0. Again from Fig. 2-21 we see that for n < 0, h[k] and x[n - k ] do not overlap, while for n ≥ 0, they overlap from k = 0 to k = n. Hence, for n < 0, y[n] = 0. For n ≤ 0, we have

Thus, we obtain the same result as shown in Eq. (2.134).


Fig. 2-21

2.29 Compute y[n] = x[n] * h[n], where
(a) x[n] = αnu[n], h[n] = βnu[n]
(b) x[n] = αnu[n], h[n] = α-n u[-n], 0 < α < 1
(a) From Eq. (2.35) we have

since

we have

Using Eq. (1.90), we obtain

or

(b)

For n ≤ 0, we have

Thus, using Eq. (1.91), we have

For n ≥ 0, we have

Thus, using Eq. (1.92), we have

Combining Eqs. (2.136a) and (2.136b), we obtain

which is sketched in Fig. 2-22.


Fig. 2-22

2.30. Evaluate y[n] = x[n] * h[n], where x[n] and h[n] are shown in Fig. 2-23, (a) by an analytical technique, and (b) by a graphical method.


Fig. 2-23

(a) Note that x[n] and h[n] can be expressed as

Now, using Eqs. (2.38), (2.130), and (2.131), we have

Thus,

or

or

(b) Sequences h[k], x[k] and h[n - k], x[k] h[n - k] for different values of n are sketched in Fig. 2-24.
From Fig. 2-24 we see that x[k] and h[n - k] do not overlap for n < 0 and n > 5, and hence, y[n] = 0 for n < 0 and n > 5. For 0 ≤ n ≤ 5, x[k] and h[n - k] overlap. Thus, summing x[k] h[n - k] for 0 ≤ n ≤ 5, we obtain

or

which is plotted in Fig. 2-25.
2.31. If x1[n] and x2[n] are both periodic sequences with common period N, the convolution of x1[n] and x2[n] does not converge. In this case, we define the periodic convolution of x1[n] and x2[n] as

Show that ƒ[n] is periodic with period N.
Since x2[n] is periodic with period N, we have

Then from Eq. (2.138) we have

Thus, ƒ[n] is periodic with period N.


Fig. 2-24



Fig. 2-25

2.32. The step response s[n] of a discrete-time LTI system is given by

Find the impulse response h[n] of the system.
From Eq. (2.41) the impulse response h[n] is given by

Properties of Discrete-Time LTI Systems
2.33. Show that if the input x[n] to a discrete-time LTI system is periodic with period N, then the output y[n] is also periodic with period N.
Let h[n] be the impulse response of the system. Then by Eq. (2.39) we have

Let n = m + N. Then

Since x[n] is periodic with period N, we have

Thus.

which indicates that the output y[n] is periodic with period N.
2.34. The impulse response h[n] of a discrete-time LTI system is shown in Fig. 2-26(a). Determine and sketch the output y[n] of this system to the input x[n] shown in Fig. 2-26(b) without using the convolution technique.
From Fig. 2-26(b) we can express x[n] as



Fig. 2-26

Since the system is linear and time-invariant and by the definition of the impulse response, we see that the output y[n] is given by

which is sketched in Fig. 2-27.


Fig. 2-27

2.35. A discrete-time system is causal if for every choice of n0 the value of the output sequence y[n ] at n = n0 depends on only the values of the input sequence x[n] for n < n0 (see Sec. 1.5D). From this definition derive the causality condition (2.44) for a discrete-time LTI system; that is,
h[n] = 0        n < 0
From Eq. (2.39) we have

Note that the first summation represents a weighted sum of future values of x[n]. Thus, if the system is causal, then

This can be true only if

Now if h[n] = 0 for n < 0, then Eq. (2.139) becomes

which indicates that the value of the output y[n] depends on only the past and the present input values.
2.36. Consider a discrete-time LTI system whose input x[n] and output y[n] are related by

Is the system causal?
By definition (2.30) and Eq. (1.48) the impulse response h[n] of the system is given by

By changing the variable k + 1 = m and by Eq. (1.50), we obtain

From Eq. (2.140) we have h[- 1] = u[0] = 1 ≠ 0. Thus, the system is not causal.
2.37. Verify the BIBO stability condition [Eq. (2.49)] for discrete-time LTI systems.
Assume that the input x[n] of a discrete-time LTI system is bounded, that is,

Then, using Eq. (2.35), we have

Since |x[n - k]| ≤ k1 from Eq. (2.141). Therefore, if the impulse response is absolutely summable, that is,

we have
|y[n]| ≤ k1 k = k2 < ∞
and the system is BIBO stable.
2.38. Consider a discrete-time LTI system with impulse response h[n] given by
h[n] = αnu[n]
(a) Is this system causal?
(b) Is this system BIBO stable?
(a) Since h[n] = 0 for n < 0, the system is causal.
(b) Using Eq. (1.91) (Prob. 1.19), we have

Therefore, the system is BIBO stable if |α| < 1 and unstable if |α| ≥ 1.
Systems Described by Difference Equations
2.39. The discrete-time system shown in Fig. 2-28 consists of one unit delay element and one scalar multiplier. Write a difference equation that relates the output y[n] and the input x[n].


Fig. 2-28

In Fig. 2-28 the output of the unit delay element is y [n - 1]. Thus, from Fig. 2-28 we see that

or

which is the required first-order linear difference equation.
2.40. The discrete-time system shown in Fig. 2-29 consists of two unit delay elements and two scalar multipliers. Write a difference equation that relates the output y[n] and the input x[n].


Fig. 2-29

In Fig. 2-29 the output of the first (from the right) unit delay element is y[n - 1] and the output of the second (from the right) unit delay element is y[n - 2]. Thus, from Fig. 2-29 we see that

or

which is the required second-order linear difference equation.
Note that, in general, the order of a discrete-time LTI system consisting of the interconnection of unit delay elements and scalar multipliers is equal to the number of unit delay elements in the system.
2.41. Consider the discrete-time system in Fig. 2-30. Write a difference equation that relates the output y[n] and the input x[n].


Fig. 2-30

Let the input to the unit delay element be q[n]. Then from Fig. 2-30 we see that

Solving Eqs. (2.146a) and (2.146b) for q[n] and q[n - 1] in terms of x[n] and y[n], we obtain

Changing n to (n - 1) in Eq. (2.147a), we have

Thus, equating Eq. (2.147b) and Eq. (2.147c), we have

Multiplying both sides of the above equation by 5 and rearranging terms, we obtain

which is the required difference equation.
2.42. Consider a discrete-time system whose input x[n] and output y[n] are related by

where a is a constant. Find y[n] with the auxiliary condition y[-1] = y-1 and

Let
y[n] = yp[n] + yh[n]
where yp[n] is the particular solution satisfying Eq. (2.149) and yh[n] is the homogeneous solution which satisfies

Assume that

Substituting Eq. (2.152) into Eq. (2.149), we obtain
Abn - aAbn-1 = Kbn
from which we obtain A = Kb/(b -a), and

To obtain yh[n], we assume
yh[n] = Bzn
Substituting this into Eq. (2.151) gives
Bzn - aBzn-1 = (z -a) Bzn-1 = 0
from which we have z = a and

Combining yp[n] and yh[n], we get

In order to determine B in Eq. (2.155) we need the value of y[0]. Setting n = 0 in Eqs. (2.149) and (2.150), we have

Setting n = 0 in Eq. (2.155), we obtain

or

Therefore, equating Eqs. (2.156) and (2.157), we have

from which we obtain

Hence, Eq. (2.155) becomes

For n < 0, we have x[n] = 0, and Eq. (2.149) becomes Eq. (2.151). Hence,

From the auxiliary condition y[-1] = y-1, we have
y[-1] = y-1 = Ba-1
from which we obtain B = y-1a. Thus,

Combining Eqs. (2.158) and (2.160), y[n] can be expressed as

Note that as in the continuous-time case (Probs. 2.21 and Probs. 2.22), the system described by Eq. (2.149) is not linear if y[-1] ≠ 0. The system is causal and time-invariant if it is initially at rest; that is, y[-1] ≠ 0. Note also that Eq. (2.149) can be solved recursively (see Prob. 2.43).
2.43. Consider the discrete-time system in Prob. 2.42. Find the output y[n] when x[n] = Kδ[n] and y[-1] = y-1 = α.
We can solve Eq. (2.149) for successive values of y[n] for n ≥ 0 as follows: rearrange Eq. (2.149) as

Then

Similarly, we can also determine y[n] for n < 0 by rearranging Eq. (2.149) as

Then

Combining Eqs. (2.163) and (2.165), we obtain

2.44. Consider the discrete-time system in Prob. 2.43 for an initially at rest condition.
(a) Find in impulse response h[n] of the system.
(b) Find the step response s[n] of the system.
(c) Find the impulse response h[n] from the result of part (b).
(a) Setting K = 1 and y[-1] = α = 0 in Eq. (2.166), we obtain

(b) Setting K = 1, b = 1, and y[-1] = y-1 = 0 in Eq. (2.161), we obtain

(c) From Eqs. (2.41) and (2.168) the impulse response h[n] is given by

When n = 0,

When n ≥ 1,

which is the same as Eq. (2.167).
2.45. Find the impulse response h[n] for each of the causal LTI discrete-time systems satisfying the following difference equations and indicate whether each system is a FIR or an IIR system.
(a) y[n] = x[n] - 2x[n - 2] + x[n - 3]
(b) y[n] + 2y[n - 1] = x[n] + x[n - 1]
(c) 
(a) By definition (2.56)

or

Since h[n] has only four terms, the system is a FIR system.
(b) h[n] = -2h[n - 1] + δ[n] + δ[n - 1]
Since the system is causal, h[- 1] = 0. Then

Since h[n] has infinite terms, the system is an IIR system.
(c) 
Since the system is causal, h[- 2] = h[- 1] = 0. Then

Since h[n] has only one term, the system is a FIR system.
SUPPLEMENTARY PROBLEMS
2.46. Compute the convolution y(t) = x(t) * h(t) of the following pair of signals:

2.47. Compute the convolution sum y[n] = x[n] * h[n] of the following pairs of sequences:

2.48. Show that if y(t) = x(t) * h(t), then
y'(t) = x'(t) * h(t) = x(t) * h'(t)
2.49. Show that
x(t) * δ'(t) = x'(t)
2.50. Let y[n] = x[n] * h[n]. Then show that
x[n - n1] * h[n - n2] = y[n - n1 - n2]
2.51. Show that

for an arbitrary starting point no.
2.52. The step response s(t) of a continuous-time LTI system is given by
s(t) = [cos ω0t]u(t)
Find the impulse response h(t) of the system.
2.53. The system shown in Fig. 2-31 is formed by connection two systems in parallel. The impulse responses of the systems are given by



Fig. 2-31

(a) Find the impulse response h(t) of the overall system.
(b) Is the overall system stable?
2.54. Consider an integrator whose input x(t) and output y(t) are related by

(a) Find the impulse response h(t) of the integrator.
(b) Is the integrator stable?
2.55. Consider a discrete-time LTI system with impulse response h[n] given by
h[n] = δ[n - 1]
Is this system memoryless?
2.56. The impulse response of a discrete-time LTI system is given by

Let y[n] be the output of the system with the input
x[n] = 2δ[n] + δ[n - 3]
Find y[1] and y[4].
2.57. Consider a discrete-time LTI system with impulse response h[n] given by

(a) Is the system causal?
(b) Is the system stable?
2.58. Consider the RLC circuit shown in Fig. 2-32. Find the differential equation relating the output current y(t) and the input voltage x(t).


Fig. 2-32

2.59. Consider the RL circuit shown in Fig. 2-33.
(a) Find the differential equation relating the output voltage y(t) across R and the the input voltage x(t).
(b) Find the impulse response h(t) of the circuit.
(c) Find the step response s(t) of the circuit.


Fig. 2-33

2.60. Consider the system in Prob. 2.20. Find the output y(t) if x(t) = e-atu(t) and y (0) = 0.
2.61. Is the system described by the differential equation

linear?
2.62. Write the input-output equation for the system shown in Fig. 2-34.


Fig. 2-34

2.63. Consider a discrete-time LTI system with impulse response

Find the input-output relationship of the system.
2.64. Consider a discrete-time system whose input x[n] and output y[n] are related by

with y[-1] = 0. Find the output y[n] for the following inputs:
(a) 
(b) 
2.65. Consider the system in Prob. 2.42. Find the eigenfunction and the corresponding eigenvalue of the system.
ANSWERS TO SUPPLEMENTARY PROBLEMS
2.46. (a) 
(b) 
(c) 
2.47. (a) 
(b) 
(c) y[n] = δ[n]
2.48. Hint: Differentiate Eqs. (2.6) and (2.10) with respect to t.
2.49. Hint: Use the result from Prob. 2.48 and Eq. (2.58).
2.50. Hint: See Prob. 2.3.
2.51. Hint: See Probs. 2.31 and Probs. 2.8.
2.52. h(t) = δ(t) - ω0[sin ω0t]u(t)
2.53. (a) h(t) = (e-2t + 2e-t) u(t)
(b) Yes
2.54. (a) h(t) = u(t)
(b) No
2.55. No, the system has memory.
2.56. 
2.57. (a) Yes; (b) Yes
2.58. 
2.59. (a) 
(b) 
(c) 
2.60. te-at u(t)
2.61. No, it is nonlinear.
2.62. 2y[n] - y[n - 1] = 4x[n] + 2x[n - 1]
2.63. y[n] = x[n] + x[n - 1]
2.64. (a) (b) 
2.65. 







CHAPTER 3Laplace Transform and Continuous-Time LTI Systems
3.1 Introduction
A basic result from Chap. 2 is that the response of an LTI system is given by convolution of the input and the impulse response of the system. In this chapter and the following one we present an alternative representation for signals and LTI systems. In this chapter, the Laplace transform is introduced to represent continuous-time signals in the s-domain (s is a complex variable), and the concept of the system function for a continuous-time LTI system is described. Many useful insights into the properties of continuous-time LTI systems, as well as the study of many problems involving LTI systems, can be provided by application of the Laplace transform technique.
3.2 The Laplace Transform
In Sec. 2.4 we saw that for a continuous-time LTI system with impulse response h(t), the output y(t) of the system to the complex exponential input of the form est is

where

A. Definition:
The function H(s) in Eq. (3.2) is referred to as the Laplace transform of h(t). For a general continuous-time signal x(t), the Laplace transform X(s) is defined as

The variable s is generally complex-valued and is expressed as

The Laplace transform defined in Eq. (3.3) is often called the bilateral (or two-sided) Laplace transform in contrast to the unilateral (or one-sided) Laplace transform, which is defined as

where 0- = limε→ 0(0 - ε). Clearly the bilateral and unilateral transforms are equivalent only if x(t) = 0 for t < 0. The unilateral Laplace transform is discussed in Sec. 3.8. We will omit the word "bilateral" except where it is needed to avoid ambiguity.
Equation (3.3) is sometimes considered an operator that transforms a signal x(t) into a function X(s) symbolically represented by

and the signal x(t) and its Laplace transform X(s) are said to form a Laplace transform pair denoted as

B. The Region of Convergence:
The range of values of the complex variables s for which the Laplace transform converges is called the region of convergence (ROC). To illustrate the Laplace transform and the associated ROC, let us consider some examples.
EXAMPLE 3.1 Consider the signal

Then by Eq. (3.3) the Laplace transform of x(t) is

because limt → ∞ e-(s + a)t = 0 only if Re(s + a) > 0 or Re(s) > -a.
Thus, the ROC for this example is specified in Eq. (3.9) as Re(s) > -a and is displayed in the complex plane as shown in Fig. 3-1 by the shaded area to the right of the line Re(s) = -a. In Laplace transform applications, the complex plane is commonly referred to as the s-plane. The horizontal and vertical axes are sometimes referred to as the σ-axis and the jω-axis, respectively.


Fig. 3-1 ROC for Example 3.1.

EXAMPLE 3.2 Consider the signal

Its Laplace transform X(s) is given by (Prob. 3.1)

Thus, the ROC for this example is specified in Eq. (3.11) as Re(s) < -a and is displayed in the complex plane as shown in Fig. 3-2 by the shaded area to the left of the line Re(s) = -a. Comparing Eqs. (3.9) and (3.11), we see that the algebraic expressions for X(s) for these two different signals are identical except for the ROCs. Therefore, in order for the Laplace transform to be unique for each signal x(t), the ROC must be specified as part of the transform.


Fig. 3-2 ROC for Example 3.2.

C. Poles and Zeros of X(s):
Usually, X(s) will be a rational function in s; that is,

The coefficients ak and bk are real constants, and m and n are positive integers. The X(s) is called a proper rational function if n > m, and an improper rational function if n ≤ m. The roots of the numerator polynomial, zk, are called the zeros of X(s) because X(s) = 0 for those values of s. Similarly, the roots of the denominator polynomial, pk, are called the poles of X(s) because X(s) is infinite for those values of s. Therefore, the poles of X(s) lie outside the ROC since X(s) does not converge at the poles, by definition. The zeros, on the other hand, may lie inside or outside the ROC. Except for a scale factor a0/b0, X(s) can be completely specified by its zeros and poles. Thus, a very compact representation of X(s) in the s-plane is to show the locations of poles and zeros in addition to the ROC.
Traditionally, an "×" is used to indicate each pole location and an "o" is used to indicate each zero. This is illustrated in Fig. 3-3 for X(s) given by

Note that X(s) has one zero at s = -2 and two poles at s = -1 and s = -3 with scale factor 2.


Fig. 3-3 s-plane representation of X(s) = (2s + 4)/(s2 + 4s + 3).

D. Properties of the ROC:
As we saw in Examples 3.1 and 3.2, the ROC of X(s) depends on the nature of x(t). The properties of the ROC are summarized below. We assume that X(s) is a rational function of s.
Property 1: The ROC does not contain any poles.
Property 2: If x(t) is a finite-duration signal, that is, x(t) = 0 except in a finite interval t1 ≤ t ≤ t2 (-∞ < t1 and t2 < ∞) then the ROC is the entire s-plane except possibly s = 0 or s = ∞.
Property 3: If x(t) is a right-sided signal, that is, x(t) = 0 for t < t1 < ∞, then the ROC is of the form
Re (s) > σmax
where σmax equals the maximum real part of any of the poles of X(s). Thus, the ROC is a half-max plane to the right of the vertical line Re(s) = σmax in the s-plane and thus to the right of all of the poles of X(s).
Property 4: If x(t) is a left-sided signal, that is, x(t) = 0 for t > t2 > - ∞ then the ROC is of the form
Re(s) < σmin
where σmin equals the minimum real part of any of the poles of X(s). Thus, the ROC is a half-plane to the left of the vertical line Re(s) = σmin in the s-plane and thus to the left of all of the poles of X(s).
Property 5: If x(t) is a two-sided signal, that is, x(t) is an infinite-duration signal that is neither right-sided nor left-sided, then the ROC is of the form
σ1 < Re(s) < σ2
where σ1 and σ2 are the real parts of the two poles of X(s). Thus, the ROC is a vertical strip in the s-plane between the vertical lines Re(s) = σ1 and Re(s) = σ2.
Note that Property 1 follows immediately from the definition of poles; that is, X(s) is infinite at a pole. For verification of the other properties see Probs. 3.2 to Probs. 3.7.
3.3 Laplace Transforms of Some Common Signals
A. Unit Impulse Function δ(t):
Using Eqs. (3.3) and (1.20), we obtain

B. Unit Step Function u(t):

where 0+ = limε→0(0 + ε).
C. Laplace Transform Pairs for Common Signals:
The Laplace transforms of some common signals are tabulated in Table 3-1. Instead of having to reevaluate the transform of a given signal, we can simply refer to such a table and read out the desired transform.

TABLE 3-1 Some Laplace Transforms Pairs


3.4 Properties of the Laplace Transform
Bawented in the following. Verification of these properties is given in Probs. 3.8 to Probs. 3.16.
A. Linearity:
If

Then
The set notation A ⊃ B means that set A contains set B, while A ∩ B denotes the intersection of sets A and B, that is, the set containing all elements in both A and B. Thus, Eq. (3.15) indicates that the ROC of the resultant Laplace transform is at least as large as the region in common between R1 and R2. Usually we have simply R′ = R1 ∩ R2. This is illustrated in Fig. 3-4.


Fig. 3-4 ROC of a1X1(s) + a2X2(s).

B. Time Shifting:
If

Equation (3.16) indicates that the ROCs before and after the time-shift operation are the same.
C. Shifting in the s-Domain:
If

Equation (3.17) indicates that the ROC associated with X(s-s0) is that of X(s) shifted by Re(s0). This is illustrated in Fig. 3-5.


Fig. 3-5 Effect on the ROC of shifting in the s-domain. (a) ROC of X(s); (b) ROC of X(s - s0).

D. Time Scaling:
If

Equation (3.18) indicates that scaling the time variable t by the factor a causes an inverse scaling of the variable s by 1/a as well as an amplitude scaling of X(s/a) by 1/|a|. The corresponding effect on the ROC is illustrated in Fig. 3-6.


Fig. 3-6 Effect on the ROC of time scaling. (a) ROC of X(s); (b) ROC of X(s/a).

E. Time Reversal:
If

Thus, time reversal of x(t) produces a reversal of both the σ- and jω-axes in the s-plane. Equation (3.19) is readily obtained by setting a = -1 in Eq. (3.18).
F. Differentiation in the Time Domain:
If

Equation (3.20) shows that the effect of differentiation in the time domain is multiplication of the corresponding Laplace transform by s. The associated ROC is unchanged unless there is a pole-zero cancellation at s = 0.
G. Differentiation in the s-Domain:
If

H. Integration in the Time Domain:
If

Equation (3.22) shows that the Laplace transform operation corresponding to time-domain integration is multiplication by 1/s, and this is expected since integration is the inverse operation of differentiation. The form of R′ follows from the possible introduction of an additional pole at s = 0 by the multiplication by 1/s.
I. Convolution:
If

This convolution property plays a central role in the analysis and design of continuous-time LTI systems.
Table 3-2 summarizes the properties of the Laplace transform presented in this section.

TABLE 3-2 Properties of the Laplace Transform


3.5 The Inverse Laplace TransformInversion
Inversion of the Laplace transform to find the signal x(t) from its Laplace transform X(s) is called the inverse Laplace transform, symbolically denoted as

A. Inversion Formula:
There is a procedure that is applicable to all classes of transform functions that involves the evaluation of a line integral in complex s-plane; that is,

In this integral, the real c is to be selected such that if the ROC of X(s) is σ1 < Re(s) < σ2, then σ1 < c < σ2. The evaluation of this inverse Laplace transform integral requires understanding of complex variable theory.
B. Use of Tables of Laplace Transform Pairs:
In the second method for the inversion of X(s), we attempt to express X(s) as a sum

where X1(s), ..., Xn(s) are functions with known inverse transforms x1(t), ..., xn(t). From the linearity property (3.15) it follows that

C. Partial-Fraction Expansion:
If X(s) is a rational function, that is, of the form

a simple technique based on partial-fraction expansion can be used for the inversion of X(s).
(a)   When X(s) is a proper rational function, that is, when m < n:
1. Simple Pole Case:
If all poles of X(s), that is, all zeros of D(s), are simple (or distinct), then X(s) can be written as

where coefficients ck are given by

2. Multiple Pole Case:
If D(s) has multiple roots, that is, if it contains factors of the form (s - pi)r, we say that pi is the multiple pole of X(s) with multiplicity r. Then the expansion of X(s) will consist of terms of the form

where 
(b)   When X(s) is an improper rational function, that is, when m ≥ n:
If m ≥ n, by long division we can write X(s) in the form

where N(s) and D(s) are the numerator and denominator polynomials in s, respectively, of X(s), the quotient Q(s) is a polynomial in s with degree m - n, and the remainder R(s) is a polynomial in s with degree strictly less than n. The inverse Laplace transform of X(s) can then be computed by determining the inverse Laplace transform of Q(s) and the inverse Laplace transform of R(s)/D(s). Since R(s)/D(s) is proper, the inverse Laplace transform of R(s)/D(s) can be computed by first expanding into partial fractions as given above. The inverse Laplace transform of Q(s) can be computed by using the transform pair

3.6 The System Function A. The System Function:
A. The System Function:
In Sec. 2.2 we showed that the output y(t) of a continuous-time LTI system equals the convolution of the input x(t) with the impulse response h(t); that is,

Applying the convolution property (3.23), we obtain

where Y(s), X(s), and H(s) are the Laplace transforms of y(t), x(t), and h(t), respectively. Equation (3.36) can be expressed as

The Laplace transform H(s) of h(t) is referred to as the system function (or the transfer function) of the system. By Eq. (3.37), the system function H(s) can also be defined as the ratio of the Laplace transforms of the output y(t) and the input x(t). The system function H(s) completely characterizes the system because the impulse response h(t) completely characterizes the system. Fig. 3-7 illustrates the relationship of Eqs. (3.35) and (3.36).


Fig. 3-7 Impulse response and system function.

B. Characterization of LTI Systems:
Many properties of continuous-time LTI systems can be closely associated with the characteristics of H(s) in the s-plane and in particular with the pole locations and the ROC.
1. Causality:
For a causal continuous-time LTI system, we have
h(t) = 0     t < 0
Since h(t) is a right-sided signal, the corresponding requirement on H(s) is that the ROC of H(s) must be of the form
Re(s) > σmax
That is, the ROC is the region in the s-plane to the right of all of the system poles. Similarly, if the system is anticausal, then
h(t) = 0     t > 0
and h(t) is left-sided. Thus, the ROC of H(s) must be of the form
Re(s) < σmin
That is, the ROC is the region in the s-plane to the left of all of the system poles.
2. Stability:
In Sec. 2.3 we stated that a continuous-time LTI system is BIBO stable if and only if [Eq. (2.21)]

The corresponding requirement on H(s) is that the ROC of H(s) contains the jω-axis (that is, s = jω) (Prob. 3.26).
3. Causal and Stable Systems:
If the system is both causal and stable, then all the poles of H(s) must lie in the left half of the s-plane; that is, they all have negative real parts because the ROC is of the form Re(s) > σmax, and since the jω axis is included in the ROC, we must have σmax < 0.
C. System Function for LTI Systems Described by Linear Constant-Coefficient Differential Equations:
In Sec. 2.5 we considered a continuous-time LTI system for which input x(t) and output y(t) satisfy the general linear constant-coefficient differential equation of the form

Applying the Laplace transform and using the differentiation property (3.20) of the Laplace transform, we obtain

or

Thus,

Hence, H(s) is always rational. Note that the ROC of H(s) is not specified by Eq. (3.40) but must be inferred with additional requirements on the system such as the causality or the stability.
D. Systems Interconnection:
For two LTI systems [with h1(t) and h2(t), respectively] in cascade [Fig. 3-8(a)], the overall impulse response h(t) is given by [Eq. (2.81), Prob. 2.14]
h(t) = h1(t) * h2(t)
Thus, the corresponding system functions are related by the product

This relationship is illustrated in Fig. 3-8(b).


Fig. 3-8 Two systems in cascade. (a) Time-domain representation; (b) s-domain representation.

Similarly, the impulse response of a parallel combination of two LTI systems [Fig. 3-9(a)] is given by (Prob. 2.53)
h(t) = h1(t) + h2(t)
Thus,

This relationship is illustrated in Fig. 3-9(b).


Fig. 3-9 Two systems in parallel. (a) Time-domain representation; (b) s-domain representation.

3.7 The Unilateral Laplace Transform A. Definitions:
The unilateral (or one-sided) Laplace transform XI(s) of a signal x(t) is defined as [Eq. (3.5)]

The lower limit of integration is chosen to be 0- (rather than 0 or 0+) to permit x(t) to include δ(t) or its derivatives. Thus, we note immediately that the integration from 0- to 0+ is zero except when there is an impulse function or its derivative at the origin. The unilateral Laplace transform ignores x(t) for t < 0. Since x(t) in Eq. (3.43) is a right-sided signal, the ROC of X1(s) is always of the form Re(s) > σmax, that is, a right half-plane in the s-plane.
B. Basic Properties:
Most of the properties of the unilateral Laplace transform are the same as for the bilateral transform. The unilateral Laplace transform is useful for calculating the response of a causal system to a causal input when the system is described by a linear constant-coefficient differential equation with nonzero initial conditions. The basic properties of the unilateral Laplace transform that are useful in this application are the time-differentiation and time-integration properties which are different from those of the bilateral transform. They are presented in the following.
1. Differentiation in the Time Domain:

provided that limt→∞ x(t)est- = 0. Repeated application of this property yields

Where

2. Integration in the Time Domain:

C. System Function:
Note that with the unilateral Laplace transform, the system function H(s) = Y(s)/X(s) is defined under the condition that the LTI system is relaxed, that is, all initial conditions are zero.
D. Transform Circuits:
The solution for signals in an electric circuit can be found without writing integrodifferential equations if the circuit operations and signals are represented with their Laplace transform equivalents. [In this subsection the Laplace transform means the unilateral Laplace transform and we drop the subscript I in XI(s).] We refer to a circuit produced from these equivalents as a transform circuit. In order to use this technique, we require the Laplace transform models for individual circuit elements. These models are developed in the following discussion and are shown in Fig. 3-10. Applications of this transform model technique to electric circuits problems are illustrated in Probs. 3.40 to Probs. 3.42.
1. Signal Sources:

where v(t) and i(t) are the voltage and current source signals, respectively.
2. Resistance R:

3. Inductance L:

The second model of the inductance L in Fig. 3-10 is obtained by rewriting Eq. (3.50) as

4. Capacitance C:

The second model of the capacitance C in Fig. 3-10 is obtained by rewriting Eq. (3.52) as



Fig. 3-10 Representation of Laplace transform circuit-element models.

SOLVED PROBLEMS
Laplace Transform
3.1. Find the Laplace transform of
(a) x(t) = -e-atu(-t)
(b) x(t) = eatu(-t)
(a) From Eq. (3.3)

(b) Thus, we obtain

(b) Similarly,

Thus, we obtain

3.2. A finite-duration signal x(t) is defined as

where t1 and t2 are finite values. Show that if X(s) converges for at least one value of s, then the ROC of X(s) is the entire s-plane.
Assume that X(s) converges at s = σ0; then by Eq. (3.3)

Let Re(s) = σ1 > σ0. Then

Since (σ1 - σ0) > 0, e-(σ1 - σ0)t is a decaying exponential. Then over the interval where x(t) ≠ 0, the maximum value of this exponential is e-(σ1 - σ0)t1, and we can write

Thus, X(s) converges for Re(s) = σ1 > σ0. By a similar argument, if σ1 < σ0, then

and again X(s) converges for Re(s) = σ1 < σ0. Thus, the ROC of X(s) includes the entire s-plane.
3.3. Let

Find the Laplace transform of x(t).
By Eq. (3.3)

Since x(t) is a finite-duration signal, the ROC of X(s) is the entire s-plane. Note that from Eq. (3.58) it appears that X(s) does not converge at s = -a. But this is not the case. Setting s = -a in the integral in Eq. (3.58), we have

The same result can be obtained by applying L'Hospital's rule to Eq. (3.58).
3.4. Show that if x(t) is a right-sided signal and X(s) converges for some value of s, then the ROC of X(s) is of the form
Re(s) > σmax
where σmax equals the maximum real part of any of the poles of X(s).
Consider a right-sided signal x(t) so that

and X(s) converges for Re(s) = σ0. Then

Let Re(s) = σ1 > σ0. Then

Thus, X(s) converges for Re(s) = σ1 and the ROC of X(s) is of the form Re(s) > σ0. Since the ROC of X(s) cannot include any poles of X(s), we conclude that it is of the form
Re(s) > σmax
where σmax equals the maximum real part of any of the poles of X(s).
3.5. Find the Laplace transform X(s) and sketch the pole-zero plot with the ROC for the following signals x(t):
(a) x(t) = e-2tu(t) + e-3tu(t)
(b) x(t) = e-3tu(t) + e2tu(-t)
(c) x(t) = e2tu(t) + e-3tu(-t)
(a) From Table 3-1

We see that the ROCs in Eqs. (3.59) and (3.60) overlap, and thus,

From Eq. (3.61) we see that X(s) has one zero at  and two poles at s = -2 and s = - 3 and that the ROC is Re(s) > -2, as sketched in Fig. 3-11(a).
(b) From Table 3-1

We see that the ROCs in Eqs. (3.62) and (3.63) overlap, and thus,

From Eq. (3.64) we see that X(s) has no zeros and two poles at s = 2 and s = - 3 and that the ROC is - 3 < Re(s) < 2, as sketched in Fig. 3-11(b).
(c) From Table 3-1

We see that the ROCs in Eqs. (3.65) and (3.66) do not overlap and that there is no common ROC; thus, x(t) has no transform X(s).


Fig. 3-11

3.6. Let
x(t) = e-a |t |
Find X(s) and sketch the zero-pole plot and the ROC for a > 0 and a < 0.
The signal x(t) is sketched in Figs. 3-12 (a) and (b) for both a > 0 and a < 0. Since x(t) is a two-sided signal, we can express it as

Note that x(t) is continuous at t = 0 and x(0-) = x(0) = x(0+) = 1. From Table 3-1

If a > 0, we see that the ROCs in Eqs. (3.68) and (3.69) overlap, and thus,

From Eq. (3.70) we see that X(s) has no zeros and two poles at s = a and s = -a and that the ROC is -a < Re(s) < a, as sketched in Fig. 3-12(c). If a < 0, we see that the ROCs in Eqs. (3.68) and (3.69) do not overlap and that there is no common ROC; thus, x(t) has no transform X(s).


(c) Fig. 3-12

Properties of the Laplace Transform
3.7. Verify the time-shifting property (3.16); that is,

By definition (3.3)

By the change of variables τ = t - t0 we obtain

with the same ROC as for X(s) itself. Hence,

where R and R′ are the ROCs before and after the time-shift operation.
3.8. Verify the time-scaling property (3.18); that is,

By definition (3.3)

By the change of variables τ = at with a > 0, we have

Note that because of the scaling s/a in the transform, the ROC of X (s/a) is aR. With a < 0, we have

Thus, combining the two results for a > 0 and a < 0, we can write these relationships as

3.9. Find the Laplace transform and the associated ROC for each of the following signals:
(a) x(t) = δ(t - t0)
(b) x(t) = u(t - t0)
(c) x(t) = e-2t [u(t) - u(t - 5)]

(e) x(t) = δ(at + b), a, b real constants
(a) Using Eqs. (3.13) and (3.16), we obtain

(b) Using Eqs. (3.14) and (3.16), we obtain

(c) Rewriting x(t) as

Then, from Table 3-1 and using Eq. (3.16), we obtain

(d) Using Eqs. (3.71) and (1.99), we obtain

(e) Let
f(t) = δ(at)
Then from Eqs. (3.13) and (3.18) we have

Now

(d) Using Eqs. (3.16) and (3.74), we obtain

3.10. Verify the time differentiation property (3.20); that is,

From Eq. (3.24) the inverse Laplace transform is given by

Differentiating both sides of the above expression with respect to t, we obtain

Comparing Eq. (3.77) with Eq. (3.76), we conclude that dx(t)/dt is the inverse Laplace transform of sX(s). Thus,

Note that the associated ROC is unchanged unless a pole-zero cancellation exists at s = 0.
3.11. Verify the differentiation in s property (3.21); that is,

From definition (3.3)

Differentiating both sides of the above expression with respect to s, we have

Thus, we conclude that

3.12. Verify the integration property (3.22); that is,

Let

Then

Applying the differentiation property (3.20), we obtain
X(s) = sF(s)
Thus,

The form of the ROC R′ follows from the possible introduction of an additional pole at s = 0 by the multiplying by 1/s.
3.13. Using the various Laplace transform properties, derive the Laplace transforms of the following signals from the Laplace transform of u(t).
(a) δ(t)     (b) δ′(t)
(c) tu(t)     (d) e-atu(t)
(e) te-atu(t)     (f) cos (ω0tu(t)
(g) e-at cos ω0tu(t)
(a) From Eq. (3.14) we have

From Eq. (1.30) we have

Thus, using the time-differentiation property (3.20), we obtain

(b) Again applying the time-differentiation property (3.20) to the result from part (a), we obtain

(c) Using the differentiation in s property (3.21), we obtain

(d) Using the shifting in the s-domain property (3.17), we have

(e) From the result from part (c) and using the differentiation in s property (3.21), we obtain

(f) From Euler's formula we can write

Using the linearity property (3.15) and the shifting in the s-domain property (3.17), we obtain

(g) Applying the shifting in the s-domain property (3.17) to the result from part (f), we obtain

3.14. Verify the convolution property (3.23); that is,

Let

Then, by definition (3.3)

Noting that the bracketed term in the last expression is the Laplace transform of the shifted signal x2(t - τ), by Eq. (3.16) we have

with an ROC that contains the intersection of the ROC of X1(s) and X2(s). If a zero of one transform cancels a pole of the other, the ROC of Y(s) may be larger. Thus, we conclude that

3.15 Using the convolution property (3.23), verify Eq. (3.22); that is,

We can write [Eq. (2.60), Prob. 2.2]

From Eq. (3.14)

and thus, from the convolution property (3.23) we obtain

with the ROC that includes the intersection of the ROC of X(s) and the ROC of the Laplace transform of u(t). Thus,

Inverse Laplace Transform
3.16. Find the inverse Laplace transform of the following X(s):




(a) From Table 3-1 we obtain
x(t) = e-t u(t)
(b) From Table 3-1 we obtain
x(t) = -e-t u(-t)
(c) From Table 3-1 we obtain
x(t) = cos 2tu(t)
(d) From Table 3-1 we obtain
s(t) = e-t cos 2tu(t)
3.17. Find the inverse Laplace transform of the following X(s):



Expanding by partial fractions, we have

Using Eq. (3.30), we obtain

Hence,

(a) The ROC of X(s) is Re(s) > - 1. Thus, x(t) is a right-sided signal and from Table 3-1 we obtain
x(t) = e-tu(t) + e-3tu(t) = (e-t + e-3t)u(t)
(b) The ROC of X(s) is Re(s) < -3. Thus, x(t) is a left-sided signal and from Table 3-1 we obtain
x(t) = -e-tu(-t) - e-3tu(-t) = -(e-t + e-3t)u(-t)
(c) The ROC of X(s) is - 3 < Re(s) < - 1. Thus, x(t) is a double-sided signal and from Table 3-1 we obtain
x(t) = -e-tu(-t) + e-3tu(t)
3.18. Find the inverse Laplace transform of

We can write
s2 + 4s + 13 = (s + 2)2 + 9 = (s + 2 -(s + 2 + j3) j3)
Then

where

Thus,

The ROC of X(s) is Re(s) > 0. Thus, x(t) is a right-sided signal and from Table 3-1 we obtain

Inserting the identity

into the above expression, after simple computations we obtain
x(t) = u(t) - e-2t(cos 3t - sin3t) u(t)
   = [1 - e-2t(cos 3t - sin 3t] u(t)
Alternate Solution:
We can write X(s) as

As before, by Eq. (3.30) we obtain

Then we have

Thus,

Then from Table 3-1 we obtain
       x(t) = u(t) - e-2t(cos 3tu(t) + e-2t sin 3tu(t)
= [1 - e-2t(cos 3t - sin 3t] u(t)
3.19. Find the inverse Laplace transform of

We see that X(s) has one simple pole at s = - 3 and one multiple pole at s = -5 with multiplicity 2. Then by Eqs. (3.29) and (3.31) we have

By Eqs. (3.30) and (3.32) we have

Hence,

The ROC of X(s) is Re(s) > - 3. Thus, x(t) is a right-sided signal and from Table 3-1 we obtain
x(t) = 2e-3t u(t) - e-5t u(t) - 10 te-5t u(t)
= [2e-3t - e-5t u(t) - 10 te-5t u(t)
Note that there is a simpler way of finding λ1 without resorting to differentiation. This is shown as follows: First find c1 and λ2 according to the regular procedure. Then substituting the values of c1 and λ2 into Eq. (3.84), we obtain

Setting s = 0 on both sides of the above expression, we have

from which we obtain λ1 = -1.
3.20. Find the inverse Laplace transform of the following X(s):




Since the ROC of X(s) is Re(s) > - 2, x(t) is a right-sided signal and from Table 3-1 we obtain
x(t) = 2δ(t) - 3e-2tu(t)
(b) Performing long division, we have

Let

where

Hence,

The ROC of X(s) is Re(s) > -1. Thus, x(t) is a right-sided signal and from Table 3-1 we obtain
x(t) = δ(t) + (2e-t + e-2t)u(t)
(c) Proceeding similarly, we obtain

Let

where

Hence,

The ROC of X(s) is Re(s) > 0. Thus, x(t) is a right-sided signal and from Table 3-1 and Eq. (3.78) we obtain
x(t) = δ′(t) - δ(t) + (2 + e-3t)u(t)
Note that all X(s) in this problem are improper fractions and that x(t) contains <(t) or its derivatives.
3.21. Find the inverse Laplace transform of

We see that X(s) is a sum
x(s) = X1(s) + X2(s) e-2s + X3(s)e-4s
where

If

then by the linearity property (3.15) and the time-shifting property (3.16) we obtain

Next, using partial-fraction expansions and from Table 3-1, we obtain

Thus, by Eq. (3.85) we have
x(t) = (e-t - e-3t)u(t) + [- e-(t - 2) + 3e-3(t - 2)]u(t - 2)
+ 2[e-(t - 4) - e-3(t - 4)]u(t - 4)
3.22. Using the differentiation in s property (3.21), find the inverse Laplace transform of

We have

and from Eq. (3.9) we have

Thus, using the differentiation in s property (3.21), we obtain
x(t) = te-atu(t)
System Function
3.23 Find the system function H(s) and the impulse response h(t) of the RC circuit in Fig. 1-32 (Prob. 1.32).
(a) Let

In this case, the RC circuit is described by [Eq. (1.105)]

Taking the Laplace transform of the above equation, we obtain

or

Hence, by Eq. (3.37) the system function H(s) is

Since the system is causal, taking the inverse Laplace transform of H(s), the impulse response hit) is

(b) Let

In this case, the RC circuit is described by [Eq. (1.107)]

Taking the Laplace transform of the above equation, we have

or

Hence, the system function H(s) is

In this case, the system function H(s) is an improper fraction and can be rewritten as

Since the system is causal, taking the inverse Laplace transform of H(s), the impulse response h(t) is

Note that we obtained different system functions depending on the different sets of input and output.
3.24. Using the Laplace transform, redo Prob. 2.5.
From Prob. 2.5 we have

Using Table 3-1, we have

Thus,

and from Table 3-1 (or Prob. 3.6) the output is

which is the same as Eq. (2.67).
3.25. The output y(t) of a continuous-time LTI system is found to be 2e-3tu(t) when the input x(t) is u(t).
(a) Find the impulse response h(t) of the system.
(b) Find the output y(t) when the input x(t) is e-tu(t).

Taking the Laplace transforms of x(t) and y(t), we obtain

Hence, the system function H(s) is

Rewriting H(s) as

and taking the inverse Laplace transform of H(s), we have
h(t) = 2δ(t) - 6e-3tu(t)
Note that h(t) is equal to the derivative of 2e-3tu(t), which is the step response s(t) of the system [see Eq. (2.13)].
(b) 
Thus,

Using partial-fraction expansions, we get

Taking the inverse Laplace transform of Y(s), we obtain
y(t) = (-e-t + 3e-3t) u(t)
3.26. If a continuous-time LTI system is BIBO stable, then show that the ROC of its system function H(s) must contain the imaginary axis; that is, s = jω.
A continuous-time LTI system is BIBO stable if and only if its impulse response h(t) is absolutely integrable, that is [Eq. (2.21)],

By Eq. (3.3)

Let s = jω. Then

Therefore, we see that if the system is stable, then H(s) converges for s = jω. That is, for a stable continuous-time LTI system, the ROC of H(s) must contain the imaginary axis s = jω.
3.27 Using the Laplace transfer, redo Prob. 2.14
(a) Using Eqs. (3.36) and (3.41), we have
Y(s) = X(s)H1(s)H2(s) = X(s)H(s)
where H(s) = H1(s)H2(s) is the system function of the overall system. Now from Table 3-1 we have

Hence,

Taking the inverse Laplace transfer of H(s), we get
h(t) = 2(e-t - e-2t)u(t)
(b) Since the ROC of H(s), Re(s) > - 1, contains the jω-axis, the overall system is stable.
3.28. Using the Laplace transform, redo Prob. 2.23.
The system is described by

Taking the Laplace transform of the above equation, we obtain

Hence, the system function H(s) is

Assuming the system is causal and taking the inverse Laplace transform of H(s), the impulse response h(t) is
h(t) = e-atu(t)
which is the same as Eq. (2.124).
3.29. Using the Laplace transform, redo Prob. 2.25.
The system is described by
y′(t) + 2y(t) = x(t) + x′(t)
Taking the Laplace transform of the above equation, we get
sY(s) + 2Y(s) = X(s) + sX(s)
or
(s + 2)Y(s) = (s + l)X(s)
Hence, the system function H(s) is

Assuming the system is causal and taking the inverse Laplace transform of H(s), the impulse response hit) is
h(t) = δ(t) - e-2tu(t)
3.30. Consider a continuous-time LTI system for which the input x(t) and output y(t) are related by

(a) Find the system function H(s).
(b) Determine the impulse response h(t) for each of the following three cases: (i) the system is causal, (ii) the system is stable, (iii) the system is neither causal nor stable.
(a) Taking the Laplace transform of Eq. (3.86), we have
s2Y(s) + sY(s) - 2Y(s) = X(s)
or
(s2 + s - 2)Y(s) = X(s)
Hence, the system function H(s) is

(b) Using partial-fraction expansions, we get

(i) If the system is causal, then h(t) is causal (that is, a right-sided signal) and the ROC of H(s) is Re(s) > 1. Then from Table 3-1 we get

(ii) If the system is stable, then the ROC of H(s) must contain the jw-axis. Consequently the ROC of H(s) is -2 < Re(s) < 1. Thus, h(t) is two-sided and from Table 3-1 we get

(iii) If the system is neither causal nor stable, then the ROC of H(s) is Re(s) < -2. Then h(t) is noncausal (that is, a left-sided signal) and from Table 3-1 we get

3.31. The feedback interconnection of two causal subsystems with system functions F(s) and G(s) is depicted in Fig. 3-13. Find the overall system function H(s) for this feedback system.


Fig. 3-13 Feedback system.

Let

Then,

since
e(t) = x(t) + r(t)
we have

Substituting Eq. (3.88) into Eq. (3.89) and then substituting the result into Eq. (3.87), we obtain
Y(s) = [X(s) + Y(s)G(s)] F(s)
or
[1 - F(s)G(s)] Y(s) = F(s)X(s)
Thus, the overall system function is

Unilateral Laplace Transform
3.32. Verify Eqs. (3.44) and (3.45); that is,


(a) Using Eq. (3.43) and integrating by parts, we obtain

Thus, we have

(b) Applying the above property to signal x′(t) = dx(t)/dt, we obtain

Note that Eq. (3.46) can be obtained by continued application of the above procedure.
3.33. Verify Eqs. (3.47) and (3.48); that is,



(b) We can write

Note that the first term on the right-hand side is a constant. Thus, taking the unilateral Laplace transform of the above equation and using Eq. (3.47), we get

3.34. (a) Show that the bilateral Laplace transform of x(t) can be computed from two unilateral Laplace transforms.
(b) Using the result obtained in part (a), find the bilateral Laplace transform of e-2|t|.
(a) The bilateral Laplace transform of x(t) defined in Eq. (3.3) can be expressed as


Next, let

Then

Thus, substituting Eqs. (3.92) and (3.94) into Eq. (3.91), we obtain

(b)
x(t) = e-2 |t|
(1) x(t) = e-2t for t > 0, which gives

(2) x(t) = e2t for t < 0. Then x(- t) = e-2t for t > 0, which gives

Thus,

(3) According to Eq. (3.95), we have

which is equal to Eq. (3.70), with a = 2, in Prob. 3.6.
3.35. Show that

Equation (3.97) is called the initial value theorem, while Eq. (3.98) is called the final value theorem for the unilateral Laplace transform.
(a) Using Eq. (3.44), we have

Thus,

since lim s→∞ e-st = 0.
(b) Again using Eq. (3.44), we have

Since

we conclude that

3.36. The unilateral Laplace transform is sometimes defined as

with 0+ as the lower limit. (This definition is sometimes referred to as the 0+ definition.)
(a) Show that

(b) Show that

(a) Let x(t) have unilateral Laplace transform (s) Using Eq. (3.99) and integrating by parts, we obtain

Thus, we have

(b) By definition (3.99)

From Eq. (1.30) we have

Taking the 0+ unilateral Laplace transform of Eq. (3.103) and using Eq. (3.100), we obtain

This is consistent with Eq. (1.21); that is,

Note that taking the 0- unilateral Laplace transform of Eq. (3.103) and using Eq. (3.44), we obtain

Application of Unilateral Laplace Transform
3.37. Using the unilateral Laplace transform, redo Prob. 2.20.
The system is described by

with y(0) = y0 and x(t) = Ke-bt u(t).
Assume that y (0) = y(0-). Let
y(t) ↔ YI(s)
Then from Eq. (3.44)
y′(t) ↔ sYI(s) - y(0-) = sYI(s) - y0
From Table 3-1 we have

Taking the unilateral Laplace transform of Eq. (3.104), we obtain

or

Thus,

Using partial-fraction expansions, we obtain

Taking the inverse Laplace transform of Y(s), we obtain

which is the same as Eq. (2.107). Noting that y(0+) = y(0) = y(0-) = y0, we write y(t) as

3.38. Solve the second-order linear differential equation

with the initial conditions y(0) = 2, y′(0) = 1, and x(t) = e-tu(t).
Assume that y (0) = y(0-) and y′(0) = y(0-). Let
y(t) ↔ YI(s)
Then from Eqs. (3.44) and (3.45)
y′(t) ↔ sYI(s) - y(0-) = sYI(s) - 2
y′(t) ↔ s2YI(s) sy(0 -) y′ (0-) s2YI(s) - 2s - 1
From Table 3-1 we have

Taking the unilateral Laplace transform of Eq. (3.105), we obtain

or

Thus,

Using partial-fraction expansions, we obtain

Taking the inverse Laplace transform of YI(s), we have

Notice that y(0+) = 2 = y (0) and y′(0+) = 1 = y′(0); and we can write y(t) as

3.39. Consider the RC circuit shown in Fig. 3-14(a). The switch is closed at t = 0. Assume that there is an initial voltage on the capacitor and vc(0-) = v0.
(a) Find the current i(t).
(b) Find the voltage across the capacitor vc(t).


Fig. 3-14 RC circuit.

(a) With the switching action, the circuit shown in Fig. 3-14(a) can be represented by the circuit shown in Fig. 3-14(b) with v(t) = Vu(t). When the current i(t) is the output and the input is vs(t), the differential equation governing the circuit is

Taking the unilateral Laplace transform of Eq. (3.106) and using Eq. (3.48), we obtain

where
I(s) = ℒI {i(t)}
Now

and

Hence, Eq. (3.107) reduces to

Solving for I(s), we obtain

Taking the inverse Laplace transform of I(s), we get

(b) When vc(t) is the output and the input is vs(t), the differential equation governing the circuit is

Taking the unilateral Laplace transform of Eq. (3.108) and using Eq. (3.44), we obtain

or

Solving for Vc(s), we have

Taking the inverse Laplace transform of Vc(s), we obtain
vc(t) = V[1 - e-t/RC ]u(t) + v0e-t/RCu(t)
Note that vc(0+) = v0 = vc(0-). Thus, we write vc(t) as
vc(t) = V(1 - e-t/RC) + v0e-t/RC     t ≥ 0
3.40. Using the transform network technique, redo Prob. 3.39.
(a) Using Fig. 3-10, the transform network corresponding to Fig. 3-14 is constructed as shown in Fig. 3-15.


Fig. 3-15 Transform circuit.

Writing the voltage law for the loop, we get

Solving for I(s), we have

Taking the inverse Laplace transform of I(s), we obtain

(b) From Fig. 3.15 we have

Substituting I(s) obtained in part (a) into the above equation, we get

Taking the inverse Laplace transform of Vc(s), we have
vc(t) = V(1 - e-t/RC)u(t) + v0e-t/RCu(t)
3.41. In the circuit in Fig. 3-16(a) the switch is in the closed position for a long time before it is opened at t = 0. Find the inductor current i(t) for t ≥ 0.
When the switch is in the closed position for a long time, the capacitor voltage is charged to 10 V and there is no current flowing in the capacitor. The inductor behaves as a short circuit, and the inductor current is  = 2A.
Thus, when the switch is open, we have i (0-) = 2 and vc(0-) = 10; the input voltage is 10 V, and therefore it can be represented as 10u(t). Next, using Fig. 3-10, we construct the transform circuit as shown in Fig. 3-16(b).


Fig. 3-16

From Fig. 3-16(b) the loop equation can be written as

or

Hence,

Taking the inverse Laplace transform of I(s), we obtain

Note that i (0+) = 2 = i (0-); that is, there is no discontinuity in the inductor current before and after the switch is opened. Thus, we have

3.42. Consider the circuit shown in Fig. 3-17(a). The two switches are closed simultaneously at t = 0. The voltages on capacitors C1 and C2 before the switches are closed are 1 and 2 V, respectively.
(a) Find the currents i1(t) and i2(t).
(b) Find the voltages across the capacitors at t = 0+.
(a) From the given initial conditions, we have

Thus, using Fig. 3-10, we construct a transform circuit as shown in Fig. 3-17(b). From


Fig. 3-17

Fig. 3-17(b) the loop equations can be written directly as

Solving for I1(s) and I2(s) yields

Taking the inverse Laplace transforms of I1(s) and I2(s), we get

(b) From Fig. 3-17(b) we have

Substituting I1(s) and I2(s) obtained in part (a) into the above expressions, we get

Then, using the initial value theorem (3.97), we have

Note that vC1(0+) ≠ vC1 (0-) and vC2(0+) ≠ vC2 (0-). This is due to the existence of a capacitor loop in the circuit resulting in a sudden change in voltage across the capacitors. This step change in voltages will result in impulses in i1(t) and i2(t). Circuits having a capacitor loop or an inductor star connection are known as degenerative circuits.
SUPPLEMENTARY PROBLEMS
3.43. Find the Laplace transform of the following x(t):
(a) x(t) = sin ω0tu(t)
(b) x(t) = cos(ω0t + ϕ) u(t)
(c) x(t) = e- atu(t) - eatu (-t)
(d) x(t) = 1
(e) x(t) = sgn t
3.44. Find the Laplace transform of x(t) given by

3.45. Show that if x(t) is a left-sided signal and X(s) converges for some value of s, then the ROC of X(s) is of the form
Re(s) < σmin
where σmin equals the minimum real part of any of the poles of X(S).
3.46. Verify Eq. (3.21); that is,

3.47. Show the following properties for the Laplace transform:
(a) If x(t) is even, then X (-s) = X(s); that is, X(s) is also even.
(b) If x(t) is odd, then X (-s) -X(s); that is, X(s) is also odd.
(c) If x(t) is odd, then there is a zero in X(s) at s = 0.
3.48. Find the Laplace transform of

3.49. Find the inverse Laplace transform of the following X(s):






3.50. Using the Laplace transform, redo Prob. 2.46.
3.51. Using the Laplace transform, show that
(a) x(t) * δ (t) = x(t)
(b) x(t) * δ′(t) = x′(t)
3.52. Using the Laplace transform, redo Prob. 2.54.
3.53. Find the output y(t) of the continuous-time LTI system with
h(t) = e- 2t u(t)
for the each of the following inputs:
(a) x(t) = e-tu(t)
(b) x(t) = e-tu(-t)
3.54. The step response of an continuous-time LTI system is given by (1 = e-t) u(t). For a certain unknown input x(t), the output y(t) is observed to be (2 - 3e-t + e-3t)u(t). Find the input x(t).
3.55. Determine the overall system function H(s) for the system shown in Fig. 3-18.


Fig. 3-18

3.56. If x(t) is a periodic function with fundamental period T, find the unilateral Laplace transform of x(t).
3.57. Find the unilateral Laplace transforms of the periodic signals shown in Fig. 3-19.


Fig. 3-19

3.58. Using the unilateral Laplace transform, find the solution of
y"(t) - y ′(t) - 6y(t) = et
with the initial conditions y (0) = 1 and y′(0) = 0 for t ≥ 0.
3.59. Using the unilateral Laplace transform, solve the following simultaneous differential equations:
y′(t) + y(t) + x′(t) + x(t) = 1
y′(t) - y(t) - 2x(t) = 0
with x(0) = 0 and y (0) = 1 for t ≥ 0.
3.60. Using the unilateral Laplace transform, solve the following integral equations:


3.61. Consider the RC circuit in Fig. 3-20. The switch is closed at t = 0. The capacitor voltage before the switch closing is v0. Find the capacitor voltage for t ≥ 0.


Fig. 3-20 RC circuit.

3.62. Consider the RC circuit in Fig. 3-21. The switch is closed at t = 0. Before the switch closing, the capacitor C1 is charged to v0 V and the capacitor C2 is not charged.


Fig. 3-21 RC circuit.

(a) Assuming C1= C2= C, find the current i(t) for t ≥ 0.
(b) Find the total energy E dissipated by the resistor R, and show that E is independent of R and is equal to half of the initial energy stored in C1.
(c) Assume that R = 0 and C1= C2= C. Find the current i(t) for t ≥ 0 and voltages  and .
ANSWERS TO SUPPLEMENTARY PROBLEMS
3.43. 

(c) if a > 0,  -a < Re(s) < a. If a < 0, X(s) does not exist since X(s) does not have an ROC.
(d) Hint: x(t) = u(t) + u (-t)
X(s) does not exist since X(s) does not have an ROC.
(e) Hint: x(t) = u(t) - u (-t)
X(s) does not exist since X(s) does not have an ROC.
3.44. 
3.45. Hint: Proceed in a manner similar to Prob. 3.4.
3.46. Hint: Differentiate both sides of Eq. (3.3) with respect to s.
3.47. Hint:
(a) Use Eqs. (1.2) and (3.17).
(b) Use Eqs. (1.3) and (3.17).
(c) Use the result from part (b) and Eq. (1.83a).
3.48. 
3.49. (a) x(t) = (1 - e-t - te-t)u(t)
(b) x(t) = - u(-t) - (1 + t)e-t u(t)
(c) x(t) = (- 1 + e-t + te-t) u(- t)



3.50. Hint: Use Eq. (3.21) and Table 3-1.
3.51. Hint:
(a) Use Eq. (3.21) and Table 3-1.
(b) Use Eqs. (3.18) and (3.21) and Table 3-1.
3.52. Hint:
(a) Find the system function H(s) by Eq. (3.32) and take the inverse Laplace transform of H(s).
(b) Find the ROC of H(s) and show that it does not contain the jω-axis.
3.53. (a) y(t) = (e- t - e- 2t) u(t)
(b) y(t) = e- t u (- t) + e- 2t u(t)
3.54. x(t) = 2(1 - e- 3t)u(t)
3.55. Hint: Use the result from Prob. 3.31 to simplify the block diagram.

3.56. 
3.57. 
3.58. 
3.59. x(t) = e-t - 1, y(t) = 2 - e- t, t ≥ 0
3.60. 
3.61. vc(t) = v0e- t/RC, t ≥ 0
3.62. 








CHAPTER 4The z-Transform and Discrete-Time LTI Systems
4.1 Introduction
In Chap. 3 we introduced the Laplace transform. In this chapter we present the z-transform, which is the discrete-time counterpart of the Laplace transform. The z-transform is introduced to represent discrete-time signals (or sequences) in the z-domain (z is a complex variable), and the concept of the system function for a discrete-time LTI system will be described. The Laplace transform converts integrodifferential equations into algebraic equations. In a similar manner, the z-transform converts difference equations into algebraic equations, thereby simplifying the analysis of discrete-time systems.
The properties of the z-transform closely parallel those of the Laplace transform. However, we will see some important distinctions between the z-transform and the Laplace transform.
4.2 The z-Transform
In Sec. 2.8 we saw that for a discrete-time LTI system with impulse response h[n], the output y[n] of the system to the complex exponential input of the form zn is

where

A. Definition:
The function H(z) in Eq. (4.2) is referred to as the z-transform of h[n]. For a general discrete-time signal x[n], the z-transform X(z) is defined as

The variable z is generally complex-valued and is expressed in polar form as

where r is the magnitude of z and Ω is the angle of z. The z-transform defined in Eq. (4.3) is often called the bilateral (or two-sided) z-transform in contrast to the unilateral (or one-sided) z-transform, which is defined as

Clearly the bilateral and unilateral z-transforms are equivalent only if x[n] = 0 for n < 0. The unilateral z-transform is discussed in Sec. 4.8. We will omit the word "bilateral" except where it is needed to avoid ambiguity.
As in the case of the Laplace transform, Eq. (4.3) is sometimes considered an operator that transforms a sequence x[n] into a function X(z), symbolically represented by

The x[n] and X(z) are said to form a z-transform pair denoted as

B. The Region of Convergence:
As in the case of the Laplace transform, the range of values of the complex variable z for which the z-transform converges is called the region of convergence. To illustrate the z-transform and the associated ROC let us consider some examples.
EXAMPLE 4.1 Consider the sequence

Then by Eq. (4.3) the z-transform of x[n] is

For the convergence of X(z) we require that

Thus, the ROC is the range of values of z for which |az-1| <1 or, equivalently, |z| > |a|. Then

Alternatively, by multiplying the numerator and denominator of Eq. (4.9) by z, we may write X(z) as

Both forms of X (z) in Eqs. (4.9) and (4.10) are useful depending upon the application. From Eq. (4.10) we see that X(z) is a rational function of z. Consequently, just as with rational Laplace transforms, it can be characterized by its zeros (the roots of the numerator polynomial) and its poles (the roots of the denominator polynomial). From Eq. (4.10) we see that there is one zero at z = 0 and one pole at z = a. The ROC and the pole-zero plot for this example are shown in Fig. 4-1. In z-transform applications, the complex plane is commonly referred to as the z-plane.


Fig. 4-1 ROC of the form |z| > |a|.

EXAMPLE 4.2 Consider the sequence

Its z-transform X(z) is given by (Prob. 4.1)

Again, as before, X(z) may be written as

Thus, the ROC and the pole-zero plot for this example are shown in Fig. 4-2. Comparing Eqs. (4.9) and (4.12) [or Eqs. (4.10) and (4.13)], we see that the algebraic expressions of X(z) for two different sequences are identical except for the ROCs. Thus, as in the Laplace transform, specification of the z-transform requires both the algebraic expression and the ROC.


Fig. 4-2 ROC of the form |z| < |a|.

C. Properties of the ROC:
As we saw in Examples 4.1 and 4.2, the ROC of X(z) depends on the nature of x[n]. The properties of the ROC are summarized below. We assume that X(z) is a rational function of z.
Property 1: The ROC does not contain any poles.
Property 2: If x[n] is a finite sequence (that is, x[n] = 0 except in a finite interval N1 ≤ n ≤ N2, where N1 and N2 are finite) and X(z) converges for some value of z, then the ROC is the entire z-plane except possibly z = 0 or z = ∞.
Property 3: If x[n] is a right-sided sequence (that is, x[n] = 0 for n < N1 < ∞) and X(z) converges for some value of z, then the ROC is of the form

where rmax equals the largest magnitude of any of the poles of X(z). Thus, the ROC is the exterior of the circle |z| = rmax in the z-plane with the possible exception of z = ∞.
Property 4: If x[n] is a left-sided sequence (that is, x[n] = 0 for n > N2 > -∞) and X(z) converges for some value of z, then the ROC is of the form

where rmin is the smallest magnitude of any of the poles of X(z). Thus, the ROC is the interior of the circle |z| rmin in the z-plane with the possible exception of z = 0.
Property 5: If x[n] is a two-sided sequence (that is, x[n] is an infinite-duration sequence that is neither right-sided nor left-sided) and X(z) converges for some value of z, then the ROC is of the form

where r1 and r2 are the magnitudes of the two poles of X(z). Thus, the ROC is an annular ring in the z-plane between the circles |z| = r1 and |z| = r2 not containing any poles.
Note that Property 1 follows immediately from the definition of poles; that is, X(z) is infinite at a pole. For verification of the other properties, see Probs. 4.2 and Probs. 4.5.
4.3 z-Transforms of Some Common Sequences
A. Unit Impulse Sequence δ [n]:
From definitions (1.45) and (4.3)

Thus,

B. Unit Step Sequence u[n]:
Setting a = 1 in Eqs. (4.8) to (4.10), we obtain

C. z-Transform Pairs:
The z-transforms of some common sequences are tabulated in Table 4-1.

TABLE 4-1 Some Common z-Transform Pairs


4.4 Properties of the z-Transform
Basic properties of the z-transform are presented in the following discussion. Verification of these properties is given in Probs. 4.8 to Probs. 4.14.
A. Linearity:
If

then

where a1 and a2 are arbitrary constants.
B. Time Shifting:
If

then

Special Cases:


Because of these relationship [Eqs. (4.19) and (4.20)], z-1 is often called the unit-delay operator and z is called the unit-advance operator. Note that in the Laplace transform the operators s-1 = 1/s and s correspond to time-domain integration and differentiation, respectively [Eqs. (3.22) and (3.20)].
C. Multiplication by 
If

then

In particular, a pole (or zero) at z = zk in X(z) moves to z = z0zk after multiplication by  and the ROC expands or contracts by the factor |z0|.
Special Case:

In this special case, all poles and zeros are simply rotated by the angle Ω0 and the ROC is unchanged.
D. Time Reversal:
If

then

Therefore, a pole (or zero) in X(z) at z = zk moves to 1/zk after time reversal. The relationship R′ = 1/R indicates the inversion of R, reflecting the fact that a right-sided sequence becomes left-sided if time-reversed, and vice versa.
E. Multiplication by n (or Differentiation in z):
If

then

F. Accumulation:
If

then

Note that  is the discrete-time counterpart to integration in the time domain and is called the accumulation. The comparable Laplace transform operator for integration is 1/s.
G. Convolution:
If

then

This relationship plays a central role in the analysis and design of discrete-time LTI systems, in analogy with the continuous-time case.
H. Summary of Some z-transform Properties:
For convenient reference, the properties of the z-transform presented above are summarized in Table 4-2.

TABLE 4-2. Some Properties of the z-Transform


4.5 The Inverse z-Transform
Inversion of the z-transform to find the sequence x[n] from its z-transform X(z) is called the inverse z-transform, symbolically denoted as

A. Inversion Formula:
As in the case of the Laplace transform, there is a formal expression for the inverse z-transform in terms of an integration in the z-plane; that is,

where C is a counterclockwise contour of integration enclosing the origin. Formal evaluation of Eq. (4.28) requires an understanding of complex variable theory.
B. Use of Tables of z-Transform Pairs:
In the second method for the inversion of X(z), we attempt to express X(z) as a sum

where X1(z), ..., Xn(z) are functions with known inverse transforms x1[n], ..., xn[n]. From the linearity property (4.17) it follows that

C. Power Series Expansion:
The defining expression for the z-transform [Eq. (4.3)] is a power series where the sequence values x[n] are the coefficients of z-n. Thus, if X(z) is given as a power series in the form

we can determine any particular value of the sequence by finding the coefficient of the appropriate power of z-1. This approach may not provide a closed-form solution but is very useful for a finite-length sequence where X(z) may have no simpler form than a polynomial in z-1 (see Prob. 4.15). For rational z-transforms, a power series expansion can be obtained by long division as illustrated in Probs. 4.16 and Probs. 4.17.
D. Partial-Fraction Expansion:
As in the case of the inverse Laplace transform, the partial-fraction expansion method provides the most generally useful inverse z-transform, especially when X(z) is a rational function of z. Let

Assuming n ≥ m and all poles pk are simple, then

where

Hence, we obtain

Inferring the ROC for each term in Eq. (4.35) from the overall ROC of X(z) and using Table 4-1, we can then invert each term, producing thereby the overall inverse z-transform (see Probs. 4.19 to Probs. 4.23).
If m > n in Eq. (4.32), then a polynomial of z must be added to the right-hand side of Eq. (4.35), the order of which is (m - n). Thus for m > n, the complete partial-fraction expansion would have the form

If X(z) has multiple-order poles, say, pi is the multiple pole with multiplicity r, then the expansion of X(z)/z will consist of terms of the form

where

4.6 The System Function of Discrete-Time LTI Systems
A. The System Function:
In Sec. 2.6 we showed that the output y[n] of a discrete-time LTI system equals the convolution of the input x[n] with the impulse response h[n]; that is [Eq. (2.35)],

Applying the convolution property (4.26) of the z-transform, we obtain

where Y(z), X(z), and H(z) are the z-transforms of y[n], x[n], and h[n], respectively. Equation (4.40) can be expressed as

The z-transform H(z) of h[n] is referred to as the system function (or the transfer function) of the system. By Eq. (4.41) the system function H(z) can also be defined as the ratio of the z-transforms of the output y[n] and the input x[n]. The system function H(z) completely characterizes the system. Fig. 4-3 illustrates the relationship of Eqs. (4.39) and (4.40).


Fig. 4-3 Impulse response and system function.

B. Characterization of Discrete-Time LTI Systems:
Many properties of discrete-time LTI systems can be closely associated with the characteristics of H(z) in the z-plane and in particular with the pole locations and the ROC.
1. Causality:
For a causal discrete-time LTI system, we have [Eq. (2.44)]

since h[n] is a right-sided signal, the corresponding requirement on H(z) is that the ROC of H(z) must be of the form

That is, the ROC is the exterior of a circle containing all of the poles of H(z) in the z-plane. Similarly, if the system is anticausal, that is,

then h[n] is left-sided and the ROC of H(z) must be of the form

That is, the ROC is the interior of a circle containing no poles of H(z) in the z-plane.
2. Stability:
In Sec. 2.7 we stated that a discrete-time LTI system is BIBO stable if and only if [Eq. (2.49)]

The corresponding requirement on H(z) is that the ROC of H(z) contains the unit circle (that is, |z| = 1). (See Prob. 4.30.)
3. Causal and Stable Systems:
If the system is both causal and stable, then all of the poles of H(z) must lie inside the unit circle of the z-plane because the ROC is of the form |z| > rmax, and since the unit circle is included in the ROC, we must have rmax < 1.
C. System Function for LTI Systems Described by Linear Constant-Coefficient Difference Equations:
In Sec. 2.9 we considered a discrete-time LTI system for which input x[n] and output y[n] satisfy the general linear constant-coefficient difference equation of the form

Applying the z-transform and using the time-shift property (4.18) and the linearity property (4.17) of the z-transform, we obtain

or

Thus,

Hence, H(z) is always rational. Note that the ROC of H(z) is not specified by Eq. (4.44) but must be inferred with additional requirements on the system such as the causality or the stability.
D. Systems Interconnection:
For two LTI systems (with h1[n] and h2[n], respectively) in cascade, the overall impulse response h[n] is given by

Thus, the corresponding system functions are related by the product

Similarly, the impulse response of a parallel combination of two LTI systems is given by

and

4.7 The Unilateral z-Transform
A. Definition:
The unilateral (or one-sided) z-transform XI(z) of a sequence x[n] is defined as [Eq. (4.5)]

and differs from the bilateral transform in that the summation is carried over only n ≥ 0. Thus, the unilateral z-transform of x[n] can be thought of as the bilateral transform of x[n]u[n]. Since x[n]u[n] is a right-sided sequence, the ROC of X I(z) is always outside a circle in the z-plane.
B. Basic Properties:
Most of the properties of the unilateral z-transform are the same as for the bilateral z-transform. The unilateral z-transform is useful for calculating the response of a causal system to a causal input when the system is described by a linear constant-coefficient difference equation with nonzero initial conditions. The basic property of the unilateral z-transform that is useful in this application is the following time-shifting property which is different from that of the bilateral transform.
Time-Shifting Property:
If x[n] ↔ XI(z), then for m ≥ 0,

The proofs of Eqs. (4.50) and (4.51) are given in Prob. 4.36.
D. System Function:
Similar to the case of the continuous-time LTI system, with the unilateral z-transform, the system function H(z) = Y(z)/X(z) is defined under the condition that the system is relaxed; that is, all initial conditions are zero.
SOLVED PROBLEMS
The z-Transform
4.1. Find the z-transform of
(a) x[n] = -anu[ - n - 1]
(b) x[n] = a-nu[- n - 1]
(a) From Eq. (4.3)

By Eq. (1.91)

Thus,

(b) Similarly,

Again by Eq. (1.91)

Thus,

4.2. A finite sequence x[n] is defined as

where N1 and N2 are finite. Show that the ROC of X(z) is the entire z-plane except possibly z = 0 or z = ∞.
From Eq. (4.3)

For z not equal to zero or infinity, each term in Eq. (4.54) will be finite and thus X (z) will converge. If N1 <0 and N2 >0, then Eq. (4.54) includes terms with both positive powers of z and negative powers of z. As |z| → 0, terms with negative powers of z become unbounded, and as |z| → ∞, terms with positive powers of z become unbounded. Hence, the ROC is the entire z-plane except for z = 0 and z = ∞. If N1 ≥ 0, Eq. (4.54) contains only negative powers of z, and hence the ROC includes z = ∞. If N2 ≤ 0, Eq. (4.54) contains only positive powers of z, and hence the ROC includes z = 0.
4.3. A finite sequence x[n] is defined as

Find X(z) and its ROC.
From Eq. (4.3) and given x[n] we have

For z not equal to zero or infinity, each term in X(z) will be finite and consequently X(z) will converge. Note that X(z) includes both positive powers of z and negative powers of z. Thus, from the result of Prob. 4.2 we conclude that the ROC of X(z) is 0 < |z| ∞.
4.4. Consider the sequence

Find X(z) and plot the poles and zeros of X(z).
By Eq. (4.3) and using Eq. (1.90), we get

From Eq. (4.55) we see that there is a pole of (N - 1)th order at z = 0 and a pole at z = a. Since x[n] is a finite sequence and is zero for n < 0, the ROC is |z| > 0. The N roots of the numerator polynomial are at

The root at k = 0 cancels the pole at z = a. The remaining zeros of X(z) are at

The pole-zero plot is shown in Fig. 4-4 with N = 8.


Fig. 4-4 Pole-zero plot with N = 8.

4.5. Show that if x[n] is a right-sided sequence and X(z) converges for some value of z, then the ROC of X(z) is of the form

where rmax is the maximum magnitude of any of the poles of X(z).
Consider a right-sided sequence x[n] so that

and X(z) converges for |z| = r0. Then from Eq. (4.3)

Now if r1 > r0, then

since (r1/r0)-n is a decaying sequence. Thus, X(z) converges for r = r1 and the ROC of X(z) is of the form
|z| > r0
Since the ROC of X(z) cannot contain the poles of X(z), we conclude that the ROC of X(z) is of the form
|z| > rmax
where rmax is the maximum magnitude of any of the poles of X (z).
If N1 <0, then

That is, X(z) contains the positive powers of z and becomes unbounded at z = ∞. In this case the ROC is of the form
∞ > |z| > rmax
From the above result we can tell that a sequence x[n] is causal (not just right-sided) from the ROC of X(z) if z = ∞ is included. Note that this is not the case for the Laplace transform.
4.6. Find the z-transform X(z) and sketch the pole-zero plot with the ROC for each of the following sequences:
(a) 
(b) 
(c) 
(a) From Table 4-1

We see that the ROCs in Eqs. (4.58) and (4.59) overlap, and thus,

From Eq. (4.60) we see that X(z) has two zeros at z = 0 and  and two poles at  and  and that the ROC is , as sketched in Fig. 4-5(a).
(b) From Table 4-1

We see that the ROCs in Eqs. (4.61) and (4.62) overlap, and thus

From Eq. (4.63) we see that X(z) has one zero at z = 0 and two poles at  and  and that the ROC is , as sketched in Fig. 4-5(b).


Fig. 4-5

(c) From Table 4-1

We see that the ROCs in Eqs. (4.64) and (4.65) do not overlap and that there is no common ROC, and thus x[n] will not have X (z).
4.7. Let

(a) Sketch x[n] for a < 1 and a > 1.
(b) Find X(z) and sketch the zero-pole plot and the ROC for a < 1 and a > 1.
(a) The sequence x[n] is sketched in Figs. 4-6(a) and (b) for both a < 1 and a > 1.


Fig. 4-6

(b) Since x[n] is a two-sided sequence, we can express it as

From Table 4-1

If a < 1, we see that the ROCs in Eqs. (4.68) and (4.69) overlap, and thus,

From Eq. (4.70) we see that X(z) has one zero at the origin and two poles at z = a and z = 1/a and that the ROC is a < |z| < 1/a, as sketched in Fig. 4-7. If a > 1, we see that the ROCs in Eqs. (4.68) and (4.69) do not overlap and that there is no common ROC, and thus x[n] will not have X(z).


Fig. 4-7

Properties of the z-Transform
4.8. Verify the time-shifting property (4.18); that is,

By definition (4.3)

By the change of variables m = n - n0, we obtain

Because of the multiplication by Z-n0, for n0 > 0, additional poles are introduced at z = 0 and will be deleted at z = ∞. Similarly, if n0 < 0, additional zeros are introduced at z = 0 and will be deleted at z = ∞. Therefore, the points z = 0 and z = ∞ can be either added to or deleted from the ROC by time shifting. Thus, we have

where R and R′ are the ROCs before and after the time-shift operation.
4.9. Verify Eq. (4.21); that is,

By definition (4.3)

A pole (or zero) at z = zk in X(z) moves to z = z0zk, and the ROC expands or contracts by the factor |z0|. Thus, we have

4.10. Find the z-transform and the associated ROC for each of the following sequences:
(a) x[n] = δ[n - n0]
(b) x[n] = u[n - n0]
(c) x[n] = an+1 u[n + 1]
(d) x[n] = u[-n]
(e) x[n] = a-n u[- n]
(a) From Eq. (4.15)

Applying the time-shifting property (4.18), we obtain

(b) From Eq. (4.16)

Again by the time-shifting property (4.18) we obtain

(c) From Eqs. (4.8) and (4.10)

By Eq. (4.20) we obtain

(d) From Eq. (4.16)

By the time-reversal property (4.23) we obtain

(e) From Eqs. (4.8) and (4.10)

Again by the time-reversal property (4.23) we obtain

4.11. Verify the multiplication by n (or differentiation in z) property (4.24); that is,

From definition (4.3)

Differentiating both sides with respect to z, we have

and

Thus, we conclude that

4.12. Find the z-transform of each of the following sequences:
(a) x[n] = nanu[n]
(b) x[n] = nan-1 u[n]
(a) From Eqs. (4.8) and (4.10)

Using the multiplication by n property (4.24), we get

(b) Differentiating Eq. (4.76) with respect to a, we have

Note that dividing both sides of Eq. (4.77) by a, we obtain Eq. (4.78).
4.13. Verify the convolution property (4.26); that is,

By definition (2.35)

Thus, by definition (4.3)

Noting that the term in parentheses in the last expression is the z-transform of the shifted signal x2[n - k], then by the time-shifting property (4.18), we have

with an ROC that contains the intersection of the ROC of X1(z) and X2(z). If a zero of one transform cancels a pole of the other, the ROC of Y(z) may be larger. Thus, we conclude that

4.14. Verify the accumulation property (4.25); that is,

From Eq. (2.40) we have

Thus, using Eq. (4.16) and the convolution property (4.26), we obtain

with the ROC that includes the intersection of the ROC of X(z) and the ROC of the z-transform of u[n]. Thus,

Inverse z-Transform
4.15. Find the inverse z-transform of

Multiplying out the factors of Eq. (4.79), we can express X(z) as

Then, by definition (4.3),

and we get

4.16. Using the power series expansion technique, find the inverse z-transform of the following X(z):
(a) 
(b) 
(a) Since the ROC is |z| > |a|, that is, the exterior of a circle, x[n] is a right-sided sequence. Thus, we must divide to obtain a series in the power of z-1. Carrying out the long division, we obtain

Thus,

and so by definition (4.3) we have

Thus, we obtain

(b) Since the ROC is |z| < |a|, that is, the interior of a circle, x[n] is a left-sided sequence. Thus, we must divide so as to obtain a series in the power of z as follows. Multiplying both the numerator and denominator of X(z) by z, we have

and carrying out the long division, we obtain

Thus,

and so by definition (4.3) we have

Thus, we get

4.17. Find the inverse z-transform of the following X(z):
(a) 
(b) 
(a) The power series expansion for log(1 - r) is given by

Now

Since the ROC is |z| > |a|, that is, |az-1| <1, by Eq. (4.80), X(z) has the power series expansion

from which we can indentify x[n] as

or

(b)

Since the ROC is |z| < |a|, that is, |a-1z| < 1, by Eq. (4.80), X(z) has the power series expansion

from which we can identify x[n] as

or

4.18. Using the power series expansion technique, find the inverse z-transform of the following X(z):
(a) 
(b) 
(a) Since the ROC is  is a left-sided sequence. Thus, we must divide to obtain a series in power of z. Carrying out the long division, we obtain

Thus,

and so by definition (4.3) we obtain

(b) Since the ROC is |z| > 1, x[n] is a right-sided sequence. Thus, we must divide so as to obtain a series in power of z -1 as follows:

Thus,

and so by definition (4.3) we obtain

4.19. Using partial-fraction expansion, redo Prob. 4.18.
(a)

Using partial-fraction expansion, we have

where

and we get

Since the ROC of X(z) is  is a left-sided sequence, and from Table 4-1 we get

which gives

(b)

Since the ROC of X(z) is |z| > 1, x[n] is a right-sided sequence, and from Table 4-1 we get

which gives

4.20. Find the inverse z-transform of

Using partial-fraction expansion, we have

where

Substituting these values into Eq. (4.83), we have

Setting z = 0 in the above expression, we have

Thus,

Since the ROC is |z| > 2, x[n] is a right-sided sequence, and from Table 4-1 we get
x[n] = (1 - 2n + n2n-1)u[n]
4.21. Find the inverse z-transform of

Note that X(z) is an improper rational function; thus, by long division, we have



Thus,

Since the ROC of X(z) is |z| < 1, x[n] is a left-sided sequence, and from Table 4-1 we get

4.22. Find the inverse z-transform of

X(z) can be rewritten as

Since the ROC is |z| > 2, x[n] is a right-sided sequence, and from Table 4-1 we have

Using the time-shifting property (4.18), we have

Thus, we conclude that

4.23. Find the inverse z-transform of

We see that X(z) can be written as

where

Thus, if

then by the linearity property (4.17) and the time-shifting property (4.18), we get

Since the ROC of X1(z) is |z| > 0, x1[n] is a right-sided sequence, and from Table 4-1 we get

Thus, from Eq. (4.84) we get

4.24. Find the inverse z-transform of

From Eq. (4.78) (Prob. 4.12)

Now, from Eq. (4.85)

and applying the time-shifting property (4.20) to Eq. (4.86), we get

since x[-1] = 0 at n = -1.
System Function
4.25. Using the z-transform, redo Prob. 2.28.
From Prob. 2.28, x[n] and h[n] are given by

From Table 4-1

Then, by Eq. (4.40)

Using partial-fraction expansion, we have

where

Thus,

Taking the inverse z-transform of Y (z), we get

which is the same as Eq. (2.134).
4.26. Using the z-transform, redo Prob. 2.29.
(a) From Prob. 2.29(a), x[n] and h[n] are given by

From Table 4-1

Then

Using partial-fraction expansion, we have

where

Thus,

and

which is the same as Eq. (2.135). When a = β,

Using partial-fraction expansion, we have

where

and

Setting z = 0 in the above expression, we have

Thus,

and from Table 4-1 we get

Thus, we obtain the same results as Eq. (2.135).
(b) From Prob. 2.29(b), x[n] and h[n] are given by

From Table 4-1 and Eq. (4.75)

Then

Using partial-fraction expansion, we have

where

Thus,

and from Table 4-1 we obtain

which is the same as Eq. (2.137).
4.27. Using the z-transform, redo Prob. 2.30.
From Fig. 2-23 and definition (4.3)

Thus, by the convolution property (4.26)

Hence,
h[n] = {1, 2, 3, 3, 2, 1}
which is the same result obtained in Prob. 2.30.
4.28. Using the z-transform, redo Prob. 2.32.
Let x [n] and y [n] be the input and output of the system. Then

Then, by Eq. (4.41)

Using partial-fraction expansion, we have

where

Thus,

Taking the inverse z-transform of H (z), we obtain

When n = 0,

Then

Thus, h[n] can be rewritten as

which is the same result obtained in Prob. 2.32.
4.29. The output y [n] of a discrete-time LTI system is found to be 2()nu [n] when the input x [n] is u [n].
(a) Find the impulse response h [n] of the system.
(b) Find the output y [n] when the input x [n] is ()nu [n].
(a)

Hence, the system function H (z) is

Using partial-fraction expansion, we have

Thus,

Taking the inverse z-transform of H (z), we obtain

(b)

Then,

Again by partial-fraction expansion we have

where

Thus,

Taking the inverse z-transform of Y (z), we obtain

4.30. If a discrete-time LTI system is BIBO stable, show that the ROC of its system function H (z) must contain the unit circle; that is, |z| = 1.
A discrete-time LTI system is BIBO stable if and only if its impulse response h [n] is absolutely summable, that is [Eq. (2.49)],

Now

Let z =e jΩ so that |z| = |e jΩ| = 1. Then

Therefore, we see that if the system is stable, then H (z) converges for z = e jΩ. That is, for a stable discrete-time LTI system, the ROC of H (z) must contain the unit circle |z| = 1.
4.31. Using the z-transform, redo Prob. 2.38.
(a) From Prob. 2.38 the impulse response of the system is

Since the ROC of H (z) is |z| > |a|, z = ∞ is included. Thus, by the result from Prob. 4.5 we conclude that h [n] is a causal sequence. Thus, the system is causal.
(b) If |α| > 1, the ROC of H (z) does not contain the unit circle | z | = 1, and hence the system will not be stable.
      If |a| < 1, the ROC of H (z) contains the unit circle | z | = 1, and hence the system will be stable.
4.32. A causal discrete-time LTI system is described by

where x [n] and y [n] are the input and output of the system, respectively.
(a) Determine the system function H (z).
(b) Find the impulse response h [n] of the system.
(c) Find the step response s [n] of the system.
(a) Taking the z-transform of Eq. (4.88), we obtain


or
Thus,

(b) Using partial-fraction expansion, we have

where

Thus,

Taking the inverse z-transform of H (z), we get
(c)


Then

Again using partial-fraction expansion, we have

Thus,

Taking the inverse z-transformation of Y (z), we obtain

4.33. Using the z-transform, redo Prob. 2.41.
As in Prob. 2.41, from Fig. 2-30 we see that

Taking the z-transform of the above equations, we get

Rearranging, we get

from which we obtain

Rewriting Eq. (4.89), we have

or

Taking the inverse z-transform of Eq. (4.90) and using the time-shifting property (4.18), we obtain

which is the same as Eq. (2.148).
4.34. Consider the discrete-time system shown in Fig. 4-8. For what values of k is the system BIBO stable?


Fig. 4-8

From Fig. 4-8 we see that

Taking the z-transform of the above equations, we obtain

Rearranging, we have

from which we obtain

which shows that the system has one zero at z = -k/3 and one pole at z = k/2 and that the ROC is |z| > | k/2|. Thus, as shown in Prob. 4.30, the system will be BIBO stable if the ROC contains the unit circle, | z | = 1. Hence, the system is stable only if | k | < 2.
Unilateral z-Transform
4.35. Find the unilateral z-transform of the following x [n]:
(a) x [n] = a nu [n]
(b) x [n] = a n+1 u [n + 1]
(a) Since x [n] = 0 for n < 0, XI(z) = X (z), and from Example 4.1 we have

(b) By definition (4.49) we have

Note that in this case x [n] is not a causal sequence; hence, X1(z) ≠ X (z) [see Eq. (4.73) in Prob. 4.10].
4.36. Verify Eqs. (4.50) and (4.51); that is, for m ≥ 0,
(a) x [n - m] ↔ z- m X1(z) + z-m + 1 x [-1] + z-m+2x [-2] + ... + x [-m]
(b) x [n + m] ↔ z m X1(z) - z mx [0] = z m-1x [1] - ... - zx [m - 1]
(a) By definition (4.49) with m ≥ 0 and using the change in variable k = n - m, we have

(b) With m ≥ 0

4.37. Using the unilateral z-transform, redo Prob. 2.42.
The system is described by

with y [-1] = y-1 and x [n] = Kb nu [n]. Let

Then from Eq. (4.50)

From Table 4-1 we have

Taking the unilateral z-transform of Eq. (4.93), we obtain

or

or

Thus,

Using partial-fraction expansion, we obtain

Taking the inverse z-transform of YI(z), we get

which is the same as Eq. (2.158).
4.38. For each of the following difference equations and associated input and initial conditions, determine the output y [n]:
(a) 
(b) 
(a)
Taking the unilateral z-transform of the given difference equation, we get

Substituting y [-1] = 1 and X1(z) into the above expression, we get

or
Thus,

Hence,

(b)

Taking the unilateral z-transform of the given difference equation, we obtain

Substituting y [-1] = 1, y [-2] = 2, and X1 (z) into the above expression, we get

or

Thus,

Hence,

4.39. Let x [n] be a causal sequence and

Show that

Equation (4.94) is called the initial value theorem for the z-transform.
Since x[n] = 0 for n < 0, we have

As z → ∞, z-n → 0 for n > 0. Thus, we get

4.40. Let x [n] be a causal sequence and

Show that if X (z) is a rational function with all its poles strictly inside the unit circle except possibly for a first-order pole at z = 1, then

Equation (4.95) is called the final value theorem for the z-transform.
From the time-shifting property (4.19) we have

The left-hand side of Eq. (4.96) can be written as

If we now let z → 1, then from Eq. (4.96) we have

SUPPLEMENTARY PROBLEMS
4.41. Find the z-transform of the following x [n]:
(a) 
(b) 
(c) 
(d) 
4.42. Show that if x [n] is a left-sided sequence and X (z) converges from some value of z, then the ROC of X (z) is of the form

where rmin is the smallest magnitude of any of the poles of X (z).
4.43. Given

(a) State all the possible regions of convergence.
(b) For which ROC is X (z) the z-transform of a causal sequence?
4.44. Verify the time-reversal property (4.23); that is,

4.45. Show the following properties for the z-transform.
(a) If x[n] is even, then X (z-1) = X (z).
(b) If x[n] is odd, then X (z-1) = - X (z).
(c) If x[n] is odd, then there is a zero in X (z) at z = 1.
4.46. Consider the continuous-time signal

Let the sequence x [n] be obtained by uniform sampling of X(t) such that x [n] = x (nTs), where Ts is the sampling interval. Find the z-transform of x [n].
4.47. Derive the following transform pairs:

4.48. Find the z-transforms of the following x [n]:
(a) x[n] = (n - 3)u [n - 3]
(b) x[n] = (n - 3)u [n]
(c) x[n] = u [n] - u [n - 3]
(d) x[n] = n {u [n] - u [n - 3]}
4.49. Using the relation

find the z-transform of the following x [n]:
(a) x [n] = na n-1 u [n]
(b) x [n] = n (n - 1)a n-2 u [n]
(c) x [n] = n (n - 1) ... (n - k + 1) a n - ku [n]
4.50. Using the z-transform, verify Eqs. (2.130) and (2.131) in Prob. 2.27; that is,
(a) x [n] *δ [n] = x [n]
(b) x [n] *δ [n - n0] = x [n - n0]
4.51. Using the z-transform, redo Prob. 2.47.
4.52. Find the inverse z-transform of

4.53. Using the method of long division, find the inverse z-transform of the following X (z):
(a) 
(b) 
(c) 
4.54. Using the method of partial-fraction expansion, redo Prob. 4.53.
4.55. Consider the system shown in Fig. 4-9. Find the system function H (z) and its impulse response h [n].


Fig. 4-9

4.56. Consider the system shown in Fig. 4-10.
(a) Find the system function H (z).
(b) Find the difference equation relating the output y [n] and input x [n].


Fig. 4-10

4.57. Consider a discrete-time LTI system whose system function H (z) is given by

(a) Find the step response s [n].
(b) Find the output y [n] to the input x [n] = nu [n].
4.58. Consider a causal discrete-time system whose output y [n] and input x [n] are related by

(a) Find its system function H(z).
(b) Find its impulse response h[n].
4.59. Using the unilateral z-transform, solve the following difference equations with the given initial conditions:
(a) 
(b) 
4.60. Determine the initial and final values of x [n] for each of the following X (z):
(a) 
(b) 
ANSWERS TO SUPPLEMENTARY PROBLEMS
4.41.
(a) 
(b) 
(c)
(d) X(z) does not exist.
4.42. Hint: Proceed in a manner similar to Prob. 4.5.
4.43.
(a) 0 < |z| < 1, 1 < | z | < 2, 2 < | z | 3, | z | > 3
(b) |z| > 3
4.44. Hint: Change n to - n in definition (4.3).
4.45. Hint:
(a) Use Eqs. (1.2) and (4.23).
(b) Use Eqs. (1.3) and (4.23).
(c) Use th	e result from part (b).
4.46.
4.47. Hint: Use Euler's formulas.

and use Eqs. (4.8) and (4.10) with a = e±jΩ0.
4.48.
(a) 
(b) 
(c) 
(d) 
4.49. Hint: Differentiate both sides of the given relation consecutively with respect to a.
(a) 
(b) 
(c) 
4.50. Hint: Use Eq. (4.26) of the z-transform and transform pairs 1 and 4 from Table 4-1.
4.51. Hint: Use Eq. (4.26) and Table 4-1.
4.52. Hint: Use the power series expansion of the exponential function er.

4.53.
(a) 
(b) 
(c) 
4.54.
(a) 
(b) 
(c) x[n] = (-1 + 2n)u[n]
4.55.

4.56.
(a) 
(b) y[n] + a1y[n - 1]+ a2y[n - 2] = b0x[n] + b1x[n - 1]+ b2x[n - 2]
4.57.
(a) 
(b) 
4.58.
(a) 
(b) 
4.59.
(a) 
(b) 
4.60.
(a) x[0] = 2, x[∞] = 0
(b) x[0] = 0, x[∞] = 1







CHAPTER 5Fourier Analysis of Continuous-Time Signals and Systems
5.1 Introduction
In previous chapters we introduced the Laplace transform and the z-transform to convert time-domain signals into the complex s-domain and z-domain representations that are, for many purposes, more convenient to analyze and process. In addition, greater insights into the nature and properties of many signals and systems are provided by these transformations. In this chapter and the following one, we shall introduce other transformations known as Fourier series and Fourier transform which convert time-domain signals into frequency-domain (or spectral) representations. In addition to providing spectral representations of signals, Fourier analysis is also essential for describing certain types of systems and their properties in the frequency domain. In this chapter we shall introduce Fourier analysis in the context of continuous-time signals and systems.
5.2 Fourier Series Representation of Periodic Signals
A. Periodic Signals:
In Chap. 1 we defined a continuous-time signal x(t) to be periodic if there is a positive nonzero value of T for which

The fundamental period T0 of x(t) is the smallest positive value of T for which Eq. (5.1) is satisfied, and 1/T0 = ƒ0 is referred to as the fundamental frequency.
Two basic examples of periodic signals are the real sinusoidal signal

and the complex exponential signal

where ω0 = 2π/T0 = 2πƒ0 is called the fundamental angular frequency.
B. Complex Exponential Fourier Series Representation:
The complex exponential Fourier series representation of a periodic signal x(t) with fundamental period T0 is given by

where ck are known as the complex Fourier coefficients and are given by

where ∫T0 denotes the integral over any one period and 0 to T0 or -T0/2 to T0/2 is commonly used for the integration. Setting k = 0 in Eq. (5.5), we have

which indicates that c0 equals the average value of x(t) over a period.
When x(t) is real, then from Eq. (5.5) it follows that

where the asterisk indicates the complex conjugate.
C. Trigonometric Fourier Series:
The trigonometric Fourier series representation of a periodic signal x(t) with fundamental period T0 is given by

where ak and bk are the Fourier coefficients given by

The coefficients ak and bk and the complex Fourier coefficients ck are related by (Prob. 5.3)

From Eq. (5.10) we obtain

When x(t) is real, then ak and bk are real and by Eq. (5.10) we have

Even and Odd Signals:
If a periodic signal x(t) is even, then bk = 0 and its Fourier series (5.8) contains only cosine terms:

If x(t) is odd, then ak = 0 and its Fourier series contains only sine terms:

D. Harmonic Form Fourier Series:
Another form of the Fourier series representation of a real periodic signal x(t) with fundamental period T0 is

Equation (5.15) can be derived from Eq. (5.8) and is known as the harmonic form Fourier series of x(t). The term C0 is known as the dc component, and the term Ckcos(kω0t - θk) is referred to as the kth harmonic component of x(t). The first harmonic component C1cos(ω0t - θ1) is commonly called the fundamental component because it has the same fundamental period as x(t). The coefficients Ck and the angles θk are called the harmonic amplitudes and phase angles, respectively, and they are related to the Fourier coefficients ak and bk by

For a real periodic signal x(t), the Fourier series in terms of complex exponentials as given in Eq. (5.4) is mathematically equivalent to either of the two forms in Eqs. (5.8) and (5.15). Although the latter two are common forms for Fourier series, the complex form in Eq. (5.4) is more general and usually more convenient, and we will use that form almost exclusively.
E. Convergence of Fourier Series:
It is known that a periodic signal x(t) has a Fourier series representation if it satisfies the following Dirichlet conditions:
1. x(t) is absolutely integrable over any period; that is,

2. x(t) has a finite number of maxima and minima within any finite interval of t.
3. x(t) has a finite number of discontinuities within any finite interval of t, and each of these discontinuities is finite.
Note that the Dirichlet conditions are sufficient but not necessary conditions for the Fourier series representation (Prob. 5.8).
F. Amplitude and Phase Spectra of a Periodic Signal:
Let the complex Fourier coefficients ck in Eq. (5.4) be expressed as

A plot of |ck| versus the angular frequency ω is called the amplitude spectrum of the periodic signal x(t), and a plot of φk versus ω is called the phase spectrum of x(t). Since the index k assumes only integers, the amplitude and phase spectra are not continuous curves but appear only at the discrete frequencies kω0. They are therefore referred to as discrete frequency spectra or line spectra.
For a real periodic signal x(t) we have  Thus,

Hence, the amplitude spectrum is an even function of ω, and the phase spectrum is an odd function of ω for a real periodic signal.
G. Power Content of a Periodic Signal:
In Chap. 1 (Prob. 1.18) we introduced the average power of a periodic signal x(t) over any period as

If x(t) is represented by the complex exponential Fourier series in Eq. (5.4), then it can be shown that (Prob. 5.14)

Equation (5.21) is called Parseval's identity (or Parseval's theorem) for the Fourier series.
5.3 The Fourier Transform
A. From Fourier Series to Fourier Transform:
Let x(t) be a nonperiodic signal of finite duration; that is,
x(t) = 0 |t| > T1
Such a signal is shown in Fig. 5-1(a). Let xT0 (t) be a periodic signal formed by repeating x(t) with fundamental period T0 as shown in Fig. 5-1(b). If we let T0 → ∞, we have



Fig. 5-1 (a) Nonperiodic signal x(t); (b) periodic signal formed by periodic extension of x(t).

The complex exponential Fourier series of xT0 (t) is given by

where

Since xΤ0(t) = x(t) for |t| < T0/2 and also since x(t) = 0 outside this interval, Eq. (5.24a) can be rewritten as

Let us define X(ω) as

Then from Eq. (5.24b) the complex Fourier coefficients ck can be expressed as

Substituting Eq. (5.26) into Eq. (5.23), we have

or
As T0 → ∞, ω0 = 2π/T0 becomes infinitesimal (ω0 → 0). Thus, let ω0 = Δω. Then Eq. (5.27) becomes

Therefore,

The sum on the right-hand side of Eq. (5.29) can be viewed as the area under the function X(ω) ejωt, as shown in Fig. 5-2. Therefore, we obtain

which is the Fourier representation of a nonperiodic x(t).


Fig. 5-2 Graphical interpretation of Eq. (5.29).

B. Fourier Transform Pair:
The function X(ω) defined by Eq. (5.25) is called the Fourier transform of x(t), and Eq. (5.30) defines the inverse Fourier transform of X(ω). Symbolically they are denoted by

and we say that x(t) and X(ω) form a Fourier transform pair denoted by

C. Fourier Spectra:
The Fourier transform X(ω) of x(t) is, in general, complex, and it can be expressed as

By analogy with the terminology used for the complex Fourier coefficients of a periodic signal x(t), the Fourier transform X(ω) of a nonperiodic signal x(t) is the frequency-domain specification of x(t) and is referred to as the spectrum (or Fourier spectrum) of x(t). The quantity |X(ω)| is called the magnitude spectrum of x(t), and φ(ω) is called the phase spectrum of x(t).
If x(t) is a real signal, then from Eq. (5.31) we get

Then it follows that

and

Hence, as in the case of periodic signals, the amplitude spectrum |X(ω)| is an even function and the phase spectrum φ(ω) is an odd function of ω.
D. Convergence of Fourier Transforms:
Just as in the case of periodic signals, the sufficient conditions for the convergence of X(ω) are the following (again referred to as the Dirichlet conditions):
1. x(t) is absolutely integrable; that is,

2. x(t) has a finite number of maxima and minima within any finite interval.
3. x(t) has a finite number of discontinuities within any finite interval, and each of these discontinuities is finite.
Although the above Dirichlet conditions guarantee the existence of the Fourier transform for a signal, if impulse functions are permitted in the transform, signals which do not satisfy these conditions can have Fourier transforms (Prob. 5.23).
E. Connection between the Fourier Transform and the Laplace Transform:
Equation (5.31) defines the Fourier transform of x(t) as

The bilateral Laplace transform of x(t), as defined in Eq. (4.3), is given by

Comparing Eqs. (5.38) and (5.39), we see that the Fourier transform is a special case of the Laplace transform in which s = jω; that is,

Setting s = σ + jω in Eq. (5.39), we have

or

which indicates that the bilateral Laplace transform of x(t) can be interpreted as the Fourier transform of x(t) e-σt.
Since the Laplace transform may be considered a generalization of the Fourier transform in which the frequency is generalized from jω to s = σ + jω, the complex variable s is often referred to as the complex frequency.
Note that since the integral in Eq. (5.39) is denoted by X(s), the integral in Eq. (5.38) may be denoted as X(jω). Thus, in the remainder of this book both X(ω) and X(jω) mean the same thing whenever we connect the Fourier transform with the Laplace transform. Because the Fourier transform is the Laplace transform with s = jω, it should not be assumed automatically that the Fourier transform of a signal x(t) is the Laplace transform with s replaced by jω. If x(t) is absolutely integrable, that is, if x(t) satisfies condition (5.37), the Fourier transform of x(t) can be obtained from the Laplace transform of x(t) with s = jω. This is not generally true of signals which are not absolutely integrable. The following examples illustrate the above statements.
EXAMPLE 5.1 Consider the unit impulse function δ(t).
From Eq. (3.13) the Laplace transform of δ(t) is

By definitions (5.31) and (1.20) the Fourier transform of δ(t) is

Thus, the Laplace transform and the Fourier transform of δ(t) are the same.
EXAMPLE 5.2 Consider the exponential signal
x(t) = e-atu(t) a > 0
From Eq. (3.8) the Laplace transform of x(t) is given by

By definition (5.31) the Fourier transform of x(t) is

Thus, comparing Eqs. (5.44) and (5.45), we have

Note that x(t) is absolutely integrable.
EXAMPLE 5.3 Consider the unit step function u(t).
From Eq. (3.14) the Laplace transform of u(t) is

The Fourier transform of u (t) is given by (Prob. 5.30)

Thus, the Fourier transform of u (t) cannot be obtained from its Laplace transform. Note that the unit step function u (t) is not absolutely integrable.
5.4 Properties of the Continuous-Time Fourier Transform
Basic properties of the Fourier transform are presented in the following. Many of these properties are similar to those of the Laplace transform (see Sec. 3.4).
A. Linearity:

B. Time Shifting:

Equation (5.50) shows that the effect of a shift in the time domain is simply to add a linear term -ωt0 to the original phase spectrum θ(ω). This is known as a linear phase shift of the Fourier transform X(ω).
C. Frequency Shifting:

The multiplication of x(t) by a complex exponential signal ejω0t is sometimes called complex modulation. Thus, Eq. (5.51) shows that complex modulation in the time domain corresponds to a shift of X(ω) in the frequency domain. Note that the frequency-shifting property Eq. (5.51) is the dual of the time-shifting property Eq. (5.50).
D. Time Scaling:

where a is a real constant. This property follows directly from the definition of the Fourier transform. Equation (5.52) indicates that scaling the time variable t by the factor a causes an inverse scaling of the frequency variable ω by 1/a, as well as an amplitude scaling of X (ω/a) by 1/|a|. Thus, the scaling property (5.52) implies that time compression of a signal (a >1) results in its spectral expansion and that time expansion of the signal (a <1) results in its spectral compression.
E. Time Reversal:

Thus, time reversal of x(t) produces a like reversal of the frequency axis for X(ω). Equation (5.53) is readily obtained by setting a = -1 in Eq. (5.52).
F. Duality (or Symmetry):

The duality property of the Fourier transform has significant implications. This property allows us to obtain both of these dual Fourier transform pairs from one evaluation of Eq. (5.31) (Probs. 5.20 and Probs. 5.22).
G. Differentiation in the Time Domain:

Equation (5.55) shows that the effect of differentiation in the time domain is the multiplication of X(ω) by jω in the frequency domain (Prob. 5.28).
H. Differentiation in the Frequency Domain:

Equation (5.56) is the dual property of Eq. (5.55).
I. Integration in the Time Domain:

Since integration is the inverse of differentiation, Eq. (5.57) shows that the frequency domain operation corresponding to time-domain integration is multiplication by 1/jω, but an additional term is needed to account for a possible dc component in the integrator output. Hence, unless X(0) = 0, a dc component is produced by the integrator (Prob. 5.33).
J. Convolution:

Equation (5.58) is referred to as the time convolution theorem, and it states that convolution in the time domain becomes multiplication in the frequency domain (Prob. 5.31). As in the case of the Laplace transform, this convolution property plays an important role in the study of continuous-time LTI systems (Sec. 5.5) and also forms the basis for our discussion of filtering (Sec. 5.6).
K. Multiplication:

The multiplication property (5.59) is the dual property of Eq. (5.58) and is often referred to as the frequency convolution theorem. Thus, multiplication in the time domain becomes convolution in the frequency domain (Prob. 5.35).
L. Additional Properties:
If x(t) is real, let

where Xe(t) and X0(t) are the even and odd components of x(t), respectively. Let

Then

Equation (5.61a) is the necessary and sufficient condition for x(t) to be real (Prob. 5.39). Equations (5.61b) and (5.61c) show that the Fourier transform of an even signal is a real function of ω and that the Fourier transform of an odd signal is a pure imaginary function of ω.
M. Parseval's Relations:

Equation (5.64) is called Parseval's identity (or Parseval's theorem) for the Fourier transform. Note that the quantity on the left-hand side of Eq. (5.64) is the normalized energy content E of x(t) [Eq. (1.14)]. Parseval's identity says that this energy content E can be computed by integrating |X(ω)|2 over all frequencies ω. For this reason, | X(ω)|2 is often referred to as the energy-density spectrum of x(t), and Eq. (5.64) is also known as the energy theorem.
Table 5-1 contains a summary of the properties of the Fourier transform presented in this section. Some common signals and their Fourier transforms are given in Table 5-2.

TABLE 5-1 Properties of the Fourier Transform



TABLE 5.2 Common Fourier Transforms Pairs


5.5 The Frequency Response of Continuous-Time LTI Systems
A. Frequency Response:
In Sec. 2.2 we showed that the output y(t) of a continuous-time LTI system equals the convolution of the input x(t) with the impulse response h(t); that is,

Applying the convolution property (5.58), we obtain

where Y(ω), X(ω), and H(ω) are the Fourier transforms of y(t), x(t), and h(t), respectively. From Eq. (5.66) we have

The function H(ω) is called the frequency response of the system. Relationships represented by Eqs. (5.65) and (5.66) are depicted in Fig. 5-3. Let

Then |H(ω)| is called the magnitude response of the system, and θH(ω) the phase response of the system.


Fig. 5-3 Relationships between inputs and outputs in an LTI system.

Consider the complex exponential signal

with Fourier transform (Prob. 5.23)

Then from Eqs. (5.66) and (1.26) we have

Taking the inverse Fourier transform of Y(ω), we obtain

which indicates that the complex exponential signal ejω0t is an eigenfunction of the LTI system with corresponding eigenvalue H(ω0), as previously observed in Chap. 2 (Sec. 2.4 and Prob. 2.17]. Furthermore, by the linearity property (5.49), if the input x(t) is periodic with the Fourier series

then the corresponding output y(t) is also periodic with the Fourier series

If x(t) is not periodic, then from Eq. (5.30)

and using Eq. (5.66), the corresponding output y(t) can be expressed as

Thus, the behavior of a continuous-time LTI system in the frequency domain is completely characterized by its frequency response H(ω). Let

Then from Eq. (5.66) we have

Hence, the magnitude spectrum |X(ω)| of the input is multiplied by the magnitude response |H(ω)| of the system to determine the magnitude spectrum |Y(ω)| of the output, and the phase response θH(ω) is added to the phase spectrum θX(ω) of the input to produce the phase spectrum θY(ω) of the output. The magnitude response |H(ω)| is sometimes referred to as the gain of the system.
B. Distortionless Transmission:
For distortionless transmission through an LTI system we require that the exact input signal shape be reproduced at the output, although its amplitude may be different and it may be delayed in time. Therefore, if x(t) is the input signal, the required output is

where td is the time delay and K (> 0) is a gain constant. This is illustrated in Figs. 5-4(a) and (b). Taking the Fourier transform of both sides of Eq. (5.79), we get

Thus, from Eq. (5.66) we see that for distortionless transmission, the system must have

Thus,

That is, the amplitude of H(ω) must be constant over the entire frequency range, and the phase of H(ω) must be linear with the frequency. This is illustrated in Figs. 5-4(c) and (d).


Fig. 5-4 Distortionless transmission.

Amplitude Distortion and Phase Distortion:
When the amplitude spectrum |H(ω)| of the system is not constant within the frequency band of interest, the frequency components of the input signal are transmitted with a different amount of gain or attenuation. This effect is called amplitude distortion. When the phase spectrum θH(ω) of the system is not linear with the frequency, the output signal has a different waveform than the input signal because of different delays in passing through the system for different frequency components of the input signal. This form of distortion is called phase distortion.
C. LTI Systems Characterized by Differential Equations:
As discussed in Sec. 2.5, many continuous-time LTI systems of practical interest are described by linear constant-coefficient differential equations of the form

with M ≤ N. Taking the Fourier transform of both sides of Eq. (5.83) and using the linearity property (5.49) and the time-differentiation property (5.55), we have

or

Thus, from Eq. (5.67)

which is a rational function of ω. The result (5.85) is the same as the Laplace transform counterpart H(s) = Y(s)/X(s) with s = jω [Eq. (3.40)]; that is,
H(ω) = H(s)|s = jω = H(jω)
5.6 Filtering
One of the most basic operations in any signal processing system is filtering. Filtering is the process by which the relative amplitudes of the frequency components in a signal are changed or perhaps some frequency components are suppressed. As we saw in the preceding section, for continuous-time LTI systems, the spectrum of the output is that of the input multiplied by the frequency response of the system. Therefore, an LTI system acts as a filter on the input signal. Here the word "filter" is used to denote a system that exhibits some sort of frequency-selective behavior.
A. Ideal Frequency-Selective Filters:
An ideal frequency-selective filter is one that exactly passes signals at one set of frequencies and completely rejects the rest. The band of frequencies passed by the filter is referred to as the pass band, and the band of frequencies rejected by the filter is called the stop band.
The most common types of ideal frequency-selective filters are the following.
1. Ideal Low-Pass Filter:
An ideal low-pass filter (LPF) is specified by

which is shown in Fig. 5-5(a). The frequency ωc is called the cutoff frequency.
2. Ideal High-Pass Filter:
An ideal high-pass filter (HPF) is specified by

which is shown in Fig. 5-5(b).
3. Ideal Bandpass Filter:
An ideal bandpass filter (BPF) is specified by

which is shown in Fig. 5-5(c).
4. Ideal Bandstop Filter:
An ideal bandstop filter (BSF) is specified by

which is shown in Fig. 5-5(d).


Fig. 5-5 Magnitude responses of ideal frequency-selective filters.

In the above discussion, we said nothing regarding the phase response of the filters. To avoid phase distortion in the filtering process, a filter should have a linear phase characteristic over the pass band of the filter; that is [Eq. (5.82b)],

where td is a constant.
Note that all ideal frequency-selective filters are noncausal systems.
B. Nonideal Frequency-Selective Filters:
As an example of a simple continuous-time causal frequency-selective filter, we consider the RC filter shown in Fig. 5-6(a). The output y(t) and the input x(t) are related by (Prob. 1.32)

Taking the Fourier transforms of both sides of the above equation, the frequency response H(ω) of the RC filter is given by

where ω0 = 1/RC. Thus, the amplitude response |H(ω)| and phase response θH(ω) are given by


which are plotted in Fig. 5-6(b). From Fig. 5-6(b) we see that the RC network in Fig. 5-6(a) performs as a low-pass filter.


Fig. 5-6 RC filter and its frequency response.

5.7 Bandwidth
A. Filter (or System) Bandwidth:
One important concept in system analysis is the bandwidth of an LTI system. There are many different definitions of system bandwidth.
1. Absolute Bandwidth:
The bandwidth WB of an ideal low-pass filter equals its cutoff frequency; that is, WB = ωc [Fig. 5-5(a)]. In this case WB is called the absolute bandwidth. The absolute bandwidth of an ideal bandpass filter is given by WB = ω2 - ω1 [Fig. 5-5(c)]. A bandpass filter is called narrowband if WB  ω0, where ω0 =  (ω1 + ω2) is the center frequency of the filter. No bandwidth is defined for a high-pass or a bandstop filter.
2. 3-dB (or Half-Power) Bandwidth:
For causal or practical filters, a common definition of filter (or system) bandwidth is the 3-dB bandwidth W3 dB. In the case of a low-pass filter, such as the RC filter described by Eq. (5.92) or in Fig. 5-6(b), W3 dB is defined as the positive frequency at which the amplitude spectrum |H(ω)| drops to a value equal to |H(0)|/, as illustrated in Fig. 5-7(a). Note that |H(0)| is the peak value of H(ω) for the low-pass RC filter. The 3-dB bandwidth is also known as the half-power bandwidth because a voltage or current attenuation of 3 dB is equivalent to a power attenuation by a factor of 2. In the case of a bandpass filter, W3 dB is defined as the difference between the frequencies at which |H(ω)| drops to a value equal to 1/ times the peak value |H(ωm)| as illustrated in Fig. 5-7(b). This definition of W3 dB is useful for systems with unimodal amplitude response (in the positive frequency range) and is a widely accepted criterion for measuring a system's bandwidth, but it may become ambiguous and nonunique with systems having multiple peak amplitude responses.
Note that each of the preceding bandwidth definitions is defined along the positive frequency axis only and always defines positive frequency, or one-sided, bandwidth only.


Fig. 5-7 Filter bandwidth.

B. Signal Bandwidth:
The bandwidth of a signal can be defined as the range of positive frequencies in which "most" of the energy or power lies. This definition is rather ambiguous and is subject to various conventions (Probs. 5.57 and Probs. 5.76).
3-dB Bandwidth:
The bandwidth of a signal x(t) can also be defined on a similar basis as a filter bandwidth such as the 3-dB bandwidth, using the magnitude spectrum |X(ω)| of the signal. Indeed, if we replace |H(ω)| by |X(ω)| in Figs. 5-5(a) to (c), we have frequency-domain plots of low-pass, high-pass, and bandpass signals.
Band-Limited Signal:
A signal x(t) is called a band-limited signal if

Thus, for a band-limited signal, it is natural to define ωM as the bandwidth.
SOLVED PROBLEMS
Fourier Series
5.1. We call a set of signals {Ψn(t)} orthogonal on an interval (a, b) if any two signals Ψm(t) and Ψk(t) in the set satisfy the condition

where * denotes the complex conjugate and α ≠ 0. Show that the set of complex exponentials {ejkω0t: k = 0, ± 1, ± 2, ...} is orthogonal on any interval over a period T0, where T0 = 2π/ω0.
For any t0 we have

since ejm2π = 1. When m = 0, we have ejmω0t|m = 0 = 1 and

Thus, from Eqs. (5.96) and (5.97) we conclude that

which shows that the set {ejkω0t: k = 0, ± 1, ± 2, ...} is orthogonal on any interval over a period T0.
5.2. Using the orthogonality condition (5.98), derive Eq. (5.5) for the complex Fourier coefficients.
From Eq. (5.4)

Multiplying both sides of this equation by e-jmω0t and integrating the result from t0 to (t0 + T0), we obtain

Then by Eq. (5.98), Eq. (5.99) reduces to

Changing index m to k, we obtain Eq. (5.5); that is,

We shall mostly use the following two special cases for Eq. (5.101): t0 = 0 and t0 = -T0/2, respectively. That is,

5.3. Derive the trigonometric Fourier series Eq. (5.8) from the complex exponential Fourier series Eq. (5.4).
Rearranging the summation in Eq. (5.4) as

and using Euler's formulas
e±jkω0t = cos kω0t ± j sin kω0t
we have

Setting

Eq. (5.103) becomes

5.4.   Determine the complex exponential Fourier series representation for each of the following signals:
(a)   x(t) = cos ω0t
(b)   x(t) = sin ω0t
(c)
(d)   x(t) = cos 4t + sin 6t
(e)   x(t) = sin2t
(a)   Rather than using Eq. (5.5) to evaluate the complex Fourier coefficients ck using Euler's formula, we get

Thus, the complex Fourier coefficients for cos ω0t are

(b) In a similar fashion we have

Thus, the complex Fourier coefficients for sin ω0 t are

(c) The fundamental angular frequency ω0 of x(t) is 2. Thus,

Now

Thus, the complex Fourier coefficients for cos(2t + π/4) are

(d) By the result from Prob. 1.14 the fundamental period T0 of x(t) is π and ω0 = 2π/T 0 = 2. Thus,

Again using Euler's formula, we have

Thus, the complex Fourier coefficients for cos 4t + sin 6t are

and all other ck = 0.
(e) From Prob. 1.16(e) the fundamental period T0 of x(t) is π and ω0 = 2π/T0 = 2. Thus,

Again using Euler's formula, we get

Thus, the complex Fourier coefficients for sin2 t are

and all other ck = 0.
5.5.   Consider the periodic square wave x(t) shown in Fig. 5-8.
(a)   Determine the complex exponential Fourier series of x(t).
(b)   Determine the trigonometric Fourier series of x(t).


Fig. 5-8

(a) Let

Using Eq. (5.102a), we have

since ω0T0 = 2π and e-jkπ = (- 1)k. Thus,

Hence,

and we obtain

(b) From Eqs. (5.105), (5.10), and (5.12) we have

Substituting these values in Eq. (5.8), we get

5.6.   Consider the periodic square wave x(t) shown in Fig. 5-9.
(a)   Determine the complex exponential Fourier series of x(t).
(b)   Determine the trigonometric Fourier series of x(t).


Fig. 5-9

(a)   Let

Using Eq. (5.102b), we have

Thus,

Hence,

and we obtain

(b) From Eqs. (5.108), (5.10), and (5.12) we have

Substituting these values into Eq. (5.8), we obtain

Note that x(t) is even; thus, x(t) contains only a dc term and cosine terms. Note also that x(t) in Fig. 5-9 can be obtained by shifting x(t) in Fig. 5-8 to the left by T0/4.
5.7. Consider the periodic square wave x(t) shown in Fig. 5-10.
(a) Determine the complex exponential Fourier series of x(t).
(b) Determine the trigonometric Fourier series of x(t).
Note that x(t) can be expressed as
x(t) = x1(t) - A
where x1(t) is shown in Fig. 5-11. Now comparing Fig. 5-11 and Fig. 5-8 in Prob. 5.5, we see that x1(t) is the same square wave of x(t) in Fig. 5-8 except that A becomes 2A.


Fig. 5-10



Fig. 5-11

(a) Replacing A by 2A in Eq. (5.106), we have

Thus,

(b) Similarly, replacing A by 2A in Eq. (5.107), we have

Thus,

Note that x(t) is odd; thus, x(t) contains only sine terms.
5.8. Consider the periodic impulse train δT0(t) shown in Fig. 5-12 and defined by



Fig. 5-12

(a) Determine the complex exponential Fourier series of δT0(t).
(b) Determine the trigonometric Fourier series of δT0(t).
(a) Let

Since δ(t) is involved, we use Eq. (5.102b) to determine the Fourier coefficients and we obtain

Hence, we get

(b) Let

Since δT0(t) is even, bk = 0, and by Eq. (5.9a), ak are given by

Thus, we get

5.9. Consider the triangular wave x(t) shown in Fig. 5-13(a). Using the differentiation technique, find (a) the complex exponential Fourier series of x(t), and (b) the trigonometric Fourier series of x(t).
The derivative x'(t) of the triangular wave x(t) is a square wave as shown in Fig. 5-13(b).
(a) Let

Differentiating Eq. (5.118), we obtain



Fig. 5-13

Equation (5.119) shows that the complex Fourier coefficients of x′(t) equal jkω0ck. Thus, we can find ck(k ≠ 0) if the Fourier coefficients of x′(t) are known. The term c0 cannot be determined by Eq. (5.119) and must be evaluated directly in terms of X(t) with Eq. (5.6). Comparing Fig. 5-13(b) and Fig. 5-10, we see that x′(t) in Fig. 5-13(b) is the same as x(t) in Fig. 5-10 with A replaced by 2A/T0. Hence, from Eq. (5.111), replacing A by 2A/T0, we have

Equating Eqs. (5.119) and (5.120), we have

From Fig. 5-13(a) and Eq. (5.6) we have

Substituting these values into Eq. (5.118), we obtain

(b) In a similar fashion, differentiating Eq. (5.8), we obtain

Equation (5.122) shows that the Fourier cosine coefficients of x′(t) equal to kω0bk and that the sine coefficients equal to -kω0ak. Hence, from Eq. (5.112), replacing A by 2A/T0, we have

Equating Eqs. (5.122) and (5.123), we have

From Eqs. (5.6) and (5.10) and Fig. 5-13(a) we have

Substituting these values into Eq. (5.8), we get

5.10. Consider the triangular wave x(t) shown in Fig. 5-14(a). Using the differentiation technique, find the triangular Fourier series of x(t).
From Fig. 5-14(a) the derivative x′(t) of the triangular wave x(t) is, as shown in Fig. 5-14(b),


Fig. 5-14


Using Eq. (5.117), Eq. (5.125) becomes

Equating Eqs. (5.126) and (5.122), we have

From Fig. 5-14(a) and Eq. (5.9a), we have

Thus, substituting these values into Eq. (5.8), we get

5.11. Find and sketch the magnitude spectra for the periodic square pulse train signal x(t) shown in Fig. 5-15(a) for (a) d = T0/4, and (b) d = T0/8.
Using Eq. (5.102a), we have



Fig. 5-15

Note that ck = 0 whenever kω0d/2 = mπ; that is,

(a)   d = T0/4, kω0d/2 = kπd/T0 = kπ/4,

The magnitude spectrum for this case is shown in Fig. 5-15(b).
(b)   d = T0/8, kω0d/2 = kπd/T0 = kπ/8,

The magnitude spectrum for this case is shown in Fig. 5-15(c).
5.12. If x1(t) and x2(t) are periodic signals with fundamental period T0 and their complex Fourier series expressions are

show that the signal x(t) = x1(t)x2(t) is periodic with the same fundamental period T0 and can be expressed as

where ck is given by

Now
X(t + T0) = x1(t + T0)x2(t + T0) = x1(t)x2(t) = x(t)
Thus, X(t) is periodic with fundamental period T0. Let

Then

since

and the term in brackets is equal to ek-m.
5.13. Let x1(t) and x2(t) be the two periodic signals in Prob. 5.12. Show that

Equation (5.130) is known as Parseval's relation for periodic signals.
From Prob. 5.12 and Eq. (5.129) we have

Setting k = 0 in the above expression, we obtain

5.14. Verify Parseval's identity (5.21) for the Fourier series; that is,

If

then

where * denotes the complex conjugate. Equation (5.131) indicates that if the Fourier coefficients of x(t) are ck, then the Fourier coefficients of x*(t) are c*-k. Setting x1(t) = x(t) and x2(t) = x*(t) in Eq. (5.130), we have dk = ck and ek = c*-k or (e-k = c*k), and we obtain

or

5.15. (a) The periodic convolution ƒ(t) = x1(t)  x2(t) was defined in Prob. 2.8. If dn and en are the complex Fourier coefficients of x1(t) and x2(t), respectively, then show that the complex Fourier coefficients ck of ƒ(t) are given by

where T0 is the fundamental period common to x1(t), x2(t), and ƒ(t).
(b) Find the complex exponential Fourier series of ƒ(t) defined in Prob. 2.8(c).
(a) From Eq. (2.70) (Prob. 2.8)

Let

Then

Since

we get

which shows that the complex Fourier coefficients ck of ƒ(t) equal T0dkek.
(b) In Prob. 2.8(c), x1(t) = x2(t) = x(t), as shown in Fig. 2-12, which is the same as Fig. 5-8 (Prob. 5.5). From Eq. (5.105) we have

Thus, by Eq. (5.133) the complex Fourier coefficients ck of ƒ(t) are

Note that in Prob. 2.8(c), ƒ(t) = x1(t)  x2(t), shown in Fig. 2-13(b), is proportional to x(t), shown in Fig. 5-13(a). Thus, replacing A by A2T0/2 in the result from Prob. 5.9, we get

which are the same results obtained by using Eq. (5.133).
Fourier Transform
5.16. (a) Verify the time-shifting property (5.50); that is,

By definition (5.31)

By the change of variable τ = t - t0, we obtain

Hence,
x(t - t0)↔e-jωt0 X(ω)
5.17. Verify the frequency-shifting property (5.51); that is,
x(t ejωt0 ↔ X(ω - ω0)
By definition (5.31)

Hence,
x(t ejωt0 ↔ X(ω - ω0)
5.18. Verify the duality property (5.54); that is,
X(t) ↔ 2πx(-ω)
From the inverse Fourier transform definition (5.32), we have

Changing t to -t, we obtain

Now interchanging t and ω, we get

Since

we conclude that
X(t) ↔ 2πx(-ω)
5.19. Find the Fourier transform of the rectangular pulse signal x(t) [Fig. 5-16(a)] defined by

By definition (5.31)

Hence, we obtain

The Fourier transform X(ω) of X(t) is sketched in Fig. 5-16(b).


Fig. 5-16 Rectangular pulse and its Fourier transform.

5.20. Find the Fourier transform of the signal [Fig. 5-17(a)]

From Eq. (5.136) we have

Now by the duality property (5.54), we have

Dividing both sides by 2π (and by the linearity property), we obtain

where pa(ω) is defined by [see Eq. (5.135) and Fig. 5-17(b)]



Fig. 5-17 sin at/πt and its Fourier transform.

5.21. Find the Fourier transform of the signal [Fig. 5-18(a)]
x(t) = e-a|t|      a > 0
Signal x(t) can be rewritten as

Then

Hence, we get

The Fourier transform X(ω) of x(t) is shown in Fig. 5-18(b).(5.138)


Fig. 5-18 e-|a|t and its Fourier transform.

5.22. Find the Fourier transform of the signal [Fig. 5-19(a)]

From Eq. (5.138) we have

Now by the duality property (5.54) we have

Dividing both sides by 2a, we obtain

The Fourier transform X(ω) of x(t) is shown in Fig. 5-19(b).


Fig. 5-19 1/(a2 = t2) and its Fourier transform.

5.23. Find the Fourier transforms of the following signals:
(a)   x(t) = 1
(b)   x(t) = e-jω0t
(c)   x(t) = e-jω0t
(d)   x(t) = cos ω0t
(e)   x(t) = sin ω0t
(a)   By Eq. (5.43) we have

Thus, by the duality property (5.54) we get


Figs. 5-20(a) and (b) illustrate the relationships in Eqs. (5.140) and (5.141), respectively.



Fig. 5-20 (a) Unit impulse and its Fourier transform; (b) constant (dc) signal and its Fourier transform.

(b) Applying the frequency-shifting property (5.51) to Eq. (5.141), we get

(c) From Eq. (5.142), it follows that

(d) From Euler's formula we have

Thus, using Eqs. (5.142) and (5.143) and the linearity property (5.49), we get

Fig. 5-21 illustrates the relationship in Eq. (5.144).
(e) Similarly, we have

and again using Eqs. (5.142) and (5.143), we get



Fig. 5-21 Cosine signal and its Fourier transform.

5.24. Find the Fourier transform of a periodic signal x(t) with period T0.
We express x(t) as

Taking the Fourier transform of both sides and using Eq. (5.142) and the linearity property (5.49), we get

which indicates that the Fourier transform of a periodic signal consists of a sequence of equidistant impulses located at the harmonic frequencies of the signal.
5.25. Find the Fourier transform of the periodic impulse train [Fig. 5-22(a)]

From Eq. (5.115) in Prob. 5.8, the complex exponential Fourier series of δT0(t) is given by

Using Eq. (5.146), we get

or

Thus, the Fourier transform of a unit impulse train is also a similar impulse train [Fig. 5-22(b)].


Fig. 5-22 Unit impulse train and its Fourier transform.

5.26. Show that

and

Equation (5.148) is known as the modulation theorem.
From Euler's formula we have

Then by the frequency-shifting property (5.51) and the linearity property (5.49), we obtain

Hence,

In a similar manner we have

and

Hence,

5.27. The Fourier transform of a signal x(t) is given by [Fig. 5-23(a)]

Find and sketch x(t).
From Eq. (5.137) and the modulation theorem (5.148), it follows that

which is sketched in Fig. 5-23(b).


Fig. 5-23

5.28. Verify the differentiation property (5.55); that is,

From Eq. (5.32) the inverse Fourier transform of X(ω) is

Then

Comparing Eq. (5.151) with Eq. (5.150), we conclude that dx(t)/dt is the inverse Fourier transform of jωX(ω). Thus,

5.29. Find the Fourier transform of the signum function, sgn(t) (Fig. 5-24), which is defined as



Fig. 5-24 Signum function.

The signum function, sgn(t), can be expressed as
sgn(t) = 2u(t) - 1
Using Eq. (1.30), we have

Let
sgn(t) ↔ X(ω)
Then applying the differentiation property (5.55), we have

Hence,

Note that sgn(t) is an odd function, and therefore its Fourier transform is a pure imaginary function of ω (Prob. 5.41).
5.30. Verify Eq. (5.48); that is,

As shown in Fig. 5-25, u(t) can be expressed as

Note that  is the even component of u (t) and  sgn(t) is the odd component of u(t). Thus, by Eqs. (5.141) and (5.153) and the linearity property (5.49), we obtain



Fig. 5-25 Unit step function and its even and odd components.

5.31. Prove the time convolution theorem (5.58); that is,
x1(t) * x2(t) ↔ X1(ω) X2(ω)
By definitions (2.6) and (5.31), we have

Changing the order of integration gives

By the time-shifting property (5.50)

Thus, we have

Hence,
x1 (t) * x2 (t) ↔ X1 (ω) X2 (ω)
5.32. Using the time convolution theorem (5.58), find the inverse Fourier transform of X(ω) = 1/(a + jω)2.
From Eq. (5.45) we have

Now

Thus, by the time convolution theorem (5.58) we have

Hence,

5.33. Verify the integration property (5.57); that is,

From Eq. (2.60) we have

Thus, by the time convolution theorem (5.58) and Eq. (5.154), we obtain

since X(ω)δ(ω) = X(0)δ(ω) by Eq. (1.25). Thus,

5.34. Using the integration property (5.57) and Eq. (1.31), find the Fourier transform of u(t).
From Eq. (1.31) we have

Now from Eq. (5.140) we have
δ(t) ↔ 1
Setting x(τ) = δ(τ) in Eq. (5.57), we have

and

5.35. Prove the frequency convolution theorem (5.59); that is,

By definitions (5.31) and (5.32) we have

Hence,

5.36. Using the frequency convolution theorem (5.59), derive the modulation theorem (5.148).
From Eq. (5.144) we have
cos ω0t ↔ π δ (ω - ω0) + π δ (ω + ω0)
By the frequency convolution theorem (5.59) we have

The last equality follows from Eq. (2.59).
5.37. Verify Parseval's relation (5.63); that is,

From the frequency convolution theorem (5.59) we have

that is,

Setting ω = 0, we get

By changing the dummy variable of integration, we obtain

5.38. Prove Parseval's identity [Eq. (5.64)] or Parseval's theorem for the Fourier transform; that is,

By definition (5.31) we have

where * denotes the complex conjugate. Thus,

Setting x1(t) = x(t) and x2(t) = x*(t) in Parseval's relation (5.63), we get

or

5.39. Show that Eq. (5.61a); that is,
X*(ω) = X(-ω)
is the necessary and sufficient condition for x(t) to be real.
By definition (5.31)

If x(t) is real, then x*(t) = x(t) and

Thus, X*(ω) = X(-ω) is the necessary condition for x(t) to be real. Next assume that X*(ω) = X(-ω). From the inverse Fourier transform definition (5.32)

Then

which indicates that x(t) is real. Thus, we conclude that
X*(ω) = X(-ω)
is the necessary and sufficient condition for x(t) to be real.
5.40. Find the Fourier transforms of the following signals:
(a) x(t) = u(-t)
(b) x(t) = eatu (-t), a > 0
From Eq. (5.53) we have
x(-t) ↔ X(-ω)
Thus, if x(t) is real, then by Eq. (5.61a) we have

(a) From Eq. (5.154)

Thus, by Eq. (5.158) we obtain

(b) From Eq. (5.155)

Thus, by Eq. (5.158) we get

5.41. Consider a real signal x(t) and let

and
x(t) = xe(t) + x0(t)
where xe(t) and xo(t) are the even and odd components of x(t), respectively. Show that

From Eqs. (1.5) and (1.6) we have

Now if x(t) is real, then by Eq. (5.158) we have

Thus, we conclude that

Equations (5.161a) and (5.161b) show that the Fourier transform of a real even signal is a real function of ω, and that of a real odd signal is an imaginary function of ω, respectively.
5.42. Using Eqs. (5.161a) and (5.155), find the Fourier transform of e-a|t|(a > 0).
From Eq. (5.155) we have

By Eq. (1.5) the even component of e-atu(t) is given by

Thus, by Eq. (5.161a) we have

or

which is the same result obtained in Prob. 5.21 [Eq. (5.138)].
5.43. Find the Fourier transform of a Gaussian pulse signal

By definition (5.31)

Taking the derivative of both sides of Eq. (5.162) with respect to ω, we have

Now, using the integration by parts formula

and letting

we have

and

since a > 0. Thus, we get

Solving the above separable differential equation for X(ω), we obtain

where A is an arbitrary constant. To evaluate A, we proceed as follows. Setting ω = 0 in Eq. (5.162) and by a change of variable, we have

Substituting this value of A into Eq. (5.163), we get

Hence, we have

Note that the Fourier transform of a Gaussian pulse signal is also a Gaussian pulse in the frequency domain. Fig. 5-26 shows the relationship in Eq. (5.165).


Fig. 5-26 Gaussian pulse and its Fourier transform.

Frequency Response
5.44. Using the Fourier transform, redo Prob. 2.25.
The system is described by
y′(t) + 2y(t) = x(t) + x′(t)
Taking the Fourier transforms of the above equation, we get
jωY(ω) + 2Y(ω) = X(ω) + jωX(ω)
or
(j + 2) Y(ω) = (1 + jω) X(ω)
Hence, by Eq. (5.67) the frequency response H(ω) is

Taking the inverse Fourier transform of H(ω), the impulse response h(t) is
h(t) = δ(t) - e-2t u(t)
Note that the procedure is identical to that of the Laplace transform method with s replaced by jω (Prob. 3.29).
5.45. Consider a continuous-time LTI system described by

Using the Fourier transform, find the output y(t) to each of the following input signals:
(a) x(t) = e-tu(t)
(b) x(t) = u(t)
(a) Taking the Fourier transforms of Eq. (5.166), we have
jωY(ω) = 2Y(ω) = X(ω)
Hence,

From Eq. (5.155)

and

Therefore,
y(t) = (e-t - e-2t) u(t)
(b) From Eq. (5.154)

Thus, by Eq. (5.66) and using the partial-fraction expansion technique, we have

where we used the fact that ƒ(ω)δ(ω) = ƒ(0)δ(ω) [Eq. (1.25)]. Thus,

We observe that the Laplace transform method is easier in this case because of the Fourier transform of u(t).
5.46. Consider the LTI system in Prob. 5.45. If the input x(t) is the periodic square waveform shown in Fig. 5-27, find the amplitude of the first and third harmonics in the output y(t).


Fig. 5-27

Note that X(t) is the same x(t) shown in Fig. 5-8 [Prob. 5.5]. Thus, setting A = 10, T0 = 2, and ω0 = 2π/T0 = π in Eq. (5.106), we have

Next, from Prob. 5.45

Thus, by Eq. (5.74) we obtain

Let

The harmonic form of y(t) is given by [Eq. (5.15)]

where Dk is the amplitude of the kth harmonic component of y(t). By Eqs. (5.11) and (5.16), Dk and dk are related by

Thus, from Eq. (5.167), with m = 0, we obtain

With m = 1, we obtain

5.47. The most widely used graphical representation of the frequency response H(ω) is the Bode plot in which the quantities 20 log10|H(ω)| and θH(ω) are plotted versus ω, with ω plotted on a logarithmic scale. The quantity 20 log10|H(ω)| is referred to as the magnitude expressed in decibels (dB), denoted by |H(ω)|dB. Sketch the Bode plots for the following frequency responses:

For ω  10,

For ω  10,

On a log frequency scale, 20 log10(ω/10) is a straight line with a slope of 20 dB/decade (a decade is a 10-to-1 change in frequency). This straight line intersects the 0-dB axis at ω = 10 [Fig. 5-28(a)]. (This value of ω is called the corner frequency.) At the corner frequency ω = 10

The plot of |H(ω)|dB is sketched in Fig. 5-28(a). Next,

Then

At ω = 10, θH(10) = tan-1 1 = π/4 radian (rad). The plot of θH(ω) is sketched in Fig. 5-28(b). Note that the dotted lines represent the straight-line approximation of the Bode plots.

For ω  100,

For ω  100,



Fig. 5-28 Bode plots.

On a log frequency scale -20 log10(ω/100) is a straight line with a slope of -20 dB/decade. This straight line intersects the 0-dB axis at the corner frequency ω = 100 [Fig. 5-29(a)]. At the corner frequency ω = 100

The plot of |H(ω)|dB is sketched in Fig. 5-29(a). Next,

Then

At ω = 100, θH(100) = -tan-1 1 = -π/4 rad. The plot of θH(ω) is sketched in Fig. 5-29(b).
(c) First, we rewrite H(ω) in standard form as

Then

Note that there are three corner frequencies, ω = 1, ω = 10, and ω = 100. At corner frequency ω = 1




Fig. 5-29 Bode plots.

At corner frequency ω = 10

At corner frequency ω = 100

The Bode amplitude plot is sketched in Fig. 5-30(a). Each term contributing to the overall amplitude is also indicated. Next,

Then

and

The plot of θH(ω) is sketched in Fig. 5-30(b).


Fig. 5-30 Bode plots.

5.48. An ideal (-π/2) radian (or - 90°) phase shifter (Fig. 5-31) is defined by the frequency response


Fig. 5-31 -π/2 rad phase shifter.


(a) Find the impulse response h(t) of this phase shifter.
(b) Find the output y(t) of this phase shifter due to an arbitrary input x(t).
(c) Find the output y(t) when x(t) = cos ω0t.
(a) Since e-jπ/2 = -j and e-jπ/2 = j, H(ω) can be rewritten as

where

Now from Eq. (5.153)

and by the duality property (5.54) we have

or

since sgn(ω) is an odd function of ω. Thus, the impulse response h(t) is given by

(b) By Eq. (2.6)

The signal y(t) defined by Eq. (5.174) is called the Hilbert transform of x(t) and is usually denoted by 
(c) From Eq. (5.144)

Then

since sgn(ω0) = 1 and sgn(-ω0) = -1. Thus, from Eq. (5.145) we get
y(t) = sin ω0t
Note that cos(ω0t π/2) = sin ω0t.
5.49. Consider a causal continuous-time LTI system with frequency response
H(ω) = A(ω) = jB(ω)
Show that the impulse response h(t) of the system can be obtained in terms of A(ω) or B(ω) alone.
Since the system is causal, by definition
h(t) = 0     t < 0
Accordingly,
h(=t) = 0     t > 0
Let
h(t) = he(t) + ho(t)
where he(t) and ho(t) are the even and odd components of h(t), respectively. Then from Eqs. (1.5) and (1.6) we can write

From Eqs. (5.61b) and (5.61c) we have

Thus, by Eq. (5.175)

Equation (5.176a) and (5.176b) indicate that h(t) can be obtained in terms of A(ω) or B(ω) alone.
5.50. Consider a causal continuous-time LTI system with frequency response
H(ω) = A(ω) = jB(ω)
If the impulse response h(t) of the system contains no impulses at the origin, then show that A(ω) and B(ω) satisfy the following equation:

As in Prob. 5.49, let
h(t) = he(t) + ho(t)
Since h(t) is causal, that is, h(t) = 0 for t < 0, we have
he(t) = - ho(t)     t < 0
Also from Eq. (5.175) we have
he(t) = ho(t)     t > 0
Thus, using Eq. (5.152), we can write

Now, from Eqs. (5.61b), (5.61c), and (5.153) we have

Thus, by the frequency convolution theorem (5.59) we obtain

and

or

Note that A(ω) is the Hilbert transform of B(ω) [Eq. (5.174)] and that B(ω) is the negative of the Hilbert transform of A(ω).
5.51. The real part of the frequency response H(ω) of a causal LTI system is known to be πδ(ω). Find the frequency response H(ω) and the impulse function h(t) of the system.
Let
H(ω) = A(ω) = jB(ω)
Using Eq. (5.177b), with A(ω) = πδ (ω), we obtain

Hence,

and by Eq. (5.154)
h(t) = u(t)
Filtering
5.52. Consider an ideal low-pass filter with frequency response

The input to this filter is

(a) Find the output y(t) for a < ωc.
(b) Find the output y(t) for a > ωc.
(c) In which case does the output suffer distortion?
(a) From Eq. (5.137) (Prob. 5.20) we have

Then when a < ωc, we have

Thus,

(b) When a > ωc, we have

Thus,

(c) In case (a), that is, when ωc > a, y(t) = x(t) and the filter does not produce any distortion. In case (b), that is, when ωc < a, y(t) = h(t) and the filter produces distortion.
5.53. Consider an ideal low-pass filter with frequency response

The input to this filter is the periodic square wave shown in Fig. 5-27. Find the output y(t).
Setting A = 10, T0 = 2, and ω0 2π/T0 π in Eq. (5.107) (Prob. 5.5), we get

Since the cutoff frequency ωc of the filter is 4π rad, the filter passes all harmonic components of x(t) whose angular frequencies are less than 4p rad and rejects all harmonic components of x(t) whose angular frequencies are greater than 4π rad. Therefore,

5.54. Consider an ideal low-pass filter with frequency response

The input to this filter is
x(t) = e-2tu(t)
Find the value of ωc such that this filter passes exactly one-half of the normalized energy of the input signal x(t).
From Eq. (5.155)

Then

The normalized energy of x(t) is

Using Parseval's identity (5.64), the normalized energy of y(t) is

from which we obtain

5.55. The equivalent bandwidth of a filter with frequency response H(ω) is defined by

where |H(ω)|max denotes the maximum value of the magnitude spectrum. Consider the low-pass RC filter shown in Fig. 5-6(a).
(a) Find its 3-dB bandwidth W3 dB.
(b) Find its equivalent bandwidth Weq.
(a) From Eq. (5.91) the frequency response H(ω) of the RC filter is given by

where ω0 1/RC. Now

The amplitude spectrum |H(ω)| is plotted in Fig. 5-6(b). When ω ω = ω0 = 1/RC, |H(ω0)| 1√2. Thus, the 3-dB bandwidth of the RC filter is given by

(b) From Fig. 5-6(b) we see that |H(0)| 1 is the maximum of the magnitude spectrum. Rewriting H(ω) as

and using Eq. (5.179), the equivalent bandwidth of the RC filter is given by (Fig. 5-32)



Fig. 5-32 Filter bandwidth.

5.56. The risetime tr of the low-pass RC filter in Fig. 5-6(a) is defined as the time required for a unit step response to go from 10 to 90 percent of its final value. Show that

where ƒ3 dB = W3 dB/2π = 1/2πRC is the 3-dB bandwidth (in hertz) of the filter.
From the frequency response H(ω) of the RC filter, the impulse response is

Then, from Eq. (2.12) the unit step response s(t) is found to be

which is sketched in Fig. 5-33. By definition of the risetime
tr = t2 - t1
where
s(t1) = 1 -e-t1/RC = 0.1 → e-t1/RC = 0.9
s(t2) = 1 - e-t2/RC = 0.9 → e-t2/RC = 0.1
Dividing the first equation by the second equation on the right-hand side, we obtain
e(t2 - t1)/RC = 9
and

which indicates the inverse relationship between bandwidth and risetime.


Fig. 5-33

5.57. Another definition of bandwidth for a signal X(t) is the 90 percent energy containment bandwidth W 90, defined by

where Ex is the normalized energy content of signal X(t). Find the W 90 for the following signals:
(a) x(t) = e-atu(t), a > 0
(b) 
(a) From Eq. (5.155)

From Eq. (1.14)

Now, by Eq. (5.180)

from which we get

Thus,

(b) From Eq. (5.137)

Using Parseval's identity (5.64), we have

Then, by Eq. (5.180)

from which we get

Note that the absolute bandwidth of X(t) is a (radians/second).
5.58. Let X(t) be a real-valued band-limited signal specified by [Fig. 5-34(b)]

Let xs(t) be defined by

(a) Sketch xs(t) for Ts < π/ωM and for Ts > π/ωM.
(b) Find and sketch the Fourier spectrum Xs(ω) of xs(t) for Ts < π/ωM and for Ts > π/ωM.
(a) Using Eq. (1.26), we have

The sampled signal xs(t) is sketched in Fig. 5-34(c) for Ts < π/ωM, and in Fig. 5-34(i) for Ts > π/ωM.
The signal xs(t) is called the ideal sampled signal, Ts is referred to as the sampling interval (or period), and ƒs = 1/Ts is referred to as the sampling rate (or frequency).
(b) From Eq. (5.147) (Prob. 5.25) we have

Let
xs(t) ↔ Xs(ω)
Then, according to the frequency convolution theorem (5.59), we have

Using Eq. (1.26), we obtain

which shows that Xs(ω) consists of periodically repeated replicas of X(ω) centered about kωs for all k. The Fourier spectrum Xs(ω) is shown in Fig. 5-34(ƒ) for Ts < π/ωM (or ωs > 2ωM), and in Fig. 5-34 (j) for Ts > π/ωM (or ωs < 2ωM), where ωs = 2< π/Ts. It is seen that no overlap of the replicas X(ω - k ωs) occurs in Xs(ω) for ωs ≥ 2ωM and that overlap of the spectral replicas is produced for ωs < 2ωM. This effect is known as aliasing.


Fig. 5-34 Ideal sampling.

5.59. Let X(t) be a real-valued band-limited signal specified by

Show that X(t) can be expressed as

where Ts = π/ωM.
Let

From Eq. (5.183) we have

Then, under the following two conditions,

we see from Eq. (5.185) that

Next, taking the Fourier transform of Eq. (5.182), we have

Substituting Eq. (5.187) into Eq. (5.186), we obtain

Taking the inverse Fourier transform of Eq. (5.188), we get

From Probs. 5.58 and Probs. 5.59 we conclude that a band-limited signal which has no frequency components higher than ƒM hertz can be recovered completely from a set of samples taken at the rate of ƒs (≥ 2ƒM) samples per second. This is known as the uniform sampling theorem for low-pass signals. We refer to Ts = π/ωM = 1/2ƒM(ωM = 2π ƒM) as the Nyquist sampling interval and ƒs = 1/Ts = 2ƒM as the Nyquist sampling rate.
5.60. Consider the system shown in Fig. 5-35(a). The frequency response H(ω) of the ideal low-pass filter is given by [Fig. 5-35(b)]

Show that if ωc = ωs/2, then for any choice of Ts,



Fig. 5-35

From Eq. (5.137) the impulse response h (t) of the ideal low-pass filter is given by

From Eq. (5.182) we have

By Eq. (2.6) and using Eqs. (2.7) and (1.26), the output y(t) is given by

Using Eq. (5.189), we get

If ωc = ωs/2, then Tsωc/π = 1 and we have

Setting t = mTs (m = integer) and using the fact that ωsTs = 2π, we get

Since

we have

which shows that without any restriction on X(t), y (mTs) = x (mTs) for any integer value of m.
Note from the sampling theorem (Probs. 5.58 and Probs. 5.59) that if ωs = 2π/Ts is greater than twice the highest frequency present in X(t) and ωc = ωs/2, then y(t) = X(t). If this condition on the bandwidth of X(t) is not satisfied, then y(t) ≠ X(t). However, if ωc = ωs/2, then y (mTs) = x (mTs) for any integer value of m.
SUPPLEMENTARY PROBLEMS
5.61. Consider a rectified sine wave signal X(t) defined by
X(t) = |A sin π t |
(a) Sketch X(t) and find its fundamental period.
(b) Find the complex exponential Fourier series of X(t).
(c) Find the trigonometric Fourier series of X(t).
5.62. Find the trigonometric Fourier series of a periodic signal X(t) defined by

5.63. Using the result from Prob. 5.10, find the trigonometric Fourier series of the signal X(t) shown in Fig. 5-36.


Fig. 5-36

5.64. Derive the harmonic form Fourier series representation (5.15) from the trigonometric Fourier series representation (5.8).
5.65. Show that the mean-square value of a real periodic signal X(t) is the sum of the mean-square values of its harmonics.
5.66. Show that if
X(t) ↔ X(ω)
then

5.67. Using the differentiation technique, find the Fourier transform of the triangular pulse signal shown in Fig. 5-37.


Fig. 5-37

5.68. Find the inverse Fourier transform of

5.69. Find the inverse Fourier transform of

5.70. Verify the frequency differentiation property (5.56); that is,

5.71. Find the Fourier transform of each of the following signals:
(a) x(t) = cos ω0tu (t)
(b) x(t) = sin ω0tu (t)
(c) x(t) = e -at cos ω0tu (t), a > 0
(d) x(t) = e -at sin ω0tu (t), a > 0
5.72. Let X(t) be a signal with Fourier transform X(ω) given by

Consider the signal

Find the value of

5.73. Let x(t) be a real signal with the Fourier transform X(ω). The analytical signal x+(t) associated with x(t) is a complex signal defined by

where  is the Hilbert transform of X(t).
(a) Find the Fourier transform X + (ω) of x+(t).
(b) Find the analytical signal x+(t) associated with cos ω0t and its Fourier transform X+ (ω).
5.74. Consider a continuous-time LTI system with frequency response H(ω). Find the Fourier transform S(ω) of the unit step response s (t) of the system.
5.75. Consider the RC filter shown in Fig. 5-38. Find the frequency response H(ω) of this filter and discuss the type of filter.


Fig. 5-38

5.76. Determine the 99 percent energy containment bandwidth for the signal

5.77. The sampling theorem in the frequency domain states that if a real signal X(t) is a duration-limited signal, that is,

then its Fourier transform X(ω) can be uniquely determined from its values X (n π/tM) at a series of equidistant points spaced π/tM apart. In fact, X(ω) is given by

Verify the above sampling theorem in the frequency domain.
ANSWERS TO SUPPLEMENTARY PROBLEMS
5.61. (a) X(t) is sketched in Fig. 5-39 and T 0 = 1.


Fig. 5-39



5.62. 
5.63. 
5.64. Hint: Rewrite ak cos kω0t + bk sin k ω0t as

and use the trigonometric formula cos(A - B) = cos A cos B + sin A sin B.
5.65. Hint: Use Parseval's identity (5.21) for the Fourier series and Eq. (5.168).
5.66. Hint: Repeat the time-differentiation property (5.55).
5.67. 
5.68. Hint: Differentiate Eq. (5.155) N times with respect to (a).

5.69. Hint: Note that
2 - ω2 + j3ω = 2 + (jω)2 + j3ω (1 + jω)(2 + jω)
and apply the technique of partial-fraction expansion.
X(t) = (e -t e-2t) u (t)
5.70. Hint: Use definition (5.31) and proceed in a manner similar to Prob. 5.28.
5.71. Hint: Use multiplication property (5.59).
(a) 
(b) 
(c) 
(d) 
5.72. Hint: Use Parseval's identity (5.64) for the Fourier transform.
1/3π
5.73. (a) 
(b) 
5.74. Hint: Use Eq. (2.12) and the integration property (5.57).
S(ω) = πH (0)δ (ω) + (1/jω) H(ω)
5.75.  high-pass filter
5.76. W99 = 2.3/a radians/second or ƒ99 = 0.366/a hertz
5.77. Hint: Expand X(t) in a complex Fourier series and proceed in a manner similar to that for Prob. 5.59.







CHAPTER 6Fourier Analysis of Discrete-Time Signals and Systems
6.1 Introduction
In this chapter we present the Fourier analysis in the context of discrete-time signals (sequences) and systems. The Fourier analysis plays the same fundamental role in discrete time as in continuous time. As we will see, there are many similarities between the techniques of discrete-time Fourier analysis and their continuous-time counterparts, but there are also some important differences.
6.2 Discrete Fourier Series
A. Periodic Sequences:
In Chap. 1 we defined a discrete-time signal (or sequence) x[n] to be periodic if there is a positive integer N for which

The fundamental period N0 of x[n] is the smallest positive integer N for which Eq. (6.1) is satisfied.
As we saw in Sec. 1.4, the complex exponential sequence

where Ω0 = 2π/N0, is a periodic sequence with fundamental period N0. As we discussed in Sec. 1.4C, one very important distinction between the discrete-time and the continuous-time complex exponential is that the signals ejω0t are distinct for distinct values of ω0, but the sequences ejΩ0n, which differ in frequency by a multiple of 2π, are identical. That is,

Let

Then by Eq. (6.3) we have

and more generally,

Thus, the sequences Ψk[n] are distinct only over a range of N0 successive values of k.
B. Discrete Fourier Series Representation:
The discrete Fourier series representation of a periodic sequence x[n] with fundamental period N0 is given by

where ck are the Fourier coefficients and are given by (Prob. 6.2)

Because of Eq. (6.5) [or Eq. (6.6)], Eqs. (6.7) and (6.8) can be rewritten as

where Σk =<N0> denotes that the summation is on k as k varies over a range of N0 successive integers. Setting k = 0 in Eq. (6.10), we have

which indicates that c0 equals the average value of x[n] over a period.
The Fourier coefficients ck are often referred to as the spectral coefficients of x[n].
C. Convergence of Discrete Fourier Series:
Since the discrete Fourier series is a finite series, in contrast to the continuous-time case, there are no convergence issues with discrete Fourier series.
D. Properties of Discrete Fourier Series:
1. Periodicity of Fourier Coefficients:
From Eqs. (6.5) and (6.7) [or (6.9)], we see that

which indicates that the Fourier series coefficients ck are periodic with fundamental period N0.
2. Duality:
From Eq. (6.12) we see that the Fourier coefficients ck form a periodic sequence with fundamental period N0. Thus, writing ck as c[k], Eq. (6.10) can be rewritten as

Let n = -m in Eq. (6.13). Then

Letting k = n and m = k in the above expression, we get

Comparing Eq. (6.14) with Eq. (6.9), we see that (1/N0)x[-k] are the Fourier coefficients of c[n]. If we adopt the notation

to denote the discrete Fourier series pair, then by Eq. (6.14) we have

Equation (6.16) is known as the duality property of the discrete Fourier series.
3. Other Properties:
When x[n] is real, then from Eq. (6.8) or [Eq. (6.10)] and Eq. (6.12) it follows that

where * denotes the complex conjugate.
Even and Odd Sequences:
When x[n] is real, let
x[n] = xe[n] + x0[n]
where xe[n] and x0[n], are the even and odd components of x[n], respectively. Let

Then

Thus, we see that if x[n] is real and even, then its Fourier coefficients are real, while if x[n] is real and odd, its Fourier coefficients are imaginary.
E. Parseval's Theorem:
If x[n] is represented by the discrete Fourier series in Eq. (6.9), then it can be shown that (Prob. 6.10)

Equation (6.19) is called Parseval's identity (or Parseval's theorem) for the discrete Fourier series.
6.3 The Fourier Transform
A. From Discrete Fourier Series to Fourier Transform:
Let x[n] be a nonperiodic sequence of finite duration. That is, for some positive integer N1,
x[n] = 0      |n| > N1
Such a sequence is shown in Fig. 6-1(a). Let xN0[n] be a periodic sequence formed by repeating x[n] with fundamental period N0 as shown in Fig. 6-1(b). If we let N0 → ∞, we have

The discrete Fourier series of xN0[n] is given by

where

Since xN0[n] = x[n] for |n| ≤ N1 and also since x[n] = 0 outside this interval, Eq. (6.22a) can be rewritten as

Let us define X(Ω) as

Then, from Eq. (6.22b) the Fourier coefficients ck can be expressed as




Fig. 6-1 (a) Nonperiodic finite sequence x[n]; (b) periodic sequence formed by periodic extension of x[n].

Substituting Eq. (6.24) into Eq. (6.21), we have

From Eq. (6.23), X(Ω) is periodic with period 2π and so is ejΩn. Thus, the product X(Ω) ejΩn will also be periodic with period 2π. As shown in Fig. 6-2, each term in the summation in Eq. (6.25) represents the area of a rectangle of height X(kΩ0)ejkΩ0n and width Ω0. As N0 → ∞, Ω0 = 2π/N0 becomes infinitesimal (Ω0 → 0) and Eq. (6.25) passes to an integral. Furthermore, since the summation in Eq. (6.25) is over N0 consecutive intervals of width Ω0 = 2π/N0, the total interval of integration will always have a width 2π. Thus, as N0 → ∞ and in view of Eq. (6.20), Eq. (6.25) becomes

Since X(Ω)ejΩn is periodic with period 2π, the interval of integration in Eq. (6.26) can be taken as any interval of length 2π.


Fig. 6-2 Graphical interpretation of Eq. (6.25).

B. Fourier Transform Pair:
The function X(Ω) defined by Eq. (6.23) is called the Fourier transform of x[n], and Eq. (6.26) defines the inverse Fourier transform of X(Ω). Symbolically they are denoted by

and we say that x[n] and X(Ω) form a Fourier transform pair denoted by

Equations (6.27) and (6.28) are the discrete-time counterparts of Eqs. (5.31) and (5.32).
C. Fourier Spectra:
The Fourier transform X(Ω) of x[n] is, in general, complex and can be expressed as

As in continuous time, the Fourier transform X(Ω) of a nonperiodic sequence x[n] is the frequency-domain specification of x[n] and is referred to as the spectrum (or Fourier spectrum) of x[n]. The quantity |X(Ω)| is called the magnitude spectrum of x[n], and ϕ(Ω) is called the phase spectrum of x[n]. Furthermore, if x[n] is real, the amplitude spectrum |X(Ω)| is an even function and the phase spectrum ϕ(Ω) is an odd function of Ω.
D. Convergence of X(Ω):
Just as in the case of continuous time, the sufficient condition for the convergence of X(Ω) is that x[n] is absolutely summable, that is,

E. Connection between the Fourier Transform and the z-Transform:
Equation (6.27) defines the Fourier transform of x[n] as

The z-transform of x[n], as defined in Eq. (4.3), is given by

Comparing Eqs. (6.32) and (6.33), we see that if the ROC of X(z) contains the unit circle, then the Fourier transform X(Ω) of x[n] equals X(z) evaluated on the unit circle, that is,

Note that since the summation in Eq. (6.33) is denoted by X(z), then the summation in Eq. (6.32) may be denoted as X(ejΩ). Thus, in the remainder of this book, both X(Ω) and X(ejΩ) mean the same thing whenever we connect the Fourier transform with the z-transform. Because the Fourier transform is the z-transform with z = ejΩ, it should not be assumed automatically that the Fourier transform of a sequence x[n] is the z-transform with z replaced by ejΩ. If x[n] is absolutely summable, that is, if x[n] satisfies condition (6.31), the Fourier transform of x[n] can be obtained from the z-transform of x[n] with z = ejΩ since the ROC of X(z) will contain the unit circle; that is, |ejΩ| = 1. This is not generally true of sequences which are not absolutely summable. The following examples illustrate the above statements.
EXAMPLE 6.1 Consider the unit impulse sequence δ[n].
From Eq. (4.14) the z-transform of δ[n] is

By definitions (6.27) and (145), the Fourier transform of δ[n] is

Thus, the z-transform and the Fourier transform of δ[n] are the same. Note that δ[n] is absolutely summable and that the ROC of the z-transform of δ[n] contains the unit circle.
EXAMPLE 6.2 Consider the causal exponential sequence
x[n] = anu[n]       a real
From Eq. (4.9) the z-transform of x[n] is given by

Thus, X(ejΩ) exists for |a| < 1 because the ROC of X(z) then contains the unit circle. That is,

Next, by definition (6.27) and Eq. (1.91) the Fourier transform of x[n] is

Thus, comparing Eqs. (6.37) and (6.38), we have

Note that x[n] is absolutely summable.
EXAMPLE 6.3 Consider the unit step sequence u[n].
From Eq. (4.16) the z-transform of u[n] is

The Fourier transform of u[n] cannot be obtained from its z-transform because the ROC of the z-transform of u[n] does not include the unit circle. Note that the unit step sequence u[n] is not absolutely summable. The Fourier transform of u[n] is given by (Prob. 6.28)

6.4 Properties of the Fourier Transform
Basic properties of the Fourier transform are presented in the following. There are many similarities to and several differences from the continuous-time case. Many of these properties are also similar to those of the z-transform when the ROC of X(z) includes the unit circle.
A. Periodicity:

As a consequence of Eq. (6.41), in the discrete-time case we have to consider values of Ω (radians) only over the range 0 ≤ Ω ≤ 2π or - π ≤ Ω ≤ π, while in the continuous-time case we have to consider values of ω (radians/second) over the entire range - ∞ < ω < ∞.
B. Linearity:

C. Time Shifting:

D. Frequency Shifting:

E. Conjugation:

where * denotes the complex conjugate.
F. Time Reversal:

G. Time Scaling:
In Sec. 5.4D the scaling property of a continuous-time Fourier transform is expressed as [Eq. (5.52)]

However, in the discrete-time case, x[an] is not a sequence if a is not an integer. On the other hand, if a is an integer, say a = 2, then x[2n] consists of only the even samples of x[n]. Thus, time scaling in discrete time takes on a form somewhat different from Eq. (6.47).
Let m be a positive integer and define the sequence

Then we have

Equation (6.49) is the discrete-time counterpart of Eq. (6.47). It states again the inverse relationship between time and frequency. That is, as the signal spreads in time (m > 1), its Fourier transform is compressed (Prob. 6.22). Note that X(mΩ) is periodic with period 2π/m since X(Ω) is periodic with period 2π.
H. Duality:
In Sec. 5.4F the duality property of a continuous-time Fourier transform is expressed as [Eq. (5.54)]

There is no discrete-time counterpart of this property. However, there is a duality between the discrete-time Fourier transform and the continuous-time Fourier series. Let
x[n] ↔ X(Ω)
From Eqs. (6.27) and (6.41)

Since Ω is a continuous variable, letting Ω = t and n = - k in Eq. (6.51), we have

Since X(t) is periodic with period T0 = 2π and the fundamental frequency ω0 = 2π/T0 = 1, Eq. (6.53) indicates that the Fourier series coefficients of X(t) will be x[-k]. This duality relationship is denoted by

where FS denotes the Fourier series and ck are its Fourier coefficients.
I. Differentiation in Frequency:

J. Differencing:

The sequence x[n] - x[n -1] is called the first difference sequence. Equation (6.56) is easily obtained from the linearity property (6.42) and the time-shifting property (6.43).
K. Accumulation:

Note that accumulation is the discrete-time counterpart of integration. The impulse term on the right-hand side of Eq. (6.57) reflects the dc or average value that can result from the accumulation.
L. Convolution:

As in the case of the z-transform, this convolution property plays an important role in the study of discrete-time LTI systems.
M. Multiplication:

where ⊗ denotes the periodic convolution defined by [Eq. (2.70)]

The multiplication property (6.59) is the dual property of Eq. (6.58).
N. Additional Properties:
If x[n] is real, let
x[n] = xe[n] + x0[n]
where xe[n] and xo[n] are the even and odd components of x[n], respectively. Let

Then

Equation (6.62) is the necessary and sufficient condition for x[n] to be real. From Eqs. (6.62) and (6.61) we have

From Eqs. (6.63a), (6.63b), and (6.64a) we see that if x[n] is real and even, then X(Ω) is real and even, while if x[n] is real and odd, X(Ω) is imaginary and odd.
O. Parseval's Relations:

Equation (6.66) is known as Parseval's identity (or Parseval's theorem) for the discrete-time Fourier transform.
Table 6-1 contains a summary of the properties of the Fourier transform presented in this section. Some common sequences and their Fourier transforms are given in Table 6-2.

TABLE 6-1 Properties of the Fourier Transform



TABLE 6-2 Common Fourier Transform Pairs


6.5 The Frequency Response of Discrete-Time LTI Systems
A. Frequency Response:
In Sec. 2.6 we showed that the output y[n] of a discrete-time LTI system equals the convolution of the input x[n] with the impulse response h[n]; that is,

Applying the convolution property (6.58), we obtain

where Y(Ω), X(Ω), and H(Ω) are the Fourier transforms of y[n], x[n], and h[n], respectively. From Eq. (6.68) we have

Relationship represented by Eqs. (6.67) and (6.68) are depicted in Fig. 6-3. Let

As in the continuous-time case, the function H(Ω) is called the frequency response of the system, |H(Ω)| the magnitude response of the system, and θH(Ω) the phase response of the system.


Fig. 6-3 Relationships between inputs and outputs in an LTI discrete-time system.

Consider the complex exponential sequence

Then, setting z = ejΩ0 in Eq. (4.1), we obtain

which indicates that the complex exponential sequence ejΩ0n is an eigenfunction of the LTI system with corresponding eigenvalue H(Ω0), as previously observed in Chap. 2 (Sec. 2.8). Furthermore, by the linearity property (6.42), if the input x[n] is periodic with the discrete Fourier series

then the corresponding output y[n] is also periodic with the discrete Fourier series

If x[n] is not periodic, then from Eqs. (6.68) and (6.28) the corresponding output y[n] can be expressed as

B. LTI Systems Characterized by Difference Equations:
As discussed in Sec. 2.9, many discrete-time LTI systems of practical interest are described by linear constant-coefficient difference equations of the form

with M ≤ N. Taking the Fourier transform of both sides of Eq. (6.76) and using the linearity property (6.42) and the time-shifting property (6.43), we have

or, equivalently,

The result (6.77) is the same as the z-transform counterpart H(z) = Y(z)/X(z) with z = ejΩ [Eq. (4.44)]; that is,

C. Periodic Nature of the Frequency Response:
From Eq. (6.41) we have

Thus, unlike the frequency response of continuous-time systems, that of all discrete-time LTI systems is periodic with period 2π. Therefore, we need observe the frequency response of a system only over the frequency range 0 ≤ Ω < 2π or -π ≤ Ω < π.
6.6 System Response to Sampled Continuous-Time Sinusoids
A. System Responses:
We denote by yc[n], ys[n], and y[n] the system responses to cos Ωn, sin Ωn, and ejΩn, respectively (Fig. 6-4). Since ejΩn = cos Ωn + j sin Ωn, it follows from Eq. (6.72) and the linearity property of the system that



Fig. 6-4 System responses to ejΩn, cos Ωn, and sin Ωn.

When a sinusoid cos Ωn is obtained by sampling a continuous-time sinusoid cos ωt with sampling interval Ts, that is,

all the results developed in this section apply if we substitute ωTs for Ω:

For a continuous-time sinusoid cos ωt there is a unique waveform for every value of ω in the range 0 to ∞. Increasing ω results in a sinusoid of ever-increasing frequency. On the other hand, the discrete-time sinusoid cos Ωn has a unique waveform only for values of Ω in the range 0 to 2π because

This range is further restricted by the fact that

Therefore,

Equation (6.84) shows that a sinusoid of frequency (π + Ω) has the same waveform as one with frequency (π - Ω). Therefore, a sinusoid with any value of Ω outside the range 0 to π is identical to a sinusoid with Ω in the range 0 to π. Thus, we conclude that every discrete-time sinusoid with a frequency in the range 0 ≤ Ω < π has a distinct waveform, and we need observe only the frequency response of a system over the frequency range 0 ≤ Ω < π.
B. Sampling Rate:
Let ωΜ (= 2πfM) be the highest frequency of the continuous-time sinusoid. Then from Eq. (6.81) the condition for a sampled discrete-time sinusoid to have a unique waveform is

where fs = 1/Ts is the sampling rate (or frequency). Equation (6.85) indicates that to process a continuous-time sinusoid by a discrete-time system, the sampling rate must not be less than twice the frequency (in hertz) of the sinusoid. This result is a special case of the sampling theorem we discussed in Prob. 5.59.
6.7 Simulation
Consider a continuous-time LTI system with input x(t) and output y(t). We wish to find a discrete-time LTI system with input x[n] and output y[n] such that

where Ts is the sampling interval.
Let Hc(s) and Hd(z) be the system functions of the continuous-time and discrete-time systems, respectively (Fig. 6-5). Let

Then from Eqs. (3.1) and (4.1) we have

Thus, the requirement y[n] = y(nTs) leads to the condition

from which it follows that

In terms of the Fourier transform, Eq. (6.89) can be expressed as

Note that the frequency response Hd(Ω) of the discrete-time system is a periodic function of ω (with period 2π/Ts), but that the frequency response Hc(ω) of the continuous-time system is not. Therefore, Eq. (6.90) or


Fig. 6-5 Digital simulation of analog systems.

Eq. (6.89) cannot, in general, be true for every ω. If the input x(t) is band-limited [Eq. (5.94)], then it is possible, in principle, to satisfy Eq. (6.89) for every ω in the frequency range (-π/Ts, π/Ts) (Fig. 6-6). However, from Eqs. (5.85) and (6.77), we see that Hd(ω) is a rational function of ω, whereas Hd(Ω) is a rational function of ejΩ(Ω = ωTs). Therefore, Eq. (6.89) is impossible to satisfy. However, there are methods for determining a discrete-time system so as to satisfy Eq. (6.89) with reasonable accuracy for every ω in the band of the input (Probs. 6.43 to Probs. 6.47).


Fig. 6-6

6.8 The Discrete Fourier Transform
In this section we introduce the technique known as the discrete Fourier transform (DFT) for finite-length sequences. It should be noted that the DFT should not be confused with the Fourier transform.
A. Definition:
Let x[n] be a finite-length sequence of length N, that is,

The DFT of x[n], denoted as X[k], is defined by

where WN is the Nth root of unity given by

The inverse DFT (IDFT) is given by

The DFT pair is denoted by

Important features of the DFT are the following:
1. There is a one-to-one correspondence between x[n] and X[k].
2. There is an extremely fast algorithm, called the fast Fourier transform (FFT) for its calculation.
3. The DFT is closely related to the discrete Fourier series and the Fourier transform.
4. The DFT is the appropriate Fourier representation for digital computer realization because it is discrete and of finite length in both the time and frequency domains.
Note that the choice of N in Eq. (6.92) is not fixed. If x[n] has length N1 < N, we want to assume that x[n] has length N by simply adding (N - N1) samples with a value of 0. This addition of dummy samples is known as zero padding. Then the resultant x[n] is often referred to as an N-point sequence, and x[k] defined in Eq. (6.92) is referred to as an N-point DFT. By a judicious choice of N, such as choosing it to be a power of 2, computational efficiencies can be gained.
B. Relationship between the DFT and the Discrete Fourier Series:
Comparing Eqs. (6.94) and (6.92) with Eqs. (6.7) and (6.8), we see that X[k] of finite sequence x[n] can be interpreted as the coefficients ck in the discrete Fourier series representation of its periodic extension multiplied by the period N0 = and N0 = N. That is,

Actually, the two can be made identical by including the factor 1/N with the DFT rather than with the IDFT.
C. Relationship between the DFT and the Fourier Transform:
By definition (6.27) the Fourier transform of x[n] defined by Eq. (6.91) can be expressed as

Comparing Eq. (6.97) with Eq. (6.92), we see that

Thus, x[k] corresponds to the sampled X(Ω) at the uniformly spaced frequencies Ω = k2π/N for integer k.
D. Properties of the DFT:
Because of the relationship (6.98) between the DFT and the Fourier transform, we would expect their properties to be quite similar, except that the DFT X[k] is a function of a discrete variable while the Fourier transform X(Ω) is a function of a continuous variable. Note that the DFT variables n and k must be restricted to the range 0 ≤ n, k < N, the DFT shifts x[n - n0] or x[k - k0] imply x[n - n0]modN or x[k - K0]mod N, where the modulo notation [m]mod N means that

for some integer i such that

For example, if x[n] = δ[n - 3], then

The DFT shift is also known as a circular shift. Basic properties of the DFT are the following:
1. Linearity:

2. Time Shifting:

3. Frequency Shifting:

4. Conjugation:

where * denotes the complex conjugate.
5. Time Reversal:

6. Duality:

7. Circular Convolution:

The convolution sum in Eq. (6.108) is known as the circular convolution of x1[n] and x2[n].
8. Multiplication:

9. Additional Properties:
When x[n] is real, let
x[n] = xe[n] + xo[n]
where xe[n] and xo[n] are the even and odd components of x[n], respectively. Let
x[n] ↔ X[k] = A[k] + jB[k] = | X[K]| ejθ[k]

From Eq. (6.110) we have

10. Parseval's Relation:

Equation (6.113) is known as Parseval's identity (or Parseval's theorem) for the DFT.
SOLVED PROBLEMS
Discrete Fourier Series
6.1. We call a set of sequences {Ψk[n]} orthogonal on an interval [N1, N2] if any two signals Ψm[n] and Ψk[n] in the set satisfy the condition

where * denotes the complex conjugate and α ≠ 0. Show that the set of complex exponential sequences

is orthogonal on any interval of length N.
From Eq. (1.90) we note that

Applying Eq. (6.116), with α = ejk(2π/N), we obtain

since ejk(2π/N)N = ejk2π = 1. Since each of the complex exponentials in the summation in Eq. (6.117) is periodic with period N, Eq. (6.117) remains valid with a summation carried over any interval of length N. That is,

Now, using Eq. (6.118), we have

where m, k < N. Equation (6.119) shows that the set {ejk(2π/N)n: k = 0, 1,..., N -1} is orthogonal over any interval of length N. Equation (6.114) is the discrete-time counterpart of Eq. (5.95) introduced in Prob. 5.1.
6.2. Using the orthogonality condition Eq. (6.119), derive Eq. (6.8) for the Fourier coefficients.
Replacing the summation variable k by m in Eq. (6.7), we have

Using Eq. (6.115) with N = N0, Eq. (6.120) can be rewritten as

Multiplying both sides of Eq. (6.121) by  and summing over n = 0 to (N0 -1), we obtain

Interchanging the order of the summation and using Eq. (6.119), we get

Thus,

6.3. Determine the Fourier coefficients for the periodic sequence x[n] shown in Fig. 6-7.
From Fig. 6-7 we see that x[n] is the periodic extension of {0, 1, 2, 3} with fundamental period N0 = 4. Thus,

By Eq. (6.8) the discrete-time Fourier coefficients ck are

Note that  [Eq. (6.17)].



6.4. Consider the periodic sequence x[n] shown in Fig. 6-8(a). Determine the Fourier coefficients ck and sketch the magnitude spectrum |ck|.
From Fig. 6-8(a) we see that the fundamental period of x[n] is N0 = 10 and Ω0 = 2π/N0 = π/5. By Eq. (6.8) and using Eq. (1.90), we get

The magnitude spectrum |ck| is plotted in Fig. 6-8(b)


Fig. 6-8

6.5. Consider a sequence

(a) Sketch x[n].
(b) Find the Fourier coefficients ck of x[n].
(a) The sequence x[n] is sketched in Fig. 6-9(a). It is seen that x[n] is the periodic extension of the sequence {1, 0, 0, 0} with period N0 = 4.


Fig. 6-9

(b) From Eqs. (6.7) and (6.8) and Fig. 6-9(a) we have

since x[1] = x[2] = x[3] = 0. The Fourier coefficients of x[n] are sketched in Fig. 6-9(b).
6.6. Determine the discrete Fourier series representation for each of the following sequences:

(a) The fundamental period of x[n] is N0 = 8, and Ω0 = 2π/N0 = π/4. Rather than using Eq. (6.8) to evaluate the Fourier coefficients ck, we use Euler's formula and get

Thus, the Fourier coefficients for x[n] are c1 = , c- 1 = c- 1 + 8 = c7 = , and all other ck 0. Hence, the discrete Fourier series of x[n] is

(b) From Prob. 1.16(i) the fundamental period of x[n] is N0 = 24, and Ω0 2π/N0 = π/12. Again by Euler's formula we have

Thus, , and all other ck = 0. Hence, the discrete Fourier series of x[n] is

(c) From Prob. 1.16(j) the fundamental period of x[n] is N0 = 8, and Ω0 = 2π/N0 = π/4. Again by Euler's formula we have

Thus, , and all other ck = 0. Hence, the discrete Fourier series of x[n] is

6.7. Let x[n] be a real periodic sequence with fundamental period N0 and Fourier coefficients ck = ak + jbk, where ak and bk are both real.
(a) Show that a-k = ak and b-k = -bk.
(b) Show that  is real if N0 is even.
(c) Show that x[n] can also be expressed as a discrete trigonometric Fourier series of the form

if N0 is odd or

if N0 is even.
(a) If x[n] is real, then from Eq. (6.8) we have

Thus,
c-k = a-k + jb-k = (ak + jbk)* = ak -jbk
and we have
a-k = ak and b-k = -bk
(b) If N0 is even, then from Eq. (6.8)

(c) Rewrite Eq. (6.7) as

If N0 is odd, then (N0 - 1) is even and we can write x[n] as

Now, from Eq. (6.17)

Thus,

If N0 is even, we can write x[n] as

Again from Eq. (6.17)

6.8. Let x1[n] and x2[n] be periodic sequences with fundamental period N0 and their discrete Fourier series given by

Show that the sequence x[n] = x1[n]x2[n] is periodic with the same fundamental period N0 and can be expressed as

where ck is given by

Now note that
x[n + N0] = x1[n + N0]x2[n + N0] = x1[n]x2[n] = x[n]
Thus, x[n] is periodic with fundamental period N0. Let


and the term in parentheses is equal to ek - m.
6.9. Let x1[n] and x2[n] be the two periodic signals in Prob. 6.8. Show that

Equation (6.127) is known as Parseval's relation for periodic sequences.
From Eq. (6.126) we have

Setting k = 0 in the above expression, we get

6.10. (a) Verify Parseval's identity [Eq. (6.19)] for the discrete Fourier series; that is,

(b) Using x[n] in Prob. 6.3, verify Parseval's identity [Eq. (6.19)].
(a) Let

Equation (6.128) indicates that if the Fourier coefficients of x[n] are ck, then the Fourier coefficients of x*[n] are  Setting x1[n] = x[n] and x2[n] = x*[n] in Eq. (6.127), we have dk = ck and ek =  (or e-k = ) and we obtain

(b) From Fig. 6-7 and the results from Prob. 6.3, we have

and Parseval's identity is verified.
Fourier Transform
6.11. Find the Fourier transform of
x[n] = -anu[-n - 1] a real
From Eq. (4.12) the z-transform of x[n] is given by

Thus, X (ejΩ) exists for |a| > 1 because the ROC of X (z) then contains the unit circle. Thus,

6.12. Find the Fourier transform of the rectangular pulse sequence (Fig. 6-10)
x[n] = u[n] - u[n - N]
Using Eq. (1.90), the z-transform of x[n] is given by

Thus, X (ejΩ) exists because the ROC of X (z) includes the unit circle. Hence,



Fig. 6-10

6.13. Verify the time-shifting property (6.43); that is,
x[n - n0] ↔ e-jΩn0X (Ω)
By definition (6.27)

By the change of variable m = n - n0, we obtain

Hence,
x[n - n0] ↔ e-jΩn0 X(Ω)
6.14. (a) Find the Fourier transform X (Ω) of the rectangular pulse sequence shown in Fig. 6-11(a).


Fig. 6-11

(b) Plot X (Ω) for N1 = 4 and N1 = 8.
(a) From Fig. 6-11 we see that
x[n] = x1[n + N1]
where x1[n] is shown in Fig. 6-11(b). Setting N = 2N1 + 1 in Eq. (6.132), we have

Now, from the time-shifting property (6.43) we obtain

(b) Setting N1 = 4 in Eq. (6.133), we get

which is plotted in Fig. 6-12(a). Similarly, for N1 = 8 we get

which is plotted in Fig. 6-12(b).


Fig. 6-12

6.15. (a) Find the inverse Fourier transform x[n] of the rectangular pulse spectrum X(Ω) defined by [Fig. 6-13(a)]

(b) Plot x[n] for W = π/4.


Fig. 6-13

(a) From Eq. (6.28)

Thus, we obtain

(b) The sequence x[n] is plotted in Fig. 6-13(b) for W = π/4.
6.16. Verify the frequency-shifting property (6.44); that is,
ejΩ0n x[n] ↔ X(Ω - Ω0)
By Eq. (6.27)

Hence,
ejΩ0n x[n] ↔ X(Ω - Ω0)
6.17. Find the inverse Fourier transform x[n] of
X (Ω) = 2πδ(Ω - Ω0) |Ω|, |Ω0| ≤ π
From Eqs. (6.28) and (1.22) we have

Thus, we have

6.18. Find the Fourier transform of
x[n] = 1 all n
Setting Ω0 = 0 in Eq. (6.135), we get

Equation (6.136) is depicted in Fig. 6-14.


Fig. 6-14 A constant sequence and its Fourier transform.

6.19. Find the Fourier transform of the sinusoidal sequence
x[n] = cos Ω0n       |Ω0| ≤ π
From Euler's formula we have

Thus, using Eq. (6.135) and the linearity property (6.42), we get
X(Ω) = π[δ(Ω - Ω0) + δ(Ω + Ω0)]    |Ω|, |Ω0| ≤ π
which is illustrated in Fig. 6-15. Thus,



Fig. 6-15 A cosine sequence and its Fourier transform.

6.20. Verify the conjugation property (6.45); that is,

From Eq. (6.27)

Hence,
x*[n] ↔ X*(-Ω)
6.21. Verify the time-scaling property (6.49); that is,
x(m)[n] ↔ X(mΩ)
From Eq. (6.48)

Then, by Eq. (6.27)

Changing the variable n = km on the right-hand side of the above expression, we obtain

Hence,
x(m)[n] ↔ X(mΩ)
6.22. Consider the sequence x[n] defined by

(a) Sketch x[n] and its Fourier transform X(Ω).
(b) Sketch the time-scaled sequence x(2)[n] and its Fourier transform X(2)(Ω).
(c) Sketch the time-scaled sequence x(3)[n] and its Fourier transform X(3)(Ω).
(a) Setting N1 = 2 in Eq. (6.133), we have

The sequence x[n] and its Fourier transform X(Ω) are sketched in Fig. 6-16(a).


Fig. 6-16

(b) From Eqs. (6.49) and (6.138) we have

The time-scaled sequence x(2)[n] and its Fourier transform X(2)(Ω) are sketched in Fig. 6-16(b).
(c) In a similar manner we get

The time-scaled sequence x(3)[n] and its Fourier transform X(3)(Ω) are sketched in Fig. 6-16(c).
6.23. Verify the differentiation in frequency property (6.55); that is,

From definition (6.27)

Differentiating both sides of the above expression with respect to Ω and interchanging the order of differentiation and summation, we obtain

Multiplying both sides by j, we see that

Hence,

6.24. Verify the convolution theorem (6.58); that is,
x1[n] * x2[n] ↔ X1(Ω) X2(Ω)
By definitions (2.35) and (6.27), we have

Changing the order of summation, we get

By the time-shifting property Eq. (6.43)

Thus, we have

Hence,
x1[n] * x2[n] ↔ X1(Ω) X2(Ω)
6.25. Using the convolution theorem (6.58), find the inverse Fourier transform x[n] of

From Eq. (6.37) we have

Thus, by the convolution theorem Eq. (6.58) we get

Hence,

6.26. Verify the multiplication property (6.59); that is,

Let x[n] = x1[n]x2[n]. Then by definition (6.27)

By Eq. (6.28)

Interchanging the order of summation and integration, we get

Hence,

6.27. Verify the properties (6.62), (6.63a), and (6.63b); that is, if x[n] is real and

where xe[n] and xo[n] are the even and odd components of x[n], respectively, then
X(-Ω) = X*(Ω)
xe[n] ↔ Re{X(Ω)} = A(Ω)
xo[n] ↔ j Im{X(Ω)} = jB(Ω)
If x[n] is real, then x*[n], = x[n], and by Eq. (6.45) we have
x*[n] ↔ X *(-Ω)
from which we get
X(Ω) = X*(-Ω) or X(-Ω) = X*(Ω)
Next, using Eq. (6.46) and Eqs. (1.2) and (1.3), we have

Adding (subtracting) Eq. (6.141) to (from) Eq. (6.140), we obtain
xe[n] ↔ A(Ω) = Re{X(Ω)}
xo[n] ↔ jB(Ω) = j Im{X(Ω)}
6.28. Show that

Let
u[n] ↔ X(Ω)
Now, note that
δ[n] = u[n] - u[n - 1]
Taking the Fourier transform of both sides of the above expression and by Eqs. (6.36) and (6.43), we have
1 = (1 - e-jΩ) X(Ω)
Noting that (1 - e-jΩ) = 0 for Ω = 0, X(Ω) must be of the form

where A is a constant. To determine A we proceed as follows. From Eq. (1.5) the even component of u[n] is given by

Then the odd component of u[n] is given by

From Eq. (6.63b) the Fourier transform of an odd real sequence must be purely imaginary. Thus, we must have A = π, and

6.29. Verify the accumulation property (6.57); that is,

From Eq. (2.132)

Thus, by the convolution theorem (6.58) and Eq. (6.142) we get

since X(Ω)δ(Ω) = X(0)δ(Ω) by Eq. (1.25).
6.30. Using the accumulation property (6.57) and Eq. (1.50), find the Fourier transform of u[n].
From Eq. (1.50)

Now, from Eq. (6.36) we have
δ[n] ↔ 1
Setting x[k] = δ[k] in Eq. (6.57), we have
x[n] = δ[n] ↔ X(Ω) = 1 and X(0) = 1

and

Frequency Response
6.31. A causal discrete-time LTI system is described by

where x[n] and y[n] are the input and output of the system, respectively (Prob. 4.32).
(a) Determine the frequency response H(Ω) of the system.
(b) Find the impulse response h[n] of the system.
(a) Taking the Fourier transform of Eq. (6.143), we obtain

or,

Thus,

(b) Using partial-fraction expansions, we have

Taking the inverse Fourier transform of H(Ω), we obtain

which is the same result obtained in Prob. 4.32(b).
6.32. Consider a discrete-time LTI system described by

(a) Determine the frequency response H(Ω) of the system.
(b) Find the impulse response h[n] of the system.
(c) Determine its response y[n] to the input

(a) Taking the Fourier transform of Eq. (6.144), we obtain

Thus,


Taking the inverse Fourier transform of H(Ω), we obtain

(c) From Eq. (6.137)

Then

Taking the inverse Fourier transform of Y(Ω) and using Eq. (6.135), we get

6.33. Consider a discrete-time LTI system with impulse response

Find the output y[n] if the input x[n] is a periodic sequence with fundamental period N0 = 5 as shown in Fig. 6-17.


Fig. 6-17

From Eq. (6.134) we have

Since Ω0 = 2π/N0 = 2π/5 and the filter passes only frequencies in the range |Ω| ≤ π/4, only the dc term is passed through. From Fig. 6-17 and Eq. (6.11)

Thus, the output y[n] is given by

6.34. Consider the discrete-time LTI system shown in Fig. 6-18.
(a) Find the frequency response H(Ω) of the system.
(b) Find the impulse response h[n] of the system.
(c) Sketch the magnitude response |H(Ω)| and the phase response θ(Ω).
(d) Find the 3-dB bandwidth of the system.
(a) From Fig. 6-18 we have



Fig. 6-18

Taking the Fourier transform of Eq. (6.145) and by Eq. (6.77), we have

(b) By the definition of h[n] [Eq. (2.30)] and Eq. (6.145) we obtain

(c) From Eq. (6.146)

which are sketched in Fig. 6-19.


Fig. 6-19

(d) Let Ω3 db be the 3-dB bandwidth of the system. Then by definition (Sec. 5.7)

we obtain

We see that the system is a discrete-time wideband low-pass finite impulse response (FIR) filter (Sec. 2.9C).
6.35. Consider the discrete-time LTI system shown in Fig. 6-20, where a is a constant and 0 < a < 1.


Fig. 6-20

(a) Find the frequency response H(Ω) of the system.
(b) Find the impulse response h[n] of the system.
(c) Sketch the magnitude response |H(Ω)| of the system for a = 0.9 and a = 0.5.
(a) From Fig. 6-20 we have

Taking the Fourier transform of Eq. (6.147) and by Eq. (6.77), we have

(b) Using Eq. (6.37), we obtain
h[n] = anu[n]
(c) From Eq. (6.148)

and

which is sketched in Fig. 6-21 for a = 0.9 and a = 0.5.
We see that the system is a discrete-time low-pass infinite impulse response (IIR) filter (Sec. 2.9C).


Fig. 6-21

6.36. Let hLPF[n] be the impulse response of a discrete-time low-pass filter with frequency response HLPF(Ω). Show that a discrete-time filter whose impulse response h[n] is given by

is a high-pass filter with the frequency response

Since -1 = ejπ, we can write

Taking the Fourier transform of Eq. (6.152) and using the frequency-shifting property (6.44), we obtain
H(Ω) = HLPF(Ω - π)
which represents the frequency response of a high-pass filter. This is illustrated in Fig. 6-22.


Fig. 6-22 Transformation of a low-pass filter to a high-pass filter.

6.37. Show that if a discrete-time low-pass filter is described by the difference equation

then the discrete-time filter described by

is a high-pass filter.
Taking the Fourier transform of Eq. (6.153), we obtain the frequency response HLPF(Ω) of the low-pass filter as

If we replace Ω by (Ω - π) in Eq. (6.155), then we have

which corresponds to the difference equation

6.38. Convert the discrete-time low-pass filter shown in Fig. 6-18 (Prob. 6.34) to a high-pass filter.
From Prob. 6.34 the discrete-time low-pass filter shown in Fig. 6-18 is described by [Eq. (6.145)]
y[n] = x[n] + x[n - 1]
Using Eq. (6.154), the converted high-pass filter is described by

which leads to the circuit diagram in Fig. 6-23. Taking the Fourier transform of Eq. (6.157) and by Eq. (6.77), we have

From Eq. (6.158)

which are sketched in Fig. 6-24. We see that the system is a discrete-time high-pass FIR filter.


Fig. 6-23



Fig. 6-24

6.39. The system function H(z) of a causal discrete-time LTI system is given by

where a is real and |a| < 1. Find the value of b so that the frequency response H(Ω) of the system satisfies the condition

Such a system is called an all-pass filter.
By Eq. (6.34) the frequency response of the system is

Then, by Eq. (6.160)

which leads to

and we see that if b = -a, Eq. (6.162) holds for all Ω and Eq. (6.160) is satisfied.
6.40. Let h[n] be the impulse response of an FIR filter so that
h[n] = 0    n < 0, n ≥ N
Assume that h[n] is real and let the frequency response H(Ω) be expressed as
H(Ω) = |H(Ω)|ejθ(Ω)
(a) Find the phase response θ (Ω) when h[n] satisfies the condition [Fig. 6-25(a)]

(b) Find the phase response θ (Ω) when h[n] satisfies the condition [Fig. 6-25(b)]

(a) Taking the Fourier transform of Eq. (6.163) and using Eqs. (6.43), (6.46), and (6.62), we obtain

Thus,

which indicates that the phase response is linear.


Fig. 6-25

(b) Similarly, taking the Fourier transform of Eq. (6.164), we get

Thus,

which indicates that the phase response is also linear.
6.41. Consider a three-point moving-average discrete-time filter described by the difference equation

(a) Find and sketch the impulse response h[n] of the filter.
(b) Find the frequency response H(Ω) of the filter.
(c) Sketch the magnitude response |H(Ω)| and the phase response θ(Ω) of the filter.
(a) By the definition of h[n] [Eq. (2.30)] we have



Fig. 6-26

which is sketched in Fig. 6-26(a). Note that h[n] satisfies the condition (6.163) with N = 3.
(b) Taking the Fourier transform of Eq. (6.168), we have

By Eq. (1.90), with α = e-jΩ, we get

where

(c) From Eq. (6.169)

which are sketched in Fig. 6-26(b). We see that the system is a low-pass FIR filter with linear phase.
6.42. Consider a causal discrete-time FIR filter described by the impulse response
h[n] = {2, 2, -2, -2}
(a) Sketch the impulse response h[n] of the filter.
(b) Find the frequency response H(Ω) of the filter.
(c) Sketch the magnitude response |H(Ω)| and the phase response θ(Ω) of the filter.
(a) The impulse response h[n] is sketched in Fig. 6-27(a). Note that h[n] satisfies the condition (6.164) with N = 4.
(b) By definition (6.27)

(c) From Eq. (6.171)

which are sketched in Fig. 6-27(b). We see that the system is a bandpass FIR filter with linear phase.


Fig. 6-27

Simulation
6.43. Consider the RC low-pass filter shown in Fig. 6-28(a) with RC = 1.
(a) Construct a discrete-time filter such that

where hc(t) is the impulse response of the RC filter, hd[n] is the impulse response of the discrete-time filter, and Ts is a positive number to be chosen as part of the design procedures.
(b) Plot the magnitude response |Hc(ω)| of the RC filter and the magnitude response |Hd(ωTs)| of the discrete-time filter for Ts = 1 and Ts = 0.1.


Fig. 6-28 Simulation of an RC filter by the impulse invariance method.

(a) The system function Hc(s) of the RC filter is given by (Prob. 3.23)

and the impulse response hc(t) is

By Eq. (6.172) the corresponding hd[n] is given by

Then, taking the z-transform of Eq. (6.175), the system function Hd(z) of the discrete-time filter is given by

from which we obtain the difference equation describing the discrete-time filter as

from which the discrete-time filter that simulates the RC filter is shown in Fig. 6-28(b).
(b) By Eq. (5.40)

By Eqs. (6.34) and (6.81)

From Eq. (6.149)

From Ts = 1,

For Ts = 0.1,

The magnitude response |Hc(ω)| of the RC filter and the magnitude response |Hd(ωTs)| of the discrete-time filter for Ts = 1 and Ts = 0.1 are plotted in Fig. 6-29. Note that the plots are scaled such that the magnitudes at ω 0 are normalized to 1.
The method utilized in this problem to construct a discrete-time system to simulate the continuous-time system is known as the impulse-invariance method.


Fig. 6-29

6.44. By applying the impulse-invariance method, determine the frequency response Hd(Ω) of the discrete-time system to simulate the continuous-time LTI system with the system function

Using the partial-fraction expansion, we have

Thus, by Table 3-1 the impulse response of the continuous-time system is

Let hd[n] be the impulse response of the discrete-time system. Then, by Eq. (6.177)
hd[n] = hc(nTs) = (e-nTs e-2nTs)u[n]
and the system function of the discrete-time system is given by

Thus, the frequency response Hd(Ω) of the discrete-time system is

Note that if the system function of a continuous-time LTI system is given by

then the impulse-invariance method yields the corresponding discrete-time system with the system function Hd(z) given by

6.45. A differentiator is a continuous-time LTI system with the system function [Eq. (3.20)]

A discrete-time LTI system is constructed by replacing s in Hc(s) by the following transformation known as the bilinear transformation:

to simulate the differentiator. Again Ts in Eq. (6.183) is a positive number to be chosen as part of the design procedure.
(a) Draw a diagram for the discrete-time system.
(b) Find the frequency response Hd(Ω) of the discrete-time system and plot its magnitude and phase responses.
(a) Let Hd(z) be the system function of the discrete-time system. Then, from Eqs. (6.182) and (6.183) we have

Writing Hd(z) as

then, from Probs. (6.35) and (6.38) the discrete-time system can be constructed as a cascade connection of two systems as shown in Fig. 6-30(a). From Fig. 6-30(a) it is seen that we can replace two unit-delay elements by one unit-delay element as shown in Fig. 6-30(b).
(b) By Eq. (6.184) the frequency response Hd(Ω) of the discrete-time system is given by(6.185)



Fig. 6-30 Simulation of a differentiator.

Note that when Ω ≪ 1, we have

If Ω = ωTs(Fig. 6-31).


Fig. 6-31

6.46. Consider designing a discrete-time LTI system with system function Hd(z) obtained by applying the bilinear transformation to a continuous-time LTI system with rational system function Hc(s). That is,

Show that a stable, causal continuous-time system will always lead to a stable, causal discrete-time system.
Consider the bilinear transformation of Eq. (6.183)

Solving Eq. (6.188) for z, we obtain

Setting s = jω in Eq. (6.189), we get

Thus, we see that the jω-axis of the s-plane is transformed into the unit circle of the z-plane. Let
z = rejΩ    and    s = s = σ + jω
Then from Eq. (6.188)

Hence,

From Eq. (6.191a) we see that if r < 1, then σ < 0, and if r > 1, then σ > 0. Consequently, the left-hand plane (LHP) in s maps into the inside of the unit circle in the z-plane, and the right-hand plane (RHP) in s maps into the outside of the unit circle (Fig. 6-32). Thus, we conclude that a stable, causal continuous-time system will lead to a stable, causal discrete-time system with a bilinear transformation (see Sec. 3.6B and Sec. 4.6B). When r = 1, then σ = 0 and

From Eq. (6.193) we see that the entire range - ∞ < ω < ∞ is mapped only into the range -π ≤ Ω ≤ π.


Fig. 6-32 Bilinear transformation.

6.47. Consider the low-pass RC filter in Fig. 6-28(a). Design a low-pass discrete-time filter by the bilinear transformation method such that its 3-dB bandwidth is π/4.
Using Eq. (6.192), Ω3 dB = π/4 corresponds to

From Prob. 5.55(a), ω3 dB = 1/RC. Thus, the system function Hc(s) of the RC filter is given by

Let Hd(z) be the system function of the desired discrete-time filter. Applying the bilinear transformation (6.183) to Eq. (6.195), we get

from which the system in Fig. 6-33 results. The frequency response of the discrete-time filter is

At Ω = 0, Hd(0) = 1, and at Ω = π/4, |Hd(π/4)| = 0.707 =  which is the desired response.


Fig. 6-33 Simulation of an RC filter by the bilinear transformation method.

6.48. Let h[n] denote the impulse response of a desired IIR filter with frequency response H(Ω) and let ho[n] denote the impulse response of an FIR filter of length N with frequency response Ho(Ω). Show that when

the mean-square error ε2 defined by

is minimized.
By definition (6.27)


where e[n] = h[n] - ho[n]. By Parseval's theorem (6.66) we have

The last two terms in Eq. (6.201) are two positive constants. Thus, ε2 is minimized when
h[n] - ho[n] = 0     0 ≤ n ≤ N - 1
that is,
h[n] = ho[n]     0 ≤ n ≤ N - 1
Note that Eq. (6.198) can be expressed as

where w[n] is known as a rectangular window function given by

Discrete Fourier Transform
6.49. Find the N-point DFT of the following sequences x[n]:
(a) x[n] = δ[n]
(b) x[n] = u[n] - u[n - N]
(a) From definitions (6.92) and (1.45), we have

Fig. 6-34 shows X[n] and its N-point DFT X[k].



(b) Again from definitions (6.92) and (1.44) and using Eq. (1.90), we obtain

since  = e-j(2π/N) kN = e-jk 2π = 1.

Fig. 6-35 shows x[n] and its N-point DFT X[k].


Fig. 6-35

6.50. Consider two sequences x[n] and h[n] of length 4 given by

(a) Calculate y[n] = x[n] ⊗ h[n] by doing the circular convolution directly.
(b) Calculate y[n] by DFT.
(a) The sequences x[n] and h[n] can be expressed as

By Eq. (6.108)

The sequences x[i] and h[n - i]mod4 for n = 0, 1, 2, 3 are plotted in Fig. 6-36(a). Thus, by Eq. (6.108) we get

which is plotted in Fig. 6-36(b).
(b) By Eq. (6.92)

Then by Eq. (6.107) the DFT of y[n] is

Since  and , we obtain

Thus, by the definition of DFT [Eq. (6.92)] we get



Fig. 6-36

6.51. Consider the finite-length complex exponential sequence

(a) Find the Fourier transform X(Ω) of x[n].
(b) Find the N-point DFT x[k] of x[n].
(a) From Eq. (6.27) and using Eq. (1.90), we have

(b) Note from Eq. (6.98) that

we obtain

6.52. Show that if x[n] is real, then its DFT x[k] satisfies the relation

where * denotes the complex conjugate.
From Eq. (6.92)

Hence, if x[n] is real, then x*[n] = x[n] and

6.53. Show that

where * denotes the complex conjugate and
x[k] = DFT{x[n]}
We can write Eq. (6.94) as

Noting that the term in brackets in the last term is the DFT of X*[k], we get

which shows that the same algorithm used to evaluate the DFT can be used to evaluate the IDFT.
6.54. The DFT definition in Eq. (6.92) can be expressed in a matrix operation form as

The N × N matrix WN is known as the DFT matrix. Note that WN is symmetric; that is,  = WN, where  is the transpose of WN·
(a) Show that

where  is the inverse of WN and  is the complex conjugate of WN·
(b) Find W4 and  explicitly.
(a) If we assume that the inverse of WN exists, then multiplying both sides of Eq. (6.206) by , we obtain

which is just an expression for the IDFT. The IDFT as given by Eq. (6.94) can be expressed in matrix form as

Comparing Eq. (6.210) with Eq. (6.209), we conclude that

(b) Let Wn + 1, k + 1 denote the entry in the (n + 1)st row and (k + 1)st column of the W4 matrix. Then, from Eq. (6.207)

and we have

6.55. (a) Find the DFT x[k] of x[n] = {0, 1, 2, 3}.
(b) Find the IDFT x[n] from x[k] obtained in part (a).
(a) Using Eqs. (6.206) and (6.212), the DFT x[k] of x[n] is given by

(b) Using Eqs. (6.209) and (6.212), the IDFT x[n] of x[k] is given by

6.56. Let x[n] be a sequence of finite length N such that

Let the N-point DFT x[k] of x[n] be given by [Eq. (6.92)]

Suppose N is even and let

The sequences f[n] and g[n] represent the even-numbered and odd-numbered samples of x[n], respectively.
(a) Show that

(b) Show that the N-point DFT x[k] of x[n] can be expressed as

(c) Draw a flow graph to illustrate the evaluation of X[k] from Eqs. (6.217a) and (6.217b) with N = 8.
(d) Assume that x[n] is complex and  have been precomputed. Determine the numbers of complex multiplications required to evaluate X[k] from Eq. (6.214) and from Eqs. (6.217a) and (6.217b) and compare the results for N = 210 = 1024.
(a) From Eq. (6.213)

Similarly

Thus,

(b) We rewrite Eq. (6.214) as

But

With this substitution Eq. (6.219) can be expressed as

Where

Note that F[k] and G[k] are the (N/2)-point DFTs of f[n] and g[n], respectively. Now

Since

Hence, Eq. (6.221) can be expressed as

(c) The flow graph illustrating the steps involved in determining X[k] by Eqs. (6.217a) and (6.217b) is shown in Fig. 6-37.
(d) To evaluate a value of X[k] from Eq. (6.214) requires N complex multiplications. Thus, the total number of complex multiplications based on Eq. (6.214) is N2. The number of complex multiplications in evaluating F[k] or G[k] is (N/2)2. In addition, there are N multiplications involved in the evaluation of . Thus, the total number of complex multiplications based on Eqs. (6.217a) and (6.217b) is 2(N/2)2 + N = N2/2 + N. For N = 210 = 1024 the total number of complex multiplications based on Eq. (6.214) is 220 ≈106 and is 106/2 = 1024 ≈ 106/2 based on Eqs. (6.217a) and (6.217b). So we see that the number of multiplications is reduced approximately by a factor of 2 based on Eqs. (6.217a) and (6.217b).
The method of evaluating x[k] based on Eqs. (6.217a) and (6.217b) is known as the decimation-in-time fast Fourier transform (FFT) algorithm. Note that since N/2 is even, using the same procedure, F[k] and G[k] can be found by first determining the (N/4)-point DFTs of appropriately chosen sequences and combining them.


Fig. 6-37 Flow graph for an 8-point decimation-in-time FFT algorithm.

6.57. Consider a sequence
x[n] = {1, 1, -1, -1, -1, 1, 1, -1}
Determine the DFT X[k] of x[n] using the decimation-in-time FFT algorithm.
From Figs. 6-38(a) and (b), the phase factors  and  are easily found as follows:

Next, from Eqs. (6.215a) and (6.215b)
f[n] = x[2n] = {x[0], x[2], x[4], x[6]} = {1, -1, -1, 1}
g[n] = x[2n + 1] = {x[1], x[3], x[5], x[7]} = {1, -1, 1, -1}
Then, using Eqs. (6.206) and (6.212), we have

and by Eqs. (6.217a) and (6.217b) we obtain

Noting that since x[n] is real and using Eq. (6.204), X[7], X[6], and X[5] can be easily obtained by taking the conjugates of X[1], X[2], and X[3], respectively.


Fig. 6-38 Phase factors  and .

6.58. Let x[n] be a sequence of finite length N such that
x[n] = 0     n < 0, n ≥ N
Let the N-point DFT x[k] of x[n] be given by [Eq. (6.92)]

Suppose N is even and let

(a) Show that the N-point DFT x[k] of x[n] can be expressed as

(b) Draw a flow graph to illustrate the evaluation of X[k] from Eqs. (6.226a) and (6.226b) with N = 8.
(a) We rewrite Eq. (6.224) as

Changing the variable n = m + N/2 in the second term of Eq. (6.228), we have

Noting that [Eq. (6.223)]

Eq. (6.229) can be expressed as

For k even, setting k = 2r in Eq. (6.230), we have

where the relation in Eq. (6.220) has been used. Similarly, for k odd, setting k = 2r + 1 in Eq. (6.230), we get

Equations (6.231) and (6.232) represent the (N/2)-point DFT of p[n] and q[n], respectively. Thus, Eqs. (6.231) and (6.232) can be rewritten as

(b) The flow graph illustrating the steps involved in determining X[k] by Eqs. (6.227a) and (6.227b) is shown in Fig. 6-39.
The method of evaluating x[k] based on Eqs. (6.227a) and (6.227b) is known as the decimation-infrequency fast Fourier transform (FFT) algorithm.


Fig. 6-39 Flow graph for an 8-point decimation-in-frequency FFT algorithm.

6.59. Using the decimation-in-frequency FFT technique, redo Prob. 6.57.
From Prob. 6.57
x[n] = {1, 1, -1, -1, -1, 1, 1, -1}
By Eqs. (6.225a) and (6.225b) and using the values of  obtained in Prob. 6.57, we have

Then using Eqs. (6.206) and (6.212), we have

and by Eqs. (6.226a) and (6.226b) we get

which are the same results obtained in Prob. 6.57.
6.60. Consider a causal continuous-time band-limited signal x(t) with the Fourier transform X(ω). Let

where Ts is the sampling interval in the time domain. Let

where Δω is the sampling interval in the frequency domain known as the frequency resolution. Let T1 be the record length of x(t), and let ωM be the highest frequency of x(t). Show that x[n] and x[k] form an N-point DFT pair if

Since x(t) < 0 for t 0, the Fourier transform X(ω) of x(t) is given by [Eq. (5.31)]

Let T1 be the total recording time of x(t) required to evaluate X(ω). Then the above integral can be approximated by a finite series as

where tn = n Δt and T1 = N Δt. Setting ω = ωk in the above expression, we have

Next, since the highest frequency of x(t) is ωM, the inverse Fourier transform of X(ω) is given by [Eq. (5.32)]

Dividing the frequency range -ωM ≤ ω ≤ ωM into N (even) intervals of length Δω, the above integral can be approximated by

where 2ωM = N Δω. Setting t = tn in the above expression, we have

Since the highest frequency in x(t) is ωM, then from the sampling theorem (Prob. 5.59) we should sample x(t) so that

where Ts is the sampling interval. Since Ts = Δt, selecting the largest value of Δt (the Nyquist interval), we have

and

Thus, N is a suitable even integer for which

From Eq. (6.240) the frequency resolution Δω is given by

Let tn = n Δt and ωk = k Δω. Then

Substituting Eq. (6.243) into Eqs. (6.237) and (6.239), we get

and

Rewrite Eq. (6.245) as

Then from Eq. (6.244) we note that X(k Δω) is periodic in k with period N. Thus, changing the variable k = m - N in the second sum in the above expression, we get

Multiplying both sides of Eq. (6.246) by Δt and noting that Δω Δt = 2π/N, we have

Now if we define

then Eqs. (6.244) and (6.247) reduce to the DFT pair; that is,

6.61. (a) Using the DFT, estimate the Fourier spectrum X(ω) of the continuous-time signal
x(t) = e-t u(t)
Assume that the total recording time of x(t) is T1 = 10 s and the highest frequency of x(t) is ωM = 100 rad/s.
(b) Let x[k] be the DFT of the sampled sequence of x(t). Compare the values of X[0], X[1], and X[10] with the values of X(0), X(Δω), and X(10Δω).
(a) From Eq. (6.241)

Thus, choosing N = 320, we obtain

and

Then from Eqs. (6.244), (6.249), and (1.92), we have

which is the estimate of X(k Δ w).
(b) Setting k = 0, k = 1, and k = 10 in Eq. (6.250), we have

From Table 5-2

and

Even though x(t) is not band-limited, we see that x[k] offers a quite good approximation to X(ω) for the frequency range we specified.
SUPPLEMENTARY PROBLEMS
6.62. Find the discrete Fourier series for each of the following periodic sequences:
(a) x[n] = cos(0, 1πn)
(b) x[n] = sin(0, 1πn)
(c) x[n] = 2 cos(1.6πn) + sin(2.4πn)
6.63. Find the discrete Fourier series for the sequence x[n] shown in Fig. 6-40.


Fig. 6-40

6.64. Find the trigonometric form of the discrete Fourier series for the periodic sequence x[n] shown in Fig. 6-7 in Prob. 6.3.
6.65. Find the Fourier transform of each of the following sequences:
(a) x[n] = a|n|, |a | < 1
(b) x[n] = sin(Ω0n), |Ω0| < π
(c) x[n] = u[-n -1]
6.66. Find the Fourier transform of the sequence x[n] shown in Fig. 6-41.


Fig. 6-41

6.67. Find the inverse Fourier transform of each of the following Fourier transforms:
(a) X(Ω) = cos(2Ω)
(b) X(Ω) = j Ω
6.68. Consider the sequence y[n] given by

Express y(Ω) in terms of X(Ω).
6.69. Let

(a) Find y[n] = x[n] * x[n].
(b) Find the Fourier transform Y(Ω) of y[n].
6.70. Verify Parseval's theorem [Eq. (6.66)] for the discrete-time Fourier transform, that is,

6.71. A causal discrete-time LTI system is described by

where x[n] and y[n] are the input and output of the system, respectively.
(a) Determine the frequency response H(Ω) of the system.
(b) Find the impulse response h[n] of the system.
(c) Find y[n] if x[n] = ()nu[n].
6.72. Consider a causal discrete-time LTI system with frequency response
H(Ω) = Re{H(Ω)} + j Im{H(Ω)} = A(Ω) = jB(Ω)
(a) Show that the impulse response h[n] of the system can be obtained in terms of A(Ω) or B(Ω) alone.
(b) Find H(Ω) and h[n] if
Re{H(Ω)} = A(Ω) = 1 + cos Ω
6.73. Find the impulse response h[n] of the ideal discrete-time HPF with cutoff frequency Ωc(0 < Ωc <π) shown in Fig. 6-42.


Fig. 6-42.

6.74. Show that if HLPF(z) is the system function of a discrete-time low-pass filter, then the discrete-time system whose system function H(z) is given by H(z) = HLPF(-z) is a high-pass filter.
6.75. Consider a continuous-time LTI system with the system function

Determine the frequency response Hd(Ω) of the discrete-time system designed from this system based on the impulse invariance method.
6.76. Consider a continuous-time LTI system with the system function

Determine the frequency response Hd(Ω) of the discrete-time system designed from this system based on the step response invariance; that is,

where sc(t) and sd[n] are the step response of the continuous-time and the discrete-time systems, respectively.
6.77. Let Hp(z) be the system function of a discrete-time prototype low-pass filter. Consider a new discrete-time low-pass filter whose system function H(z) is obtained by replacing z in Hp(z) with (z - α)/(1 - α z), where a is real.
(a) Show that

(b) Let Ωp1 and Ω1 be the specified frequencies (< π) of the prototype low-pass filter and the new low-pass filter, respectively. Then show that

6.78. Consider a discrete-time prototype low-pass filter with system function

(a) Find the 3-dB bandwidth of the prototype filter.
(b) Design a discrete-time low-pass filter from this prototype filter so that the 3-dB bandwidth of the new filter is 2π/3.
6.79. Determine the DFT of the sequence
x[n] = an    0 ≤ n ≤ N -1
6.80. Evaluate the circular convolution

(a) Assuming N = 4.
(b) Assuming N = 8.
6.81. Consider the sequences x[n] and h[n] in Prob. 6.80.
(a) Find the 4-point DFT of x[n], h[n], and y[n].
(b) Find y[n] by taking the IDFT of Y[k].
6.82. Consider a continuous-time signal x(t) that has been prefiltered by a low-pass filter with a cutoff frequency of 10 kHz. The spectrum of x(t) is estimated by use of the N-point DFT. The desired frequency resolution is 0.1 Hz. Determine the required value of N (assuming a power of 2) and the necessary data length T1.
ANSWERS TO SUPPLEMENTARY PROBLEMS
6.62.
6.63.
6.64. 
6.65.
6.66. X(Ω) = j2(sin Ω + 2 sin 2Ω + 3 sin 3Ω)
6.67.
6.68. 
6.69.
6.70. Hint: Proceed in a manner similar to that for solving Prob. 5.38.
6.71.
6.72. (a) Hint: Process in a manner similar to that for Prob. 5.49.
(b) H(Ω) = 1 + e-j Ω, h[n] = δ[n] + δ[n - 1]
6.73. 
6.74. Hint: Use Eq. (6.156) in Prob. 6.37.
6.75. H(Ω) = Ts e-Ts  where Ts is the sampling interval of hc (t)
6.76. Hint: hd[n] = sd [n] - sd [n - 1]

6.77. Hint: Set  and solve for α
6.78. Hint: Use the result from Prob. 6.77.

6.79. 
6.80. (a) y[n] = {3, 3, 3, 3}
(b) y[n] = {1, 2, 3, 3, 2, 1, 0, 0}
6.81. (a) [X[0], X[1], X[2], X[3]] = [4, 0, 0, 0][H[0], H[1], H[2], H[3]] = [3, -j, 1, j][Y[0], Y[1], Y[2], Y[3]] = [12, 0, 0, 0]
(b) y[n] = {3, 3, 3, 3}
6.82. N = 218 and T1 = 13.1072 s







CHAPTER 7State Space Analysis
7.1 Introduction
So far we have studied linear time-invariant systems based on their input-output relationships, which are known as the external descriptions of the systems. In this chapter we discuss the method of state space representations of systems, which are known as the internal descriptions of the systems. The representation of systems in this form has many advantages:
1. It provides an insight into the behavior of the system.
2. It allows us to handle systems with multiple inputs and outputs in a unified way.
3. It can be extended to nonlinear and time-varying systems.
Since the state space representation is given in terms of matrix equations, the reader should have some familiarity with matrix or linear algebra. A brief review is given in App. A.
7.2 The Concept of State
A. Definition:
The state of a system at time t0 (or n0) is defined as the minimal information that is sufficient to determine the state and the output of the system for all times t ≥ t0 (or n ≥ n0) when the input to the system is also known for all times t ≥ t0 (or n ≥ n0). The variables that contain this information are called the state variables. Note that this definition of the state of the system applies only to causal systems.
Consider a single-input single-output LTI electric network whose structure is known. Then the complete knowledge of the input x(t) over the time interval -∞ to t is sufficient to determine the output y(t) over the same time interval. However, if the input x(t) is known over only the time interval t0 to t, then the current through the inductors and the voltage across the capacitors at some time t0 must be known in order to determine the output y(t) over the time interval t0 to t. These currents and voltages constitute the "state" of the network at time t0. In this sense, the state of the network is related to the memory of the network.
B. Selection of State Variables:
Since the state variables of a system can be interpreted as the "memory elements" of the system, for discrete-time systems which are formed by unit-delay elements, amplifiers, and adders, we choose the outputs of the unit-delay elements as the state variables of the system (Prob. 7.1). For continuous-time systems which are formed by integrators, amplifiers, and adders, we choose the outputs of the integrators as the state variables of the system (Prob. 7.3). For a continuous-time system containing physical energy-storing elements, the outputs of these memory elements can be chosen to be the state variables of the system (Probs. 7.4 and Probs. 7.5). If the system is described by the difference or differential equation, the state variables can be chosen as shown in the following sections.
Note that the choice of state variables of a system is not unique. There are infinitely many choices for any given system.
7.3 State Space Representation of Discrete-Time LTI Systems
A. Systems Described by Difference Equations:
Suppose that a single-input single-output discrete-time LTI system is described by an Nth-order difference equation

We know from previous discussion that if x [n ] is given for n ≥ 0, Eq. (7.1) requires N initial conditions y [-1], y [-2], ..., y [-N ] to uniquely determine the complete solution for n > 0. That is, N values are required to specify the state of the system at any time.
Let us define N state variables q1[n ], q2[n ], ..., qN[n ] as

Then from Eqs. (7.2) and (7.1) we have

and

In matrix form Eqs. (7.3a) and (7.3b) can be expressed as

Now we define an N × 1 matrix (or N-dimensional vector) q[n], which we call the state vector:

Then Eqs. (7.4a) and (7.4b) can be rewritten compactly as

where

Equations (7.6a) and (7.6b) are called an N-dimensional state space representation (or state equations) of the system, and the N × N matrix A is termed the system matrix. The solution of Eqs. (7.6a) and (7.6b) for a given initial state is discussed in Sec. 7.5.
B. Similarity Transformation:
As mentioned before, the choice of state variables is not unique and there are infinitely many choices of the state variables for any given system. Let T be any N × N nonsingular matrix (App. A) and define a new state vector

where q[n] is the old state vector which satisfies Eqs. (7.6a) and (7.6b). Since T is nonsingular; that is, T-1 exists, and we have

Now

Thus, if we let

then Eqs. (7.9a) and (7.9b) become

Equations (7.11a) and (7.11b) yield the same output y[n] for a given input x[n] with different state equations. In matrix algebra, Eq. (7.10a) is known as the similarity transformation and matrices A and Â are called similar matrices (App. A).
C. Multiple-Input Multiple-Output Systems:
If a discrete-time LTI system has m inputs and p outputs and N state variables, then a state space representation of the system can be expressed as

where

and

7.4 State Space Representation of Continuous-Time LTI Systems
A. Systems Described by Differential Equations:
Suppose that a single-input single-output continuous-time LTI system is described by an Nth-order differential equation

One possible set of initial conditions is y(0), y(1)(0), ..., y(N-1)(0), where y(k)(t) = dky(t)/dtk. Thus, let us define N state variables q1(t), q2(t), ..., qN (t) as

Then from Eqs. (7.14) and (7.13) we have

and

where qk(t) = dqk(t)/dt.
In matrix form Eqs. (7.15a) and (7.15b) can be expressed as


Now we define an N × 1 matrix (or N-dimensional vector) q(t) which we call the state vector:

The derivative of a matrix is obtained by taking the derivative of each element of the matrix. Thus,

Then Eqs. (7.16a) and (7.16b) can be rewritten compactly as

where

As in the discrete-time case, Eqs. (7.19a) and (7.19b) are called an N-dimensional state space representation (or state equations) of the system, and the N × N matrix A is termed the system matrix. In general, state equations of a single-input single-output continuous time LTI system are given by

As in the discrete-time case, there are infinitely many choices of state variables for any given system. The solution of Eqs. (7.20a) and (7.20b) for a given initial state are discussed in Sec. 7.6.
B. Multiple-Input Multiple-Output Systems:
If a continuous-time LTI system has m inputs, p outputs, and N state variables, then a state space representation of the system can be expressed as

where

and

7.5 Solutions of State Equations for Discrete-Time LTI Systems
A. Solution in the Time Domain:
Consider an N-dimensional state representation

where A, b, c, and d are N × N, N × 1, 1 × N, and 1 × 1 matrices, respectively. One method of finding q[n], given the initial state q[0], is to solve Eq. (7.22a) iteratively. Thus,

By continuing this process, we obtain

If the initial state is q[n0] and x[n] is defined for n ≥ n0, then, proceeding in a similar manner, we obtain

The matrix An is the n-fold product

and is known as the state-transition matrix of the discrete-time system. Substituting Eq. (7.23) into Eq. (7.22b), we obtain

The first term cAnq[0] is the zero-input response, and the second and third terms together form the zero-state response.
B. Determination of An:
Method 1: Let A be an N × N matrix. The characteristic equation of A is defined to be (App. A)

where |λ I - A| means the determinant of λI - A and I is the identity matrix (or unit matrix) of Nth order. The roots of c(λ) = 0, λk (k = 1, 2,..., N), are known as the eigenvalues of A. By the Cayley-Hamilton theorem An can be expressed as [App. A, Eq. (A.57)]

When the eigenvalues λk are all distinct, the coefficients b0, b1, ..., bN-1 can be found from the conditions

For the case of repeated eigenvalues, see Prob. 7.25.
Method 2: The second method of finding An is based on the diagonalization of a matrix A. If eigenvalues λk of A are all distinct, then An can be expressed as [App. A, Eq. (A.53)]

where matrix P is known as the diagonalization matrix and is given by [App. A, Eq. (A.36)]

and xk(k = 1, 2,..., N) are the eigenvectors of A defined by

Method 3: The third method of finding An is based on the spectral decomposition of a matrix A. When all eigenvalues of A are distinct, then A can be expressed as

where λk (k = 1, 2,..., N) are the distinct eigenvalues of A and Ek (k = 1, 2,..., N) are called constituent matrices, which can be evaluated as [App. A, Eq. (A.67)]

Then we have

Method 4: The fourth method of finding An is based on the z-transform.

which is derived in the following section [Eq. (7.41)].
C. The z-Transform Solution:
Taking the unilateral z-transform of Eqs. (7.22a) and (7.22b) and using Eq. (4.51), we get

where and

Rearranging Eq. (7.36a), we have

Premultiplying both sides of Eq. (7.37) by (zI - A)-1 yields

Hence, taking the inverse unilateral z-transform of Eq. (7.38), we get

Substituting Eq. (7.39) into Eq. (7.22b), we get

A comparison of Eq. (7.39) with Eq. (7.23) shows that

D. System Function H(z):
In Sec. 4.6 the system function H(z) of a discrete-time LTI system is defined by H (z) = Y (z)/X (z) with zero initial conditions. Thus, setting q[0] = 0 in Eq. (7.38), we have

The substitution of Eq. (7.42) into Eq. (7.36b) yields

Thus,

E. Stability:
From Eqs. (7.25) and (7.29) or (7.34) we see that if the magnitudes of all eigenvalues λk of the system matrix A are less than unity, that is,

then the system is said to be asymptotically stable; that is, if, undriven, its state tends to zero from any finite initial state q0. It can be shown that if all eigenvalues of A are distinct and satisfy the condition (7.45), then the system is also BIBO stable.
7.6 Solutions of State Equations for Continuous-Time LTI Systems
A. Laplace Transform Method:
Consider an N-dimensional state space representation

where A, b, c, and d are N × N, N × 1, 1 × N, and 1 × 1 matrices, respectively. In the following we solve Eqs. (7.46a) and (7.46b) with some initial state q(0) by using the unilateral Laplace transform. Taking the unilateral Laplace transform of Eqs. (7.46a) and (7.46b) and using Eq. (3.44), we get

where X(s) = I{x(t)}, Y(s) = I{y(t)}, and

Rearranging Eq. (7.47a), we have

Premultiplying both sides of Eq. (7.48) by (sI A)-1 yields

Substituting Eq. (7.49) into Eq. (7.47b), we get

Taking the inverse Laplace transform of Eq. (7.50), we obtain the output y(t). Note that c(sI - A)-1q(0) corresponds to the zero-input response and that the second term corresponds to the zero-state response.
B. System Function H(s):
As in the discrete-time case, the system function H(s) of a continuous-time LTI system is defined by H(s) = Y(s)/X(s) with zero initial conditions. Thus, setting q(0) = 0 in Eq. (7.50), we have

Thus,

C. Solution in the Time Domain:
Following

we define

where k! = k (k - 1) ... 2 · 1. If t = 0, then Eq. (7.53) reduces to

where 0 is an N × N zero matrix whose entries are all zeros. As in ea(t - τ) = eat e-aτ = e-aτ eat, we can show that

Setting τ = t in Eq. (7.55), we have

Thus,

which indicates that e-At is the inverse of eAt.
The differentiation of Eq. (7.53) with respect to t yields

which implies

Now using the relationship [App. A, Eq. (A.70)]

and Eq. (7.58), we have

Now premultiplying both sides of Eq. (7.46a) by e-At, we obtain

or

From Eq. (7.59) Eq. (7.60) can be rewritten as

Integrating both sides of Eq. (7.61) from 0 to t, we get

or

Hence,

Premultiplying both sides of Eq. (7.62) by eAt and using Eqs. (7.55) and (7.56), we obtain

If the initial state is q(t0) and we have x(t) for t ≥ t0, then

which is obtained easily by integrating both sides of Eq. (7.61) from t0 to t. The matrix function eAt is known as the state-transition matrix of the continuous-time system. Substituting Eq. (7.63) into Eq. (7.46b), we obtain

D. Evaluation of eAt:
Method 1: As in the evaluation of An, by the Cayley-Hamilton theorem we have

When the eigenvalues λk of A are all distinct, the coefficients b0, b1, ..., bN-1 can be found from the conditions

For the case of repeated eigenvalues see Prob. 7.45.
Method 2: Again, as in the evaluation of An, we can also evaluate eAt based on the diagonalization of A. If all eigenvalues λk of A are distinct, we have

where P is given by Eq. (7.30).
Method 3: We could also evaluate eAt using the spectral decomposition of A, that is, find constituent matrices Ek (k = 1, 2,..., N) for which

where λk (k = 1, 2,..., N) are the distinct eigenvalues of A. Then, when eigenvalues λk of A are all distinct, we have

Method 4: Using the Laplace transform, we can calculate eAt. Comparing Eqs. (7.63) and (7.49), we see that

E. Stability:
From Eqs. (7.63) and (7.68) or (7.70), we see that if all eigenvalues λk of the system matrix A have negative real parts, that is,

then the system is said to be asymptotically stable. As in the discrete-time case, if all eigenvalues of A are distinct and satisfy the condition (7.72), then the system is also BIBO stable.
SOLVED PROBLEMS
State Space Representation
7.1. Consider the discrete-time LTI system shown in Fig. 7-1. Find the state space representation of the system by choosing the outputs of unit-delay elements 1 and 2 as state variables q1[n] and q2[n], respectively.
From Fig. 7-1 we have



Fig. 7-1

In matrix form

or

where

7.2. Redo Prob. 7.1 by choosing the outputs of unit-delay elements 2 and 1 as state variables v1[n] and v2[n], respectively, and verify the relationships in Eqs. (7.10a) and (7.10b).
We redraw Fig. 7-1 with the new state variables as shown in Fig. 7-2. From Fig. 7-2 we have



Fig. 7-2

In matrix form

or


where

Note that v1[n] = q2[n] and v2[n] = q1[n]. Thus, we have

Now using the results from Prob. 7.1, we have

which are the relationships in Eqs. (7.10a) and (7.10b).
7.3. Consider the continuous-time LTI system shown in Fig. 7-3. Find a state space representation of the system.


Fig. 7-3

We choose the outputs of integrators as the state variables q1(t), q2(t), and q3(t) as shown in Fig. 7-3. Then from Fig. 7-3 we obtain

In matrix form

7.4. Consider the mechanical system shown in Fig. 7-4. It consists of a block with mass m connected to a wall by a spring. Let k1 be the spring constant and k2 be the viscous friction coefficient. Let the output y(t) be the displacement of the block and the input x(t) be the applied force. Find a state space representation of the system.


Fig. 7-4 Mechanical system.

By Newton's law we have
or

The potential energy and kinetic energy of a mass are stored in its position and velocity. Thus, we select the state variables q1(t) and q2(t) as
q1(t) = y(t
q2(t) = (t
Then we have

In matrix form

7.5. Consider the RLC circuit shown in Fig. 7-5. Let the output y(t) be the loop current. Find a state space representation of the circuit.


Fig. 7-5 RLC circuit.

We choose the state variables q1(t) = iL(t) and q2(t) = vc(t). Then by Kirchhoff's law we get

Rearranging and writing in matrix form, we get

7.6. Find a state space representation of the circuit shown in Fig. 7-6, assuming that the outputs are the currents flowing in R1 and R2.


Fig. 7-6

We choose the state variables q1(t) = iL(t) and q2(t) = vc(t). Then are two voltage sources and let x1(t) = v1(t) and x2(t) = v2(t). Let y1(t) = i1(t) and y2(t) = i2(t). Applying Kirchhoff's law to each loop, we obtain

Rearranging and writing in matrix form, we get

where

State Equations of Discrete-Time LTI Systems Described by Difference Equations
7.7. Find state equations of a discrete-time system described by

Choose the state variables q1[n] and q2[n] as

Then from Eqs. (7.79) and (7.80) we have

In matrix form

7.8. Find state equations of a discrete-time system described by

Because of the existence of the term x[n - 1] on the right-hand side of Eq. (7.82), the selection of y[n - 2] and y[n - 1] as state variables will not yield the desired state equations of the system. Thus, in order to find suitable state variables, we construct a simulation diagram of Eq. (7.82) using unit-delay elements, amplifiers, and adders. Taking the z-transforms of both sides of Eq. (7.82) and rearranging, we obtain

from which (noting that z-k corresponds to k unit time delays) the simulation diagram in Fig. 7-7 can be drawn. Choosing the outputs of unit-delay elements as state variables as shown in Fig. 7-7, we get

In matrix form



Fig. 7-7

7.9. Find state equations of a discrete-time LTI system with system function

From the definition of the system function [Eq. (4.41)]

we have
(1 + a1z-1 + a2z-2)Y (z) = (b0 + b1z-1 + b2z-2)X(z)
Rearranging the above equations, we get
Y(z) - a1z1Y(z) -a2z-2Y(z) + b0X(z) + b1z-1X(z) + bz-2X(z)
from which the simulation diagram in Fig. 7-8 can be drawn. Choosing the outputs of unit-delay elements as state variables as shown in Fig. 7-8, we get

In matrix form

Note that in the simulation diagram in Fig. 7-8 the number of unit-delay elements is 2 (the order of the system) and is the minimum number required. Thus, Fig. 7-8 is known as the canonical simulation of the first form and Eq. (7.85) is known as the canonical state representation of the first form.


Fig. 7-8 Canonical simulation of the first form.

7.10. Redo Prob. 7.9 by expressing H(z) as
H(z) = H1(z) H2(z)
where

Let

Then we have

Rearranging Eq. (7.88), we get

From Eqs. (7.89) and (7.90) the simulation diagram in Fig. 7-9 can be drawn. Choosing the outputs of unit-delay elements as state variables as shown in Fig. 7-9, we have



Fig. 7-9 Canonical simulation of the second form.

In matrix form

The simulation in Fig. 7-9 is known as the canonical simulation of the second form, and Eq. (7.91) is known as the canonical state representation of the second form.
7.11. Consider a discrete-time LTI system with system function

Find a state representation of the system.
Rewriting H(z) as

Comparing Eq. (7.93) with Eq. (7.84) in Prob. 7.9, we see that

Substituting these values into Eq. (7.85) in Prob. 7.9, we get

7.12. Consider a discrete-time LTI system with system function

Find a state representation of the system such that its system matrix A is diagonal.
First we expand H(z) in partial fractions as

where

Let

Then
(1 - pkz-1)Yk(z) = akX(z)
or
Yk(z) = pkz-1Yk(z)+ akX(z)
from which the simulation diagram in Fig. 7-10 can be drawn. Thus, H(z) = H1(z) + H2(z) can be simulated by the diagram in Fig. 7-11 obtained by parallel connection of two systems. Choosing the outputs of unit-delay elements as state variables as shown in Fig. 7-11, we have

In matrix form

Note that the system matrix A is a diagonal matrix whose diagonal elements consist of the poles of H(z).


Fig. 7-10



Fig. 7-11

7.13. Sketch a block diagram of a discrete-time system with the state representation

We rewrite Eq. (7.98) as

from which we can draw the block diagram in Fig. 7-12.


Fig. 7-12

State Equations of Continuous-Time LTI Systems Described by Differential Equations
7.14. Find state equations of a continuous-time LTI system described by

Choose the state variables as

Then from Eqs. (7.100) and (7.101) we have

In matrix form

7.15. Find state equations of a continuous-time LTI system described by

Because of the existence of the term 4(t) on the right-hand side of Eq. (7.103), the selection of y(t) and ý(t) as state variables will not yield the desired state equations of the system. Thus, in order to find suitable state variables, we construct a simulation diagram of Eq. (7.103) using integrators, amplifiers, and adders. Taking the Laplace transforms of both sides of Eq. (7.103), we obtain

Dividing both sides of the above expression by s2 and rearranging, we get

from which (noting that s-k corresponds to integration of k times) the simulation diagram in Fig. 7-13 can be drawn. Choosing the outputs of integrators as state variables as shown in Fig. 7-13, we get

In matrix form



Fig. 7-13

7.16. Find state equations of a continuous-time LTI system with system function

From the definition of the system function [Eq. (3.37)]

we have
(s3 + a1s2 + a2s + a3)Y(s) = (b0s3 + b1s2 + b2s + b3)X(s)
Dividing both sides of the above expression by s3 and rearranging, we get

from which (noting that s-k corresponds to integration of k times) the simulation diagram in Fig. 7-14 can be drawn. Choosing the outputs of integrators as state variables as shown in Fig. 7-14, we get

In matrix form

As in the discrete-time case, the simulation of H(s) shown in Fig. 7-14 is known as the canonical simulation of the first form, and Eq. (7.106) is known as the canonical state representation of the first form.


Fig. 7-14 Canonical simulation of the first form.

7.17. Redo Prob. 7.16 by expressing H(s) as
H(s)=H1(s)H2(s)
where

Let

Then we have

Rearranging the above equations, we get

from which, noting the relation shown in Fig. 7-15, the simulation diagram in Fig. 7-16 can be drawn. Choosing the outputs of integrators as state variables as shown in Fig. 7-16, we have



Fig. 7-15



Fig. 7-16 Canonical simulation of the second form.

In matrix form

As in the discrete-time case, the simulation of H(s) shown in Fig. 7-16 is known as the canonical simulation of the second form, and Eq. (7.109) is known as the canonical state representation of the second form.
7.18. Consider a continuous-time LTI system with system function

Find a state representation of the system.
Rewrite H(s) as

Comparing Eq. (7.111) with Eq. (7.105) in Prob. 7.16, we see that

Substituting these values into Eq. (7.106) in Prob. 7.16, we get

7.19. Consider a continuous-time LTI system with system function

Find a state representation of the system such that its system matrix A is diagonal.
First we expand H(s) in partial fractions as

where

Let

Then
or

from which the simulation diagram in Fig. 7-17 can be drawn. Thus, H(s) = H1(s) + H2(s) + H3(s) can be simulated by the diagram in Fig. 7-18 obtained by parallel connection of three systems. Choosing the outputs of integrators as state variables as shown in Fig. 7-18, we get

In matrix form

Note that the system matrix A is a diagonal matrix whose diagonal elements consist of the poles of H(s).


Fig. 7-17



Fig. 7-18

Solutions of State Equations for Discrete-Time LTI Systems
7.20. Find An for

by the Cayley-Hamilton theorem method.
First, we find the characteristic polynomial c(λ) of A.

Thus, the eigenvalues of A are λ1 =  and λ2 = . Hence, by Eqs. (7.27) and (7.28) we have

and b0 and b1 are the solutions of

from which we get

Hence,

7.21. Repeat Prob. 7.20 using the diagonalization method.
Let x be an eigenvector of A associated with λ. Then

For  we have

The solutions of this system are given by x1 = 2x2. Thus, the eigenvectors associated with λ1 are those vectors of the form

For  we have

The solutions of this system are given by x1 = 4x2. Thus, the eigenvectors associated with λ2 are those vectors of the form

Let α = β = 1 in the above expressions and let

Then

and by Eq. (7.29) we obtain

7.22. Repeat Prob. 7.20 using the spectral decomposition method.
Since all eigenvalues of A are distinct, by Eq. (7.33) we have

Then, by Eq. (7.34) we obtain

7.23. Repeat Prob. 7.20 using the z-transform method.
First, we must find (zI - A)-1.


Then by Eq. (7.35) we obtain

From the above results we note that when the eigenvalues of A are all distinct, the spectral decomposition method is computationally the most efficient method of evaluating An.
7.24. Find An for

The characteristic polynomial c(λ) of A is

Thus, the eigenvalues of A are λ1 = 1 and , and by Eq. (7.33) we have

Thus, by Eq. (7.34) we obtain

7.25. Find An for

The characteristic polynomial c(λ) of A is

Thus, the eigenvalues of A are λ1 = λ2 = 2. We use the Cayley-Hamilton theorem to evaluate An. By Eq. (7.27) we have

where b0 and b1 are determined by setting λ = 2 in the following equations [App. A, Eqs. (A.59) and (A.60)]:

Thus,

from which we get

and
7.26. Consider the matrix A in Prob. 7.25. Let A be decomposed as
where

(a) Show that N2 = 0.
(b) Show that D and N commute, that is, DN = ND.
(c) Using the results from parts (a) and (b), find An.
(a) By simple multiplication we see that

(b) Since the diagonal matrix D can be expressed as 21, we have
DN = 2IN = 2N = 2NI = N(2l) = ND
that is, D and N commute.
(c) Using the binomial expansion and the result from part(b), we can write

Since N2 = 0, then Nk = 0 for k ≥ 2, and we have
An = (D + N)n = Dn + nDn - 1 N
Thus [see App. A, Eq. (A.43)],

which is the same result obtained in Prob. 7.25.
Note that a square matrix N is called nilpotent of index r if Nr-1 ≠ 0 and Nr = 0.
7.27. The minimal polynomial m(λ) of A is the polynomial of lowest order having 1 as its leading coefficient such that m(A) = 0. Consider the matrix

(a) Find the minimal polynomial m(λ) of A.
(b) Using the result from part (a), find An.
(c) The characteristic polynomial c(λ) of A is

Thus, the eigenvalues of A are λ1 = -3 and λ2 = λ3 = 2. Consider
m(λ) = (λ + 3)(λ - 2) = λ2 + λ - 6
Now

Thus, the minimal polynomial of A is
m(λ) = (λ + 3) (λ - 2) = λ2 + λ - 6
(b) From the result from part (a) we see that An can be expressed as a linear combination of I and A only, even though the order of A is 3. Thus, similar to the result from the Cayley-Hamilton theorem, we have

where b0 and b1 are determined by setting λ = -3 and λ = 2 in the equation
b0 + b1λ = λn
Thus,
b0 - 3b1 = (-3)nb0 + 2b1 = 2n
from which we get

and

7.28. Using the spectral decomposition method, evaluate An for matrix A in Prob. 7.27.
Since the minimal polynomial of A is
m(λ) = (λ + 3) (λ - 2) = (λ - λ1) (λ - λ2)
which contains only simple factors, we can apply the spectral decomposition method to evaluate An. Thus, by Eq. (7.33) we have

Thus, by Eq. (7.34) we get

which is the same result obtained in Prob. 7.27(b).
7.29. Consider the discrete-time system in Prob. 7.7. Assume that the system is initially relaxed.
(a) Using the state space representation, find the unit step response of the system.
(b) Find the system function H(z).
(a) From the result of Prob. 7.7 we have

where

Setting q[0] = 0 and x[n] = u[n] in Eq. (7.25), the unit step response s[n] is given by

Now, from Prob. 7.20 we have

and

Thus,

which is the same result obtained in Prob. 4.32(c)
(b) By Eq. (7.44) the system function H(z) is given by
H(z) = c(zI-A)-1b+ d
Now

Thus,

which is the same result obtained in Prob. 4.32(a).
7.30. Consider the discrete-time LTI system described by

(a) Show that the unit impulse response h[n] of the system is given by

(b) Using Eq. (7.117), find the unit impulse response h[n] of the system in Prob. 7.29.
(a) By setting q[0] = 0, x[k], = δ[k], and x[n] = δ[n] in Eq. (7.25), we obtain

Note that the sum in Eq. (7.118) has no terms for n = 0 and that the first term is cAn-1b for n > 0. The second term on the right-hand side of Eq. (7.118) is equal to d for n = 0 and zero otherwise. Thus, we conclude that

(b) From the result of Prob. 7.29 we have
and

Thus, by Eq. (7.117) h[n] is

which is the same result obtained in Prob. 4.32(b).
7.31. Use the state space method to solve the difference equation [Prob. 4.38(b)]

with  and y[-1] = 1,y[-2] = 2.
Rewriting Eq. (7.119), we have

Let q1[n] = y[n - 2] and q2[n] = y[n - 1]. Then

In matrix form

where

and

Then, by Eq. (7.25)

Now from the result of Prob. 7.24 we have

and

Thus,


which is the same result obtained in Prob. 4.38(b).
7.32. Consider the discrete-time LT I system shown in Fig. 7-19.
(a) Is the system asymptotically stable?
(b) Find the system function H(z).
(c) Is the system BIBO stable?


Fig. 7-19

(a) From Fig. 7-19 and choosing the state variables q1[n] and q2[n] as shown, we obtain

In matrix form

where

Now

Thus, the eigenvalues of A are  and . Since |λ2| > 1, the system is not asymptotically stable.
(b) By Eq. (7.44) the system function H(z) is given by

(c) Note that there is pole-zero cancellation in H(z) at . Thus, the only pole of H(z) is , which lies inside the unit circle of the z-plane. Hence, the system is BIBO stable.
Note that even though the system is BIBO stable, it is essentially unstable if it is not initially relaxed.
7.33. Consider an Nth-order discrete-time LTI system with the state equation
q[n + 1] = Aq[n] + bx[n]
The system is said to be controllable if it is possible to find a sequence of N input samples x [n0], x[n0 + 1], ..., x[n0 + N - 1] such that it will drive the system from q[n0] = q0 to q[n0 + N] = q1 and q0 and q1 are any finite states. Show that the system is controllable if the controllability matrix defined by

has rank N.
We assume that n0 = 0 and q[0] = 0. Then, by Eq. (7.23) we have

which can be rewritten as

Thus, if q[N] is to be an arbitrary N-dimensional vector and also to have a nonzero input sequence, as required for controllability, the coefficient matrix in Eq. (7.122) must be nonsingular; that is, the matrix
Mc = [b Ab ... AN-1b]
must have rank N.
7.34. Consider an Nth-order discrete-time LTI system with state space representation
q[n + 1] = Aq[n] + b x[n]
y[n] = cq[n]
The system is said to be observable if, starting at an arbitrary time index n0, it is possible to determine the state q[n0] = q0 from the output sequence y[n0], y[n0 + 1], ..., y[n0 + N - 1]. Show that the system is observable if the observability matrix defined by

has rank N.
We assume that n0 = 0 and x[n] = 0. Then, by Eq. (7.25) the output y[n] for n = 0, 1, ..., N - 1, with x[n] = 0, is given by

or

Rewriting Eq. (7.125) as a matrix equation, we get

Thus, to find a unique solution for q[0], the coefficient matrix of Eq. (7.126) must be nonsingular; that is, the matrix

must have rank N.
7.35. Consider the system in Prob. 7.7.
(a) Is the system controllable?
(b) Is the system observable?
(c) Find the system function H(z).
(a) From the result of Prob. 7.7 we have

Now

and by Eq. (7.120) the controllability matrix is

and |Mc| = -1 ≠ 0. Thus, its rank is 2, and hence the system is controllable.
(b) Similarly,

and by Eq. (7.123) the observability matrix is

and  Thus, its rank is 2, and hence the system is observable.
(c) By Eq. (7.44) the system function H(z) is given by

7.36. Consider the system in Prob. 7.7. Assume that

Find x[0] and x[1] such that q[2] = 0.
From Eq. (7.23) we have

Thus,

from which we obtain .
7.37. Consider the system in Prob. 7.7. We observe y[0] = 1 and y[1] = 0 with x[0] = x[1] = 0. Find the initial state q[0].
Using Eq. (7.125), we have

Thus,

Solving for q1[0] and q2[0], we obtain

7.38. Consider the system in Prob. 7.32.
(a) Is the system controllable?
(b) Is the system observable?
(a) From the result of Prob. 7.32 we have

Now

and by Eq. (7.120) the controllability matrix is

and  Thus, its rank is 2, and hence the system is controllable.
(b) Similarly,

and by Eq. (7.123) the observability matrix is

and |M0| = 0. Thus, its rank is less than 2, and hence the system is not observable.
Note from the result from Prob. 7.32(b) that the system function H(z) has pole-zero cancellation. If H(z) has pole-zero cancellation, then the system cannot be both controllable and observable.
Solutions of State Equations for Continuous-Time LTI Systems
7.39. Find eAt for

using the Cayley-Hamilton theorem method.
First, we find the characteristic polynomial c(λ) of A.

Thus, the eigenvalues of A are λ1 = -2 and λ2 = -3. Hence, by Eqs. (7.66) and (7.67) we have

and b0 and b1 are the solutions of
b0 - 2b1 = e-2tb0 - 3b1 = e-3t
from which we get
b0 = 3e-2t - 2e-3t       b1 = e-2t - e-3t
Hence,

7.40. Repeat Prob. 7.39 using the diagonalization method.
Let x be an eigenvector of A associated with λ. Then
[λI - A]x = 0
For λ = λ1 = -2 we have

The solutions of this system are given by x2 = - 2x1. Thus, the eigenvectors associated with λ1 are those vectors of the form

For λ = λ2 = -3 we have

The solutions of this system are given by x2 = - 3x1. Thus, the eigenvectors associated with λ2 are those vectors of the form

Let α = β = 1 in the above expressions and let

Then

and by Eq. (7.68) we obtain

7.41 Repeat Prob. 7.39 using the spectral decomposition method.
Since all eigenvalues of A are distinct, by Eq. (7.33) we have

Then by Eq. (7.70) we obtain

7.42. Repeat Prob. 7.39 using the Laplace transform method.
First, we must find (sI - A)-1.

Then, by Eq. (7.71) we obtain

Again we note that when the eigenvalues of A are all distinct, the spectral decomposition method is computationally the most efficient method of evaluating eAt.
7.43. Find eAt for

The characteristic polynomial c(λ) of A is

Thus, the eigenvalues of A are λ1 = -1 and λ2 = -3. Since all eigenvalues of A are distinct, by Eq. (7.33) we have

Then, by Eq. (7.70) we obtain

7.44. Given matrix

(a) Show that A is nilpotent of index 3.
(b) Using the result from part (a) find eAt.
(a) By direct multiplication we have

Thus, A is nilpotent of index 3.
(b) By definition (7.53) and the result from part (a)

7.45. Find eAt for matrix A in Prob. 7.44 using the Cayley-Hamilton theorem method.
First, we find the characteristic polynomial c(λ) of A.

Thus, λ = 0 is the eigenvalues of A with multiplicity 3. By Eq. (7.66) we have
eAt = b0I + b1A + b2A2
where b0, b1, and b2 are determined by setting λ = 0 in the following equations [App. A, Eqs. (A.59) and (A.60)]:

Thus,

Hence,

which is the same result obtained in Prob. 7.44(b).
7.46. Show that
eA+B = eAeB
provided A and B commute; that is, AB = BA.
By Eq. (7.53)

and

Thus, if AB = BA, then
eA+B = eAeB
7.47. Consider the matrix

Now we decompose A as
where              
(a) Show that the matrix N is nilpotent of index 3.
(b) Show that Λ and N commute; that is, ΛΝ = ΝΛ.
(c) Using the results from parts (a) and (b), find eAt.
(a) By direct multiplication we have

Thus, N is nilpotent of index 3.
(b) Since the diagonal matrix Λ can be expressed as 21, we have
ΛN = 2IN = 2N = 2NI = N(21) = NΛ
that is, Λ and N commute.
(c) Since Λ and N commute, then, by the result from Prob. 7.46
eAt = e(Λ + N)t = eΛteNt
Now [see App. A, Eq. (A.49)]

and using similar justification as in Prob. 7.44(b), we have

Thus,

7.48. Using the state variables method, solve the second-order linear differential equation

with the initial conditions y(0) = 2, y′(0) = 1, and x(t) = e-tu(t) (Prob. 3.38).
Let the state variables q1(t) and q2(t) be
q1(t) = y(t)     q2(t) = y′(t)
Then the state space representation of Eq. (7.127) is given by [Eq. (7.19)]

with

Thus, by Eq. (7.65)

with d = 0. Now, from the result of Prob. 7.39,

and

Thus,

which is the same result obtained in Prob. 3.38.
7.49. Consider the network shown in Fig. 7-20. The initial voltages across the capacitors C1 and C2 are  V and 1V, respectively. Using the state variable method, find the voltages across these capacitors for t > 0. Assume that R1 = R2 = R3 = 1 Ω and C1 = C2 = 1 F.


Fig. 7-20

Let the state variables q1(t) and q2(t) be

Applying Kirchhoff's current law at nodes 1 and 2, we get

Substituting the values of R1, R2, R3, C1, and C2 and rearranging, we obtain

In matrix form

with

Then, by Eq. (7.63) with x(t) = 0 and using the result from Prob. 7.43, we get

Thus,

7.50. Consider the continuous-time LTI system shown in Fig. 7-21.
(a) Is the system asymptotically stable?
(b) Find the system function H(s).
(c) Is the system BIBO stable?


Fig. 7-21

(a) From Fig. 7-21 and choosing the state variables q1(t) and q2(t) as shown, we obtain

In matrix form

where

Now

Thus, the eigenvalues of A are λ1 = - 1 and λ2 = 2. Since Re{λ2} > 0, the system is not asymptotically stable.
(b) By Eq. (7.52) the system function H(s) is given by

(c) Note that there is pole-zero cancellation in H(s) at s = 2. Thus, the only pole of H(s) is -1, which is located in the left-hand side of the s-plane. Hence, the system is BIBO stable.
Again, it is noted that the system is essentially unstable if the system is not initially relaxed.
7.51. Consider an Nth-order continuous-time LTI system with state equation

The system is said to be controllable if it is possible to find an input x(t) which will drive the system from q(t0) = q0 to q(t1) = q1 in a specified finite time and q0 and q1 are any finite state vectors. Show that the system is controllable if the controllability matrix defined by

has rank N.
We assume that t0 = 0 and q[0] = 0. Then, by Eq. (7.63) we have

Now, by the Cayley-Hamilton theorem we can express e-Aτ as

Substituting Eq. (7.130) into Eq. (7.129) and rearranging, we get

Let

Then Eq. (7.131) can be rewritten as

or

For any given state q1 we can determine from Eq. (7.132) unique βk's (k = 0, 1, ..., N - 1), and hence x(t), if the coefficients matrix of Eq. (7.132) is nonsingular, that is, the matrix
Mc = [b Ab ... AN-1b]
has rank N.
7.52. Consider an Nth-order continuous-time LTI system with state space representation

The system is said to be observable if any initial state q(t0) can be determined by examining the system output y(t) over some finite period of time from t0 to t1. Show that the system is observable if the observability matrix defined by

has rank N.
We prove this by contradiction. Suppose that the rank of M0 is less than N. Then there exists an initial state q[0] = q0 ≠ 0 such that
M0q0 = 0
or

Now from Eq. (7.65), for x(t) = 0 and t0 = 0,

However, by the Cayley-Hamilton theorem, eAt can be expressed as

Substituting Eq. (7.136) into Eq. (7.135), we get

in view of Eq. (7.134). Thus, q0 is indistinguishable from the zero state, and hence, the system is not observable. Therefore, if the system is to be observable, then Mo must have rank N.
7.53. Consider the system in Prob. 7.50.
(a) Is the system controllable?
(b) Is the system observable?
(a) From the result from Prob. 7.50 we have

Now

and by Eq. (7.128) the controllability matrix is

and |Mc| = 0. Thus, it has a rank less than 2, and hence, the system is not controllable.
(b) Similarly,

and by Eq. (7.133) the observability matrix is

and |M0| = -2 ≠ 0. Thus, its rank is 2, and hence, the system is observable.
Note from the result from Prob. 7.50(b) that the system function H(s) has pole-zero cancellation. As in the discrete-time case, if H(s) has pole-zero cancellation, then the system cannot be both controllable and observable.
7.54. Consider the system shown in Fig. 7-22.
(a) Is the system controllable?
(b) Is the system observable?
(c) Find the system function H(s).


Fig. 7-22

(a) From Fig. 7-22 and choosing the state variables q1(t) and q2(t) as shown, we have

In matrix form

where

Now

and by Eq. (7.128) the controllability matrix is

and |Mc| = 0. Thus, its rank is less than 2, and hence, the system is not controllable.
(b) Similarly,

and by Eq. (7.133) the observability matrix is

and |Mo| = 0. Thus, its rank is less than 2, and hence, the system is not observable.
(c) By Eq. (7.52) the system function H(s) is given by

Note that the system is both uncontrollable and unobservable.
SUPPLEMENTARY PROBLEMS
7.55. Consider the discrete-time LTI system shown in Fig. 7-23. Find the state space representation of the system with the state variables q1[n] and q2[n ] as shown.


Fig. 7-23

7.56. Consider the discrete-time LTI system shown in Fig. 7-24. Find the state space representation of the system with the state variables q1[n] and q2[n] as shown.


Fig. 7-24

7.57. Consider the discrete-time LTI system shown in Fig. 7-25.
(a) Find the state space representation of the system with the state variables q1[n] and q2[n] as shown.
(b) Find the system function H(z).
(c) Find the difference equation relating x[n] and y[n].


Fig. 7-25

7.58. A discrete-time LTI system is specified by the difference equation
y[n] + y[n - 1] - 6y[n - 2] = 2x[n - 1] + x[n - 2]
Write the two canonical forms of state representation for the system.
7.59. Find An for

(a) Using the Cayley-Hamilton theorem method.
(b) Using the diagonalization method.
7.60. Find An for

(a) Using the spectral decomposition method.
(b) Using the z-transform method.
7.61. Given a matrix

(a) Find the minimal polynomial m(X) of A.
(b) Using the result from part (a), find An.
7.62. Consider the discrete-time LTI system with the following state space representation:

(a) Find the system function H(z).
(b) Is the system controllable?
(c) Is the system observable?
7.63. Consider the discrete-time LTI system in Prob. 7.55.
(a) Is the system asymptotically stable?
(b) Is the system BIBO stable?
(c) Is the system controllable?
(d) Is the system observable?
7.64. The controllability and observability of an LTI system may be investigated by diagonalizing the system matrix A. A system with a state space representation

(where Λ is a diagonal matrix) is controllable if the vector b has no zero elements, and it is observable if the vector ĉ has no zero elements. Consider the discrete-time LTI system in Prob. 7.55.
(a) Let v[n] = Tq[n]. Find the matrix T such that the new state space representation will have a diagonal system matrix.
(b) Write the new state space representation of the system.
(c) Using the result from part (b), investigate the controllability and observability of the system.
7.65. Consider the network shown in Fig. 7-26. Find a state space representation for the network with the state variables q1(t) = iL(t), q2(t) = vC(t) and outputs y1(t), = i1(t), y2(t) = vC(t), assuming R1 = R2 = 1 Ω, L = 1 H, and C = 1 F.


Fig. 7-26

7.66. Consider the continuous-time LTI system shown in Fig. 7-27
(a) Find the state space representation of the system with the state variables q1(t) and q2(t) as shown.
(b) For what values of α will the system be asymptotically stable?


Fig. 7-27

7.67. A continuous-time LTI system is described by

Write the two canonical forms of state representation for the system.
7.68. Consider the continuous-time LTI system shown in Fig. 7-28.
(a) Find the state space representation of the system with the state variables q1(t) and q2(t) as shown.
(b) Is the system asymptotically stable?
(c) Find the system function H(s).
(d) Is the system BIBO stable?


Fig. 7-28

7.69. Find eAt for

(a) Using the Cayley-Hamilton theorem method.
(b) Using the spectral decomposition method.
7.70. Consider the matrix A in Prob. 7.69. Find e-At and show that e-At = [e-At]-1.
7.71. Find eAt for

(a) Using the diagonalization method.
(b) Using the Laplace transform method.
7.72. Consider the network in Prob. 7.65 (Fig. 7-26). Find vC(t) if x(t) = u(t) under an initially relaxed condition.
7.73. Using the state space method, solve the linear differential equation
y″(t) + 3y′(t) + 2y(t) = 0
with the initial conditions y(0) = 0, y′(0) = 1.
7.74. As in the discrete-time case, controllability and observability of a continuous-time LTI system may be investigated by diagonalizing the system matrix A. A system with state space representation

where Λ is a diagonal matrix, is controllable if the vector  has no zero elements, and is observable if the vector  has no zero elements. Consider the continuous-time system in Prob. 7.50.
(a) Find a new state space representation of the system by diagonalizing the system matrix A.
(b) Is the system controllable?
(c) Is the system observable?
ANSWERS TO SUPPLEMENTARY PROBLEMS
7.55.

7.56.

7.57.(a)
(b) 
(c) 
7.58. (1) 
(2) 
7.59. 
7.60. 
7.61. (a) m(λ) = (λ - 3)(λ + 3) = λ2 - 9
(b) 
7.62. (a) 
(b) The system is controllable.
(c) The system is not observable.
7.63. (a) The system is asymptotically stable.
(b) The system is BIBO stable.
(c) The system is controllable.
(d) The system is not observable.
7.64. (a) 
(b) 
(c) The system is controllable but not observable.
7.65.
7.66.(a) 
(b) α ≥ 4
7.67.(1) 
(2) 
7.68. (a) 
(b) The system is not asymptotically stable.
(c) 
(d) The system is BIBO stable.
7.69. 
7.70. 
7.71. 
7.72. 
7.73. y(t) = e-t - e-2t, t > 0
7.74. (a) 
(b) The system is not controllable.
(c) The system is observable.







CHAPTER 8Random Signals
8.1 Introduction
Random signals, as mentioned in Chap. 1, are those signals that take random values at any given time and must be characterized statistically. However, when observed over a long period, a random signal may exhibit certain regularities that can be described in terms of probabilities and statistical averages. The probabilistic model used to describe random signals is called a random (or stochastic) process.
8.2 Random Processes
A. Definition:
Consider a random experiment with outcomes λ and a sample space S. If to every outcome λ ∈ S we assign a real-valued time function X(t, λ), we create a random (or stochastic) process. A random process X(t, λ) is therefore a function of two parameters, the time t and the outcome λ. For a specific λ, say, λi, we have a single time function X(t, λi) = xi(t). This time function is called a sample function or a realization of the process. The totality of all sample functions is called an ensemble. For a specific time tj, X (tj, λ) = Xj denotes a random variable. For fixed t (= tj) and fixed λ(= λi), X (tj, λi) = xi(tj) is a number.
Thus, a random process is sometimes defined as a family of random variables indexed by the parameter t ∈ T, where T is called the index set.
Fig. 8-1 illustrates the concepts of the sample space of the random experiment, outcomes of the experiment, associated sample functions, and random variables resulting from taking two measurements of the sample functions.
In the following we use the notation X(t) to represent X(t, λ).
EXAMPLE 8.1 Consider a random experiment of flipping a coin. The sample space is S = {H, T} where H denotes the outcome that "head" appears and T denotes the outcome that "tail" appears. Let
X(t, H) = x1(t) = sin ω1tX (t, T) = x2(t) = sin ω2t
where ω1 and ω2 are some fixed numbers. Then X(t) is a random signal with x1(t) and x2(t) as sample functions. Note that x1(t) and x2(t) are deterministic signals. Randomness of X(t) comes from the outcomes of flipping a coin.
EXAMPLE 8.2 Consider a random experiment of flipping a coin repeatedly and observing the sequence of outcomes. Then S = {λi, i = 1, 2, ...}, where λi = H or T.
Let           X(t, λi) = sin (Ωi t), (i - 1) T ≤ τ ≤ iT
where Ωi = ω1 if λi = H and Ωi = ω2 if λi = T.


Fig. 8-1 Random process.



Fig. 8-2

One realization (or sample function) of the random signal X(t) is shown in Fig. 8-2. This kind of random signal is the sort of signal that might be produced by a frequency shift keying (FSK) modem where the frequencies are determined by random sequence of data bits 1 or 0 (by replacing H = 1 and T = 0).
EXAMPLE 8.3 Often a random signal X(t) is specified in terms of random variables.
X(t) = a cos(ω0t + Θ)
where a and ω0 are fixed amplitude and frequency and Θ is a random variable (r.v.) uniformly distributed over [0, 2π]; that is, r.v. Θ is defined by Θ(λ) = λ for each λ in S = [0, 2π]. That is,
X(t, λ) = a cos(ω0t + λ) for 0 ≤ = λ ≤ 2π
The ensemble of X(t, λ) is the set of cosine functions that have the same amplitude and frequency, but whose phase angle are functions of uniform r.v. over S [0, 2π]. Some sampling functions of X(t, λ) are plotted in Fig. 8-3.
EXAMPLE 8.4 Let X1, X2, ... be independent r.v. with

Then X(n) is a discrete-time random sequence. A sample sequence of X(n) is shown in Fig. 8-4.


Fig. 8-3



Fig. 8-4

B. Description of a Random Process:
In a random process {X(t), t ∈ T }, the index set T is called the parameter set of the random process. The values assumed by X(t) are called states, and the set of all possible values forms the state space E of the random process. If the index set T of a random process is discrete, then the process is called a discrete-parameter (or discrete-time) process. A discrete-parameter process is also called a random sequence and is denoted by {Xn, n = 1, 2, ...}. If T is continuous, then we have a continuous-parameter (or continuous-time) process. If the state space E of a random process is discrete, then the process is called a discrete-state process, often referred to as a chain. In this case, the state space E is often assumed to be {0, 1, 2, ...}. If the state space E is continuous, then we have a continuous-state process.
A complex random process X(t) is defined by
X(t) = X1(t) + jX2(t)
where X1(t) and X2(t) are (real) random processes and j =  Throughout this book, all random processes are real random processes unless specified otherwise.
8.3 Statistics of Random Processes
A. Probabilistic Expressions:
Consider a random process X(t). For a particular time t1, X(t1) = X1 is a random variable, and its distribution function FX(x1; t1) is defined as

where x1 is any real number.
And FX(x1; t1) is called the first-order distribution of X(t). The corresponding first-order density function is obtained by

Similarly, given t1 and t2, X(t1) = X1 and X(t2) = X2 represent two random variables. Their joint distribution is called the second-order distribution and is given by

where x1 and x2 are any real numbers.
The corresponding second-order density function is obtained by

In a similar manner, for n random variables X (ti) = Xi(i = 1, ..., n), the nth-order distribution is

The corresponding nth-order density function is

In a similar manner, we can define a joint distribution between two random processes X(t) and Y(t). The joint distribution for X(t1) and Y(t2) is defined by

and corresponding joint density function by

The joint nth-order distribution for X(t) and Y (t) is defined by

and the corresponding nth-order density function by

B. Statistical Averages:
As in the case of random variables, random processes are often described by using statistical averages (or ensemble averages).
The mean of X(t) is defined by

where X(t) is treated as a random variable for a fixed value of t.
For discrete time processes, we use the following notation:

where pX(xn) = P (X = xn).
The autocorrelation of X(t) is defined by

The autocorrelation describes the relationship (correlation) between two samples of X(t). In order to see how the correlation between two samples depends on how far apart the samples are spaced, the autocorrelation function is often expressed as

Note that

and

The autocovariance of X(t) is defined by

It is clear that if μX(t) = 0, then CXX(t1, t2) = RXX(t1, t2).
Note that CXX(t1, t2) and RXX(t1, t2) are deterministic functions of t1 and t2.
The variance of X(t) is given by

If X(t) is a complex random process, then the autocorrelation and autocovariance of X(t) are defined by

where * denotes the complex conjugate.
In a similar manner, for discrete-time random processes (or random sequences), X(n), the autocorrelation and autocovariance of X(n) are defined by

and

If μX(n) = 0, then CXX(n1, n2) = RXX(n1, n2).
For two different random signals X(t) and Y(t), we have the following definitions. The cross-correlation of X(t) and Y (t) is defined by

The cross-covariance of X(t) and Y (t) is defined by

Some Properties of X(t) and Y (t):
Two random processes X(t) and Y (t) are independent if for all t1 and t2,

They are uncorrelated if for all t1 and t2

or

They are orthogonal if for all t1 and t2

By changing t1 and t2 by n1 and n2, respectively, similar definitions can be obtained for two different random sequences X (n) and Y (n).
C. Stationarity:
1. Strict-Sense Stationary:
A random process X(t) is called strict-sense stationary (SSS) if its statistics are invariant to a shift of origin. In other words, the process X(t) is SSS if

for any c.
From Eq. (8.31) it follows that ƒX(x1; t1) = ƒX(x1; t1 + c) for any c. Hence, the first-order density of a stationary X(t) is independent of t:

Similarly, ƒX(x1, x2; t1, t2) = ƒX(x1, x2; t1 + c, t2 + c) for any c. Setting c = - t1, we obtain

which indicates that if X(t) is SSS, the joint density of the random variables X(t) and X(t + τ) is independent of t and depends only on the time difference τ.
2. Wide-Sense Stationary:
A random process X(t) is called wide-sense stationary (WSS) if its mean is constant

and its autocorrelation depends only on the time difference τ

From Eqs. (8.17) and (8.35) it follows that the autocovariance of a WSS process also depends only on the time difference τ:

Setting τ = 0 in Eq. (8.35), we obtain

Thus, the average power of a WSS process is independent of t and equals RXX(0).
Similarly, a discrete-time random process X (n) is WSS if

and

Then

Setting k = 0 in Eq. (8.39) we have

Note that an SSS process is WSS but a WSS process is not necessarily SSS.
Two processes X(t) and Y (t) are called jointly wide-sense stationary (jointly WSS) if each is WSS and their cross-correlation depends only on the time difference τ:

From Eq. (8.42) it follows that the cross-covariance of jointly WSS X(t) and Y (t) also depends only on the time difference τ:

Similar to Eqs. (8.27) to (8.30), two jointly WSS random process X(t) and Y (t) are independent if for all x and y

They are uncorrelated if for all τ

or

They are orthogonal if for all τ

Similarly, two random sequences X(n) and Y(n) are jointly WSS if each is WSS and their cross-correlation depends only on the time difference k:

Then the cross-covariance of jointly WSS X(n) and Y(n) is

They are uncorrelated if for all k

or

They are orthogonal if for all k

D. Time Averages and Ergodicity:
The time-averaged mean of a sample function x(t) of a random process X(t) is defined as

where the symbol <·> denotes time-averaging.
Similarly, the time-averaged autocorrelation of the sample function X(t) is defined as(8.53)

Note that  and  XX(τ) are random variables; their values depend on which sample function of X(t) is used in the time-averaging evaluations.
If X(t) is stationary, then by taking the expected value on both sides of Eqs. (7.20) and (7.21), we obtain

which indicates that the expected value of the time-averaged mean is equal to the ensemble mean, and

which also indicates that the expected value of the time-averaged autocorrelation is equal to the ensemble autocorrelation.
A random process X(t) is said to be ergodic if time averages are the same for all sample functions and equal to the corresponding ensemble averages. Thus, in an ergodic process, all its statistics can be obtained by observing a single sample function X(t) = X(t, λ) (λ fixed) of the process.
A stationary process X(t) is called ergodic in the mean if

Similarly, a stationary process X(t) is called ergodic in the autocorrelation if

The time-averaged mean of a sample sequence x (n) of a random sequence X (n) is defined as

Similarly, the time-average autocorrelation of the sample sequence x (n) is defined as

If X(n) is stationary, then

and

Thus, X (n) is also ergodic in the mean and autocorrelation if

Testing for the ergodicity of a random process is usually very difficult. A reasonable assumption in the random analysis of most random signals is that the random waveforms are ergodic in the mean and in the autocorrelation. Fundamental electrical engineering parameters, such as dc value, root-mean-square (rms) value, and average power can be related to the statistical averages of an ergodic random process. They are summarized in the following:
1.  is equal to the dc level of the signal.
2.  is equal to the normalized power in the dc component.
3.  is equal to the total average normalized power.
4.  is equal to the average normalized power in the time-varying or ac component of the signal.
5.  is equal to the rms value of the ac component of the signal.
8.4 Gaussian Random Process:
Consider a random process X(t), and define n random variables X(t1), ..., X (tn) corresponding to n time instants t1, ..., tn. Let X be a random vector (n × 1 matrix) defined by

Let x be an n-dimensional vector (n × 1 matrix) defined by

so that the event {X(t1) ≤ x1, ..., X (tn) ≤ xn} is written {X ≤ x}. Then X(t) is called a Gaussian (or normal) process if X has a jointly multivariate Gaussian density function for every finite set of {ti} and every n.
The multivariate Gaussian density function is given by

where T denotes the "transpose," μ is the vector means, and C is the covariance matrix, given by

where

which is the covariance of X (ti) and X (tj), and det C is the determinant of the matrix C.
Alternate Definition:
A random process X(t) is a Gaussian process if for any integers n and any subset {t1, ..., tn} of T, and any real coefficients ak(1 ≤ k ≤ n), the r.v.

is a Gaussian r.v..
Some of the important properties of a Gaussian process are as follows:
1. A Gaussian process X(t) is completely specified by the set of means

and the set of autocorrelations

2. If the set of random variables X (ti), i = 1, ..., n, is uncorrelated, that is,

then X (ti) are independent.
3. If a Gaussian process X(t) is WSS, then X(t) is SSS.
4. If the input process X(t) of a linear system is Gaussian, then the output process Y (t) is also Gaussian.
SOLVED PROBLEMS
8.1 Consider a random process X(t) defined by

where ω is a constant and Y is a uniform r.v. over (0, 1).
(a) Describe X(t).
(b) Sketch a few typical sample functions of X(t).
(a) The random process X(t) is a continuous-parameter (or time), continuous-state random process. The state space is E {x: - 1 < x < 1} and the index parameter set is T = {t: t ≥ 0}.
(b) Three sample functions of X(t) are sketched in Fig. 8-5.
8.2. Consider a random signal X(t) given by

where {Ak} is a sequence of independent r.v.'s with P [Ak = A] = P [Ak = - A] = , p (t) is a unit amplitude pulse of duration Tb, and Td is a r.v uniformly distributed over [0, Tb].
(a) Describe X(t).
(b) Sketch a sample function of X(t).
(a) The random signal X(t) is a continuous time, discrete-state random process. The state space is (A, - A) and the index parameter set is T = {t: - ∞ < t < ∞). X(t) is known as a random binary signal.
(b) A sample function of X(t) is sketched in Fig. 8-6.


Fig. 8-5



Fig. 8-6 Random binary signal.

8.3. Consider a random process {X(t); t ≥ 0}, where X(t) represents the total number of "events" that have occurred in the interval (0, t).
(a) Describe X(t).
(b) Sketch a sample function of X(t).
(a) From definition, X(t) must satisfy the following conditions:
1. X(t) ≥ 0 and X (0) = 0
2. X(t) is integer valued.
3. X(t1) ≤ X(t2) if t1 < t2
4. X(t2) = X(t1) equals the number of events that have occurred on the interval (t1, t2).
Thus, X(t) is a continuous-time discrete state random process.
Note that X(t) is known as a counting process. A counting process X(t) is said to possess independent increments if the number of events which occur in disjoint intervals are independent.
(b) A sample function of X(t) is sketched in Fig. 8-7.


Fig. 8-7 A sampling function of a counting process.

8.4. Let W1, W2, ... be independent identically distributed (i.i.d.) zero-mean Gaussian r.v.'s. Let

with X0 = 0. The collection of r.v.'s X (n) = {Xn, n ≥ 0} is a random process.
(a) Describe X (n).
(b) Sketch a sample function of X (n).
(a) The random signal X (n) is a discrete time, continuous-state random process. The state space is E (-∞, ∞) and the index parameter set is T = {0, 1, 2, ...}.
(b) A sample function of X(t) is sketched in Fig. 8-8.


Fig. 8-8

8.5. Let Z1, Z2, ... be independent identically distributed r.v.'s with P (Zn = 1) = p and P (Zn = -1) = q = 1 - p for all n. Let

and X0 = 0. The collection of r.v.'s {Xn, n ≥ 0} is a random process, and it is called the simple random walk X(n) in one dimension.
(a) Describe the simple random walk X (n).
(b) Construct a typical sample sequence (or realization) of X (n).
(a) The simple random walk X(n) is a discrete-parameter (or time), discrete-state random process. The state space is E {..., -2, -1, 0, 1, 2, ...}, and the index parameter set is T = {0, 1, 2, ...}.
(b) A sample sequence x (n) of a simple random walk X (n) can be produced by tossing a coin every second and letting x (n) increase by unity if a head appears and decrease by unity if a tail appears. Thus, for instance,

The sample sequence x (n) obtained above is plotted in Fig. 8-9. The simple random walk X (n) specified in this problem is said to be unrestricted because there are no bounds on the possible values of Xn.


Fig. 8-9

8.6. Give an example of a complex random signal.
Consider a random signal X(t) given by

where ω is a constant, and A (t) and Θ(t) are real random signals. Now X(t) can be rewritten as

where Re denotes "take real part of." Then

is a complex random signal.
Statistics of Random Processes
8.7. Let a random signal X(t) be specified by

where Y is an exponential r.v. with pdf

Find the first-order cdf of X(t), FX(x; t).

Next, if x > t, then t - x < 0 and Y ≥ 0, and

Thus,

8.8. A discrete-time random sequence X(n) is defined by X(n) = An(n ≥ 0), where A is a uniform r.v. over (0, 1). Find the mean μX(n) and autocorrelation RXX(n, m) of X(n).
The pdf of A is given by

Then

and

8.9. Show that

From definition (8.13)

since E [Y2] = 0, for any r.v. Y.
8.10. Show that

Since E [Y2] = 0, for any r.v. Y, we have

From Eqs. (8.85) and (8.86) we have

which imply that

8.11. Consider a random signal given by

where ω0 is a constant and A is an uniform r.v. over [0,1]. Find the mean μX(t) and autocorrelation RXX(t1, t2) of X(t).
The pdf of A is given by


since

since

8.12. A random sequence X (n) is defined as

where A and B are independent zero mean Gaussian r.v.'s of variance  and  respectively.
(a) Find the mean μX(n) and autocorrelation RXX (n, m) of X (n).
(b) Find E [X2(n)].
(a) 
since E [A] = E [B] = 0.

since E [AB] = E [BA] = E [A ]E [B] = 0.
(b) Setting m = n in Eq. (8.92), we obtain

8.13. A counting process X(t) of Prob. 8.3 is said to be a Poisson process with rate (or intensity) λ (> 0) if
1. X (0) = 0.
2. X(t) has independent increments.
3. The number of events in any interval of length t is Poisson distributed with mean λ t; that is, for all s, t > 0,

(a) Find the mean μX(t) and E [X2(t)].
(b) Find the autocorrelation RXX(t1, t2) of X(t).
(a) Setting s = 0 in Eq. (8.94) and using condition 1, we have

Thus,

Now, the Taylor expansion of eλt is given by

Differentiating twice with respect to λt, we obtain

Using Eqs. (9.97) and (9.98), we obtain and

and

(b) Next, let t1 < t2, the r.v.'s X(t1) and X(t2 - t1) are independent since the intervals (0, t1) and (t1, t2) are non-overlapping, and they are Poisson distributed with mean λt1 and λ(t2 - t1), respectively. Thus,

Now using identity
X(t1) X(t2) = X(t1) [X(t1) + X(t2) - X(t1)] = [X2(t1)] + X(t1) [X(t2) - X(t1)]
we have

Interchanging t1 and t2, we have

Thus, combining Eqs. (102) and (8.103), we obtain

8.14. Let X(t) and Y (t) be defined by

where ω is constant and A and B are independent random variables both having zero mean and variance σ2. Find the cross-correlation of X(t) and Y (t).
The cross-correlation of X(t) and Y (t) is

or

where τ = t2 -t1.
8.15. Consider a random process X(t) given by

where A and ω are constants and Θ is a uniform random variable over [-π, π]. Show that X(t) is WSS.
From Eq. (B.57) (Appendix B), we have

Thus,


Since the mean of X(t) is a constant and the autocorrelation of X(t) is a function of time difference only, we conclude that X(t) is WSS.
Note that RXX (τ) is periodic with the period T0 = 2π/ω. A WSS random process is called periodic if its autocorrelation is periodic.
8.16. Consider a random process X(t) given by

where ω and θ are constants and A is a random variable. Determine whether X(t) is WSS.

which indicates that the mean of X(t) is not constant unless E [A] = 0.

Thus, we see that the autocorrelation of X(t) is not a function of the time difference τ only, and the process X(t) is not WSS.
8.17. Consider a random process X(t) given by

where ω is constant and A and B are random variables.
(a) Show that the condition

is necessary for X(t) to be stationary.
(b) Show that X(t) is WSS if and only if the random variables A and B are uncorrelated with equal variance; that is,

and

(a) μX(t) = E [X (t)] = E [A ]cos ωt + E [B ]sin ωt must be independent of t for X(t) to be stationary. This is possible only if μX(t) = 0; that is,
E [A] = E [B] = 0
(b) If X(t) is WSS, then from Eq. (8.37)

But

Thus

Using the preceding result, we obtain

which will be a function of τ only if E [AB] = 0.
Conversely, if E [AB] = 0 and E [A2] = E [B2] = σ2, then from the result of part (a) and Eq. (8.118), we have

Hence, X(t) is WSS.
8.18. A random process X(t) is said to be covariance-stationary if the covariance of X(t) depends only on the time difference τ = t2 - t1; that is,

Let X(t) be given by

where A and B are independent random variables for which
E [A] = E [B] = 0 and E [A2] = E [B2] = 1
Show that X(t) is not WSS, but it is covariance-stationary.
μX(t) = E [X (t)] = E [(A + 1) cos t + B sin t)]
= E [A + 1] cos t + E [B ]sin t
= cos t
which depends on t. Thus, X(t) cannot be WSS.

Now

Substituting these values into the expression of RXX(t1, t2), we obtain

From Eq. (8.17), we have

Thus, X(t) is covariance-stationary.
8.19. Show that if a random process X(t) is WSS, then it must also be covariance stationary.
If X(t) is WSS, then
E [X (t)] = μ (constant) for all t
RXX(t, t + τ) = RXX(t) for all t
Now
CXX(t, t + τ) = Cov[X(t)X(t + τ)] = RXX(t, t + τ) - E [X (t)] E [X (t + τ)]
= RXX(τ) - μ2
which indicates that CXX(t, t + τ) depends only on τ. Thus, X(t) is covariance stationary.
8.20. Show that if X(t) is WSS, then

where RXX(τ) is the autocorrelation of X(t).
Using the linearity of E (the expectation operator) and Eqs. (8.35) and (8.37), we have

8.21. Let X(t) = A cos (ωt + Θ), where ω is constant and both A and Θ are r.v.'s with pdf ƒA(a) and ƒΘ(θ), respectively. Find the conditions that X(t) is WSS.

The first condition for the double integral to be independent of t is for A and Θ to be statistically independent. Then

The second condition is for Θ to be uniformly distributed over [0, 2 π]. Then we have μX(t) = 0 since 
Next

Since A and Θ are independent, we have

and E [cos ω (t2 + t1) = 2Θ] = 0 since Θ is uniformly distributed over [0, 2π].
Thus,

So, we conclude that X(t) is WSS if A and Θ are independent, and Θ is uniformly distributed over [0, 2π].
8.22. Let Z(t) = X(t) + Y(t), where random processes X(t) and Y(t) are independent and WSS. Is Z(t) WSS?


Since the mean of Z (t) is constant and its autocorrelation depends only on τ, Z (t) is WSS.
8.23. Let Z (t) = X(t) + Y(t), where random processes X(t) and Y (t) are jointly WSS. Show that if X(t) and Y (t) are orthogonal, then


Since X(t) and Y (t) are orthogonal, then RXY (τ) = 0, and we have
RZZ(τ) = RXX(τ) + RYY(τ)
8.24. A random signal X(t) is defined as X(t) = At + B, where A and B are independent r.v.'s with both zero mean and unit variance. Is X(t) WSS?
μX(t) = E [X (t)] = E [At + B] = E [A] t + E [B] = 0
since E [A] = E [B] = 0.

since E [A2] = E [B2] = 1 and E [AB] = E [BA] = E [A] E [B] = 0.
Since RXX(t1, t2) is not the function of |t2 - t1|, X(t) is not WSS.
8.25. Let X (n) = {Xn, n ≥ 0} be a random sequence of iid r.v.'s with mean 0 and variance 1. Show that X (n) is WSS.
μX(n) = E [X (n)] = E [Xn] = 0 constant

which depends only on k. Thus X (n) is WSS.
8.26. A random signal X(t) is defined as X(t) = A, where A is a r.v. uniformly distributed over [0, 1]. Is X(t) ergodic in the mean?

Since  ≠ μX(t), X(t) is not ergodic in the mean.
8.27. Show that the process X(t) defined in Eq. (8.108) (Prob. 8.15) is ergodic in both the mean and the autocorrelation.
From Eq. (8.53), we have

where T0 = 2π/ω.
From Eq. (8.54), we have

Thus, we have

Hence, by definitions (8.57) and (8.58), we conclude that X(t) is ergodic in both the mean and the autocorrelation.
8.28. Consider a random process Y (t) defined by

where X(t) is given by

where ω is constant and A = N [0; σ2].
(a) Determine the pdf of Y (t) at t = tk.
(b) Is Y (t) WSS?
(a) 
Then from the result of Example B.10 (Appendix B) we see that Y (tk) is a Gaussian random variable with

and

Hence, by Eq. (B.53), the pdf of Y(tk) is

(b) From Eqs. (8.137) and (8.138), the mean and variance of Y (t) depend on time t (tk), so Y (t) is not WSS.
8.29. Show that if a Gaussian random process is WSS, then it is SSS.
If the Gaussian process X(t) is WSS, then
μi = E [X (ti)] = μ (= constant) for all ti
and
RXX(ti, tj) = RXX(tj - ti)
Therefore, in the expression for the joint probability density of Eq. (8.67) and Eqs. (8.68), (8.69), and (8.70).
μ1 = μ2 = ... = μn = μ → E [X (ti)] = E [X (ti + c)]
Cij = CXX(ti, tj) = RXX (ti, tj) - μiμj
= RXX(tj - ti) = μ2 = CXX(ti + c, tj + c)
for any c. It then follows that

for any c. Therefore, X(t) is SSS by Eq. (8.31)
8.30. Let X be an n-dimensional Gaussian random vector [Eq. (8.65)] with independent components. Show that the multivariate Gaussian joint density function is given by

where μi = E [Xi] and σ2i = var(Xi).
The multivariate Gaussian density function is given by Eq. (8.67). Since Xi = X (ti) are independent, we have

Thus, from Eq. (8.69) the covariance matrix C becomes

It therefore follows that

and

Then we can write

Substituting Eqs. (8.143) and (8.145) into Eq. (8.67), we obtain Eq. (8.140).
SUPPLEMENTARY PROBLEMS
8.31. Consider a random process X(t) defined by
X(t) = cos Ωt
where Ω is a random variable uniformly distributed over [0, ω0]. Determine whether X(t) is stationary.
8.32. Consider the random process X(t) defined by
X(t) = A cos ω t
where ω is a constant and A is a random variable uniformly distributed over [0, 1]. Find the autocorrelation and autocovariance of X(t).
8.33. Let X(t) be a WSS random process with autocorrelation
RXX (τ) = Ae-α | τ |
Find the second moment of the random variable Y = X (5) - X (2).
8.34. A random signal X(t) is given by X(t) = A (t) cos (ωt + Θ), where A (t) is a zero mean WSS random signal with autocorrelation RAA(τ), and Θ is a r.v. uniformly distributed over [0, 2π] and independent of A (t). The total average power of A (t) is 1 watt.
(a) Show that X(t) is WSS.
(b) Find the total average power of X(t).
8.35. A random signal X(t) is given by X(t) = A + B cos (ωt + Θ), where A, B, and Θ are independent r.v.'s uniformly distributed over [0, 1], [0, 2] and [0, 2π], respectively. Find the mean and the autocorrelation of X(t).
8.36. Let X(t) be a WSS random process with mean μX. Let Y (t) = a X (t). Is Y (t) WSS?
8.37. Let X(t) and Y (t) be defined by
X(t) = A + Bt, Y (t) = B + At
where A and B are independent r.v.'s with zero means and variance  and , respectively. Find the autocorrelations and cross-correlation of X(t) and Y (t).
8.38. Let Z (t) = X(t)Y (t), where X(t) and Y (t) are independent and WSS. Is Z (t) WSS?
8.39. Two random signals X(t) and Y (t) are given by
X(t) = A cos ωt + B sin ω t, Y (t) = B cos ωt - A sin ωt
where ω is a constant, and A and B are independent r.v.'s with zero mean and same variance σ2. Find the cross-correlation function of X(t) and Y (t).
ANSWERS TO SUPPLEMENTARY PROBLEMS
8.31. Nonstationary.
Hint: Examine specific sample functions of X(t) for different frequencies, say, Ω = π/2, π, and 2π.
8.32.
8.33. 2A (1 - e-3a)
8.34. (a) Yes. (b) 1/2 watts.
8.35.
8.36. Yes.
8.37.
8.38. Yes.
8.39. RXY(t1, t2) = σ2 sin ω(t1 - t2)







CHAPTER 9Power Spectral Density and Random Signals in Linear System
9.1 Introduction
In this chapter, the notion of power spectral density for a random signal is introduced. This concept enables us to study wide-sense stationary random signals in the frequency domain and define a white-noise process. The response of a linear system to random signal is then studied.
9.2 Correlations and Power Spectral Densities
In the following, we assume that all random processes are WSS.
A. Autocorrelation RXX(τ):
The autocorrelation of X(t) is [Eq. (8.35)]

Properties of RXX(τ):

Property 3 [Eq. (9.4)] is easily obtained by setting τ = 0 in Eq. (9.1). If we assume that X(t) is a voltage waveform across a 1 - Ω resistor, then E [X2(t)] is the average value of power delivered to the 1 - Ω resistor by X(t). Thus, E[X2(t)] is often called the average power of X(t). Properties 1 and 2 are verified in Prob. (9.1).
In case of a discrete-time random process X(n), the autocorrelation function of X(n) is defined by [Eq. (8.39)]

Various properties of RXX(k) similar to those of RXX(τ) can be obtained by replacing τ by k in Eqs. (9.2) to (9.4).
B. Cross-Correlation RXY(t):
The cross-correlation of X(t) and Y(t) is [Eq. (8.42)]

Properties of RXY(t):

These properties are verified in Prob. 9.2.
Similarly, the cross-correlation function of two discrete-time jointly WSS random sequences X(n) and Y(n) is defined by

And various properties of RXY(k) similar to those of RXY(τ) can be obtained by replacing τ by k in Eqs. (9.7) to (9.9).
C. Power Spectral Density or Power Spectrum:
Let RXX(τ) be the autocorrelation of X(t). Then the power spectral density (or power spectrum) of X(t) is defined by the Fourier transform of RXX(τ) as

Thus,
Equations (9.11) and (9.12) are known as the Wiener-Khinchin relations.
Properties of SXX(ω):

Similarly, the power spectral density SXX(Ω) of a discrete-time random process X(n) is defined as the Fourier transform of RXX(k):

Thus, taking the inverse Fourier transform of SXX(Ω), we obtain(9.16)

Properties of SX(Ω):

Note that property 1 [Eq. (9.18)] follows from the fact that e-jΩk is periodic with period 2π. Hence it is sufficient to define SXX(Ω) only in the range (-π, π).
D. Cross-Power Spectral Densities:
The cross-power spectral density (or cross-power spectrum) SXY(ω) of two continuous-time random processes X(t) and Y(t) is defined as the Fourier transform of RXY(τ):

Thus, taking the inverse Fourier transform of SXY(ω), we get

Properties of SXY(ω):
Unlike SXX(ω), which is a real-valued function of ω, SXY(ω), in general, is a complex-valued function.

Similarly, the cross-power spectral density SXY(Ω) of two discrete-time random processes X(n) and Y(n) is defined as the Fourier transform of RXY(k):

Thus, taking the inverse Fourier transform of SXY(Ω), we get

Properties of SXY(Ω):
Unlike SXX(Ω), which is a real-valued function of Ω, SXY(Ω), in general, is a complex-valued function.

9.3 White Noise
A random process X(t) is called white noise if [Fig. 9-1(a)]

Taking the inverse Fourier transform of Eq. (9.31), we have

which is illustrated in Fig. 9-1(b). It is usually assumed that the mean of white noise is zero.
Similarly, a zero-mean discrete-time random sequence X(n) is called a discrete-time white noise if

Again the power spectral density of X(n) is constant. Note that SXX(Ω + 2π) = SXX(Ω) and the average power of X(n) is σ2 = Var[X(n)], which is constant. Taking the discrete-time inverse Fourier transform of Eq. (9.33), we have



Fig. 9-1 White noise.

Band-Limited White Noise:
A random process X(t) is called band-limited white noise if

Then

And SXX(ω) and RXX(τ) of band-limited white noise are shown in Fig. 9-2.
Note that the term white or band-limited white refers to the spectral shape of the process X(t) only, and these terms do not imply that the distribution associated with X(t) is Gaussian.


Fig. 9-2 Band-limited white noise.

Narrowband Random Process:
Suppose that X(t) is a WSS process with zero mean and its power spectral density SXX(ω) is nonzero only in some narrow frequency band of width 2W that is very small compared to a center frequency ωc. Then the process X(t) is called a narrowband random process.
In many communication systems, a narrowband process (or noise) is produced when white noise (or broadband noise) is passed through a narrowband linear filter. When a sample function of the narrowband process is viewed on an oscilloscope, the observed waveform appears as a sinusoid of random amplitude and phase. For this reason, the narrowband noise X(t) is conveniently represented by the expression

9.4 Response of Linear System to Random Input
A. Linear System:
As we discussed in Chap. 1 (Sec. 1.5), a system is a mathematical model for a physical process that relates the input (or excitation) signal x to the output (or response) y, and the system is viewed as a transformation (or mapping) of x into y. This transformation is represented by the operator T as (Eq. (1.60))

For a continuous-time linear time-invariant (LTI) system, Eq. (9.38) can be expressed as Eq. (2.60)

where h(t) is the impulse response of a continuous-time LTI system. For a discrete-time LTI system, Eq. (9.38) can be expressed as (Eq. (2.45))

where h(n) is the impulse response (or unit sample response) of a discrete-time LTI system.
B. Response of a Continuous-Time Linear System to Random Input:
When the input to a continuous-time linear system represented by Eq. (9.38) is a random process {X(t), t ∈ Tx}, then the output will also be a random process {Y(t), t ∈ Ty}; that is,

For any input sample function xi(t), the corresponding output sample function is

If the system is LTI, then by Eq. (9.39), we can write

Note that Eq. (9.43) is a stochastic integral. Then

If the input X(t) is WSS, then from Eq. (9.43) we have

where H(0) is the frequency response of the linear system at ω = 0. Thus, the mean of the output is a constant.
The autocorrelation of the output given in Eq. (9.45) becomes

which indicates that RYY(t1, t2) is a function of the time difference τ = t2 - t1. Hence,

Thus, we conclude that if the input X(t) is WSS, the output Y(t) is also WSS.
The cross-correlation function between input X(t) and Y(t) is given by

When input X(t) is WSS, Eq. (9.49) becomes

which indicates that RXY(t1, t2) is a function of the time difference τ = t2 - t1. Hence

Thus, we conclude that if the input X(t) to an LTI system is WSS, the output Y(t) is also WSS. Moreover, the input X(t) and output Y(t) are jointly WSS.
In a similar manner, it can be shown that (Prob. 9.11)

Substituting Eq. (9.51) into Eq. (9.52), we have

Now taking Fourier transforms of Eq. (9.51), (9.52), and (9.53) and using convolution property of Fourier transform [Eq. (5.58)], we obtain

The schematic of these relations is shown in Fig. 9-3.


Fig. 9-3.

Equation (9.56) indicates the important result that the power spectral density of the output is the product of the power spectral density of the input and the magnitude squared of the frequency response of the system.
When the autocorrelation of the output RYY(τ) is desired, it is easier to determine the power spectral density SYY(ω) and then to evaluate the inverse Fourier transform (Prob. 9.13). Thus,

By Eq. (9.4), the average power in the output Y(t) is

C. Response of a Discrete-Time LTI System to Random Input:
When the input to a discrete-time LTI system is a discrete-time random sequence X(n), then by Eq. (2.39), the output Y(n) is

The autocorrelation function of Y(n) is given by (Prob. 9.22)

The cross-correlation function of X(n) and Y(n) is given by (Prob. 9.23)

When X(n) is WSS, then from Eq. (9.59)

where H(0) = H(Ω) |Ω = 0 and H(Ω) is the frequency response of the system defined by the Fourier transform of H(n).
The autocorrelation function of Y(n) is, from Eq. (9.60)

Setting m = n + k, we get

Similarly, from Eq. (9.61), we obtain

and (Prob. 9.24)

Substituting Eq. (9.65) into Eq. (9.66), we obtain

Now taking Fourier transforms of Eq. (9.65), (9.66) and (9.67), we obtain

Similarly, when the autocorrelation function of the output RYY(k) is desired, it is easier to determine the power spectral density SYY(Ω) and then take the inverse Fourier transform. Thus,

By Eq. (9.21), the average power in the output Y(n) is

SOLVED PROBLEMS
Correlations and Power Spectral Densities
9.1. Let X(t) be a WSS random process. Verify Eqs. (9.2) and (9.3); that is,(a) RXX(-τ) = RXX(τ)
(b) 
(a) From Eq. (8.35)

Setting t + τ = t', we have
(b) 
9.2. Let X(t) and Y(t) be WSS random processes. Verify Eqs. (9.7) and (9.8); that is,
(a) 
(b) 
(a) By Eq. (8.42)

Setting t - τ = t', we obtain

(b) From the Cauchy-Schwarz inequality Eq. (B.129) (Appendix B), it follows that

9.3. Show that the power spectrum of a (real) random process X(t) is real, and verify Eq. (9.14); that is,
SXX(-ω) = SXX(ω)
From Eq. (9.11) and by expanding the exponential, we have

Since RXX(-τ) = RXX(t) [Eq. (9.2)] imaginary term in Eq. (9.73) then vanishes and we obtain

which indicates that SXX(ω) is real.
Since the cosine is an even function of its arguments, that is, cos (-ωτ) = cos ωτ, it follows that
SXX(-ω) = SXX(ω)
which indicates that the power spectrum of X(t) is an even function of frequency.
9.4. Let X(t) and Y(t) be both zero-mean and WSS random processes. Consider the random process Z(t) defined by

(a) Determine the autocorrelation and the power spectrum of Z(t) if X(t) and Y(t) are jointly WSS.
(b) Repeat part (a) if X(t) and Y(t) are orthogonal.
(c) Show that if X(t) and Y(t) are orthogonal, then the mean square of Z(t) is equal to the sum of the mean squares of X(t) and Y(t).
(a) The autocorrelation of Z(t) is given by

If X(t) and Y(t) are jointly WSS, then we have

where τ = t2 - t1.
Taking the Fourier transform of both sides of Eq. (9.77), we obtain

(b) If X(t) and Y(t) are orthogonal [Eq. (8.47)],
RXY(τ) = RYX(τ) = 0
Then Eqs. (9.77) and (9.78) become

and

(c) From Eqs. (9.79) and (8.37)

or

which indicates that the mean square of Z(t) is equal to the sum of the mean squares of X(t) and Y(t).
9.5. Two random processes X(t) and (Y(t) are given by

where A and ω are constants and Θ is a uniform random variable over [0, 2π]. Find the cross-correlation of X(t) and Y(t), and verify Eq. (9.7).
From Eq. (8.42), the cross-correlation of X(t) and Y(t) is

Similarly,

From Eqs. (9.84) and (9.85)

which verifies Eq. (9.7).
9.6. A class of modulated random signal Y(t) is defined by

where X(t) is the random message signal and A cos (ωct + Θ) is the carrier. The random message signal X(t) is a zero-mean stationary random process with autocorrelation RXX(τ) and power spectrum SXX(ω). The carrier amplitude A and the frequency ωc are constants, and phase Θ is a random variable uniformly distributed over [0, 2π]. Assuming that X(t) and Θ are independent, find the mean, autocorrelation, and power spectrum of Y(t).

since X(t) and Θ are independent and E [X(t)] = 0.

Since the mean of Y(t) is a constant and the autocorrelation of Y(t) depends only on the time difference τ, Y(t) is WSS. Thus,

By Eqs. (9.11) and (5.144)

Then, using the frequency convolution theorem (5.59) and Eq. (2.59), we obtain

9.7. Consider a random process X(t) that assumes the values ± A with equal probability. A typical sample function of X(t) is shown in Fig. 9-4. The average number of polarity switches (zero crossings) per unit time is α. The probability of having exactly k crossings in time τ is given by the Poisson distribution [Eq. (B.48)]


Fig. 9-4 Telegraph signal.

where Z is the random variable representing the number of zero crossing. The process X(t) is known as the telegraph signal. Find the autocorrelation and the power spectrum of X(t).
If τ is any positive time interval, then

which indicates that the autocorrelation depends only on the time difference τ. By Eq. (9.2), the complete solution that includes τ < 0 is given by

which is sketched in Fig. 9-5(a).
Taking the Fourier transform of both sides of Eq. (9.91), we see that the power spectrum of X(t) is [Eq. (5.138)]

which is sketched in Fig. 9-5(b).


Fig. 9-5

9.8. Consider a random binary process X(t) consisting of a random sequence of binary symbols 1 and 0. A typical sample function of X(t) is shown in Fig. 9-6. It is assumed that
1. The symbols 1 and 0 are represented by pulses of amplitude + A and -A V, respectively, and duration Tbs.
2. The two symbols 1 and 0 are equally likely, and the presence of a 1 or 0 in any one interval is independent of the presence in all other intervals.
3. The pulse sequence is not synchronized, so that the starting time td of the first pulse after t = 0 is equally likely to be anywhere between 0 to Tb. That is, td is the sample value of a random variable Td uniformly distributed over [0, Tb].


Fig. 9-6 Random binary signal.

Find the autocorrelation and power spectrum of X(t).
The random binary process X(t) can be represented by

where {Ak} is a sequence of independent random variables with P[Ak = A] = P[Ak = -A] = , p(t) is a unit amplitude pulse of duration Tb, and Td is a random variable uniformly distributed over [0, Tb].

Let t2 > t1. When t2 - t1 > Tb, then t1 and t2 must fall in different pulse intervals [Fig. 9-7(a)] and the random variables X(t1) and X(t2) are therefore independent. We thus have

When t2 - t1 < Tb, then depending on the value of Td, t1 and t2 may or may not be in the same pulse interval [Fig. 9-7(b) and (c)]. If we let B denote the random event "t1 and t2 are in adjacent pulse intervals," then we have

Now

Since P(B) will be the same when t1 and t2 fall in any time range of length Tb, it suffices to consider the case 0 < t < Tb, as shown in Fig. 9-7(b). From Fig. 9-7 (b);

From Eq.(B.4), we have

Thus,

where τ = t2 - t1.


Fig. 9-7

Since RXX(-τ) = RXX(τ), we conclude that

which is plotted in Fig. 9-8(a).
From Eqs. (9.94) and (9.97), we see that X(t) is WSS. Thus, from Eq. (9.11), the power spectrum of X(t) is (Prob. 5.67)

which is plotted in Fig. 9-8(b).


Fig. 9-8

Response of Linear System to Random Input
9.9. A WSS random process X(t) is applied to the input of an LTI system with impulse response h(t) = 3e-2t u(t).
Find the mean value of the output Y(t) of the system if E[X(t)] = 2.
By Eq. (5.45), the frequency response H(ω) of the system is

Then, by Eq. (9.46), the mean value of Y(t) is

9.10. Let Y(t) be the output of an LTI system with impulse response h(t), when X(t) is applied as input. Show that
(a) 
(b) 
(a) Using Eq. (9.43), we have

(b) Similarly,

9.11. Let X(t) be a WSS random input process to an LTI system with impulse response h(t), and let Y(t) be the corresponding output process. Show that

where * denotes the convolution and H *(ω) is the complex conjugate of H(ω).
(a) If X(t) is WSS, then Eq. (9.99) of Prob. 9.10 becomes

which indicates that RXY(t1, t2) is a function of the time difference τ = t2 - t1 only. Hence, Eq. (9.105) yields

(b) Similarly, if X(t) is WSS, then Eq. (9.100) becomes

or

(c) Taking the Fourier transform of both sides of Eq. (9.101) and using Eqs. (9.22) and (5.58), we obtain
SXY(ω) = H(ω) SXX(ω)
(d) Similarly, taking the Fourier transform of both sides of Eq. (9.102) and using Eqs. (9.11), (5.58), and (5.53), we obtain
SYY(ω) = H *(ω)SXY(ω)
Note that by combining Eqs. (9.103) and (9.104), we obtain Eq. (9.56); that is,
SYY(ω) = H *(ω)H(ω)SXX(ω) = |H(ω)|2SXX(ω)
9.12. Let X(t) and Y(t) be the wide-sense stationary random input process and random output process, respectively, of a quadrature phase-shifting filter (-π/2 rad phase shifter of Prob. 5.48). Show that

where  XX(τ) is the Hilbert transform of RXX(τ).
(a) The Hilbert transform  (t) of X(t) was defined in Prob. 5.48 as the output of a quadrature phase-shifting filter with

Since |H(ω)|2 = 1, we conclude that if X(t) is a WSS random signal, then Y(t) =  (t) and by Eq. (9.56)

Hence,

(b) Using Eqs. (9.101) and (5.174), we have

9.13. A WSS random process X(t) with autocorrelation
RXX(τ) = Ae-a|τ|
where A and a are real positive constants, is applied to the input of an LTI system with impulse response
h(t) = e- bt u(t)
where b is a real positive constant. Find the autocorrelation of the output Y(t) of the system.
Using Eq. (5.45), we see that the frequency response H(ω) of the system is

So

Using Eq. (5.138), we see that the power spectral density of X(t) is

By Eq. (9.56), the power spectral density of Y(t) is

Taking the inverse Fourier transform of both sides of the above equation and using Eq. (5.139), we obtain

9.14. Verify Eq. (9.13); that is, the power spectrum of any WSS process X(t) is real and
SXX(ω) ≥ 0
for every ω.
The realness of the power spectrum of X(t) was shown in Prob. 9.3. Consider an ideal bandpass filter with frequency response (Fig. 9-9)

with a random process X(t) as its input. From Eq. (9.56) it follows that the power spectrum SYY(ω) of the resulting output Y(t) equals

Hence, from Eq. (9.58), we have

which indicates that the area of SXX(ω) in any interval of ω is nonnegative. This is possible only if SXX(ω) ≥ 0 for every ω.


Fig. 9-9

9.15. Consider a WSS process X(t) with autocorrelation RXX(τ) and power spectrum SXX(ω). Let X'(t) = dX(t)/dt. Show that

A system with frequency response H(ω) = jω is a differentiator (Fig. 9-10). Thus, if X(t) is its input, then its output is Y(t) = X'(t) [see Eq. (5.55)].
(a) From Eq. (9.103)



Fig. 9-10 Differentiator.

Taking the inverse Fourier transform of both sides, we obtain

(b) From Eq. (9.104)
SX'X' (ω) = H *(ω)SXX'(ω) = -jωSXX'(ω)
Again taking the inverse Fourier transform of both sides and using the result of part (a), we have

(c) From Eq. (9.56)
SX'X'(ω) = |H(ω)|2SXX(ω) = |jω|2SXX(ω) = ω2SXX(ω)
9.16. Suppose that the input to the differentiator of Fig. 9-10 is the zero-mean random telegraph signal of Prob. 9.7.
(a) Determine the power spectrum of the differentiator output and plot it.
(b) Determine the mean-square value of the differentiator output.
(a) From Eq. (9.92) of Prob. 9.7

For the differentiator H(ω) = jω, and from Eq. (9.56), we have

which is plotted in Fig. 9-11.


Fig. 9-11

(b) From Eq. (9.58) or Fig. 9-11

9.17. Suppose the random telegraph signal of Prob. 9.7 is the input to an ideal bandpass filter with unit gain and narrow bandwidth WB(=2πB)(≪ωc) centered at ωc = 2α. Find the dc component and the average power of the output.
From Eqs. (9.56) and (9.92) and Fig. 9-5 (b), the resulting output power spectrum
SYY(ω) = |H(ω)|2SXX(ω)
is shown in Fig. 9-12. Since H(0) = 0, from Eq. (9.46) we see that
μY = H(0)μX = 0
Hence, the dc component of the output is zero.
From Eq. (9.92) (Prob. 9.7)

Since WB ≪ wc,

The average output power is



Fig. 9-12

9.18. Suppose that a WSS random process X(t) with power spectrum SXX(ω) is the input to the filter shown in Fig. 9-13. Find the power spectrum of the output process Y(t).


Fig. 9-13

From Fig. 9-13, Y(t) can be expressed as

From Eq. (2.1) the impulse response of the filter is

and by Eqs. (5.140) and (5.50) the frequency response of the filter is
H(ω) = 1 - e-jωT
Thus, by Eq. (9.56) the output power spectrum is

9.19. Suppose that X(t) is the input to an LTI system with impulse response h1(t) and that Y(t) is the input to another LTI system with impulse response h2(t). It is assumed that X(t) and Y(t) are jointly wide-sense stationary. Let V(t) and Z(t) denote the random process at the respective system outputs (Fig. 9-14). Find the cross-correlation and cross spectral density of V(t) and Z(t) in terms of the cross-correlation and cross spectral density of X(t) and Y(t).


Fig. 9-14

Using Eq. (9.43), we have

since X(t) and Y(t) are jointly WSS.
Equation (9.116) indicates that RVZ(t1, t2) depends only on the time difference τ = t2 - t1. Thus,

Taking the Fourier transform of both sides of Eq. (9.117), we obtain(9.117)

Let τ + α - β = λ, or equivalently τ = λ - α + β. Then

where H1(ω) and H2(ω) are the frequency responses of the respective systems in Fig. 9-14.
9.20. The input X(t) to the RC filter shown in Fig. 9-15 is a white-noise process.
(a) Determine the power spectrum of the output process Y(t).
(b) Determine the autocorrelation and the mean-square value of Y(t).


Fig. 9-15 RC filter.

From Eq. (5.91) the frequency response of the RC filter is

(a) From Eqs. (9.31) and (9.56)

(b) Rewriting Eq. (9.119) as

and using the Fourier transform pair Eq. (5.138), we obtain

Finally, from Eq. (9.120)

9.21. The input X(t) to an ideal bandpass filter having the frequency response characteristic shown in Fig. 9-16 is a white-noise process. Determine the total noise power at the output of the filter.


Fig. 9-16


The total noise power at the output of the filter is

where B = WB/(2π) (in Hz).
9.22. Verify Eq. (9.60); that is

From Eq. (9.59) we have

9.23. Verify Eq. (9.61); that is

From Eq. (9.59), we have

9.24. Verify Eq. (9.66); that is
RYY(k) = h(-k) * RXY(k)
From Eq. (9.64), and using Eq. (9.65), we obtain

9.25. The output Y(n) of a discrete-time system is related to the input X(n) by

If the input is a zero-mean discrete-time white noise with power spectral density σ2, find E[Y2(t)].
The impulse response h(n) of the system is given by
h(n) = δ(n) = δ(n - 1)
Taking the discrete-time Fourier transform of h(n), the frequency response H(Ω) of the system is given by
H(Ω) = 1 = e-jΩ
Then

Since SXX(Ω) = σ2, and by Eq. (9.70), we have

Thus, by Eq. (9.72) we obtain

9.26. The discrete-time system shown in Fig. 9-17 consists of one unit delay element and one scalar multiplier (a < 1). The input X(n) is discrete-time white noise with average power σ2. Find the spectral density and average power of the output Y(n).


Fig. 9-17

From Fig. 9-17, Y(n) and X(n) are related by

The impulse response h(n) of the system is defined by

Solving Eq. (9.128), we obtain

where u(n) is the unit step sequence defined by

Taking the Fourier transform of Eq. (9.129), we obtain

Now, by Eq. (9.34),

and by Eq. (9.70) the power spectral density of Y(n) is

Taking the inverse Fourier transform of Eq. (9.130), we obtain

Thus, by Eq. (9.72) the average power of Y(n) is

SUPPLEMENTARY PROBLEMS
9.27. A sample function of a random telegraph signal X(t) is shown in Fig. 9-18. This signal makes independent random shifts between two equally likely values, A and 0. The number of shifts per unit time is governed by the Poisson distribution with parameter α.


Fig. 9-18

(a) Find the autocorrelation and the power spectrum of X(t).
(b) Find the rms value of X(t).
9.28. Suppose that X(t) is a Gaussian process with

Find the probability that X(4) ≤ 1.
9.29. The output of a filter is given by
Y(t) = X(t + T) - X(t - T)
where X(t) is a WSS process with power spectrum SXX(ω) and T is a constant. Find the power spectrum of Y(t).
9.30. Let  (t) is the Hilbert transform of a WSS process X(t). Show that

9.31. A WSS random process X(t) is applied to the input of an LTI system with impulse response h(t) = 3e- 2t u(t). Find the mean value of Y(t) of the system if E[X(t)] = 2.
9.32. The input X(t) to the RC filter shown in Fig. 9-19 is a white noise specified by Eq. (9.31). Find the mean-square value of Y(t).


Fig. 9-19 RC filter.

9.33. The input X(t) to a differentiator is the random telegraph signal of Prob. 9.7.
(a) Determine the power spectral density of the differentiator output.
(b) Find the mean-square value of the differentiator output.
9.34. Suppose that the input to the filter shown in Fig. 9-20 is a white noise specified by Eq. (9.51). Find the power spectral density of Y(t).


Fig. 9-20

9.35. Suppose that the input to the discrete-time filter shown in Fig. 9-21 is a discrete-time white noise with average power σ2. Find the power spectral density of Y(n).


Fig. 9-21

ANSWERS TO SUPPLEMENTARY PROBLEMS
9.27. 

9.28. 0.159
9.29. SYY(ω) = 4 sin2ωTSXX(ω)
9.30. Hint: Use relation (b) of Prob. 9.12 and definition (5.174).
9.31. Hint: Use Eq. (9.46).

9.32. Hint: Use Eqs. (9.57) and (9.58).

9.33.
9.34. 
9.35. SY(Ω) = σ2(1 + a2 + 2a cos Ω)







APPENDIX AReview of Matrix Theory
A.1 Matrix Notation and Operations
A. Definitions:
1. An m × n matrix A is a rectangular array of elements having m rows and n columns and is denoted as

When m = n, A is called a square matrix of order n.
2. A 1 × n matrix is called an n-dimensional row vector:

An m × 1 matrix is called an m-dimensional column vector:

3. A zero matrix 0 is a matrix having all its elements zero.
4. A diagonal matrix D is a square matrix in which all elements not on the main diagonal are zero:

Sometimes the diagonal matrix D in Eq. (A.4) is expressed as

5. The identity (or unit) matrix I is a diagonal matrix with all of its diagonal elements equal to 1.

B. Operations:
Let A = [aij]m×n, B = [bij]m×n, and C = [cij]m×n.
a. Equality of Two Matrices:

b. Addition:

c. Multiplication by a Scalar:

If α = -1, then B = -A is called the negative of A.
EXAMPLE A.1 Let

Then



Notes:
1. A = B and B = C ⇒ A = C
2. A + B = B + A
3. (A + B) + C = A + (B + C)

5. A - A = A + (-A) = 0
6. (α + β)A = αA + αB
7. α (A + B) = αA + αB
8. α (βA) = (αβ)A = β (αA)
d. Multiplication:
Let A = [aij]m × n, B = [bij]n × p, and C = [cij]m × p.

The matrix product AB is defined only when the number of columns of A is equal to the number of rows of B. In this case A and B are said to be conformable.
EXAMPLE A.2 Let

Then

but BA is not defined.
Furthermore, even if both AB and BA are defined, in general

EXAMPLE A.3 Let

Then

An example of the case where AB = BA follows.
EXAMPLE A.4 Let

Then

Notes:

It is important to note that AB = 0 does not necessarily imply A = 0 or B = 0.
EXAMPLE A.5 Let

Then

A.2 Transpose and Inverse
A. Transpose:
Let A be an n × m matrix. The transpose of A, denoted by AT, is an m × n matrix formed by interchanging the rows and columns of A.

EXAMPLE A.6

If AT = A, then A is said to be symmetric, and if AT = -A, then A is said to be skew-symmetric.
EXAMPLE A.7 Let

Then A is a symmetric matrix and B is a skew-symmetric matrix.
Note that if a matrix is skew-symmetric, then its diagonal elements are all zero.
Notes:

B. Inverses:
A matrix A is said to be invertible if there exists a matrix B such that

The matrix B is called the inverse of A and is denoted by A-1. Thus,

EXAMPLE A.8

Thus,

Notes:

Note that if A is invertible, then AB = 0 implies that B = 0 since
A-1 AB = IB = B = A-10 = 0
A.3 Linear Independence and Rank
A. Linear independence:
Let A = [a1 a2 ... an], where ai denotes the ith column vector of A. A set of column vectors ai(i = 1, 2,..., n) is said to be linearly dependent if there exist numbers αi (i = 1, 2,..., n) not all zero such that

If Eq. (A.18) holds only for all αi = 0, then the set is said to be linearly independent.
EXAMPLE A.9 Let

Since 2a1 + (-3)a2 + a3 = 0, a1, a2, and a3 are linearly dependent. Let

Then

implies that α1 = α2 = α3 = 0. Thus, d1, d2, and d3 are linearly independent.
B. Rank of a Matrix:
The number of linearly independent column vectors in a matrix A is called the column rank of A, and the number of linearly independent row vectors in a matrix A is called the row rank of A. It can be shown that

Note:
If the rank of an N × N matrix A is N, then A is invertible and A-1 exists.
A.4 Determinants
A. Definitions:
Let A = [aij] be a square matrix of order N. We associate with A a certain number called its determinant, denoted by det A or |A|. Let Mij be the square matrix of order (N - 1) obtained from A by deleting the ith row and jth column. The number Aij defined by

is called the cofactor of aij. Then det A is obtained by

or

Equation (A.21a) is known as the Laplace expansion of |A| along the ith row, and Eq. (A.21b) the Laplace expansion of |A| along the jth column.
EXAMPLE A.10 For a 1 × 1 matrix,

For a 2 × 2 matrix,

For a 3 × 3 matrix,

Using Eqs. (A.21a) and (A.23), we obtain

B. Determinant Rank of a Matrix:
The determinant rank of a matrix A is defined as the order of the largest square submatrix M of A such that det M ≠ 0. It can be shown that the rank of A is equal to the determinant rank of A.
EXAMPLE A.11 Let

Note that |A| = 0. One of the largest submatrices whose determinant is not equal to zero is

Hence the rank of the matrix A is 2. (See Example A.9.)
C. Inverse of a Matrix:
Using determinants, the inverse of an N × N matrix A can be computed as

and

where Aij is the cofactor of aij defined in Eq. (A.20) and "adj" stands for the adjugate (or adjoint). Formula (A.25) is used mainly for N = 2 and N = 3.
EXAMPLE A.12 Let

Then

Thus,

For a 2 × 2 matrix,

From Eq. (A.25) we see that if det A = 0, then A-1 does not exist. The matrix A is called singular if det A = 0, and nonsingular if det A ≠ 0. Thus, if a matrix is nonsingular, then it is invertible and A-1 exists.
A.5 Eigenvalues and Eigenvectors
A. Definitions:
Let A be an N × N matrix. If

for some scalar λ and nonzero column vector x, then λ is called an eigenvalue (or characteristic value) of A and x is called an eigenvector associated with 1.
B. Characteristic Equation:
Equation (A.28) can be rewritten as

where I is the identity matrix of Nth order. Equation (A.29) will have a nonzero eigenvector x only if λI - A is singular, that is,

which is called the characteristic equation of A. The polynomial c(λ) defined by

is called the characteristic polynomial of A. Now if λ1, λ2, ..., λi are distinct eigenvalues of A, then we have

where m1 + m2 + ... + mi = N and mi is called the algebraic multiplicity of λi.
Theorem A.1:
Let λk (k = 1, 2,..., i) be the distinct eigenvalues of A and let xk be the eigenvectors associated with the eigenvalues λk. Then the set of eigenvectors x1, x2, ..., xi are linearly independent.
Proof The proof is by contradiction. Suppose that x1, x2, ..., xi are linearly dependent.
Then there exists α1, α2, ..., αi not all zero such that

Assuming α1 ≠ 0, then by Eq. (A.33) we have

Now by Eq. (A.28)

and

Then Eq. (A.34) can be written as

Since λk (k = 1, 2,..., i) are distinct, Eq. (A.35) implies that α1 = 0, which is a contradiction. Thus, the set of eigenvectors x1, x2, ..., xi are linearly independent.
A.6 Diagonalization and Similarity Transformation
A. Diagonalization:
Suppose that all eigenvalues of an N × N matrix A are distinct. Let x1, x2, ..., xN be eigenvectors associated with the eigenvalues λ1, λ2, ..., λN. Let

Then


where

By Theorem A.1, P has N linearly independent column vectors. Thus, P is nonsingular and P-1 exists, and hence

We call P the diagonalization matrix or eigenvector matrix, and Λ the eigenvalue matrix.
Notes:
1. A sufficient (but not necessary) condition that an N × N matrix A be diagonalizable is that A has N distinct eigenvalues.
2. If A does not have N independent eigenvectors, then A is not diagonalizable.
3. The diagonalization matrix P is not unique. Reordering the columns of P or multiplying them by nonzero scalars will produce a new diagonalization matrix.
B. Similarity Transformation:
Let A and B be two square matrices of the same order. If there exists a nonsingular matrix Q such that

then we say that B is similar to A and Eq. (A.40) is called the similarity transformation.
Notes:
 
1. If B is similar to A, then A is similar to B.
2. If A is similar to B and B is similar to C, then A is similar to C.
3. If A and B are similar, then A and B have the same eigenvalues.
4. An N × N matrix A is similar to a diagonal matrix D if and only if there exist N linearly independent eigenvectors of A.
A.7 Functions of a Matrix
A. Powers of a Matrix:
We define powers of an N × N matrix A as

It can be easily verified by direct multiplication that if

Then

Notes:
 
1. If the eigenvalues of A are λ1, λ2, ..., λi, then the eigenvalues of An are 
2. Each eigenvector of A is still an eigenvector of An.
3. If P diagonalizes A, that is,

then it also diagonalizes An, that is,

since

B. Function of a Matrix:
Consider a function of λ defined by

With any such function we can associate a function of an N × N matrix A:

If A is a diagonal matrix D in Eq. (A.42), then using Eq. (A.43), we have

If P diagonalizes A, that is [Eq. (A.44)],
P-1 AP = Λ
then we have

and

Thus, we obtain

Replacing D by Λ in Eq. (A.49), we get

where λk are the eigenvalues of A.
C. The Cayley-Hamilton Theorem:
Let the characteristic polynomial c (λ) of an N × N matrix A be given by [Eq. (A.31)]
c (λ) = |λI - A| = λN + cN-1 λN-1 + ... + c1λ + c0
The Cayley-Hamilton theorem states that the matrix A satisfies its own characteristic equation; that is,

EXAMPLE A.13 Let

Then, its characteristic polynomial is

and

Rewriting Eq. (A.54), we have

Multiplying through by A and then substituting the expression (A.55) for AN on the right and rearranging, we get

By continuing this process, we can express any positive integral power of A as a linear combination of I, A, ..., AN-1.
Thus, ƒ(A) defined by Eq. (A.48) can be represented by

In a similar manner, if λ is an eigenvalue of A, then ƒ(λ) can also be expressed as

Thus, if all eigenvalues of A are distinct, the coefficients bm (m = 0, 1, ..., N -1) can be determined by the following N equations:

If all eigenvalues of A are not distinct, then Eq. (A.59) will not yield N equations. Assume that an eigenvalue λi has multiplicity r and all other eigenvalues are distinct. In this case differentiating both sides of Eq. (A.58) r times with respect to λ and setting λ = λi, we obtain r equations corresponding to λi:

Combining Eqs. (A.59) and (A.60), we can determine all coefficients bm in Eq. (A.57).
D. Minimal Polynomial of A:
The minimal (or minimum) polynomial m (λ) of an N × N matrix A is the polynomial of lowest degree having 1 as its leading coefficient such that m (A) = 0. Since A satisfies its characteristic equation, the degree of m (λ) is not greater than N.
EXAMPLE A.14 Let

The characteristic polynomial is

and the minimal polynomial is
m (λ)= λ - α

Notes:
 
1. Every eigenvalue of A is a zero of m (λ).
2. If all the eigenvalues of A are distinct, then c (λ) = m (λ).
3. c (λ) is divisible by m (λ).
4. m (λ) may be used in the same way as c (λ) for the expression of higher powers of A in terms of a limited number of powers of A.
It can be shown that m(λ) can be determined by

where d (λ) is the greatest common divisor (gcd) of all elements of adj(λI - A).
EXAMPLE A.15 Let

Then

Thus, d (λ) = λ - 2 and

and

E. Spectral Decomposition:
It can be shown that if the minimal polynomial m (λ) of an N × N matrix A has the form

then A can be represented by

where E j (j = 1, 2,..., i) are called constituent matrices and have the following properties:

Any matrix B for which B2 = B is called idempotent. Thus, the constituent matrices Ej are idempotent matrices. The set of eigenvalues of A is called the spectrum of A, and Eq. (A.63) is called the spectral decomposition of A. Using the properties of Eq. (A.64), we have

and

The constituent matrices Ej can be evaluated as follows. The partial-fraction expansion of

leads to

Then

where

Let ej(λ) = kjgj(λ). Then the constituent matrices Ej can be evaluated as

EXAMPLE A.16 Consider the matrix A in Example A.15:

From Example A.15, we have
m(λ) = (λ-1)(λ-2)
Then

and
e1(λ) = -(λ-2) e2(λ) = λ-1
Then

A.8 Differentiation and Integration of Matrices
A. Definitions:
The derivative of an m × n matrix A(t) is defined to be the m × n matrix, each element of which is the derivative of the corresponding element of A; that is,

Similarly, the integral of an m × n matrix A(t) is defined to be

EXAMPLE A.17 Let


B. Differentiation of the Product of Two Matrices:
If the matrices A(t) and B(t) can be differentiated with respect to t, then








APPENDIX BReview of Probability
B.1 Probability
A. Random Experiments:
In the study of probability, any process of observation is referred to as an experiment. The results of an observation are called the outcomes of the experiment. An experiment is called a random experiment if its outcome cannot be predicted. Typical examples of a random experiment are the roll of a die, the toss of a coin, drawing a card from a deck, or selecting a message signal for transmission from several messages.
B. Sample Space and Events:
The set of all possible outcomes of a random experiment is called the sample space S. An element in S is called a sample point. Each outcome of a random experiment corresponds to a sample point.
A set A is called a subset of B, denoted by A ⊂ B if every element of A is also an element of B. Any subset of the sample space S is called an event. A sample point of S is often referred to as an elementary event. Note that the sample space S is the subset of itself, that is, S ⊂ S. Since S is the set of all possible outcomes, it is often called the certain event.
C. Algebra of Events:
1. The complement of event A, denoted Ā, is the event containing all sample points in S but not in A.
2. The union of events A and B, denoted A ∪ B, is the event containing all sample points in either A or B or both.
3. The intersection of events A and B, denoted A ∩ B, is the event containing all sample points in both A and B.
4. The event containing no sample point is called the null event, denoted ∅. Thus ∅ corresponds to an impossible event.
5. Two events A and B are called mutually exclusive or disjoint if they contain no common sample point, that is, A ∩ B ∅.
By the preceding set of definitions, we obtain the following identities:

D. Venn Diagram:
A graphical representation that is very useful for illustrating set operation is the Venn diagram. For instance, in the three Venn diagrams shown in Fig. B-1, the shaded areas represent, respectively, the events A ∪ B, A ∩ B, and Ā.


Fig. B-1

E. Probabilities of Events:
An assignment of real numbers to the events defined on S is known as the probability measure. In the axiomatic definition, the probability P(A) of the event A is a real number assigned to A that satisfies the following three axioms:

With the preceding axioms, the following useful properties of probability can be obtained.

Note that Property 4 can be easily derived from axiom 2 and property 3. Since A ⊂ S, we have
P(A) ≤ P(s) = 1
Thus, combining with axiom 1, we obtain

Property 5 implies that

since P (A ∩ B) ≥ 0 by axiom 1.
One can also define P(A) intuitively, in terms of relative frequency. Suppose that a random experiment is repeated n times. If an event A occurs nA times, then its probability P(A) is defined as

Note that this limit may not exist.
EXAMPLE B.1 Using the axioms of probability, prove Eq. (B.4).

Then the use of axioms 1 and 3 yields
P(S) = 1 = P(A) + P(Ā)
P(Ā) = 1 - P(A)
Thus
EXAMPLE B.2 Verify Eq. (B.5).
A = A ∪ ∅ and A ∩ ∅ = ∅
Therefore, by axiom 3,
P(A) = P(A ∩ ∅) = P(A) + P(∅)
and we conclude that
P(∅) = 0
EXAMPLE B.3 Verify Eq. (B.6).
Let A ⊂ B. Then from the Venn diagram shown in Fig. B-2, we see that
B = A ∪ (B ∩ Ā) and A ∩ (B ∩ Ā) = ∅
Hence, from axiom 3,
P(B) = P(A) = P(B ∩ Ā) ≥ P(A)
because by axiom 1, P(B ∩ Ā) ≥ 0.


Fig. B-2

EXAMPLE B.4 Verify Eq. (B.8).
From the Venn diagram of Fig. B-3, each of the sets A ∪ B and B can be expressed, respectively, as a union of mutually exclusive sets as follows:
A ∪ B = A ∪ (Ā ∩ B) and B = (A ∩ B) ∪ (Ā ∩ B)
Thus, by axiom 3,

and

From Eq. (B.13) we have

Substituting Eq. (B.14) into Eq. (B.12), we obtain
P(A ∪ B) = P(A) + P(B) - P (A ∩ B)


Fig. B-3

F. Equally Likely Events:
Consider a finite sample space S with finite elements
S = {λ1, λ2, ..., λn}
where λi's are elementary events. Let P(λi) = pi, Then

3. If , where I is a collection of subscripts, then

When all elementary events λi (i = 1, 2,..., n) are equally likely events, that is
p1 = p2 = ... = Pn
then from Eq. (B.15), we have

and

where n(A) is the number of outcomes belonging to event A and n is the number of sample points in S.
G. Conditional Probability:
The conditional probability of an event A given the event B, denoted by P(A | B), is defined as

where P(A ∩ B) is the joint probability of A and B. Similarly,

is the conditional probability of an event B given event A. From Eqs. (B.19) and (B.20) we have

Equation (B.21) is often quite useful in computing the joint probability of events.
From Eq. (B.21) we can obtain the following Bayes rule:

EXAMPLE B.5 Find P(A | B) if (a) A ∩ B = ∅, (b) A ⊂ B, and (c) B ⊂ A.
(a) If A ∩ B ∅, then P(A ∩ B) = P(∅) = 0. Thus,

(b) If A ⊂ B, then A ∩ B = A and

(c) If B ⊂ A, then A ∩ B = B and

H. Independent Events:
Two events A and B are said to be (statistically) independent if

This, together with Eq. (B.21), implies that for two statistically independent events

We may also extend the definition of independence to more than two events. The events A1, A2, ..., An are independent if and only if for every subset {Ai1, Ai2, ..., Aik} (2 ≤ k ≤ n) of these events,

I. Total Probability:
The events A1, A2, ..., An are called mutually exclusive and exhaustive if

Let B be any event in S. Then

which is known as the total probability of event B. Let A = Ai in Eq. (B.22); using Eq. (B.27) we obtain

Note that the terms on the right-hand side are all conditioned on events Ai, while that on the left is conditioned on B. Equation (B.28) is sometimes referred to as Bayes' theorem.
EXAMPLE B.6 Verify Eq. (B.27).
Since B ∩ S = B [and using Eq. (B.26)], we have

Now the events B ∩ Ak (k = 1, 2,..., N) are mutually exclusive, as seen from the Venn diagram of Fig. B-4. Then by axiom 3 of the probability definition and Eq. (B.21), we obtain



Fig. B-4

B.2 Random Variables
A. Random Variables:
Consider a random experiment with sample space S. A random variable X(λ) is a single-valued real function that assigns a real number called the value of X(λ) to each sample point λ of S. Often we use a single letter X for this function in place of X(λ) and use r.v. to denote the random variable. A schematic diagram representing a r.v. is given in Fig. B-5.


Fig. B-5 Random variable X as a function.

The sample space S is termed the domain of the r.v. X, and the collection of all numbers [values of X(λ)] is termed the range of the r.v. X. Thus, the range of X is a certain subset of the set of all real numbers and it is usually denoted by RX. Note that two or more different sample points might give the same value of X(λ), but two different numbers in the range cannot be assigned to the same sample point.
The r.v. X induces a probability measure on the real line as follows:

If X can take on only a countable number of distinct values, then X is called a discrete random variable. If X can assume any values within one or more intervals on the real line, then X is called a continuous random variable. The number of telephone calls arriving at an office in a finite time is an example of a discrete random variable, and the exact time of arrival of a telephone call is an example of a continuous random variable.
B. Distribution Function:
The distribution function [or cumulative distribution function (cdf)] of X is the function defined by

Properties of FX(x):

From definition (B.29) we can compute other probabilities:

C. Discrete Random Variables and Probability Mass Functions:
Let X be a discrete r.v. with cdf FX(x). Then FX(x) is a staircase function (see Fig. B-6), and FX(x) changes values only in jumps (at most a countable number of them) and is constant between jumps.


Fig. B-6

Suppose that the jumps in FX(x) of a discrete r.v. X occur at the points x1, x2, ..., where the sequence may be either finite or countably infinite, and we assume xi < xj if i < j. Then

Let

The function pX(x) is called the probability mass function (pmf) of the discrete r.v. X.
Properties of pX(X):

The cdf FX(x) of a discrete r.v. X can be obtained by

D. Examples of Discrete Random Variables:
1. Bernoulli Distribution:
Ar.v. X is called a Bernoulli r.v. with parameter p if its pmf is given by

where 0 ≤ p ≤ 1. By Eq. (B.29), the cdf FX(x) of the Bernoulli r.v. X is given by

2. Binomial Distribution:
Ar.v. X is called a binomial r.v. with parameters (n, p) if its pmf is given by

where 0 ≤ p ≤ 1 and

which is known as the binomial coefficient. The corresponding cdf of X is

3. Poisson Distribution:
Ar.v. X is called a Poisson r.v. with parameter λ (>0) if its pmf is given by

The corresponding cdf of X is

E. Continuous Random Variables and Probability Density Functions:
Let X be a r.v. with cdf FX(x). Then FX(x) is continuous and also has a derivative dFX(x)/dx that exists everywhere except at possibly a finite number of points and is piecewise continuous. Thus, if X is a continuous r.v., then

In most applications, the r.v. is either discrete or continuous. But if the cdf FX(x) of a r.v. X possesses both features of discrete and continuous r.v.'s, then the r.v. X is called the mixed r.v.
Let

The function fX(x) is called the probability density function (pdf) of the continuous r.v. X.
Properties of fX(x):

The cdf FX(x) of a continuous r.v. X can be obtained by

F. Examples of Continuous Random Variables:
1. Uniform Distribution:
Ar.v. X is called a uniform r.v. over (a, b) if its pdf is given by

The corresponding cdf of X is

2. Exponential Distribution:
Ar.v. X is called an exponential r.v. with parameter λ (> 0) if its pdf is given by

The corresponding cdf of X is

3. Normal (or Gaussian) Distribution:
Ar.v. X = N(μ; σ2) is called a normal (or Gaussian) r.v. if its pdf is given by

The corresponding cdf of X is

B.3 Two-Dimensional Random Variables
A. Joint Distribution Function:
Let S be the sample space of a random experiment. Let X and Y be two r.v.'s defined on S. Then the pair (X, Y) is called a two-dimensional r.v. if each of X and Y associates a real number with every element of S. The joint cumulative distribution function (or joint cdf) of X and Y, denoted by FXY(x, y), is the function defined by

Two r.v.'s X and Y will be called independent if

for every value of x and y.
B. Marginal Distribution Function:
Since {X ≤ ∞} and {Y ≤ ∞} are certain events, we have

so that

The cdf's FX(x) and FY(y), when obtained by Eqs. (B.64) and (B.65), are referred to as the marginal cdf's of X and Y, respectively.
C. Joint Probability Mass Functions:
Let (X, Y) be a discrete two-dimensional r.v. and (X, Y) takes on the values (xi, yj) for a certain allowable set of integers i and j. Let

The function pXY(xi, yj) is called the joint probability mass function (joint pmf) of (X, Y).
Properties of pXY(xi, yj):

The joint cdf of a discrete two-dimensional r.v. (X, Y) is given by

D. Marginal Probability Mass Functions:
Suppose that for a fixed value X = xi, the r.v. Y can only take on the possible values yj (j = 1, 2,..., n).
Then

Similarly,

The pmf's pX(xi) and pY(yj), when obtained by Eqs. (B.70) and (B.71), are referred to as the marginal pmf's of X and Y, respectively. If X and Y are independent r.v.'s, then

E. Joint Probability Density Functions:
Let (X, Y) be a continuous two-dimensional r.v. with cdf FXY(x, y) and let

The function fXY(x, y) is called the joint probability density function (joint pdf) of (X, Y). By integrating Eq. (B.73), we have

Properties of fXY(x, y):

F. Marginal Probability Density Functions:
By Eqs. (B.64), (B.65), and definition (B.51), we obtain

The pdf's fX(x) and fY(x), when obtained by Eqs. (B.77) and (B.78), are referred to as the margina l pdf's of X and Y, respectively. If X and Y are independent r.v.'s, then

The conditional pdf of X given the event {Y y } is

where fY(y) is the marginal pdf of Y.
B.4 Functions of Random Variables
A. Random Variable g(X):
Given a r.v. X and a function g(x), the expression

defines a new r.v. Y. With y a given number, we denote Dy the subset of RX (range of X) such that g(x) ≤ y. Then

where (X ∈ Dy) is the event consisting of all outcomes λ such that the point X(λ) ∈ Dy. Hence,

If X is a continuous r.v. with pdf fX(x), then

Determination of fY(y) from fX(x):
Let X be a continuous r.v. with pdf fX(x). If the transformation y = g(x) is one-to-one and has the inverse transformation

then the pdf of Y is given by

Note that if g(x) is a continuous monotonic increasing or decreasing function, then the transformation y = g(x) is one-to-one. If the transformation y = g(x) is not one-to-one, fY(y) is obtained as follows:
Denoting the real roots of y = g(x) by xk, that is,
y = g(x1) = ... = g(xk) = ...
then

where g′(x) is the derivative of g(x).
EXAMPLE B.7 Let Y = aX + b. Show that if X = N (μ; σ2), then Y = N(aμ + b; a2σ2).
The equation y = g(x) = ax + b has a single solution x1 = (y - b)/a, and g′(x) = a. The range of y is (-∞, ∞). Hence, by Eq. (B.86)

Since X = N(μ; σ2), by Eq. (B.60)

Hence, by Eq. (B.87)

which is the pdf of N(aμ + b; a2σ2). Thus, if X = N(μ; σ2), then Y = N(aμ + b; a2σ2).
EXAMPLE B.8 Let Y = X2. Find fY(y) if X = N(0; 1).
If y < 0, then the equation y = x2 has no real solutions; hence, fY(y) = 0.
If y > 0, then y = x2 has two solutions

Now, y = g(x) = x2 and g′(x) = 2x. Hence, by Eq. (B.86)

Since X = N(0; 1) from Eq. (B.60), we have

Since fX(x) is an even function from Eq. (B.90), we have

B. One Function of Two Random Variables:
Given two random variables X and Y and a function g(x, y), the expression

is a new random variable. With z a given number, we denote by Dz the region of the xy plane such that g(x, y) ≤ z. Then

where {(X, Y) ∈ Dz} is the event consisting of all outcomes λ such that the point {X(λ), Y(λ)} is in Dz. Hence,

If X and Y are continuous r.v.'s with joint pdf fXY(x, y), then

EXAMPLE B.9 Consider two r.v.'s X and Y with joint pdf fXY(x, y). Let Z = X + Y.
(a) Determine the pdf of Z.
(b) Determine the pdf of Z if X and Y are independent.


Fig. B-7

(a) The range RZ of Z corresponding to the event (Z ≤ z) = (X + Y ≤ z) is the set of points (x, y) which lie on and to the left of the line z = x + y (Fig. B-7). Thus, we have

Then

(b) If X and Y are independent, then Eq. (B.97) reduces to

The integral on the right-hand side of Eq. (B.98) is known as a convolution of fX(z) and fY(z). Since the convolution is commutative, Eq. (B.98) can also be written as

C. Two Functions of Two Random Variables:
Given two r.v.'s. X and Y and two functions g(x, y) and h(x, y), the expression

defines two new r.v.'s Z and W. With z and w two given numbers we denote Dzw the subset of RXY [range of (X, Y)] such that g(x, y) ≤ z and h(x, y) ≤ w. Then

where {(X, Y) ∈ Dzw} is the event consisting of all outcomes λ such that the point {X(λ), Y(λ)} ∈ Dzw. Hence,

In the continuous case we have

Determination of fZW(z, w) from fXY(x, y):
Let X and Y be two continuous r.v.'s with joint pdf fXY(x, y). If the transformation

is one-to-one and has the inverse transformation

then the joint pdf of Z and W is given by

where x = q(z, w), y = r(z, w) and

which is the Jacobian of the transformation (B.103).
EXAMPLE B.10 Consider the transformation

Eq. (B.105) yields

B.5 Statistical Averages
A. Expectation:
The expectation (or mean) of a r.v. X, denoted by E(X) or μX, is defined by

The expectation of Y = g(X) is given by

The expectation of Z = g(X, Y) is given by

Note that the expectation operation is linear, that is,

where c is a constant.
EXAMPLE B.11 If X and Y are independent, then show that

and

If X and Y are independent, then by Eqs. (B.79) and (B.111) we have

Similarly,

B. Moment:
The nth moment of a r.v. X is defined by

C. Variance:
The variance of a r.v. X, denoted by  or Var(X), is defined by

Thus,

The positive square root of the variance, or σX, is called the standard deviation of X. The variance or standard variation is a measure of the "spread" of the values of X from its mean μX. By using Eqs. (B.112) and (B.113), the expression in Eq. (B.117) can be simplified to

Mean and variance of various random variables are tabulated in Table B-1.
TABLE B-1

D. Covariance and Correlation Coefficient:
The (k, n)th moment of a two-dimensional r.v. (X, Y) is defined by

The (1, 1)th joint moment of (X, Y),

is called the correlation of X and Y. If E(X Y) = 0, then we say that X and Y are orthogonal. The covariance of X and Y, denoted by Cov(X, Y) or σXY, is defined by

Expanding Eq. (B.122), we obtain

If Cov(X, Y) = 0, then we say that X and Y are uncorrelated. From Eq. (B.123) we see that X and Y are uncorrelated if

Note that if X and Y are independent, then it can be shown that they are uncorrelated. However, the converse is not true in general; that is, the fact that X and Y are uncorrelated does not, in general, imply that they are independent. The correlation coefficient, denoted by ρ(X, Y) or ρXY, is defined by

It can be shown that (Example B.15)

E. Some Inequalities:
1. Markov Inequality:
If fX(x) = 0 for x < 0, then for any α > 0,

2. Chebyshev Inequality:
For any ∊ > 0, then

where  is the variance of X. This is known as the Chebyshev inequality.
3. Cauchy-Schwarz Inequality:
Let X and Y be real random variables with finite second moments. Then

This is known as the Cauchy-Schwarz inequality.
EXAMPLE B.12 Verify Markov inequality, Eq. (B.127).
From Eq. (B.54)

Since fX(x) = 0 for x < 0, Hence,

Hence,

EXAMPLE B.13 Verify Chebyshev inequality, Eq. (B.128).
From Eq. (B.54)

By Eq. (B.118)

Hence,

or

EXAMPLE B.14 Verify Cauchy-Schwarz inequality Eq. (B.129).
Because the mean-square value of a random variable can never be negative,
E[(X - αY)2] ≥ 0
for any value of α. Expanding this, we obtain
E[X2] = 2αE[XY ] + α2E[Y2] ≥ 0
Choose a value of a for which the left-hand side of this inequality is minimum

which results in the inequality

or

EXAMPLE B.15 Verify Eq. (B.126).
From the Cauchy-Schwarz inequality Eq. (B.129) we have

or

Then

from which it follows that
|ρXY| ≤ 1







APPENDIX CProperties of Linear Time-Invariant Systems and Various Transforms
C.1 Continuous-Time LTI Systems
Unit impulse response: h(t)
Convolution: 
Causality: h(t) = 0, t < 0
Stability: 
C.2 The Laplace Transform
The Bilateral (or Two-Sided) Laplace Transform:
Definition:

Properties of the Bilateral Laplace Transform:
Linearity: 
Time shifting: 
Shifting in s: 
Time scaling: 
Time reversal: 
Differentiation in 
Differentiation in s: 
Integration: 
Convolution: 
Some Laplace Transforms Pairs:

The Unilateral (or One-Sided) Laplace Transform: Definition:
Definition:

Some Special Properties:
Differentiation in the Time Domain:

Integration in the Time Domain:

Initial value theorem: 
Final Value theorem: 
C.3 The Fourier Transform
Definition:

Properties of the Fourier Transform:
Linearity: a1x1(t) + a2x2(t) ↔ a1X1(ω) + a2X2(ω)
Time shifting: 
Frequency shifting: 
Time scaling: 
Time reversal: 
Duality: 
Time differentiation: 
Frequency differentiation: 
Integraton: 
Convolution: 
Multiplication: 
Real signal: 
Even component: 
Odd component: 
Parseval's Relations:

Common Fourier Transforms Pairs:

C.4 Discrete-Time LTI Systems
Unit sample response: h[n]
Convolution: 
Causality: h[n] = 0, n < 0
Stability: 
C.5 The z-Transform
The Bilateral (or Two-Sided) z-Transform:
Definition:

Properties of the z-Transform:
Lineartity: 
Time shifiting: 
Multiplication by 
Multiplication by 
Time reversal: 
Multiplication by n: 
Accumulation: 
Convolution: 
Some Common z-Transforms Pairs:


The Unilateral (or One-Sided) z-Transform:

Some Special Properties:
Time-Shifting Property:
x[n - m] ↔ z-mXI(z) + z-m+1x[-l] + z-m+2x[-2] + ··· + x[-m]
x[n + m] ↔ zmX1(z) - zmx[0] - zm-1x[1] - ··· - zx[m -1]
Initial value theorem: 
Final value theorem: 
C.6 The Discrete-Time Fourier Transform
Definition:

Properties of the Discrete-Time Fourier Transform:
Periodicity: x[n] ↔ X (Ω) = X (Ω + 2π)
Linearity: a1x1[n] + a2x2[n] ↔ a1x1(Ω) + a2x2(Ω)
Time shifting: 
Frequency shifting: 
Conjugation: x*[n] ↔ x*(-Ω)
Time reversal: x[- n] ↔ X (- Ω)
Time scaling: 
Frequency differentiation: 
First difference: x[n] - x[n -1] ↔ (1 - e-jΩ) x (Ω)
Accumulation: 
Convolution: x1[n] * x2[n] ↔ x1(Ω) x2(Ω)
Multiplication: 
Real sequence: 
Even component: xe[n] ↔ Re{x (Ω)} = A (Ω)
Odd component: x0[n] ↔ j Im{x (Ω)} = jB (Ω)
Parseval's Relations:

Some Common Fourier Transform Pairs:
δ[n] ↔ 1

C.7 Discrete Fourier Transform
Definition:

Properties of the DF T:
Linearity: a1x1[n] + a2x2[n] ↔ a1x1[k] + a2x2[k]
Time shifting: 
Frequency shifting: 
Conjugation: x*[n] ↔ x*[-k]mod N
Time reversal: 
Duality: X[n] ↔ Nx[-k]mod N
Circular convolution: x1[n] ⊗ x2[n] ↔ x1[k]x2[k]
Multiplication: 
Real sequence: 
Even component: xe[n] ↔ Re{x[k]} = A[k]
Odd component: x0[n] ↔ j Im{x[k]} = jB[k]
Parseval's Relation:

Note

C.8 Fourier Series
x(t + T0) = x(t)
Complex Exponential Fourier Series:

Trigonometric Fourier Series:

Harmonic Form Fourier Series:

Relations among Various Fourier Coefficients:

Parseval's Theorem for Fourier Series:

C.9 Discrete Fourier Series

Parseval's Theorem for Discrete Fourier Series:








APPENDIX DReview of Complex Numbers
D.1 Representation of Complex Numbers
The complex number z can be expressed in several ways.
Cartesian or rectangular form:

where  and a and b are real numbers referred to the real part and the imaginary part of z. a and b are often expressed as

where "Re" denotes the "real part of" and "Im" denotes the "imaginary part of." Polar form:

where r > 0 is the magnitude of z and θ is the angle or phase of z. These quantities are often written as

Fig. D-1 is the graphical representation of z. Using Euler's formula,

or from Fig. D-1 the relationships between the Cartesian and polar representations of z are



Fig. D-1

D.2 Addition, Multiplication, and Division
If z1 = a1 + jb1 and z2 = a2 + jb2, then

If  and, then

D.3 The Complex Conjugate
The complex conjugate of z is denoted by z* and is given by

Useful relationships:
1. 
2. 
3. z + z* = 2 Re {z}
4. z - z* = j2 Im{z}
5. 
6. 
7. 
D.4 Powers and Roots of Complex Numbers
The nth power of the complex number z = rejθ is

from which we have De Moivre's relation

The nth root of a complex z is the number w such that

Thus, to find the nth root of a complex number z, we must solve

which is an equation of degree n and hence has n roots. These roots are given by








APPENDIX EUseful Mathematical Formulas
E.1 Summation Formulas

E.2 Euler's Formulas

E.3 Trigonometric Identities


E.4 Power Series Expansions

E.5 Exponential and Logarithmic Functions

E.6 Some Definite Integrals








INDEX
Please note that index links point to page beginnings from the print edition. Locations are approximate in e-readers, and you may need to page down one or more times after clicking a link to get to the indexed material.
Absolute bandwidth, 209
Accumulation, 155
Additivity, 16
Adjoint (or adjugate) matrix, 449
Advance, unit, 154
Aliasing, 253
Algebra of events, 459
All-pass filter, 301
Amplitude distortion, 206
Amplitude spectrum, 195
Analog signals, 2
Analytic signal, 257
Anticausal sequence, 58
Anticausal signals, 53
Aperiodic sequences (see Nonperiodic sequences)
Aperiodic signals (see Nonperiodic signals)
Asymptotically stable systems, 337, 340
Autocorrelation, 396, 417
time-averaged, 399
Autocovariance, 396
Auxiliary conditions:
difference equations, 59
differential equations, 55
Average power, 5
normalized, 5
Band-limited signal, 209, 252
Band-limited white noise, 420
Bandpass signal, 209
Bandwidth:
absolute, 209
energy containment, 251
equivalent, 249
filter (or system), 209
signal, 209
3-dB (or half power), 209
Bayes rule, 463
Bayes' theorem, 464
Bernoulli distribution, 466
Binomial distribution, 466
Bilateral (or two-sided) Laplace transform, 101
Bilateral (or two-sided) z-transform, 149
Bilinear transformation, 307
Bode plots, 240
Bounded-input/bounded-output (BIBO) stability, 17, 54, 58, 71, 90, 111, 131, 180, 337, 340
Canonical simulation:
the first form, 347, 353
the second form, 348, 354
Canonical State representation:
the first form, 347, 353
the second form, 348, 354
Cauchy-Schwarz inequlity, 476
Causal sequence, 58
Causal signal, 53
Causal system, 16
Causality, 44, 53, 55, 58, 89, 111, 158
Cayley-Hamilton theorem, 335, 339, 360, 454
Chain, 394
Characteristic equation, 335, 450
Characteristic function (see Eigenfunction)
Characteristic polynomial, 450
Characteristic values (see Eigenvalues)
Chebyshev inequlity, 476
Circular convolution, 277
Circular shift, 276
Cofactor, 448
Complement, 459
Complex frequency, 199
Complex numbers, 487
Complex random process, 394
Complex signals, 2
Compressor, 43
Conditional probability, 462
Connection between:
the Fourier transform (continuous-time) and
the Laplace transform, 198
the Fourier transform (discrete-time) and
the z-transform, 266
Constituent matrix, 336, 340, 456
Continuous-time LTI systems, 51
causality, 53, 111
described by differential equations, 54, 206
eigenfunctions, 54
frequency response, 203
impulse response, 51
properties, 53
response, 51
stability, 54
state space representation, 332
step response, 52
system (or transfer) function, 110
Continuous-time signals, 1
Continuous-time systems, 15
Controllability matrix, 368, 380
Controllable systems, 368, 380
Convolution:
circular, 277
continuous-time, 52
discrete-time, 56
in frequency, 201
integral, 52
periodic, 67, 86
properties, 52, 57
sum, 56
Convolution property:
discrete Fourier transform (DFT), 277
Fourier transform (continuous-time), 201, 235
Fourier transform (discrete-time), 269, 291
Laplace transform, 108
z-transform, 155, 168
Convolution theorem:
frequency, 201, 233
time, 201, 231
Correlation coefficient, 475
Correlations, 424
Counting process, 403
Covariance, 475
Covariance stationary, 410
Covariance matrix, 400
Cross-correlation, 396, 417
Cross-covariance, 397
Cross-power spectral density, 419
Decimation-in-frequency, 320
Decimation-in-time, 317
Degenerative circuits, 143
Delay, unit, 41, 154
Determinants, 448
Laplace expansion, 448
Deterministic signals, 3
DFS (see Discrete Fourier series)
DFT (see Discrete Fourier transform)
DFT matrix, 315
Diagonal matrix, 443
Diagonalization matrix, 451
Difference equations, 59
recursive, 59
Differential equations, 54
homogeneous solution, 54
particular solution, 54
Digital signals, 2
Digital simulation of analog signals, 274
Dirac delta function (δ-function) (see Unit impulse function)
Dirichlet conditions:
for Fourier series, 195
for Fourier transforms, 198
Discrete Fourier series (DFS), 261, 278
properties, 262
Discrete Fourier transform (DFT):
definition, 275
inverse, 275
N-point, 276
properties, 276
Discrete frequency (or line) spectra, 195
Discrete-time LTI systems:
causality, 58, 158
described by difference equations, 59
eigenfunctions, 58
finite impulse response (FIR), 60
impulse response, 56
infinite impulse response (IIR), 60
properties, 57
response, 56
stability, 58, 159
state space representation, 330
step response, 57
system function, 158
Discrete-time signals, 1
Discrete-time systems, 15
Distortionless transmission, 205
Distribution:
Bernoulli, 466
binomial, 466
exponential, 467
normal (or Gaussian), 468
Poisson, 466
uniform, 467
Distribution function, 465
cumulative (cdf), 465
Duality property:
discrete Fourier series, 262
discrete Fourier transform, 277
Fourier transform (continuous-time), 200, 223
Fourier transform (discrete-time), 268
Duration-limited signal, 258
Eigenfunctions (or characteristic function), 46
of continuous-time LTI systems, 54
of discrete-time LTI systems, 58
Eigenvalues (or characteristic values), 46, 96, 335, 450
Eigenvectors, 335, 450
Energy containment bandwidth, 251
Energy content, 5
normalized, 5
Energy-density spectrum, 202
Energy signal, 5
Energy theorem, 202
Ensemble, 392
average, 395
Equivalence property, 34
Equivalent bandwidth, 249
Even signal, 3
Events, 459
algebra of, 459
certain, 459
elementary, 459
equally likely, 462
independent, 463
null, 459
Equally likely events, 462
Ergodicity, 399
Ergodic, in the autocorrelation, 399
in the mean, 399
Expectation (or mean), 473
Exponential distribution, 467
Exponential sequences:
complex, 12
real, 13
Exponential signals:
complex, 8
real, 9
Fast Fourier transform (FFT):
decimation-in-frequency algorithm, 320
decimation-in-time algorithm, 317
Feedback systems, 17
FFT (see Fast Fourier transform)
Filter:
bandwidth, 209
ideal band pass, 207
ideal band stop, 207
ideal frequency-selective, 206
ideal low-pass, 207
ideal high-pass, 207
narrowband, 209
nonideal frequency-selective, 208
Filtering, 206
Final-value theorem:
unilateral Laplace transform, 135
unilateral z-transform, 187
Finite-duration signal, 104
Finite impulse response (FIR), 60
Finite sequence, 152
FIR (see Finite impulse response)
First difference sequence, 269
Fourier series:
coefficients, 194
complex exponential, 194
convergence, 195
discrete (DFS), 261, 278
harmonic form, 195
trigonometric, 194
Fourier spectra, 198, 265
Fourier transform (continuous-time), 198
convergence, 198
definition, 198
inverse, 198
properties, 200
tables, 202, 203
Fourier transform (discrete-time), 265
convergence, 266
definition, 265
inverse, 265
properties, 267
tables, 270, 271
Frequency:
angular, 193
fundamental, 193
complex, 198
convolution theorem, 201
fundamental, 9, 10, 193
radian, 9
Frequency response:
continuous-time LTI systems, 203, 237
discrete-time LTI systems, 271, 294
Frequency selective filter, 206
Frequency shifting, 200, 219, 267, 276
Gain, 205
Gaussian pulse, 236
Gaussian (or normal) random process, 400
Generalized derivatives, 8
Generalized functions, 7
Harmonic component, 195
Hilbert transform, 245
Homogeneity, 16
Identity matrix, 335, 444
IIR (see Infinite impulse response)
Impulse-invariant method, 306
Impulse response:
continuous-time LTI systems, 51
discrete-time LTI systems, 56
Impulse train, periodic, 216
Independent events, 463
Independent increments, 403
Index set, 392
Infinite impulse response (IIR), 60
Initial condition, 55
Initial rest, 55
Initial state, 381
Initial-value theorem:
unilateral Laplace transform, 135
unilateral z-transform, 186
Initially relaxed condition (see Initial rest)
Interconnection of systems, 72, 112
Intersection, 459
Inverse transform (see Fourier, Laplace, etc.)
Invertible system, 48
Jacovian, 473
Joint, cumulative distribution function (cdf), 468
distribution function, 468
probability density function (pdf), 469
probability mass function (pmf), 468
Jointly wide-sense stationary (WSS), 398
Laplace transform:
bilateral (two-sided), 101
definition, 101
inverse, 109
properties, 106, 120
region of convergence (ROC), 102
tables, 105, 109
unilateral (one-sided), 101, 113, 134
Left-sided signal, 104
Line spectra, 195
Linear system, 16
response to random input, 421, 423
Linear time-invariant (LTI) system, 16
continuous-time, 51
discrete-time, 56
Linearity, 16, 55, 106
Magnitude response, 204, 272
Magnitude spectrum, 195, 198
Marginal, distribution function, 468
pdf, 469
pmf, 469
Markov inequlity, 476
Matrix (or matrices):
characteristic equation, 450
characteristic polynomial, 450
conformable, 445
constituent, 336, 340, 456
controllability, 368, 380
covariance, 400
diagonal, 443
diagonalization, 451
differentiation, 458
eigenvalues, 450
eigenvectors, 450
function of, 452
idempotent, 456
identity (or unit), 335, 444
integration, 458
inverse, 446, 449
minimal polynomials, 361, 455
nilpotent, 361
nonsingular, 331, 450
observability, 369, 381
power, 452
rank, 448
similar, 331, 452
singular, 450
skew-symmetric, 446
spectral decomposition, 335, 340, 456
spectrum, 456
state-transition, 335
symmetric, 446
system, 331
transpose, 446
Mean, 395
Modulation theorem, 228
Moment, 474
Mutually exclusive (or disjoint) events, 459
Narrowband random process, 420
N-dimensional state equations, 331
Nilpotent, 361
Noncausal system, 16
Nonideal frequency-selective filter, 208
Nonlinear system, 16
Nonperiodic (or aperiodic), sequence, 5
signals, 4
Nonrecursive equation, 59
Nonsingular matrix, 331, 450
Normal (or Gaussian) distribution, 468
Normalized average power, 5
Normalized energy content, 5
N-point, DFT, 276
Sequence, 276
Null event, 459
Nyquist sampling interval, 254
Nyquist sampling rate, 254
Observability matrix, 369, 381
Observable system, 369, 381
Odd signal, 3
Orthogonal, random variables, 475
sequences, 278
signals, 210
Parseval's identity (see Parseval's theorem)
Parseval's relation, 202
discrete Fourier series (DFS), 284
discrete Fourier transform (DFT), 277
Fourier series, 221
Fourier transform (continuous-time), 202, 233, 234
Fourier transform (discrete-time), 270
periodic sequences, 284
periodic signals, 221
Parseval's theorem:
discrete Fourier series (DFS), 263, 284
discrete Fourier transform (DFT), 277
Fourier series, 196
Fourier transform (continuous-time), 202, 234
Fourier transform (discrete-time), 270
Partial fraction expansion, 110, 158
Pass band, 206
Period, 4
fundamental, 4
Periodic convolution:
continuous-time, 67
discrete-time, 86
Periodic impulse train, 216
Periodic sequences, 261
Periodic signals, 4
Phase distortion, 206
Phase response, 204, 272
Phase shifter, 245
Phase spectrum, 195, 198
Poisson, distribution, 466
random process, 407
Poles, 103
Power, 5
average, 5
Power series expansion, 157
Power signals, 5
Power spectral density (or power spectrum), 417, 418, 424
cross-, 419
Probability, 459
axiomatic definition, 460
conditional, 462
density function (pdf), 467
mass function (pmf), 465
measure, 460
total, 463
Random sequence, 394
Random signals, 3, 392
Random binary signal, 401, 429
Random experiment, 459
Random (or stochastic) processes, 392
atocorrelation, 396
autocovariance, 396
continuous-parameter, 394
cross-correlation, 396
cross-covariance, 397
description, 394
discrete-parameter, 394
independent, 397
orthogonal, 397
parameter set, 394
probabilistic expressions, 394
realization, 392
state space, 394
statistics of, 394
strict-sense stationary (SSS), 397
uncorrelated, 397
wide-sense stationary (WSS), 397
Random variable (r.v.), 464
Bernoulli, 466
binomial, 466
continuous, 467
exponential, 467,
normal (or Gaussian), 468
Poisson, 466
two-dimensional, 468
uniform, 467
Real signals, 2
Recursive equation, 59
Region of convergence (ROC):
Laplace transform, 102
z-transform, 149
Relationship between:
the DFT and the DFS, 276
the DFT and the discrete-time Fourier transform, 276
Response:
frequency, 203, 237, 271, 294
impulse, 51, 56
magnitude, 204, 272
phase, 204, 272
step, 52, 57
system, 273
to random input, 421, 431
zero-input, 55
zero-state, 55
Right-sided signal, 104
Rise time, 250
Sampled signal, ideal, 252
Sample space, 392, 459
Sample function, 392
Samples, 2
Sampling, 1
interval, 2
Nyquist, 254
rate (or frequency), 252, 274
Nyquist, 254
Sampling theorem:
in the frequency domain, 258
uniform, 254
Sequence, 1
complex exponential, 12
exponential, 13
finite, 152
first difference, 269
left sided, 152
nonperiodic (or aperiodic), 5
N-point, 276
orthogonal, 278
periodic, 5
right-sided, 152
sinusoidal, 14
two-sided, 152
Sift-invariant, 16
Simple random walk, 404
Shifting in the s-domain, 106
Signal bandwidth, 209
Signals:
analog, 2
analytical, 257
anticausal, 53
band-limited, 209, 252, 254
bandpass, 209
causal, 53
complex, 2
complex exponential, 8
continuous-time, 1
deterministic, 3
digital, 2
discrete-time, 1
duration-limited, 258
energy, 5
even, 3
finite-duration, 104
Gaussian pulse, 236
high-pass, 209
ideal sampled, 252
left-sided, 104
low-pass, 209
nonperiodic (or aperiodic), 4
odd, 3
periodic, 4
power, 5
random, 3, 392
random binary, 401, 429
real, 2
real exponential,
right-sided, 104
sinusoidal, 9
telegraph, 427, 428
time-limited, 104
two-sided, 104
Signum function, 254
Similar matrices, 331, 452
Similarity transformation, 331, 451
Simulation, 274, 304
by bilinear transformation, 307
canonical, 347, 348
impulse-invariance method, 306
Singular matrix, 450
Sinusoidal sequences, 14
Sinusoidal signals, 9
Spectral coefficients, 262
Spectral decomposition, 335, 340, 456
Spectrum (or spectra), 195
amplitude, 195
discrete frequency, 195
energy-density, 202
Fourier, 198, 265
line, 195
magnitude, 195, 198
phase, 195, 198
s-plane, 102
Stability:
asymptotical, 337, 340
bounded-input/bounded-output (BIBO), 17, 54, 58, 71, 90, 111, 131, 180, 337, 340
Stable systems, 17
Standard deviation, 475
State, 329, 394
State equations:
continuous-time, 333, 337
discrete-time, 331, 334
State space, 329
State space representation:
continuous-time LTI systems, 332
discrete-time LTI systems, 331, 334
canonical:
the first form, 347
the second form, 348
State-transition matrix, 335
State variables, 329
State vectors, 330
Stationarity, 397
Stationary;
strict-sense (SSS), 397
wide-sense (WSS), 397
Statistical (or ensemble) average, 395, 473
Step response, 52, 57
Stop band, 206
Superposition property, 16
Systems:
causal and noncausal, 16
continuous-time and discrete-time, 15
continuous-time LTI, 51
controllable, 368
described by difference equations, 59, 91
described by differential equations, 54, 75
discrete-time LTI, 56
feedback, 17
interconnection of, 112
invertible, 48
linear and nonlinear, 16
linear time-invariant (LTI), 17, 51
memoryless, 15
multiple-input multiple-output, 331
observable, 369
stable, 17
time-invariant and time-varying, 16
with and without memory, 15
System function:
continuous-time LTI systems, 110, 129, 338
discrete-time LTI systems, 158, 176, 337
System representation, 14
System response, 273
Telegraph signal, 427, 428
Testing function, 7
3-dB bandwidth, 209
Time averages, 399
Time-averaged, autocorrelation, 399
mean, 399
Time convolution theorem, 201, 231
Time delay, 205
Time-invariance, 55
Time-invariant systems, 16
Time reversal, 118, 155, 200, 268, 277
Time scaling, 107, 200, 268
Time shifting, 106, 154, 200, 267, 276
Time-varying systems, 16
Total probability, 463
Transfer function, 111
Transform circuits, 114
Transforms (see Fourier, Laplace, etc.)
Two-dimensional r.v., 468
Two-sided signal, 104
Uniform distribution, 467
Uniform sampling theorem, 254
Unilateral Laplace transform, 101, 134
Unilateral z-transform, 149, 184
Union, 459
Unit-advance operator, 154
Unit circle, 150
Unit-delay operator, 154
Unit-delay element, 41
Unit impulse function, 6
Unit impulse sequence, 11
Unit ramp function, 40
Unit sample response, 56 (See also Impulse response)
Unit sample sequence (see Unit impulse sequence)
Unit step function, 6, 33
Unit step sequence, 11
Variance, 474
Vector mean, 400
Venn diagram, 459
White noise, 419
band-limited, 420
z-plane, 150
z-transform:
bilateral (or two-sided), 148
definition, 148
inverse, 156
properties, 153, 166
region of convergence (ROC), 149
tables, 153, 156
unilateral (or one-sided), 149, 184
Zero-input response, 55
Zero padding, 276
Zero-state response, 55
Zeros, 103








Downloadable videos may be obtained from McGraw-Hill Professional's Media Center at http://mhprofessional.com/mediacenter. Enter this eBook's ISBN and your e-mail address at the Media Center to receive an e-mail message with a download link. This Book's ISBN is 9780071829465.

Back















