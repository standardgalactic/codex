      System to Intermediate System (IS-IS) protocol across the VC links to
      prevent loops. Users can catch a glimpse of it in action by
      running show virtual-chassis protocol
      database:

{master}
lab@s1> show virtual-chassis protocol database
fpc0:
---------------------------------------------------------------------
LSP ID                      Sequence Checksum Lifetime
0019.e256.5880.00-00         0x6a3ba   0x434d      116
0019.e256.5881.00-00         0x6a359   0x5c9f      114
0019.e257.3d80.00-00         0x6a37e   0xd5bf      118
0019.e257.3d81.00-00         0x6a3bf   0xb434      118
0019.e257.3d82.00-00         0x1f4bb   0x39d1      118
  5 LSPs

fpc1:
--------------------------------------------------------------------
LSP ID                      Sequence Checksum Lifetime
0019.e256.5880.00-00         0x6a3ba   0x434d      118
0019.e256.5881.00-00         0x6a359   0x5c9f      116
0019.e257.3d80.00-00         0x6a37e   0xd5bf      116
0019.e257.3d81.00-00         0x6a3bf   0xb434      116
0019.e257.3d82.00-00         0x1f4bb   0x39d1      116
  5 LSPs
This output shows the link-state database that is used to create
      active backplane routes toward each individual switch that is a member
      of the VC. Each switch member has a unique IS-IS link-state packet (LSP).
The following output lists each member switch as a separate FPC
      with individual interfaces that are used for interconnection. Each
      interface has a metric value that is based on its bandwidth. For
      example, a standard VCP has a metric of 10, while 10 GB uplink
      interfaces configured as VCPs have a metric of 30 and a 1 GB interface
      configured as a VCP has a metric of 300:

lab@s1> show virtual-chassis protocol interface
fpc0:
---------------------------------------------------------------------
IS-IS interface database:
Interface             State         Metric
internal-0/24         Up             10
internal-1/25         Up             10
internal-2/24         Up             10
internal-2/27         Up             10
vcp-0                 Up             10
vcp-1                 Up             10

fpc1:
---------------------------------------------------------------------
IS-IS interface database:
Interface             State         Metric
internal-0/27         Up             10
internal-1/24         Up             10
vcp-0                 Up             10
vcp-1                 Up             10
From the perspective of Switch 0 (sw0), sw1
      is accessible along a route that has a metric of 10:

lab@s1> show virtual-chassis protocol route
fpc0:
---------------------------------------------------------------------

Dev 0019.e257.3d80 ucast routing table             Current version: 1462
----------------
System ID          Version   Metric Interface     Via
0019.e256.5880        1462       10 vcp-0         0019.e256.5880
0019.e256.5881        1462       20 vcp-0         0019.e256.5880
0019.e257.3d81        1462       20 internal-0/24 0019.e257.3d82
0019.e257.3d82        1462       10 internal-0/24 0019.e257.3d82

...
On each VC, the VCCPd daemon, which is
      responsible for backplane link management, contains built-in
      reconvergence elements that speed up IS-IS backplane routing to a
      recovery time of just less than one second. Detecting and successfully
      reconverging the ring-based VC topology takes about 300 milliseconds.
      This time is the same as the fast-hello Virtual Router Redundancy Protocol (VRRP) interval used in
      standard IP routing and is significantly faster than many spanning
      tree implementations.


Local-repair
While 300 milliseconds is acceptable for many data applications,
        new video and voice solutions require convergence times to be closer
        to the SONET convergence times of about 50 milliseconds.
        Starting with Release 9.3, JUNOS software supports enhanced VCCPd
        "local-repair," which guarantees a backplane reconvergence time of
        approximately 50 milliseconds. When a VCP interface failure is
        discovered in the VC, the switch adjacent to the interface creates a
        local loopback interface and starts sending traffic in the other
        direction around the VC ring. At the same time, the rest of the VC
        members recompute the new VC topology.
Depending on the VCPs being used for VC backplane connectivity,
        you might have to enable the local-repair feature. By default,
        built-in dedicated VCPs are enabled. However, you must explicitly
        configure 1GE or 10GE interfaces using the following commands:

[edit]
lab@s1# set system virtual-chassis fast-failover xe  enable

[edit]
lab@s1# set system virtual-chassis fast-failover ge  enable






Highly Available Designs for VCs



So far in this chapter, we have discussed the hardware and
      software architecture of the EX Series switches and you have seen that
      VCs offer many cool features. This section discusses how to use these
      features to improve your network's availability.
The examples in this section describe tools to optimize link-based
      redundancy at the network layer. As mentioned earlier,
      even though different types of redundancy solutions and third-party
      appliances are currently available, the solutions discussed in this
      section use product features you already have. They do not require
      additional purchases, only time for configuration testing and
      deployment.




Manipulating a split VC



While you can connect all member switches with the single VC backplane
        cable in a braided VC configuration, for the most optimal high
        availability design it is highly recommended that you use both VCPs
        and that you create ring-based virtual backplane connectivity. The
        ring topology can tolerate a single link failure: when such a failure
        occurs, control traffic is just rerouted in another direction using
        interlink IS-IS routing. For example, in Figure 5-3, traffic moving from
        Switch 0, port ge-0/0/1 to Switch
        3, port ge-3/0/5 would normally
        take Link A because this is the shortest path. If Link A fails,
        traffic moves in the opposite direction of the ring and follows Link
        B. As mentioned earlier, the convergence time of a single VCP link
        failure is about 300 milliseconds, which is fast enough for most data
        applications.
The likelihood of two VC backplane links failing simultaneously
        is very slim, however; redundancy built into the VC design minimizes
        the impact of a dual failure. Simultaneous failure of two VC links is
        referred to as a VC split failure. If this type
        of failure occurs, each separate section elects its own master and
        continues running. If the master and backup switches are both on one
        side of the split, the other side does not have an active master or
        backup for the time that it takes to elect a new master. To minimize
        the chance of a split resulting in the master and backup switches
        being on the same side of the split, we recommend that you configure
        the master and the backup switches as far as possible from each other,
        in terms of routing hops. For example, Figure 5-3 shows recommended
        placement of the master and backup in a ring topology.









Figure 5-3. Ring-based VC backplane topology


Recovery from dual-link failures in a ring-based VC topology has
        one potential problem that is observed in only certain scenarios. If
        the network design architecture uses a globally assigned IP addressing
        scheme and if switches are participating in Interior Gateway Protocol
        (IGP) routing protocols, duplicate IP address and router IDs
        could potentially black-hole traffic and cause networkwide outages.
        Just imagine the VC split scenario after both halves have recovered.
        They both would have the same management IP address and the same
        router ID, resulting in duplicate entries in domainwide routing tables
        and IGP databases.
To mitigate this problem, the recovery algorithm for VC
        dual-link failure has been modified in JUNOS Release 9.3 and later.
        After a VC split, only one-half of the chassis remains active and
        continues forwarding. The other half of the VC is taken offline until
        the VCP link is repaired.
The VC routing protocol algorithm uses the following rules to
        decide which VC side should stay active:



If both the master and the backup switches are in the same
            half, that half stays active and the other half is deactivated and
            its switches stop forwarding.


If master and backup are in different VC halves, the half
            with the most members stays active. The other half is deactivated
            and its switches stop forwarding.


If master and backup are in different VC halves, and both
            halves have the same number of active members, the half with the
            original backup switch stays active. The backup switch is promoted
            to master and a new backup is elected on that half. The other VC
            half is brought offline and its switches stop forwarding.








Server resilience with VCs



The need for highly available access to applications has led to further
        advances in server-based technologies. Even when the entire end-to-end
        network data path is globally and locally redundant, a single point of
        failure still exists: the last connection from the switch port to the
        application server. This problem can be solved by having many
        different clustering and load-balancing systems, or it can be
        addressed with application-level redundancy. However, a fairly
        inexpensive solution is already sitting in many data centers.
Most new chassis or shelf-based blade servers have two or four
        Ethernet interfaces that can all be used simultaneously. With some
        extra configuration on both sides, we can utilize all available
        interfaces on these servers, thus increasing the bandwidth capacity as
        well as filling in the last gap of network layer redundancy. As a
        matter of fact, with VCs, we can actually provide both link and switch
        redundancy.
The EX Series supports a link-bundling concept called Link
        Aggregated Group (LAG). Up to eight individual interfaces can be bundled into
        a single LAG. As shown in Figure 5-4, the links
        between Switch 0 and Switch 1 are simply bundled together. Both links
        are active at all times, and traffic is load-balanced across both
        according to the load-balancing hashing algorithm. Active bundling is
        interoperable with all vendors that support the 802.3ad Ethernet
        standard. To enable LAG on Ethernet interfaces, you configure the
        following:

[edit]
lab@s1# set chassis aggregated-devices ethernet device-count 2

[edit]
lab@s1# set interfaces ae0 aggregated-ether-options minimum-links 1

[edit]
lab@s1# set interfaces ae0 aggregated-ether-options link-speed 10g









Figure 5-4. Separating the master and backup switches in a VC


While this initial configuration example uses only two
        interfaces, a single VC domain can support up to 64 LAGs. We have
        defined the minimum number of links to be 1. If your particular
        network design has a bandwidth requirement and your topology provides
        additional redundant paths, you would set the minimum number of links
        to 2. With that configuration, the aggregated Ethernet interface would
        be brought down and traffic would shift over to the second redundant
        path if the first path fails.
The next configuration step is to add interfaces to the LAG
        bundle. The following configuration joins the xe-0/1/0 and 1/1/0 interfaces to the ae0 interface and assigns a single IP
        address to this bundle:

[edit]
lab@s1# set interfaces xe-0/1/0 ether-options 802.ad ae0

[edit]
lab@s1# set interfaces xe-1/1/0 ether-options 802.ad ae0

[edit]
lab@s1# set interfaces ae0 unit 0 family inet address 192.168.133.1/24
To complete the last leg of redundant connectivity, you
        configure LAGs on the server side of the connection, using
        the NIC teaming feature that is
        provided with many server platforms (see Figure 5-5). You can configure LAGs between either two or all four of the server's network
        interface cards (NICs) and VC switch members. However, to provide full switch
        resiliency, you should connect the NICs to two different switch
        members. Because both switches are part of a single VC, they are just
        two different ports on the same virtual switch. This configuration is
        identical to the previous one, except that the aggregated Ethernet
        member ports are 1G Ethernet interfaces connected to the servers.









Figure 5-5. NIC Teaming



[edit]
lab@s1# set chassis aggregated-devices ethernet device-count 6

[edit]
lab@s1# set interfaces ae2 aggregated-ether-options minimum-links 1

[edit]
lab@s1# set interfaces ae2 aggregated-ether-options link-speed 1g

[edit]
lab@s1# set interfaces ge-3/1/0 ether-options 802.ad ae1

[edit]
lab@s1# set interfaces xe-1/1/0 ether-options 802.ad ae1

[edit]
lab@s1# set interfaces ae2 unit 0 family inet address 192.168.134.1/24
To verify the configuration, you can use the following commands.
        In the show interfaces ae0 terse command, we care
        only that both the Admin and Link states are up. The show interfaces ae0
        statistics command gives more detailed information about the
        aggregated interface, including the traffic load:

lab@s1> show interfaces ae0 terse
Interface             Admin      Link Proto      Local         Remote
ae0                    up       up
ae0.0                  up       up     inet     192.168.133.1/24

lab@s1> show interfaces ae0 statistics
Physical interface: ae0, Enabled, Physical link is Down
Interface index: 153, SNMP ifIndex: 30
Link-level type: Ethernet, MTU: 1514, Speed: Unspecified, Loopback: Disabled,
Source filtering: Disabled, Flow control: Disabled, Minimum links needed: 1,
Minimum bandwidth needed: 0
Device flags : Present Running
Interface flags: Hardware-Down SNMP-Traps Internal: 0x0
Current address: 02:19:e2:50:45:e0, Hardware address: 02:19:e2:50:45:e0
Last flapped : Never
Statistics last cleared: Never
Input packets : 0
Output packets: 0
Input errors: 0, Output errors: 0
Logical interface ae0.0 (Index 71) (SNMP ifIndex 34)
Flags: Hardware-Down Device-Down SNMP-Traps Encapsulation: ENET2
Statistics Packets pps Bytes bps
Bundle:
Input : 0 0 0 0
Output: 0 0 0 0
Protocol inet, MTU: 1500
Flags: None
Addresses, Flags: Dest-route-down Is-Preferred Is-Primary
Destination: 192.168.133/24, Local: 192.168.133.1/24, Broadcast 192.168.133.255
The IEEE 802.3ad Ethernet standard does not define LAG
        liveness detection or LAG auto-configuration. If a single link fails,
        it is removed from the bundle and traffic continues forwarding using another
        interface. To improve on this, the Internet Engineering Task Force (IETF) has standardized
        another protocol, called Link Active Connection Protocol (LACP), which supports
        LAG liveness and auto-configuration. LACP is not just a switch
        protocol, but is also widely accepted and supported by many different
        server platform vendors.
With LACP, each side of the connection sends periodic Hello
        packets. The default Hello time is one second. LACP supports active
        and passive modes. You should configure one side of the connection as
        active and the other as passive.

Note
LACP passive mode stops any Hello advertisements. Make sure
          you do not configure both ends as passive. Otherwise, neither of the
          ends will initiate any communication, and LAG will never come
          up.

LACP is not enabled on LAGs by default. To turn it on, you configure the following:

[edit interfaces]
lab@s1#set ae0 aggregated-ether-options lacp active periodic fast

[edit interfaces]
lab@s1#set ae1 aggregated-ether-options lacp active periodic fast
To verify that LAG with fast LACP is enabled, use the show lacp interfaces
        xe-0/1/0 command:

lab@s1> show lacp interfaces xe-0/1/0

Aggregated interface: ae0

    LACP state:       Role  Exp Def Dist Col Syn Aggr Timeout Activity

      xe-0/1/0      Actor   No  Yes   No  No  No  Yes    Fast   Active

      xe-0/1/0    Partner   No  Yes   No  No  No  Yes    Fast  Passive

    LACP protocol:   Receive State    Transmit State         Mux State

      xe-0/1/0          Defaulted     Fast periodic           Detached















Control System Chassis



In today's tight market space, service and application providers have to maintain their
    competitiveness through constant innovation. New services, faster
    time-to-market deployment schedules, and tighter SLA requirements create
    more pressure on existing network resources. Consolidation of resources
    and access to CPU and I/O in server-based application space has already
    been in the works for several years. Just as it makes absolute business
    sense to consolidate at the application level, it also makes strong
    business sense to lower the cost and high availability pressure by
    consolidating the routing and forwarding planes and by creating a new
    concept of a service-driven network infrastructure.
With this service-driven network you can consolidate all services
    into a shared IP/Multiprotocol Label Switching (MPLS) backbone while providing strict separation of services in
    terms of quality and reliability. All voice applications can be entirely
    separated from video traffic and virtual private network (VPN) transit data services. Additionally, if applicable, the same
    converged IP/MPLS backbone can provide a secure and reliable transit path
    for financial market data and stock tickers. The bottom line is that all
    services can use the same infrastructure without affecting each other's
    reliability and high availability.
Consolidation need not be limited to revenue-generating services.
    Supporting network infrastructure should be consolidated, yielding higher
    capital expenditure (CAPEX) and OPEX savings. A multiple-POP and mega-POP routing
    infrastructure can be collapsed into a single platform. Likewise,
    continental and intercontinental uplinks can be collapsed and shared by
    all services, yielding further cost reduction. Border Gateway Protocol (BGP)-based network services such as
    route reflection, network peering, and policy management can be collapsed into a single platform as
    well.
Building network solutions based on the virtualized router concept
    allows for faster service deployment and higher operational efficiency.
    Migration and provisioning of geographically dispersed wide area network
    (WAN) infrastructure built on a single device in each POP is far more
    efficient. You do not have to build a separate lab or run separate
    continental links for the pre-staging exercise and deployment testing. As
    a result of the concise separation of the control domain, you can safely
    use the existing infrastructure for testing and preprovisioning without
    affecting data paths in the live network. This particular approach
    decreases the time needed to bring new products to market.
From the high availability perspective, strict separation of
    IP/MPLS-based services results in better fault isolation, thus increasing
    network uptime. Having fewer devices results in easier management and a
    clear separation between different management groups. Additionally,
    upgrade cycles and software deployment are far less complex because of the
    consolidation of network resources.
Most of these issues are addressed by logical routers, which were
    introduced in JUNOS 6.0. While this control plane separation has been
    widely accepted, the relatively new Juniper platform called Juniper
    Control System (JCS) 1200 takes services and management separation to the next
    level. The JCS1200 works in conjunction with T Series routers to provide
    scaling of control and forwarding plane resources.




Requirements and Implementation



The JCS1200 is a separate routing node that can host up to 12 REs each running its own version of JUNOS software. The
      JCS connects to a single or to multiple T Series platforms as an
      external virtual router. In this configuration, a single T Series router
      with its own REs becomes a Root System Domain (RSD). Both REs on the T Series router run their own version of JUNOS
      independently of the JUNOS version on the JCSs. Either one or two REs on
      the JCS join together to create a hardware-based virtual router, called
      the Protected System Domain (PSD). All the forwarding resources (i.e., the FPCs and Physical Interface Cards or PICs) of single or multiple RSDs become available to any
      of the REs on the JCS, resulting in the possibility of up to 12
      standalone PSDs (i.e., routers) that each has access to any of the
      available FPCs. Even though the PSD is a virtual entity because it is
      composed of separate hardware components, it still feels, looks, and
      operates like a single JUNOS router. When combined with 16 logical
      routers—the number supported by a single copy of JUNOS software—and a
      maximum of 12 PSDs per JCS, a single JCS1200 can support up to 192
      logical routers.
Figure 5-6 shows
      two PSDs consisting of two REs each and two FPCs that are separated from
      the rest of the forwarding hardware in this RSD, a T Series router. The T Series router is connected to the
      JCS1200 through an Ethernet connection across which all control traffic
      is passed to the RE or set of REs in the JCS chassis.









Figure 5-6. JCS1200 and T640 configured as two PSDs


Because each RSD and PSD is a separate entity running its own
      version of JUNOS, neither PSD nor RSD failures or switchovers affect
      each other. Control traffic destined to individual REs is isolated
      between RSDs and PSDs. Both the RSD and PSD support all high
      availability features such as GRES, NSR, and ISSU. With a fully populated JCS, you would actually end
      up with 12 different hardware routers, and you would be able to access
      any of the forwarding components in all three RSDs. Each of the 12
      partitions can have its own administrator, yet they all could share
      uplinks when needed.
To create an RSD on a T Series router you configure the
      following:

[edit]
lab@r1# show
chassis {
    system-domains {
        root-domain-id 1;
        protected-system-domains {
            psd1 {
                fpcs [0 1];
                control-system-id 1;
                control-slot-numbers [0 1];
                }
            psd2 {
                fpcs [6 7];
                control-system-id 2;
                control-slot-numbers [10 11];
            }
        }
    }
}
In this configuration, the RSD has a unique identifier of 1,
      because it is a single T Series chassis. However, its FPCs are associated with two different PSDs, psd1 and psd2, as shown in Figure 5-6. psd1 owns rsd1's FPC 0 and 1,
      and it owns REs 0 and 1 on the JCS. psd2 owns rsd1's FPC 6 and 7 and
      JCS REs 10 and 11. This configuration results in two
      different, fully operational JUNOS routing domains.





Consolidation Example and Configuration



The configuration in the previous section splits the T Series forwarding node into two
      different routing domains from the administrative and control plane
      perspective. While you could have configured most of this separation
      using logical routers, you can accomplish certain aspects of
      partitioning, including different JUNOS versions, and high availability
      and strict fault isolation, only by actually separating the hardware
      control plane resources using the JCS1200.
To take consolidation to the next step, let's analyze the example
      of a collapsed POP deployment in which P and PE
      MPLS services have been collapsed into four redundant PSDs using one JCS and two T Series nodes, as shown in
      Figure 5-7.









Figure 5-7. Consolidated P/PE POP


As you can see in Figure 5-7, connectivity to the core
      network is provided by MPLS-enabled uplinks. This POP uses two T640s and two
      M320s as the P and PE MPLS-enabled routers, respectively. The
      P and PE routers are interconnected with meshed
      interfaces for redundancy. With the JCS1200 control system we can
      eliminate both M320s from the picture. Additionally, we can eliminate
      the meshed redundant links in favor of internally provisioned tunnel
      interfaces between the PSDs. The consolidated POP now consists of two
      T640 RSDs, rsd1 and rsd2, and a single JCS1200 control system.
      Each RSD services the two PSDs that are used for different MPLS
      applications. rsd1-psd1 and rsd2-psd1
      combinations are used as core facing P routers. rsd1-psd2
      and rsd2-psd2 combinations are used as aggregation
      PE routers. For redundancy purposes,
      each PSD is managed with two redundant REs; therefore, with eight REs in
      use, the JCS1200 control system is able to fully serve two more fully
      redundant PSDs, thus providing scalable growth potential.
Now, let's look at the configuration of the consolidation
      deployment shown in Figure 5-7:

[edit]
lab@r1# show
chassis {
    system-domains {
         root-domain-id 1;
         protected-system-domains {
              psd1 {
                   fpcs [0];
                   control-system-id 1;
                   control-slot-numbers [0];
              }
              psd2 {
                   fpcs [1 2 3];
                   control-system-id 2;
                   control-slot-numbers [1 2 3];
              }
         }
         root-domain-id 2;
              protected-system-domains {
              psd1 {
                   fpcs [0];
                   control-system-id 1;
                   control-slot-numbers [0];
                   }
               psd2 {
                   fpcs [1 2 3];
                   control-system-id 2;
                   control-slot-numbers [1 2 3];
              }
         }
    }
...
Two T640s are configured as rsd1 and rsd2, each providing P and PE
      services. psd1 in both JCSs is used
      as the P node and consists of FPC 0
      connections. psd2 in both JCSs acts
      in the PE role and consists of FPCs
      1, 2, and 3. To verify the configuration, issue the show chassis hardware
      command on each PSD:

lab@psd1> show chassis hardware
rsd-re0:
---------------------------------------------------------------------
Hardware inventory:
Item Version Part number Serial number Description
Chassis S19068 T640
Midplane REV 04 710-002726 AX5666 T640 Backplane
FPM GBUS REV 02 710-002901 HE3251 T640 FPM Board
FPM Display REV 02 710-002897 HE7860 FPM Display
CIP REV 05 710-002895 HC0474 T-series CIP
PEM 1 Rev 03 740-002595 MH15367 Power Entry Module
SCG 0 REV 04 710-003423 HF6042 T640 Sonet Clock Gen.
SCG 1 REV 11 710-003423 HW7765 T640 Sonet Clock Gen.
Routing Engine 0 REV 04 740-014082 1000660098 RE-A-2000
Routing Engine 1 REV 01 740-005022 210865700324 RE-3.0
CB 0 REV 06 710-007655 WE9377 Control Board (CB-T)
CB 1 REV 06 710-007655 WE9379 Control Board (CB-T)
FPC 4 REV 02 710-002385 HC0619 FPC Type 2
CPU REV 06 710-001726 HB1916 FPC CPU
MMB 1 REV 03 710-004047 HE3195 MMB-288mbit
ICBM REV 04 710-003384 HC0377 FPC ICBM
PPB 0 REV 02 710-003758 HC0585 PPB Type 2
PPB 1 REV 02 710-003758 HC0574 PPB Type 2
SPMB 0 REV 10 710-003229 WE9582 T-series Switch CPU
SPMB 1 REV 10 710-003229 WE9587 T-series Switch CPU
SIB 0 REV 05 750-005486 HV8445 SIB-I8-F16
SIB 1 REV 05 750-005486 HW2650 SIB-I8-F16
SIB 2 REV 05 750-005486 HW7041 SIB-I8-F16
SIB 3 REV 05 750-005486 HV4274 SIB-I8-F16
SIB 4 REV 05 750-005486 HV8464 SIB-I8-F16
Fan Tray 0 Front Top Fan Tray
Fan Tray 1 Front Bottom Fan Tray
Fan Tray 2 Rear Fan Tray
psd1-re0:
---------------------------------------------------------------------
Hardware inventory:
Item Version Part number Serial number Description
Chassis 740-023156 SNJCSJCSAC00 JCS1200 AC Chassis
Routing Engine 0 REV 01 740-023157 SNBLJCSAC004 RE-JCS1200-1x2330
As you can see, the hardware inventory is very similar to the
      output of the show chassis hardware
      command on a regular T640. However, the output shows several new items.
      All the chassis hardware of a single T640 is listed under rsd-re0. The new section psd1-re0 lists the
      JCS1200 hardware and the REs associated with psd1. The following shows the hardware
      components belonging to psd2:

lab@psd2> show chassis hardware
rsd-re0:
---------------------------------------------------------------------
Hardware inventory:
Item Version Part number Serial number Description
Chassis S19068 T640
Midplane REV 04 710-002726 AX5666 T640 Backplane
FPM GBUS REV 02 710-002901 HE3251 T640 FPM Board
FPM Display REV 02 710-002897 HE7860 FPM Display
CIP REV 05 710-002895 HC0474 T-series CIP
PEM 1 Rev 03 740-002595 MH15367 Power Entry Module
SCG 0 REV 04 710-003423 HF6042 T640 Sonet Clock Gen.
SCG 1 REV 11 710-003423 HW7765 T640 Sonet Clock Gen.
Routing Engine 0 REV 04 740-014082 1000660098 RE-A-2000
Routing Engine 1 REV 01 740-005022 210865700324 RE-3.0
CB 0 REV 06 710-007655 WE9377 Control Board (CB-T)
CB 1 REV 06 710-007655 WE9379 Control Board (CB-T)
FPC 5 REV 01 710-010233 HM4187 E-FPC Type 1
CPU REV 01 710-010169 HS9939 FPC CPU-Enhanced
MMB 1 REV 01 710-010171 HR0833 MMB-288mbit
SPMB 0 REV 10 710-003229 WE9582 T-series Switch CPU
SPMB 1 REV 10 710-003229 WE9587 T-series Switch CPU
SIB 0 REV 05 750-005486 HV8445 SIB-I8-F16
SIB 1 REV 05 750-005486 HW2650 SIB-I8-F16
SIB 2 REV 05 750-005486 HW7041 SIB-I8-F16
SIB 3 REV 05 750-005486 HV4274 SIB-I8-F16
SIB 4 REV 05 750-005486 HV8464 SIB-I8-F16
Fan Tray 0 Front Top Fan Tray
Fan Tray 1 Front Bottom Fan Tray
Fan Tray 2 Rear Fan Tray
psd2-re0:
---------------------------------------------------------------------
Hardware inventory:
Item Version Part number Serial number Description
Chassis 740-023156 SNJCSJCSAC00 JCS1200 AC Chassis
Routing Engine 0 REV 01 740-023157 SNBLJCSAC006 RE-JCS1200-1x2330
Routing Engine 1 REV 01 740-023157 SNBLJCSAC005 RE-JCS1200-1x2330
To enable the JCS to fully control all of its routing nodes and to
      remotely control the RSD hardware using the JCS, you must configure a set of
      special "blade bay data" commands. The blade bay configuration is then
      passed from the JCS to the REs used in the RSDs. The blade bay data
      configuration uses a special 60-byte test string in the following
      format:

Vn-JCSn-SDn-PSDn-REPn-REBn-PRDplatform-type
For our example, you execute the following blade bay configuration
      commands on the JCS management console of both JCSs.
To assign the REs in slots 1 (primary) and 2 (backup) to rsd1 and psd1:

system> baydata —b 01 —data "V01-JCS01-SD01-PSD01-REP01-REB02-PRDT640"
system> baydata —b 02 —data "V01-JCS01-SD01-PSD01-REP01-REB02-PRDT640"
To assign the REs in slots 3 (primary) and 4 (backup) to rsd1 and psd2:

system> baydata —b 03 —data "V01-JCS01-SD01-PSD02-REP03-REB04-PRDT640"
system> baydata —b 04 —data "V01-JCS01-SD01-PSD02-REP03-REB04-PRDT640"

Note
Essentially, the blade bay configuration file and the allocation
        of REs to PSDs must match what is provided in the JUNOS CLI
        configuration. Because a blade bay is not a typical JUNOS-style
        configuration, we do not elaborate on its syntax and command
        availability. To further use and manipulate blade bay configuration
        options, refer to the JCS1200 documentation guide found at the Juniper
        Networks Technical Support website at http://www.juniper.net.






Taking Consolidation to the Next Level: Scalable Route
      Reflection



Many of you are probably already familiar with the specifics of BGP and how BGP redistributes learned routes to its peers.
      For those who have forgotten it, here's a quick review. BGP never
      readvertises routes learned from IBGP to other IBGP peers. This property mandates that your
      network have a full-mesh IBGP peering among all IBGP speakers. The
      scalability of the full mesh can be solved with two different, yet
      equally well-designed, methods: using route reflectors or
      confederations. Chapter 13
      provides detailed explanations of these solutions.
While both solutions are viable, the market has turned toward
      route reflection as the more popular IBGP scaling solution. Most
      networks build IBGP peering based on redundant pairs or quadruples of route
      reflectors. Some larger-scale networks even use hierarchical route
      reflection designs with multiple planes of route reflectors.
Service providers, large-scale enterprises, and even smaller
      enterprises are in the process of or have already deployed MPLS-based
      solutions for their networks. The most widespread MPLS application is Layer 3 VPNs, also known as 2547 VPNs (named after the original
      IETF draft, draft-ietf-l3vpn-rfc2547bis), now standardized under RFC 4364. An important property of this application is
      that all customer VPN routes are stored on route reflectors.
A second widely implemented MPLS application is Virtual Private LAN Service (VPLS). Here, all Layer 2
      MAC addresses are stored in the form of BGP routes on
      route reflectors. A third
      application, providing scalable solutions, is multicast-enabled Layer 3
      VPNs, known as NGEN MVPN and defined in
      draft-ietf-l3vpn-2547bis-mcast-bgp-07. Most multicast content requests and advertisements known as PIM
      joins and PIM register messages have found their way into the BGP
      control plane, thus adding to the scalability requirement of route
      reflectors. You can see that in addition to regular IBGP peering, the
      widespread use of MPLS applications places extra strain on the
      scalability of route reflectors. While this is not a problem for
      networks with a small number of customer routes, Tier 1 and Tier 2
      service providers might reach the limits of control plane scalability if
      they use regular routers as route reflectors.
The key fact to remember is that route reflection is an
      application of the control plane. Thus, traffic forwarding is not a
      requirement. Therefore, you can safely take one RE, or two for
      redundancy, from the pool of 12 REs in the JCS1200 to serve as a route
      reflector pair. Designing a route reflector in this way provides the
      most scalable solution for converged MPLS networks, without incurring
      additional infrastructure costs for the forwarding plane.














Part II. JUNOS HA Techniques
















Chapter 6. JUNOS Pre-Upgrade Procedures



Because JUNOS software is provided as a package, software upgrades are
  relatively straightforward, low-risk events. However, major upgrades still
  require that you fully reset the updated routing engine (RE), and some planning is necessary to ensure
  that traffic continues to be transported across the network during the
  upgrade. In this chapter, we discuss device and network considerations and
  preparations that you should make before performing a software
  upgrade.













JUNOS Package Overview



Before looking at how or when to upgrade, we need to talk a bit about the naming
    conventions used to identify a JUNOS software package.




Software Package Naming Conventions



Most commercially available JUNOS software packages follow a naming
      convention based on the following model:




Package_name

-

M.NZNumber

-

Audience

-

Validation

.tgz



In this model:




Package_name



Identifies the type of package and platform family to which
            the JUNOS software applies. The software has two major categories
            of packages: jbundle
            and jinstall.
            jbundle comprises just the
            JUNOS components, while jinstall contains the JUNOS components as well as a
            revised FreeBSD. The jinstall
            and jbundle packages are
            provided in platform-specific variations, as shown in Table 6-1.



M.N



Represents the software major release number.



Z



Indicates the type of software release. In most cases, it is an
            R, to indicate that this is
            released software. While not appropriate for
            a high availability network, alpha and beta software is made
            available to some customers for testing purposes. Such software is
            tagged with an A (for alpha-level software),
            B (for beta-level software), or
            I (for internal, test, or experimental
            versions of software) in the Z position.



Number



Describes the incremental release of the major revision or
            the internal build number for non-R releases.



Audience



Identifies the software package as being suitable for a worldwide
            audience (marked as export) or as being suitable for a domestic
            audience. This distinction is made because the domestic version
            contains strong encryption algorithms, which are considered to be
            munitions by the U.S. government, and as such, their export is
            controlled.



Validation



Indicates whether the software has been "signed." An MD5 hash of the
            software image is available for packages that have been signed.
            This hash is used during the software load process to verify the
            integrity of the image. If the image has been tampered with, the
            hash of the image will no longer match the signature.





Table 6-1. JUNOS package_names and platforms









Name


Description and
              relevant platform







jbundle-



M and T Series




