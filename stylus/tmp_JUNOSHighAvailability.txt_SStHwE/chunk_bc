In Chapter 13,
        we describe how to configure route reflectors. We also
        explain how route reflectors and
        clusters support high availability by reducing
        the number of peering relationships that need to be managed by the
        chassis and that need to be configured and monitored by the network
        administrators. Chapter 13 includes a
        collection of high availability-specific design considerations that
        bear repeating here.
When using route reflection, consider these design issues:



Because failure of a route reflector would result in the
            cluster being isolated from the rest of the BGP network, clusters
            should always be configured with redundant route
            reflectors.


Routers chosen to be reflectors should have consistently
            reliable power, cooling, and network connectivity and should be
            central to the network design.


Route reflectors within an Autonomous System (AS) should in most cases be fully
            meshed. This permits every route reflector to receive all known
            BGP paths, thus ensuring completeness of the local routing
            table.


The logical topology should follow the physical topology.
            Route reflection simplifies support of the network by reducing the
            number of required IBGP relationships. However, configuring a
            logical reflector topology that deviates from the physical
            topology adds so much complexity that it may be better not to
            deploy route reflection in the first place.



Route reflection by itself promotes scalability. However, as the
        network grows in size and geographic coverage, two route reflectors
        may no longer be adequate to meet high availability requirements.
        Figure 18-1 shows four
        routers that are part of a large network. Two are in Dallas, and the
        other two are in Chicago. Connectivity among these devices forms a
        physical full mesh.









Figure 18-1. Physical connectivity between Dallas and Chicago


Figure 18-2 shows
        the same four routers, but notice that in this diagram, Dallas Router
        2 and Chicago Router 2 have changed places. Because these routers are
        part of a full mesh, the diagram still accurately reflects their
        relationships with the other routers. Also notice that Figure 18-2 indicates that the
        Dallas and Chicago routers are part of an IBGP full mesh.
With the routers arranged in this manner, we can conveniently
        assign pairs of chassis to serve as route reflector (RR) and backup
        route reflector (BRR). Furthermore, we can take advantage of the geographic
        distance between Dallas and Chicago to add another layer of redundancy
        to our network. Figure 18-3 shows
        the creation of clusters 1.1.1.1 and 2.2.2.2.









Figure 18-2. Dallas and Chicago IBGP relationships











Figure 18-3. Clusters 1.1.1.1 and 2.2.2.2


In Figure 18-3, Chicago
        Router 1 is serving as the RR and Dallas Router 2 is serving as the
        BRR for Cluster 1.1.1.1. Dallas Router 1 and Chicago Router 2 are
        serving as RR and BRR, respectively, for Cluster 2.2.2.2. Not only are
        both clusters being served by a primary and backup route reflector,
        but also the route reflector scheme itself features geographic
        separation for added redundancy and high availability.
The astute reader may notice that two additional geographically
        separated route reflector pairings are possible with this design.
        Figure 18-4 shows the
        placement of clusters 3.3.3.3 and 4.4.4.4.









Figure 18-4. Addition of clusters 3.3.3.3 and 4.4.4.4


In Figure 18-4,
        Cluster 3.3.3.3 has been used for corporate expansion into Canadian
        markets, and Cluster 4.4.4.4 is reserved for future growth.





What's the point?



Figures 18-1 through 18-4 are cumulative, and demonstrate how
        route reflection can be used in a scalable manner to promote
        redundancy and create high availability. This design features a few
        interesting characteristics that are a direct result of RR and BRR
        placement:



No single router is a primary route reflector for more than
            one cluster.


Loss of any single router in the top-tier Dallas-Chicago
            mesh would not isolate any cluster.


Loss of connectivity to an entire city, Dallas or Chicago,
            would not isolate any cluster.



Additionally, this design supports the modern definition of
        scalability because it would be easy to shrink the network as needed.
        You could decommission individual neighbors or entire clusters with a
        few simple configuration changes and without impacting peering
        relationships to other clusters.

Note
JUNOS route reflector and cluster configuration syntax, along
          with IBGP configuration options, is covered in Chapter 13.







MPLS for Network Scalability and High Availability



MPLS is a way to establish and maintain engineered paths that are capable
      of deviating from the IGP-dictated "best path" between any two given
      points. This concept is critical to network scalability, because in the
      absence of a traffic engineering tool such as MPLS, there would be no
      effective way to spread traffic across links to utilize all available
      bandwidth. Recall that all IGPs identify and use only the lowest-cost path through a
      network, leaving other links underutilized.
MPLS label-switched paths (LSPs) differ from classic ATM virtual circuits in that
      LSP configuration is performed only at the point of entry, or ingress,
      of the traffic-engineered domain. Signaling protocols look at the path
      characteristics defined on the ingress router and signal the path to the
      egress point accordingly.
Figure 18-5 shows a small
      network that we use to demonstrate how MPLS meets network scalability
      needs and high availability requirements. In this network, routers
      r1, r2, r3, and
      r4 are IBGP neighbors within AS
      64512. Router r1 has is an External BGP (EBGP) relationship with AS 11, r2 with AS 22, r3 with AS 33, and r4 with AS 44.
In this simplified example, the administrators have decided to
      enable MPLS to control the path taken by traffic transiting between AS
      22 and AS 33. The administrators have decided that this transit traffic
      should pass through r4 regardless of
      the IGP-dictated shortest path through the network. Notice that the link
      between r2 and r1 is a Fast Ethernet segment, whereas the
      other links in the network are T1. IGP metrics would most likely
      describe the path through r1 as the
      best way for packets to travel between r2 and r3.









Figure 18-5. AS 64512 baseline for MPLS






Basic LSP configuration syntax



Because MPLS LSPs are unidirectional, you must configure path characteristics
        on both r2 and r3 (the ingress routers for each direction),
        defining r4 as a transit point.
        Family MPLS should be enabled on all interfaces participating in MPLS switching. Additional relevant
        configuration elements on r2
        include:

[edit protocols]
lab@r2# show mpls
no-cspf;
label-switched-path to-r3 {
    to 10.0.0.3;
    primary transit-r4;
}
path transit-r4 {
    10.0.0.4 loose;
}
interface all;
Relevant configuration elements on r3 include:

[edit protocols mpls]
lab@r3# show
no-cspf;
label-switched-path to-r2 {
    to 10.0.0.2;
    primary transit-r4;
}
path transit-r4 {
    10.0.0.4 loose;
}
interface all;
The only MPLS-related configuration required for r4 is to enable support for label allocation
        and path signaling:

[edit protocols]
lab@r4# show rsvp
interface all;

[edit r4 protocols]
lab@r4# show mpls
no-cspf;
interface all;
With MPLS configured and confirmed, operational transit traffic
        now follows the LSP. JUNOS show
        commands demonstrate desired functionality. Let's look first at r2:

[edit]
lab@r2# run show mpls lsp
Ingress LSP: 1 sessions
To              From        State Rt ActivePath       P     LSPname
10.0.0.3        10.0.0.2    Up     1 transit-r4       *     to-r3
Total 1 displayed, Up 1, Down 0

Egress LSP: 1 sessions
To              From        State   Rt Style Labelin Labelout LSPname
10.0.0.2        10.0.0.3    Up       0  1 FF       3        - to-r2
Total 1 displayed, Up 1, Down 0

Transit LSP: 0 sessions
Total 0 displayed, Up 0, Down 0
Even though the output from r2 appears to be quite positive, it is best
        to confirm from both ingress points by running the command on r3 as well:

[edit]
lab@r3# run show mpls lsp
Ingress LSP: 1 sessions
To              From        State Rt ActivePath       P     LSPname
10.0.0.2        10.0.0.3    Up     1 transit-r4       *     to-r2
Total 1 displayed, Up 1, Down 0

Egress LSP: 1 sessions
To              From        State   Rt Style Labelin Labelout LSPname
10.0.0.3        10.0.0.2    Up       0  1 FF       3        - to-r3
Total 1 displayed, Up 1, Down 0

Transit LSP: 0 sessions
Total 0 displayed, Up 0, Down 0
A quick check of the defined transit point confirms that both
        LSPs have been established and both are using r4 as a transit point as expected:

[edit]
lab@r4# run show mpls lsp
Ingress LSP: 0 sessions
Total 0 displayed, Up 0, Down 0

Egress LSP: 0 sessions
Total 0 displayed, Up 0, Down 0

Transit LSP: 2 sessions
To              From         State   Rt Style Labelin Labelout LSPname
10.0.0.2        10.0.0.3     Up       1  1 FF  299792        3 to-r2
10.0.0.3        10.0.0.2     Up       1  1 FF  299776        3 to-r3
Total 2 displayed, Up 2, Down 0
Figure 18-6 shows the
        bidirectional LSPs we have created that transit r4.





Secondary LSPs



Looking at Figure 18-6
        and the configuration syntax, a very serious high availability concern becomes quite
        apparent. Failure of r4 or of any
        of the links leading to r4 would
        cause the LSP setup to fail. In a larger network, either because of a
        routing loop or because the loose hop is simply not reachable
        (remember, this is a simplified example), loss of the LSP would result
        in complete loss of deterministic paths for transit traffic, which
        would more than likely have a negative impact on any service-level
        agreements (SLAs) tied to the transit traffic. How can we keep this
        failure from impacting the availability of the network?
By configuring a secondary LSP network, you identify two paths,
        a primary and a backup, that meet the requirements for the LSP. The
        primary path is used when available. When this path is not available,
        transit traffic automatically fails over to the secondary path.









Figure 18-6. Bidirectional LSP transiting r4


To configure a secondary path in this example, we add path
        signaling and label switching support to r1:

[edit protocols]
lab@r1# show rsvp
interface all;

[edit protocols]
lab@r1# show mpls
no-cspf;
interface all;
Then we configure the secondary path. Recall that LSPs are
        unidirectional, so we need to change the path characteristics on both
        r2 and r3. Here are the necessary configuration
        changes on r3:

[edit protocols mpls]
lab@r3# show
no-cspf;
label-switched-path to-r2 {
    to 10.0.0.2;
    primary transit-r4;
}
path transit-r4 {
    10.0.0.4 loose;
}
interface all;

[edit protocols mpls]
lab@r3# set label-switched-path to-r2 secondary transit-r1

[edit protocols mpls]
lab@r3# set path transit-r1 10.0.0.1 loose

[edit protocols mpls]
lab@r3# show
no-cspf;
label-switched-path to-r2 {
    to 10.0.0.2;
    primary transit-r4;
    secondary transit-r1;
}
path transit-r4 {
    10.0.0.4 loose;
}
path transit-r1 {
    10.0.0.1 loose;
}
interface all;
And here are the necessary configuration changes on r2:

[edit protocols mpls]
lab@r2# show
no-cspf;
label-switched-path to-r3 {
    to 10.0.0.3;
    primary transit-r4;
}
path transit-r4 {
    10.0.0.4 loose;
}
interface all;

[edit protocols mpls]
lab@r2# set label-switched-path to-r3 secondary transit-r1

[edit protocols mpls]
lab@r2# set path transit-r1 10.0.0.1 loose

[edit protocols mpls]
lab@r2# show
no-cspf;
label-switched-path to-r3 {
    to 10.0.0.3;
    primary transit-r4;
    secondary transit-r1;
}
path transit-r4 {
    10.0.0.4 loose;
}
path transit-r1 {
    10.0.0.1 loose;
}
interface all;
We now use show commands
        to verify the existence of the secondary LSP, first on
        r2, then on r3:

[edit protocols mpls]
lab@r2# run show mpls lsp detail
Ingress LSP: 1 sessions

10.0.0.3
  From: 10.0.0.2, State: Up, ActiveRoute: 1, LSPname: to-r3
  ActivePath: transit-r4 (primary)
  LoadBalance: Random
  Encoding type: Packet, Switching type: Packet, GPID: IPv4
 *Primary   transit-r4       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.29 10.0.1.17
  Secondary transit-r1       State: Dn
    SmartOptimizeTimer: 180
Total 1 displayed, Up 1, Down 0

[edit protocols mpls]
lab@r3# run show mpls lsp detail
Ingress LSP: 1 sessions

10.0.0.2
  From: 10.0.0.3, State: Up, ActiveRoute: 1, LSPname: to-r2
  ActivePath: transit-r4 (primary)
  LoadBalance: Random
  Encoding type: Packet, Switching type: Packet, GPID: IPv4
 *Primary   transit-r4       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.18 10.0.1.30
  Secondary transit-r1       State: Dn
    SmartOptimizeTimer: 180
Total 1 displayed, Up 1, Down 0
Notice that in both the show
        commands, the secondary LSP is tagged as being down. This state is the
        default behavior of JUNOS. When configured, the secondary LSPs are not
        signaled until the primary LSP goes down.





Hot standby



From a transit switching perspective, having the secondary
        LSP remain down until needed may be an acceptable state.
        After all, only a few packets would be lost
        during LSP switchover from primary to secondary. From a high
        availability perspective, however, the failover delay is completely
        unacceptable. Setup and use of the secondary LSP can take several
        seconds, which equates to an eternity in network time.
JUNOS supports an option that allows the secondary path to be
        signaled and established before the primary path has failed. This
        feature, called hot standby, reduces traffic loss
        when the primary LSP fails. You enable hot standby by adding the
        standby statement to the secondary path attribute of the LSP, as shown here on
        r3:

[edit protocols mpls]
lab@r3# set label-switched-path to-r2 secondary transit-r1 standby

[edit protocols mpls]
lab@r3# show
no-cspf;
label-switched-path to-r2 {
    to 10.0.0.2;
    primary transit-r4;
    secondary transit-r1 {
        standby;
    }
}
path transit-r4 {
    10.0.0.4 loose;
}
path transit-r1 {
    10.0.0.1 loose;
}
interface all;
After making the same change on r2, we again look at the paths that have
        been established between r2 and
        r3 using the show mpls lsp detail command. Here are both paths established on r3:

[edit protocols mpls]
lab@r3# run show mpls lsp detail
Ingress LSP: 1 sessions

10.0.0.2
  From: 10.0.0.3, State: Up, ActiveRoute: 1, LSPname: to-r2
  ActivePath: transit-r4 (primary)
  LoadBalance: Random
  Encoding type: Packet, Switching type: Packet, GPID: IPv4
 *Primary   transit-r4       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.18 10.0.1.30
  Standby   transit-r1       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.22 10.0.1.26
Total 1 displayed, Up 1, Down 0
The same state is visible on r2:

[edit protocols mpls]
lab@r2# run show mpls lsp detail
Ingress LSP: 1 sessions

10.0.0.3
  From: 10.0.0.2, State: Up, ActiveRoute: 1, LSPname: to-r3
  ActivePath: transit-r4 (primary)
  LoadBalance: Random
  Encoding type: Packet, Switching type: Packet, GPID: IPv4
 *Primary   transit-r4       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.29 10.0.1.17
  Standby   transit-r1       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.25 10.0.1.21
Total 1 displayed, Up 1, Down 0

Note
To save time, you can also configure hot standby state at the
          protocol's MPLS level of hierarchy, if you want all secondary LSPs
          to be actively signaled:

[edit protocols mpls]
lab@r2# set standby

[edit protocols mpls]
lab@r2# show
no-cspf;
standby;
...

A quick check of the transit routers, r4 and r1, confirms that the primary and secondary
        LSPs are up. On r4, both of the
        primary LSPs are visible:

[edit]
lab@r4# run show mpls lsp transit detail
Transit LSP: 2 sessions

10.0.0.2
  From: 10.0.0.3, LSPstate: Up, ActiveRoute: 1
  LSPname: to-r2, LSPpath: Primary
  Suggested label received: -, Suggested label sent: -
  Recovery label received: -, Recovery label sent: 3
  Resv style: 1 FF, Label in: 299792, Label out: 3
  Time left:  157, Since: Mon Feb 16 20:20:14 2009
  Tspec: rate 0bps size 0bps peak Infbps m 20 M 1500
  Port number: sender 1 receiver 30045 protocol 0
  PATH rcvfrom: 10.0.1.17 (t1-0/0/3.0) 435 pkts
  Adspec: received MTU 1500 sent MTU 1500
  PATH sentto: 10.0.1.30 (t1-0/0/2.0) 432 pkts
  RESV rcvfrom: 10.0.1.30 (t1-0/0/2.0) 433 pkts
  Record route: 10.0.1.17 <self> 10.0.1.30

10.0.0.3
  From: 10.0.0.2, LSPstate: Up, ActiveRoute: 1
  LSPname: to-r3, LSPpath: Primary
  Suggested label received: -, Suggested label sent: -
  Recovery label received: -, Recovery label sent: 3
  Resv style: 1 FF, Label in: 299776, Label out: 3
  Time left:  159, Since: Mon Feb 16 20:18:03 2009
  Tspec: rate 0bps size 0bps peak Infbps m 20 M 1500
  Port number: sender 1 receiver 55528 protocol 0
  PATH rcvfrom: 10.0.1.30 (t1-0/0/2.0) 439 pkts
  Adspec: received MTU 1500 sent MTU 1500
  PATH sentto: 10.0.1.17 (t1-0/0/3.0) 435 pkts
  RESV rcvfrom: 10.0.1.17 (t1-0/0/3.0) 436 pkts
  Record route: 10.0.1.30 <self> 10.0.1.17
Total 2 displayed, Up 2, Down 0
On r1, both of the secondary
        LSPs are also visible:

[edit]
lab@r1# run show mpls lsp transit detail
Transit LSP: 2 sessions

10.0.0.2
  From: 10.0.0.3, LSPstate: Up, ActiveRoute: 1
  LSPname: to-r2, LSPpath: Secondary
  Suggested label received: -, Suggested label sent: -
  Recovery label received: -, Recovery label sent: 3
  Resv style: 1 FF, Label in: 299808, Label out: 3
  Time left:  144, Since: Mon Feb 16 21:06:03 2009
  Tspec: rate 0bps size 0bps peak Infbps m 20 M 1500
  Port number: sender 2 receiver 30048 protocol 0
  PATH rcvfrom: 10.0.1.21 (t1-0/0/2.0) 368 pkts
  Adspec: received MTU 1500 sent MTU 1500
  PATH sentto: 10.0.1.26 (fe-0/0/0.0) 367 pkts
  RESV rcvfrom: 10.0.1.26 (fe-0/0/0.0) 368 pkts
  Record route: 10.0.1.21 <self> 10.0.1.26

10.0.0.3
  From: 10.0.0.2, LSPstate: Up, ActiveRoute: 1
  LSPname: to-r3, LSPpath: Secondary
  Suggested label received: -, Suggested label sent: -
  Recovery label received: -, Recovery label sent: 3
  Resv style: 1 FF, Label in: 299792, Label out: 3
  Time left:  146, Since: Mon Feb 16 21:05:59 2009
  Tspec: rate 0bps size 0bps peak Infbps m 20 M 1500
  Port number: sender 2 receiver 55533 protocol 0
  PATH rcvfrom: 10.0.1.26 (fe-0/0/0.0) 368 pkts
  Adspec: received MTU 1500 sent MTU 1500
  PATH sentto: 10.0.1.21 (t1-0/0/2.0) 367 pkts
  RESV rcvfrom: 10.0.1.21 (t1-0/0/2.0) 368 pkts
  Record route: 10.0.1.26 <self> 10.0.1.21
Total 2 displayed, Up 2, Down 0
Figure 18-7 shows the
        current state of our bidirectional LSPs. Notice that the primary path
        transits r4 and that the secondary
        path transits r1.




Fast reroute



With both the primary and secondary LSPs currently signaled as being
          up, a thorough lab test reveals a significant improvement in
          failover time over what was seen without the standby option. However, the testing would
          also show that an unacceptable amount of traffic is still lost
          during the switchover. Traffic continues to be lost while the
          network forwards information about the failed primary upstream to
          the ingress router. With the configuration elements shown so far,
          only when the ingress router learns that the primary LSP is down
          does it start using the secondary LSP. The amount of traffic lost is
          proportional to the hop-count distance between the point of failure
          and the ingress router. For example, if an LSP transiting nine
          routers experiences a failure in the seventh router from the
          ingress, the failover time could be substantial because the signal
          to fail over would have to be carried across multiple hops back to
          the ingress router.
Fast reroute works in conjunction with
          the hot standby LSP to minimize the amount of traffic lost when an
          LSP fails. Fast reroute allows the router immediately upstream from
          a failed link or node along an LSP to find an alternate route around
          the failure and forward traffic, while simultaneously sending
          notification of the failure back to the ingress node to trigger use
          of the secondary LSP. To enable fast reroute, add the fast-reroute statement to the LSP
          configuration stanza, as shown here for r2:








Figure 18-7. Primary and secondary LSPs


[edit protocols mpls]
lab@r2# set label-switched-path to-r3 fast-reroute

[edit protocols mpls]
lab@r2# show
no-cspf;
label-switched-path to-r3 {
    to 10.0.0.3;
    fast-reroute;
    primary transit-r4;
    secondary transit-r1 {
        standby;
    }
}
path transit-r4 {
    10.0.0.4 loose;
}
path transit-r1 {
    10.0.0.1 loose;
}
interface all;
After adding the fast-reroute statement to both r2 and r3 and committing the configuration
          changes, use the show mpls lsp ingress
          detail command to view the effects of the change. Router r2 shows that fast reroute has been
          enabled:

[edit protocols mpls]
lab@r2# run show mpls lsp ingress detail
Ingress LSP: 1 sessions

10.0.0.3
  From: 10.0.0.2, State: Up, ActiveRoute: 1, LSPname: to-r3
  ActivePath: transit-r4 (primary)
  FastReroute desired
  LoadBalance: Random
  Encoding type: Packet, Switching type: Packet, GPID: IPv4
 *Primary   transit-r4       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.29 10.0.1.17
  Standby   transit-r1       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.25 10.0.1.21
Total 1 displayed, Up 1, Down 0
Output from this command shows the change was successful on
          r3 as well:

[edit protocols mpls]
lab@r3# run show mpls lsp ingress detail
Ingress LSP: 1 sessions

10.0.0.2
  From: 10.0.0.3, State: Up, ActiveRoute: 1, LSPname: to-r2
  ActivePath: transit-r4 (primary)
  FastReroute desired
  LoadBalance: Random
  Encoding type: Packet, Switching type: Packet, GPID: IPv4
 *Primary   transit-r4       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.18 10.0.1.30
  Standby   transit-r1       State: Up
    SmartOptimizeTimer: 180
    Received RRO (ProtectionFlag 1=Available 2=InUse
    4=B/W 8=Node 10=SoftPreempt):
          10.0.1.22 10.0.1.26
Total 1 displayed, Up 1, Down 0
Testing confirms that life is good. The deployment of hot
          standby secondary LSPs with fast reroute enabled means the
          administrators can balance traffic across the topology while
          maintaining high availability.





Link and node-link protection



Fast reroute is configured on a per-LSP basis and as such each fast
          reroute LSP protects only one primary LSP, a one-to-one scheme.
          While fast reroute does work well, in very large-scale networks it
          is obviously suboptimal.
Link and node-link protection are alternatives to fast reroute
          that can protect multiple LSPs by using only a single backup LSP.
          As its name implies, link protection can provide
          redundancy for links. Node-link protection can provide redundancy
          for nodes and links. Both types of protection are standards-based
          and designed to interoperate with other vendors' equipment.
In this example, which is based on Figure 18-7, we will remove the
          fast reroute statements and enable node-link-protection on the to-r3 LSP:

[edit protocols mpls]
lab@r2# show
label-switched-path to-r3 {
    to 10.0.0.3;
    node-link-protection;
    }
interface all;
Node-link protection must also be enabled under protocols rsvp on all interfaces which the
          LSP will transit:

[edit protocols rsvp]
lab@r2# show
interface all {
    link-protection;
}

Note
Link and node-link protection require Constrained Shortest
            Path First (CSPF) to be enabled within the MPLS domain to
            calculate the redundant path. When enabling CSPF in an Open
            Shortest Path First (OSPF) domain, recall that traffic-engineering must also be enabled
            to allow CSPF to build the traffic engineering database (TED). The
            highlighted lines in the following output are symptoms of failure
            to enable traffic engineering, in this case on r2:

[edit]
lab@r2# run show mpls lsp extensive
Ingress LSP: 1 sessions

10.0.0.3
  From: 10.0.0.2, State: Dn, ActiveRoute: 0, LSPname: to-r3
  ActivePath: (none)
  Node/Link protection desired
  LoadBalance: Random
  Encoding type: Packet, Switching type: Packet, GPID: IPv4
  Primary                    State: Dn
    SmartOptimizeTimer: 180
    Will be enqueued for recomputation in 26 second(s).
    1 May 21 12:32:22.651 CSPF: could not determine self[7 times]
  Created: Thu May 21 12:29:25 2009
Total 1 displayed, Up 0, Down 1

A quick show mpls lsp on
          r2 confirms that the LSP to r3 is up. Notice that additional bypass
          LSPs have been automatically calculated by CSPF and signaled by RSVP
          to protect the path:

[edit]
lab@r2# run show mpls lsp
Ingress LSP: 1 sessions
To              From      State Rt ActivePath       P     LSPname
10.0.0.3        10.0.0.2  Up     0                  *     to-r3
Total 1 displayed, Up 1,  Down 0

Egress LSP: 2 sessions
To              From      State   Rt Style Labelin Labelout LSPname
10.0.0.2        10.0.0.3  Up       0  1 SE       3        - to-r2
10.0.0.2        10.0.0.4  Up       0  1 SE       3        - Bypass->10.0.1.30
Total 2 displayed, Up 2,  Down 0

Transit LSP: 1 sessions
To              From      State   Rt Style Labelin Labelout LSPname
10.0.0.3        10.0.0.4  Up       0  1 SE  299776   299792 Bypass->10.0.1.17
Total 1 displayed, Up 1,  Down 0







Traffic Engineering Case Study



At the beginning of this section, we explained that MPLS is itself a scalability
      mechanism, in that it enables traffic engineering.
      Traffic engineering is the act of creating and assigning traffic flows
      to deterministic paths through the network that may be contrary to the
      IGP-dictated "shortest path." This allows the administrator to control
      link utilization and traffic patterns regardless of the size of the
      network.
Figure 18-8 shows a slightly
      more complex topology than in the previous example to further
      demonstrate how to use MPLS to protect availability and scale a
      network.
In this example, user communities on r4 and r5
      have availability-related service agreements within AS 64512. AS 64512
      also has bandwidth-related SLAs with peer ASs. By using MPLS to engineer the user community traffic across the
      less desirable Fast Ethernet and slower OC-3 ATM links, the network is
      able to meet both requirements while optimizing use of available
      bandwidth.
For both the AS-internal and AS-external customers, availability is protected by
      allowing traffic to fail over to alternate paths as needed. The
      higher-bandwidth links are used for AS-external traffic and the
      lower-bandwidth links are reserved for traffic with source and
      destination addresses internal to the AS.









Figure 18-8. MPLS and network scalability
















Chapter 19. Choosing, Migrating, and Merging Interior Gateway Protocols



Interior Gateway Protocols (IGPs) provide the foundation of traffic flow and control inside an
  Autonomous System (AS). This chapter discusses the two most commonly used
  industry-standard IGPs: Open Shortest Path First (OSPF) and Intermediate
  System to Intermediate System (IS-IS). The first section examines the
  advantages and disadvantages of each protocol, and also looks at how each
  one supports high availability. The next section examines what is involved
  in migrating from one of these IGPs to the other. The chapter finishes with
  considerations for merging networks running the same IGP.













Choosing Between IS-IS and OSPF



OSPF and IS-IS are two robust and scalable link-state protocols that are used in
    many networks throughout the Internet. They are based on industry
    standards, meaning they are available and interoperable with all major
    vendors' routing equipment. These protocols are also arguably among the
    earliest forms of high availability. Sure, dynamic routing protocols were
    designed to ease the administrative burden as networks grow, but the real
    power is in their ability to dynamically detect an issue in a network and
    to recover from that issue by rerouting traffic along an alternate path.
    This is high availability in action!
Since OSPF and IS-IS were first implemented in commercial routers,
    back in the early 1990s, there has been a great deal of debate over which
    protocol is superior. Today, many of each protocol's unique features have
    been implemented by the other, making both protocols very similar in
    feature set and functionality. That being said, there are still aspects of
    each that can be considered advantageous or disadvantageous, depending on
    the situation and environment.




OSPF



OSPF was developed by the Internet Engineering Task Force (IETF) as a reliable
      routing protocol that runs directly on IP. It was designed specifically
      with IP networks in mind, and has found a home largely in medium-size to
      large enterprise networks, although it is used by several service
      providers as well.
JUNOS Software supports the full range of OSPF features you would
      expect to find in high-caliber
      networking devices. JUNOS Software also supports a wide range of OSPF's high availability features. And
      like any protocol, OSPF has its advantages and disadvantages.


OSPF Fundamentals
This book assumes you have good knowledge of routing and routing
        protocols. Should you want or need more information on OSPF
        fundamentals, you can find a wide range of material on the Internet.
        You can also learn more about the protocol and its implementation on
        JUNOS devices by attending Juniper Networks training courses.
For more details on Juniper Networks training, see http://www.juniper.net/training.





Advantages



