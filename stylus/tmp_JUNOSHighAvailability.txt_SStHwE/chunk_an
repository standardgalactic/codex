error: device fe-0/0/1 not found
To remedy this situation, you can attempt to bring the PIC back
        online. However, if the problem is a compatibility issue between
        chassis hardware and the new revision of JUNOS, the operation will
        fail. In this example there is no compatibility issue and the PIC is
        easily brought back online:

[edit]
lab@j2300-1# run request chassis fpc online slot 0
Online initiated, use "show chassis fpc" to verify

[edit]
lab@j2300-1# run show chassis fpc pic-status
Slot 0   Online       FPC
  PIC 0  Online       2x FE, 2x T1
Because the PIC, inseparable from the FPC on a J Series chassis,
        could be brought back online, it is time to check the logfiles
        to see what really happened:

[edit]
lab@j2300-1# run show log messages | match fpc
Apr 21 14:02:41   chassisd[2361]: CHASSISD_IFDEV_DETACH_FPC: ifdev_detach(0)
May 10 18:34:24   chassisd[2366]: CHASSISD_SNMP_TRAP9:
SNMP trap generated: FRU power on (jnxFruContentsIndex 7,
jnxFruL1Index 1, jnxFruL2Index 0, jnxFruL3Index 0,
jnxFruName FPC @ 0/*/*, jnxFruType 3,
jnxFruSlot 1, jnxFruOfflineReason 2, jnxFruLastPowerOff 0,
jnxFruLastPowerOn 3008)
May 11 18:30:54   fpc0: PFEMAN: Session manager active
May 11 18:30:54   fpc0: PFEMAN: Established connection to Master
May 11 18:30:54   fpc0: PFEMAN: sent Resync request to Master
May 11 18:30:54   fpc0: SNTP: Daemon created
May 11 18:30:54   fpc0: Version 7.1R1.3 by builder on 2005-02-11 04:39:17 UTC
May 11 18:30:54   fpc0: SNTPD: Initial time of day set.
May 11 18:30:54   fpc0: PFEMAN: received Resync complete.
May 11 18:30:54   fpc0: Installing T1/E1 driver ...
May 19 17:15:15  j2300-1 mgd[3182]: UI_CMDLINE_READ_LINE:
User 'dspitzer', command 'run show chassis fpc pic-status '
May 19 17:15:32  j2300-1 mgd[3182]: UI_CMDLINE_READ_LINE:
User 'dspitzer', command 'run request chassis fpc offline slot 0 '
May 19 17:15:32  j2300-1 chassisd[2371]: CHASSISD_FRU_OFFLINE_NOTICE:
Taking FPC 0 offline: Offlined by cli command
May 19 17:15:32  j2300-1 chassisd[2371]: CHASSISD_IFDEV_DETACH_FPC:
ifdev_detach(0)
May 19 17:15:32  j2300-1 fpc0: CMLC: Master closed connection
May 19 17:15:32  j2300-1 fpc0: Going disconnected;
Routing engine chassis socket closed abruptly
May 19 19:26:20  j2300-1 mgd[3190]: UI_CMDLINE_READ_LINE:
User 'lab', command 'run show log messages | match fpc '
In this case, no compatibility issue between the new operating
        system and the device hardware is reported. The FPC and PIC, taken
        offline by user dspitzer, can be
        brought back online and interface information will return to the
        pertinent show interfaces command
        output.
On with the verification!





Verify memory



To verify the memory on the REs, execute the show system storage
        command on both the master and backup REs, assuming the chassis has
        two REs. You issue the command twice because each RE has separate
        memory, including CompactFlash, a hard drive, and possibly also
        removable media. Failure of any memory component on one of the REs
        negatively impacts the resilience and potentially the availability of
        the chassis. The following output is from a J2300 router:

lab@j2300-1> show system storage
Filesystem         Size     Used      Avail  Capacity   Mounted on
/dev/ad0s1a        213M      59M       152M       28%  /
devfs              1.0K     1.0K         0B      100%  /dev
devfs              1.0K     1.0K         0B      100%  /dev/
/dev/md0           133M     133M         0B      100%  /junos
/cf                213M      59M       152M       28%  /junos/cf
devfs              1.0K     1.0K         0B      100%  /junos/dev/
procfs             4.0K     4.0K         0B      100%  /proc
/dev/bo0s1e         24M     7.0K        24M        0%  /config
/dev/md1            63M     6.2M        52M       11%  /mfs
/dev/md2            58M    10.0K        53M        0%  /jail/tmp
/dev/md3           7.7M     106K       7.0M        1%  /jail/var
devfs              1.0K     1.0K         0B      100%  /jail/dev
/dev/md4           1.9M     6.0K       1.7M        0%  /jail/html/oem
When the same command is run on a different chassis, an M40e,
        the output is in fact different, but still comparable in many
        ways:

[edit]
lab@m40-3-re0# run show system storage

Filesystem      512-blocks     Used    Avail Capacity Mounted on
/dev/ad0s1a         437522    67746   334776     17%  /
devfs                   32       32        0    100%  /dev/
/dev/vn0             22816    22816        0    100%  /packages/mnt/jbase
/dev/vn1             77956    77956        0    100%  /packages/mnt/jkernel-
/dev/vn2             20720    20720        0    100%  /packages/mnt/jpfe-
/dev/vn3              4492     4492        0    100%  /packages/mnt/jdocs-
/dev/vn4             27556    27556        0    100%  /packages/mnt/jroute-
/dev/vn5              9620     9620        0    100%  /packages/mnt/jcrypto-
mfs:152            4064278        8  3739128      0%
/tmp/dev/ad0s1e      48570       10    44676      0%  /config
procfs                   8        8        0    100%  /proc
/dev/ad1s1f       52249814   212192 47857638      0%  /var
In the output from the show system
        storage command, it is important that you scan the Filesystem column for storage areas. One
        memory location is visible in the J2300 command outputâ€”a CompactFlash drive. Two memory locations, a CompactFlash
        and a hard drive, are visible in the M40e command output.
The CompactFlash memory is a nonrotating drive. A copy of the
        JUNOS operating system and recent configuration files are present on
        the CompactFlash. It is identified as /ad0 in the first column of the command
        output shown earlier. If the router's CompactFlash has failed or is faulty, no
        Filesystem listings will be listed
        for /ad0.
The hard drive is a rotating drive. A copy of the JUNOS
        operating system and all archived files are present on the hard drive.
        This drive is also used to store system logfiles and diagnostic dump
        files. It is identified as /ad1 in
        the command output shown earlier. Routers with a failed or faulty hard
        drive will have no Filesystem
        listings for /ad1.

Note
When redundant components are present in the upgraded chassis,
          administrators sometimes force the chassis to fail back and forth
          between the components before making configuration changes to resume
          actively passing traffic. If you decide to do this, make sure you do
          it within the established maintenance window:

lab@m40-3-re0> request chassis routing-engine master switch







Network State (Routes, Peering Relationships, and
      Databases)



After you confirm that the hardware and all configured interfaces on
      the device are stable and functional, it is time to check for peering
      relationships with neighbors and expected routes from other devices in
      the network. To begin this process, first confirm that no related
      configuration elements have been disabled or deleted.
For the sake of literary excitement, let's assume we have just
      completed our hardware and interface and memory verifications for
      r1. Router r1 is an Internet gateway router in the small
      enterprise network shown in Figure 8-1.









Figure 8-1. Small enterprise network


Now that the device state is verified, we need to check peering
      relationships, route table content, and protocol databases.




Verify routing



In many cases, a suspected "routing issue" that occurs during a
        software upgrade or at any other
        time is nothing more than a configuration error or an
        overlooked configuration change.
        Depending on what changes were applied before the upgrade to make the
        chassis a less desirable choice for transit traffic, it may be
        necessary to remove some configuration workarounds to bring up peering
        relationships and populate routing tables:

[edit protocols ospf]
lab@r1# show
area 0.0.0.0 {
    interface ge-0/1/1.0;
    interface ge-0/2/0.0 {
        disable;
        passive;
    }
    interface ge-0/0/0.0;
    interface ge-0/0/1.0 {
        passive;
    }
    interface lo0.0 {
        passive;
    }
    interface ge-0/1/0.0 {
        disable;
        passive;
    }
}
The output from this command indicates the OSPF protocol has been disabled on several interfaces.
        This action was most likely taken to stop the router from attracting
        transit traffic during the software upgrade. Significant time can be
        wasted attempting to troubleshoot OSPF adjacencies across these
        disabled links. Let's delete the disable tag before moving to the next
        step:

[edit protocols ospf]
lab@r1# show
area 0.0.0.0 {
    interface ge-0/1/1.0;
    interface ge-0/2/0.0 {
        passive;
    }
    interface ge-0/0/0.0;
    interface ge-0/0/1.0 {
        passive;
    }
    interface lo0.0 {
        passive;
    }
    interface ge-0/1/0.0 {
        passive;
    }
}
 [edit protocols ospf]
lab@r1# commit
commit complete
Now that the disable tag has
        been removed, use simple show
        commands to check OSPF peering relationships with neighboring routers
        and awareness of passive interfaces. The output from this command is
        consistent with the network map in Figure 8-1:

[edit protocols]
lab@r1# run show ospf neighbor
Address          Interface         State     ID             Pri  Dead
10.0.1.21        ge-0/1/1.0        Full      10.0.0.3       128    37
10.0.1.26        ge-0/0/0.0        Full      10.0.0.2       128    36

[edit protocols]
lab@r1# run show ospf interface
Interface           State     Area        DR ID       BDR ID       Nbrs
ge-0/1/1.0          BDR     0.0.0.0     10.0.0.3    10.0.0.1        1
ge-0/0/0.0          BDR     0.0.0.0     10.0.0.2    10.0.0.1        1
ge-0/2/0.0          DRother 0.0.0.0     0.0.0.0     0.0.0.0         0
ge-0/1/0.0          DRother 0.0.0.0     0.0.0.0     0.0.0.0         0
ge-0/0/1.0          DRother 0.0.0.0     0.0.0.0     0.0.0.0         0
lo0                 DRother 0.0.0.0     0.0.0.0     0.0.0.0         0

Note
In small to medium-size enterprise networks, a manual review
          of the output from these commands is probably enough to let you know
          whether there is an issue. However, in larger networks with a
          greater number of interfaces, the chance of missing something
          important increases.
Because human error is, in fact, the leading cause of network
          downtime, you should have a copy of the pre-upgrade output of these
          commands to compare to the post-upgrade list of peering
          relationships.






Routing table consistency



Assuming that all IGP routing configuration has been returned to its
        pre-upgrade state, you now check the routing tables for consistency
        across the network. Although r1 is
        the focus of this effort, to completely check consistency you must
        also look at the table on a few other routers in the network.

Note
Many of the changes that were recommended as part of the
          pre-upgrade and upgrade processes would be "undone" by loading a
          saved configuration file, checking the differences, and committing.
          In this section, we'll attempt to address issues on adjacent devices
          that would impact high availability and might not be remedied by a
          rollback or load function on the device being
          upgraded.
Resuming functionality using a rollback or file load command
          is still one of the most common methods of configuration recovery
          and is addressed in the next section of this chapter.

Starting with r1, check
        routes to the loopback addresses of other routers in the network:

[edit]
lab@r1# run show route 10.0.0/24

inet.0: 32 destinations, 32 routes (32 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

10.0.0.1/32        *[Direct/0] 02:07:42
                    > via lo0.0
10.0.0.2/32        *[OSPF/10] 01:18:14, metric 1
                    > to 10.0.1.26 via ge-0/0/0.0
10.0.0.3/32        *[OSPF/10] 01:18:19, metric 1
                    > to 10.0.1.21 via ge-0/1/1.0
10.0.0.4/32        *[OSPF/10] 01:18:09, metric 2
                      to 10.0.1.21 via ge-0/1/1.0
                    > to 10.0.1.26 via ge-0/0/0
10.0.0.5/32        *[OSPF/10] 01:18:04, metric 2
                    > to 10.0.1.21 via ge-0/1/1.0
10.0.0.6/32        *[OSPF/10] 01:18:04, metric 3
                    > to 10.0.1.21 via ge-0/1/1.0
Router r1 looks fine, but
        when we run the same command on r2,
        some inconsistencies quickly become apparent:

[edit]
lab@r2# run show route 10.0.0/24

inet.0: 31 destinations, 32 routes (31 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

10.0.0.1/32        *[OSPF/10] 00:04:58, metric 3
                    > to 10.0.1.29 via ge-0/1/1.0
10.0.0.2/32        *[Direct/0] 02:07:47
                    > via lo0.2
10.0.0.3/32        *[OSPF/10] 00:04:58, metric 2
                    > to 10.0.1.29 via ge-0/1/1.0
10.0.0.4/32        *[OSPF/10] 01:18:24, metric 1
                    > to 10.0.1.29 via ge-0/1/1.0
10.0.0.5/32        *[OSPF/10] 01:18:09, metric 2
                    > to 10.0.1.29 via ge-0/1/1.0
10.0.0.6/32        *[OSPF/10] 01:18:09, metric 3
                    > to 10.0.1.29 via ge-0/1/1.0
Router r2 appears to be
        transiting r4 to get to the
        loopback address of r1 even though
        r2 has a direct connection. A quick
        configuration check of r2 shows a
        high metric setting for the direct link to r1:

[edit]
lab@r2# edit protocols ospf area 0

[edit protocols ospf area 0.0.0.0]
lab@r2# show
interface ge-0/2/0.0 {
    passive;
}
interface ge-0/0/0.0 {
    metric 65000;
}
interface ge-0/1/0.0 {
    passive;
}
interface ge-0/0/1.0 {
    passive;
}
interface ge-0/1/1.0;
interface lo0.0;
This configuration is most likely a leftover from the efforts to
        make r1 look less attractive to
        transit traffic during the software upgrade, and is easy enough to
        fix:

[edit protocols ospf area 0.0.0.0]
lab@r2# delete interface ge-0/0/0.0 metric

[edit protocols ospf area 0.0.0.0]
lab@r2# show
interface ge-0/2/0.0 {
    passive;
}
interface ge-0/0/0.0;
interface ge-0/1/0.0 {
    passive;
}
interface ge-0/0/1.0 {
    passive;
}
interface ge-0/1/1.0;
interface lo0.2;

[edit protocols ospf area 0.0.0.0]
lab@r2# commit
commit complete
After the change is committed, the metric and path back to
        r1 look much better:

[edit protocols ospf area 0.0.0.0]
lab@r2# run show route 10.0.0/24

inet.0: 31 destinations, 31 routes (31 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

10.0.0.1/32        *[OSPF/10] 00:00:06, metric 1
                    > to 10.0.1.25 via ge-0/0/0.0
10.0.0.2/32        *[Direct/0] 02:13:00
                    > via lo0.2
10.0.0.3/32        *[OSPF/10] 00:00:06, metric 2
                      to 10.0.1.29 via ge-0/1/1.0
                    > to 10.0.1.25 via ge-0/0/0.0
10.0.0.4/32        *[OSPF/10] 01:23:37, metric 1
                    > to 10.0.1.29 via ge-0/1/1.0
10.0.0.5/32        *[OSPF/10] 01:23:22, metric 2
                    > to 10.0.1.29 via ge-0/1/1.0
10.0.0.6/32        *[OSPF/10] 01:23:22, metric 3
                    > to 10.0.1.29 via ge-0/1/1.0
Based on the setting found on r2, we suspect the same setting may be in
        place on r3, which also has a
        direct interface connection to r1.
        A quick look at the r3
        configuration confirms the suspicion:

[edit protocols ospf]
lab@r3# show
area 0.0.0.0 {
    interface ge-0/1/00 {
    }
    interface ge-0/2/00 {
        passive;
    }
    interface ge-0/1/10 {
        metric 65000;
    }
    interface ge-0/0/1.301 {
        passive;
    }
    interface ge-0/0/1.302 {
        passive;
    }
    interface lo0.0 {
        passive;
    }
    interface ge-0/0/0.0;
}
After removing the metric setting on the interface between
        r3 and r1, routing among the loopbacks in the
        network looks fine. Issuing a few additional show route commands indicates that
        intradomain routing is back to where it should be.
The next thing to confirm is routing to destinations external to
        the local domain. To do this, we start by going as deep into the
        network as possible. Browsing to http://www.juniper.net from host systems on the Corp
        601 and 602 subnets is successful and confirms routing to the external
        destinations. However, because the network has high availability
        requirements, redundant Internet gateways r1 and r2
        have been provisioned. To make sure this redundancy is still in
        effect, we must confirm that both r1 and r2
        are sending the default route out to the Internet. We can check this
        by looking into the OSPF database. Because the OSPF standard dictates all routers within an area share
        a common topological view, and all the devices in this network are in
        the same area, the database can be checked on any of our routers.
        Here's the output from r5:

[edit]
lab@r5# run show ospf database

    OSPF link state database, Area 0.0.0.0
 Type       ID            Adv Rtr       Seq      Age  Opt  Cksum  Len
Router   10.0.0.1      10.0.0.1     0x8000000c   511  0x22 0x3a09  96
Router   10.0.0.2      10.0.0.2     0x8000000c   511  0x22 0x80b0  96
Router   10.0.0.3      10.0.0.3     0x8000000d   510  0x22 0x448b 108
Router   10.0.0.4      10.0.0.4     0x8000000d   510  0x22 0xa1f5 108
Router  *10.0.0.5      10.0.0.5     0x8000000e   509  0x22 0x52d3  84
Router   10.0.0.6      10.0.0.6     0x8000000c   510  0x22 0x8977  84
Network  10.0.1.1      10.0.0.6     0x80000005  1852  0x22 0xa657  32
Network  10.0.1.5      10.0.0.6     0x80000005   966  0x22 0x7e7b  32
Network *10.0.1.9      10.0.0.5     0x80000004   965  0x22 0x38c2  32
Network *10.0.1.13     10.0.0.5     0x80000004  1851  0x22 0x1ed7  32
Network  10.0.1.17     10.0.0.3     0x80000005   966  0x22 0xeb09  32
Network  10.0.1.21     10.0.0.3     0x80000005  1852  0x22 0x995a  32
Network  10.0.1.26     10.0.0.2     0x80000004  1289  0x22 0x658c  32
Network  10.0.1.29     10.0.0.4     0x80000004  1292  0x22 0x5d8c  32
    OSPF AS SCOPE link state database
 Type       ID            Adv Rtr       Seq      Age  Opt  Cksum  Len
Extern   0.0.0.0       10.0.0.2     0x80000001   716  0x22 0x9a0c  36
Because redundant gateways have been configured, the link-state
        database on r5 should contain two
        Type External entries for the default route to the Internet. One would
        come from r1 and the other from
        r2. Router r5 contains only the link-state advertisement (LSA) from r2. While we do have functionality, at this
        point we are lacking the type of redundancy necessary for a high
        availability network. It is time to take a closer look at the r1 OSPF configuration.
In this case, the default route to the Internet is a static
        route redistributed into OSPF using policy. The policy configuration is still in place on r1:

[edit]
lab@r1# show policy-options
policy-statement stat {
    term 1 {
        from {
            protocol static;
            route-filter 0.0.0.0/0 exact;
        }
        then accept;
    }
}
However, the export statement
        for redistribution is not in the OSPF configuration and
        needs to be added:

[edit]
lab@r1# show protocols ospf
area 0.0.0.0 {
    interface ge-0/1/1.0;
    interface ge-0/2/0.0 {
        passive;
    }
    interface ge-0/0/0.0;
    interface ge-0/0/1.0 {
        passive;
    }
    interface lo0.0 {
        passive;
    }
    interface ge-0/1/0.0 {
        passive;
    }
}

[edit]
lab@r1# set protocols ospf export stat

[edit]
lab@r1# edit protocols ospf

[edit protocols ospf]
lab@r1# show
export stat;
area 0.0.0.0 {
    interface ge-0/1/1.0;
    interface ge-0/2/0.0 {
        passive;
    }
    interface ge-0/0/0.0;
    interface ge-0/0/1.0 {
        passive;
    }
    interface lo0.0 {
        passive;
    }
    interface ge-0/1/0.0 {
        passive;
    }
}

[edit protocols ospf]
lab@r1# commit
commit complete
All routers shown in Figure 8-1
        should now show redundant gateway routes, one generated by r1 and the second generated by r2. A quick check of the OSPF database on a
        few routers confirms this is true. Router r1 looks good at this point:

[edit]
lab@r1# run show ospf database logical-router r1 | match 0.0.0.0
    OSPF link state database, Area 0.0.0.0
Extern  *0.0.0.0       10.0.0.1     0x80000001   651  0x22 0xa007  36
Extern   0.0.0.0       10.0.0.2     0x80000002   226  0x22 0x980d  36
Router r5 now has the LSA as
        well:

[edit]
lab@r5# run show ospf database | match 0.0.0.0
    OSPF link state database, Area 0.0.0.0
Extern   0.0.0.0       10.0.0.1      0x80000001   682  0x22 0xa007  36
Extern   0.0.0.0       10.0.0.2      0x80000002   255  0x22 0x980d  36
Router r3 also looks
        good:

[edit]
lab@r3# run show ospf database | match 0.0.0.0
    OSPF link state database, Area 0.0.0.0
Extern   0.0.0.0       10.0.0.1      0x80000001   690  0x22 0xa007  36
Extern   0.0.0.0       10.0.0.2      0x80000002   265  0x22 0x980d  36

Note
Here's an additional step for the big guys: in most enterprise
          networks, bringing up the IGP and verifying the default gateways
          leads directly to verifying functionality of services in the
          network; however, in carrier networks and some large enterprises, it
          is also necessary to confirm Border Gateway Protocol (BGP)
          functionality.

As with all verification procedures in this chapter, to accurately
        verify proper BGP routing, you need to have a good understanding of
        what is normal for the network. This requires significant preparation
        and data collection before the JUNOS upgrade begins. Without this
        data, you could end up spending a lot of time troubleshooting
        nonproblems while genuine problems are ignored... and Rome burns.






State of Existing Services



While upgrades are often performed to allow the addition of new services
      to a network, care must be taken not to damage existing services in the
      process. After all, networks do not exist just to give the
      mathematically savvy something to do at night. They exist to provide a
      service to customers. It is in fact services that justify the existence
      of networks. Keeping the issue of high availability front and center,
      post-upgrade verifications should extend into the services provided by
      the network.
Once routing and peering information is confirmed accurate, it is
      time to check the applications running in the production network.
      Generally speaking, the applications tend to be fairly immune to changes
      in the plumbing that carries their packets. However, sometimes there are
      major problems.


Change in DHCP-Relay Functionality
During a routine JUNOS upgrade a few years ago, a colleague
        discovered an undocumented change in functionality of the
        DHCP-relay feature in a Juniper product. In earlier releases of
        the operating system, DHCP-relay set a very high Time to Live (TTL) on Dynamic Host Configuration
        Protocol (DHCP) requests forwarded from host systems. Newer versions of
        the operating system set the TTL for a relayed request to 16, which
        dramatically reduced the distance DHCP requests could be
        pushed.
Because JUNOS was upgraded during a late-night maintenance
        window, very few users were on the network and the issue was not
        discovered until the next day. After much wringing of hands and
        gnashing of teeth, the root cause of the problem was discovered and an
        emergency fallback to the earlier JUNOS version was launched that
        morning.
The lesson of the story is to always
        confirm functionality all the way through the application layer.
        Although individual procedures are dictated by services deployed and
        range far too wide to try to list in this chapter, a good rule of
        thumb is to include testing of existing services and baseline
        functionality in the first half of the maintenance window allotted for
        the upgrade.






Filesystems and Logs



By default, the JUNOS operating system stores event information pertinent to JUNOS upgrade
      attempts. Install logs capture information about
      JUNOS upgrade successes and failures, and are extremely useful in
      troubleshooting.




Install logfiles



Our first example is from a successful install on a device with
        multiple logical routers. The output generated by running this command
        on R105 is what you would want to see after an install:

lab@R105> show log install
2009-01-16 16:51:13 UTC mgd[4537]: /usr/libexec/ui/package
-X update /var/tmp/jbundle-9.0R4.5-domestic.tgz
<output>
Installing package '/var/tmp/jbundle-9.0R4.5-domestic.tgz' ...
...
rpdc: invalid daemon: profilerd
Restarting rmopd ...
Restarting spd ...
Restarting kmd ...
Saving package file in /var/sw/pkg/jbundle-9.0R4.5.tgz ...
Saving state for rollback ...
</output>
<package-result>0</package-result>
If a problem is encountered during the upgrade attempt, the
        install log indicates it with warnings and error messages. The command
        output from R106 indicates the install was aborted because of an
        incompatibility between configuration elements on the router and the
        version of JUNOS the administrator is attempting to load:

lab@R106> show log install.0.gz
2008-05-20 16:20:16 UTC mgd[5098]: /usr/libexec/ui/package
-X update -reboot /var/tmp/junos-jseries-8.2R4.5-domestic.tgz
<output>
NOTICE: Validating configuration against junos-jseries-8.2R4.5-domestic.tgz.
NOTICE: Use the 'no-validate' option to skip this if desired.
Checking compatibility with configuration
Initializing...
Verified manifest signed by PackageProduction_8_5_0
Using /var/tmp/junos-jseries-8.2R4.5-domestic.tgz
Checking junos requirements on /
Available space: 118262 require: 43134
mv: rename /cf/sbin/preinit to /cf/sbin/preinit.bak: No such file or directory
Verified manifest signed by PackageProduction_8_2_0
Hardware Database regeneration succeeded
Validating against /config/juniper.conf.gz
mgd: error: Check-out pass for Routing protocols process (
/usr/sbin/rpd) dumped core (0x86)
&lt;xnm:error xmlns="http://xml.juniper.net/xnm/1.1/xnm"
xmlns:xnm="http://xml.juniper.net/xnm/1.1/xnm"&gt;
&lt;source-daemon&gt;none&lt;/source-daemon&gt;
&lt;message&gt;Opening configuration database:
Could not open configuration database&lt;/message&gt;
&lt;/xnm:error&gt;
mgd: error: configuration check-out failed
Validation failed
WARNING: Current configuration not compatible with
/var/tmp/junos-jseries-8.2R4.5-domestic.tgz
</output>
<package-result>1</package-result>





Messages file



The messages file captures, among other things, protocol process and
        daemon restarts as well as changes in interface state and protocol
        peering state following an upgrade. This information can be useful
        when troubleshooting interfaces and neighbor relationships after an
        upgrade. The messages file also contains all
        event information for the boot process that must run after a software
        upgrade. This information can be used in conjunction with the output
        from the install log when troubleshooting or simply to confirm there
        are no upgrade-related issues.
The following is a sample of messages file
        output from a system boot event following a software upgrade. The
        copyright keyword is used for the
        find operation because it is part of the first message in a boot
        sequence:

lab@r105> show log messages | find copyright
Dec  8 13:27:21  r5-main /kernel: Copyright (c) 1996-2008,
Juniper Networks, Inc.
Dec  8 13:27:21  r5-main /kernel: All rights reserved.
Dec  8 13:27:21  r5-main /kernel: Copyright (c) 1992-2006 The FreeBSD Project.
Dec  8 13:27:21  r5-main /kernel: Copyright (c) 1979, 1980,
1983, 1986, 1988, 1989, 1991, 1992, 1993, 1994
Dec  8 13:27:21  r5-main /kernel:
The Regents of the University  of California. All rights reserved.
Dec  8 13:27:21  r5-main /kernel: JUNOS 9.0R4.5 #0: 2008-11-18 18:59:52 UTC
Dec  8 13:27:21  r5-main /kernel:     builder@nidhogg.juniper.net:
/volume/build/junos/9.0/release/9.0R4.5/obj-i386/sys/compile/JUNIPER
Dec  8 13:27:21  r5-main /kernel:
Timecounter "i8254" frequency 1193182 Hz quality 0
Dec  8 13:27:21  r5-main /kernel:
CPU: AMD Athlon(tm) Processor (908.09-MHz 686-class CPU)
Dec  8 13:27:21  r5-main /kernel:
Origin = "AuthenticAMD"  Id = 0x642  Stepping = 2
Dec  8 13:27:21  r5-main /kernel:
Features=0x183f9ff<FPU,VME,DE,PSE,TSC,MSR,
PAE,MCE,CX8,SEP,MTRR,PGE,MCA,CMOV,PAT,PSE36,MMX,FXSR>
Dec  8 13:27:21  r5-main /kernel:   AMD
Features=0xc0440800<SYSCALL,<b18>,MMX+,3DNow+,3DN

Note
The previous command example showed creative use of the
          | operator in conjunction with an
          awareness of the content of the logfile. Similar information could
          be gathered by using the show system
          boot-messages command.






Syslog settings



In networks with high availability requirements, system logs are often stored on the local chassis and sent to a
        remote syslog server. The collection of syslog information from
        devices around the network allows you to correlate network events.
        This speeds up root-cause identification and issue resolution. Chapter 9 describes Network Time
        Protocol (NTP) functionality and shows how syslog analysis can benefit
        from having accurate timestamps on log entries.






Removal of Configuration Workarounds



At this point, all devices and services have been confirmed
      functional.
In many cases, the easiest way to remove configuration workarounds
      is to copy the saved configuration file, the one created before starting
      the JUNOS upgrade procedure, back to the router and load it into the
      candidate configuration file. If the file is stored on the chassis being
      upgraded, copying the file is unnecessary and you can simply load the
      configuration into the candidate file. Before launching a commit, it is critical to compare the
      candidate configuration with the active configuration to make sure the
      changes are desirable:

[edit]
lab@r1# load override pre-upgrade-r1
load complete

[edit]
lab@r1# show|compare
[edit]
-  chassis {
-      redundancy {
-          routing-engine 0 backup;
-          routing-engine 1 master;
-      }
-  }

[edit]
lab@r1# commit
commit complete














Fallback Procedures



The fallback process is specific to components, topology, and services
    enabled in the network, but all fallback procedures should include some
    flavor of the following elements:



The fallback procedure should be documented and readily
        available before beginning the upgrade.


The fallback procedure should include an estimated time duration
        needed to perform the fallback. General practice is to use only half
        of a maintenance window for a major change (such as a JUNOS upgrade).
        The other half of the maintenance window should be reserved for
        fallback time and to restore devices, connectivity, and services to
        the premaintenance window state. (As the old saying goes, "How far can
        a man walk into a forest? Halfway. If he goes further than halfway, he
        is in fact walking out of the forest.")


The fallback procedure should identify one person, by name or
        title, responsible for initiating the fallback procedure. This one
        person will be responsible for deciding when things have Gone Horribly
        Wrong. If this occurs, the fallback procedure must be
        triggered.


Because there are many degrees of functionality between Horribly
        Wrong and completely successful, one person should be responsible for
        determining whether any partial functionality achieved is an
        acceptable state.


Because there are occasionally unintended consequences or side
        effects of reaching full functionality, one person must determine
        whether any of them are a threat and merit a fallback.
















Applicability



The recommendations outlined in this chapter and in Chapter 6 are by no means specific to
    software upgrades. Any network with high availability requirements should
    follow a similar planning, execution, and verification pattern for every
    network change. As we mentioned earlier in the chapter, following a
    network change, begin at the Physical layer on the device that has been
    upgraded and expand upward through the OSI model and outward through the
    rest of the network.













Chapter 9. Monitoring for High Availability



High availability is not something that is stumbled upon accidentally.
  Planning, deploying, and maintaining a network requires a staff of
  hard-working individuals who actually know what they are doing. The tasks
  required to maintain high availability in your network do not end with your
  configuration of Virtual Router Redundancy Protocol (VRRP), Graceful Restart
  (GR), or routing engine (RE) failover, but encompass the ability of your
  personnel to monitor the network and head off any errors that could degrade
  service.
Through the use of logging, Simple Network Management Protocol (SNMP),
  and traffic flow monitoring, the resourceful network engineer is able to
  diagnose issues that could lead to network downtime, and plan for a solution
  to the issue.













I Love Logs



This was actually a bumper sticker that came as swag from a logging
    software company that sat on a coworker's desk for more than a yearâ€”no one
    really did love logs enough to put it on his car. Logging is one of those
    necessary evils that exist in a network. Monitoring logs is essential to
    maintaining high availability in the network, but without proper planning
    and design of a logging system, it can quickly become a bane of those
    running the network.
In looking at developing a logging posture that enables increased
    high availability in your network, it is important to understand the
    basics of logging and how a little planning can make logging not only more
    efficient, but also far more effective in achieving your goals of high
    availability.




Syslog Overview



From its start in the early 1980s to its eventual formalization in 2001
      to the current day, syslog has been the standard on which many network
      monitoring solutions are built. The flexibility and ubiquity of the
      protocol have allowed networking device makers and operating system
      manufacturers to create frameworks with which to transmit the minutest
      events occurring on their systems.


In its most simplistic terms, the syslog protocol provides a
        transport to allow a machine to send event notification messages
        across IP networks to event message collectors, also known as syslog
        servers. Since each process, application, and operating system was
        written somewhat independently, there is little uniformity to the
        content of syslog messages. For this reason, no assumption is made
        upon the formatting or contents of the messages. The protocol is
        simply designed to transport these event messages.

â€”RFC 3164, August 2001



With equipment makers relying on the use of syslog in the network to provide information to network
      engineers, sometimes syslog collection systems either cannot handle the
      load of messages, or can handle the load but cannot provide useful
      information. If your network is anything like most networks, it is
      possible for the equipment to create thousands upon thousands of
      messages per second. Those in charge of monitoring these networks with
      the thousands of messages find it necessary to increase the speed of
      their syslog servers and the size of the storage devices where the
