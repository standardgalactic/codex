











HOW TO SPEAK TECH
THE NON-TECHIE'S GUIDE TO TECHNOLOGY BASICS IN BUSINESS
_____________
Vinay Trivedi








How to Speak Tech: The Non-Techie's Guide to Technology Basics in Business
Copyright © 2014 by Vinay Trivedi
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection with reviews or scholarly analysis or material supplied specifically for the purpose of being entered and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of this publication or parts thereof is permitted only under the provisions of the Copyright Law of the Publisher's location, in its current version, and permission for use must always be obtained from Springer. Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law.
ISBN-13 (pbk): 978-1-4302-6610-5
ISBN-13 (electronic): 978-1-4302-6611-2
Trademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every occurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.
While the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.
President and Publisher: Paul Manning
Acquisitions Editor: Robert Hutchinson
Editorial Board: Steve Anglin, Mark Beckner, Ewan Buckingham, Gary Cornell, Louise Corrigan, James DeWolf, Jonathan Gennick, Jonathan Hassell, Robert Hutchinson, Michelle Lowman, James Markham, Matthew Moodie, Jeff Olson, Jeffrey Pepper, Douglas Pundick, Ben Renow-Clarke, Dominic Shakeshaft, Gwenan Spearing, Matt Wade, Steve Weiss, Tom Welsh
Coordinating Editor: Rita Fernando
Copy Editor: Laura Poole, Judy Ann Levine
Compositor: SPi Global
Indexer: SPi Global
Cover Designer: Anna Ishchenko
Distributed to the book trade worldwide by Springer Science+Business Media New York, 233 Spring Street, 6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.
For information on translations, please e-mail rights@apress.com, or visit www.apress.com.
Apress and friends of ED books may be purchased in bulk for academic, corporate, or promotional use. eBook versions and licenses are also available for most titles. For more information, reference our Special Bulk Sales-eBook Licensing web page at www.apress.com/bulk-sales.
Any source code or other supplementary materials referenced by the author in this text is available to readers at www.apress.com. For detailed information about how to locate your book's source code, go to www.apress.com/source-code/.







Apress Business: The Unbiased Source of Business Information
Apress business books provide essential information and practical advice, each written for practitioners by recognized experts. Busy managers and professionals in all areas of the business world—and at all levels of technical sophistication—look to our books for the actionable ideas and tools they need to solve problems, update and enhance their professional skills, make their work lives easier, and capitalize on opportunity.
Whatever the topic on the business spectrum—entrepreneurship, finance, sales, marketing, management, regulation, information technology, among others—Apress has been praised for providing the objective information and unbiased advice you need to excel in your daily work life. Our authors have no axes to grind; they understand they have one job only—to deliver up-to-date, accurate information simply, concisely, and with deep insight that addresses the real needs of our readers.
It is increasingly hard to find information—whether in the news media, on the Internet, and now all too often in books—that is even-handed and has your best interests at heart. We therefore hope that you enjoy this book, which has been carefully crafted to meet our standards of quality and unbiased coverage.
We are always interested in your feedback or ideas for new titles. Perhaps you'd even like to write a book yourself. Whatever the case, reach out to us at editorial@apress.com and an editor will respond swiftly. Incidentally, at the back of this book, you will find a list of useful related titles. Please visit us at www.apress.com to sign up for newsletters and discounts on future purchases.
The Apress Business Team







To my loving family and friends, who selflessly support me, tolerate my idiosyncrasies, and never say no, I am forever thankful.







Contents
About the Author
Acknowledgments
Preface
Chapter 1: The Internet
Chapter 2: Hosting and the Cloud
Chapter 3: The Back End: Programming Languages
Chapter 4: The Front End: Presentation
Chapter 5: Databases: The Model
Chapter 6: Leveraging Existing Code: APIs, Libraries, Web Services, and Open-Source Projects
Chapter 7: Software Development: Working in Teams
Chapter 8: Software Development: The Process
Chapter 9: Software Development: Debugging and Testing
Chapter 10: Promoting and Tracking: Attract and Understand Your Users
Chapter 11: Performance and Scalability
Chapter 12: Security Threats: To Defend and Protect
Conclusion
Index







About the Author

Vinay Trivedi works in private equity at The Blackstone Group and previously in venture capital at Romulus Capital. He worked in product marketing and analytics at Locu, a venture-backed technology startup acquired by GoDaddy.
Trivedi served as a team member on the TED fellow-led Future of the Book education technology challenge. He founded a national non-profit organization called SeniorLink, a youth volunteer agency for teaching seniors about computers and the Internet. He is involved in The Blackstone Charitable Foundation's Entrepreneurship Initiative to help strengthen entrepreneurial ecosystems around the US, consults and volunteers at youth mentor programs, and helps promote STEM education initiatives and public policy in New York. Trivedi earned the Congressional Award Gold Medal in Washington, DC for his work in technology education and community service.
Trivedi received his A.B. (Honors) in Computer Science from Harvard University, where he graduated Phi Beta Kappa and as a John Harvard Scholar, Weissman Scholar, and Detur Book Prize Winner.







Acknowledgments

An investment in knowledge pays the best interest.

—Ben Franklin
This book could not exist without my parents and grandparents who instilled in me the value of education and community. They raised me with the beliefs that knowledge and hard work are the key to a more independent and fulfilling life, and that because we are all products of our environments, we must do what we can to give back to others. This advice awoke my passion for sharing my learning experiences and led to my starting and managing SeniorLink, writing this book, and dedicating myself to contributing to tech education. An especially loud thank you goes to Jesal, Priyanka, Deepak, and Aditi. The hours you selflessly gave up to help me edit descriptions, brainstorm designs, and contact partners is something no one else would or could have provided. Everyone's endless support and unconditional love are the real bibliography of this book.
I want to thank Dr. Henry Leitner, Professor Jim Waldo, and Paul Bottino for their guidance throughout this process. It was long and challenging, but their reassurance and mentorship was instrumental in this effort. Carol, you tirelessly helped me promote the book and its mission; this is yet another sign of the generosity and thoughtfulness with which I have been blessed. The section "Version Control Terminology" in Chapter 7 closely follows the exposition of Kalid Azad in "A Visual Guide to Version Control" on his wonderful website BetterExplained.com, and I acknowledge his courtesy in allowing me to reproduce his figures in that section.
Last, and certainly not least, Robert, Rita, and the entire team at Apress/Springer, these words and this book would not have made it this far without your expertise. Your experience is deep, your recommendations were always on point, and you pushed us to meet deadlines and make what once was only a passing idea into reality. You helped me create an expression of my passion for innovation, technology, and education. Your contribution is invaluable.







Preface
On January 5, 2012, Michael Bloomberg, then the mayor of New York City, tweeted: "My New Year's resolution is to learn to code with Codecademy in 2012! Join me. http://codeyear.com/#codeyear." Codecademy, a startup that offers coding lessons to millions of users, is doing a remarkable job of making useful knowledge available for free. But why is learning how to code, of all things, among Bloomberg's top personal priorities? 
It is probably not the case that Mr. Bloomberg aspires to an entry-level job at Google. His tweet reflects his sense of the new credentials for informed citizenship and economic participation in a society transformed by the Internet and technology in general. These ubiquitous forces have revolutionized how we conduct business, communicate with each other, organize our lives, order our food, plan our weddings, file our taxes, purchase our wardrobes, and much more. Technology is no longer important only to tech companies; it is shaping and disrupting every industry. No longer do just technologists require tech knowledge. It is becoming more and more evident that computers, the Internet, and coding need be incorporated into mandatory instruction at schools alongside reading and math.
Many aspiring entrepreneurs have consulted me for advice about startups. Because they have little technical knowledge of their own, they are dependent on the knowledge and work of others to achieve their ambitions. Many of the questions I get reveal unrealistic expectations. It is neither practical nor logical that everyone learn to code sophisticated applications. But I began to realize that both people who work with tech and those who live in today's world being continuously reshaped by tech should understand, at least a high level, how an Internet app is built and how it works.
Most businesses, organizations, policy makers, and economists understand that innovation is closely linked to technology. Innovation is highly dependent on investment, well-funded research, and an educated population. The US government has launched several initiatives in partnership with US companies to jumpstart science, technology, engineering, and math (STEM) programs in recognition of the fact that no country can maintain prominence in the world unless it leads in STEM.
The purpose of this book is not to teach you how to code, but to outline the major concepts and technologies related to each phase of a web application's life cycle. Whether you are an aspiring Internet tech entrepreneur, a judicious venture capital investor, or a seasoned corporate executive looking to revive your company with a hot new Internet product for your existing clients, you will need to manage some part of launching your application. Even if you have no intention of launching an Internet company yourself, anyone living in a world in which "google" and "friend" are dictionary verbs should read this book. To too many people, the Internet and its apps are a black box, but there is no reason anyone should think that pinning a pair of sneakers on Pinterest is magical. As long as you understand a few key concepts, you can plumb the mystery.
As in any line of specialized work, techies deploy technical vocabularies largely incomprehensible to non-techie colleagues and outsiders. As a result, techies remain in short demand and non-techie people remain non-technical. This dynamic is perversely inefficient in an age of constant information overflow. If techies made a more conscious effort to demystify their jargon and if employers encouraged their non-techie employees to become more digitally literate, we could have a more informed population, better managers, more efficient processes, smarter investors, more discoveries, and a faster rate of innovation. That's exciting!
In this book I have assembled what I believe everyone should know about the workings of an Internet application. From the back end and the front end to working with teammates, promoting the application, and securing it, I see the list of topics included in this book as topics that you are likely to come across in the tech world and that are always likely to be true of Internet applications. Though technology evolves rapidly, applications will usually have the same few components with the same types of challenges. The tools we use might change overtime, but this book focuses on the basic concepts, not on specific technologies.
The material is presented in a way for anyone to digest. With a mix of stories, analogies, and de-jargonized explanations, I strived for accessibility. Though the material comes from academic papers, news articles, textbooks, and interviews, the tone of the book is one that should feel like a friend explaining these concepts to you in a casual, albeit slightly nerdy, conversation. There is always a tension between simplification and thoroughness. I strove to preserve the integrity of the information while shaving away non-essentials.
The book is connected through a narrative starring you as the main character. You are building an application called MyAppoly. This application can exist in pretty much any context. If you are an entrepreneur, MyAppoly is your ticket to a $1 billion exit (hello Instagram). If you are a non-profit executive, MyAppoly helps you fundraise and network your volunteers. If you work at a Fortune 500 company, MyAppoly can help you stay modern and relevant. MyAppoly is anything on the web you want it to be.
Reading this book will help you meet the New Year's resolution urged by Mr. Bloomberg. Everybody knows that our world is quickly moving ahead and is becoming more and more technical. Each new discovery is challenging us all to learn new things and question what we knew to be true and what we thought was the limit. You may not have the time to learn how to code, but you should at least understand the basics of how entrepreneurs and institutions build and maintain products. Maybe it'll make you a better manager if you can relate to your developers' world. Maybe you'll be a better investor with insight to help you differentiate between the realistic and the impossible. Maybe you'll even land your dream job at that tech company you recently discovered. At the very least, you should be able to understand tech news without having to consult an expert. It is time for technology to be democratized. It is time for you to learn how to speak tech.







CHAPTER1
The Internet
The progenitor of the Internet was the Advanced Research Projects Agency Network (ARPANET), which was funded by the US Department of Defense to enable computers at universities and research laboratories to share information. ARPANET was first deployed in 1969, fully operationalized in 1975, and superseded by NSFNET in 1990. ARPANET's packet switching and TCP/IP communication protocols formed the backbone of what became the global Internet.
How did they do it? Consider the following analogy. Local towns tend to be well connected by roads. To get from one town to the next, major roads are needed. To connect different states together, large highways were eventually built. All a local town needed to do to be a part of the larger national network of roads was to make a single road linking the town to the highway. If they were able to do that, all of the town's residents could travel across the country by car. Similarly, using communication lines, satellites, and radio transmission for communication, ARPANET connected all the local area networks (LANs) and wide area networks (WANs) together in a central highway, called the WAN backbone.
To get this setup to work, the Advanced Research Projects Agency (ARPA) created two pieces of software that defined how information would travel on this highway. The two pieces are the Internet Protocol (IP) and the Transmission Control Protocol (TCP). In the same way that highways have exit signs and speed limits, there need to be rules to guide information flow on the Internet.
These two components of the Internet's software layer operate on top of the hardware layer. Conceptually, the Internet is a network of networks—but how are these networks connected? To create a primitive local network, one can connect a few computers together with a wire, a telephone system, or even a fiber optic cable (which uses light instead of electricity to carry data). Imagine several local networks. To link them together, you need to use a "connector" computer called a router. By using routers, you can gradually expand the size of the subnetwork. As all of these local networks come together, you need one connection to the backbone, some cable that has several local networks attached to it. Eventually, all of these subnetworks can connect to a backbone that serves as the central communication line, and thus we have the Internet.
Given that the Internet is essentially a physical connection of computer clusters, how does it actually function as a smoothly performing single-communication-line network? The answer to this question is where the real technical innovation of the Internet lies.
Packet Switching, IP, and TCP
Keep in mind that the whole reason researchers began investigating networks is that they needed a way to share information among computers. Historically, when two computers were connected with a cable, communication was often one-way and exclusive. If computer A sent something to computer B, data moved along the wire, and if any other attempt at communication was made, the data would collide and no one would get what they wanted. To create a network whereby millions of people could all interact simultaneously and without delay, the technology of the time would have required a prodigious mess of expensive and unscalable cable. The only way to reduce the need for wires was to develop a method for data to travel from multiple sources to multiple destinations on the same communication line.
This was accomplished by applying the concept of packet switching. Researchers discovered that they could break any information (such as text documents, music files, and image files) into small "packets"; doing this enabled a single wire to carry mixed information. Two challenges seem obvious. How do all the packets make it to the right destination and, on their arrival, how does the recipient put them back together?
Every page you see on the Internet is a document that somebody wrote, and it lives on a computer somewhere. When you browse with Google, you are essentially asking for permission to look at some document that is sitting on a computer. For you to view it, Google needs to send it to you—and TCP/IP helps do just that. TCP breaks the document into little packets and assigns each one a few key tags.
First, TCP numbers the packets so they can be reassembled. Next, it gives each packet an "error check" value (called checksum) that is used to assess whether the arriving data were altered in any way. Last, the packet is given its destination and origin addresses, so it can be routed properly. Google Maps can't help you here, but IP can.
IP defines a unique address for every device on the Internet. You may have noticed this number. It follows the general structure of four groups of numbers called octets, wherein the value of each octet is between 0 and 255.

 Note  The octet's numerical interval—0 to 255—is based on 32-bit addressing. Such facts are fun for hard-core techies but outside the scope of this book. I point this out because it has an interesting corollary. This 32-bit addressing method limits the number of mathematically possible IP addresses to about 4 billion, a number that will not fit the world's projected needs. A shift will therefore be undertaken to a newer IP system that uses 128-bit addressing. I suppose the original architects didn't envision that 4 billion people (and counting) would use their technology.
So these packets leaving Google have your computer's IP address on them. IP helps route these packages to their destination and does so in a process much like the US Postal Service's method for delivering mail. The packets are routed from their start to the next closest router in the direction of the destination. At each step, they are sorted and pushed off to the next closest router. This process continues until they arrive at their destination. In this way, IP software allows an interconnected set of networks and routers to function as a single network.
One prized characteristic of IP is network stability. If a particular segment of the overall network goes down, packets can be redirected through another router. At last, when the packets reach you, TCP verifies that all packets have arrived before reassembling them for viewing.
Several computer companies actually developed independent solutions to the problem of computer-to-computer communication, but they charged people to use them and they weren't mutually compatible. Why buy company A's solution if it did not connect you to people who bought company B's solution? What distinguished ARPA was that it made available all of its research results and made TCP/IP free. When the US military branches adopted TCP/IP in 1982, ARPA's open and free approach was legitimized and institutionalized. Ever since, computers can use the Internet only if they have TCP/IP software installed.
Now there was a way to transfer data around the Internet, but there still needed to be a way to display and view the data received from other computers. In 1990, Tim Berners-Lee and others at the European Organization for Nuclear Research (CERN) developed the precursors of HyperText Markup Language (HTML) and HyperText Transfer Protocol (HTTP), which jointly enable the exchange of information online. After 1991, the first graphical browsers—programs that allow you to view web pages on the Internet—were released. This created an attractive and efficient way to share and consume information on the web.
Around this time, the ban restricting the Internet to research and governmental purposes was lifted, the cost of personal computers dropped, and online service providers such as AOL emerged, offering cheap access to the Internet. This confluence of factors led to the Internet's rapid growth.
No one person or company runs the Internet. The Internet Architecture Board (IAB), an open community of researchers, is responsible for the overall development of the Internet, and a subgroup called the Internet Engineering Task Force (IETF) is in charge of technical matters.
HTTP and Using the Internet
How data travel physically is pretty straightforward—but how do you tell someone to send the data in the first place? What is actually happening behind the scenes when somebody—let's say you—visits your brainchild, MyAppoly?

 MyAppoly  In case you skipped the preface, you should be aware that this book is structured as a loose narrative starring you as the main character. The premise is that you are building a web application called MyAppoly. The name is just a catch-all; any resemblance to any real application is purely coincidental. I encourage you to imagine MyAppoly in any context that catches your fancy. If you are a killer app entrepreneur or angel investor, MyAppoly will be your ticket to a $1 billion exit strategy. If you are a nonprofit executive, MyAppoly will help you raise funds and connect your volunteers. If you work at a Fortune 500 firm, MyAppoly will help your company stay competitive and ahead of the curve of evolving consumer expectations.
You open your web browser and want to access a picture on the MyAppoly website. The physical web pages you visit are documents coded in HTML, most likely, and they are stored somewhere on a computer called the server—probably one owned or rented by your company. The server hosts the website online (Chapter 2). All of the files of your application live on the server. If your site has pictures or videos, they are stored on the server and every such file is referred to as a resource.
You proceed to type your web address—the uniform resource locator (URL), www.MyAppoly.com—into the browser. Technically, you could have typed the specific IP address of the MyAppoly server because that is where the page lives, but who has the capacity to remember the IP address linked to every website? Do you remember the mailing address of every one of your friends? Unless you are a savant, probably not. Instead, you probably look up their addresses in an address book by searching their names. Similarly, the Domain Name System (DNS) maps domain names such as MyAppoly.com to their IP addresses.
You reach MyAppoly.com and click on a link to view the picture gallery on the website. (lgnore for the moment how you view the homepage; we'll get to that in a moment.) Remember, all of these pictures also live on the server. Let's say that they are all in a folder called "Pictures." You might notice the URL change to MyAppoly.com/Pictures.  If you click on the first picture, maybe you are taken to http://www.MyAppoly.com/Pictures/pic1.jpg. If you break the URL down into its component parts, we can see that the domain name indicates the proper server, and all of the stuff after the .com tells you where on the server the files are located (in tech-speak: the hierarchical location of the file). In other words, the URL is the text address of a resource. For a letter, the ZIP code gets you into the right town and neighborhood, but you need the street and number to find a particular house. The same is true of URLs.
Revealing exactly where on the server your files are located can be imprudent from a security perspective. Do you give out your house address to strangers? Probably not. Similarly, you probably don't want the URL to expose exactly where all the resources reside on the server because that might allow a hacker to exploit it in some way. Therefore, it is possible to have a URL show a fake folder—called, say, "Images"—rather than its actual location in "Pictures" (see Chapter 12).
How exactly do you receive the web pages you want to view on MyAppoly's site, such http://www.MyAppoly.com/Pictures/pic1.jpg? First, your browser accesses the DNS to obtain the IP address corresponding MyAppoly so it knows where your server is. Your browser does not physically travel to the MyAppoly server to fetch the picture, so it must send some sort of message over the Internet telling the server to send the file. An HTTP request does just that. HTTP is a set of rules for exchanging resources (text, images, audio, and so on) on the Internet.
Your request can be of two types: GET and POST. The GET method tells the server that you want to get files from the server. On receiving a GET request, the server retrieves the appropriate files and sends them back to your browser. The other request type is POST, which the browser uses if you are sending data to the server (for example, data to be stored in a database or a query word to search). Technically, both methods can serve either function, but they are slightly different in how data are actually sent over the Internet.  With the GET method, the information you send to the server is added to the URL. If you are searching for the phrase "mediterranean" on MyAppoly, for example, the GET request might make the URL look like www.MyAppoly.com/search?q= mediterranean. If the search term is sent via POST, the term would be within the HTTP message and not visible in the URL. On the surface, it seems your data are hidden, which is good, but the data can still be accessed in other ways, so we cannot assume it is completely secure. These are the two major types of requests your browser can make to the server; just 10 minutes on Facebook probably consists of hundreds of these requests. Because a user (known as a client) is accessing a server, the Internet is said to follow a client/server architecture.
So your browser has issued a request, which finds the MyAppoly server and tells it to GET the page containing the pic1 file (a web page). The server fetches the resource, and sends it back using TCP/IP. When the data are returned to the browser, it contains some key parts so the browser can rebuild the document for viewing. It consists of the HTTP header, which provides useful information about the data as well as the content itself. The browser can use this to display, or render, the resource. The process is not necessarily finished, however, because the browser may need to issue more requests. This is because the server can only send one resource at a time back to the browser, and several requests are needed to construct a web page. If you want to view the page that has the pic1 picture on it, you are asking for two resources: the HTML page and the pic1 image located on the page. Therefore, the browser needs at least two requests. If the page has an image on it, as in this example, or requires formatting that is defined in another file on the server (Chapter 4), additional HTTP requests are needed to retrieve those resources from the server.  Everything you see—from the web page and its style to the pictures and their captions—was fetched using these HTTP requests.
This process might seem inefficient. When you "refresh" a web page containing several pictures, your browser has to do a lot of work: "I need the page. Oh, I also need the first photo. And the second one. And the third one. By the way, could I also have the sheet that tells me how to style everything? Oh, I need the audio file, too, if your page plays the music in the background." This might take a long time and the page may not have changed since your last visit. Browsers are smart and can cache, or save, recently viewed web pages. In this way, they avoid superfluous interaction with the server. Caching assumes that none of the resources that make up a page has changed, but a simple HTTP request can confirm that fact. Only when the web page changes does the browser need to fire off its barrage of requests.
In accessing websites, users often activate cookies. Cookies are essentially things that websites can store on your computer that describe data you may have given the website. Let's say you visit MyAppoly, fill out a few forms, and view a few products. MyAppoly can store which products you viewed in a cookie. When you return, the site can access your cookie and customize the experience based on the information enclosed. On one hand, cookies are desirable because they help make websites relevant; on the other hand, they raise some privacy concerns. Do you want a website collecting information about you without your explicit consent? (More on cookies follows in Chapters 10 and 12.)
Conclusion
With your knowledge of some of the basic elements, operations, and tools of the Internet, you are probably eager to move on to the challenge of creating the next big thing—MyAppoly! But first stop to consider what exactly is involved in website hosting, which is the precondition for your application being on the Internet.







CHAPTER2
Hosting and the Cloud
Picture yourself walking in to your favorite clothing store. All of the shirts, shoes, and belts were manufactured outside of the retail store, in some factory. Now imagine if the store entirely forgot to stock its shelves. This sounds silly, but the store would close if it did not make its products available for purchase. The process of making a product available to the customer is called distribution when referring to traditional bricks-and-mortar businesses. A clothing company might distribute its products to boutique fashion stores as well as large outlet malls, for example. The Internet equivalent of distribution is hosting. Hosting is the process by which you make a website accessible to other users on the Internet.
It's almost entirely impossible to escape a conversation about the Internet without mentioning the world cloud. But what does "the cloud" refer to, and why has it become a ubiquitous term?
Hosting
As you learned in Chapter 1, when you visit a website, your browser sends a message to a server asking for a file. To recap the client-server interaction with a quick analogy: Think of yourself as the king who sends your messenger (the browser and the request) to fetch and interpret information from a certain Mr. Server, the only person who can provide the information you requested. How did Mr. Server have the information to begin with? The process by which a website is put on a server and made available on the Internet is called hosting. Hosts are computers that are connected to the Internet nonstop so that the website files you put on them are always accessible. Imagine if you put your files on a server that someone turned off every night. When trying to access your website, your nighttime visitors and customers would receive error messages. That means lost business and fewer Facebook likes—so an always-on high-speed Internet connection for the hosting server is a must. The only other thing these computers need is special web hosting software that I don't get into here. In a nutshell, that is pretty much what you need to host a website properly.
Chances are good that you don't want to host a website yourself. You certainly could—it would be free (electricity and Internet not included) and you would have complete flexibility over the computer. The downside is that you have to manage it, which requires significant technical knowledge.
Enough people in the world have realized that self-hosting is troublesome, so smart entrepreneurs started companies that provide hosting. They have optimized computers for storing websites, accessibility, and speed. They removed all unneeded things on a computer. Say goodbye to card games and paint programs. No more monitor or keyboard either. Hosting providers started accumulating racks of these hard-core computers and began renting space on these servers to people who wanted to host their websites.
Hosting providers quickly became popular. They allowed website developers to focus on the product and minimize worries about its distribution. Hosting providers typically offer a control panel for website developers to manage the site and allow files to be moved to these remote servers using File Transfer Protocol (FTP). FTP allows programs to exchange resources between computers connected to the Internet. If you host MyAppoly on a computer (server) that lives elsewhere, as is likely the case, FTP allows you to establish a connection and interact with the remote computer to move your website files onto it, even if it's located in a different state or country. This interaction is typically limited to moving files around (file management), but you don't need much more than that.
Hosting Considerations
It is safe to say that if you are serious about MyAppoly as a website, there is a 100 percent chance that you will need to host it. If you search for "web hosting" on the Internet, you will be inundated with plans that make the whole process seem a lot more difficult than it really is. When choosing a hosting provider, consider the following parameters:

Server type. Different servers have different web hosting software (such as Apache, OS X Server, and Windows Server), which does not matter for the average user and use case, but might be important for people with strong opinions and background in this area.
Disk space allowance. The size of your website and database will determine how much memory you need on the server.
Bandwidth. Every time someone visits your web page, the browser requests files from the server. The amount of data transferred from your server back to the user is known as bandwidth. The more users visiting your site or the more images and resources your web page has, the higher the bandwidth. Some hosting providers base a portion of pricing on bandwidth needs. This makes sense, because it costs more to serve 1 million customers than just one.
Reliability and uptime. Make sure the hosting provider actually keeps your website on the Internet and accessible as close to continuously (24 hours a day, 7 days a week, 365 days a year) as possible.
Price. Shop around for different options. The prices vary among the categories outlined next, but within a particular category prices may vary just a little. Have a realistic budget in mind that balances your requirements with available packages.
E-mail addresses. Hosting providers often provide e-mail addresses associated with your domain name (e.g., firstname@MyAppoly.com). This might come in handy and, depending on its purpose, can add to the overall legitimacy of your website.
The Different Types of Hosting
Hosting providers make their servers available for use in the following different ways.

Shared hosting. When multiple users share a single server, it's called shared hosting. The hosting provider essentially divides a server into a certain number of parts and rents each part out to a different user. Because you are sharing a server with several other users, shared hosting plans are typically the cheapest. They are also the most inflexible. If you need to update the version of a programming language or change another configuration on the shared server, you cannot. If you did, it might affect the other websites running on the same server. There is a trade-off between price and flexibility, but this solution typically works for simple websites, such as personal websites.
Dedicated server. As the name suggests, a dedicated server plan allows you to rent an entire server to do whatever you please. Clearly, this is going to be a much more expensive proposition, but you can customize the server in any way you want, and you can ensure that other people's problems do not become yours. This type of plan makes sense if you are hosting several websites, especially if they have significant traffic and substantial database use (user names, credit card information, what items are most popular, etc.).
Virtual dedicated server. A virtual dedicated server, a.k.a. virtual private server, is in some ways a combination of a shared hosting plan and a dedicated server plan. By means of server virtualization, hosting providers can offer server space that you can treat as a dedicated server but is really shared. You can customize it however you want, unlike in shared hosting plans, yet several customers can be linked to the same server. Computer space and processing power are not wasted, thus benefiting the hosting provider, and customers do not have to pay for an entire server if they don't need it.
Colocated hosting. In a colocated hosting plan, there is no renting going on. You actually own the server, but rather than managing it yourself, you give it to a hosting provider. What you pay for is bandwidth (Internet usage) and maintenance fees (for cooling and so on).
The Cloud
The National Oceanic and Atmospheric Administration defines a cloud as "a visible aggregate of minute water droplets or ice particles in the atmosphere above the Earth's surface." The cloud you hear about in terms of computing has to do with aggregate networks of computers and the delivery of tiny packages of information invisibly over the Internet. You may notice a nebulous similarity.
The cloud is best understood when you realize that it is not all that different from what came before it. The genesis of the cloud concept dates back to mainframe computers in the 1950s. They were massive in size, exorbitant in cost, and available only to the government and large organizations that could afford them. Rather than have people work directly on the computer, users would access the mainframe via a terminal. Users would use these dummy computers to gain access to the real computer, where all the data and programs lived. All information and functionality were centralized, in a sense, but as computers become cheaper, the idea of a personal computer began to take over. Why have terminals logging on to a big, old supercomputer when you can make personal computers that can store and process their own information? Thus, computation power shifted to individual devices.
With the advancement of network technology and data processing capabilities, however, the relegation of computational power to personal computers increasingly became a limiting factor, and the countervailing benefits of migrating offline services to the Internet became increasingly attractive. The cloud is in some ways a reversion to the older model of computing—a mainframe writ large, if you will. Cloud is not going back to a mainframe per se, but to distributed computers and an interface to access them. The cloud is a bunch of computers connected in a large network such that they remain decentralized but able to interact collaboratively and selectively via Internet exchange.
So when someone says, "I stored it in the cloud," or "I accessed it in the cloud," you have a fair idea of what they are talking about. Through the Internet, they accessed some service (such as Dropbox) and stored their files there. Rather than saving it on their hard drive or relying on offline services, the cloud leverages the Internet to make resources and services available. In addition to photos, music, and any other type of information you can imagine, the network of servers that forms the cloud also stores everything from simple websites to more complicated web software that can be accessed online (such as on Salesforce.com).
Cloud computing refers to the access and use of information and software on the cloud by computers and other digital devices such as smartphones and tablets. It encompasses the idea that you no longer have to save everything to your physical computer. Cloud computing means that you don't need to go to a store to buy software on a disc that you have to physically insert and install onto your machine. Instead, most of the companies offer the same solutions for you online. Just go to their website and check! If they don't, they are probably developing an online version of the software in an attempt to stay current.
According to the U.S. National Institute of Standards and Technology (NIST), the cloud computing model "is composed of five essential characteristics, three service models, and four deployment models."1
The five essential characteristics of cloud computing identified by NIST are the following:

On-demand self-service. What this means is that you, the client, should be able to get access to the cloud's resources and software whenever you want. If you want to log on to Salesforce.com, all you need is a computer with Internet connection and a valid username and password.
Broad network access. The cloud should be accessible across a broad range of client platforms (whatever devices you are using to access the resources and/or software—tablet, smartphone, laptop, etc.).
Resource pooling. Computers in the cloud work with each other to support multiple users and solve problems intractably large for single isolated computers.
Rapid elasticity. Because computers in the cloud can talk to each other over the network, if certain computers are overwhelmed with traffic, other computers can pick up the slack. The cloud allows capabilities to be scaled up and down rapidly.
Measured service. In the traditional software model, customers would buy a program, say, Quicken, for $50 and would be allowed to install and use the program. Whether you used it every day or wanted to just try it out, it would cost you the same. Now, because software can be delivered over the cloud, companies can measure a customer's usage in terms of storage, processing, bandwidth, and other metrics. Pricing based on usage is now possible, which leads to a more efficient market.
The three service models for cloud computing identified by NIST are the following:

Software as a Service (SaaS). Any application that a user can access through a browser, such as Dropbox, falls under SaaS. Also related are APIs, which are discussed in more detail in Chapter 6. Because the service is offered over the cloud and a copy of the application is not installed on every user's computer, we can say the cloud is based on a multitenant architecture.
Platform as a Service (PaaS). This term will be familiar to people who have been exposed to cloud platforms for the programming community, such as Heroku. These platforms provide environments for users to build their own software. Among other things, they provide web servers for you to build your own web applications.
Infrastructure as a Service (IaaS). The growth of the cloud has created an entire business out of cloud infrastructure. All of the servers that make up the cloud need to be managed and offered to people who want to host and store things on the cloud.
NIST identifies the following four deployment models for provisioning cloud infrastructures:

Private cloud. For exclusive use by a single organization comprising multiple consumers, such as business units.
Community cloud. For exclusive use by a specific community of consumers from organizations that have shared concerns, such as security and compliance requirements.
Public cloud. For open use by the general public. It may be owned, managed, and operated by and on the premises of a business, academic, or government organization, or some combination thereof.
Hybrid cloud. Composed of two or more distinct private, community, or public cloud infrastructures
Before the cloud, hosting was inefficient. If you were launching a new promotion, you might expect a lot of traffic and would accordingly need to purchase enough servers to serve your site during peak times. After the surge of visitors, you would be left with a lot of expensive servers and relatively low traffic. The cloud allows what used to be a fixed-cost operation to become variable. Because the cloud allows you to add and take away server capacity easily, you can pay for only what you use.
Amazon.com, which has built up data centers to power its own website, has made its additional servers and tools available to others through a service called Amazon Web Services (AWS). Amazon Elastic Cloud Compute (EC2) is part of AWS that allows you to remotely and flexibly add or remove servers according to your plan. Microsoft, IBM, and Google offer similar services.
Benefits of Cloud Computing
The benefits of cloud computing include the following.

Wide accessibility and independence. You can access the application or data on any device connected to the cloud, so you are no longer tied to your physical computer to get what you need.
Backup. Because your data are stored securely in the cloud, if you drop your laptop in the bathtub or lose your tablet in the subway, the data are safe and sound.
Prorated pricing. Pricing is metered or based on usage.
Lower hardware costs. A lot of the computation is taken care of by the computers in the cloud, so you do not necessarily need the fastest computer with the most up-to-date hardware. All you need is a computer with reasonable Internet speed. You also don't need to store all the information yourself, because it can all live on the cloud. Indeed, inexpensive laptops such as Chromebooks lack everything except the parts that make a computer reliable and only provide fast Internet access.
Better performance. Because the cloud reduces the need for you to install feature-redundant programs, your computer has more free memory to work faster at other things.
Fewer maintenance issues. Because software lives on the cloud, you are a lot less likely to have problems of installed software conflicting with other software or newer versions of the same software.
Always have the latest version. When you visit a website, the version of the application you will be accessing will always be the most up-to-date version.
Easier group collaboration. With everyone on the cloud, groups can collaborate more easily. Think Google Drive and document collaboration.
Increased computing power. The computation power of a single computer is limited. On the cloud, however, processing power can be spread across the entire network.
Disadvantages of the Cloud
If you are on cloud nine after reading the long list of cloud benefits, I am sorry to have to rain on your parade. The cloud is not perfect for the following reasons:

Internet dependency. If the Internet is down, you cannot access the data and software you might need. Many cloud applications allow you to download your data as a workaround, although some of them charge you to do so.
Security and privacy vulnerability. The cloud is becoming more and more secure, but companies are justifiably wary about having their data exist where they can't see it or own it. If your data exist with someone else, are you keeping your information private? What if the application is compromised? That means your data are open for someone (whether criminal hackers, government surveillance agencies, or unauthorized personnel) or even for everyone to see.  Despite increasingly sophisticated security defenses, vulnerability to malicious attacks and hacking thefts is a growing worry at all levels (Chapter 13).
Dependency on browser reliability. Problems with a browser's capabilities and reliability may significantly degrade the user experience. Fortunately, there are several major browsers to choose from, all of which are continually upgrading under competitive pressure.
Conclusion
One reason the Internet has grown so rapidly is because hosting has become so easy. Entrepreneurs can quickly make their creations available to their customers in a cost-efficient manner by utilizing the latest hosting technologies. Tech startups are an especially active and attractive business development space because large companies tend to be too tied to legacy systems to take full competitive advantage of cloud benefits.
Before you can host a website on the cloud, you need to build it. Whether you are an opportunistic entrepreneur aiming to modernize old software or you work at a large company hoping to stay current with new web technology services, it is essential for you to actually understand the parts of a web application. You will acquire this understanding by building MyAppoly, as you will start to do in the next chapter.
1 Peter Mell and Timothy Gance, "The NIST Definition of Cloud Computing." National Institute of Standards and Technology Special Publication 800-145, 2011. Available at http://csrc.nist.gov/publications/nistpubs/800-145/SP800-145.pdf.







CHAPTER3
The Back End: Programming Languages
You have spent the last couple of weeks brainstorming your idea, researching the opportunity, and refining the vision for MyAppoly. What's next? Many would-be products never make it past this question due to the technological barrier. Internet applications are programmed or coded in a programming language. The code defines how the application will run and respond, defining everything from what you see to what happens in the background. Without the ability to code, your team will find it difficult to create MyAppoly. Although many have worked to democratize programming with such educational services as Codeacademy, it remains an insurmountable obstacle to most. It's really a shame that so many worthwhile products die before a single line of code is written.
You will not let MyAppoly follow the same fate. Beyond recruiting a team and other nontechnical tasks of development, you understand that you have to choose a programming language in which to code the product. You may have heard of languages like PHP, Python, Ruby, and C, but what are they, and what are the major differences between them? What factors should you consider when selecting a language? Luckily, your trusty chief technology officer (CTO) has taken time to explain the basics of MyAppoly as you build and launch the startup.
First, it will probably be helpful to understand the actual function that these programming languages serve. Among other things, the programming language will be used to define what is referred to as the back end. The back end is responsible for processing all of a user's actions. For example, when you click "Add Friend" on Facebook, the back end of Facebook processes the click by interacting with a database that stores all of Facebook's users and other sources of information to deliver what you are expecting. In this example, the back end sends a message to your friend and then generates a pop-up box that says "Friend Request Sent." For Google, the back end might take your search query terms and pass them through its algorithms to deliver results. The back end is also called "server side" because the code defining the back end lives on the server. The user does not interact with the back end directly but through an interface that we call the front end. The front end refers to the view and range of interactions you can perform on a given page. The "Add Friend" button you clicked and your friend's profile page is the front end of Facebook. It's the part you see. We discuss the front end in more detail in Chapter 4.
The rate at which programming languages have developed is astonishing. For this reason, an understanding of the basic technical terminologies and classifications of programming languages will help you keep up with the speed of development. The language you ultimately pick will be based on your needs and your particular situation, and this chapter outlines some factors you might want to consider. Paul Graham states the point nicely: "If you have a choice of several languages, it is, all other things being equal, a mistake to program in anything but the most powerful one."1
The popularity rankings of programming languages fluctuate from year to year. For example, the top ten programming languages in December 2013 show the following ranking changes compared to December 2012:2





1. C
(same)


2. Java
(same)


3. Objective-C
(same)


4. C++
(same)


5. C#
(same)


6. PHP
(same)


7. Visual Basic
(same)


8. Python
(same)


9. Transact-SQL
(up from #21)


10. JavaScript
(up from #11)


Classifying Programming Languages by Level
A programming language is a "notational system intended primarily to facilitate human-machine interaction."3 Computers understand only binary—a system that uses ones and zeros. All files, programs, and data are made up of ones and zeros. Each one or zero is called a bit, and when read in a group of eight is called a byte. Computers interpret bytes depending on how a programmer chooses to label or specify them. For example, let's consider the byte 0100 0001. When you tell the computer to read it as a number, it is 65, but as a character, it is an A. When you put together a huge number of bytes together, you can form, for example, the online version of this book.
Programmers can manipulate bits and bytes through a computer's machine language, which is defined by its hardware design. But imagine coding MyAppoly in a language where you are changing ones and zeros. It would be extremely difficult, tedious, and costly. Thus, although machine code is closest to what the computer actually understands, programmers needed a way to define simple functions that can do the inefficient bit coding for them. These functions form what we call assembly language. A programmer can code in assembly language and use an assembler to convert the program into machine language for the computer to understand.
Programmers, trained to be creatures of efficiency, found assembly language to be tedious as well. Therefore, they created high-level languages that allow us to write in relatively normal terms how we want the computer to act. Through either translation or compilation (explained in the next section) high-level language code is converted to machine language directly or by way of assembly language.
High-level languages, such as PHP and Python, are called high-level because, for all practical purposes, they remove the programmer from thinking about specific bits and bytes. It follows that assembly languages are considered low-level because they are closer to the hardware. Some languages, such as C, are considered middle-level because they allow a programmer to think about bits but offer more functionality and relate more closely to natural language expression than do assembly languages.
Processing High-Level Languages
For coding MyAppoly, you will use a high-level programming language. High-level languages may be subdivided by various criteria. One useful taxonomy, based on how a language is read by computers, divides high-level languages into two subtypes, compiled and interpreted:4

Compiled languages are passed through a compiler that translates the code into a language that can be understood by the computer's processor. If you coded MyAppoly in C, you would have used a compiled language, and the program you code would be referred to as the source language. The compiler essentially translates the source language into machine-executable code, much as one might translate an English book into French. Compiled languages have the advantage of being very fast to run, but the disadvantage of being slow to develop (Chapter 11.)
Interpreted languages such as PHP are different from compiled languages inasmuch as they are not translated into machine instructions to be read by the computer. Instead, they are interpreted much as an English speaker might have a live interpreter translate continuously and simultaneously to someone who only speaks French. One disadvantage is that the execution is one to two orders of magnitude slower than in compiled systems. Compare the time it would take a French speaker to sight-read a French translation of this book with the time it would take the same French speaker to listen to this book being read aloud by a French interpreter translating live from the English original. Another less obvious disadvantage is that interpreted languages often use more memory and as a result can be less efficient. Although they are slower to run, interpreted languages tend to be much quicker to develop. This development speed advantage is a major consideration for you, because you want to get MyAppoly running on the Internet as fast as possible.
Other Taxonomies of High-Level Languages
Experience with coding makes it easier to understand the following descriptions. The main takeaway for noncoders is that different languages are built for different types of tasks. One language is suitable for building a complicated recursive trading algorithm for a financial investor, whereas another language is good for building a simple tool for a teacher to sort through a list of last names for grading.
Ideal taxonomies are sharply bounded and mutually exclusive: a language in one group would not share characteristics with a language in a different group. If I asked you to classify all the types of dogs in the world with a single system, you would find it a difficult task. There are different ways you could try, but inevitably there would be overlap between the various groups. As with Labradoodles and Cockapoos, overlap between languages is extensive, so that labels tend to blur arbitrarily at the edges. Nonetheless, the following taxonomies of high-level languages contain keywords you will most often hear in the tech world:

Imperative languages were originally defined around the ways computers were structured—that is, their architecture. The code that defines an application lives on a computer, and the data it collects also must live somewhere. In one such setup, called von Neumann computer architecture, the program code and the data are stored in the same memory area in the computer. As a result, these languages tend to be iterative (go back and forth between the code and the data) and are typically written as a sequence of commands that store the results of evaluation in variables. Examples include C and Java.
Functional languages rely little on stored variables or iteration, and instead use functions and recursion as their main method of computing. The theoretical underpinnings of functional programming lie in lambda calculus. Examples include CAML, Standard ML, Scheme, Lisp, and Haskell.
Object-oriented languages use the notion of an object as a data structure with related functions. For example, in an application that a student might use for school, a programmer might create something called an object to define a student. The object contains variables that define the student's name and advisor. The object also has a set of related functions, called methods, which describe how the computer should retrieve the student's grades or print out a list of the student's courses. Essentially, everything that might usefully describe the student is defined in this student object. Other important features of object-oriented programming are inheritance and classes. These terms refer to a concept known as abstraction, which allows a programmer to define shared attributes in one place. Let's unpack that concept. Suppose you now wanted to create an object that defined teachers. You probably would want a variable to store the teacher's name and a way to print out a list of courses the teacher teaches. Note that these are the same things we defined in our student object. Rather than rewrite the same things that we just spent hours defining for students, object-oriented programming enables effective abstraction. You can create an object that defines "person" and have students and teachers be types of "person" objects. Examples of object-oriented languages include Java and C++.
Scriptinglanguages are technically not full languages but share many qualities with imperative and object-oriented programming. These languages are known as weakly typed, in reference to the flexibility with which they define and convert between types. Recall that variables and their underlying bytes need to be specified as a particular type so the computer interprets it the way we want. An advantage of this flexibility is that it allows for rapid development, but a disadvantage might be reliability (discussed in the next section). Scripting languages such as Python and Ruby have been built to interface easily with a number of different systems, such as APIs and databases (Chapter 6).
Parallel programming languages allow for multiple execution sequences to proceed concurrently. Typically, parallelism is used to speed up a process—evaluating two things at once is quicker than doing one after the other—or to react to events that might occur at the same time.
Query languages are used to interact with a database (Chapter 5).
Markup languages use tags and special characters in order to give structure to a document (Chapter 4). The structure given by markup is typically used to visualize a document.  Examples include HTML and XML.
Choosing the Right Language
You now have a better understanding of programming languages and their different types, but you may not be closer to picking a language for MyAppoly. Language design principles serve as good background in evaluating language effectiveness, but practical factors will ultimately govern your decision.
Technical and Design Considerations
In evaluating programming languages for their suitability to your requirements, first consider their technical and design parameters, as follows:

Orthogonality refers to the extent to which a language lacks exceptions. For example, if you were able to add two numbers but not subtract them, this lack of orthogonality would be confusing for a programmer.
Naturalness of application refers to the relative ease of coding your application in a particular language based on the functionalities and constructs it provides. One reason object-oriented programming became popular is because it allows the programmer to think about the solution to the programming problem in many ways and therefore fit many programmers' uses.
Reliability refers to a program's ability to perform the same way each time. Potentially, a program can run into different type errors (again, relating to how a computer interprets the bytes that make up a variable) either when you run the program (at "runtime") or beforehand when compiling, if it is a compiled language. It is cheaper to catch errors at compile time rather than at runtime, but as we mentioned before, compiled languages typically take longer to develop and execute. The trade-off between cost of execution and reliability is one that programmers face constantly (Chapter 11).
Support for abstraction  (as discussed in the previous section) refers to the ability to define and use complicated structures or operations in a way that allows many of the details to be ignored.
Portability refers to the ease with which your program can be translated into another programming language. If, for whatever reason, you decide that it will be better to code MyAppoly in Python rather than PHP, knowing if the code is fairly portable might weigh in on your initial decision.
Practical Considerations
The following practical considerations will ultimately govern your selection of the programming language for building MyAppoly:

Simplicity. This practical factor relates to applicability: If you have the wrong tool for the job, your code to express something that should be simple might instead become unnecessarily complicated.
Documentation. Languages that have thorough documentation are very useful, especially if you and your team are learning to code as you go. Documentation refers to the manual and other resources that explain a language's syntax (what specifically you need to type to get it to work), usage, and constructs.

Development time. Driving this practical consideration is a trend in the computer industry. Historically, speed and memory usage were key factors in choosing a language, because processors were slow and memory was limited.  Now, however, hardware is cheap and becoming cheaper. Because hardware is now less expensive than hiring a new programmer, organizations tend to prioritize development time over execution time.

Reliably updated. More popular languages, such as PHP and Python, tend to be updated regularly based on programmer feedback. Each update makes existing functionality easier, introduces new functionality, or fixes bugs.
Maintainability. This practical factor relates to a language's simplicity and readability.  If MyAppoly is to last for a meaningful period of time, you will need to find new programmers to satisfy your growing needs. If understanding, updating, and correcting your code is a nightmare, your product can be seriously delayed.
Committed community. Another benefit of popular languages is the community of programmers using it. If a large number of people use the language, chances are good that the functionality you are trying to implement has already been programmed by someone else. Forums and blogs are full of sample code to help you get your product up faster and with relatively less struggle.
Talent pool. The committed community also composes your target market for technical hires. If you choose an obscure language, your ability to scale up your team by hiring will be limited. On the other hand, when you choose a language that might be more difficult to learn, you know that the programmers you do find will be strong and experienced.
Support libraries and tools. A large community of programmers might post new open-source libraries and other support tools that you can directly plug into your program to save the time of programming it yourself. This is explained further in Chapter 6.
Application programming interfaces (APIs). If you are using a popular language, chances are good that your language will be supported and well documented if you choose to integrate your application with, say, Facebook's API or Twitter's API (Chapter 6).
Frameworks. Templates, libraries, and other tools support the development of web applications by equipping developers with functionalities that are common among all web applications (Chapter 6).
Integrated Development Editors (IDEs). This one-stop application has an editor for typing code, a source compiler to translate the code, a debugger to help spot any errors in the program, and a few other project management-related tools. For some programmers, a programming environment, or the collection of tools used to develop a product, is very important.
Conclusion
While you are debating which language to use for MyAppoly, you immerse yourself in the world of tech startups and realize that this world has increasingly begun to prioritize development time over almost all other considerations. Part of the underlying reason for this is due to hardware becoming cheaper, consistent with the oft-cited Moore's law. Another reason relates to the rapid-iteration doctrine of product development (Chapter 8). Your competitors are building as fast as they can. Conscious of your need to be the first mover, you expeditiously consider all of the factors set out in this chapter, consult a well-known popularity ranking of languages, and settle on the most popular scripting language of the day. You have little time to waste.
1 Paul Graham, "Beating the Averages" (2003), at www.paulgraham.com/avg.html.
2 TIOBE Software BV, www.tiobe.com/index.php/content/paperinfo/tpci/index.html. Note that not all of these langauges relate to the back end.
3 Robert J. Schalko, Programming Languages and Methodologies. Jones and Bartlett, 2007.
4 Many experts identify a hybrid of compiled and interpreted as a distinct third type of implementation See, for example, Robert W. Sebesta, Concepts of Programming Languages, 8th ed. Addison Wesley, 2008.







CHAPTER4
The Front End: Presentation
Selecting a language for the back end is helpful, but without a front end, a user cannot do anything. Recall that the front end is the interface for users to interact with the back end. It refers to what you see and the range of interactions you can perform on a given web page, such as the search box and search button on Google. You read a few books on web design and have started to think about the nontechnical aspects of how MyAppoly will look, which pages you want, and how they will be connected. You have finalized a set of features you want to include in MyAppoly and have developed wireframes or mock-ups of each web page.
Wireframes are simple blueprints for a web page. Putting together wireframes forces you to think about two main things: information design and interaction design. Information design relates to how you present all of the content or messaging you want to communicate to the user. Interaction design refers to the series of actions you want the user to perform to complete some overall task. For example, the placement of product descriptions and reviews on a site such as Amazon is an example of information design, while the flow through which a user reviews an item, adds it to the cart, and ultimately purchases it relates to interaction design.
You review the mock-ups that make up the bulk of what MyAppoly will look like with your CTO. In effect, you have created a paper version of your website. Great! How do you actually get your website to look like that? How will you create the interactive effects you see on most modern web pages? Though someone else on the team might be the frontend engineer, certain languages and tools have become so ubiquitous that it is worth learning the basics because together they form the presentation layer and define what your users will experience.
Frontend Technologies
The standard frontend tools for creating the presentation layer include HTML/XHTML, CSS, JavaScript, and Ajax.
HTML
HTML is a markup language that defines the content and structure of web pages. As discussed in Chapter 3, a markup language uses tags and special characters to give structure to a document. These tags are usually two types: container and stand alone. Container tags flank text to which you want to apply some formatting or context. For example, take the example of <h1>Read my book</h1> in HTML. When a browser translates the markup code in the web page that you see—a process called rendering—it will see the starting <h1> tag and know to bold and enlarge the text that comes after it. Thus "Read my book" will look like a heading. When the browser gets to < /h1>, this end tag tells the browser to no longer bold any text and to continue reading the code until it hits the next tag. Such tags can also be nested, such that if you wanted two paragraphs to be written as a bolded heading, you could write <h1><p>Paragraph One</p><p>Paragraph Two</p>< /h1>. Whereas container tags indicate both start as well as end tags to some formatting, stand alone tags are those for which you simply place into code to create some type of effect. When a browser reads <br> in HTML, it knows to create a line break in the web page before continuing to read the rest of the code.
Thus, markup languages use combinations of tags to define the structure of a document. This text is a title, this text is a paragraph, there are now three line breaks, and a final paragraph. The style sheet (which will be discussed with CSS) adds style to the tags so rendered documents don't look so bare. Style sheets will tell the browser things such as, "Make all those <h1> tags blue!" Thus the markup document and style sheet together form the bare essentials of web pages.
With that introduction, let's jump back to HTML. There exists a metalanguage called Standard Generalized Markup Language (SGML) that is used to create markup languages and in the 1980s, Tim Berners-Lee used it to create HTML, the main markup language used to display web pages on the Internet. The code for HTML is consistent with our description of markup language used earlier in that it uses tags that are hierarchically nested to give structure to text that forms the web page. Recall that elements can be nested, in the same way that the two paragraph elements were contained within the h1 tag described previously. On visiting a web page, a user is essentially opening an HTML file, which the browser then renders into a more legible format.
The structure of HTML pages typically consists of the following six parts:

DOCTYPE Over time, new versions of HTML and XHTML (discussed later) have been developed. Therefore we have the problem where web pages are written in one of several versions and types of markup languages available. When you go to your favorite website, how does your browser know which markup language created that web page? If it did not know, it might interpret one of the tags incorrectly, or might not present the page at all. Therefore, DOCTYPE, or Document Type Definition/Declaration (DTD) tells the browser which markup language it's dealing with.
HTML The HTML tag contains all the tags or elements of the document (except the DOCTYPE). It defines the area that the browser should render.
head The head element contains the information that the browser uses to read the rest of the document. In addition to the title of the document, the head element includes other scripts, meta-information, and style sheets—all discussed later.
title This defines the title of the web page. In addition to communicating information to a user, a good title is one of the keys to search engine optimization (SEO), the process by which you can improve your position in search engine results (Chapter 10).
meta Metatags are used to convey information to the browser such as the keywords of the page or a description of the page content. These tags also will be more relevant in the context of SEO.
body The body element contains the actual page to be read including all text, images, links, and so forth.
Numerous other types of tags exist that introduce graphical images, tables, lists, and almost anything else you can think of. Tags can have attributes that can give more descriptions about the element, such as information used for styling. The newest version, HTML5, also supports new features such as animation, geolocation, touchscreen support, and content handling.
The following two sections discuss other terms you are likely to come across when discussing markup languages and the design of web pages.
XML and XHTML
XML is a markup metalanguage, which means it is used to define other markup languages, so in some ways it is similar to SGML. It is much more simplified, however, than SGML, the technology from which HTML was developed. XML consists of two parts: a document and a style sheet. The document contains tags or elements, as described earlier, and the style sheet gives style to the various elements.
Why do we need XML? SGML includes a lot of complex and often unnecessary aspects. As a result, HTML is often forgiving of errors and is more lax with rules, leading to different renditions of the same HTML document by different browsers or devices. It becomes difficult for you as a developer to control user experience if each browser reads your work differently. XML is stricter and has guidelines that need to be followed. Therefore, if we were able to rewrite HTML with XML, we could create a markup language that might not run into as many problems as HTML. This has been done, and the resulting markup language is called XHTML. With stricter coding guidelines, XHTML pages can be more accessible on different devices and browsers, though XHTML is not backwards-compatible, meaning old browsers may not be able to read newer versions of the markup language. Another drawback is that development time might be slow on account of the strict rules. Because consumers nowadays as a matter of course demand accessibility—ensuring that people can access the resource on pretty much any device—many expect that XHTML is on the way to becoming the new standard.
CSS
Cascading Style Sheets (CSS) is a language telling the browser how to style and position the various HTML/XHTML elements. CSS syntax is brief. It includes a selector that tells the browser which of the HTML tags to style, and a declaration that tells the browser how to style the elements.
There are three places where CSS styling can live. First, it can be placed in-line as in <p style="color:blue">My first paragraph. < /p> tells the browser to color the paragraph blue. Second, all style codes can be aggregated at the top of the page within the <head> tags. The browser is smart enough to match the style code to its corresponding HTML tags. Last, all CSS can be put in an external sheet with a link to the HTML page. You specify which HTML tags you would like to style, and the browser can manage the rest. Of the three options, the external CSS file is considered best practice for a number of reasons:

Readability—It is better to keep structure and style separate
Speed—Faster load time
Maintenance—Imagine you want to update the style of your entire site. If the CSS were in-line, you would have to go through all the HTML to update all of the style. Good luck not forgetting anything! If everything lived in an external file, you could easily make updates. You could even create an entirely new file called "style version 2" and then change the link in the HTML document. Depending on your mood, you could simply change the link from styleversion1.css to styleversion2.css and the entire feel of your website could change. This capability resembles abstraction (Chapter 3), and it's powerful.
Inheritance was discussed in Chapter 3. The same concept exists in CSS, whereby the style of parent elements is passed on to children elements. Thus, if we had an element denoting a paragraph within an element denoting a bold heading, the paragraph would also appear bold. In other words, if there is an element that is in another element, it will inherit the style of the larger container in which it is in. This idea of inheritance informs the word cascading in the term CSS.
CSS contains eight general style categories as follow, together with examples:

Type—Font, color
Background—Color, image
Block—Word spacing, line spacing
Box—Width, height, float
Border—Style, color
List—Style, position
Positioning—Position, height, visibility
Extensions—Page break, cursor
Positioning in CSS is based on a box model whereby every element can be described as being located in a nested zone, ranging outward from content in the center, through padding, and then on to borders and margins. In some ways, designing a web page with CSS is nothing more than arranging boxes on a screen. Okay, perhaps not that simple.
The version CSS3 includes animation effects, rounded corners, shadow effects, and other new stylization techniques.
JavaScript
With HTML and CSS, you can create great looking websites, but they will lack the interactivity we are used to in the Web 2.0 era. Drop-down menus, boxes that appear when you click the mouse, and so much more can be attributed to JavaScript, an object-based scripting language (remember the definitions from Chapter 3?). It lives in the HTML document and is rendered by your browser (the client trying to access the site); therefore we say JavaScript requires client-side processing. This is why JavaScript, unlike the other scripting languages we have discussed, is considered front end and not the back end. It is not executed by the server. (Note: node.js is similar to JavaScript but processed server side.)
First introduced by NetScape, JavaScript is now ubiquitous across the web and will serve to make MyAppoly interactive and animated. Let's say you want an information pop-up window to be displayed after the user clicks on the MyAppoly logo. How is this accomplished? On clicking the logo, the HTML element where the logo is contained will contain some reference to a JavaScript function. This bit of code is called an event handler. Event handlers "listen" for certain actions that the user completes—called events—at which point the corresponding JavaScript function fires. The JavaScript function will create a pop-up window object, and will present the window to the user. Thus, events, event handlers, and functions define a large part of how JavaScript can be used to add interactivity to a web page.
As with CSS, it can be placed either directly into the HTML document in the <head> element, or it can be put in an external .js file with a link from within the HTML document.
Document Object Model and DHTML
What if you wanted to change the color of the background when the user clicked on the MyAppoly logo and produced a pop-up window? This is slightly more complicated because this requires that JavaScript not only creates a pop-up window but also alters the HTML/CSS code that defines the background color of the page. This Dynamic HTML (DHTML) combination of static HTML and JavaScript is enabled by the Document Object Model (DOM). The World Wide Web Consortium (W3C) defines DOM as, "a platform- and language-neutral interface that will allow programs and scripts to dynamically access and update the content, structure and style of documents." The DOM essentially defines all the elements on an HTML page as objects, places them in a tree-like structure based on how the elements are nested so you can easily reference them, and makes every element on the page interactive. You can theoretically reference any element in the document by tracing the path from the top of the tree to that element. Once at that element, you can manipulate it to create your desired effect. The DOM adds interactivity to every element on the page.
 jQuery
Some of the things you might use JavaScript for—such as drop-down menus, zoom-in functionality, and sliding panels—are common across all web pages. jQuery is JavaScript's most popular library—that is, a collection of useful prewritten code that you can use in your own applications (Chapter 6).
Ajax
Ajax, like DHTML, is a combination technology. First amalgamated in 2005, Asynchronous JavaScript and XML—Ajax for short—combines CSS, HTML, DOM, XML, and XMLHttpRequest to allow a web page to perform functions in the background. Let's break this apart. Imagine we have a form for individuals to complete on MyAppoly. This form requires the user to enter their state. Most modern websites will suggest a few options for you in a dynamic drop-down menu as you type. So when you type in "M," a list of states beginning with M automatically appears beneath the search box. When you type in an A, Massachusetts appears for you to select. If you wanted such a drop down without Ajax, the alternative would be a page that refreshed entirely every time you typed a new letter so that it could get a new list of potential states that might be valid. You would type a letter, which the page would take and use it to search through its database. After it found all states satisfying that letter, the page would refresh to display the updated information. Assuming it takes 5 seconds for a page to refresh, this would be annoying and people would become impatient with your website.
So what does Ajax do? Every time you want to get the updated list of states from the database, you have to ask the server for the information. When you type the letter "M," you tell the server you want all the states starting with M. The server gets the information, and typically passes you to a new web page containing the content. But that is not what you want because that is the bad "refresh" option described earlier. What Ajax does instead is that it uses a request type called the XMLHttpRequest object to transfer data the user requests from the server to the client. Ajax uses XML to format this data and last, XHTML, CSS, DOM, and JavaScript are used to take the data and update only the portion of the web page that is affected to achieve our desired effect. The major point is that this is all done asynchronously, meaning the user does not know it is happening in the background. In fact, the user can continue to interact with the web page while Ajax is working to get more up-to-date information from the server. It is sophisticated and often difficult to write, but several Ajax toolkits exist to simplify its usage.
Ajax only works in modern browsers, and so this technology too has backward-compatibility issues.
Portability and Accessibility
With no true enforced standard on browser rendering, all browsers render HTML pages and CSS slightly differently. Therefore, what might appear fine on Safari will appear odd on Internet Explorer. What might look okay on the PC version of a browser might be unfit on a Mac. Due to changing technologies, what works on a version of Chrome today might not work on an outdated version. All of these observations mean that, as the developer, you are not reaching your entire audience in the same way. It is in your interest to either make sure your website looks fine on every display of every potential device—which is not realistic—or to hope that web standards or some other solution comes along.
Web Standards
Imagine if every car company required a unique type of gas. It would be nearly impossible for gas stations to carry every type of gas a customer might need. Auto manufacturers realized from the outset that designing their vehicles to run on only a few industry-standard grades of gasoline was the optimal solution.
Currently, the Internet is like a world without gasoline standards. A much better solution to the issues of accessibility and portability is through web standards both on the frontend side of coding as well as for browser rendering. The W3C is "an international community where member organizations, a full-time staff, and the public work together to develop Web standards." What they produce and promote are not strict rules but rather suggested guidelines. Many note a convergence toward these recommendations recently, something that would be extremely beneficial for developers and users alike.
Several validators exist to ensure that your web pages satisfy the guidelines of the W3C. Validation is beneficial because it helps make sure your web page is most widely accessible, often improves display time, and might help with search optimization, discussed in Chapter 10.
Responsive Design
What if you could spot which device people were using when they came to your site? If that were possible, you could then show them a special version of the site adjusted for their device. This is possible, and until a standard exists, such responsive design is a quick pseudosolution. Because it is possible to identify where a user is coming from and what device he or she is on, you can choose to display a certain CSS sheet or have different versions of the code that you can use to make the experience more optimal for the user. However, as you can imagine, covering all cases of browser and device combinations is difficult.
Conclusion
You now understand what is required conceptually to translate the paper version of MyAppoly into a working front end. At this point, you tell your CTO that you understand how users access a website and interact with your functionality, and how your back end processes users' actions. But how does your back end remember and organize information, such as a user's name or list of friends? To answer that question, you need to understand databases, the subject of the next chapter.







CHAPTER5
Databases: The Model
The information that some companies collect is an asset in and of itself due to the power of data. Working for a local business data provider startup, I focused on collecting menus from local businesses, primarily restaurants. With a database of all food items and their prices sold at restaurants, I was able to learn some interesting facts! What is the average price of a pizza in San Francisco, you ask? Which restaurants in a five-mail radius of my hotel have kimchi on the menu? No problem. Collecting such data allows you to answer people's previously unanswerable questions.
The last component of the typical web application that you need to familiarize yourself with is the database, often called the model. All people who will become users of MyAppoly need to sign up on your website and enter their information. You know you have to store that data somewhere, but how do you store it? While this chapter does not discuss how to programmatically add and update data in a database, it reviews how databases are typically structured and the ways they differ. You will collect all types of data on MyAppoly. From usernames and passwords to the number of times someone has logged in, the amount of information that you will want to save will grow exponentially as your site becomes more and more successful. Your team is deciding what type of database to use. Your CTO gives you the following chapter to get you up to speed. Luckily, it will teach you a lot of terms you are likely to come across given the trends in big data.
Database Systems
Most of us are familiar with some sort of record-keeping system. Some people store all their receipts in preparation for paying taxes. Others who run a business might keep track of their sales and expenses to make sure they are hitting their targets. Physicians keep files on all of their patients, typically organizing them by last name. A database system is nothing more than an electronic filing cabinet and "requests" are nothing more than interaction with the cabinet whether that be adding, removing, updating, or retrieving a file.
Which type of data might you collect? Typically, you want to save data that will be of some value to you in the future. This is called persistent data because it is saved for longer than the length of the user's session or a day's worth of activity. Because the power of data and analysis unlocks efficiency, insights, and opportunities, the importance of databases is critical to all businesses today.
The Four Components of Databases
Databases consist of four components: data, hardware, software, and users.
Data
Data is the sine qua non of a database—being the actual information you want to save to the database. In addition to the data, you also save information about the data and the database. You save this metadata in a place called the catalog—just in case you will need to refresh your memory on what you are collecting and how it's being stored.
Hardware
Data is physically stored on some component of hardware—typically in secondary storage, such as magnetic disks. Storage hardware can be categorized in several ways: primary, secondary, and tertiary.
Primary storage provides fast access to data because the computer's central processing unit (CPU) directly interacts with it. The CPU carries out the instructions of a computer program, so think of it as the brain of the process or the executer of the program. When a computer must perform a task that you requested, it will access primary storage for the specific instructions of what to do before the CPU executes. Terms associated with primary storage include registers, cache memory (also referred to as static random access memory or RAM), and dynamic RAM (DRAM) or main memory. These various parts are associated with different tasks, but they all serve to keep data and programs that the CPU is actively using ready for execution. We say that primary storage is volatile because after the machine is turned off or is without power, the contents are lost. Primary storage is very expensive, however, and therefore tends to be limited in size.
Secondary storage, by contrast, is nonvolatile, cheaper, and is not directly accessed by the CPU. Because the data persists even after the computer is turned off and because we typically would like the safety of our data to be independent from our computer's power, it may not be surprising to learn that secondary storage serves as our mass storage site. The CPU cannot directly access the contents on secondary storage; therefore data must first be copied into primary storage before execution, a fact that highlights why speed tends to be slower. Hard disk drives, flash memory, and floppy disks are examples of secondary storage.
Tertiary storage is similar to secondary storage in that it is not accessed directly by the CPU. It is used for massive data storage and is much slower than secondary storage.
The key tradeoff distinguishing these three classes of storage is access time vs. expense, a consideration touched on in the "Optimization" section at the end of this chapter.
Software
When users interact with MyAppoly, they will perform certain actions that you will want to save in your database. A database management system (DMBS) constitutes the interfacing software layer between the users and the actual database and is responsible for defining the database and storing, manipulating, and sharing the data across users and applications.
Think of DMBS as the Head of File Management and is responsible for managing the requests of all of MyAppoly's visitors. When a user clicks "accept friend" on Facebook, a request is created and directed at the database. The DMBS must accept and analyze all requests before sending the request along its way into the database, at which point it is interpreted (as to what is it telling the database to do) and executed. The code that actually accesses the database on behalf of the DMBS is called a data sublanguage; one popular version is SQL.
The DBMS provides a way to control for concurrency (multiple requests to the database happening at the same time), redundancy (the same piece of information stored multiple times in the database), security, recovery (protection against some sort of database failure), and optimization—aspects that greatly improve the value to the developer.
Users
Your end users are the users of MyAppoly who, by interacting with the application, generate requests to the database in the background. When a new user registers, for example, a database request is issued asking the DMBS to add a new user to the database.
Three-Level Architecture
You will also hear database systems described as an architecture consisting of three parts: the physical, the logical or conceptual, and the external. The physical layer is where the data actually lies, the logical consists of how the DBMS interfaces, and the external layer includes user interactions with the database.
The three levels, three tiers, or three schemata of database systems is not too different than the way I described a database earlier, but it is helpful to understand jargon that might be thrown around in the context of database systems. It is also easier now to explain the database independence principle. The DBMS provides a form of abstraction so that users can interact with the database without needing to know the details of how the data is stored. What this means is that any of the tiers can be changed in its implementation, and it should not affect the entire system. For example, let's say your programmer codes the application in a way that displays all of the names of MyAppoly users in a list. You would like all the users, however, to be displayed on a graph. It would be a pain for the programmer to rewrite requests to the database to get the information.
Thanks to the data independence principle that has guided database system design, your programmer can change the list to a graph, and it will not affect how the programmer interacts with the rest of the system. The change only affects the visualization layer and has nothing to do with how you get the data or how the data is stored. In summary, the system separates the management of data from the actual use of the data, which has several benefits. I suppose one could argue that the process of writing requests that get handled by a middleman (DBMS) before going to the database slows down the processing time, but the benefits outweigh the lag cost.
Entity-Relationship Model
With advances in technology, it may be possible for computers to learn and think like humans. With that being said, the manner in which a computer thinks will most likely not mirror how our brains work. For example, I doubt our brain stores things in our memory the same way I might save things in a database.
The entity-relationship model will help us understand how we can move from human perception of information to a database recognizable representation of the data. In this model, I call everything that exists an entity. A car, a class, and a store are all entities. I describe entities with attributes. A car has a color, a class has a name, and a store has a location. In this way, entities and attributes can describe anything. Relationships can exist between different entities too. An entity that describes a teacher might be related to the entity that describes a class. In this way using entities and relationships, I have boiled down what I observe in the world into a way I can conceptually describe it to a database. One can imagine that we can get even more complicated by creating subtypes of entities (types of teachers, types of cars), and introducing concepts such as inheritance (Chapter 3). There is a balance between generalization and specialization of entities, but the point is that there are many ways in which a model can describe the world.
Classification
Criteria for classifying database management systems include the following, which are considered in turn in the succeeding sections:

Data Model The entity relationship model is a high-level way of describing data. Lower-level ways to describe how to store the data to a computer are discussed in the next section.
Number of SitesCentralized and distributed are buzzwords you have probably encountered before. How they relate to databases is discussed in the section "Centralized vs. Distributed."
Cost Solutions can run the gamut in terms of cost. Free and open-source solutions, such as mySQL, are available. Commercial solutions are offered to larger enterprises that want more customizable systems.
Data Model
There are several ways to store data in a database that are efficient and organized. Four of the most common data models are discussed in the following sections: the relational model, the non-relational model, the object-oriented model, and the object-relational model.
Relational Model
A model proposed in 1969 by Edgar F. Codd called the relational model became and continues to be the de facto standard. Relation is nothing more than a mathematical term for a table, so from that fact alone we can ascertain that the relational model uses tables to represent data. In the relational model, a table is used to describe a set of similar entities such as users or patients. Each row represents a specific entity (also can be called an object or record), and each column can be considered an attribute or field. In our users table, each row will describe a user of MyAppoly. The columns can be items such as first name, last name, password, address, and email address. It is common for tables to have a primary key, or a column for which the value is unique for every row. Let's say I want to assign every user a customer ID number. I do not want two customers to have the same ID number because that would defeat the purpose of the unique customer ID number. Therefore, when constructing the table, I could make this ID number a primary key. This now allows me to refer to individual customers with no chance of overlap.
Now I have described how data can be stored in tables in a database, but how do I interact with it?  Typically the database and its specific tables are modified by using particular operators that the DBMS and its data sublanguage provide. As with all languages, they are picky and require specific requests, so you have to spell out very clearly what you want to have happen. Recall that the data sublanguage gives programmers a way to communicate with the DBMS. If this were SQL, a popular query language, these operators would consist of INSERT (add entry to table), DELETE (remove row from table), SELECT (retrieve information already stored in database), and UPDATE (modify existing rows), among several others. These operators tell the DBMS to adjust the table entry accordingly. Structured Query Language (SQL) is a proprietary query language (data sublanguage) developed by IBM that is now the de facto international standard and supported by most relational systems.
There are several commands worth knowing in addition to the ones above that allow for more complicated operations. One example is JOIN, which is used in conjunction with SELECT. Let's say you have the user table as described above. Let's say you have a separate table called pets that stores data on all of the users' pets. Every row represents a pet, and the columns include name, owner, color, and type. In this example, the owner column of the pets table should map to a user in the user table. What if I wanted to answer the following question: "What are the addresses of all users who own pythons as pets?" This request involves accessing the pets table to retrieve all owners who have pythons, and then giving each owner in your list of owners to the users table to obtain their addresses. This is a two-step process that can be solved by using the JOIN operation, which essentially combines two tables based on a shared column. (In this example, the shared columns would be user and owner.) I highlight this example to show that requests can get complicated given the structure of relational systems, but operations exist to let developers obtain answers to their questions.
The query language and operators I just described are nonprocedural, meaning that users can specify what they want but not necessarily how the DBMS should go about executing the request. I'm not sure if you would have a strong opinion on how specifically your data is retrieved, but it could matter from a speed perspective.
Let's review a few other concepts and terms for thoroughness. A series of requests that make up a particular task is called a transaction—all of the requests required to successfully register a user and his or her pet, for example, could define a transaction. A few rules exist for transactions that are not exclusive to relational systems, but will be discussed here briefly. First, transactions are atomic meaning they are all or nothing. Either the user is entirely registered, or the database remains unchanged. This is helpful because if the process were to fail halfway through for whatever reason, there are no incomplete rows in the table. That would complicate the system. Second, transactions are durable, meaning that they immediately change the database and are available right after the transaction execution completes. There can be no lag in this otherwise it would affect the entire experience of the site. Imagine if it took a week for your friend list to update in Facebook every time you accepted a new friend. Third, transactions are isolated, meaning two transactions are considered independently by the DBMS. The system is designed in a way that transactions can either execute concurrently or one after another—called serializable—and the outcome should be the same. These are over simplified descriptions of transactions, but they should help you understand how a DBMS thinks about requests.
One last term to mention is normalization, which refers to the process of eliminating redundancy in databases. Let us go back to our example of users and pets tables. What if a developer created a column in our pets' table called owner's address? Isn't that information already saved in the users table? In this case, the address columns would be redundant and is wasted space. Normalization helps us reduce wasted space. There is an argument, however, against this point. If I frequently ask the question I posed earlier about the address of all people who own pythons (the one for which I had to use the JOIN), my stance might be different. JOINs can be expensive because the DBMS has to combine two tables, and then find the information, a potentially costly process depending on the size of the tables. It might just be easier and more economical to create the redundant column. These types of decisions can be made by the smartest database administrators (DBAs), who understand exactly how requests are executed. Luckily, you've hired one—and after reading this chapter you might even understand what he tells you.
Non-Relational Model
Non-relational models, as you may have guessed, are models that do not use tables as the primary data structure. Therefore, this category encompasses a lot, including networks, hierarchic models (think family trees), and lists.
One term worth a special mention is NoSQL. Relational systems are great for more complicated data where queries will be requested that concern the relationships between entities (for example, between users and their pets). NoSQL provides what some would say is a different solution. NoSQL systems tend to be great for storing large amounts of simple data such as key-value pairs (firstName: Vinay). For these simple use cases, NoSQL offers great performance and scalability. If your team feels that such a strict relational structure is not necessary and that you are dealing with storing and retrieving large amounts of data where the relationship between stored items is not as important, NoSQL database management systems are very useful and arguably more effective. Open-source NoSQL databases such as MongoDB are available.
Object-Oriented Model
Object-oriented databases center around the concept of the object (Chapter 3) in the context of object-oriented programming languages. These objects describe entities, and each object consists of a series of properties or attributes describing it, as well as a series of methods or functions that are associated with the object. As before, there has to be something unique about each object, so an Object ID is often used. Technically, relationships between objects use pointers, but you can think of it in the following way: the object describing a pet owner might store the Object ID of the pet object that describes the details of the pet. So if you want details of User X's pet, you could access user X's object, get the user's corresponding pet ID, then access the pet object. These objects can sit in a hierarchy to give more structure to the system, and take advantage of inheritance and other qualities associated with object-oriented models.
Object-oriented models are powerful because they allow you to preserve the complicated nature of certain objects. In addition, they help separate the structure of objects from the operations you can apply to the objects. The details of how these models work are left for more technical works, but many of the terms discussed under "Relational Model" carry over to this one. For example, object-oriented models also have a query language: Object Query Language (OQL).
Object-Relational Model
Object-oriented aspects have found their way into relational systems to merge the best part of both worlds in object-relational models. In an object-relational system, you can define your own type, for example.
XML
As mentioned in Chapters 3 and 4, XML is a markup language that can be used to give structure to documents. Therefore, it can be used to define the structure of data, and is a primary way data is communicated over the web (think web pages or RSS feeds).
Centralized vs. Distributed
The client-server architecture describes the relationship between the end user and the DBMS. Clients consist of all the applications that interact with the DBMS, and the DBMS is in effect the server that processes and executes the requests.
The actual computer that stores everything can either be centralized or distributed in nature. When centralized, all requests come to a single server that houses all of the data.  There is a single point of failure in this system for if this server were to go down for whatever reason, the application relying on it would not work. Therefore, as communication and database technologies improved, organizations realized there could be many benefits to a distributed system. Rather than having one central database server, what if I had multiple, all of which could communicate with each other? If this were the case, different parts of a single request could be executed on different machines within the distributed network of database servers.  Though the data may rest at several different sites all managed across a network by software, the users of the applications would never know. To revisit our example, the users table and the pets table could theoretically be stored on different servers.
A typical distributed system has several qualities, but some of the most important ones include local autonomy (every site should be independent from one another), no central site, local independence (users should not need to know in which site the data is stored), and pretty much any other thing that allows it to function exactly like a nondistributed system.
As is evident from the popularity of the word distributed in the technical world, distributed processing has become popular, and for good reasons. Benefits include the following:

Superior rationality—Data already tends to be distributed. If you think about the common organization, data already tends to be distributed. Whether it is distributed across geographic offices, or internally by division, there are logical divisions to databases. Therefore, if the other benefits are compelling, why not retain this distributed quality? It could allow each group to manage their data and their data alone without having to come up with a way to combine it all into a centralized server. It makes sense for Pepsi Asia, for example, to store some of its data in Asia.
Improved efficiency and performance—Intuitively, the processing of data is most efficient when done closest to the site where the request was given. Therefore, it makes sense for the most relevant data to live near the people using it the most often. A distributed system can help with the most efficient allocation of an entire organization's data. In addition, performance can be improved when queries are spread over multiple machines.
Increased reliability—Because information is available on multiple computers (several copies can be created as well), the probability that the entire system is down at any given point is much lower than in the centralized system case.
Adjustable expansion—Distributed systems are easily horizontally scalable, which means it is easy to add more computers to the system. Say for MyAppoly you are currently using five computers around the United States to store information. It is relatively easy to add a sixth, especially compared to vertically scaling, which is adding more memory to a particular computer. What makes a service such as Amazon Web Services (Chapter 2) so popular is that they allow organizations to add and take away servers depending on demand. This seamless way to scale and shrink allows organizations to more effectively spend their resources. No sense in buying a lot of capacity when you only ever use that much capacity once in a while (e.g., when you have a promotion on your website).
The drawbacks to such a system are fewer but they exist. First, because data can reside in several sites, query processing that requires bringing together data across servers can be costly. Second, the various functionalities that the DBMS provides (security, concurrency management, and so on) tends to be more technically complicated in a distributed system compared to a centralized system— but you're most likely not the one who has to worry about that.
Other Topics
Other database management requirements concern concurrency, security, and optimization, treated in turn in the next sections.
Concurrency
You do not want multiple users who are using a database to somehow have conflicting requests. If a Facebook friend blocks you at the same time you send him a message, how will Facebook handle this? This question relates to concurrency.
In the context of databases, concurrent access means that multiple users can access the database or multiple transactions can be processed at the same time. Because DBMSs support concurrency, they also need to be able to control the potential dilemmas that could arise when multiple transactions are trying to alter the database simultaneously.
For example, if two users are adding a recommendation to a restaurant on Yelp at the same time, how does Yelp manage this? Knowing that DBMSs manage this is enough, but for more specifics, a few concurrency problems  follow:

Lost update—When two transactions attempt to update the same value, one might be lost.
Uncommitted dependency—One transaction can access a value that was updated but not yet committed by another transaction.
Inconsistent analysis—When two transactions have competing objectives, the steps might not be consistent with each other.
These problems are typically solved with a concept known as locking, whereby a particular object or value will be locked and accessible only by one process. Several types of locks exist, but the main point is that locking adds wait time between each process to ensure no two transactions are colliding. Most solutions tend to be imperfect, and deadlock is one such example. Two transactions might lock each other out of accessing some information leading to an infinite wait time. The deadlock must be broken by breaking one of the transactions, introducing complexity and delay in the system.
Security
In February 2013, a security breach at Twitter may have compromised the data of up to 250,000 users. "'This attack was not the work of amateurs, and we do not believe it was an isolated incident,' Bob Lord, Twitters director of information security, said in a blog post. 'The attackers were extremely sophisticated, and we believe other companies and organizations have also been recently similarly attacked.'"1 Database security breaches resulting in users' passwords or customer credit card numbers being spread across the Internet are becoming commonplace. Customers of any site demand a level of protection, and companies must ensure that their data is kept secure or else customers will leave.
DBMSs protect data against unauthorized access as well as against the improper modification of data (preserving data integrity) in the two following ways:

Discretionary control—In this model, every user is assigned appropriate access rights to different objects in the database. Discretionary control tends to be the more flexible of the two control types.
Mandatory control—With mandatory control, every object in the database is assigned a classification level and every user is given a clearance level, much as depicted in movies of the FBI and CIA, and therefore can only access the set of data objects in the user's classification level.
Essentially, the DBMS can label users and data accordingly to make sure that no one who is unauthorized can access or manipulate private data. The usual username and password are effective and necessary barriers as well. As added checks, audit trails and system logs that track the transaction, time of transaction, and the user who issued the request are maintained by the DBMS. These logs can be used to track activity around a phishing incident.
Several complications can arise that do not immediately seem solvable by the control systems described here, although DBAs can use DBMSs and other tools to secure against them. One example pertains to the flow of information from the "haves" to the "have nots." A situation could arise in which a user with higher clearance, for example, alters the data in a way to give someone with lower clearance access to it. This flow must be controlled.
The managers of MyAppoly's database may want to control for inference—a problem for statistical databases. MyAppoly might use statistical databases that do not provide users with specific information about other users, but do offer aggregate statistics about a pool of users. Individuals users reluctant to allow others to know particulars about their salary, age, or some other personal metric might be more willing to share information if it is only be accessible when made anonymous in the form of a statistic. One problem is that particular filtering might allow someone to infer something about a particular user, something that should not be allowed. For example, if you request the average salary of all python owners in Cambridge, but there is only one such person that fits that description, the average salary the request will return will be an individual's salary. DBMS should protect against this.
Data encryption is a method used for protecting data stored or communicated across the Internet (Chapter 12). Data encryption entails taking plain text (as we would read it) and passing it through an algorithm with a particular encryption key to generate cipher text. The data is said to be encrypted and it cannot be read—it literally looks like a random assortment of characters. Only with the proper decryption key can the text be deciphered.  In this way, when a hacker attempts to access data, he or she presumably will be deterred.
In summary, DBMSs and other tools exist to protect data. No solution is perfect, which is why I continue to see security as a threat. The goal of a security system is to deter incursions by rendering the cost of breaking into the system greater than the potential payoff.
Optimization
There are numerous ways in which database systems can be optimized so that less space is used and information can be accessed and updated more quickly. In relational systems, queries are nonprocedural and rely on an automatic optimizer to parse the request and choose the best method for executing it. The way in which the query is constructed can influence how the optimizer chooses to respond, however, so database engineers have developed a keen eye for knowing what types of queries can be made more efficient. In non-relational systems, on the other hand, optimization is controlled manually, so slightly different methods are used.
Another area in which databases can be optimized, especially for space, is redundancy. Data is said to be integrated when redundancy among information is removed. It is wasteful, for example, to have a patient's name, insurance plan, and address written on every piece of paper within his or her file. Integrated data implies that such repetition is removed and kept in only one table, if you are talking about relational databases.
Above most other considerations, database design must be intelligent. It should reflect the things you need the fastest or update the most. Otherwise, regardless of how well queries are written or what tools are used, a flawed database design will be suboptimal and result in wasted time, money, and space.  Part of this relates to how tables are organized, another on how data is stored. The process of searching and storing can be optimized using concepts like binary search, sorting algorithms, and hash tables, methods that serve one purpose: to make it more efficient to sort, insert, and access data. The way in which you store your information should be determined by how you expect to use it later.
Big Data
How is big data different from normal data? Organizations have of course been amassing information even before there were computers, but there is an important change: the sheer amount. The size of data collected is unable to be processed by our traditional methods. Relational databases are being pushed to the wayside because they are simply too costly and restrictive; the more scalable and flexible non-relational models are coming to the fore. To give you an idea of scale and growth, it is estimated that 1.2 zettabytes (1.2 ×1021 bytes) of data were created in 2010.2 As a comparison, it is estimated that there are between 1020 and 1024 grains of sand on our planet. Imagine sifting through all of that! If you assume the amount of data we have doubles every couple of years, we are in store for a lot of data centers.
Before commenting on how one should even begin to study so much data, there are a few terms used to describe data. The first is volume, one that is most obvious from big data's name. The amount of information is the largest problem and has prompted engineers to think about how distributed systems can serve as the solution. Next, is velocity, which refers to the rate at which new data is created. The velocity of Facebook data is probably much higher than that of MyAppoly, for example. With so many interactions on Facebook every day, Facebook needs to make an important decision: what do they keep and what do they throw away? Online shopping websites face a similar dilemma. Every time a user clicks on an item or writes a search term, the company must decide if they want to save that information. The more they elect to save, however, may compromise the speed with which they are able to convert data into insights. If retailers are slow to customize the web experience for their customers due to a delay in data processing, faster competitors are likely to win.  Last, there is variety.  If the data comes from several sources and in several forms, it presents a much more complicated set of issues to tackle.
You can now go to your data team and tell them that the variety of your data complicates your ability to store it, and that the current velocity is overwhelming your limited volume server capacity. More colloquially, you might just say, "We need more money!"—but they would be more impressed with the more technical expression.
Let us assume you have a large volume of data of significant variety. The chances that you can make any sense of the information by looking at it are little to none, especially if there is no obvious pattern.  The word of the moment in big data is Hadoop, a software framework that allows organizations to run tests on their data with the hope of answering questions. As I covered in earlier chapters, distributed architecture is the key behind our ability to store and process this quantity of information. Hadoop allows organizations to make sense of their data by spreading it out over a bunch of powerful processors that work in parallel to study the information. The general method Hadoop uses is called MapReduce, which consists of two parts: map and reduce. Let us say you have some question you want to determine from your data set and your engineers write a function that theoretically will yield the right answer. Map will separate the task into several parts and distribute it across several machines. When this phase is complete, reduce will combine the results into a single task to complete the analysis. No single machine would be able to do this much work in a timely manner, so Hadoop's divide-and-conquer strategy is a very powerful tool.
Conclusion
You have learned some of the basics of how MyAppoly runs on the back end, what users will interact with at the front end, and the way data factors into all of it. Your team of engineers is thrilled you have made it so far.
1 Nicole Perlroth, "Twitter Hacked: Data for 250,000 Users May Be Stolen," New York Times, February 1, 2013.
2 Jim Kaskade, "Making Sense of Big Data (webinar video)," 2010, http://www.infochimps.com/video/making-sense-of-big-data/.







CHAPTER6
Leveraging Existing Code: APIs, Libraries, Web Services, and Open-Source Projects
Imagine how much more slowly society would have developed if no one had been allowed to leverage the inventions of others to design new products. The legal concept of intellectual property was contrived to give inventors sufficient financial incentive to create things from which to derive income for a term of exclusive right. Eventually, however, others can and should be able to use these innovations to make discoveries of their own. Maybe a drug developed for one disease can be adapted to cure another. Maybe flexible glass made for more durable dishware can be used to make unbreakable cellphone screens. Our society constantly searches for the next best thing, and we cannot expect to find it if we always begin from scratch.
Now that you understand all the critical components of your MyAppoly application, you must enlarge your field of play to take in an important trend: leveraging existing code. There are many features that you may want MyAppoly to have—say, a map. But it is hardly worthwhile for your team to build one from scratch, especially given that other companies have already dedicated the time and money to doing just that. Google's willingness to allow developers to use Google Maps points to a key driver of the rapid proliferation of websites and applications in the Web 2.0 era. The popular adage, "Don't reinvent the wheel," defines the tech world, and a culture of sharing information has emerged. Internet companies utilize partnerships and contracts to protect their information, but in general, they are more open to sharing their discoveries than the organizations of the past. This collaborative culture partly explains the extraordinary growth, fertility, and wealth of Silicon Valley.
You want to give your team the time to focus on what really makes MyAppoly different, not on rebuilding independently the code that other companies already offer to you. But what are these readymade tools available for you to leverage? This chapter describes the essentials of application programming interfaces (APIs) and web feeds.1 It also addresses the open-source movement and its importance.
Application Programming Interfaces
APIs, web services, and feeds all pertain to the general concept of exchanging data among people, websites, and applications. This section focuses on APIs. Say you want to show your users the locations of the nearest ATMs on a map and want to accept credit card payments on MyAppoly. Either your team can build these features from scratch, which will increase development time considerably and perhaps delay your scheduled launch date, or you can utilize the work of others, such as Google Maps and Stripe. Some APIs you may choose to use are free, while others require payment. Google Maps allows you to use their maps within MyAppoly for free, whereas Stripe processes all of your credit card payments for you for a minor fee. The ability to easily integrate technology in this manner, whereby one company's product can be plugged into another's, has allowed startups to grow quickly and become sophisticated in a relatively short period of time. MyAppoly can benefit from not only the APIs of others, but also by releasing one of its own. If other companies become dependent on MyAppoly's data and functionality made available through the API, MyAppoly's business model become even more attractive.
Using Others' APIs
Notable advantages and disadvantages of using APIs include the following:
Advantages:

Comparative advantage.Using others' APIs allows you to focus on your core competency and value proposition. In the same way a traditional business might contract out the nonessential work, such as document editing or reimbursements, you can contract out such work as payment processing and map features.
Save development time. Contracting out your noncore work to other companies by using their APIs saves your development team's time.
Leverage expertise of others. Companies typically release APIs in the area of their core competency. Whether their API provides information or functionality, chances are the company invests a lot of time. Even if your team could do a great job, why not give the responsibility to a company that is purely focusing on solving that problem?
Noncore functionality maintenance. If your team had to build a noncore feature, they also would have to spend time maintaining it. This amounts to additional waste. The companies whose technology you are using update their APIs regularly, so you get maintenance into the bargain, often free.
Access to information. Many APIs provide access to data. Often this data is available on the company's website itself, but in a format that is not conducive to collecting at scale. If the API can give you the same data in a format that allows you to manipulate it more easily, it will save you a lot of time and hassle. An example could be a weather API. The website might show you the daily weather in a particular city over time, but imagine if you were conducting a global warming study and had to collect the daily data from 10 cities over the past 50 years. It would be very tedious if you had to manually copy each number from the website. Instead, an API might allow you to access and download all of the data you want quickly.
Disadvantages:

Dependence.If, for whatever reason, the company behind an API decides to change its policies or shut it down, your web application will no longer function properly. This risk is a part of any business that depends on third parties. Personal judgment will influence your decision here. Do they have a lot of big clients? Are they performing well? Answers to these questions help predict reliability.
Not custom-made. It is also possible that your exact needs are not satisfied by any API on the market. APIs provide general solutions, but there is no guarantee that it will work perfectly for your site. Therefore, there may be a tension between conforming to what an API provides in the name of the advantages listed above and building the entire feature from scratch to get the full range of functionality.
Where you come out after weighing this list of pros and cons will depend on the specific API and its functionalities. Just remember that leveraging existing code is a trend across the industry, and excessive conservatism may drop you behind MyAppoly competitors.
Making an API Available
Imagine that MyAppoly allowed its users to match random handwriting samples to specific individuals. You might be building this tool for forensic investigation, electronic payment verification, historians, amateur genealogists, or holograph archivists. One way you could make all of these use scenarios possible simultaneously would be to release a MyAppoly API. The advantages of releasing a MyAppoly API include the following:

Scale Client Integrationy. If ou want to give different potential partners access to your data or functionality, it would be a pain if you had to build a custom solution for each one. Why not standardize the interface in the form of an API and let your partners access what they want within the parameters you provide?
Others Dependent on You. If your API becomes a critical part of the operation of several companies, you have generated an additional source of value for your company. Others rely on you. Think about all of the third-party applications that are built on top of Facebook. If Facebook were to disappear, imagine how many other companies would collapse. This undoubtedly supports Facebook's valuation and success.
Variety of Distribution Methods. Users are accustomed to getting exactly the information they want in the exact way they want it. Providing data or functionality on your website alone often times does not cut it anymore. An API can make the same information that you already present on your website available to people who want to consume it in a different way. By providing this alternate access method, you are increasing your market.
Revenue Source. If you decide to charge others for using your API, this could represent a substantial source of revenue.
More Data. Every time someone makes a call to your API, you can record that information. This data might eventually help you identify trends in the industry or opportunities for new features.
Brand Value. Even if you choose to make your API available for free and it is widely used, the value you generate will be returned to you in the form of brand and reputation. It can help establish you as a leader in your field, and a proponent of knowledge sharing in an information era.
You must also consider the disadvantages:

Resources. APIs may be complicated to build, maintain, and support. It requires API programmers to think about abstraction and to anticipate how the API might be used. Even after the API is built, it must be maintained. If you give data through your API, for example, and you expand your data set of holographs with audio files, you have to update your API correspondingly. Last, developers trying to integrate your API into their applications might require support. An API without support will lead to frustration and eventual abandonment. If you want to make an API available, be forewarned that it can be a resource-intensive process.
Security. An API allows users to gain access to your data. This portal might expose vulnerabilities whereby other information you do not want to share can be improperly accessed. The proliferation of APIs suggests, however, that these risks are avoidable or manageable (Chapter 12).
Cost. Every time that a developer uses your API (by making an API "call" or "request") requires MyAppoly to take action. Who pays to process these requests? You do! In case users start making millions of requests, you need some sense in advance of how you are prepared to pay to process them.
From the accelerating explosion in the number of APIs—from a mere handful in 2005 to more than 10,000 in September 2013—you may safely infer that MyAppoly will need an API to be a player. The success of your API will depend on how easy it is for developers to learn, use, and integrate.
How Do APIs Work?
The technicals underlying APIs can get complicated, but the basics are simple. First, it is important to emphasize that APIs are software-to-software interfaces. Applications talk to each other without your involvement. If MyAppoly needs to show the weather, for example, the software can communicate ("make calls") with a weather API in the background to obtain the most updated weather report before showing it to the user, as detailed in the following example.
One way you could retrieve the weather would be through the weather website (direct access). Alternatively, you could go through a middleman—an API—that stands in between you and the weather website. Now instead of accessing the weather directly through a browser, the API tells you that you have to submit whatever you want via code. But how do you know what to write? The API provides documentationthat tells you what the valid requests are and how exactly to write them. (The API is picky and only processes your code if you write it in the precise way it understands). So, after reading the API's instructions and rules, you specify that you want the weather by writing the getWeather function outlined in the API's documentation. Now when you give the function to the API, it turns around and interacts with the server to retrieve your information. The API then returns the results to you in a format that easily digestible (typically XML or JSON, discussed in the "JavaScript Object Notation" section of this chapter). Depending on what the API allows you to do, you could theoretically get all the information you want through the API and never have to visit the website again through a browser. It's not all about the data though, so do focus on design and user experience. Plenty of your visitors will expect your actual website to look good even if you do offer an API. APIs that extend functionality in addition to data operate in a similar fashion.
From this example, you can see that APIs are designed for software developers and can be defined as "a set of routines (usually functions) and accompanying protocols for using these functions that provide building blocks for software development." They allow users to connect to the application via a secure channel and then run functions using code to get information or to borrow functionality.
Two of the more popular types of APIs—Representational State Transfer (REST) and Simple Object Access Protocol (SOAP)—are described and compared in the next sections.
REST
REST is an architectural style developed by Roy Thomas Fielding to allow the Web to be more scalable. To follow REST's architectural style and be considered RESTful, one must follow a set of conditions that are not presented here but are easily found online. Every request has two parts: the endpoint, or URL, and the message that contains the request. This REST request is sent to a special URL using the GET method described in Chapter 1. This request consists of a few parts, including the endpoint URL, a developer ID or "key" if required (this helps the API keep track of who is making requests and how often), the desired action (e.g., getWeather), and parameters that give more information on the request (e.g., getWeather for today and yesterday only). All of this information is encoded in the URL and sent to the API via GET. The API then decodes the request, interacts with the server to complete the desired action, and then returns a response to the user.
SOAP
Started as a Microsoft Initiative in 1997, SOAP is another way to request data from remote servers using HTTP. Whereas with the REST API the specifics of the request are sent encoded in the URL, with SOAP the specifics of request itself are contained within the body of an XML document. The XML document that makes up the message has to satisfy the specifications outlined in a Web Services Description Language (WSDL) file. Ultimately, the message is sent via POST or GET, but here again, the specifics of the requests, such as the desired action, developer key, and parameters, are contained within the message. In a way, SOAP is like sending a request in an envelope (the exact request is hidden) whereas REST is sending a request on a postcard (viewable by all). Both travel via mail, or HTTP, though. As can be the case with REST APIs, the response is an XML object.
Comparing REST and SOAP
REST and SOAP present the following points of comparison:

Overhead. Because REST requests are entirely contained in the URL whereas SOAP requests are contained in a document, SOAP requests tend to be larger, to require more overhead, and to contain a lot more information in different parts of the message that further define the request.
Transparency. To continue the postcard and envelope analogy, everything is out in the open with REST and hence monitored more easily, whereas only the address (endpoint URL) is viewable with SOAP.
Ease of use. REST requests tend to be more simply generated. This comparison is corroborated by the observation that 85% of Amazon's requests come from REST APIs.
Development time. REST APIs are typically quicker to develop than SOAP APIs, contingent on your individual programmer's expertise.
Flexibility. Because SOAP uses XML documents as messages, more information can be included in SOAP requests than in REST requests, giving SOAP APIs greater flexibility.
Authentication
As discussed in the opening sections of this chapter, you might want to keep track of who is using your API and what they are requesting. You also might want to communicate with your API users to confirm the accuracy of the data and security of the connection. Approaches to authentication include the following:

Open API. No authentication is used. There are no barriers to use the API, the code can be distributed more freely, and there is less hassle to manage provided you do not need to keep track of individual users. This would be an attractive option if you are interested only in what people are doing anonymously in aggregate and not in what specific individuals  are doing or controlling who uses your API and how they use it.
HTTP authentication. Authentication information can be passed in the HTTP headers of incoming requests
Message-based authentication. Credentials such as a unique developer's key can be passed in the API request as part of the message.
SSL endpoint.This method helps the client keep track of the server. After receiving a server certificate, the client can check to see if it changes. If it does, the information is likely coming from a source other than the server and therefore cannot be trusted. This authentication method prevents a middleman from impersonating the person you are expecting. Because the SSL endpoint method does not help identify the client, it is used in conjunction with one of the other methods listed here.
Client-side certificates.You can configure your API to create certificates that are given to a client (via a secured channel) that would reappear and be authenticated every time the client makes a request. Although this is considered a robust way of dealing with authentication, it can be considerably slower.
JavaScript Object Notation
Another formatting language besides XML (Chapters 4 and 5) that you are likely to come across is JavaScript Object Notation (JSON),which uses a collection of name/value pairs that can be nested. For example, a book can have several name/value pairs. A book (object) can contain many name/value pairs that describe its author, title, and year published, for example. JSON is widely available online because many APIs "return" (deliver the response in) JSON rather than XML. There are different ways of structuring the same information.
Feeds
Feedly, Google Reader (R.I.P.), and Flipboard are probably familiar to you as tools used to aggregate content across your favorite newspapers and blogs. How do feeds work? Essentially, feeds are no more than XML documents that are passed from one party (e.g., a blogger) to another (e.g., the news aggregator that displays all the content for you on a nice-looking web page). Feed creators, such as newspapers, bundle the articles they want to share into a document formatted in XML and upload it online automatically with code. Whenever a user accesses a feed, they download XML documents and view them through the reader of their choice. It is a way for users to obtain the content they want from websites without copying it manually or scraping it themselves—that is, deploying a customized script by which uses programmatically copy content from webpages. Feeds allow their creators to make information available for consumption in yet another format. To publish a feed, you must conform to one of two forms, Rich Site Summary (RSS) or Atom. RSS, developed by Netscape, is the more popular.
To consume a feed, a viewer simply accesses the URL of the feed to request the information (your news aggregator probably does this for you when you click refresh), which is ultimately sent through a process that returns an XML object with the requested information. To distribute a feed, a script automatically updates the XML document and makes it available to users who request it.
Many tools exist to help you produce a feed, but the issues for you to think about before publishing a feed include the following:

Security.When you make information available, there is always the possibility that other information could be accessed (Chapter 12).
Legal.Whenever you are distributing information, be sure to check the legal rules surrounding constraining its use and dissemination.
Update frequency.The frequency with which you update the XML content should be closely monitored because it can contribute to costs as users start to access feed at scale. Let's say MyAppoly is a news publisher. Every time a user wants to get the latest news content from MyAppoly on their news aggregator, a script could programmatically go to the MyAppoly database, access all of the most recent news stories, create a new XML file with the content, and update the feed. At that point, the news aggregator would download the new file through the feed, and make it available to the user. This can be a very costly process if MyAppoly is the size of The New York Times. One way to reduce your cost exposure is by caching (Chapter 3). Rather than updating the XML document every time a user asks for your feed, update it more sparingly to save costs. Feed-side caching handles this on the side of the feed script. A method called timestamp-based caching checks the existence and currency of the timestamp and returns the cache to the user or runs a script to obtain the more current XML document. Other methods include updating the XML document at specific intervals or intermittently based on traffic or some other metric. Another way you can cache is by generation-sidecaching at the time of feed generation, which allows you to use scripts to automatically update the cache whenever new content is added.
Usefulness to competitors.Not everything is suitable for feed distribution. Remember that your competitors can access the feed as well!
Libraries
A library is a collection of code that defines commonly used functionality (such as dropdown menus or animated pop-up windows). For example, you might create a library of math variables and functions. You can share your library so that anyone else who wants to use math in their programs can import your shared library and save a lot of time. Shared code and libraries reduce development time and the number of developers needed.
On the other hand, how much do you trust the reliability of your system on someone else's contributed software? There are no right and wrong answers to this conundrum, but your ultimate decision definitely requires good judgment in view of and on behalf of your team of programmers.
Open Source
Related to the spread of information through APIs, feeds, and libraries is this growth of open-source projects. The definition of open source makes more sense when you consider its interesting history.
Though hard to imagine, there was a time when all software development resided with researchers and was developed at universities; the finds could be shared without any limitations. All research was for public good, after all. During the 1960s, corporations such as IBM and others borrowed this custom: when they released their first large-scale commercial computers, they gave away their software for free. With developments in technology, the arrival of competition, and the falling price of hardware, companies saw software as an alternate revenue source that could support its business. Software became commercial proprietary property and any reproduction without the company's knowledge was illegal.
Richard Stallman at MIT was very upset with this change in culture. Concerned that his new project and its distribution would be dictated by MIT, Stallman left MIT to continue working on his GNU software, a computer operating system made of entirely free software (such as a free version of Windows or Mac OS). As Stallman developed, his opposition to commercialization and his advocacy for equal access inspired others. In 1985, he founded the Free Software Foundation (FSF) to support the GNU project. The FSF promulgated the General Public License (1988) to protect free use and free access under the doctrine of copyleft. Rather than protecting a property's rights (copyright), "copyleft protected the freedom to copy, distribute, and change a product."
With the declaration of the GPL, the commercial view of proprietary software was challenged, and in the 1990s there emerged a misunderstanding over the word free. The "free" in FSF pertains to freedom, not to its price.
According to the FSF definition, "a program is free software if the program's users have the four essential freedoms:"2

The freedom to run the program, for any purpose (freedom 0).
The freedom to study how the program works, and adapt it to your needs (freedom 1). Access to the source code is a precondition for this.
The freedom to redistribute copies so you can help your neighbor (freedom 2).
The freedom to improve the program, and release your improvements to the public, so that the whole community benefits (freedom 3). Access to the source code is a precondition for this.
Through its public education efforts—FSF used such slogans as "free speech, not free beer"—FSF tried to correct the misperception that it was against commercial software.
Recall from Chapter 3 that the code that makes up a program is called the source code. To be read, understood, and altered, source code must be accessible to the public—hence the term, open source. Access to source code, however, is a necessary but not a sufficient condition for a program to be considered open source. Founded in 1988, the Open Source Initiative (OSI) promulgates "The Open Source Definition" as follows:3
Open source doesn't just mean access to the source code. The distribution terms of open-source software must comply with the following criteria:

Free Redistribution: The license shall not restrict any party from selling or giving away the software as a component of an aggregate software distribution containing programs from several different sources. The license shall not require a royalty or other fee for such sale.
Source Code: The program must include source code, and must allow distribution in source code as well as compiled form. Where some form of a product is not distributed with source code, there must be a well-publicized means of obtaining the source code for no more than a reasonable reproduction cost, preferably downloading via the Internet without charge. The source code must be the preferred form in which a programmer would modify the program. Deliberately obfuscated source code is not allowed. Intermediate forms such as the output of a preprocessor or translator are not allowed.
Derived Works: The license must allow modifications and derived works, and must allow them to be distributed under the same terms as the license of the original software.
Integrity of the Author's Source Code: The license may restrict source-code from being distributed in modified form only if the license allows the distribution of "patch files" with the source code for the purpose of modifying the program at build time. The license must explicitly permit distribution of software built from modified source code. The license may require derived works to carry a different name or version number from the original software.
No Discrimination Against Persons or Groups: The license must not discriminate against any person or group of persons.
No Discrimination Against Fields of Endeavor: The license must not restrict anyone from making use of the program in a specific field of endeavor. For example, it may not restrict the program from being used in a business, or from being used for genetic research.
Distribution of License: The rights attached to the program must apply to all to whom the program is redistributed without the need for execution of an additional license by those parties.
License Must Not Be Specific to a Product: The rights attached to the program must not depend on the program's being part of a particular software distribution. If the program is extracted from that distribution and used or distributed within the terms of the program's license, all parties to whom the program is redistributed should have the same rights as those that are granted in conjunction with the original software distribution.
License Must Not Restrict Other Software: The license must not place restrictions on other software that is distributed along with the licensed software. For example, the license must not insist that all other programs distributed on the same medium must be open-source software.
License Must Be Technology-Neutral: No provision of the license may be predicated on any individual technology or style of interface.
Around this time Red Hat was founded as an organization solely dedicated to improving and educating others on how to use Linux, an open-source computer operating system. It represented for the first time a company whose entire business model was focused on copyleft products. Open-source systems began challenging the position of large software corporations, including Microsoft.4
Since then, many other open-source licenses have been created, such as the Mozilla Public License by Netscape. The growing culture of open source is represented today by about fifty specific OSI-approved open-source licenses divided broadly into the following two groups:

Academic licenses. Under these licenses, universities distribute their research to the public and allow their software and source code to be used, copied, modified, and distributed. The Berkley Software Distribution (BSD) license is the archetype and represents the "no-strings-attached" approach. All code licensed under academic licenses gets added to a general pool of code that can be used for any purpose, including the creation of commercial software. You can borrow code from this pool and never contribute.
Reciprocal licenses. Under these licenses, code is entered into the pool of open-source software. Anybody can use the code from this pool for whatever purpose, but if you choose to spread your newly created code, it must be distributed under the same license. In other words, you may leverage this code to produce something new, but you must contribute your new creation to the pool of code for others' use. GPL is an example of a reciprocal license.
Many people have concerns about the quality of open-source code. One rubric for evaluation consists of reliability, performance, and total cost of ownership. Other metrics used to evaluate quality include portability, flexibility, and freedom. Whatever metrics you select, the key point is that the many advantages of leveraging existing code are offset by the risk inherent in relying on others' code and systems. Nonetheless, several open-source projects have attained the same complexity levels and quality standards as commercial software.
Conclusion
With a more in-depth understanding of APIs and the other ways you can leverage existing code, your team sketches out a full development plan. The use of a few key APIs and shared libraries helps accelerate the schedule, but the team has planned to slowly shift away from using one of them because it does not give them the full range of functionality they need for MyAppoly. They ruled out several other contributed libraries and tools because of their reported poor performance and unreliability, thus your team seems to have exercised sound judgment. There is a balance to strike between doing things in-house versus outsourcing, and now that you have made the decision, your team is off building. But how will they work together to build it?
1 This chapter draws adaptively on schemata in Paul Reinheimer, Professional Web APIs with PHP. Wrox, 2006.
2 "The Free Software Definition," www.gnu.org/philosophy/free-sw.html, 2013.
3 "The Open Source Definition," http://opensource.org/osd, n.d.
4 See, for example, Vinod Valloppillil, "Open Source Software," Microsoft Memorandum, August 11, 1998. Available at http://catb.org/∼esr/halloween/halloween1.html#quote8.







CHAPTER7
Software Development: Working in Teams
Too many cooks spoil the broth. This adage applies to web development unless your engineers are organized in the right way.
You spent the last few weeks assembling a small team of qualified and ambitious engineers. They understand how the back end, front end, and database interact and are ready to start programming MyAppoly. How will they work together? Can two of them work on the same file at the same time? How will one engineer understand what the other engineer did? Is there a way for the engineers to work ahead on one part of MyAppoly even though another part may not have been built yet?
Such questions about collaboration and organization are important for any technical endeavor. Various design principles and tools support team-based coding, making it faster and safer. To interact with your engineers, you need to understand the terminology of these principles and tools—particularly version control systems.
Documentation and Commenting
The simplest of ways for engineers to make it easier for others to understand a program is to comment the code of the program. Every programming language allows programmers to write comments next to particular lines or blocks of code to explain how the code works or what it does. "Changes the user's password" or "Adds user to the database" the team, and old programmers will switch roles. Your CTO may even review code that other engineers write. Comments and, by extension, documentation could be a couple of the millions of comments in Facebook's code base. As emphasized in Chapter 3, the readability of code is just as important as writeability, especially when you think about long-term code maintainability. New programmers will join are very important. In this context, documentation refers to the set of descriptions and instructions for a process or program, the purpose of which is to help people learn how to use or understand something more quickly. If your head engineer for MyAppoly builds a new feature that allows the application to process credit card payments, he or she might want to write up a few paragraphs explaining how the feature is integrated with the rest of the program, how the code works, where the code is saved, how the logic works, what API he used, and so on.
In addition to these external notes, style of coding also matters greatly for teams. By style I mean naming conventions (how you name variables in the program), use of indentation and whitespace, and structure of comments, among other things. Imagine code that asks a user for their name and then saves it in the database. This code could all be written on one line, or someone could break it apart to make it go across three or four lines. One engineer might indent the text and call the variable that stores the user's name "name," whereas another engineer might call it something apparently random like "var293." One is obviously more easily understood than the other. These coding styles are important; thus setting out a standard style guide for all engineers to use is a common practice. If these principles are followed, all formatting and labeling will be consistent across features and it will be easier for new engineers to collaborate and review code.
Program Architecture
There are several ways you can organize MyAppoly's code. In Chapters 3 through 5 you learned how code variously performs the program's logic, constructs what the user sees, and inserts and deletes data from databases. However, if all this code were lumped together, the program would be too slow and unmanageable. Imagine if all of a program's code sat in one file (putting aside for a moment whether this is actually possible). If two engineers wanted to work on it, how could they do so? This is one of many concerns that would be present if this were how you chose to structure MyAppoly.
Thankfully, web programming is a mature endeavor, and smart people have developed more efficient ways to organize code. These design principles not only improve efficiency and code performance but also conduce to better collaboration and code maintainability.
One of the most common categories of organization is the multi-tier application architecture. The most widespread of these is the three-tiered architecture. The three tiers correspond more or less to the subjects of Chapters 3-5. One tier concerns all things connected with the database, often called the model (Chapter 5). The middle tier is the application processing tier, which contains the logic of the program and relates to the back end of the program (Chapter 3). The last tier is all things that a user sees: the view or presentation (Chapter 4).
In this architecture, the relationship is linear in that the presentation (or view) and the model do not interact directly but through the logic layer. Another popular architecture that many web frameworks use is model-view-controller (MVC). MVC has the same three components but is not exactly three-tiered like the architecture just described. In MVC applications, there are interactions between the view and the model, so MVC is likened more to a triangle. Several other architectures exist, but rather than go into the details of how each one is different, it is important to understand why these architectures exist in the first place.
First, they support concurrent work (this design principle is often called separation of concerns). One engineer can work on the view or presentation of MyAppoly while another works on the model. The controller engineer can tell the model engineer, "I don't care how you do it, but I need the first name of the user." Thus, the view engineer and controller engineer can continue coding assuming they have some variable that contains the name of the user, and they don't have to worry about the code that physically goes into the database to get the name. The model engineer can provide the other two engineers with a simple description: "If you want the name, call the function getUserName. If you want the user's password, call getUserPassword." The model engineer can then code, in whatever way she wants, the database interaction. From this example, you can see that the three parts of the application can be developed relatively independently, because each engineer does not need to be concerned with the specifics of how the other parts are coded.
Second, this setup supports maintainability. Imagine that the controller engineer tells the model engineer he needs a few things. He needs a user's name, address, and ZIP code. The model engineer sets off to work and writes up three functions to get these pieces of data from the database and programmatically gives them to the controller engineer, who can then manipulate them before passing them off to the view engineer to display to the user. Three months later, it is found out that the model code is just too slow. It is really inefficient in getting information from the database for some reason. Luckily, the view and controller engineers do not have to do anything. The model engineer can update his or her functions to improve performance, and nothing else in the application has to change. If the team were not using the MVC architecture, this database code could be in any file. It would be a lot harder to hunt down every database-related code and change it.
A third reason one might choose to use such an architecture is code reusability. Imagine if the view engineer wants to show the user's name on every page. The inefficient way of doing this would be to rewrite the database code that gets the user's name on every single webpage of the whole site. Instead, the view and controller engineers can call the same function getUserName that goes into the model and executes the query to get the name. This abstraction enables engineers to be more efficient and economical with their code.
The sum of these three reasons is clear: shortened development time. Products can be built and maintained more quickly, and precious engineering time can be dedicated to more value-added activities.
Revision Control
The architecture of a program, as discussed in the preceding section, helps separate functionality so that multiple engineers can work on a given project at the same time. But this is not a complete solution. How does a team of engineers effectively coordinate their programming efforts? How does one person notify the team that he or she has updated a particular file? Do they e-mail newly updated files around? If you are an engineer and you are about to update a file, how do you know that you are working on the most up-to-date version? Perhaps you introduced a bug or an error into the code, and you want to go back to an older version. How do all of these frustrations get resolved? Revision control (also known as version control or source control ) refers to the management of changes to a document, computer program, music file, and so on. Some sort of revision control system like Github is probably what your team will use to solve these problems.
Taxonomy
Before getting into the terminology of revision control, note that—just as in the case of programming languages (Chapter 3)—creating mutually exclusive categories for revision control is hard. Many elements are shared across the boundaries of the groups discussed here, and some revision control systems have features of multiple groups.
Conflict Resolution
The two dominant ways to help resolve conflicts when multiple engineers are collaborating on a given project are file locking and version merging.
File locking is a method of preventing concurrent access to a given file. Think of a standard document that contains an essay. If two people edit the essay simultaneously, how do we determine whose edits are recorded? Can they be merged? File locking gives "write" privileges (the ability to edit a file) to the first person who opens the file. To all other users who then try to open the file, it will be locked, available only for viewing ("read" access).
The benefit of file locking is that it avoids complicated problems with merging two sets of edits to the same file. When Engineer 2 opens a file already being edited by Engineer 1, he or she will be notified. There are drawbacks, however, to this solution. Engineer 2 has to wait for Engineer 1 to finish editing, introducing delay in the process and slowing down development. Imagine, in a worst-case scenario, that Engineer 1 called out sick for a few days after opening a file. Not only would Engineer 2 be locked out of the file, he or she would need to get a higher authority to force-unlock the file, a process called file unlock. Last, another drawback is that this system might give a false sense of security. Two files that depend on each other could be locked by different engineers and altered in such a way that their compatibility is compromised. In other words, just because one file is locked does not mean other files are also locked and overall functionality preserved.
The other dominant conflict resolution method, version merging, allows a single file to be edited by multiple users. After several engineers finish working on the same file, the system helps merge the changes. When the engineers all work on different parts of a particular file, merging is easy. When two engineers change the same part of the file in different ways, there is a conflict that has to be resolved manually. Though conflicts sound hectic, many feel this type of system works more smoothly because there is less wasted time. As long as engineers do not work on the same parts of files, few conflicts emerge and the merging process moves seamlessly. The process works well for text-based documents, because text can be easily compared and merged. This does not work well for music or art files.
Centralized versus Distributed
Revision control systems tend to either be centralized or distributed in nature. With centralized systems, all of the code is stored on a central server, called the main repository. Whenever an engineer wants to open a file or save a file, he or she has to be connected to the network to access the central server. This model is referred to as a client-server model because the engineers who access the central server are called clients and any number of clients can exist. Although a full copy of the code can exist on a client's computer, the history of all the changes is kept on the main server. Owing to the engineer's dependence on this central server, there is a single point of failure. If for whatever reason the central server goes down, the code and its history are either lost or temporarily unavailable.
In the distributed model, multiple copies of the entire code base and history (repository) exist on the machines of the engineers who are working on the project. They can open and save files when not connected to a network and can perform functions more quickly. Changes can be merged across engineers who have a repository ("repo" for short), although a distributed system can also be used as a centralized server. Every local copy (or working copy) is in effect a backup. There has been a significant shift to more distributed models for these few reasons.
Version Control Terminology
Let's imagine that MyAppoly has a few features that need to be built.1 Because this is a fun startup, let's call these features by random objects you find in the grocery story (Figure 7-1.). Code the first one up and call it milk. The next one is egg, and last is juice. However, notice that juice has some problems because certain users do not like it, and so the team alters the feature and renames it soup.

Figure 7-1. Basic checkins
Figure 7-1 shows what the repository looks like over time. Every time a new feature is added, the new files are "added" into the repo for the first time. Version r4 is known as the "head" of the repo because it is the latest version. Every time a file is uploaded, it is "checked in"; and every time a file is downloaded, it is "checked out." Figure 7-2 represents the passage from version r3 to version r4.

Figure 7-2. Checkout
After someone checked out version r3 (at the time that r3 was the head), she could have either reverted her changes back to r3 (meaning she discarded what she did), or checked in her work and updated the repo. Even after she checks in her work and creates r4, there is still the option to roll back to r3, as shown in Figure 7-3.

Figure 7-3. Diffs
Figure 7-3 shows a powerful feature of version control. Rather than storing the entire prior version of the application, a version control system stores the differences (diffs for short) and uses these to re-create prior versions. This means it needs to save fewer things, which allows it run more quickly.
Imagine that you hire an engineer to work on a new feature called rice. This is purely a test feature, so you do not want him checking in code that might get entirely discarded. You create a separate copy of the repo, called a branch, as shown in Figure 7-4.

Figure 7-4. Branching
After the new engineer finishes coding rice and you finish coding the new feature bread, you realize that rice is actually very good. A few users try it, and they want rice to become a main feature. Now you need to merge these changes together, as shown in Figure 7-5.

Figure 7-5. Merging
Things worked well in this example because the creation of rice and bread did not overlap. In other words, they did not affect the same files. What if the same file were edited by multiple users? It becomes harder to determine which edits should be prioritized over others. In this case, there is a conflict that can only be resolved manually. An example of a conflict is shown in Figure 7-6. To prevent conflicts, a merging system can employ the file locking technique described in the "Conflict Resolution" section.

Figure 7-6. Conflicts
Another feature of version control systems allows you to tag, or label, different versions of the application. This is helpful if you want to quickly access a prior version of the application that you know works really well or was the one you launched publicly. Rather than having to remember tags as r1 or r9, you can apply names to the versions worth remembering, as shown in Figure 7-7.

Figure 7-7. Tagging
Benefits
A revision control system such as Git supplies the following benefits:

Your team has solved the issue of reversibility because, by comparing diffs across files, they can reconstruct any prior version of the application they want to.
With diffs, your teammates can quickly see the last changes to any given file.
Tagging and other comments give more description to describe and define certain versions of the code.
Thanks to the sophisticated way these systems merge files, issues of concurrency are also solved. There should be little downtime as a result.
Files are managed properly without having to find a solution to send updated copies back and forth manually.
Because checkins are associated with a name, there is more ownership and accountability over contributed code.
Revision control provides a means for testing new features on a separate branch, to be merged or rejected at a later date.
Conclusion
Programming in teams can present several obstacles. With the proper style guide, program architecture, and version control system in place, teammates will be able to collaborate more effectively to meet the schedule for launch. Next time someone tells you too many cooks spoil the broth, tell them to get Git.
1 This section closely follows the exposition of Kalid Azad in "A Visual Guide to Version Control" on BetterExplained.com, http://betterexplained.com/articles/a-visual-guide-to-version-control/. Figures 7-1. through 7-7 here are reproduced courtesy of Kalid Azad.







CHAPTER8
Software Development:  The Process
In 2011, having spent the previous three years advising technology startup companies and entrepreneurs on how most efficiently to organize and allocate their resources and design their processes,  Eric Ries set out his methodology in the book The Lean Startup, whose release was a catalytic event in the spread of the Lean Startup methodology.1 The philosophy of this movement—which articulates and advocates an efficient, continuous, scientific, and feedback-driven approach to product development at technology startups—informs this chapter.
Your CTO says that MyAppoly must have a well-defined process for developing the product. Should you aim to release a new feature once a month? Should someone build and test a new feature before showing it to the team? What should the flow of software development look like, and how can it be managed? This chapter does not directly relate to technical aspects of a web application, but any process of developing software must account for the technical complexity and unpredictability that exists in software production. A high-level understanding of how most startup teams work—or any software development team for that matter—is important. Your team wants to educate you on such things because they believe it will make you a better manager. They will tell you about how they want to design the software development process (the way the product will be built). Without an agreed-on process, development can become expensive, chaotic, and disorganized and end in failure. Luckily, your engineers know what they are talking about.
The Waterfall
In the beginning, there was a waterfall—not in the sense of going over Niagara in a barrel but in the sense of a general process of project production, as might be applied to building a house. The waterfall development model consists of distinct stages that are sequential. Let's say you consider buying a house. Congratulations! You are picky and cannot find anything on the market that you want, so you decide to build your own.
First, you sit down with an architect to begin the process. You define your requirements up front, including the specifics of the construction timeline and details of the house, such as materials to be used and the number of bedrooms and bathrooms you want. After these requirements have been fleshed out and the particulars are analyzed for their feasibility, you work with the architect to put together a specific blueprint of what the house will look like. This sketch forms the design phase. Only when this is finalized does the architect collect his fee; then you gather a construction team and start implementing the plan. Your house is officially under construction. The general contractor tells you to come back in ten months when the entire house will be finished. You spend that time doing what you normally do, knowing that your house will be ready in about a year. When it's done, you visit the property and take a tour of the finished product. In a way, you are testing it to make sure it was built to specification. Assuming there are no gaping holes in the walls or missing staircases, you will hopefully move in to the house and invite me to your housewarming party.
From this mundane retelling of what in actuality is a large life-event, you can tell that this development process followed a waterfall model, the stages of which are the following:

Requirements
Design
Implementation
Verification
Maintenance
This model is sequential owing to the nature of the project. Home construction is predictable, and it has been done before. First, inputs are known, and outputs are expected after a given amount of labor. Second, such a strict sequential order is necessary because the project becomes incredibly expensive if you backtrack. If, for example, in the implementation phase of construction you wanted to change the layout of the house, imagine about the cost of demolition, redesigning, and then reimplementing. For this reason, the waterfall model recommends that every detail of the entire project is confirmed before moving on to the next stage.
A More Appropriate Approach
If you have been exposed to any software development, you know that such a granular level of predictability (knowing every detail of the whole project) is never possible. Who knows what new technology will be out in a few months? Maybe we will want to use that. What if the aspects of the code do not exactly work the way we expect them to? We might have to engineer it a different way. What if we find out that our initial page layout is too confusing? All of these issues happen regularly, so a long-term and sequential process cannot necessarily be applied to software. Additionally, unlike home construction, there is room for more rapid iteration. Software typically does not require the same capital commitment as home building. Therefore, characteristics that supported the waterfall model no longer apply to software. So your team asks rhetorically, "Why apply the waterfall model to MyAppoly?"
Iterative and Incremental Development
Emerging in the 1960s, a more iterative and evolutionary model called iterative and incremental development (IID) was accepted by many software developers. It broke down the long-term waterfall model into small mini-projects, called iterations. The entire construction of MyAppoly could be broken down to all of its features. This list could include the ability to transfer money, upload photos, add friends, export data to a spreadsheet, or pretty much anything you can imagine. These iterations, when viewed together, form the whole project, but each iteration is a complete stand-alone aspect of the end goal. The export to spreadsheet feature should be independent (from an engineering standpoint) from the photo uploading function. Each incremental iteration adds something new to the existing product. "Tune-up" iterations also exist to enhance older features.
There are many philosophies of development that fall under the umbrella of iterative and incremental, but they all tend to recommend that the length of an iteration be somewhere between one and six weeks. These iterations have deadlines, a practice called timeboxing. Although these deadlines are not supposed to shift, they are not intended to be used to pressure developers. If it seems like too much is on an engineer's plate, this process recommends reducing the scope of the iteration rather than shifting the deadline. If the feature was supposed to allow users to upload photos and videos, a reduction in scope could remove the video functionality to meet the deadline.
According to most IID processes, the team members working on the iteration make that decision alone, without the urging of external players. In 1955, Cyril Parkinson articulated his eponymous law, which states that "work expands so as to fill the time available for its completion."2 By Parkinson's law, if you have one week to do something that you really know will only take two days, the entire iteration will still take one week because you will be unproductive. Timeboxing is designed to prevent such procrastination.
Rapid iterations are needed to accommodate the unpredictability that defines software development. You can iterate on a single aspects multiple times, and so this is not as rigidly sequential either. If you were to apply this to house construction, an iteration might consist of a single bedroom. Subsequent iterations could add different bedrooms based on regular feedback on the prior iterations. If you wanted to redo a particular bedroom, you could do that as well.
One concern might be prioritization. If someone breaks apart a large project into several iterations, which ones are tackled first? IID processes typically have two approaches to prioritization: risk-driven and client-driven. The risk-driven approach recommends working on the most risky parts of the project first. These iterations could consist of the most integral parts of the application or the ones that are most complex and likely to fail. By building these first, you can be slightly more confident that the backlog of iterations remaining to implement will not compromise the future of the entire application or business. The client-driven approach, as the name suggests, says that customer feedback and demand should influence prioritization. Such feedback-driven decision-making is considered evolutionary and adaptive in nature, because it demonstrates that the overall process is not preordained. Preordination was the very thing developers ran away from when the waterfall was abandoned. No company is going to blindly commit to one prioritization strategy over the other exclusively, but they are outlined here to help frame the discussion within your own team.
There is a difference between delivery and iteration: not every iteration has to be delivered (made available for use by your customers). You might want several features to be built before releasing a new version, for example. With that being said, feedback from your deliveries might inform future iterations and deliveries, and so it's probably not the best to start hoarding your features to drop on your customers all at once.
Two well-known IID methods are Unified Process (UP), developed in the 1990s, and Evo, which dates back to the 1960s. Each prescribes slightly different parameters (different lengths of time for timeboxing or different methods of prioritization), but they follow the same overall philosophy.
Agile Development
Agile is another one of those words that you are bound to hear when you get involved in the tech world. Agile development, like the IIDs already mentioned, is a philosophy of organization and management more than it is a technical aspect of the actual web application. Some say it is a type of IID, others say it borrows from but significantly expands on IIDs. Whatever your standpoint, one fact that we cannot deny is that almost anyone and everyone wants to say they have an agile development process. From Google and Microsoft and Facebook to MyAppoly, from the titans to the up-and-coming web startups, all follow the life of agile.
Agile development displays many of the qualities of IIDs. The product backlog consists of all the things yet to complete, hopefully organized according to some prioritization methodology. The teams work on a few iterations at a time, and when complete the iterations will be released to users for feedback. This feedback adds and reprioritizes items in the product backlog. As several authors on the subject will point out, agile does not refer to the enhanced speed with which your team will build the entire application, but the philosophy that describes the methodology of building it. It holds four concise principles as core to its philosophy:

Individuals and interactions over processes and tools
Working software over comprehensive documentation
Customer collaboration over contract negotiation
Responding to change over following a plan
Agile development emphasizes team dynamics and the overall work environment. Rather than outline characteristics, I describe them from my own experience of agile culture at a startup. Agile development recommends group collaboration and open spaces, so you will often find work environments to be big rooms where everyone shares a large work surface. There might be no cubicles because such separation introduces rigidity and obstacles to creativity. Whiteboards are plentiful so that self-managed teams can sketch out ideas, interact with their thoughts, and photograph them for future use. These brainstorming sessions are meant to be simple, as simplicity is encouraged in all aspects. No need for complicated slide shows and spreadsheets to demonstrate a point. Instead, jot it down on the wall, take a photo, and send it around. No time should be wasted doing things that do not ultimately add value.
These companies are characterized by self-managed teams, where authority is shifted from the manager to the group. These teams brainstorm ideas for the next iteration, delegate their own tasks, tackle them in groups, reconvene to check on progress, and hold each other accountable. With the risk of getting overly sociological, older management beliefs said that people did not like work, avoided responsibility, and could only be motivated by negative incentives. This is thrown out the window here. Agile development has roots in more natural management styles, wherein the belief is that people feel empowered in teams, enjoy tackling hard problems, and love coming to work. Assuming the environment is right, the team will naturally be more productive and deliver a better product. Rather than dictating what must be done, project managers are there to provide the resources, maintain the vision, remove the impediments, and promote the principles that favor the team's adoption of the agile culture at the core of agile development.
The two most common agile development models are Scrum and Extreme Programming (XP). I do not discuss the specifics of these models because they can be easily researched, but they both embody the four agile principles.
Benefits
Regardless of the admittedly biased way I presented IIDs and agile methods compared to waterfall, the benefits of the popular process approaches should be clear. Rapid iterations allow for teams to refine their way of coding and working together, thereby improving efficiency. They help teams manage complexity, which is highly relevant for many applications, even though they may seem simple on the surface. This ability to incrementally add features and revisit old ones lowers the overall risk and delivers versions of the product more quickly to the customer. The customer loves a product that regularly improves, and the company should, too, because it means you can collect feedback and use it to inform your product, which in turn makes the experience better for the customer. Your team will feel empowered that they are delivering changes frequently to the customer, and they will be motivated to work together in teams. Their buy-in leads to greater productivity, and in the end, the value accrues to the business and the customers. This is the end goal of any development process, and agile development methods are gaining popularity because they are doing just that.
Release Management
When your team plans to deliver their latest code to the customers, how exactly is that process managed? It is important, because you do not want to accidentally ship buggy or faulty code. Let's pause for a second. To deliver and ship code mean the same thing—that is, to make that code go "live" for the customers' use. There are three main environments or servers, as follow.
The first is the development environment. Here is where all the coding and testing takes place. Your team will code a new feature and test it. It exists on its own server and cannot be accessed by the public. Therefore, it does not matter if you screw up, because it does not affect the "live" version of the site that lives on a different server.
When the programmer finally feels like the iteration is complete and ready to ship (in the form of an update or new version), it must first go to the staging environment. This is a different server, but this one is an exact copy of the live application. This serves as another check to confirm whether the new feature is ready for final delivery.
If the code seems to work perfectly in the staging environment, the code is ready for deployment. It gets moved to the production environment or production server, which is the one that holds the public site. This is the server that responds to all the users who visit the site. This whole process is designed to make sure that this server is only updated with clean code, though of course no process is perfect and sometimes bugs creep in.
Typically a team has a process for moving code from development to staging to production, and whoever issues those approvals should be attentive when confirming an iteration's acceptability. Nothing is worse than shipping buggy code, because customer headaches and complaints surely follow. Worse, the twitch reflex of many customers is just to abandon flaky websites.
Conclusion
You spend your weekends redesigning your office and installing whiteboards everywhere on the working assumption that the most effective way to manage software teams is to provide them the ideal conditions to manage themselves. You do not want your application to fail because of a poor work environment or bad software development process, so you spend your time immersing yourself in one of the methodologies outlined in this chapter and try your hardest to stay committed to it, because you are confident that in the end it will create and deliver the most value. You have a better understanding of some of the checks and balances required of rolling out new features, but before you introduce any new feature, it would be prudent to make sure it is bug-free and not going to create more chaos than it solves. Your next task it to learn how your team does just that.
1 Eric Ries, The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses. Crown Business, 2011.
2 Cyril Northcote Parkinson, "Parkinson's Law," The Economist, November 19, 1955. Available at http://www.economist.com/node/14116121.







CHAPTER9
Software Development: Debugging and Testing
In 2009, Toyota faced allegations that some of its cars had sticky accelerator pedals. In January 2010, the company recalled 4.1 million Toyota vehicles to repair faulty accelerator pedals. The defect was a bug in the product, and we can draw an analogy between Toyota and pretty much any Internet company. Facebook, Google, and MyAppoly all have and will continue to have bugs, but great teams identify them in the development process before the product is released for customers' use.
It is unrealistic to expect that every line of code your team produces will work perfectly the first time. Required in the process of writing code is debugging, a term used to describe error removal. These errors include not only improper spelling and other syntax-related problems but also problems in logic or how code interacts with other code. Because your team and users will discover bugs in the product routinely and irregularly, debugging is a continuous process that takes an unpredictable amount of time. As MyAppoly grows larger and involves a growing number of engineers, quickly identifying the source of a bug will become increasingly difficult. Understanding how your engineers approach debugging at a conceptual level will help you see how experimental and variable the process is.
A Bug's Life
To put it simply, a bug is an error. Bugs are (unintentionally) created by programmers, but not simple fixes do not solve all bugs. At a high level there are two kinds of bugs: syntax-related and semantic-related. Syntax bugs result from bad code. Your programmer might have forgotten a semicolon, misspelled a variable name, or tried to use a structure not provided by the programming language in use. These bugs are often easy to fix because a compiler, interpreter, or other tool can easily point out when something is blatantly wrong. If code were a Microsoft Word document, some of these errors could be caught by a spell check.
Semantic bugs consist of two types. First, runtime errors cause a program to crash and prevent its execution. An example would be if the program were trying to divide by 0 (a mathematical impossibility). Second, logical errors, which do not disrupt the completion of a program, will produce an unexpected result, such as adding something instead of subtracting it. If you contradict yourself in an essay, you commit an error in logic, but as long as the grammar and spelling are correct, a spell check will not flag it as such. Therefore, semantic errors are more difficult to catch, isolate, and resolve than are syntax errors.
The difficulty of grouping all known bugs into descriptive categories has not prevented debugging gurus from attempting the feat. A team of developers devised the following typology1:

Common bugs are ones that behave predictably and can as a result be fixed relatively easily.
Sporadic bugs appear unexpectedly and produce unexpected results. Isolating and testing them is difficult.
Heisenbugs ring a bell if you studied quantum mechanics or followed Breaking Bad. The Heisenberg uncertainty principle asserts that there is a limit to the precision with which certain pairs of physical properties of a particle, such as position and momentum, can be known simultaneously. The more you learn about one, the less you know about the other. In like fashion, Heisenbugs appear to hide as you begin to look for them, complicating the debugging process. They are a result of memory access violations, attempts to optimize your program by taking "illegal" shortcuts, and the order in which different parts of your program execute. Adding statements that print out text, for example, might change the order of operations and result in a Heisenbug.
Bugs hiding behind bugs are multiple bugs at the same time. Solving these situations can be a confusing task.
Secret bugs are bugs that you cannot identify owing to external reasons, such as confidentiality. For example, a customer testing MyAppoly reports a bug but is unwilling or unable to provide you the relevant details of the incident. You know a bug exists—making MyAppoly unshippable—but nothing more. Secrets are no fun!
Although programmers create all bugs, they are not all due to negligence. Some bugs emerge because code is updated that is no longer compatible with prior logic. Some bugs emerge because two engineers merge their code, and they do not work perfectly together. Just because a bug exists does not mean someone has to be blamed.
The Debugging Process
The six-step debugging process laid out in the following sections is a distillation of the "13 Golden Rules of Debugging" of Grötker and his Synopsys colleagues.2 These six steps are not canonical, but you may be confident when engineers on your team say they are "debugging" a customer-reported issue that they are following some variant of the steps that follow.
1. Track the Problem
To manage the errors of MyAppoly effectively, keeping track of all the bugs is a great start. If you maintain proper documentation on every bug you encounter, this knowledge can help you resolve future issues more rapidly. Additionally, because you have a full team working on the product, tracking can help them stay in sync about problems and their solutions. There are many out-of-the-box tracking systems you can use, most of which operate as follows. When a bug is found, an issue or "ticket" is created that describes the bug. Its status remains open until it is resolved, in which case the status is switched to closed. Several intermediary statuses exist, such as in progress, and tickets can be assigned to different team members as a delegation method.
What constitutes proper documentation? Because bugs can be influenced by any program input, everything that might have contributed to the problem should be recorded. This includes things like the particular action the user took, the operating system, browser, actual behavior, expected behavior, and pretty much anything else you can think of that might be relevant. These variables will either be static—properties related to configuration and compatibility that don't change, such as the operating system and browser—or dynamic—referring to aspects of memory and network that change frequently.
2. Reproduce the Problem
After you learn of a bug and record its details, the next step is to reproduce the problem. This involves re-creating the environment, as well as checking whether the behavior matches the one reported. This is something of a manual process at first, but you want to re-create the program failure so that you can use the same test to confirm whether you have properly fixed it.
3. Develop and Run Tests
After every adjustment made to the code, try to reproduce the failure to see if the last update eliminated the problem. Manually reproducing the problem will be tiring, so a better way to automate the tests will allow you to hone in on the problem. There are several techniques for doing so; the availability of each one is based on which programming language you are using, the specifications of the system overall, and what tools you are willing to buy.
The scientific method will inform your debugging approach: hypothesize, edit code accordingly, test, evaluate, and repeat as necessary. After testing the program following every code adjustment, log the change to create an audit trail so you can backtrack if you need to restore the code to its original state. Additionally, it is good practice to create different versions of the file for every change you make.
How will you test the bugs in MyAppoly before you launch the new version? You can't waste any time because you told your customers that the new version was going to go live in 24 hours. The tests you run will typically occur at one of the three levels: the presentation layer, the functionality layer, or the unit layer.
Although you can test at any of the layers, you will typically choose one based on ease of execution, ease of interaction, and ease of result assessment, among other considerations. Let's dive into this quickly.
The presentation layeris the view component in the model-view-controller description of our application. To test from the presentation layer, you interact with the application as a user ordinarily would, using the mouse and keyboard. Let's say there was a bug in checking some account value on MyAppoly. After a $100 deposit, the account value still reads $0. You could use the presentation layer to check if you fixed the bug by refreshing the web page and reviewing the account value. Testing at this layer is generally feasible and quick. Even so, visibility is limited because you cannot necessarily see how the back end is making the calculation just by looking at the final value.
The functionality layer consists of all the code that defines the functionality of the program—essentially the controller and model parts of MyAppoly. To test at this layer, programmers typically write a script (some code that executes a specific action) to interact with the program. To run the test, they simply execute the script, which is typically as easy as clicking the refresh button on a browser. Automation allows testing at scale. To continue the account example, your team could write a script that updates the value of the account twenty times and automatically checks whether each change is correct. This seems easy, but there are some start-up costs for developers to write the script. If you expect to be running the same test a lot, however, it might be worth the investment.
The last layer is the unit layer. If you could break down the full program into small, one-operation chunks, you would have a unit. In the account example, each of the following could represent a unit: retrieving the old account value, getting the deposit value, calculating the new account value, and storing the new value. Testing these specific units can be automated with a script. These tests give the programmer the most detailed look at how the program is operating at the building block level. Often these tests are of equality, meaning they will test if the expected output equals the actual output. If it is true, the tests will continue; otherwise an error will be reported. In this way, the bug can be immediately traced to a particular area of the program. In the current example, there are several units. First, you can check the value by which the account is going to change. If you entered $100, let's make sure the back end is actually interpreting it as $100. Now, let us get the existing value of our account. Are you getting the right account? Finally, you can add $100 to the existing account value to obtain the final value. Is the final value correct? By breaking it down into a step-by-step analysis, you can determine the specific step at which the failure occurs.
Your programmers have various tools and techniques to aid their efforts to track and test bugs. An easy debugging methodology involves the print function, printf, which prints out pretty much anything you tell it to. In the example of updating your MyAppoly account value, you could add print statements in between every line of code. You could print the old account value, the amount you want to change it by, and the new account value. If any one of those numbers were off, you can then see exactly where things went wrong. In this way, print functions can be used to create unit tests. But adding print statements everywhere in your code, as you can imagine, is messy and tedious. Luckily, there are better ways.
One tool you can use is a debugger. Debuggers essentially stop a program from executing so programmers can make observations. By setting breakpoints, a programmer can effectively create unit tests to see whether values have changed properly as each line of code is executed. Presumably, if an error occurs, the programmer will be able to identify where it occurred. An example of a debugger you might hear of is the GNU debugger GDP, which is run from the command line and is typically associated with the C programming language.
Depending on your programming language, your team could also use the stack trace debugging method. The stack is an area of memory that is used to store active function calls. Let's continue the example from before. The back end manages the actual deposit of $100 into a MyAppoly account. Specifically, there might be a function responsible for all adjustments to a user's account value. When a user adds money to an account, all of the variables, arguments, and return addresses (e.g., the particular amount, the user's account number, the user's existing account value) associated with the function will be stored on the stack as part of a stack frame. Multiple functions can be called before the first function is complete. For example, the "update account value" function requires the "get account value" function, which calls the "get user number" function, which calls the "get user name" function, which calls the "get user zip code" function, and so on. These stack frames pile on top of each other. If there are so many nested function calls, however, enough frames can pile on top of each other that the stack runs out of memory and cannot handle any more frames. This is called a stack overflow error. The stack trace debugging method can help keep track of the stack frames and the particulars of the function calls.
Another type of debugger is a memory debugger, which is common in languages such as C and C++ that give programmers access to memory management.
4. Interpret Test Results to Identify Bug Origin
With every test, you come closer to understanding the root cause of the bug. If you ask your programmers, there is nothing more fulfilling than finding the logical error or missing statement that caused so much pain. Keep in mind that the bug could exist pretty much anywhere, whether in your source code, in the compiler (if you are using a compiled language), or inherited from a third-party library or API.
5. Fix the Bug Locally
At this point, fixing the bug should be relatively easy! Even if it is not simple and requires you to reprogram an entire feature, at least you know why the program is crashing. Your team will run regression tests, which test if all of the other functionality of the site is still working. Fixing one bug and introducing another is hardly the right solution. Assuming your regression tests pass, you can safely assume that you have eliminated the threat without creating more chaos.
6. Deliver Fixed Product
Once you have fixed the bug and run confirmation tests, the MyAppoly team will update the public version of the site, so users are no longer accessing a buggy application. While you were debugging, your team released a notice on the website alerting users of the issue, and you can take down that notice now.
Conclusion
Now you understand a bit more about the process and techniques your team will use to make MyAppoly approach perfection. You do not want any users abandoning MyAppoly due to unreliability. With these tools in hand, your team has built, tested, debugged, and launched a working version of MyAppoly. What is equally impressive is that you understood everything they did, at least on a basic level. Improving the product is always a goal, but the one taken up in the next chapter is mission-critical: get users.
1 Thorsten Grötker, Ulrich Holtmann, Holger Keding, and Markus Wloka, The Developer's Guide to Debugging. Springer, 2008.
2 Ibid.







CHAPTER10
Promoting and Tracking: Attract and Understand Your Users
Everyone can relate to the problem of trying to get someone to do something. In one way or another, we have all tried to convince people to complete a certain action. How do you get your friends to come to your birthday party? How do you keep your employees from taking extra-long coffee breaks? For a tech startup, one pertinent issue is user acquisition. The success of many web applications, including MyAppoly, is often based on how many people actually start using the application. Imagine if Facebook only had two users, for example. It would be pretty ineffective, would it not? Users can learn about your website though a number of different ways, each one called a channel. One might be through a partner organization. For example, if MyAppoly partnered with Facebook, maybe new users might learn about and join MyAppoly because Facebook suggested it. Another channel could through referrals (jargon for word-of-mouth). Yet another could be through an email marketing campaign. As you can see, there are several ways to reach potential customers and convince them that it is worth their time to consider your website. Two channels of increasing importance owing to the rise of search engines are organic and paid search. Organic search refers to the results that pop up from standard search. When you Google something, you notice paid advertisements relevant to your search terms at the very top and along the right side of the search engine results page (SERP). Paid search refers to the presence of these ads.
Since the birth of business, businesses have collected data on their customers to satisfy their need to understand how they are performing operationally. Grocery stores, for example, need to know who their customers are, what products they look at most frequently, which products they purchase, which aisles tend to be most popular, and whether the store layout is optimized for the shopping experience. Intelligently, grocery stores introduced loyalty cards to accomplish some of these tasks. On the surface, customers see this card merely as a tool to obtain discounts, but they are really empowering the stores with their personal data.
Loyalty cards work for grocery stores because customers who enter the store rarely leave without purchasing anything. Online customers are very different. If you were only able to collect data about your MyAppoly customers when they made purchases, you would lose a lot of valuable data. Since websites are so easy to access, users can spend ten seconds on a site from the convenience of their home and determine whether or not it's worth their time. And so, in this dog-eat-dog world of Internet applications, you want to figure out who your customers are, what they want, and what they think about your product. You cannot feasibly interview or survey every user, and so much of this information has to be inferred. But you need to base that inference off of something hard and tangible, and this is where analytics comes in to play.
This chapter describes how websites attract users and how they track what those people are doing.
Search Engine Optimization
It might be hard to imagine, but at one point Google did not exist. In fact, the ability to search across the Internet was nonexistent until about 1994 when a company called Yahoo! created a database of all webpages it could find manually by surfing the web. Yahoo! wanted to give users a way to search through the catalog to find the pages relevant to their query. This was limited in its effectiveness due to the manual process of collecting the pages. From this point, several players worked on creating better search engines. AskJeeves proposed a new method of asking questions that was not exactly a true search but gave a powerful ability to users. These early players fought over their respective strategies, each one uncertain about who would win. Little did they know that they were going to lose the fight to two Stanford University graduate students named Larry Page and Sergey Brin. In 1998, these two students worked on a way to scan the entire Internet, store a description about each page, and finally recall and display them based on a user's search query. The number googol (a one followed by one hundred zeros) inspired the name of their search engine. Google was born, and the nature of how we use the Internet fundamentally changed.
Some estimate that 64 percent of Internet users use search as the number one way they find things on the Internet. Fifty-nine percent of Internet users in the United States use a search engine every day. Search engines can play an important role in getting MyAppoly recognized. To understand search engine optimization as a tactic to bring in new users, it is important to understand how search works at least at a basic level.
Google stores information about all websites and the keywords that describe those websites. When you search for a term or phrase, Google maps your search to the descriptive keywords linked to each website and accordingly shows you the results.
Now how does Google actually collect the name and keywords of websites? Gone are the days of manual entry into a large catalog! Necessity is the mother of innovation, as the adage goes, so as the number of web pages grew at an inordinate rate, the manual process of logging them became untenable. Smart Google engineers have refined their ability to "crawl" over the Internet using botsaka spiders. Bots scan a web page and follow any links contained on the page. If the bots keep doing this, they're bound to cover pretty much the entire Internet. As they scan each page, they record keywords that appear, the number of pages that link to it, and several other characteristics that help to describe the page and assign it a rank comparing its relative importance to others on the same topic. The rank determines the order in which it is displayed when someone searches for a topic on Google, which is important since most users don't look much past the first page of search results; higher rank generally means higher traffic to your site. This is organic search, and the phrase search engine optimization (SEO refers to the process by which you can improve your rank on Google and other search engines).
That is how these search engines build a huge database of webpages and their accompanying keywords. How does the actual matching of search query to database keywords work? Because queries can vary based on capitalization (e.g., Tennis vs. tennis), plurality (e.g., rackets vs. racket), spelling (racket vs. racquet), and the inclusion of nonessential words and phrases (e.g., the and a), search engines typically remove all these variants before checking the query against the database. After the query is matched and all the corresponding web page results are selected, Google must rank them before presenting them to the searcher. After all, that is why Google became the top search engine, right? The best results always seem to be at the top, thus minimizing the time a user spends finding the right link. In some ways, this is where the real magic of search engines lies. The actual crawling of the Internet and creation of the database are predictable, but the information collected and how that information is used to rank and display results are the differentiation factors in search.
Google's ranking algorithm is called PageRank, and it has become very famous over the years. Though the exact algorithm is not public, we have an idea of what factors it considers and, as a result, you may be able to use Google's ranking methodology to inform you how you build your webpages.
One thing the algorithm targets is keyword density. Pages that reference a keyword frequently are more likely to be relevant than ones that mention it less often. A page that mentions "tennis racket" one hundred times on a webpage is more likely related to tennis than a page that contains only one occurrence of "tennis racket." Search engineers quickly realized that web developers might just clutter the page with redundant occurrences of the keyword in order to obtain a higher rank. Smart, huh? Google and other search engines are smart too, so they monitor such behavior and flag it for rank demotion.
Search engines also look for keyword proximity and prominence. They want to make sure that the words tennis and racket appear next to each other, in other words, because if they do, the chances that they relate to the search "tennis racket" are high. Keyword prominence can also indicate the relative importance of the article. Is tennis racket the title of web page, or merely in the footnote of an image? This also factors into the rank, as most would agree it should. Last and certainly not least is link popularity. If a lot of other websites have links that lead to your page, the chances are good that your page is high in importance. This is especially true if important websites (also determined by link popularity and the other factors) have links to your page. To continue the example, if the US Open tennis website linked to your page on tennis rackets, that's an important piece of information and Google gives you credit for it. What they don't give you credit for are links you put on your own page that take you to other well-known sites. In other words, including a link to the U.S. Open site on your own page will not affect your rank. Anyone can put links on their website, so that's not important.
Based on this knowledge, you can optimize your webpages to achieve a higher rank through the process of SEO. You have direct control over things that appear on the webpage. A clear descriptive title, strategic use of keywords (keeping in mind proximity, density, and prominence), and descriptions that you can add to the meta tags (you can use these to provide data to the Google bots) and image tags in HTML all influence your rank. Regularly updating the content on your page might also be perceived as a sign of credibility, as are search specific site maps that define and describe for crawlers your page layout, description, importance of each page, and so forth.
As you may have guessed, it is a lot more difficult to control your link popularity. Short of forcing others to include links to your webpage, there is very little you can do other than building your credibility and awareness.
Suffice it to say that the better you understand how the ranking algorithm of search engines work, the better you will be at SEO.
Search Engine Marketing
If you could advertise your company on Google or Bing, you would get some of the most highly trafficked real estate on the Internet. Luckily, you can.
The concept of search engine marketing (SEM) is quite simple. Let's continue to suppose MyAppoly is a tennis website. You know that users searching for tennis rackets are likely in the market for new tennis gear. In order for your ad to be displayed when a user searches "tennis bag," you must "purchase" the keyword "tennis bag." When a user searches on the term "tennis bag," your advertisement will pop up on the side, and you will be charged the amount at the rate you purchased the keyword for if the user clicks on your end. There is an additional layer of complexity to this, however.
You can imagine that many people want to buy the same keyword. All tennis websites probably want to buy the keyword "tennis bag." How does Google know which ads of all the websites trying to buy any particular keyword to show in the space provided (again, at the top and right of the SERP)? This is determined by a real-time hybrid auction based on two factors: the bid and the quality. When you create your campaign and ads, you specify how much you want to bid. Naturally the higher you bid, the more likely it is that your ad is shown over other competitors who are purchasing the same keyword. This makes sense since Google wants to maximize its revenue and, all else equal, a higher bid is better than a lower one. Quality is equally important, because Google is paid only when a user clicks on the ad. Let's illustrate by example. What if MyAppoly started buying ads for lacrosse helmets thinking that any athlete looking to buy equipment may also be interested in tennis gear? MyAppoly may win the auction for lacrosse helmets, but as soon as users click MyAppoly's ads, they will realize that MyAppoly is a tennis website and not related to lacrosse at all. They will probably hit the back button immediately. Google can track this activity, and if they notice that your ads are not serving the customers what they actually want, your quality score will go down. Therefore, if you want to win auctions, you need to have well-targeted ads (good quality) and bid something that is likely to beat your competitors.
This is what is known as a performance-based advertising model, which is nice because, unlike traditional advertising methods, you only pay if users click on your ads. The incentives for all parties are aligned. The advertiser only pays if the ad is clicked. Google collects money by showing ads that users are likely to click. The user will only see ads that are relevant. Everyone wins! Compare this to the yellow pages, in which model advertisers play a flat amount for placement regardless of the number of people that see it and users have to sift through hundreds of irrelevant ads. This is inefficient.
Search advertising is effective because of the context the search query provides. Wasting money on advertising to the wrong population is a danger of the past, because you only purchase keywords that describe your target market. Additionally, when a user clicks on an ad, you can control where they go on your webpage. Therefore, rather than directing all users to the home­page of MyAppoly, why not direct people who click the "tennis racket" ad to the list of tennis rackets on MyAppoly?
Analytics
Users discover your site through these channels, and you want to track their activity once they arrive. Analytics refers to software programs that generate metrics describing how users are using your site. The history of analytics begins not as a set of user tracking metrics but rather as an error log to assist debugging.1 Early developers created automatically generated web documents that logged errors, hence the name web logs. Over time, however, business and marketing employees realized that the web log data had value beyond debugging. This data could be used to understand the customer.
In 1995, Stephen Turner wrote a program called Analog that analyzed these convoluted log files, extracting the portions of it that were useful for analysis. Webtrends, a later program, helped people visualize the data stored in these logs. The goal of collecting and visualizing meaningful data about website usage is motivated by a desire to make decisions based on information about users and their habits.
What are these data that web application companies are interested in collecting? There are several types, but the one discussed here is clickstream data. When users visit MyAppoly, they leave behind a trail made of all the pages they visited and what they clicked. This record of a user's activity defines the term clickstream data, and there are four main ways of collecting it: web log, web beacon, JavaScript tagging, and packet sniffing.
Web Log
When a user visits a website, the browser sends a request to the server where the website lives. The server updates its logs with something like "User X visited page at 12:00pm" before sending the page back. That's a web log. One downside is that any server request is logged, even the bots of search engines. If you want to monitor only real user activity, this may not be the best method to use. Additionally, unique visitors are difficult to identify which can complicate a marketing analysis. You do not want to count the same person twice, or else your numbers will be off. Lastly, do you recall the term cache? It takes time for browsers to send requests to servers and to receive the page with all of its images and other resources. Therefore, browsers save a copy of the website locally as to minimize the server interaction for efficiency purposes. Web logs will not kick in when the server is not utilized, so all activity on cached versions of this webpage will not be logged. This is an example of server-side data collection (the software used to store the data lives on the server).
Web Beacon
You might be familiar with web beacons if you have received an email with images temporarily hidden until you authorize the e-mail client to show it. This is enabled as a tactic to fight spam. Why? Web beacons typically are 1x1 pixel images that are transparent and live on separate servers. To load an image, recall that browsers have to issue additional server requests. When browsers request this web beacon from a third-party server, their activity is logged. The use of third-party servers offers a bit more flexibility over want you want to collect (either include the web beacon or do not), and allows you to aggregate data across multiple webpages—that is, you can issue requests for this web beacon from a number of websites, all of which must go to the same server to get the 1x1 pixel image. As a result, all the data are stored on the same server, even though the requests came from different sources. With that being said, since this method relies on an image request, any tool that blocks image requests prevents you from gathering useful data. Spammers use this to verify that your email account is still active, so email clients disable images until you authorize their authenticity.
JavaScript Tags
JavaScript tagging changed the nature of web analytics. If you recall from our discussion on JavaScript, the language has listeners that wait for particular events (such as the clicking a button) before executing certain code. In the same way a JavaScript listener might wait to fire code showing a pop up until a user has clicked on a button, JavaScript could use the "page has finished loading" event as a trigger to execute code. If you could make that code collect information about the user, the time, and a description of the webpage, you could use JavaScript as an analytics tool. This is exactly what has happened. As long as you include the line of JavaScript code that fires when the page loads, you are good to go.
There is slightly more action than this simple explanation because cookies can play a large role. As discussed, when you visit a website, it might put something called a cookie on your browser that stores information about you, what you clicked on, and other relevant information. Next time you visit the website, they will look for any cookies, and if they find one, they know that you have been to their site before and can use the contained information to customize the page for you. If the cookie stored information about the shoes you were shopping for, those shoes might appear at the top of the page the next time you visit. Back to the analytics: JavaScript code uses cookies to log activity. It stores a cookie on your browser, and after other parts of the code are finished executing, the information then gets passed to a server for storing. These servers are owned either by the company or by third-party analytics providers.
Whereas before analytics were entirely managed by a company's internal IT team, entire companies have now emerged with the sole purpose of helping others with analytics. These companies provide some JavaScript code for others to put on their websites, and they take care of the rest. The value they provide is also in the form of visualization tools to help you understand your data. Google provides analytics services for free. Google Analytics works by storing data in a log file. Every few hours, Google processes these log files, and then makes the data available for the user to review on its Analytics website.
The benefits of this approach are many, the primary one being ease of use. Simply by pasting code the analytics company provides, you have a full tracking and visualization system up and running. Voilà! Even if your page is cached, the JavaScript code will execute when the user views it, so it beats web logs in that way. You can add special tags to the JavaScript code too that help you filter data more easily afterwards.
With that being said, 2-6 percent of users are estimated to have turned off JavaScript, and so you can expect a small amount of data leakage with this approach. Also, do keep in mind that if you use a third-party analytics provider (i.e., you do not build and collect all of this information yourself), your sensitive MyAppoly data will be in the hands of other people.
Packet Sniffing
Packet sniffing essentially creates a middleman between a website visitor and the MyAppoly server hosting the website. After a user requests the MyAppoly homepage, it passes through a mix of software and hardware called a packet sniffer where information about the user is recorded before being routed to the server. When the server responds with the webpage, the files go back through the packet sniffer before getting routed to the user. That's it. On one pass, it gets the user data, and on the other pass, it gets the webpage information.
What is convenient about this approach is that no additional code needs to be entered in the actual source code and it can easily intercept all requests made to your server, thus covering your entire website's needs. The drawback is the actual setup process, which might be convoluted and will ultimately fall on the heads of the technical people working on your application—I am sure they are busy enough as is. You also have to be careful with packet sniffers since they will intercept a lot of sensitive information such as users' passwords and credit card numbers. Dealing with this data in a privacy-respecting way is crucial.
Visualization
This progression from log files to JavaScript tags and the host of visual tools that accompanied these changes has been rapid, which underscores the important role analytics plays. The particular way data are displayed influences the insights you can draw. As a result, many players are trying to make this process easier still. The next stage of analytics, therefore, seems to be not so much aggregation-focused but visualization- and processing-focused.
Rather than look at information like number of visitors in data tables, click density (aka site overlay) provides insights on what users might be thinking based on what they are clicking. The eventual development of heat maps accomplishes the same thing. Heat maps are images of a web page colored to indicate where users are looking based on the mouse movements are a proxy for eye movements. Dark red is used for areas in which the mouse often hovers, whereas blue areas are locations on the page where the mouse rarely goes.
Analytics should seek to answer not only the what (how many views, how many signups, and so forth) but also the why. Focus has been redirected to tasks, with funnels being the popular way to visualize task completion. In this way, companies can see exactly how many people drop off at each stage of the process.
The list of visualization types could continue indefinitely, but the main point is that these data are valuable to companies, and they are looking for any way to make sense of them. Presentation matters.
Conclusion
Keep in mind that several tools have been omitted in this discussion. If you sign up for Google Analytics, you will notice that there are many helpful methods to segment the data and extract hidden insights. For example, Google Analytics allows you to see what words people search for on search engines before coming to your website. This is called site search analytics (SSA), and unlike from clickstream analyses and heat maps from which you infer things about users, SSA provides context.
Now that you have been primed on the basics of promoting your website, increasing your rank in searches, and on how to track your users, MyAppoly is ready for the big leagues. It's ready to grow. But can you handle the growth?
1 Avinash Kaushik, Web Analytics 2.0: The Art of Online Accountability and Science of Customer Centricity. Sybex, 2009.







CHAPTER11
Performance and Scalability
As an eager entrepreneur, you stay up "on-call" most nights monitoring MyAppoly's usage and making sure that any issues are addressed immediately. You do not want your site's reputation to be compromised by an inability to address a situation right away. You spend time sifting through the analytics reports that your team has given you, and you notice signs of success. The number of users you have is growing rapidly. Your team approaches you one day and recommends that you set up a meeting to discuss how to improve the site now that it is getting unprecedented traffic. The perfectionists among the engineers are complaining that certain pages are much too slow compared to their potential and that the current setup cannot support many more users, especially at the rate of growth the metrics suggest. These are good problems to have because it means you are doing something right, but they still need to be addressed.
Performance relates to a website's speed, how fast it loads, how many bytes are transferred, and any other metric that can measure the rate at which a user views requested information. Poorly performing websites in this age of instant gratification and low patience mean fewer users, falling revenue, and declining value.
Scalability refers to the ability of a web application to handle a growing amount of work and to expand as demand increases. It is about how well your site can accommodate more users, more data, and more computation. Not just one more or a few more, but thousands or even millions more.
From these definitions of performance and scalability, you can see that they are not exactly the same thing but are issues that might arise together. If your user base is growing, you want to make sure your site is operating as efficiently as possible. Performance enhancing techniques might enable you to use your hardware more effectively, which is linked to scalability. Your team prepares a primer on techniques they are researching and hoping to implement for MyAppoly. The list is not exhaustive but gives you a sense for how companies approach these issues.
Practices to Improve Performance
The back end and front end can be optimized to improve performance. The back end only accounts for 10-20 percent of the overall time it takes for a page to load, whereas 80-90 percent of the time is spent downloading components for the user interface—such as issuing HTTP requests for the assortment of images, videos, audio files, and so forth that appear on a given web page. This might influence how you prioritize and allocate resources, though page load time is just one of many things to consider (time, costs, etc.).
Before jumping to changes you can make in the back end or front end, your server selection can affect performance. A user's proximity to your servers does have an impact on page load time. In a world where your users are coming from all areas of the world, having all your servers in one location might be suboptimal. A content delivery network (CDN) is a collection of web servers already distributed across geographic areas so your users can be directed to the optimal one. This point illustrates that factors beyond your control may affect performance. Now, for the things you can influence.
Back End Considerations
There are a few key drivers to backend efficiency that have their roots in the good design principles already mentioned such as abstraction, which allows you to reuse code effectively in different sections of your application.
Additionally, successful technology companies go back to rewrite their code to make things more efficient. Optimizing database queries and other operations (such as algorithms for determining recommendations for movies on Netflix) is one step, but another way is to completely rewrite the application in a different programming language. Many technology companies rewrite their application in lower-level programming languages that provide greater control over memory usage and are closer to what a computer actually understands. As a result, they are often more efficient. Twitter, for example, has been shifting away from Ruby on Rails to Java Virtual Machine. Facebook has created a way to transform its PHP code into highly optimized C++ programmatically. The list continues, but because this is an intensive process, companies make this transition when they are at significant scale and have the resources to devote to this endeavor.
In database-driven websites, webpages rely on a lot of data stored in databases. If the same information is requested regularly (for example, you keep revisiting your "My Friends" page on Facebook), it makes little sense to fetch the full list of friends from the database every time. Facebook, Twitter, Wikipedia, YouTube, and others use Memcached to save data and objects in RAM—somewhere easily accessible.
Front End Considerations
When a user visits your website, a small percentage of the total time it takes to load a page is spent on the initial HTTP request requesting the HTML document. Some component of the time is attributed to backend processing, but a bulk, as mentioned, is spent downloading all the various resources associated with the particular web page. This section highlights some practices, tools, and methods for all frontend developers to think about when performance is the objective.
Fewer HTTP Requests 
Because 80-90 percent of page-load time owes to the subsequent HTTP requests required to download the various resources of the page, having fewer of them shortens page-load time. There are a variety of methods such as combining scripts and stylesheets into a fewer number of documents, using CSS sprites, image maps, and inline images—all of which can help to reduce load time by up to 50 percent.
As you may have seen on websites, an image can serve as a hyperlink. When you click on an image, it might take you to another web page, in other words. The browser had to issue a separate request to download the image that is on the page. Imagine if you had a page of ten images with each image linking to a different site. The browser would be required to issue ten HTTP requests to gather the images alone. What if, instead, you could combine all the images into a single file and associate links to different areas of the image? Suppose, for example, that the image were a python on MyAppoly. The head could serve as a link to the homepage, whereas the tail could serve as a link to the footnotes section. This is an image map.
Instead of displaying an entire image, you can use different CSS attributes to hide portions of an image you do not want to display. This is a CSS sprite and it allows you to use the same image in different areas of your site.
These are among the many methods you can use to reduce the number of HTTP requests yet maintain the quality of your site. By thinking creatively about CSS and HTML, you can trim many inefficiencies.
Cache
If your users return to the same pages over and over, it makes little sense for them to re-request all of the resources on the page each time. You don't ask your best friends for their cell-phone numbers every time you meet, do you? At some point, they would just ask you to save it. In the same way, your browser gets fed up with asking the server and instead stores webpages in a place called cache. Examples of resources that are cached include images, CSS files, and JavaScript files, all of which change relatively few times compared to the HTML. This is why it is good to keep CSS and JavaScript in external sheets. It enables caching, which is vital to performance. There are several other tools and techniques that can enhance the effects of caching. For example, your browser has to still check with the server to make sure its cached copies are up to date. This request is a waste of time. Instead, if the HTML document contains an expired header with a date, the browser knows for how long the cached copy is valid, thus saving page-load time.
Compression
Have you ever opened a file on your computer that was a .zip? It was somehow condensed into a smaller size in some obscure format that had to then be unzipped or unpackaged? That is the key behind gzipping resources. By reducing the size of the response, the number of packets is fewer and the time it takes to transfer them from the server to the browser is reduced. Keep in mind that there are costs entailed with compression. It takes additional CPU cycles to compress on the server and for the browser to decompress the files. Therefore, it does not make sense to compress every possible resource, so this is another cost-benefit analysis for your team!
Placement of Scripts and Stylesheets
Remember that stylesheets (CSS documents) and scripts (JavaScript documents) are often externally linked to an HTML document for the benefits of caching. Where you link these documents, however, will determine how the page will load. Typically, you want progressive loading, which means you want the browser to show whatever content it has available to show immediately. Let's say it will take a page 5 seconds to load. In the first scenario, the screen is blank for the full five seconds, after which the entire page is instantaneously shown. In the second scenario, the page loads in parts over the five seconds until it is finished. Which is better? Many would say the latter, because users are uncomfortable with unresponsiveness and like to know that something is happening. You want to put the stylesheets at the top and the scripts at the bottom with continuous loading as the objective. Many browsers do not load elements until they have processed the stylesheet so they do not have to redraw any elements. Thus, stylesheets should not go at the top. For scripts, all content below the script is halted from rendering until the script has been fully downloaded, which is why scripts should be moved to the end.
Minification and Obfuscation
What is another way you can reduce the size of a document? One way would be to delete all the unneeded whitespace, comments, and characters from a document. In this way, the document is minified and the amount of information that has to be relayed from the server to the browser is reduced. Obfuscation—the art of making your code illegible—can actually lengthen your code but is still widely practiced. Haven't you seen The Social Network? Everyone is out to steal your code, so you better make it look unintelligible. (Though it's prudent to have a healthy level of skepticism, not everyone is out to rob you of your work.)
How Do You Physically Manage More Users?
The story of MyAppoly illustrates how typical startups scale. It's very different from how traditional brick-and-mortar businesses scale (such as Starbucks opening new coffee shops) but it shows some similar characteristics. Most web application endeavors, such as MyAppoly, start with a single server linked to a single database with a set amount of storage. As more users start using your application, you realize you need multiple servers to respond to all of their requests. After more users join, however, and they begin interacting with each other, the sheer amount of data you collect overwhelms your capacity. Thus, you have to grow your databases. One term often mentioned with scaling is replication, the process of duplicating what you have. In the case of web applications, you could duplicate your database and server on other hardware to serve more users. Because you want to show every user the same home page, the solution of creating duplicate servers makes sense.
With databases, it is a bit trickier. If the queries are only "reads"—meaning the user wants to access information stored in the database—multiple copies of the database would work to satisfy a growing user base. What if a user wants to "write" or change what is stored in the database—say, add a friend? You have to make sure all the databases get updated. It's probably inefficient to add your "new friend" to all copies of the database, so you resolve this situation with a master database designated as the write database, such that all updates have to be handled by this database alone. All other "slave" copies of the database are used purely for reads. In this way, databases can be scalable.
Apart from throwing in more resources in the form of servers and databases, how do you actually direct users to the different copies that exist? You have to use a load balancer to make sure that no one server is overwhelmed with requests and, instead, all requests are equally balanced across all available servers. Load balancers can employ any number of sophisticated algorithms to decide what the optimal assignment pattern is. One that is quite easy to understand is the round-robin approach, which involves circulating incoming users to different servers. This method is imperfect, however, because it does not take into account the actual availability of the servers and so may be suboptimal. You might accidentally assign two heavy users to the same server, even though other servers are being used more lightly. An assignment based on usage could work better. Reliability is another benefit: load balancers can redirect traffic if a server goes down. How exactly you add the additional CPUs to manage the problem is a hardware issue best referred to a specialty company.
Conclusion
There is much uncovered material that you will undoubtedly save for the discussion with your team. But you come away realizing that performance is key and you now have a perspective on how developers think about improving the load time. You also realize that as your web app starts to scale, you have to pay for more resources to support them all. It is not as easy as basic replication because there is a trade-off between cost and performance. That is a very real problem—but, again, a good one to have. Another big problem emerges as you grow: your site is more prone to security attacks. The next chapter exposes you to some of the big attacks and the methods for stopping them.







CHAPTER12
Security Threats:  To Defend and Protect
In 2012, an attacker obtained around 8 million hashed passwords from LinkedIn and eHarmony. Before long, LinkedIn was confronted with a $5 million class-action lawsuit over its security practices. It seems as if every month a major security breach at some large company hits the news, alarming or harming users and driving them to seek minimal exposure and avoid openness. The Internet presents fields of opportunity for virtuous entrepreneurs and customers, but malicious people prowl the Internet too.
Criminal and commercial hackers pursue money, personal data, intelligence, and power; vandal hackers punish political enemies, celebrate anarchy, or create embarrassment. One of the most famous hacker groups is Anonymous, whose anonymous members are united by their staunch opposition to Internet censorship and surveillance, Anonymous has attacked several government websites as well as private applications. In 2012, Time listed Anonymous on its list of the most influential groups in the world. Hackers derive power from their ability to exploit holes of which others are unaware. Large companies paid $32.8 billion on computer security in 2012. Your MyAppoly team is well versed on some of the most virulent security attacks and lays out its concerns to you as follow.
General Classification of Attacks
There are multiple ways of classifying the various types of attacks to which your web application can fall victim, often presented mnemonically as acronyms. Two such acronymic classifications spell out as follows:
IIMF

I. Interception refers to the act of accessing and reading information to which you are not supposed to be privy.
I. Interruption is the act of preventing legitimate users from using the website.
M. Modification is the changing of data that is not supposed to be changed.
F.Fabrication is the creation of data, essentially a type of forgery.
STRIDE

S. Spoofing refers to identity theft.
T. Tampering is the modification of data that should not be modifiable.
R. Repudiation allows users to deny that they ever completed a given action (such as withdrawing money from an account).
I. Information disclosure refers to the leaking of information to users who should not be able to read it.
D. Denial of service means blocking other users from using a website.
E.Elevation of privilege characterizes the scenario in which a user is able to gain access and perform actions that exceed his or her proper authorization.
These acronyms aspire to be comprehensive, but they are not mutually exclusive. Attacks can involve simultaneous update of existing information and fabrication of new data, for example. Spoofing might allow you to read information you should not, tamper with it, and prevent the actual user from ever accessing his or her account again. Several attack types often occur together.
Good Practices to Adopt
The lesson of this chapter is not that people are out there to get you. As your site expands and is more successful, your data and users' information accrue greater value. So as long as you take some general precautions, you will at least have covered the obvious bases. No one can say you are completely protected, but at least you will have made your attackers work for their rewards. The list of precautions could fill a book, but here are four that will get you started thinking about the right things:

Don't trust the user. You want to provide the best experience to the user but that does not mean you need to trust them. If you are asking for their birthday, do not assume they will necessarily enter a birthday. What if they tried to type in a word instead? Or worse, as we will soon find out, what if they typed in some malicious code that your server actually ran causing something bad to happen? In short, make sure you validate the user's actions at every step and at every layer set up checks at the user layer, the model layer, and the database layer. Better safe than sorry!
Reduce attack surface. Some websites allow users to perform a number of actions without signing in and creating an account. Any functionality that is exposed and can be accessed by unauthorized users constitutes an attack surface and potentially leaves your site vulnerable. Minimize this to whatever extent is possible without compromising your user experience too significantly.
Keep as much hidden as possible. The more you expose, especially of your code, the more the attacker can use against you. If you accidentally reveal the structure of your file directory (where your files are located on your server and what they are called), your attackers might be able to access, change, or delete them.
Establish privileges. Every user on your site has specific access authorizations and every developer on your team has a specific scope for the application of his or her expertise. Establish a system of privileges as discussed in Chapter 5 that grants users and developers rights appropriate to their roles and expertise. People without the proper credentials should not be able to touch sensitive information.
Specific Problems and Solutions
The Open Web Application Security Project (OWASP) is an open community of contributors and companies dedicated to "enabling organizations to develop, purchase, and maintain applications that can be trusted." It produces free tools and documents for anyone to put to use improving overall application security on the Internet. OWASP's list of the top 10 risks facing web applications in 2013 is summarized in the following sections.1
1. Injection
Injections are dangerous because they exploit the very tools you give your users. Attackers can construct lines of code to input into your applications to cause disruption. SQL injections refer to attacks in which the code is directed at the database. For example, let's say you ask me for my first name on MyAppoly. In the name field, I might write code that told the database to delete itself. Upon submitting the form, if your system does not actually check to validate the input as a name, the server might actually run the code. Injections allow attackers to run their code on your servers.
The defense against such attacks include making sure that all user input is carefully analyzed before converting it into a query or command.
2. Broken Authentication and Session Management
Servers cannot identify who made a particular HTTP request, they simply respond to them (this quality is referred to as HTTP's statelessness). Applications, however, often need to remember who is issuing the request so it can access the right rows in a database, for example. If we are discussing an online banking web application, we would hope that the balance shown for your checking account is in fact your account. How does it remember you after you sign in though? Essentially, you are assigned something called a token or session identifier that is linked to your data. When you issue a request, your token gets sent along too. The server can verify that you have an active session (that is, you are logged in) and can then go about executing your request. What if someone were to steal your session ID? Or even your username and password? They could do everything you would be able to when logged in. They could withdraw all of your money. They could send an email on your behalf asking for money from friends.
Web applications must defend against such attacks by using strong authentication and session management controls. No skimping!
3. Cross-Site Scripting
Cross-site scripting (XSS) is essentially HTML injection whereby attackers contrive to insert or replace their own code onto your web pages. On MyAppoly's homepage, an attacker might be able to also put a "Credit Card Number" field. When the form is completed a user's info could be sent to the attacker. Cookies can even be stolen, allowing attackers to impersonate others when they visit websites. Cookies, recall, are receipts you leave behind when you visit a site. If I show your receipt to a website, I might be able to trick it into thinking that I am you. Attackers use this method to commit identity theft.
The key to preventing these attacks is to make sure that intended HTML and browser content is kept separate from untrusted data, by encouraging your users to use an up-to-date browser with strong security features and by validating all input. One technique is to encode all output (for example, use "&quot;" instead of an actual quotation mark). If your code is quirky, you'll know when something doesn't quite fit the pattern. This might help your system flag foreign, untrusted code.
4. Insecure Direct Object References
You do not want any attacker getting hold of a particular file name because it might allow them to access the actual file on the server. Even worse, if all of your file names follow a pattern, the attacker may be able to guess the name of and access other files. For example, if you label all photos of users as "photo1," "photo2," and so forth, the attacker can reasonably guess that "photo5" might also exist.
In order to defend from these attacks, developers should refrain from mentioning or exposing the exact names of resources.
5. Security Misconfiguration
After files are moved from the engineer's computer to the actual server off which the site is hosted (the production server), certain privileges have to be assigned to each file.  Some files can remain hidden, for example, while others can have full read and write access. This is one of the many security configurations that can be set by your team, and any misconfiguration can result in easy attacks.
Paying close attention to how security is configured and building in a process to confirm that no patches exist will significantly reduce the frequency of such attacks.
6. Sensitive Data Exposure
Passwords should never be stored in plaintext. If an attacker or anyone authorized were able to look at your database and figure out all of your user's passwords, you would have a huge problem on your hands. Even your team should not be able to see passwords.  Thus, most technology companies input passwords through a cryptographic hash function that obscures them completely but outputs them uniquely. One common hash function is the SHA1 function. From the stored SHA1 version of a password, no one can guess what the plaintext password is, nor is there a way to reverse the hash function to obtain the plaintext version. The takeaway is to use a strong hash function and to make sure that information that is sensitive is securely encrypted in this manner. Always.
Secure Sockets Layer (SSL) and Transport Layer Security (TLS) are cryptographic defense tools used to make sure that any data transferred between the browser and the server are not tampered with or intercepted. As you know, browsers interact with servers all of the time and all the packets of information are traveling across the hardware of the Internet. It is not unrealistic to think that some of these packets might be intercepted and changed from time to time. Luckily, technologies exist to encrypt the data as it travels. Websites are beginning to use such technologies such as SSL/TLS on login pages so that sensitive information such as passwords is not lost. Securing your site with SSL/TLS costs money and might slow down your site, but your users will eventually demand it if they twig that you are skimping just to save some money.
7. Missing Function Level Access Control
Users typically navigate around a site by clicking on the various links a webpage displays. To go from the main page of the MyAppoly online clothing store to the men's section, for example, you might click the link that reads "Men." If the MyAppoly store decided to hide the link to the men's section, would you be unable to navigate to that page? You could guess it. Maybe it's www.myappoly.com/men? What if we considered a more harmful example? If I see that my online MyAppoly banking URL is myappoly.com/vinay/privateaccount, I might be able to guess that my friend Bob's account can be accessed from myappoly.com/bob/privateaccount. This might be an overly simple example, but Bob's a pretty simple guy. The point is that URLs can often be guessed. Thus, make sure that access to any web page is authorized.
8. Cross-Site Request Forgery
Cookies are stored on your browser and are used by web sites to recognize you when you return. They have a lifetime that lasts longer than your visit to a website. Let's say you visit an online shopping website and purchase a few items. You leave the site and your cookie is still active with the information about what you clicked on, bought, and so on. Attacker Bob comes along on the Internet and somehow gets you to open a page to some random website, perhaps by sending you an email or through some sort of pop-up box issued via . When you click on the link, it takes you to a website. This otherwise harmless website, however, under the hood could be asking your browser to issue a request to contact the shopping website. Because your cookie is still active, the shopping website authenticates you, and executes some action that the attacker wanted, such as ordering an item. In cross-site request forgery (CSRF) attacks, hackers leverage your browser's credentials to execute requests.
Preventing these attacks could involve using an additional token that is unique to each session. Therefore, even though a cookie may still be active, the token (which should be hidden) will indicate that certain requests are invalid.
9. Using Known Vulnerable Components
Certain libraries and other code you may leverage may be replete with vulnerabilities. If you know about the vulnerability, look out for new versions of the code to use instead, because contributed code is regularly updated to minimize bugs and vulnerability.
10. Unvalidated Redirects and Forwards
A URL redirect or URL forward expresses itself you attempt to access a particular URL and a different URL opens. For example, Facebook and Google have purchased most of the URLs that sound like their name. If you type in googl.com, google.com will still appear. How is that possible? Google knows that there are common misspellings of its search engine, so using a URL redirect, it can capture these URL typos and auto-forward the user to the proper site. These redirects are often used for advertisements as well. When trying to access an article on many news sites, you may be taken to an advertisement page before being auto-redirected to the article. When your users are trying to access your site, attackers may be able to trick your users into clicking links that redirect them to other unsafe pages. If you use many redirects on your own website, users will find it difficult to distinguish between legitimate and dangerous redirects.
Providing a way for users to authenticate valid redirects is helpful—but try just avoiding them all together.
Conclusion
Your team has briefed you on enough threats that you could spend a lifetime securing against them. Making sure your team is careful at every step is essential to providing baseline defense. As in the fight against terrorism, there is an ongoing shadowy but sharp battle between attackers and defenders, with each side trying to develop the next best thing to make their goals easier to accomplish.
Security will only be an issue if your site is worth hacking, so look on the bright side when you start seeing concerns surface!
1 Open Web Application Security Project, "OWASP Top 10-2013: The Ten Most Critical Web Application Security Risks." www.owasp.org/index.php/OWASP_Top_10







CONCLUSIONC
Conclusion
This book is designed to acquaint non-techies with the "need-to-know" technical aspects of Internet applications. There will always be a back end, a debugging methodology, and security concerns, so the concepts in this book will never be rendered obsolete no matter how rapid the evolution of the technologies.
There it is. You are now initiated into some of the deepest secrets of web applications. You have become familiar with the terminology, events, and requirements called into play by an Internet startup. Next time you find yourself in a business meeting that touches on some technical aspects of your company's latest web application release or sitting in a social group chatting about the newest application everyone is registering for—don't cower. Join in, emboldened by what you have learned in this book!
And good luck with your MyAppoly!








I
Index
A

Advanced Research Projects Agency (ARPA)
Advanced Research Projects Agency Network (ARPANET)

Ajax

Amazon Elastic Cloud Compute (EC2)

Amazon Web Services (AWS)

Application programming interfaces (APIs)
    advantages
    authentication
        client-side certificates
        HTTP authentication
        message-based authentication
        Open API
        SSL endpoint
    disadvantages
    documentation
    JSON
    MyAppoly API
        advantages
        disadvantages
    REST
    SOAP
    working principle
B

Back end technology. See Programming languages


Berkley Software Distribution (BSD) license
C

Cascading Style Sheets (CSS)

Content delivery network (CDN)

Cross-site request forgery (CSRF)

Cross-site scripting (XSS)
D

Database management system (DMBS)

Database systems
    architecture
    big data
    centralized system
    concurrency
    data
    data model
        NoSQL
        object-oriented model
        object-relational models
        relational model
        XML
    distributed system
    entity-relationship model
    hardware
    optimization
    security
        data encryption
        DBMSs
        goal of
        MyAppoly
    software
    users

Debugging process
    fix bug
    functionality layer
    logical error/missing statement
    presentation layer
    regression tests
    reproduce the problem
    stack overflow error
    tracking
    unit layer
    unit tests

Document Object Model (DOM)

Dynamic HTML (DHTML)
E

Entity-relationship model
F

Feeds
    competitors
    legal
    rich site summary/atom
    security
    update frequency

Feed-side caching

File Transfer Protocol (FTP)

Frontend technology
    Ajax
    CSS
    HTML
    information design
    interaction design
    JavaScript
        DHTML
        document object model
        event handler
        jQuery
        scripting languages
    portability and accessibility
        responsive design
        web standards
    XHTML
    XML
G

Generation-side caching

GET and POST method

Google
H

Hosting
    bandwidth
    cloud computing
        AWS
        benefits
        disadvantages
        information and software
        NIST
        personal computers
    definition
    disk space allowance
    distribution
    e-mail addresses
    FTP
    prices
    reliability and uptime
    server type
    types of

HTTP requests
    CSS sprite
    hyperlink
    image map

Hyper text markup language (HTML)
I

Infrastructure as a Service (IaaS)

Integrated Development Editors (IDEs)

Internet
    ARPANET
    components
    HTTP
        cookies
        MyAppoly
        server
        TCP/IP
        URL
    Internet Protocol
    packet switching
    TCP

Internet Engineering Task Force (IETF)

Iterative and incremental development
    iterations
        client-driven approach
        incremental iteration
        rapid iterations
        risk-driven approach
        timeboxing
    unified process (UP)
J

JavaScript
    DHTML
    document object model
    event handler
    jQuery
    scripting languages
    tagging

JavaScript Object Notation (JSON)

jQuery
K

Keyword density
L

Library
M

Model-view-controller (MVC)

MyAppoly
N

Non-relational model

NoSQL
O

Object Query Language (OQL)

Open-source projects
    definition
    FSF definition
    OSI
        academic licenses
        free redistribution
        integrity
        license distribution
        modifications and derived works
        no discrimination, fields of endeavor
        no discrimination, persons or groups
        reciprocal licenses
        source code

Open Web Application Security Project (OWASP)
    authentication and session management
    CSRF
    function level access control
    injections
    insecure direct object references
    security misconfiguration
    SHA1 function
    SSL/TLS
    URL redirect vs. URL forward
    vulnerable components
    XSS
P, Q

Packet sniffing

PageRank

Performance
    backend
        database-driven websites
        lower-level programming
        Netflix
        PHP code
    bit trickier
    brick-and-mortar
    frontend developers
        cache
        compression
        HTTP requests (see HTTP requests)
        minification
        obfuscation
        scripts and stylesheets
    improve performance practices
    load balancer
    round-robin approach

Platform as a Service (PaaS)

Programming languages
    APIs
    applicability
    assembly language
    back end
    bit and byte
    committed community
    development time
    documentation
    frameworks
    front end
    high-level languages
        compiled languages
        functional languages
        imperative languages
        interpreted languages
        markup languages
        object-oriented languages
        parallel programming languages
        PHP and Python
        scripting
    IDEs
    library and tools
    maintainability
    reliably update
    talent pool
    technical and design parameters

Promoting and tracking
    analytics
        Analog program
        clickstream data
        history of
        JavaScript tagging
        packet sniffing
        web beacons
        web log
    channel
    grocery stores
    organic search
    paid search
    search engine marketing
        performance-based advertising model
        real-time hybrid auction
    search engine optimization
        credibility
        Google
        keyword density
        proximity and prominence
        Yahoo
    visualization
        heat maps
        site overlay
R

Representational State Transfer (REST)

Revision control
    benefits
    description
    taxonomy
        centralized vs. distributed systems
        file locking
        version merging
    version control terminology
        branching
        checkins
        checkout
        conflicts
        differences
        merging
        tagging
S

Scalability

Search advertising

Search engine optimization
    Google
        bots
        MyAppoly
        PageRank
        search query matching
    Yahoo

Search engine results page (SERP)

Security threats
    attacks types
        IIMF
        precautions
        STRIDE
    OWASP
        authentication and session management
        CSRF
        function level access control
        injections
        insecure direct object references
        security misconfiguration
        SHA1 function
        SSL/TLS
        URL redirect vs. URL forward
        vulnerable components
        XSS

Simple Object Access Protocol (SOAP)

Software as a Service (SaaS)

Software development
    agile development
        principles
        product backlog
        scrum and extreme programming
    benefits
    bugs hiding behind bugs
    common bugs
    debugging process
        fix bug
        functionality layer
        logical error/missing statement
        presentation layer
        regression tests
        reproduce the problem
        stack overflow error
        tests
        tracking
        unit layer
        unit tests
    documentation and commenting
    Heisenberg uncertainty principle
    iterative and incremental development
        iterations
        rapid iterations
        timeboxing
        unified process (UP)
    program architecture
        code reusability
        maintainability
        MVC
        separation of concerns
        three-tiered architecture
    release management
        deployment
        development environment
        shipping buggy code
        staging environment
    revision control (see Revision control)
    secret bugs
    semantic bugs
        logical errors
        runtime errors
    sporadic bugs
    syntax bugs
    waterfall model

Standard Generalized Markup Language (SGML)

Structured Query Language (SQL)
T

Timestamp-based caching
U

U.S. National Institute of Standards and Technology (NIST)
V

Version control terminology
    branching
    checkins
    checkout
    conflicts
    differences
    merging
    tagging
W, X

Waterfall development model

Web beacons

Web log

Web Services Description Language (WSDL) file

World Wide Web Consortium (W3C)
Y, Z

Yahoo!








Other Apress Business Titles You Will Find Useful


From Techie to BossCromar978-1-4302-5932-9
Managing Projects inthe Real WorldMcBride978-1-4302-6511-5
How to Recruit and HireGreat Software EngineersMcCuller978-1-4302-4917-7


Managing Humans,2nd EditionLopp978-1-4302-4314-4
No Drama ProjectManagementGerardi978-1-4302-3990-1
Preventing Good Peoplefrom Doing Bad ThingsAnderson / Mutch978-1-4302-3921-5


Technical SupportEssentialsSanchez978-1-4302-2547-8
How to Secure YourH-1B VisaBach / Werner978-1-4302-4728-9
Tech Job HuntHandbookGrossman978-1-4302-4548-3


Available at www.apress.com




